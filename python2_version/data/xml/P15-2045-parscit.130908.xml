<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016216">
<title confidence="0.992913">
Improving distant supervision using inference learning
</title>
<author confidence="0.994586">
Roland Roller1, Eneko Agirre2, Aitor Soroa2 and Mark Stevenson1
</author>
<affiliation confidence="0.977419">
1 Department of Computer Science, University of Sheffield
</affiliation>
<email confidence="0.835437">
roland.roller,mark.stevenson@sheffield.ac.uk
</email>
<address confidence="0.330582">
2 IXA NLP group, University of the Basque Country
</address>
<email confidence="0.96705">
e.agirre,a.soroa@ehu.eus
</email>
<sectionHeader confidence="0.992507" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998210625">
Distant supervision is a widely applied ap-
proach to automatic training of relation
extraction systems and has the advantage
that it can generate large amounts of la-
belled data with minimal effort. How-
ever, this data may contain errors and
consequently systems trained using dis-
tant supervision tend not to perform as
well as those based on manually labelled
data. This work proposes a novel method
for detecting potential false negative train-
ing examples using a knowledge inference
method. Results show that our approach
improves the performance of relation ex-
traction systems trained using distantly su-
pervised data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855846153846">
Distantly supervised relation extraction relies on
automatically labelled data generated using infor-
mation from a knowledge base. A sentence is
annotated as a positive example if it contains a
pair of entities that are related in the knowledge
base. Negative training data is often generated us-
ing a closed world assumption: pairs of entities not
listed in the knowledge base are assumed to be un-
related and sentences containing them considered
to be negative training examples. However this as-
sumption is violated when the knowledge base is
incomplete which can lead to sentences containing
instances of relations being wrongly annotated as
negative examples.
We propose a method to improve the quality of
distantly supervised data by identifying possible
wrongly annotated negative instances. Our pro-
posed method includes a version of the Path Rank-
ing Algorithm (PRA) (Lao and Cohen, 2010; Lao
et al., 2011) which infers relation paths by com-
bining random walks though a knowledge base.
We use this knowledge inference to detect possi-
ble false negatives (or at least entity pairs closely
connected to a target relation) in automatically la-
belled training data and show that their removal
can improve relation extraction performance.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999608242424243">
Distant supervision is widely used to train relation
extraction systems with Freebase and Wikipedia
commonly being used as knowledge bases, e.g.
(Mintz et al., 2009; Riedel et al., 2010; Krause
et al., 2012; Zhang et al., 2013; Min et al., 2013;
Ritter et al., 2013). The main advantage is its
ability to automatically generate large amounts of
training data automatically. On the other hand,
this automatically labelled data is noisy and usu-
ally generates lower performance than approaches
trained using manually labelled data. A range of
filtering approaches have been applied to address
this problem including multi-class SVM (Nguyen
and Moschitti, 2011) and Multi-Instance learn-
ing methods (Riedel et al., 2010; Surdeanu et al.,
2012). These approaches take into account the fact
that entities might occur in different relations at
the same time and may not necessarily express the
target relation. Other approaches focus directly on
the noise in the data. For instance Takamatsu et al.
(2012) use a generative model to predict incorrect
data while Intxaurrondo et al. (2013) use a range
of heuristics including PMI to remove noise. Au-
genstein et al. (2014) apply techniques to detect
highly ambiguous entity pairs and discard them
from their labelled training set.
This work proposes a novel approach to the
problem by applying an inference learning method
to identify potential false negatives in distantly la-
belled data. Our method makes use of a modi-
fied version of PRA to learn relation paths from a
knowledge base and uses this information to iden-
tify false negatives.
</bodyText>
<page confidence="0.921227">
273
</page>
<bodyText confidence="0.293531666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.96226" genericHeader="method">
3 Data and Methods
</sectionHeader>
<bodyText confidence="0.999937">
We chose to apply our approach to relation ex-
traction tasks from the biomedical domain since
this has proved to be an important problem within
these documents (Jensen et al., 2006; Hahn et al.,
2012; Cohen and Hunter, 2013; Roller and Steven-
son, 2014). In addition, the first application of dis-
tant supervision was to biomedical journal articles
(Craven and Kumlien, 1999). In addition, the most
widely used knowledge source in this domain, the
UMLS Metathesaurus (Bodenreider, 2004), is an
ideal resource to apply inference learning given its
rich structure.
We develop classifiers to identify relations
found in two subsets of UMLS: the National Drug
File-Reference Terminology (ND-FRT) and the
National Cancer Institute Thesaurus (NCI). A cor-
pus of approximately 1,000,000 publications is
used to create the distantly supervised training
data. The corpus contains abstracts published be-
tween 1990 and 2001 annotated with UMLS con-
cepts using MetaMap (Aronson and Lang, 2010).
</bodyText>
<subsectionHeader confidence="0.999803">
3.1 Distantly labelled data
</subsectionHeader>
<bodyText confidence="0.9998642">
Distant supervision is carried out for a target
UMLS relation by identifying instance pairs and
using them to create a set of positive instance
pairs. Any pairs which also occur as an instance
pair of another UMLS relation are removed from
this set. A set of negative instance pairs is then
created by forming new combinations that do not
occur within the positive instance pairs. Sentences
containing a positive or negative instance pair are
then extracted to generate positive and negative
training examples for the relation. These candi-
date sentences are then stemmed (Porter, 1997)
and PoS tagged (Charniak and Johnson, 2005).
The sets of positive and negative training exam-
ples are then filtered to remove sentences that meet
any of the following criteria: contain the same
positive pair more than once; contain both a posi-
tive and negative pair; more than 5 words between
the two elements of the instance pair; contain very
common instance pairs.
</bodyText>
<subsectionHeader confidence="0.999876">
3.2 PRA-Reduction
</subsectionHeader>
<bodyText confidence="0.953055194444445">
PRA (Lao and Cohen, 2010; Lao et al., 2011)
is an algorithm that infers new relation instances
from knowledge bases. By considering a knowl-
edge base as a graph, where nodes are connected
through typed relations, it performs random walks
over it and finds bounded-length relation paths that
connect graph nodes. These paths are used as
features in a logistic regression model, which is
meant to predict new relations in the graph. Al-
though initially conceived as an algorithm to dis-
cover new links in the knowledge base, PRA can
also be used to learn relevant relation paths for
any given relation. For instance, if x and y are
related via sibling relation, the model trained by
PRA would learn that the relation path parent(x,a)
∧ parent(a,y)1 is highly relevant, as siblings share
the same parents.
Knowledge graphs were extracted from the ND-
FRT and NCI vocabularies generating approxi-
mately 200, 000 related instance pairs for ND-
FRT and 400, 000 for NCI. PRA is then run on
both graphs in order to learn paths for each tar-
get relation. Table 1 shows examples of the paths
PRA generated for the relation biological-process-
involves-gene-product together with their weights.
We only make use of relation paths with positive
weights generated by PRA.
path weight
gene-encodes-gene-product(x,a) n gene- 10.53
plays-role-in-process(a,y)
isa(x,a) n biological-process-involves-gene- 6.17
product(a,y)
isa(x,a) n biological-process-involves-gene- 2.80
product(a,y)
gene-encodes-gene-product(x,a) n gene- -0.06
plays-role-in-process(a,b) n isa(b,y)
</bodyText>
<tableCaption confidence="0.678442">
Table 1: Example PRA-induced paths and weights
</tableCaption>
<bodyText confidence="0.998080294117647">
for the NCI relation biological-process-involves-
gene-product.
The paths induced by PRA are used to iden-
tify potential false negatives in the negative train-
ing examples (Section 3.1). Each negative training
example is examined to check whether the entity
pair is related in UMLS by following any of the
relation paths extracted by PRA for the relevant
target relation. Examples containing related en-
tity pairs are assumed to be false negatives, since
the relation can be inferred from the knowledge
base, and removed from the set of negatives train-
ing examples. For instance, using the path in the
top row of Table 1, sentences containing the enti-
ties x and y would be removed if the path gene-
encodes-gene-product(x,a) ∧ gene-plays-role-in-
process(a,y) could be identified within UMLS.
</bodyText>
<footnote confidence="0.9801875">
1An underline (‘ ’) prefix represents the inverse of a rela-
tion while n represents path composition.
</footnote>
<page confidence="0.993315">
274
</page>
<subsectionHeader confidence="0.984152">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999966436619719">
Relation Extraction system: The MultiR system
(Hoffmann et al., 2010) with features described by
Surdeanu et al. (2011) was used for the experi-
ments.
Datasets: Three datasets were created to train
MultiR and evaluate performance. The first (Un-
filtered) uses the data obtained using distant su-
pervision (Section 3.1) without removing any ex-
amples identified by PRA. The overall ratio of
positive to negative sentences in this dataset was
1:5.1. However, this changes to 1:2.3 after remov-
ing examples identified by PRA. Consequently the
bias in the distantly supervised data was adjusted
to 1:2 to increase comparability across configura-
tions. Reducing bias was also found to increase re-
lation extraction performance, producing a strong
baseline. The PRA-reduced dataset is created by
applying PRA reduction (Section 3.2) to the Un-
filtered dataset to remove a portion of the nega-
tive training examples. Removing these examples
produces a dataset that is smaller than Unfiltered
and with a different bias. Changing the bias of
the training data can influence the classification re-
sults. Consequently the Random-reduced dataset
was created by removing randomly selected nega-
tive examples from Unfiltered to produce a dataset
with the same size and bias as PRA-reduced. The
Random-reduced dataset is used to show that ran-
domly removing negative instances leads to lower
results than removing those suggested by PRA.
Evaluation: Two approaches were used to eval-
uate performance.
The Held-out datasets consist of the Unfiltered,
PRA-reduced and Random-reduced data sets. The
set of entity pairs obtained from the knowledge
base is split into four parts and a process similar
to 4-fold cross validation applied. In each fold the
automatically labelled sentences obtained from the
pairs in 3 of the quarters are used as training data
and sentences obtained from the remaining quarter
used for testing.
The Manually labelled dataset contains 400
examples of the relation may-prevent and 400 of
may-treat which were manually labelled by two
annotators who were medical experts. Both rela-
tions are taken from the ND-FRT subset of UMLS.
Each annotator was asked to label every sentence
and then re-examine cases where there was dis-
agreement. This process lead to inter-annotator
agreement of 95.5% for may-treat and 97.3% for
may-prevent. The annotated data set is publicly
available2. Any sentences in the training data con-
taining an entity pair that occurs within the man-
ually labelled dataset are removed. Although this
dataset is smaller than the held-out dataset, its an-
notations are more reliable and it is therefore likely
to be a more accurate indicator of performance ac-
curacy. This dataset is more balanced than the
held-out data with a ratio of 1:1.3 for may-treat
and 1:1.8 for may-prevent.
Evaluation metric: Our experiments use en-
tity level evaluation since this is the most appropri-
ate approach to determine suitability for database
population. Precision and recall are computed
based on the proportion of entity pairs identified.
For the held-out data the set of correct entity pairs
are those which occur in sentences labeled as pos-
itive examples of the relation and which are also
listed as being related in UMLS. For the manually
labelled data it is simply the set of entity pairs that
occur in positive examples of the relation.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.997176">
4.1 Held-out data
</subsectionHeader>
<bodyText confidence="0.999947692307693">
Table 2 shows the results obtained using the held-
out data. Overall results, averaged across all re-
lations with maximum recall, are shown in the top
portion of the table and indicate that applying PRA
improves performance. Although the highest pre-
cision is obtained using the Unfiltered classifier,
the PRA-reduced classifier leads to the best recall
and F1. Performance of the Random-reduced clas-
sifier indicates that the improvement is not simply
due to a change in the bias in the data but that the
examples it contains lead to an improved model.
The lower part of Table 2 shows results for each
relation. The PRA-reduced classifier produces the
best results for the majority of relations and always
increases recall compared to Unfiltered.
It is perhaps surprising that removing false neg-
atives from the training data leads to an increase
in recall, rather than precision. False negatives
cause the classifier to generate an overly restrictive
model of the relation and to predict positive ex-
amples of a relation as negative. Removing them
leads to a less constrained model and higher recall.
There are two relations where there is also an in-
crease in precision (contraindicating-class-of and
mechanism-of-action-of) and these are also the
ones for which the fewest training examples are
</bodyText>
<footnote confidence="0.973144">
2https://sites.google.com/site/umlscorpus/home
</footnote>
<page confidence="0.99215">
275
</page>
<table confidence="0.999721428571429">
Unfiltered Random-reduced PRA-reduced
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
Overall 62.30 51.82 56.58 44.49 74.26 55.64 56.85 77.10 65.44
NCI relations
biological process involves gene product 89.61 43.18 57.86 65.67 78.79 71.38 70.63 84.85 76.97
disease has normal cell origin 60.20 83.86 69.95 43.2 95.21 58.85 42.80 91.88 57.91
gene product has associated anatomy 41.65 64.04 49.96 29.22 74.63 41.81 37.94 65.28 47.82
gene product has biochemical function 86.43 72.00 78.33 60.66 91.57 72.90 70.58 95.80 81.17
process involves gene 78.92 50.71 61.54 51.38 80.64 62.73 68.16 87.34 76.47
ND-FRT relations
contraindicating class of 40.00 20.83 26.14 28.48 72.50 39.58 41.30 82.50 54.33
may prevent 27.48 14.69 18.87 20.61 44.79 27.94 38.11 35.63 36.64
may treat 48.66 39.63 43.14 39.57 50.00 43.84 50.88 57.93 54.11
mechanism of action of 47.15 40.63 43.12 40.25 59.38 47.62 52.85 59.38 55.82
</table>
<tableCaption confidence="0.971352">
Table 2: Evaluation using held-out data
</tableCaption>
<table confidence="0.99918325">
Unfiltered Random-reduced PRA-reduced
relation Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
may prevent 54.17 21.67 30.95 53.57 25.00 34.09 39.66 38.33 38.98
may treat 40.00 47.48 43.42 43.21 50.36 46.51 41.05 67.63 51.09
</table>
<tableCaption confidence="0.999933">
Table 3: Evaluation using manually labelled data
</tableCaption>
<figure confidence="0.984647">
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.99999">
Figure 1: Precision/Recall Curve for Held-out data
</figureCaption>
<bodyText confidence="0.9997335">
available. The classifier has access to such a lim-
ited amount of data for these relations that remov-
ing the false negatives identified by PRA allows it
to learn a more accurate model.
Figure 1 presents a precision/recall curve com-
puted using MultiR’s output probabilities. Results
for the PRA-reduced and the Random-reduced
classifiers show that reducing the amount of nega-
tive training data increases recall. However, using
PRA-reduced generally leads to higher precision,
indicating that PRA is able to identify suitable in-
stances for removal from the training set. The Un-
filtered classifier produces good results but preci-
sion and recall are lower than PRA-reduced.
</bodyText>
<subsectionHeader confidence="0.986273">
4.2 Manually labelled
</subsectionHeader>
<bodyText confidence="0.999521695652174">
Table 3 shows results of evaluation on the more
reliable manually labelled data set. The best over-
all performance is once again obtained using the
PRA-reduced classifier. There is an increase in re-
call for both relations and a slight increase in pre-
cision for may treat. Performance of the Random-
reduced classifier also improves due to an increas-
ing recall but remains below PRA-reduced. Per-
formance of the Random-reduced classifier is also
better than Unfiltered, with the overall improve-
ment largely resulting from increased recall, but
below PRA-reduced. These results confirm that re-
moving examples identified by PRA improves the
quality of training data.
Further analysis indicated that the PRA-reduced
classifier produces the fewest false negatives in its
predictions on the manually annotated dataset. It
incorrectly labels 82 entity pairs (45 may-treat, 37
may-prevent) as negative while Unfiltered predicts
120 (73, 47) and Random-reduced 114 (69, 45).
This supports our initial hypothesis that remov-
ing potential false negatives from training data im-
proves classifier predictions.
</bodyText>
<sectionHeader confidence="0.98973" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999669222222222">
This paper proposes a novel approach to identify-
ing incorrectly labelled instances generated using
distant supervision. Our method applies an infer-
ence learning method to detect and discard pos-
sible false negatives from the training data. We
show that our method improves performance for
a range of relations in the biomedical domain by
making use of information from UMLS.
In future we would like to explore alternative
</bodyText>
<figure confidence="0.998648818181818">
Precision
0.9
0.8
0.7
0.6
0.5
0.4
1
unfiltered
PRA-reduced
Random-reduced
</figure>
<page confidence="0.99729">
276
</page>
<bodyText confidence="0.999871583333333">
methods for selecting PRA relation paths to iden-
tify false negatives. Furthermore we would like
to examine the PRA-reduced data in more detail.
We would like to find which kind of entity pairs
are detected by our proposed method and whether
the reduced data can also be used to extend the
positive training data. We would also like to ap-
ply the approach to other domains and alternative
knowledge bases. Finally it would be interesting
to compare our approach to other state of the art re-
lation extraction systems for distant supervision or
biased-SVM approaches such as Liu et al. (2003).
</bodyText>
<sectionHeader confidence="0.969307" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99992175">
The authors are grateful to the Engineering
and Physical Sciences Research Council for
supporting the work described in this paper
(EP/J008427/1).
</bodyText>
<sectionHeader confidence="0.991846" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998146615384615">
A. Aronson and F. Lang. 2010. An overview of
MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Associ-
ation, 17(3):229–236.
Isabelle Augenstein, Diana Maynard, and Fabio
Ciravegna. 2014. Relation extraction from the web
using distant supervision. In Proceedings of the
19th International Conference on Knowledge Engi-
neering and Knowledge Management (EKAW 2014),
Link¨oping, Sweden, November.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267–
D270.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K Bretonnel Cohen and Lawrence E Hunter. 2013.
Text mining for translational bioinformatics. PLoS
computational biology, 9(4):e1003044.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting informa-
tion from text sources. In In Proceedings of the Sev-
enth International Conference on Intelligent Systems
for Molecular Biology (ISMB), pages 77–86. AAAI
Press.
Udo Hahn, K Bretonnel Cohen, Yael Garten, and
Nigam H Shah. 2012. Mining the pharmacoge-
nomics literaturea survey of the state of the art.
Briefings in bioinformatics, 13(4):460–494.
Raphael Hoffmann, Congle Zhang, and Daniel S.
Weld. 2010. Learning 5000 relational extractors. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 286–295, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez
de Lacalle, and Eneko Agirre. 2013. Remov-
ing noisy mentions for distant supervision. Proce-
samiento del Lenguaje Natural, 51:41–48.
Lars Juhl Jensen, Jasmin Saric, and Peer Bork. 2006.
Literature mining for the biologist: from informa-
tion retrieval to biological discovery. Nature reviews
genetics, 7(2):119–129.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. In Proceedings of the 11th International
Conference on The Semantic Web - Volume Part
I, ISWC’12, pages 263–278, Berlin, Heidelberg.
Springer-Verlag.
Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Mach. Learn., 81(1):53–67, October.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 529–539, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and
Philip S. Yu. 2003. Building text classifiers using
positive and unlabeled examples. In Intl. Conf. on
Data Mining, pages 179–188.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777–782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2 - Volume 2, ACL ’09, pages 1003–1011,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant super-
vision from external semantic repositories. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
</reference>
<page confidence="0.958999">
277
</page>
<reference confidence="0.999849122448979">
’11, pages 277–282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
M. F. Porter. 1997. Readings in information retrieval.
chapter An Algorithm for Suffix Stripping, pages
313–316. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the European
Conference on Machine Learning and Knowledge
Discovery in Databases (ECML PKDD ’10).
Alan Ritter, Luke Zettlemoyer, Oren Etzioni, et al.
2013. Modeling missing data in distant supervision
for information extraction. Transactions of the As-
sociation for Computational Linguistics, 1:367–378.
Roland Roller and Mark Stevenson. 2014. Self-
supervised relation extraction using umls. In Pro-
ceedings of the Conference and Labs of the Evalua-
tion Forum 2014, Sheffield, England.
Mihai Surdeanu, David McClosky, Mason Smith, An-
drey Gusev, and Christopher Manning. 2011. Cus-
tomizing an information extraction system to a new
domain. In Proceedings of the ACL 2011 Work-
shop on Relational Models of Semantics, pages 2–
10, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 455–465, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
’12, pages 721–729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 810–815, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
</reference>
<page confidence="0.996918">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196511">
<title confidence="0.985389">Improving distant supervision using inference learning</title>
<author confidence="0.875588">Eneko Aitor</author>
<abstract confidence="0.91376645">1Department of Computer Science, University of 2IXA NLP group, University of the Basque e.agirre,a.soroa@ehu.eus Abstract Distant supervision is a widely applied approach to automatic training of relation extraction systems and has the advantage that it can generate large amounts of labelled data with minimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aronson</author>
<author>F Lang</author>
</authors>
<title>An overview of MetaMap: historical perspective and recent advances.</title>
<date>2010</date>
<journal>Journal of the American Medical Association,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="5044" citStr="Aronson and Lang, 2010" startWordPosition="776" endWordPosition="779"> and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used to create the distantly supervised training data. The corpus contains abstracts published between 1990 and 2001 annotated with UMLS concepts using MetaMap (Aronson and Lang, 2010). 3.1 Distantly labelled data Distant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed</context>
</contexts>
<marker>Aronson, Lang, 2010</marker>
<rawString>A. Aronson and F. Lang. 2010. An overview of MetaMap: historical perspective and recent advances. Journal of the American Medical Association, 17(3):229–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Augenstein</author>
<author>Diana Maynard</author>
<author>Fabio Ciravegna</author>
</authors>
<title>Relation extraction from the web using distant supervision.</title>
<date>2014</date>
<booktitle>In Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014),</booktitle>
<location>Link¨oping, Sweden,</location>
<contexts>
<context position="3351" citStr="Augenstein et al. (2014)" startWordPosition="513" endWordPosition="517">of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an inference learning method to identify potential false negatives in distantly labelled data. Our method makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. 273 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), </context>
</contexts>
<marker>Augenstein, Maynard, Ciravegna, 2014</marker>
<rawString>Isabelle Augenstein, Diana Maynard, and Fabio Ciravegna. 2014. Relation extraction from the web using distant supervision. In Proceedings of the 19th International Conference on Knowledge Engineering and Knowledge Management (EKAW 2014), Link¨oping, Sweden, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bodenreider</author>
</authors>
<title>The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl 1):D267– D270.</title>
<date>2004</date>
<contexts>
<context position="4551" citStr="Bodenreider, 2004" startWordPosition="704" endWordPosition="705">g (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used to create the distantly supervised training data. The corpus contains abstracts published between 1990 and 2001 annotated with UMLS concepts using MetaMap (Aronson and Lang, 2010). 3.1 Distantly labelled data Distant supervision is carried out for a target UMLS relation by identifying </context>
</contexts>
<marker>Bodenreider, 2004</marker>
<rawString>Olivier Bodenreider. 2004. The unified medical language system (umls): integrating biomedical terminology. Nucleic acids research, 32(suppl 1):D267– D270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5703" citStr="Charniak and Johnson, 2005" startWordPosition="882" endWordPosition="885">stant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 PRA-Reduction PRA (Lao and Cohen, 2010; Lao et al., 2011) is an algorithm that infers new relation instances from knowledge bases. By considering a knowledge base as a graph, where nodes are connected through typed relations, it performs random walks over it and finds bou</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bretonnel Cohen</author>
<author>Lawrence E Hunter</author>
</authors>
<title>Text mining for translational bioinformatics. PLoS computational biology,</title>
<date>2013</date>
<pages>9--4</pages>
<contexts>
<context position="4290" citStr="Cohen and Hunter, 2013" startWordPosition="663" endWordPosition="666">to learn relation paths from a knowledge base and uses this information to identify false negatives. 273 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used to create the distantly s</context>
</contexts>
<marker>Cohen, Hunter, 2013</marker>
<rawString>K Bretonnel Cohen and Lawrence E Hunter. 2013. Text mining for translational bioinformatics. PLoS computational biology, 9(4):e1003044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources. In</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB),</booktitle>
<pages>77--86</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4440" citStr="Craven and Kumlien, 1999" startWordPosition="686" endWordPosition="689">the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used to create the distantly supervised training data. The corpus contains abstracts published between 1990 and 2001 annotated with UMLS concepts using MetaMap (Aronson and Lang, 2</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology (ISMB), pages 77–86. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>K Bretonnel Cohen</author>
<author>Yael Garten</author>
<author>Nigam H Shah</author>
</authors>
<title>Mining the pharmacogenomics literaturea survey of the state of the art.</title>
<date>2012</date>
<booktitle>Briefings in bioinformatics,</booktitle>
<pages>13--4</pages>
<contexts>
<context position="4266" citStr="Hahn et al., 2012" startWordPosition="659" endWordPosition="662">ied version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. 273 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used t</context>
</contexts>
<marker>Hahn, Cohen, Garten, Shah, 2012</marker>
<rawString>Udo Hahn, K Bretonnel Cohen, Yael Garten, and Nigam H Shah. 2012. Mining the pharmacogenomics literaturea survey of the state of the art. Briefings in bioinformatics, 13(4):460–494.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>286--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8607" citStr="Hoffmann et al., 2010" startWordPosition="1337" endWordPosition="1340">target relation. Examples containing related entity pairs are assumed to be false negatives, since the relation can be inferred from the knowledge base, and removed from the set of negatives training examples. For instance, using the path in the top row of Table 1, sentences containing the entities x and y would be removed if the path geneencodes-gene-product(x,a) ∧ gene-plays-role-inprocess(a,y) could be identified within UMLS. 1An underline (‘ ’) prefix represents the inverse of a relation while n represents path composition. 274 3.3 Evaluation Relation Extraction system: The MultiR system (Hoffmann et al., 2010) with features described by Surdeanu et al. (2011) was used for the experiments. Datasets: Three datasets were created to train MultiR and evaluate performance. The first (Unfiltered) uses the data obtained using distant supervision (Section 3.1) without removing any examples identified by PRA. The overall ratio of positive to negative sentences in this dataset was 1:5.1. However, this changes to 1:2.3 after removing examples identified by PRA. Consequently the bias in the distantly supervised data was adjusted to 1:2 to increase comparability across configurations. Reducing bias was also foun</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 286–295, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ander Intxaurrondo</author>
</authors>
<title>Mihai Surdeanu, Oier Lopez de Lacalle, and Eneko Agirre.</title>
<date>2013</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>51--41</pages>
<marker>Intxaurrondo, 2013</marker>
<rawString>Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de Lacalle, and Eneko Agirre. 2013. Removing noisy mentions for distant supervision. Procesamiento del Lenguaje Natural, 51:41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Juhl Jensen</author>
<author>Jasmin Saric</author>
<author>Peer Bork</author>
</authors>
<title>Literature mining for the biologist: from information retrieval to biological discovery. Nature reviews genetics,</title>
<date>2006</date>
<pages>7--2</pages>
<contexts>
<context position="4247" citStr="Jensen et al., 2006" startWordPosition="655" endWordPosition="658"> makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. 273 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 pub</context>
</contexts>
<marker>Jensen, Saric, Bork, 2006</marker>
<rawString>Lars Juhl Jensen, Jasmin Saric, and Peer Bork. 2006. Literature mining for the biologist: from information retrieval to biological discovery. Nature reviews genetics, 7(2):119–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Krause</author>
<author>Hong Li</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
</authors>
<title>Large-scale learning of relationextraction rules with distant supervision from the web.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Conference on The Semantic Web - Volume Part I, ISWC’12,</booktitle>
<pages>263--278</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2396" citStr="Krause et al., 2012" startWordPosition="362" endWordPosition="365">the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might oc</context>
</contexts>
<marker>Krause, Li, Uszkoreit, Xu, 2012</marker>
<rawString>Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-scale learning of relationextraction rules with distant supervision from the web. In Proceedings of the 11th International Conference on The Semantic Web - Volume Part I, ISWC’12, pages 263–278, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<journal>Mach. Learn.,</journal>
<volume>81</volume>
<issue>1</issue>
<contexts>
<context position="1830" citStr="Lao and Cohen, 2010" startWordPosition="271" endWordPosition="274">a is often generated using a closed world assumption: pairs of entities not listed in the knowledge base are assumed to be unrelated and sentences containing them considered to be negative training examples. However this assumption is violated when the knowledge base is incomplete which can lead to sentences containing instances of relations being wrongly annotated as negative examples. We propose a method to improve the quality of distantly supervised data by identifying possible wrongly annotated negative instances. Our proposed method includes a version of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., </context>
<context position="6069" citStr="Lao and Cohen, 2010" startWordPosition="944" endWordPosition="947">irs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 PRA-Reduction PRA (Lao and Cohen, 2010; Lao et al., 2011) is an algorithm that infers new relation instances from knowledge bases. By considering a knowledge base as a graph, where nodes are connected through typed relations, it performs random walks over it and finds bounded-length relation paths that connect graph nodes. These paths are used as features in a logistic regression model, which is meant to predict new relations in the graph. Although initially conceived as an algorithm to discover new links in the knowledge base, PRA can also be used to learn relevant relation paths for any given relation. For instance, if x and y a</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Mach. Learn., 81(1):53–67, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>529--539</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1849" citStr="Lao et al., 2011" startWordPosition="275" endWordPosition="278">using a closed world assumption: pairs of entities not listed in the knowledge base are assumed to be unrelated and sentences containing them considered to be negative training examples. However this assumption is violated when the knowledge base is incomplete which can lead to sentences containing instances of relations being wrongly annotated as negative examples. We propose a method to improve the quality of distantly supervised data by identifying possible wrongly annotated negative instances. Our proposed method includes a version of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al.</context>
<context position="6088" citStr="Lao et al., 2011" startWordPosition="948" endWordPosition="951">ning a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 PRA-Reduction PRA (Lao and Cohen, 2010; Lao et al., 2011) is an algorithm that infers new relation instances from knowledge bases. By considering a knowledge base as a graph, where nodes are connected through typed relations, it performs random walks over it and finds bounded-length relation paths that connect graph nodes. These paths are used as features in a logistic regression model, which is meant to predict new relations in the graph. Although initially conceived as an algorithm to discover new links in the knowledge base, PRA can also be used to learn relevant relation paths for any given relation. For instance, if x and y are related via sibl</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 529–539, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Yang Dai</author>
<author>Xiaoli Li</author>
<author>Wee Sun Lee</author>
<author>Philip S Yu</author>
</authors>
<title>Building text classifiers using positive and unlabeled examples.</title>
<date>2003</date>
<booktitle>In Intl. Conf. on Data Mining,</booktitle>
<pages>179--188</pages>
<marker>Liu, Dai, Li, Lee, Yu, 2003</marker>
<rawString>Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and Philip S. Yu. 2003. Building text classifiers using positive and unlabeled examples. In Intl. Conf. on Data Mining, pages 179–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>777--782</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2434" citStr="Min et al., 2013" startWordPosition="370" endWordPosition="373"> Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 777–782, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09,</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2354" citStr="Mintz et al., 2009" startWordPosition="354" endWordPosition="357">ur proposed method includes a version of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take in</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 1003–1011, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>277--282</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2847" citStr="Nguyen and Moschitti, 2011" startWordPosition="431" endWordPosition="434">used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled t</context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 277–282, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>Readings in information retrieval. chapter An Algorithm for Suffix Stripping,</title>
<date>1997</date>
<pages>313--316</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5659" citStr="Porter, 1997" startWordPosition="877" endWordPosition="878">3.1 Distantly labelled data Distant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 PRA-Reduction PRA (Lao and Cohen, 2010; Lao et al., 2011) is an algorithm that infers new relation instances from knowledge bases. By considering a knowledge base as a graph, where nodes are connected through typed relations, it</context>
</contexts>
<marker>Porter, 1997</marker>
<rawString>M. F. Porter. 1997. Readings in information retrieval. chapter An Algorithm for Suffix Stripping, pages 313–316. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</booktitle>
<contexts>
<context position="2375" citStr="Riedel et al., 2010" startWordPosition="358" endWordPosition="361">ncludes a version of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact t</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Modeling missing data in distant supervision for information extraction.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--367</pages>
<contexts>
<context position="2456" citStr="Ritter et al., 2013" startWordPosition="374" endWordPosition="377">et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not nece</context>
</contexts>
<marker>Ritter, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Alan Ritter, Luke Zettlemoyer, Oren Etzioni, et al. 2013. Modeling missing data in distant supervision for information extraction. Transactions of the Association for Computational Linguistics, 1:367–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Roller</author>
<author>Mark Stevenson</author>
</authors>
<title>Selfsupervised relation extraction using umls.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference and Labs of the Evaluation Forum 2014,</booktitle>
<location>Sheffield, England.</location>
<contexts>
<context position="4319" citStr="Roller and Stevenson, 2014" startWordPosition="667" endWordPosition="671">from a knowledge base and uses this information to identify false negatives. 273 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 273–278, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 3 Data and Methods We chose to apply our approach to relation extraction tasks from the biomedical domain since this has proved to be an important problem within these documents (Jensen et al., 2006; Hahn et al., 2012; Cohen and Hunter, 2013; Roller and Stevenson, 2014). In addition, the first application of distant supervision was to biomedical journal articles (Craven and Kumlien, 1999). In addition, the most widely used knowledge source in this domain, the UMLS Metathesaurus (Bodenreider, 2004), is an ideal resource to apply inference learning given its rich structure. We develop classifiers to identify relations found in two subsets of UMLS: the National Drug File-Reference Terminology (ND-FRT) and the National Cancer Institute Thesaurus (NCI). A corpus of approximately 1,000,000 publications is used to create the distantly supervised training data. The </context>
</contexts>
<marker>Roller, Stevenson, 2014</marker>
<rawString>Roland Roller and Mark Stevenson. 2014. Selfsupervised relation extraction using umls. In Proceedings of the Conference and Labs of the Evaluation Forum 2014, Sheffield, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>David McClosky</author>
<author>Mason Smith</author>
<author>Andrey Gusev</author>
<author>Christopher Manning</author>
</authors>
<title>Customizing an information extraction system to a new domain.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics,</booktitle>
<pages>2--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8657" citStr="Surdeanu et al. (2011)" startWordPosition="1345" endWordPosition="1348">y pairs are assumed to be false negatives, since the relation can be inferred from the knowledge base, and removed from the set of negatives training examples. For instance, using the path in the top row of Table 1, sentences containing the entities x and y would be removed if the path geneencodes-gene-product(x,a) ∧ gene-plays-role-inprocess(a,y) could be identified within UMLS. 1An underline (‘ ’) prefix represents the inverse of a relation while n represents path composition. 274 3.3 Evaluation Relation Extraction system: The MultiR system (Hoffmann et al., 2010) with features described by Surdeanu et al. (2011) was used for the experiments. Datasets: Three datasets were created to train MultiR and evaluate performance. The first (Unfiltered) uses the data obtained using distant supervision (Section 3.1) without removing any examples identified by PRA. The overall ratio of positive to negative sentences in this dataset was 1:5.1. However, this changes to 1:2.3 after removing examples identified by PRA. Consequently the bias in the distantly supervised data was adjusted to 1:2 to increase comparability across configurations. Reducing bias was also found to increase relation extraction performance, pro</context>
</contexts>
<marker>Surdeanu, McClosky, Smith, Gusev, Manning, 2011</marker>
<rawString>Mihai Surdeanu, David McClosky, Mason Smith, Andrey Gusev, and Christopher Manning. 2011. Customizing an information extraction system to a new domain. In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, pages 2– 10, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2928" citStr="Surdeanu et al., 2012" startWordPosition="444" endWordPosition="447">sed as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an in</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 455–465, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>721--729</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3187" citStr="Takamatsu et al. (2012)" startWordPosition="486" endWordPosition="489">On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an inference learning method to identify potential false negatives in distantly labelled data. Our method makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. 273 Proceedings of</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 721–729, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingxing Zhang</author>
<author>Jianwen Zhang</author>
<author>Junyu Zeng</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Zhifang Sui</author>
</authors>
<title>Towards accurate distant supervision for relational facts extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>810--815</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2416" citStr="Zhang et al., 2013" startWordPosition="366" endWordPosition="369">rithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011) which infers relation paths by combining random walks though a knowledge base. We use this knowledge inference to detect possible false negatives (or at least entity pairs closely connected to a target relation) in automatically labelled training data and show that their removal can improve relation extraction performance. 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different rel</context>
</contexts>
<marker>Zhang, Zhang, Zeng, Yan, Chen, Sui, 2013</marker>
<rawString>Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui. 2013. Towards accurate distant supervision for relational facts extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 810–815, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>