<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001813">
<title confidence="0.9979655">
Re-examining Automatic Keyphrase Extraction Approaches
in Scientific Articles
</title>
<author confidence="0.980517">
Su Nam Kim
</author>
<affiliation confidence="0.94613">
CSSE dept.
University of Melbourne
</affiliation>
<email confidence="0.996461">
snkim@csse.unimelb.edu.au
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976355">
We tackle two major issues in automatic
keyphrase extraction using scientific arti-
cles: candidate selection and feature engi-
neering. To develop an efficient candidate
selection method, we analyze the nature
and variation of keyphrases and then se-
lect candidates using regular expressions.
Secondly, we re-examine the existing fea-
tures broadly used for the supervised ap-
proach, exploring different ways to en-
hance their performance. While most
other approaches are supervised, we also
study the optimal features for unsuper-
vised keyphrase extraction. Our research
has shown that effective candidate selec-
tion leads to better performance as evalua-
tion accounts for candidate coverage. Our
work also attests that many of existing fea-
tures are also usable in unsupervised ex-
traction.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998619823529412">
Keyphrases are simplex nouns or noun phrases
(NPs) that represent the key ideas of the document.
Keyphrases can serve as a representative summary
of the document and also serve as high quality in-
dex terms. It is thus no surprise that keyphrases
have been utilized to acquire critical information
as well as to improve the quality of natural lan-
guage processing (NLP) applications such as doc-
ument summarizer(D´avanzo and Magnini, 2005),
information retrieval (IR)(Gutwin et al., 1999) and
document clustering(Hammouda et al., 2005).
In the past, various attempts have been made to
boost automatic keyphrase extraction performance
based primarily on statistics(Frank et al., 1999;
Turney, 2003; Park et al., 2004; Wan and Xiao,
2008) and a rich set of heuristic features(Barker
and Corrnacchia, 2000; Medelyan and Witten,
</bodyText>
<author confidence="0.816184">
Min-Yen Kan
</author>
<affiliation confidence="0.9985505">
Department of Computer Science
National University of Singapore
</affiliation>
<email confidence="0.975601">
kanmy@comp.nus.edu.sg
</email>
<bodyText confidence="0.99962212195122">
2006; Nguyen and Kan, 2007). In Section 2, we
give a more comprehensive overview of previous
attempts.
Current keyphrase technology still has much
room for improvement. First of all, although sev-
eral candidate selection methods have been pro-
posed for automatic keyphrase extraction in the
past (e.g. (Frank et al., 1999; Park et al., 2004;
Nguyen and Kan, 2007)), most of them do not ef-
fectively deal with various keyphrase forms which
results in the ignorance of some keyphrases as can-
didates. Moreover, no studies thus far have done
a detailed investigation of the nature and varia-
tion of manually-provided keyphrases. As a con-
sequence, the community lacks a standardized list
of candidate forms, which leads to difficulties in
direct comparison across techniques during evalu-
ation and hinders re-usability.
Secondly, previous studies have shown the ef-
fectiveness of their own features but not many
compared their features with other existing fea-
tures. That leads to a redundancy in studies and
hinders direct comparison. In addition, existing
features are specifically designed for supervised
approaches with few exceptions. However, this
approach involves a large amount of manual labor,
thus reducing its utility for real-world application.
Hence, unsupervised approach is inevitable in or-
der to minimize manual tasks and to encourage
utilization. It is a worthy study to attest the re-
liability and re-usability for the unsupervised ap-
proach in order to set up the tentative guideline for
applications.
This paper targets to resolve these issues of
candidate selection and feature engineering. In
our work on candidate selection, we analyze the
nature and variation of keyphrases with the pur-
pose of proposing a candidate selection method
which improves the coverage of candidates that
occur in various forms. Our second contribution
re-examines existing keyphrase extraction features
</bodyText>
<page confidence="0.971004">
9
</page>
<note confidence="0.9989785">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9–16,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99840525">
reported in the literature, in terms of their effec-
tiveness and re-usability. We test and compare
the usefulness of each feature for further improve-
ment. In addition, we assess how well these fea-
tures can be applied in an unsupervised approach.
In the remaining sections, we describe an
overview of related work in Section 2, our propos-
als on candidate selection and feature engineering
in Section 4 and 5, our system architecture and
data in Section 6. Then, we evaluate our propos-
als, discuss outcomes and conclude our work in
Section 7, 8 and 9, respectively.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969923076923">
The majority of related work has been carried
out using statistical approaches, a rich set of
symbolic resources and linguistically-motivated
heuristics(Frank et al., 1999; Turney, 1999; Barker
and Corrnacchia, 2000; Matsuo and Ishizuka,
2004; Nguyen and Kan, 2007). Features used can
be categorized into three broad groups: (1) docu-
ment cohesion features (i.e. relationship between
document and keyphrases)(Frank et al., 1999;
Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007), and to lesser,
(2) keyphrase cohesion features (i.e. relationship
among keyphrases)(Turney, 2003) and (3) term
cohesion features (i.e. relationship among compo-
nents in a keyphrase)(Park et al., 2004).
The simplest system is KEA (Frank et al.,
1999; Witten et al., 1999) that uses TF*IDF (i.e.
term frequency * inverse document frequency) and
first occurrence in the document. TF*IDF mea-
sures the document cohesion and the first occur-
rence implies the importance of the abstract or
introduction which indicates the keyphrases have
a locality. Turney (2003) added the notion of
keyphrase cohesion to KEA features and Nguyen
and Kan (2007) added linguistic features such
as section information and suffix sequence. The
GenEx system(Turney, 1999) employed an inven-
tory of nine syntactic features, such as length in
words and frequency of stemming phrase as a
set of parametrized heuristic rules. Barker and
Corrnacchia (2000) introduced a method based
on head noun heuristics that took three features:
length of candidate, frequency and head noun fre-
quency. To take advantage of domain knowledge,
Hulth et al. (2001) used a hierarchically-organized
domain-specific thesaurus from Swedish Parlia-
ment as a secondary knowledge source. The
Textract (Park et al., 2004) also ranks the can-
didate keyphrases by its judgment of keyphrases’
degree of domain specificity based on subject-
specific collocations(Damerau, 1993), in addi-
tion to term cohesion using Dice coefficient(Dice,
1945). Recently, Wan and Xiao (2008) extracts
automatic keyphrases from single documents, uti-
lizing document clustering information. The as-
sumption behind this work is that the documents
with the same or similar topics interact with each
other in terms of salience of words. The authors
first clustered the documents then used the graph-
based ranking algorithm to rank the candidates in
a document by making use of mutual influences of
other documents in the same cluster.
</bodyText>
<sectionHeader confidence="0.989233" genericHeader="method">
3 Keyphrase Analysis
</sectionHeader>
<bodyText confidence="0.999757441176471">
In previous study, KEA employed the index-
ing words as candidates whereas others such as
(Park et al., 2004; Nguyen and Kan, 2007) gen-
erated handcrafted regular expression rules. How-
ever, none carefully undertook the analysis of
keyphrases. We believe there is more to be learned
from the reference keyphrases themselves by do-
ing a fine-grained, careful analysis of their form
and composition. Note that we used the articles
collected from ACM digital library for both ana-
lyzing keyphrases as well as evaluating methods.
See Section 6 for data in detail.
Syntactically, keyphrases can be formed by ei-
ther simplex nouns (e.g. algorithm, keyphrase,
multi-agent) or noun phrases (NPs) which can be a
sequence of nouns and their auxiliary words such
as adjectives and adverbs (e.g. mobile network,
fast computing, partially observable Markov de-
cision process) despite few incidences. They can
also incorporate a prepositional phrase (PP) (e.g.
quality of service, policy of distributed caching).
When keyphrases take the form of an NP with an
attached PP (i.e. NPs in of-PP form), the preposi-
tion of is most common, but others such as for, in,
via also occur (e.g. incentive for cooperation, in-
equality in welfare, agent security via approximate
policy, trade in financial instrument based on log-
ical formula). The patterns above correlate well
to part-of-speech (POS) patterns used in modern
keyphrase extraction systems.
However, our analysis uncovered additional lin-
guistic patterns and alternations which other stud-
ies may have overlooked. In our study we also
found that keyphrases also occur as a simple con-
</bodyText>
<page confidence="0.996406">
10
</page>
<table confidence="0.990769454545455">
Criteria Rules
Frequency (Rule1) Frequency heuristic i.e. frequency &gt; 2 for simplex words vs. frequency &gt; 1 for NPs
Length (Rule2) Length heuristic i.e. up to length 3 for NPs in non-of-PP form vs. up to length 4 for NPs in of-PP form
(e.g. synchronous concurrent program vs. model of multiagent interaction)
Alternation (Rule3) of-PP form alternation
(e.g. number of sensor = sensor number, history ofpast encounter = past encounter history)
(Rule4) Possessive alternation
(e.g. agent’s goal = goal of agent, security’s value = value of security)
Extraction (Rules) Noun Phrase = (NN|NNS|NNP|NNPS|JJ|JJR|JJS)*(NN|NNS|NNP|NNPS)
(e.g. complexity, effective algorithm, grid computing, distributed web-service discovery architecture)
(Rule6) Simplex Word/NP IN Simplex Word/NP
</table>
<tableCaption confidence="0.738936333333333">
(e.g. quality of service, sensitivity of VOIP traffic (VOIP traffic extracted),
simplified instantiation of zebroid (simplified instantiation extracted))
Table 1: Candidate Selection Rules
</tableCaption>
<bodyText confidence="0.998044666666666">
junctions (e.g. search and rescue, propagation and
delivery), and much more rarely, as conjunctions
of more complex NPs (e.g. history of past en-
counter and transitivity). Some keyphrases appear
to be more complex (e.g. pervasive document edit
and management system, task and resource allo-
cation in agent system). Similarly, abbreviations
and possessive forms figure as common patterns
(e.g. belief desire intention = BDI, inverse docu-
ment frequency = (IDF); Bayes’ theorem, agent’s
dominant strategy).
A critical insight of our work is that keyphrases
can be morphologically and semantically altered.
Keyphrases that incorporate a PP or have an un-
derlying genitive composition are often easily var-
ied by word order alternation. Previous studies
have used the altered keyphrases when forming in
of-PP form. For example, quality of service can
be altered to service quality, sometimes with lit-
tle semantic difference. Also, as most morpho-
logical variation in English relates to noun num-
ber and verb inflection, keyphrases are subject to
these rules as well (e.g. distributed system =� dis-
tributing system, dynamical caching =� dynamical
cache). In addition, possessives tend to alternate
with of-PP form (e.g. agent’s goal = goal of agent,
security’s value = value of security).
</bodyText>
<sectionHeader confidence="0.981721" genericHeader="method">
4 Candidate Selection
</sectionHeader>
<bodyText confidence="0.999985142857143">
We now describe our proposed candidate selection
process. Candidate selection is a crucial step for
automatic keyphrase extraction. This step is corre-
lated to term extraction study since top Nth ranked
terms become keyphrases in documents. In pre-
vious study, KEA employed the indexing words
as candidates whereas others such as (Park et al.,
2004; Nguyen and Kan, 2007) generated hand-
crafted regular expression rules. However, none
carefully undertook the analysis of keyphrases. In
this section, before we present our method, we first
describe the detail of keyphrase analysis.
In our keyphrase analysis, we observed that
most of author assigned keyphrase and/or reader
assigned keyphrase are syntactically more of-
ten simplex words and less often NPs. When
keyphrases take an NP form, they tend to be a sim-
ple form of NPs. i.e. either without a PP or with
only a PP or with a conjunction, but few appear as
a mixture of such forms. We also noticed that the
components of NPs are normally nouns and adjec-
tives but rarely, are adverbs and verbs. As a re-
sult, we decided to ignore NPs containing adverbs
and verbs in this study as our candidates since they
tend to produce more errors and to require more
complexity.
Another observation is that keyphrases contain-
ing more than three words are rare (i.e. 6% in our
data set), validating what Paukkeri et al. (2008)
observed. Hence, we apply a length heuristic. Our
candidate selection rule collects candidates up to
length 3, but also of length 4 for NPs in of-PP
form, since they may have a non-genetive alter-
nation that reduces its length to 3 (e.g. perfor-
mance of distributed system = distributed system
performance). In previous studies, words occur-
ring at least twice are selected as candidates. How-
ever, during our acquisition of reader assigned
keyphrase, we observed that readers tend to collect
NPs as keyphrases, regardless of their frequency.
Due to this, we apply different frequency thresh-
olds for simplex words (&gt;= 2) and NPs (&gt;= 1).
Note that 30% of NPs occurred only once in our
data.
Finally, we generated regular expression rules
to extract candidates, as presented in Table 1. Our
candidate extraction rules are based on those in
Nguyen and Kan (2007). However, our Rule6
for NPs in of-PP form broadens the coverage of
</bodyText>
<page confidence="0.995377">
11
</page>
<bodyText confidence="0.998776125">
possible candidates. i.e. with a given NPs in of-
PP form, not only we collect simplex word(s),
but we also extract non-of-PP form of NPs from
noun phrases governing the PP and the PP. For
example, our rule extracts effective algorithm of
grid computing as well as effective algorithm and
grid computing as candidates while the previous
works’ rules do not.
</bodyText>
<sectionHeader confidence="0.978305" genericHeader="method">
5 Feature Engineering
</sectionHeader>
<bodyText confidence="0.99979925">
With a wider candidate selection criteria, the onus
of filtering out irrelevant candidates becomes the
responsibility of careful feature engineering. We
list 25 features that we have found useful in ex-
tracting keyphrases, comprising of 9 existing and
16 novel and/or modified features that we intro-
duce in our work (marked with *). As one of
our goals in feature engineering is to assess the
suitability of features in the unsupervised setting,
we have also indicated which features are suitable
only for the supervised setting (S) or applicable to
both (S, U).
</bodyText>
<subsectionHeader confidence="0.969307">
5.1 Document Cohesion
</subsectionHeader>
<bodyText confidence="0.999494368421053">
Document cohesion indicates how important the
candidates are for the given document. The most
popular feature for this cohesion is TF*IDF but
some works have also used context words to check
the correlation between candidates and the given
document. Other features for document cohesion
are distance, section information and so on. We
note that listed features other than TF*IDF are re-
lated to locality. That is, the intuition behind these
features is that keyphrases tend to appear in spe-
cific area such as the beginning and the end of doc-
uments.
F1 : TF*IDF (S,U) TF*IDF indicates doc-
ument cohesion by looking at the frequency of
terms in the documents and is broadly used in pre-
vious work(Frank et al., 1999; Witten et al., 1999;
Nguyen and Kan, 2007). However, a disadvan-
tage of the feature is in requiring a large corpus
to compute useful IDF. As an alternative, con-
text words(Matsuo and Ishizuka, 2004) can also
be used to measure document cohesion. From our
study of keyphrases, we saw that substrings within
longer candidates need to be properly counted, and
as such our method measures TF in substrings as
well as in exact matches. For example, grid com-
puting is often a substring of other phrases such as
grid computing algorithm and efficient grid com-
puting algorithm. We also normalize TF with re-
spect to candidate types: i.e. we separately treat
simplex words and NPs to compute TF. To make
our IDFs broadly representative, we employed the
Google n-gram counts, that were computed
over terabytes of data. Given this large, generic
source of word count, IDF can be incorporated
without corpus-dependent processing, hence such
features are useful in unsupervised approaches
as well. The following list shows variations of
TF*IDF, employed as features in our system.
</bodyText>
<listItem confidence="0.9580295">
• (F1a) TF*IDF
• (F1b*) TF including counts of substrings
• (F1c*) TF of substring as a separate feature
• (F1d*) normalized TF by candidate types
(i.e. simplex words vs. NPs)
• (F1e*) normalized TF by candidate types as
a separate feature
• (F1f*) IDF using Google n-gram
F2 : First Occurrence (S,U) KEA used the first
appearance of the word in the document(Frank et
</listItem>
<bodyText confidence="0.978740653846154">
al., 1999; Witten et al., 1999). The main idea
behind this feature is that keyphrases tend to oc-
cur in the beginning of documents, especially in
structured reports (e.g., in abstract and introduc-
tion sections) and newswire.
F3 : Section Information (S,U) Nguyen and
Kan (2007) used the identity of which specific
document section a candidate occurs in. This lo-
cality feature attempts to identify key sections. For
example, in their study of scientific papers, the
authors weighted candidates differently depending
on whether they occurred in the abstract, introduc-
tion, conclusion, section head, title and/or refer-
ences.
F4* : Additional Section Information (S,U)
We first added the related work or previous work
as one of section information not included in
Nguyen and Kan (2007). We also propose and test
a number of variations. We used the substrings
that occur in section headers and reference titles
as keyphrases. We counted the co-occurrence of
candidates (i.e. the section TF) across all key sec-
tions that indicates the correlation among key sec-
tions. We assign section-specific weights as in-
dividual sections exhibit different propensities for
generating keyphrases. For example, introduction
</bodyText>
<page confidence="0.993988">
12
</page>
<bodyText confidence="0.995640666666667">
contains the majority of keyphrases while the ti-
tle or section head contains many fewer due to the
variation in size.
</bodyText>
<listItem confidence="0.996092333333333">
• (F4a*) section, ’related/previous work’
• (F4b*) counting substring occurring in key
sections
• (F4c*) section TF across all key sections
• (F4d*) weighting key sections according to
the portion of keyphrases found
</listItem>
<bodyText confidence="0.900480833333333">
F5* : Last Occurrence (S,U) Similar to dis-
tance in KEA , the position of the last occurrence
of a candidate may also imply the importance of
keyphrases, as keyphrases tend to appear in the
last part of document such as the conclusion and
discussion.
</bodyText>
<subsectionHeader confidence="0.989976">
5.2 Keyphrase Cohesion
</subsectionHeader>
<bodyText confidence="0.957459619047619">
The intuition behind using keyphrase cohesion is
that actual keyphrases are often associated with
each other, since they are semantically related to
topic of the document. Note that this assumption
holds only when the document describes a single,
coherent topic – a document that represents a col-
lection may be first need to be segmented into its
constituent topics.
F6* : Co-occurrence of Another Candidate
in Section (S,U) When candidates co-occur in
several key sections together, then they are more
likely keyphrases. Hence, we used the number of
sections that candidates co-occur.
F7* : Title overlap (S) In a way, titles also rep-
resent the topics of their documents. A large col-
lection of titles in the domain can act as a prob-
abilistic prior of what words could stand as con-
stituent words in keyphrases. In our work, as we
examined scientific papers from computer science,
we used a collection of titles obtained from the
large CiteSeer1 collection to create this feature.
</bodyText>
<listItem confidence="0.90810075">
• (F7a*) co-occurrence (Boolean) in title col-
location
• (F7b*) co-occurrence (TF) in title collection
F8 : Keyphrase Cohesion (S,U) Turney (2003)
</listItem>
<bodyText confidence="0.484206666666667">
integrated keyphrase cohesion into his system by
checking the semantic similarity between top N
ranked candidates against the remainder. In the
</bodyText>
<footnote confidence="0.73503">
1It contains 1.3M titles from articles, papers and reports.
</footnote>
<bodyText confidence="0.999828916666667">
original work, a large, external web corpus was
used to obtain the similarity judgments. As we
did not have access to the same web corpus and
all candidates/keyphrases were not found in the
Google n-gram corpus, we approximated this fea-
ture using a similar notion of contextual similarity.
We simulated a latent 2-dimensional matrix (simi-
lar to latent semantic analysis) by listing all candi-
date words in rows and their neighboring words
(nouns, verbs, and adjectives only) in columns.
The cosine measure is then used to compute the
similarity among keyphrases.
</bodyText>
<subsectionHeader confidence="0.984201">
5.3 Term Cohesion
</subsectionHeader>
<bodyText confidence="0.999508571428572">
Term cohesion further refines the candidacy judg-
ment, by incorporating an internal analysis of the
candidate’s constituent words. Term cohesion
posits that high values for internal word associa-
tion measures correlates indicates that the candi-
date is a keyphrase (Church and Hanks, 1989).
F9 : Term Cohesion (S,U) Park et al. (2004)
used in the Dice coefficient (Dice, 1945)
to measure term cohesion particularly for multi-
word terms. In their work, as NPs are longer than
simplex words, they simply discounted simplex
word cohesion by 10%. In our work, we vary the
measure of TF used in Dice coefficient,
similar to our discussion earlier.
</bodyText>
<listItem confidence="0.998996857142857">
• (F9a) term cohesion by (Park et al., 2004),
• (F9b*) normalized TF by candidate types
(i.e. simplex words vs. NPs),
• (F9c*) applying different weight by candi-
date types,
• (F9d*) normalized TF and different weight-
ing by candidate types
</listItem>
<subsectionHeader confidence="0.978685">
5.4 Other Features
</subsectionHeader>
<bodyText confidence="0.931272454545454">
F10 : Acronym (S) Nguyen and Kan (2007) ac-
counted for the importance of acronym as a fea-
ture. We found that this feature is heavily depen-
dent on the data set. Hence, we used it only for
N&amp;K to attest our candidate selection method.
F11 : POS sequence (S) Hulth and Megyesi
(2006) pointed out that POS sequences of
keyphrases are similar. It showed the distinctive
distribution of POS sequences of keyphrases and
use them as a feature. Like acronym, this is also
subject to the data set.
</bodyText>
<page confidence="0.997788">
13
</page>
<bodyText confidence="0.9906745">
F12 : Suffix sequence (S) Similar to acronym,
Nguyen and Kan (2007) also used a candidate’s
suffix sequence as a feature, to capture the propen-
sity of English to use certain Latin derivational
morphology for technical keyphrases. This fea-
ture is also a data dependent features, thus used in
supervised approach only.
F13 : Length of Keyphrases (S,U) Barker and
Corrnacchia (2000) showed that candidate length
is also a useful feature in extraction as well as in
candidate selection, as the majority of keyphrases
are one or two terms in length.
</bodyText>
<sectionHeader confidence="0.976422" genericHeader="method">
6 System and Data
</sectionHeader>
<bodyText confidence="0.999962379310345">
To assess the performance of the proposed candi-
date selection rules and features, we implemented
a keyphrase extraction pipe line. We start with
raw text of computer science articles converted
from PDF by pdftotext. Then, we parti-
tioned the into section such as title and sections
via heuristic rules and applied sentence segmenter
2, ParsCit3(Councill et al., 2008) for refer-
ence collection, part-of-speech tagger4 and lem-
matizer5(Minnen et al., 2001) of the input. Af-
ter preprocessing, we built both supervised and
unsupervised classifiers using Naive Bayes from
the WEKA machine learning toolkit(Witten and
Frank, 2005), Maximum Entropy6, and simple
weighting.
In evaluation, we collected 250 papers from
four different categories7 of the ACM digital li-
brary. Each paper was 6 to 8 pages on average.
In author assigned keyphrase, we found many
were missing or found as substrings. To rem-
edy this, we collected reader assigned keyphrase
by hiring senior year undergraduates in computer
science, each whom annotated five of the papers
with an annotation guideline and on average, took
about 15 minutes to annotate each paper. The fi-
nal statistics of keyphrases is presented in Table
2 where Combined represents the total number of
keyphrases. The numbers in O denotes the num-
ber of keyphrases in of-PP form. Found means the
</bodyText>
<footnote confidence="0.9880905">
2http://www.eng.ritsumei.ac.jp/asao/resources/sentseg/
3http://wing.comp.nus.edu.sg/parsCit/
4http://search.cpan.org/dist/Lingua-EN-
Tagger/Tagger.pm
5http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html
6http://maxent.sourceforge.net/index.html
7C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial Intelligence-
Multiagent Systems) and J4 (Social and Behavioral Sciences-
Economics)
</footnote>
<bodyText confidence="0.8218505">
number of author assigned keyphrase and reader
assigned keyphrase found in the documents.
</bodyText>
<table confidence="0.9740952">
Author Reader Combined
Total 1252 (53) 3110 (111) 3816 (146)
NPs 904 2537 3027
Average 3.85 (4.01) 12.44 (12.88) 15.26 (15.85)
Found 769 2509 2864
</table>
<tableCaption confidence="0.998953">
Table 2: Statistics in Keyphrases
</tableCaption>
<sectionHeader confidence="0.984043" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999922823529412">
The baseline system for both the supervised and
unsupervised approaches is modified N&amp;K which
uses TF*IDF, distance, section information and
additional section information (i.e. F1-4). Apart
from baseline , we also implemented basic
KEA and N&amp;K to compare. Note that N&amp;K is con-
sidered a supervised approach, as it utilizes fea-
tures like acronym, POS sequence, and suffix se-
quence.
Table 3 and 4 shows the performance of our can-
didate selection method and features with respect
to supervised and unsupervised approaches using
the current standard evaluation method (i.e. exact
matching scheme) over top 5th,10th,15th candi-
dates.
BestFeatures includes F1c:TF of substring as
a separate feature, F2:first occurrence, F3:section
information, F4d:weighting key sections, F5:last
occurrence, F6:co-occurrence of another candi-
date in section, F7b:title overlap, F9a:term co-
hesion by (Park et al., 2004), F13:length of
keyphrases. Best-TF*IDF means using all best
features but TF*IDF.
In Tables 3 and 4, C denotes the classifier tech-
nique: unsupervised (U) or supervised using Max-
imum Entropy (S)8.
In Table 5, the performance of each feature is
measured using N&amp;K system and the target fea-
ture. + indicates an improvement, - indicates a
performance decline, and ? indicates no effect
or unconfirmed due to small changes of perfor-
mances. Again, supervised denotes Maximum
Entropy training and Unsupervised is our unsu-
pervised approach.
</bodyText>
<sectionHeader confidence="0.997197" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.9969255">
We compared the performances over our candi-
date selection and feature engineering with sim-
ple KEA , N&amp;K and our baseline system. In eval-
uating candidate selection, we found that longer
</bodyText>
<footnote confidence="0.86939">
8Due to the page limits, we present the best performance.
</footnote>
<page confidence="0.991968">
14
</page>
<table confidence="0.999959">
Method Features C Match Precision Five Fscore Match Precising Ten Fscore Match Fifteen Fscore
Recall Recall Precision Recall
All KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.79 15.84% 5.19% 7.82% 1.39 13.88% 9.09% 10.99% 1.84 12.24% 12.03% 12.13%
N&amp;K S 1.32 26.48% 8.67% 13.06% 2.04 20.36% 13.34% 16.12% 2.54 16.93% 16.64% 16.78%
baseline U 0.92 18.32% 6.00% 9.04% 1.57 15.68% 10.27% 12.41% 2.20 14.64% 14.39% 14.51%
S 1.15 23.04% 7.55% 11.37% 1.90 18.96% 12.42% 15.01% 2.44 16.24% 15.96% 16.10%
1-ength&lt;=3 KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.81 16.16% 5.29% 7.97% 1.40 14.00% 9.17% 11.08% 1.84 12.24% 12.03% 12.13%
N&amp;K S 1.40 27.92% 9.15% 13.78% 2.10 21.04% 13.78% 16.65% 2.62 17.49% 17.19% 17.34%
baseline U 0.92 18.4% 6.03% 9.08% 1.58 15.76% 10.32% 12.47% 2.20 14.64% 14.39% 14.51%
S 1.18 23.68% 7.76% 11.69% 1.90 19.00% 12.45% 15.04% 2.40 16.00% 15.72% 15.86%
1-ength&lt;=3 KEA U 0.01 0.24% 0.08% 0.12% 0.05 0.52% 0.34% 0.41% 0.07 0.48% 0.47% 0.47%
Candidates S 0.83 16.64% 5.45% 8.21% 1.42 14.24% 9.33% 11.27% 1.87 12.45% 12.24% 12.34%
+ Alternation N&amp;K S 1.53 30.64% 10.04% 15.12% 2.31 23.08% 15.12% 18.27% 2.88 19.20% 18.87% 19.03%
baseline U 0.98 19.68% 6.45% 9.72% 1.72 17.24% 11.29% 13.64% 2.37 15.79% 15.51% 15.65%
S 1.33 26.56% 8.70% 13.11% 2.09 20.88% 13.68% 16.53% 2.69 17.92% 17.61% 17.76%
</table>
<tableCaption confidence="0.997715">
Table 3: Performance on Proposed Candidate Selection
</tableCaption>
<table confidence="0.999718166666667">
Features C Match Five Fscore Match Ten Fscore Match Fifteen Fscore
Prec. Recall Prec. Recall Prec. Recall
Best U 1.14 .228 .747 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
S 1.56 .312 .102 .154 2.50 .250 .164 .198 3.15 .210 .206 .208
Best U 1.14 .228 .74 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
w/o TF*IDF S 1.56 .311 .102 .154 2.46 .246 .161 .194 3.12 .208 .204 .206
</table>
<tableCaption confidence="0.99933">
Table 4: Performance on Feature Engineering
</tableCaption>
<figure confidence="0.727539428571429">
Feature
F1a,F2,F3,F4a,F4d,F9a
F1a,F1c,F2,F3,F4a,F4d,F5,F7b,F9a
F1b,F1c,F1d,F1f,F4b,F4c,F7a,F7b,F9b-d,F13
F1d,F1e,F1f,F4b,F4c,F6,F7a,F9b-d
F1e,F10,F11,F12
F1b
</figure>
<tableCaption confidence="0.9898">
Table 5: Performance on Each Feature
</tableCaption>
<bodyText confidence="0.999950614035088">
length candidates play a role to be noises so de-
creased the overall performance. We also con-
firmed that candidate alternation offered the flexi-
bility of keyphrases leading higher candidate cov-
erage as well as better performance.
To re-examine features, we analyzed the impact
of existing and new features and their variations.
First of all, unlike previous studies, we found that
the performance with and without TF*IDF did not
lead to a large difference which indicates the im-
pact of TF*IDF was minor, as long as other fea-
tures are incorporated. Secondly, counting sub-
strings for TF improved performance, while ap-
plying term weighting for TF and/or IDF did not
impact on the performance. We estimated the
cause that many of keyphrases are substrings of
candidates and vice versa. Thirdly, section in-
formation was also validated to improve perfor-
mance, as in Nguyen and Kan (2007). Extend-
ing this logic, modeling additional section infor-
mation (related work) and weighting sections both
turned out to be useful features. Other locality
features were also validated as helpful: both first
occurrence and last occurrence are helpful as it
implies the locality of the key ideas. In addi-
tion, keyphrase co-occurrence with selected sec-
tions was proposed in our work and found empiri-
cally useful. Term cohesion (Park et al., 2004) is a
useful feature although it has a heuristic factor that
reduce the weight by 10% for simplex words. Nor-
mally, term cohesion is subject to NPs only, hence
it needs to be extended to work with multi-word
NPs as well. Table 5 summarizes the reflections
on each feature.
As unsupervised methods have the appeal of not
needing to be trained on expensive hand-annotated
data, we also compared the performance of super-
vised and unsupervised methods. Given the fea-
tures initially introduced for supervised learning,
unsupervised performance is surprisingly high.
While supervised classifier produced a matching
count of 3.15, the unsupervised classifier obtains a
count of 2.61. We feel this indicates that the exist-
ing features for supervised methods are also suit-
able for use in unsupervised methods, with slightly
reduced performance. In general, we observed that
the best features in both supervised and unsuper-
vised methods are the same – section information
and candidate length. In our analysis of the im-
pact of individual features, we observed that most
features affect performance in the same way for
both supervised and unsupervised approaches, as
shown in Table 5. These findings indicate that al-
though these features may be been originally de-
signed for use in a supervised approach, they are
stable and can be expected to perform similar in
unsupervised approaches.
</bodyText>
<figure confidence="0.998092571428571">
A Method
+ S
U
- S
U
? S
U
</figure>
<page confidence="0.933869">
15
</page>
<sectionHeader confidence="0.984723" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999964866666667">
We have identified and tackled two core issues
in automatic keyphrase extraction: candidate se-
lection and feature engineering. In the area of
candidate selection, we observe variations and al-
ternations that were previously unaccounted for.
Our selection rules expand the scope of possible
keyphrase coverage, while not overly expanding
the total number candidates to consider. In our
re-examination of feature engineering, we com-
piled a comprehensive feature list from previous
works while exploring the use of substrings in de-
vising new features. Moreover, we also attested to
each feature’s fitness for use in unsupervised ap-
proaches, in order to utilize them in real-world ap-
plications with minimal cost.
</bodyText>
<sectionHeader confidence="0.995695" genericHeader="acknowledgments">
10 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99491175">
This work was partially supported by a National Research
Foundation grant, Interactive Media Search (grant # R 252
000 325 279), while the first author was a postdoctoral fellow
at the National University of Singapore.
</bodyText>
<sectionHeader confidence="0.998788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862931034483">
Ken Barker and Nadia Corrnacchia. Using noun phrase
heads to extract document keyphrases. In Proceedings of
the 13th Biennial Conference of the Canadian Society on
Computational Studies of Intelligence: Advances in Arti-
ficial Intelligence. 2000.
Regina Barzilay and Michael Elhadad. Using lexical chains
for text summarization. In Proceedings of the ACL/EACL
1997 Workshop on Intelligent Scalable Text Summariza-
tion. 1997, pp. 10–17.
Kenneth Church and Patrick Hanks. Word association
norms, mutual information and lexicography. In Proceed-
ings of ACL. 1989, 76–83.
Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit:
An open-source CRF reference string parsing package. In
Proceedings of LREC. 2008, 28–30.
Ernesto D ´Avanzo and Bernado Magnini. A Keyphrase-
Based Approach to Summarization:the LAKE System at
DUC-2005. In Proceedings of DUC. 2005.
F. Damerau. Generating and evaluating domain-oriented
multi-word terms from texts. Information Processing and
Management. 1993, 29, pp.43–447.
Lee Dice. Measures of the amount of ecologic associations
between species. Journal of Ecology. 1945, 2.
Eibe Frank and Gordon Paynter and Ian Witten and Carl
Gutwin and Craig Nevill-manning. Domain Specific
Keyphrase Extraction. In Proceedings of IJCAI. 1999,
pp.668–673.
Carl Gutwin and Gordon Paynter and Ian Witten and Craig
Nevill-Manning and Eibe Frank. Improving browsing in
digital libraries with keyphrase indexes. Journal of Deci-
sion Support Systems. 1999, 27, pp.81–104.
Khaled Hammouda and Diego Matute and Mohamed Kamel.
CorePhrase: keyphrase extraction for document cluster-
ing. In Proceedings of MLDM. 2005.
Annette Hulth and Jussi Karlgren and Anna Jonsson and
Henrik Bostrm and Lars Asker. Automatic Keyword Ex-
traction using Domain Knowledge. In Proceedings of CI-
CLing. 2001.
Annette Hulth and Beata Megyesi. A study on automatically
extracted keywords in text categorization. In Proceedings
of ACL/COLING. 2006, 537–544.
Mario Jarmasz and Caroline Barriere. Using semantic sim-
ilarity over tera-byte corpus, compute the performance of
keyphrase extraction. In Proceedings of CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summarization. In
Proceedings of SIGIR. 2001, pp. 349–357.
Y. Matsuo and M. Ishizuka. Keyword Extraction from a Sin-
gle Document using Word Co-occurrence Statistical Infor-
mation. In International Journal on Artificial Intelligence
Tools. 2004, 13(1), pp. 157–169.
Olena Medelyan and Ian Witten. Thesaurus based automatic
keyphrase indexing. In Proceedings of ACM/IEED-CS
joint conference on Digital libraries. 2006, pp.296–297.
Guido Minnen and John Carroll and Darren Pearce. Applied
morphological processing of English. NLE. 2001, 7(3),
pp.207–223.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extrac-
tion in Scientific Publications. In Proceeding of ICADL.
2007, pp.317-326.
Youngja Park and Roy Byrd and Branimir Boguraev. Auto-
matic Glossary Extraction Beyond Terminology Identifi-
cation. In Proceedings of COLING. 2004, pp.48–55.
Mari-Sanna Paukkeri and Ilari Nieminen and Matti Polla
and Timo Honkela. A Language-Independent Approach
to Keyphrase Extraction and Evaluation. In Proceedings
of COLING. 2008.
Peter Turney. Learning to Extract Keyphrases from Text.
In National Research Council, Institute for Information
Technology, Technical Report ERB-1057. 1999.
Peter Turney. Coherent keyphrase extraction via Web min-
ing. In Proceedings of IJCAI. 2003, pp. 434–439.
Xiaojun Wan and Jianguo Xiao. CollabRank: towards a col-
laborative approach to single-document keyphrase extrac-
tion. In Proceedings of COLING. 2008.
Ian Witten and Gordon Paynter and Eibe Frank and Car
Gutwin and Graig Nevill-Manning. KEA:Practical Au-
tomatic Key phrase Extraction. In Proceedings of ACM
DL. 1999, pp.254–256.
Ian Witten and Eibe Frank. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kauf-
mann, 2005.
Yongzheng Zhang and Nur Zinchir-Heywood and Evange-
los Milios. Term based Clustering and Summarization of
Web Page Collections. In Proceedings of Conference of
the Canadian Society for Computational Studies of Intel-
ligence. 2004.
</reference>
<page confidence="0.998691">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.761234">
<title confidence="0.998844">Re-examining Automatic Keyphrase Extraction in Scientific Articles</title>
<author confidence="0.998126">Su Nam</author>
<affiliation confidence="0.9927">CSSE University of</affiliation>
<email confidence="0.975019">snkim@csse.unimelb.edu.au</email>
<abstract confidence="0.989941142857143">We tackle two major issues in automatic keyphrase extraction using scientific artiselection engi- To develop an efficient candidate selection method, we analyze the nature and variation of keyphrases and then select candidates using regular expressions. Secondly, we re-examine the existing features broadly used for the supervised approach, exploring different ways to enhance their performance. While most other approaches are supervised, we also study the optimal features for unsupervised keyphrase extraction. Our research has shown that effective candidate selection leads to better performance as evaluation accounts for candidate coverage. Our work also attests that many of existing features are also usable in unsupervised extraction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Nadia Corrnacchia</author>
</authors>
<title>Using noun phrase heads to extract document keyphrases.</title>
<date>2000</date>
<booktitle>In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial Intelligence.</booktitle>
<contexts>
<context position="1752" citStr="Barker and Corrnacchia, 2000" startWordPosition="258" endWordPosition="261"> serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate selection methods have been proposed for automatic keyphrase extraction in the past (e.g. (Frank et al., 1999; Park et al., 2004; Nguyen and Kan, 2007)), most of them do not effectively deal with various keyphrase forms which results in the ignorance of some keyphrases</context>
<context position="4701" citStr="Barker and Corrnacchia, 2000" startWordPosition="715" endWordPosition="718">, we assess how well these features can be applied in an unsupervised approach. In the remaining sections, we describe an overview of related work in Section 2, our proposals on candidate selection and feature engineering in Section 4 and 5, our system architecture and data in Section 6. Then, we evaluate our proposals, discuss outcomes and conclude our work in Section 7, 8 and 9, respectively. 2 Related Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses TF*IDF (i.e. term frequency * </context>
<context position="21710" citStr="Barker and Corrnacchia (2000)" startWordPosition="3456" endWordPosition="3459">S sequence (S) Hulth and Megyesi (2006) pointed out that POS sequences of keyphrases are similar. It showed the distinctive distribution of POS sequences of keyphrases and use them as a feature. Like acronym, this is also subject to the data set. 13 F12 : Suffix sequence (S) Similar to acronym, Nguyen and Kan (2007) also used a candidate’s suffix sequence as a feature, to capture the propensity of English to use certain Latin derivational morphology for technical keyphrases. This feature is also a data dependent features, thus used in supervised approach only. F13 : Length of Keyphrases (S,U) Barker and Corrnacchia (2000) showed that candidate length is also a useful feature in extraction as well as in candidate selection, as the majority of keyphrases are one or two terms in length. 6 System and Data To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2, ParsCit3(Councill et al., 2008) for reference collection, part-of-speech tagger4 </context>
</contexts>
<marker>Barker, Corrnacchia, 2000</marker>
<rawString>Ken Barker and Nadia Corrnacchia. Using noun phrase heads to extract document keyphrases. In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial Intelligence. 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL</booktitle>
<pages>10--17</pages>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. Using lexical chains for text summarization. In Proceedings of the ACL/EACL 1997 Workshop on Intelligent Scalable Text Summarization. 1997, pp. 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of ACL.</booktitle>
<pages>76--83</pages>
<contexts>
<context position="20230" citStr="Church and Hanks, 1989" startWordPosition="3199" endWordPosition="3202">ing a similar notion of contextual similarity. We simulated a latent 2-dimensional matrix (similar to latent semantic analysis) by listing all candidate words in rows and their neighboring words (nouns, verbs, and adjectives only) in columns. The cosine measure is then used to compute the similarity among keyphrases. 5.3 Term Cohesion Term cohesion further refines the candidacy judgment, by incorporating an internal analysis of the candidate’s constituent words. Term cohesion posits that high values for internal word association measures correlates indicates that the candidate is a keyphrase (Church and Hanks, 1989). F9 : Term Cohesion (S,U) Park et al. (2004) used in the Dice coefficient (Dice, 1945) to measure term cohesion particularly for multiword terms. In their work, as NPs are longer than simplex words, they simply discounted simplex word cohesion by 10%. In our work, we vary the measure of TF used in Dice coefficient, similar to our discussion earlier. • (F9a) term cohesion by (Park et al., 2004), • (F9b*) normalized TF by candidate types (i.e. simplex words vs. NPs), • (F9c*) applying different weight by candidate types, • (F9d*) normalized TF and different weighting by candidate types 5.4 Othe</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Church and Patrick Hanks. Word association norms, mutual information and lexicography. In Proceedings of ACL. 1989, 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Councill</author>
<author>C Lee Giles</author>
<author>Min-Yen Kan</author>
</authors>
<title>ParsCit: An open-source CRF reference string parsing package.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<pages>28--30</pages>
<contexts>
<context position="22260" citStr="Councill et al., 2008" startWordPosition="3547" endWordPosition="3550">ach only. F13 : Length of Keyphrases (S,U) Barker and Corrnacchia (2000) showed that candidate length is also a useful feature in extraction as well as in candidate selection, as the majority of keyphrases are one or two terms in length. 6 System and Data To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2, ParsCit3(Councill et al., 2008) for reference collection, part-of-speech tagger4 and lemmatizer5(Minnen et al., 2001) of the input. After preprocessing, we built both supervised and unsupervised classifiers using Naive Bayes from the WEKA machine learning toolkit(Witten and Frank, 2005), Maximum Entropy6, and simple weighting. In evaluation, we collected 250 papers from four different categories7 of the ACM digital library. Each paper was 6 to 8 pages on average. In author assigned keyphrase, we found many were missing or found as substrings. To remedy this, we collected reader assigned keyphrase by hiring senior year under</context>
</contexts>
<marker>Councill, Giles, Kan, 2008</marker>
<rawString>Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit: An open-source CRF reference string parsing package. In Proceedings of LREC. 2008, 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernesto D ´Avanzo</author>
<author>Bernado Magnini</author>
</authors>
<title>A KeyphraseBased Approach to Summarization:the LAKE System at DUC-2005.</title>
<date>2005</date>
<booktitle>In Proceedings of DUC.</booktitle>
<marker>´Avanzo, Magnini, 2005</marker>
<rawString>Ernesto D ´Avanzo and Bernado Magnini. A KeyphraseBased Approach to Summarization:the LAKE System at DUC-2005. In Proceedings of DUC. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Damerau</author>
</authors>
<title>Generating and evaluating domain-oriented multi-word terms from texts.</title>
<date>1993</date>
<booktitle>Information Processing and Management.</booktitle>
<pages>29--43</pages>
<contexts>
<context position="6400" citStr="Damerau, 1993" startWordPosition="975" endWordPosition="976">ch as length in words and frequency of stemming phrase as a set of parametrized heuristic rules. Barker and Corrnacchia (2000) introduced a method based on head noun heuristics that took three features: length of candidate, frequency and head noun frequency. To take advantage of domain knowledge, Hulth et al. (2001) used a hierarchically-organized domain-specific thesaurus from Swedish Parliament as a secondary knowledge source. The Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering information. The assumption behind this work is that the documents with the same or similar topics interact with each other in terms of salience of words. The authors first clustered the documents then used the graphbased ranking algorithm to rank the candidates in a document by making use of mutual influences of other documents in the same cluster. 3 Keyphrase Analysis In previous study, KEA employed the indexing wo</context>
</contexts>
<marker>Damerau, 1993</marker>
<rawString>F. Damerau. Generating and evaluating domain-oriented multi-word terms from texts. Information Processing and Management. 1993, 29, pp.43–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Dice</author>
</authors>
<title>Measures of the amount of ecologic associations between species.</title>
<date>1945</date>
<journal>Journal of Ecology.</journal>
<pages>2</pages>
<contexts>
<context position="6465" citStr="Dice, 1945" startWordPosition="985" endWordPosition="986">arametrized heuristic rules. Barker and Corrnacchia (2000) introduced a method based on head noun heuristics that took three features: length of candidate, frequency and head noun frequency. To take advantage of domain knowledge, Hulth et al. (2001) used a hierarchically-organized domain-specific thesaurus from Swedish Parliament as a secondary knowledge source. The Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering information. The assumption behind this work is that the documents with the same or similar topics interact with each other in terms of salience of words. The authors first clustered the documents then used the graphbased ranking algorithm to rank the candidates in a document by making use of mutual influences of other documents in the same cluster. 3 Keyphrase Analysis In previous study, KEA employed the indexing words as candidates whereas others such as (Park et al., 2004; Nguy</context>
<context position="20317" citStr="Dice, 1945" startWordPosition="3217" endWordPosition="3218">to latent semantic analysis) by listing all candidate words in rows and their neighboring words (nouns, verbs, and adjectives only) in columns. The cosine measure is then used to compute the similarity among keyphrases. 5.3 Term Cohesion Term cohesion further refines the candidacy judgment, by incorporating an internal analysis of the candidate’s constituent words. Term cohesion posits that high values for internal word association measures correlates indicates that the candidate is a keyphrase (Church and Hanks, 1989). F9 : Term Cohesion (S,U) Park et al. (2004) used in the Dice coefficient (Dice, 1945) to measure term cohesion particularly for multiword terms. In their work, as NPs are longer than simplex words, they simply discounted simplex word cohesion by 10%. In our work, we vary the measure of TF used in Dice coefficient, similar to our discussion earlier. • (F9a) term cohesion by (Park et al., 2004), • (F9b*) normalized TF by candidate types (i.e. simplex words vs. NPs), • (F9c*) applying different weight by candidate types, • (F9d*) normalized TF and different weighting by candidate types 5.4 Other Features F10 : Acronym (S) Nguyen and Kan (2007) accounted for the importance of acro</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Lee Dice. Measures of the amount of ecologic associations between species. Journal of Ecology. 1945, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Gordon Paynter</author>
<author>Ian Witten</author>
<author>Carl Gutwin</author>
<author>Craig Nevill-manning</author>
</authors>
<title>Domain Specific Keyphrase Extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<pages>668--673</pages>
<contexts>
<context position="1632" citStr="Frank et al., 1999" startWordPosition="238" endWordPosition="241">esent the key ideas of the document. Keyphrases can serve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate selection methods have been proposed for automatic keyphrase extraction in the past (e.g. (Frank et al., 1999; Park et al., 2004; Nguyen and Kan, 200</context>
<context position="4657" citStr="Frank et al., 1999" startWordPosition="709" endWordPosition="712">r further improvement. In addition, we assess how well these features can be applied in an unsupervised approach. In the remaining sections, we describe an overview of related work in Section 2, our proposals on candidate selection and feature engineering in Section 4 and 5, our system architecture and data in Section 6. Then, we evaluate our proposals, discuss outcomes and conclude our work in Section 7, 8 and 9, respectively. 2 Related Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 19</context>
<context position="14786" citStr="Frank et al., 1999" startWordPosition="2317" endWordPosition="2320">t popular feature for this cohesion is TF*IDF but some works have also used context words to check the correlation between candidates and the given document. Other features for document cohesion are distance, section information and so on. We note that listed features other than TF*IDF are related to locality. That is, the intuition behind these features is that keyphrases tend to appear in specific area such as the beginning and the end of documents. F1 : TF*IDF (S,U) TF*IDF indicates document cohesion by looking at the frequency of terms in the documents and is broadly used in previous work(Frank et al., 1999; Witten et al., 1999; Nguyen and Kan, 2007). However, a disadvantage of the feature is in requiring a large corpus to compute useful IDF. As an alternative, context words(Matsuo and Ishizuka, 2004) can also be used to measure document cohesion. From our study of keyphrases, we saw that substrings within longer candidates need to be properly counted, and as such our method measures TF in substrings as well as in exact matches. For example, grid computing is often a substring of other phrases such as grid computing algorithm and efficient grid computing algorithm. We also normalize TF with resp</context>
<context position="16230" citStr="Frank et al., 1999" startWordPosition="2559" endWordPosition="2562">generic source of word count, IDF can be incorporated without corpus-dependent processing, hence such features are useful in unsupervised approaches as well. The following list shows variations of TF*IDF, employed as features in our system. • (F1a) TF*IDF • (F1b*) TF including counts of substrings • (F1c*) TF of substring as a separate feature • (F1d*) normalized TF by candidate types (i.e. simplex words vs. NPs) • (F1e*) normalized TF by candidate types as a separate feature • (F1f*) IDF using Google n-gram F2 : First Occurrence (S,U) KEA used the first appearance of the word in the document(Frank et al., 1999; Witten et al., 1999). The main idea behind this feature is that keyphrases tend to occur in the beginning of documents, especially in structured reports (e.g., in abstract and introduction sections) and newswire. F3 : Section Information (S,U) Nguyen and Kan (2007) used the identity of which specific document section a candidate occurs in. This locality feature attempts to identify key sections. For example, in their study of scientific papers, the authors weighted candidates differently depending on whether they occurred in the abstract, introduction, conclusion, section head, title and/or </context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, Nevill-manning, 1999</marker>
<rawString>Eibe Frank and Gordon Paynter and Ian Witten and Carl Gutwin and Craig Nevill-manning. Domain Specific Keyphrase Extraction. In Proceedings of IJCAI. 1999, pp.668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Gutwin</author>
<author>Gordon Paynter</author>
<author>Ian Witten</author>
<author>Craig Nevill-Manning</author>
<author>Eibe Frank</author>
</authors>
<title>Improving browsing in digital libraries with keyphrase indexes. Journal of Decision Support Systems.</title>
<date>1999</date>
<pages>27--81</pages>
<contexts>
<context position="1438" citStr="Gutwin et al., 1999" startWordPosition="211" endWordPosition="214">for candidate coverage. Our work also attests that many of existing features are also usable in unsupervised extraction. 1 Introduction Keyphrases are simplex nouns or noun phrases (NPs) that represent the key ideas of the document. Keyphrases can serve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improveme</context>
</contexts>
<marker>Gutwin, Paynter, Witten, Nevill-Manning, Frank, 1999</marker>
<rawString>Carl Gutwin and Gordon Paynter and Ian Witten and Craig Nevill-Manning and Eibe Frank. Improving browsing in digital libraries with keyphrase indexes. Journal of Decision Support Systems. 1999, 27, pp.81–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Hammouda</author>
<author>Diego Matute</author>
<author>Mohamed Kamel</author>
</authors>
<title>CorePhrase: keyphrase extraction for document clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of MLDM.</booktitle>
<contexts>
<context position="1485" citStr="Hammouda et al., 2005" startWordPosition="217" endWordPosition="220"> that many of existing features are also usable in unsupervised extraction. 1 Introduction Keyphrases are simplex nouns or noun phrases (NPs) that represent the key ideas of the document. Keyphrases can serve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate se</context>
</contexts>
<marker>Hammouda, Matute, Kamel, 2005</marker>
<rawString>Khaled Hammouda and Diego Matute and Mohamed Kamel. CorePhrase: keyphrase extraction for document clustering. In Proceedings of MLDM. 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annette Hulth</author>
<author>Jussi Karlgren</author>
<author>Anna Jonsson</author>
<author>Henrik Bostrm</author>
<author>Lars Asker</author>
</authors>
<title>Automatic Keyword Extraction using Domain Knowledge.</title>
<date>2001</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<contexts>
<context position="6103" citStr="Hulth et al. (2001)" startWordPosition="932" endWordPosition="935"> which indicates the keyphrases have a locality. Turney (2003) added the notion of keyphrase cohesion to KEA features and Nguyen and Kan (2007) added linguistic features such as section information and suffix sequence. The GenEx system(Turney, 1999) employed an inventory of nine syntactic features, such as length in words and frequency of stemming phrase as a set of parametrized heuristic rules. Barker and Corrnacchia (2000) introduced a method based on head noun heuristics that took three features: length of candidate, frequency and head noun frequency. To take advantage of domain knowledge, Hulth et al. (2001) used a hierarchically-organized domain-specific thesaurus from Swedish Parliament as a secondary knowledge source. The Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering information. The assumption behind this work is that the documents with the same or similar topics interact with each other</context>
</contexts>
<marker>Hulth, Karlgren, Jonsson, Bostrm, Asker, 2001</marker>
<rawString>Annette Hulth and Jussi Karlgren and Anna Jonsson and Henrik Bostrm and Lars Asker. Automatic Keyword Extraction using Domain Knowledge. In Proceedings of CICLing. 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annette Hulth</author>
<author>Beata Megyesi</author>
</authors>
<title>A study on automatically extracted keywords in text categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING.</booktitle>
<pages>537--544</pages>
<contexts>
<context position="21120" citStr="Hulth and Megyesi (2006)" startWordPosition="3358" endWordPosition="3361">we vary the measure of TF used in Dice coefficient, similar to our discussion earlier. • (F9a) term cohesion by (Park et al., 2004), • (F9b*) normalized TF by candidate types (i.e. simplex words vs. NPs), • (F9c*) applying different weight by candidate types, • (F9d*) normalized TF and different weighting by candidate types 5.4 Other Features F10 : Acronym (S) Nguyen and Kan (2007) accounted for the importance of acronym as a feature. We found that this feature is heavily dependent on the data set. Hence, we used it only for N&amp;K to attest our candidate selection method. F11 : POS sequence (S) Hulth and Megyesi (2006) pointed out that POS sequences of keyphrases are similar. It showed the distinctive distribution of POS sequences of keyphrases and use them as a feature. Like acronym, this is also subject to the data set. 13 F12 : Suffix sequence (S) Similar to acronym, Nguyen and Kan (2007) also used a candidate’s suffix sequence as a feature, to capture the propensity of English to use certain Latin derivational morphology for technical keyphrases. This feature is also a data dependent features, thus used in supervised approach only. F13 : Length of Keyphrases (S,U) Barker and Corrnacchia (2000) showed th</context>
</contexts>
<marker>Hulth, Megyesi, 2006</marker>
<rawString>Annette Hulth and Beata Megyesi. A study on automatically extracted keywords in text categorization. In Proceedings of ACL/COLING. 2006, 537–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Caroline Barriere</author>
</authors>
<title>Using semantic similarity over tera-byte corpus, compute the performance of keyphrase extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of CLINE.</booktitle>
<marker>Jarmasz, Barriere, 2004</marker>
<rawString>Mario Jarmasz and Caroline Barriere. Using semantic similarity over tera-byte corpus, compute the performance of keyphrase extraction. In Proceedings of CLINE. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn Lawrie</author>
<author>W Bruce Croft</author>
<author>Arnold Rosenberg</author>
</authors>
<title>Finding Topic Words for Hierarchical Summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<pages>349--357</pages>
<marker>Lawrie, Croft, Rosenberg, 2001</marker>
<rawString>Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg. Finding Topic Words for Hierarchical Summarization. In Proceedings of SIGIR. 2001, pp. 349–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Keyword Extraction from a Single Document using Word Co-occurrence Statistical Information.</title>
<date>2004</date>
<booktitle>In International Journal on Artificial Intelligence Tools.</booktitle>
<volume>13</volume>
<issue>1</issue>
<pages>157--169</pages>
<contexts>
<context position="4728" citStr="Matsuo and Ishizuka, 2004" startWordPosition="719" endWordPosition="722">tures can be applied in an unsupervised approach. In the remaining sections, we describe an overview of related work in Section 2, our proposals on candidate selection and feature engineering in Section 4 and 5, our system architecture and data in Section 6. Then, we evaluate our proposals, discuss outcomes and conclude our work in Section 7, 8 and 9, respectively. 2 Related Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses TF*IDF (i.e. term frequency * inverse document frequency)</context>
<context position="14984" citStr="Matsuo and Ishizuka, 2004" startWordPosition="2351" endWordPosition="2354">sion are distance, section information and so on. We note that listed features other than TF*IDF are related to locality. That is, the intuition behind these features is that keyphrases tend to appear in specific area such as the beginning and the end of documents. F1 : TF*IDF (S,U) TF*IDF indicates document cohesion by looking at the frequency of terms in the documents and is broadly used in previous work(Frank et al., 1999; Witten et al., 1999; Nguyen and Kan, 2007). However, a disadvantage of the feature is in requiring a large corpus to compute useful IDF. As an alternative, context words(Matsuo and Ishizuka, 2004) can also be used to measure document cohesion. From our study of keyphrases, we saw that substrings within longer candidates need to be properly counted, and as such our method measures TF in substrings as well as in exact matches. For example, grid computing is often a substring of other phrases such as grid computing algorithm and efficient grid computing algorithm. We also normalize TF with respect to candidate types: i.e. we separately treat simplex words and NPs to compute TF. To make our IDFs broadly representative, we employed the Google n-gram counts, that were computed over terabytes</context>
</contexts>
<marker>Matsuo, Ishizuka, 2004</marker>
<rawString>Y. Matsuo and M. Ishizuka. Keyword Extraction from a Single Document using Word Co-occurrence Statistical Information. In International Journal on Artificial Intelligence Tools. 2004, 13(1), pp. 157–169.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
<author>Ian Witten</author>
</authors>
<title>Thesaurus based automatic keyphrase indexing.</title>
<booktitle>In Proceedings of ACM/IEED-CS joint conference on Digital libraries. 2006,</booktitle>
<pages>296--297</pages>
<marker>Medelyan, Witten, </marker>
<rawString>Olena Medelyan and Ian Witten. Thesaurus based automatic keyphrase indexing. In Proceedings of ACM/IEED-CS joint conference on Digital libraries. 2006, pp.296–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<volume>7</volume>
<issue>3</issue>
<pages>207--223</pages>
<location>NLE.</location>
<contexts>
<context position="22346" citStr="Minnen et al., 2001" startWordPosition="3558" endWordPosition="3562">didate length is also a useful feature in extraction as well as in candidate selection, as the majority of keyphrases are one or two terms in length. 6 System and Data To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2, ParsCit3(Councill et al., 2008) for reference collection, part-of-speech tagger4 and lemmatizer5(Minnen et al., 2001) of the input. After preprocessing, we built both supervised and unsupervised classifiers using Naive Bayes from the WEKA machine learning toolkit(Witten and Frank, 2005), Maximum Entropy6, and simple weighting. In evaluation, we collected 250 papers from four different categories7 of the ACM digital library. Each paper was 6 to 8 pages on average. In author assigned keyphrase, we found many were missing or found as substrings. To remedy this, we collected reader assigned keyphrase by hiring senior year undergraduates in computer science, each whom annotated five of the papers with an annotati</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen and John Carroll and Darren Pearce. Applied morphological processing of English. NLE. 2001, 7(3), pp.207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thuy Dung Nguyen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Key phrase Extraction in Scientific Publications.</title>
<date>2007</date>
<booktitle>In Proceeding of ICADL.</booktitle>
<pages>317--326</pages>
<contexts>
<context position="1900" citStr="Nguyen and Kan, 2007" startWordPosition="277" endWordPosition="280">lity of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate selection methods have been proposed for automatic keyphrase extraction in the past (e.g. (Frank et al., 1999; Park et al., 2004; Nguyen and Kan, 2007)), most of them do not effectively deal with various keyphrase forms which results in the ignorance of some keyphrases as candidates. Moreover, no studies thus far have done a detailed investigation of the nature and variation of manually-provided keyphrases. As a c</context>
<context position="4751" citStr="Nguyen and Kan, 2007" startWordPosition="723" endWordPosition="726">unsupervised approach. In the remaining sections, we describe an overview of related work in Section 2, our proposals on candidate selection and feature engineering in Section 4 and 5, our system architecture and data in Section 6. Then, we evaluate our proposals, discuss outcomes and conclude our work in Section 7, 8 and 9, respectively. 2 Related Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses TF*IDF (i.e. term frequency * inverse document frequency) and first occurrence i</context>
<context position="7082" citStr="Nguyen and Kan, 2007" startWordPosition="1084" endWordPosition="1087">945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering information. The assumption behind this work is that the documents with the same or similar topics interact with each other in terms of salience of words. The authors first clustered the documents then used the graphbased ranking algorithm to rank the candidates in a document by making use of mutual influences of other documents in the same cluster. 3 Keyphrase Analysis In previous study, KEA employed the indexing words as candidates whereas others such as (Park et al., 2004; Nguyen and Kan, 2007) generated handcrafted regular expression rules. However, none carefully undertook the analysis of keyphrases. We believe there is more to be learned from the reference keyphrases themselves by doing a fine-grained, careful analysis of their form and composition. Note that we used the articles collected from ACM digital library for both analyzing keyphrases as well as evaluating methods. See Section 6 for data in detail. Syntactically, keyphrases can be formed by either simplex nouns (e.g. algorithm, keyphrase, multi-agent) or noun phrases (NPs) which can be a sequence of nouns and their auxil</context>
<context position="11195" citStr="Nguyen and Kan, 2007" startWordPosition="1709" endWordPosition="1712">as well (e.g. distributed system =� distributing system, dynamical caching =� dynamical cache). In addition, possessives tend to alternate with of-PP form (e.g. agent’s goal = goal of agent, security’s value = value of security). 4 Candidate Selection We now describe our proposed candidate selection process. Candidate selection is a crucial step for automatic keyphrase extraction. This step is correlated to term extraction study since top Nth ranked terms become keyphrases in documents. In previous study, KEA employed the indexing words as candidates whereas others such as (Park et al., 2004; Nguyen and Kan, 2007) generated handcrafted regular expression rules. However, none carefully undertook the analysis of keyphrases. In this section, before we present our method, we first describe the detail of keyphrase analysis. In our keyphrase analysis, we observed that most of author assigned keyphrase and/or reader assigned keyphrase are syntactically more often simplex words and less often NPs. When keyphrases take an NP form, they tend to be a simple form of NPs. i.e. either without a PP or with only a PP or with a conjunction, but few appear as a mixture of such forms. We also noticed that the components </context>
<context position="13043" citStr="Nguyen and Kan (2007)" startWordPosition="2024" endWordPosition="2027">g. performance of distributed system = distributed system performance). In previous studies, words occurring at least twice are selected as candidates. However, during our acquisition of reader assigned keyphrase, we observed that readers tend to collect NPs as keyphrases, regardless of their frequency. Due to this, we apply different frequency thresholds for simplex words (&gt;= 2) and NPs (&gt;= 1). Note that 30% of NPs occurred only once in our data. Finally, we generated regular expression rules to extract candidates, as presented in Table 1. Our candidate extraction rules are based on those in Nguyen and Kan (2007). However, our Rule6 for NPs in of-PP form broadens the coverage of 11 possible candidates. i.e. with a given NPs in ofPP form, not only we collect simplex word(s), but we also extract non-of-PP form of NPs from noun phrases governing the PP and the PP. For example, our rule extracts effective algorithm of grid computing as well as effective algorithm and grid computing as candidates while the previous works’ rules do not. 5 Feature Engineering With a wider candidate selection criteria, the onus of filtering out irrelevant candidates becomes the responsibility of careful feature engineering. W</context>
<context position="14830" citStr="Nguyen and Kan, 2007" startWordPosition="2325" endWordPosition="2328">*IDF but some works have also used context words to check the correlation between candidates and the given document. Other features for document cohesion are distance, section information and so on. We note that listed features other than TF*IDF are related to locality. That is, the intuition behind these features is that keyphrases tend to appear in specific area such as the beginning and the end of documents. F1 : TF*IDF (S,U) TF*IDF indicates document cohesion by looking at the frequency of terms in the documents and is broadly used in previous work(Frank et al., 1999; Witten et al., 1999; Nguyen and Kan, 2007). However, a disadvantage of the feature is in requiring a large corpus to compute useful IDF. As an alternative, context words(Matsuo and Ishizuka, 2004) can also be used to measure document cohesion. From our study of keyphrases, we saw that substrings within longer candidates need to be properly counted, and as such our method measures TF in substrings as well as in exact matches. For example, grid computing is often a substring of other phrases such as grid computing algorithm and efficient grid computing algorithm. We also normalize TF with respect to candidate types: i.e. we separately t</context>
<context position="16497" citStr="Nguyen and Kan (2007)" startWordPosition="2603" endWordPosition="2606"> TF including counts of substrings • (F1c*) TF of substring as a separate feature • (F1d*) normalized TF by candidate types (i.e. simplex words vs. NPs) • (F1e*) normalized TF by candidate types as a separate feature • (F1f*) IDF using Google n-gram F2 : First Occurrence (S,U) KEA used the first appearance of the word in the document(Frank et al., 1999; Witten et al., 1999). The main idea behind this feature is that keyphrases tend to occur in the beginning of documents, especially in structured reports (e.g., in abstract and introduction sections) and newswire. F3 : Section Information (S,U) Nguyen and Kan (2007) used the identity of which specific document section a candidate occurs in. This locality feature attempts to identify key sections. For example, in their study of scientific papers, the authors weighted candidates differently depending on whether they occurred in the abstract, introduction, conclusion, section head, title and/or references. F4* : Additional Section Information (S,U) We first added the related work or previous work as one of section information not included in Nguyen and Kan (2007). We also propose and test a number of variations. We used the substrings that occur in section </context>
<context position="20880" citStr="Nguyen and Kan (2007)" startWordPosition="3312" endWordPosition="3315">rk et al. (2004) used in the Dice coefficient (Dice, 1945) to measure term cohesion particularly for multiword terms. In their work, as NPs are longer than simplex words, they simply discounted simplex word cohesion by 10%. In our work, we vary the measure of TF used in Dice coefficient, similar to our discussion earlier. • (F9a) term cohesion by (Park et al., 2004), • (F9b*) normalized TF by candidate types (i.e. simplex words vs. NPs), • (F9c*) applying different weight by candidate types, • (F9d*) normalized TF and different weighting by candidate types 5.4 Other Features F10 : Acronym (S) Nguyen and Kan (2007) accounted for the importance of acronym as a feature. We found that this feature is heavily dependent on the data set. Hence, we used it only for N&amp;K to attest our candidate selection method. F11 : POS sequence (S) Hulth and Megyesi (2006) pointed out that POS sequences of keyphrases are similar. It showed the distinctive distribution of POS sequences of keyphrases and use them as a feature. Like acronym, this is also subject to the data set. 13 F12 : Suffix sequence (S) Similar to acronym, Nguyen and Kan (2007) also used a candidate’s suffix sequence as a feature, to capture the propensity o</context>
<context position="28582" citStr="Nguyen and Kan (2007)" startWordPosition="4509" endWordPosition="4512">lyzed the impact of existing and new features and their variations. First of all, unlike previous studies, we found that the performance with and without TF*IDF did not lead to a large difference which indicates the impact of TF*IDF was minor, as long as other features are incorporated. Secondly, counting substrings for TF improved performance, while applying term weighting for TF and/or IDF did not impact on the performance. We estimated the cause that many of keyphrases are substrings of candidates and vice versa. Thirdly, section information was also validated to improve performance, as in Nguyen and Kan (2007). Extending this logic, modeling additional section information (related work) and weighting sections both turned out to be useful features. Other locality features were also validated as helpful: both first occurrence and last occurrence are helpful as it implies the locality of the key ideas. In addition, keyphrase co-occurrence with selected sections was proposed in our work and found empirically useful. Term cohesion (Park et al., 2004) is a useful feature although it has a heuristic factor that reduce the weight by 10% for simplex words. Normally, term cohesion is subject to NPs only, hen</context>
</contexts>
<marker>Nguyen, Kan, 2007</marker>
<rawString>Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extraction in Scientific Publications. In Proceeding of ICADL. 2007, pp.317-326.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Youngja Park</author>
<author>Roy Byrd</author>
<author>Branimir Boguraev</author>
</authors>
<title>Automatic Glossary Extraction Beyond Terminology Identification.</title>
<booktitle>In Proceedings of COLING. 2004,</booktitle>
<pages>48--55</pages>
<marker>Park, Byrd, Boguraev, </marker>
<rawString>Youngja Park and Roy Byrd and Branimir Boguraev. Automatic Glossary Extraction Beyond Terminology Identification. In Proceedings of COLING. 2004, pp.48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari-Sanna Paukkeri</author>
<author>Ilari Nieminen</author>
<author>Matti Polla</author>
<author>Timo Honkela</author>
</authors>
<title>A Language-Independent Approach to Keyphrase Extraction and Evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="12187" citStr="Paukkeri et al. (2008)" startWordPosition="1881" endWordPosition="1884">ss often NPs. When keyphrases take an NP form, they tend to be a simple form of NPs. i.e. either without a PP or with only a PP or with a conjunction, but few appear as a mixture of such forms. We also noticed that the components of NPs are normally nouns and adjectives but rarely, are adverbs and verbs. As a result, we decided to ignore NPs containing adverbs and verbs in this study as our candidates since they tend to produce more errors and to require more complexity. Another observation is that keyphrases containing more than three words are rare (i.e. 6% in our data set), validating what Paukkeri et al. (2008) observed. Hence, we apply a length heuristic. Our candidate selection rule collects candidates up to length 3, but also of length 4 for NPs in of-PP form, since they may have a non-genetive alternation that reduces its length to 3 (e.g. performance of distributed system = distributed system performance). In previous studies, words occurring at least twice are selected as candidates. However, during our acquisition of reader assigned keyphrase, we observed that readers tend to collect NPs as keyphrases, regardless of their frequency. Due to this, we apply different frequency thresholds for sim</context>
</contexts>
<marker>Paukkeri, Nieminen, Polla, Honkela, 2008</marker>
<rawString>Mari-Sanna Paukkeri and Ilari Nieminen and Matti Polla and Timo Honkela. A Language-Independent Approach to Keyphrase Extraction and Evaluation. In Proceedings of COLING. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Learning to Extract Keyphrases from Text.</title>
<date>1999</date>
<booktitle>In National Research Council, Institute for Information Technology, Technical Report ERB-1057.</booktitle>
<contexts>
<context position="4671" citStr="Turney, 1999" startWordPosition="713" endWordPosition="714">t. In addition, we assess how well these features can be applied in an unsupervised approach. In the remaining sections, we describe an overview of related work in Section 2, our proposals on candidate selection and feature engineering in Section 4 and 5, our system architecture and data in Section 6. Then, we evaluate our proposals, discuss outcomes and conclude our work in Section 7, 8 and 9, respectively. 2 Related Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses </context>
</contexts>
<marker>Turney, 1999</marker>
<rawString>Peter Turney. Learning to Extract Keyphrases from Text. In National Research Council, Institute for Information Technology, Technical Report ERB-1057. 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Coherent keyphrase extraction via Web mining.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<pages>434--439</pages>
<contexts>
<context position="1646" citStr="Turney, 2003" startWordPosition="242" endWordPosition="243">of the document. Keyphrases can serve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate selection methods have been proposed for automatic keyphrase extraction in the past (e.g. (Frank et al., 1999; Park et al., 2004; Nguyen and Kan, 2007)), most of t</context>
<context position="5088" citStr="Turney, 2003" startWordPosition="773" endWordPosition="774">lated Work The majority of related work has been carried out using statistical approaches, a rich set of symbolic resources and linguistically-motivated heuristics(Frank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses TF*IDF (i.e. term frequency * inverse document frequency) and first occurrence in the document. TF*IDF measures the document cohesion and the first occurrence implies the importance of the abstract or introduction which indicates the keyphrases have a locality. Turney (2003) added the notion of keyphrase cohesion to KEA features and Nguyen and Kan (2007) added linguistic features such as section information and su</context>
<context position="19159" citStr="Turney (2003)" startWordPosition="3034" endWordPosition="3035">yphrases. Hence, we used the number of sections that candidates co-occur. F7* : Title overlap (S) In a way, titles also represent the topics of their documents. A large collection of titles in the domain can act as a probabilistic prior of what words could stand as constituent words in keyphrases. In our work, as we examined scientific papers from computer science, we used a collection of titles obtained from the large CiteSeer1 collection to create this feature. • (F7a*) co-occurrence (Boolean) in title collocation • (F7b*) co-occurrence (TF) in title collection F8 : Keyphrase Cohesion (S,U) Turney (2003) integrated keyphrase cohesion into his system by checking the semantic similarity between top N ranked candidates against the remainder. In the 1It contains 1.3M titles from articles, papers and reports. original work, a large, external web corpus was used to obtain the similarity judgments. As we did not have access to the same web corpus and all candidates/keyphrases were not found in the Google n-gram corpus, we approximated this feature using a similar notion of contextual similarity. We simulated a latent 2-dimensional matrix (similar to latent semantic analysis) by listing all candidate</context>
</contexts>
<marker>Turney, 2003</marker>
<rawString>Peter Turney. Coherent keyphrase extraction via Web mining. In Proceedings of IJCAI. 2003, pp. 434–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianguo Xiao</author>
</authors>
<title>CollabRank: towards a collaborative approach to single-document keyphrase extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1686" citStr="Wan and Xiao, 2008" startWordPosition="248" endWordPosition="251">erve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, Min-Yen Kan Department of Computer Science National University of Singapore kanmy@comp.nus.edu.sg 2006; Nguyen and Kan, 2007). In Section 2, we give a more comprehensive overview of previous attempts. Current keyphrase technology still has much room for improvement. First of all, although several candidate selection methods have been proposed for automatic keyphrase extraction in the past (e.g. (Frank et al., 1999; Park et al., 2004; Nguyen and Kan, 2007)), most of them do not effectively deal with various</context>
<context position="6496" citStr="Wan and Xiao (2008)" startWordPosition="988" endWordPosition="991">ules. Barker and Corrnacchia (2000) introduced a method based on head noun heuristics that took three features: length of candidate, frequency and head noun frequency. To take advantage of domain knowledge, Hulth et al. (2001) used a hierarchically-organized domain-specific thesaurus from Swedish Parliament as a secondary knowledge source. The Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering information. The assumption behind this work is that the documents with the same or similar topics interact with each other in terms of salience of words. The authors first clustered the documents then used the graphbased ranking algorithm to rank the candidates in a document by making use of mutual influences of other documents in the same cluster. 3 Keyphrase Analysis In previous study, KEA employed the indexing words as candidates whereas others such as (Park et al., 2004; Nguyen and Kan, 2007) generated han</context>
</contexts>
<marker>Wan, Xiao, 2008</marker>
<rawString>Xiaojun Wan and Jianguo Xiao. CollabRank: towards a collaborative approach to single-document keyphrase extraction. In Proceedings of COLING. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Gordon Paynter</author>
<author>Eibe Frank</author>
<author>Car Gutwin</author>
<author>Graig Nevill-Manning</author>
</authors>
<title>KEA:Practical Automatic Key phrase Extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of ACM DL.</booktitle>
<pages>254--256</pages>
<contexts>
<context position="5260" citStr="Witten et al., 1999" startWordPosition="800" endWordPosition="803">rank et al., 1999; Turney, 1999; Barker and Corrnacchia, 2000; Matsuo and Ishizuka, 2004; Nguyen and Kan, 2007). Features used can be categorized into three broad groups: (1) document cohesion features (i.e. relationship between document and keyphrases)(Frank et al., 1999; Matsuo and Ishizuka, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), and to lesser, (2) keyphrase cohesion features (i.e. relationship among keyphrases)(Turney, 2003) and (3) term cohesion features (i.e. relationship among components in a keyphrase)(Park et al., 2004). The simplest system is KEA (Frank et al., 1999; Witten et al., 1999) that uses TF*IDF (i.e. term frequency * inverse document frequency) and first occurrence in the document. TF*IDF measures the document cohesion and the first occurrence implies the importance of the abstract or introduction which indicates the keyphrases have a locality. Turney (2003) added the notion of keyphrase cohesion to KEA features and Nguyen and Kan (2007) added linguistic features such as section information and suffix sequence. The GenEx system(Turney, 1999) employed an inventory of nine syntactic features, such as length in words and frequency of stemming phrase as a set of paramet</context>
<context position="14807" citStr="Witten et al., 1999" startWordPosition="2321" endWordPosition="2324">r this cohesion is TF*IDF but some works have also used context words to check the correlation between candidates and the given document. Other features for document cohesion are distance, section information and so on. We note that listed features other than TF*IDF are related to locality. That is, the intuition behind these features is that keyphrases tend to appear in specific area such as the beginning and the end of documents. F1 : TF*IDF (S,U) TF*IDF indicates document cohesion by looking at the frequency of terms in the documents and is broadly used in previous work(Frank et al., 1999; Witten et al., 1999; Nguyen and Kan, 2007). However, a disadvantage of the feature is in requiring a large corpus to compute useful IDF. As an alternative, context words(Matsuo and Ishizuka, 2004) can also be used to measure document cohesion. From our study of keyphrases, we saw that substrings within longer candidates need to be properly counted, and as such our method measures TF in substrings as well as in exact matches. For example, grid computing is often a substring of other phrases such as grid computing algorithm and efficient grid computing algorithm. We also normalize TF with respect to candidate type</context>
<context position="16252" citStr="Witten et al., 1999" startWordPosition="2563" endWordPosition="2566">rd count, IDF can be incorporated without corpus-dependent processing, hence such features are useful in unsupervised approaches as well. The following list shows variations of TF*IDF, employed as features in our system. • (F1a) TF*IDF • (F1b*) TF including counts of substrings • (F1c*) TF of substring as a separate feature • (F1d*) normalized TF by candidate types (i.e. simplex words vs. NPs) • (F1e*) normalized TF by candidate types as a separate feature • (F1f*) IDF using Google n-gram F2 : First Occurrence (S,U) KEA used the first appearance of the word in the document(Frank et al., 1999; Witten et al., 1999). The main idea behind this feature is that keyphrases tend to occur in the beginning of documents, especially in structured reports (e.g., in abstract and introduction sections) and newswire. F3 : Section Information (S,U) Nguyen and Kan (2007) used the identity of which specific document section a candidate occurs in. This locality feature attempts to identify key sections. For example, in their study of scientific papers, the authors weighted candidates differently depending on whether they occurred in the abstract, introduction, conclusion, section head, title and/or references. F4* : Addi</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Ian Witten and Gordon Paynter and Eibe Frank and Car Gutwin and Graig Nevill-Manning. KEA:Practical Automatic Key phrase Extraction. In Proceedings of ACM DL. 1999, pp.254–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="22516" citStr="Witten and Frank, 2005" startWordPosition="3584" endWordPosition="3587"> To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2, ParsCit3(Councill et al., 2008) for reference collection, part-of-speech tagger4 and lemmatizer5(Minnen et al., 2001) of the input. After preprocessing, we built both supervised and unsupervised classifiers using Naive Bayes from the WEKA machine learning toolkit(Witten and Frank, 2005), Maximum Entropy6, and simple weighting. In evaluation, we collected 250 papers from four different categories7 of the ACM digital library. Each paper was 6 to 8 pages on average. In author assigned keyphrase, we found many were missing or found as substrings. To remedy this, we collected reader assigned keyphrase by hiring senior year undergraduates in computer science, each whom annotated five of the papers with an annotation guideline and on average, took about 15 minutes to annotate each paper. The final statistics of keyphrases is presented in Table 2 where Combined represents the total </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian Witten and Eibe Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongzheng Zhang</author>
<author>Nur Zinchir-Heywood</author>
<author>Evangelos Milios</author>
</authors>
<title>Term based Clustering and Summarization of Web Page Collections.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference of the Canadian Society for Computational Studies of Intelligence.</booktitle>
<marker>Zhang, Zinchir-Heywood, Milios, 2004</marker>
<rawString>Yongzheng Zhang and Nur Zinchir-Heywood and Evangelos Milios. Term based Clustering and Summarization of Web Page Collections. In Proceedings of Conference of the Canadian Society for Computational Studies of Intelligence. 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>