<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989374">
A New Approach for English-Chinese Named Entity Alignment
</title>
<author confidence="0.994967">
Donghui Feng*
</author>
<affiliation confidence="0.85788175">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina Del Rey, CA, U.S.A, 90292
</affiliation>
<email confidence="0.993541">
donghui@isi.edu
</email>
<author confidence="0.967458">
Yajuan Lv† Ming Zhou†
</author>
<affiliation confidence="0.970978">
†Microsoft Research Asia
</affiliation>
<address confidence="0.937664">
5F Sigma Center, No.49 Zhichun Road, Haidian
Beijing, China, 100080
</address>
<email confidence="0.993895">
{t-yjlv, mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.992479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999734125">
Traditional word alignment approaches cannot
come up with satisfactory results for Named
Entities. In this paper, we propose a novel
approach using a maximum entropy model for
named entity alignment. To ease the training
of the maximum entropy model, bootstrapping
is used to help supervised learning. Unlike
previous work reported in the literature, our
work conducts bilingual Named Entity
alignment without word segmentation for
Chinese and its performance is much better
than that with word segmentation. When
compared with IBM and HMM alignment
models, experimental results show that our
approach outperforms IBM Model 4 and
HMM significantly.
</bodyText>
<sectionHeader confidence="0.998694" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997505">
This paper addresses the Named Entity (NE)
alignment of a bilingual corpus, which means
building an alignment between each source NE and
its translation NE in the target language. Research
has shown that Named Entities (NE) carry
essential information in human language (Hobbs et
al., 1996). Aligning bilingual Named Entities is an
effective way to extract an NE translation list and
translation templates. For example, in the
following sentence pair, aligning the NEs, [Zhi
Chun road] and [知春路] can produce a translation
template correctly.
</bodyText>
<listItem confidence="0.999628666666667">
• Can I get to [LN Zhi Chun road] by eight
o’clock?
• 八点我能到 [LN 知春路]吗?
</listItem>
<bodyText confidence="0.978261428571429">
In addition, NE alignment can be very useful for
Statistical Machine Translation (SMT) and Cross-
Language Information Retrieval (CLIR).
A Named Entity alignment, however, is not easy
to obtain. It requires both Named Entity
Recognition (NER) and alignment be handled
correctly. NEs may not be well recognized, or only
</bodyText>
<footnote confidence="0.6172595">
* The work was done while the first author was
visiting Microsoft Research Asia.
</footnote>
<bodyText confidence="0.999740071428572">
parts of them may be recognized during NER.
When aligning bilingual NEs in different
languages, we need to handle many-to-many
alignments. And the inconsistency of NE
translation and NER in different languages is also a
big problem. Specifically, in Chinese NE
processing, since Chinese is not a tokenized
language, previous work (Huang et al., 2003)
normally conducts word segmentation and
identifies Named Entities in turn. This involves
several problems for Chinese NEs, such as word
segmentation error, the identification of Chinese
NE boundaries, and the mis-tagging of Chinese
NEs. For example, “国防部长” in Chinese is really
one unit and should not be segmented as [ON 国防
部]/长. The errors from word segmentation and
NER will propagate into NE alignment.
In this paper, we propose a novel approach using
a maximum entropy model to carry out English-
Chinese Named Entity1 alignment. NEs in English
are first recognized by NER tools. We then
investigate NE translation features to identify NEs
in Chinese and determine the most probable
alignment. To ease the training of the maximum
entropy model, bootstrapping is used to help
supervised learning.
On the other hand, to avoid error propagations
from word segmentation and NER, we directly
extract Chinese NEs and make the alignment from
plain text without word segmentation. It is unlike
previous work reported in the literature. Although
this makes the task more difficult, it greatly
reduces the chance of errors introduced by
previous steps and therefore produces much better
performance on our task.
To justify our approach, we adopt traditional
alignment approaches, in particular IBM Model 4
(Brown et al., 1993) and HMM (Vogel et al.,
1996), to carry out NE alignment as our baseline
systems. Experimental results show that in this task
our approach outperforms IBM Model 4 and HMM
significantly. Furthermore, the performance
</bodyText>
<footnote confidence="0.983828666666667">
1 We only discuss NEs of three categories: Person
Name (PN), Location Name (LN), and Organization
Name (ON).
</footnote>
<bodyText confidence="0.999566090909091">
without word segmentation is much better than that
with word segmentation.
The rest of this paper is organized as follows: In
section 2, we discuss related work on NE
alignment. Section 3 gives the overall framework
of NE alignment with our maximum entropy
model. Feature functions and bootstrapping
procedures are also explained in this section. We
show experimental results and compare them with
baseline systems in Section 4. Section 5 concludes
the paper and discusses ongoing future work.
</bodyText>
<sectionHeader confidence="0.999607" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999986862745098">
Translation knowledge can be acquired via word
and phrase alignment. So far a lot of research has
been conducted in the field of machine translation
and knowledge acquisition, including both
statistical approaches (Cherry and Lin, 2003;
Probst and Brown, 2002; Wang et al., 2002; Och
and Ney, 2000; Melamed, 2000; Vogel et al., 1996)
and symbolic approaches (Huang and Choi, 2000;
Ker and Chang, 1997).
However, these approaches do not work well on
the task of NE alignment. Traditional approaches
following IBM Models (Brown et al., 1993) are not
able to produce satisfactory results due to their
inherent inability to handle many-to-many
alignments. They only carry out the alignment
between words and do not consider the case of
complex phrases like some multi-word NEs. On
the other hand, IBM Models allow at most one
word in the source language to correspond to a
word in the target language (Koehn et al., 2003;
Marcu, 2001). Therefore they can not handle
many-to-many word alignments within NEs well.
Another well-known word alignment approach,
HMM (Vogel et al., 1996), makes the alignment
probabilities depend on the alignment position of
the previous word. It does not explicitly consider
many-to-many alignment either.
Huang et al. (2003) proposed to extract Named
Entity translingual equivalences based on the
minimization of a linearly combined multi-feature
cost. But they require Named Entity Recognition
on both the source side and the target side.
Moore’s (2003) approach is based on a sequence of
cost models. However, this approach greatly relies
on linguistic information, such as a string repeated
on both sides, and clues from capital letters that are
not suitable for language pairs not belonging to the
same family. Also, there are already complete
lexical compounds identified on the target side,
which represent a big part of the final results.
During the alignment, Moore does not hypothesize
that translations of phrases would require splitting
predetermined lexical compounds on the target set.
These methods are not suitable for our task,
since we only have NEs identified on the source
side, and there is no extra knowledge from the
target side. Considering the inherent characteristics
of NE translation, we can find several features that
can help NE alignment; therefore, we use a
maximum entropy model to integrate these features
and carry out NE alignment.
</bodyText>
<sectionHeader confidence="0.9478495" genericHeader="method">
3 NE Alignment with a Maximum Entropy
Model
</sectionHeader>
<bodyText confidence="0.999639294117647">
Without relying on syntactic knowledge from
either the English side or the Chinese side, we find
there are several valuable features that can be used
for Named Entity alignment. Considering the
advantages of the maximum entropy model
(Berger et al., 1996) to integrate different kinds of
features, we use this framework to handle our
problem.
Suppose the source English NE
nee, nee = {e1,e2 ... en}, consists of n English
words and the candidate Chinese NE
nec , nec = {c1,c2 ... cm}, is composed of m
Chinese characters. Suppose also that we have M
feature functions hm (nec, nee), m =1,... , M. For
each feature function, we have a model parameter
λm , m =1,... , M. The alignment probability can
be defined as follows (Och and Ney, 2002):
</bodyText>
<equation confidence="0.994871857142857">
P(nec  |ne e) = p,M (nec  |nee)
h ne ne
m (
,
c
e
) ]
</equation>
<bodyText confidence="0.97335">
The decision rule to choose the most probable
aligned target NE of the English NE is (Och and
Ney, 2002):
</bodyText>
<equation confidence="0.990771777777778">
arg max{P(nec  |nee )}
nec
 ∑= 
m 1
M
λ ( , )
m m
h ne ne
c e (3.2)
</equation>
<bodyText confidence="0.999660428571429">
In our approach, considering the characteristics
of NE translation, we adopt 4 features: translation
score, transliteration score, the source NE and
target NE’s co-occurrence score, and distortion
score for distinguishing identical NEs in the same
sentence. Next, we discuss these four features in
detail.
</bodyText>
<subsectionHeader confidence="0.992174">
3.1 Feature Functions
3.1.1 Translation Score
</subsectionHeader>
<bodyText confidence="0.99646">
It is important to consider the translation
probability between words in English NE and
characters in Chinese NE. When processing
</bodyText>
<equation confidence="0.73588575">
M
∑ ∑
exp[
ne &apos; m 1
c
λm
M
∑
exp[
λm
1
m
hm(ne�,nee
) ]
(3.1)
ˆ
nec =
=
arg max
nec
</equation>
<bodyText confidence="0.996534636363636">
Chinese sentence without segmentation, word here
refers to single Chinese character.
The translation score here is used to represent
how close an NE pair is based on translation
probabilities. Supposing the source English NE
nee consists of n English words,
nee = {e1, e2...en } and the candidate Chinese NE
nec is composed of m Chinese
characters, nec = {c1, c2...cm } , we can get the
translation score of these two bilingual NEs based
on the translation probability between ei and cj:
</bodyText>
<equation confidence="0.9870152">
m n
S nee nec
( , ) = ∑∑ p cj ei
(  |) (3.3)
j=1 i=1
</equation>
<bodyText confidence="0.999977333333333">
Given a parallel corpus aligned at the sentence
level, we can achieve the translation probability
between each English word and each Chinese
character p(cj  |ei ) via word alignments with IBM
Model 1 (Brown et al., 1993). Without word
segmentation, we have to calculate every possible
candidate to determine the most probable
alignment, which will make the search space very
large. Therefore, we conduct pruning upon the
whole search space. If there is a score jump
between two adjacent characters, the candidate will
be discarded. The scores between the candidate
Chinese NEs and the source English NE are
calculated via this formula as the value of this
feature.
</bodyText>
<subsectionHeader confidence="0.499867">
3.1.2 Transliteration Score
</subsectionHeader>
<bodyText confidence="0.994037631578948">
Although in theory, translation scores can build
up relations within correct NE alignments, in
practice this is not always the case, due to the
characteristics of the corpus. This is more obvious
when we have sparse data. For example, most of
the person names in Named Entities are sparsely
distributed in the corpus and not repeated regularly.
Besides that, some English NEs are translated via
transliteration (Lee and Chang, 2003; Al-Onaizan
and Knight, 2002; Knight and Graehl, 1997)
instead of semantic translation. Therefore, it is
fairly important to make transliteration models.
Given an English Named Entity e,
e = {e1, e2 ...en }, the procedure of transliterating e
into a Chinese Named Entity c, c = {c 1, c2 ... cm } ,
can be described with Formula (3.4) (For
simplicity of denotation, we here use e and c to
represent English NE and Chinese NE instead of
nee and nec ).
</bodyText>
<equation confidence="0.80440625">
)
c arg
= max P(c  |e)
c
</equation>
<bodyText confidence="0.800929">
According to Bayes’ Rule, it can be transformed
to:
</bodyText>
<equation confidence="0.953064333333333">
c) arg
= max P(c) * P(e  |c)
c
</equation>
<bodyText confidence="0.9999829">
Since there are more than 6k common-used
Chinese characters, we need a very large training
corpus to build the mapping directly between
English words and Chinese characters. We adopt a
romanization system, Chinese PinYin, to ease the
transformation. Each Chinese character
corresponds to a Chinese PinYin string. And the
probability from a Chinese character to PinYin
string is P(r  |c) ≈1 , except for polyphonous
characters. Thus we have:
</bodyText>
<equation confidence="0.989123">
c) = arg max ( ) * (  |) * (  |)
P c P r c P e r
c
</equation>
<bodyText confidence="0.999921466666667">
Our problem is: Given both English NE and
candidate Chinese NEs, finding the most probable
alignment, instead of finding the most probable
Chinese translation of the English NE. Therefore
unlike previous work (Lee and Chang, 2003;
Huang et al., 2003) in English-Chinese
transliteration models, we transform each
candidate Chinese NE to Chinese PinYin strings
and directly train a PinYin-based language model
with a separate English-Chinese name list
consisting of 1258 name pairs to decode the most
probable PinYin string from English NE.
To find the most probable PinYin string from
English NE, we rewrite Formula (3.5) as the
following:
</bodyText>
<equation confidence="0.995643666666667">
r) arg
= max P(r) * P(e  |r)
r
</equation>
<bodyText confidence="0.998600333333333">
where r represents the romanization (PinYin
string), r = {r 1, r2 ... rm }. For each of the factor, we
have
</bodyText>
<equation confidence="0.9854612">
m
P e r
(  |) = ∏ P(ei  |ri) (3.8)
i=1
m
P r P r P r r P r i r i r
( ) ( ) (  |) ( |
= ) (3.9)
1 2 1 ∏ − 2 i − 1
3
</equation>
<bodyText confidence="0.995855526315789">
where ei is an English syllable and ri is a
Chinese PinYin substring.
For example, we have English NE “Richard” and
its candidate Chinese NE “g-Aft”. Since both the
channel model and language model are PinYin
based, the result of Viterbi decoding is from “Ri
char d” to “Li Cha De”. We transform “g-Aft” to
the PinYin string “Li Cha De”. Then we compare
the similarity based on the PinYin string instead of
with Chinese characters directly. This is because
when transliterating English NEs into Chinese, it is
very flexible to choose which character to simulate
the pronunciation, but the PinYin string is
relatively fixed.
For every English word, there exist several ways
to partition it into syllables, so here we adopt a
dynamic programming algorithm to decode the
English word into a Chinese PinYin sequence.
Based on the transliteration string of the English
</bodyText>
<equation confidence="0.3759025">
(3.4)
(3.5)
(3.6)
(3.7)
i
=
</equation>
<bodyText confidence="0.990223444444444">
NE and the PinYin string of the original candidate
Chinese NE, we can calculate their similarity with
the XDice coefficient (Brew and McKelvie, 1996).
This is a variant of Dice coefficient which allows
“extended bigrams”. An extended bigram (xbig) is
formed by deleting the middle letter from any
three-letter substring of the word in addition to the
original bigrams.
Suppose the transliteration string of the English
NE and the PinYin string of the candidate Chinese
NE are etl and cpy , respectively. The XDice
coefficient is calculated via the following formula:
XDice ( etl , cpy ) =
English strings are exactly Chinese person
strings. To deal with the two situations, let
denote the surface English string, the final
transliteration score is defin
names’PinYin
</bodyText>
<equation confidence="0.937248473684211">
esur
ed by taking the
maximum value of the two XDice coefficients:
Tl c e =
( , )
py etl XDice cpy esur
, ), ( ,
count(*,nee
count ne ne
(
c
c|
=
∑
)
)
P o (ne
nee)
e (3.12)
</equation>
<bodyText confidence="0.98809">
where
nee) is the number of times
and nee appear together and count(*,nee )
is the number of times that nee appears. This
probability is a good indication for determining
bilingual NE alignment.
</bodyText>
<subsectionHeader confidence="0.693691">
3.1.4 Distortion Score
</subsectionHeader>
<bodyText confidence="0.938908022727272">
When translating NEs across languages, we
notice that the difference of their positions is also a
good indication for determining their relation, and
this is a must when there are identical candidates in
the target language. The bigger the difference is,
the less probable they can be translations of each
other. Therefore, we define the distortion score
between the source English NE and the candidate
Chinese NE as another feature.
Suppose the index of the start position of the
English NE is i, an
count(nec,
nec
d the length of the English
sentence is m. We then have the relative position of
i
pose = , and the
Another point to note is that foreign person
names and Chinese person names have different
translation strategies. The transliteration
framework above is only applied on foreign names.
For Chinese person name translation, the surface
(3.11)
This formula does not differentiate foreign
person names and Chinese person names, and
foreign person
transliteration strings or
Chinese person
strings can be
handled appropriately. Besides this, since the
English string and the
string share the same
character set, our approach can also work as an
alternative if the transliteration decoding fails.
For example, for the English name
the
alignment to a Chinese NE should be
If
the transliteration decoding fails, its
string,
still has a very strong relation with the
surface string
via the XDice coefficient.
This can make the system more powerful.
</bodyText>
<subsubsectionHeader confidence="0.778528">
3.1.3 Co-occurrence Score
</subsubsectionHeader>
<bodyText confidence="0.9533625">
Another approach is to find the co-occurrences
of source and target NEs in the whole corpus. If
both NEs co-occur very often, there exists a big
chance that they align to each other. The
knowledge acquired from the whole corpus is an
extra and valuable feature for NE alignment. We
calculate the co-occurrence score of the source
English NE and the can
</bodyText>
<figure confidence="0.9454206">
names’
names’PinYin
PinYin
“Cuba”,
“古巴”.
PinYin
“Guba”,
“Cuba”
didate Chinese NE with the
following formula:
m
candidate Chinese
relative
position
0 &lt;_ pose,
</figure>
<figureCaption confidence="0.891875428571428">
&lt;_ 1. The distortion
score is defined with the following formula:
(3.13)
where ABS means the absolute value. If there
are multiple identical candidate Chinese NEs at
different positions in the target language, the one
with the largest distortion score will win.
</figureCaption>
<subsectionHeader confidence="0.999308">
3.2 Bootstrapping with the MaxEnt Model
</subsectionHeader>
<bodyText confidence="0.9993855">
To apply the maximum entropy model for NE
alignment, we process in two steps: selecting the
NE candidates and training the maximum entropy
model parameters.
</bodyText>
<subsubsectionHeader confidence="0.575066">
3.2.1 NE Candidate Selection
</subsubsectionHeader>
<bodyText confidence="0.994543428571429">
To get an NE alignment with our maximum
entropy model, we first use NLPWIN (Heidorn,
2000) to identify Named Entities in English. For
each word in the recognized NE, we find all the
possible translation characters in Chinese through
the translation table acquired from IBM Model 1.
Finally, we have all the selected characters as the
</bodyText>
<figureCaption confidence="0.795296">
data. With an open-ended window for each
seed, all the possible sequences located within the
window are considered as possible candidates for
NE alignment. Their lengths range from 1 to the
empirically determined length of the window.
During the can
</figureCaption>
<equation confidence="0.949880166666667">
NE’s
posc ,
posc
Dist ne c ne e
( , )1 = −ABS pose posc
( − )
</equation>
<bodyText confidence="0.893734">
“seed”
didate selection, the pruning
strategy discussed above is applied to reduce the
search space.
</bodyText>
<figure confidence="0.980820375">
2x xbigs ( etl ) n xbigs(cpy) (3.10)
xbigs
(et,) (cPy )
+ xbigs
era
max(XDice(c
))
the source English NE
</figure>
<bodyText confidence="0.998752857142857">
For example, in Figure 1, if “China” only has a
translation probability over the threshold value
with “中”, the two seed data are located with the
index of 0 and 4. Supposing the length of the
window to be 3, all the candidates around the seed
data including “中国”, with the length ranging
from 1 to 3, are selected.
</bodyText>
<figureCaption confidence="0.998772">
Figure 1. Example of Seed Data
</figureCaption>
<subsectionHeader confidence="0.344512">
3.2.2 MaxEnt Parameter Training
</subsectionHeader>
<bodyText confidence="0.999900538461539">
With the four feature functions defined in
Section 3.1, for each identified NE in English, we
calculate the feature scores of all the selected
Chinese NE candidates.
To achieve the most probable aligned Chinese
NE, we use the published package YASMET2 to
conduct parameter training and re-ranking of all
the NE candidates. YASMET requires supervised
learning for the training of the maximum entropy
model. However, it is not easy to acquire a large
annotated training set. Here bootstrapping is used
to help the process. Figure 2 gives the whole
procedure for parameter training.
</bodyText>
<listItem confidence="0.9926744">
1. Set the coefficients λ, as uniform
distribution;
2. Calculate all the feature scores to get the
N-best list of the Chinese NE candidates;
3. Candidates with their values over a given
threshold are considered to be correct and
put into the re-ranking training set;
4. Retrain the parameters λ, with YASMET;
5. Repeat from Step 2 until λ, converge, and
take the current ranking as the final result.
</listItem>
<figureCaption confidence="0.997461">
Figure 2. Parameter Training
</figureCaption>
<sectionHeader confidence="0.985888" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.661808">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999833">
We perform experiments to investigate the
performance of the above framework. We take the
LDC Xinhua News with aligned English-Chinese
sentence pairs as our corpus.
The incremental testing strategy is to investigate
the system’s performance as more and more data
are added into the data set. Initially, we take 300
</bodyText>
<footnote confidence="0.734919">
2 http://www.isi.edu/~och/YASMET.html
</footnote>
<bodyText confidence="0.999589857142857">
sentences as the standard testing set, and we
repeatedly add 5k more sentences into the data set
and process the new data. After iterative re-ranking,
the performance of alignment models over the 300
sentence pairs is calculated. The learning curves
are drawn from 5k through 30k sentences with the
step as 5k every time.
</bodyText>
<subsectionHeader confidence="0.998337">
4.2 Baseline System
</subsectionHeader>
<bodyText confidence="0.999882047619048">
A translated Chinese NE may appear at a
different position from the corresponding English
NE in the sentence. IBM Model 4 (Brown et al.,
1993) integrates a distortion probability, which is
complete enough to account for this tendency. The
HMM model (Vogel et al., 1996) conducts word
alignment with a strong tendency to preserve
localization from one language to another.
Therefore we extract NE alignments based on the
results of these two models as our baseline systems.
For the alignments of IBM Model 4 and HMM, we
use the published software package, GIZA++ 3
(Och and Ney, 2003) for processing.
Some recent research has proposed to extract
phrase translations based on the results from IBM
Model (Koehn et al., 2003). We extract English-
Chinese NE alignments based on the results from
IBM Model 4 and HMM. The extraction strategy
takes each of the continuous aligned segments as
one possible candidate, and finally the one with the
highest frequency in the whole corpus wins.
</bodyText>
<figureCaption confidence="0.988491">
Figure 3. Example of Extraction Strategy
</figureCaption>
<bodyText confidence="0.991694">
Figure 3 gives an example of the extraction
strategy. “China” here is aligned to either “中国”
or “中”. Finally the one with a higher frequency in
the whole corpus, say, “中国”, will be viewed as
the final alignment for “China”.
</bodyText>
<subsectionHeader confidence="0.991919">
4.3 Results Analysis
</subsectionHeader>
<bodyText confidence="0.995455142857143">
Our approach first uses NLPWIN to conduct
NER. Suppose S’ is the set of identified NE with
NLPWIN. S is the alignment set we compute with
our models based on S’, and T is the set consisting
of all the true alignments based on S’. We define
the evaluation metrics of precision, recall, and F-
score as follows:
</bodyText>
<figure confidence="0.64345794117647">
3 http://www.isi.edu/~och/GIZA++.html
[China] hopes to further economic ... [EU].
中 国 希 望 中 欧 经 贸 关 系...
[China] hopes to further economic ... [EU].
中 国 希 望 中 欧 经 贸 关 系...
Aligned Candidates: China 4中国
China 4中
2 × precision recall
×
F score
− = (4.3)
precision +recall
precision = S I T (4.1)
recall =
SIT (4.2)
T
Section
</figure>
<figureCaption confidence="0.701747333333333">
we perform all the experiments on
data without word segmentation and get the
performance for NE alignment with IBM Model 4,
the HMM model, and the maximum entropy model.
Figure 4, 5, and 6 give the learning curves for
precision, recall, an
</figureCaption>
<figure confidence="0.899854916666667">
4.1,
d F-score, respectively, with
these experiments.
Precision Without Word Segmentation
0
5k 10k 15k 20k 25k 30k data size
ing Curve with Precision
1 Recall Without Word Segmentation
0.8
0.2
0
5k 10k 15k 20k 25k 30k data size
</figure>
<figureCaption confidence="0.999729">
Figure 5. Learning Curve with Recall
</figureCaption>
<table confidence="0.976222375">
F-score Without Word Segmentation
om published LDC Xinhua News corpus.
precision recall F-score
MaxEnt 0.56705 0.734491 0.64
(Seg)
MaxEnt precision and for recall. NE alignment with the From these curves, we see that HMM generally 0.717838
(Unseg) 0.636015 works a little better than IBM Model 4, both for
0.823821
HMM maximum entropy model greatly outperforms IBM Score. Since with this framework, we first use 0.320856
(Seg) Model 4 and HMM in precision, recall, and F- 0.372208
NLPWIN to recognize NEs in English, we have
NE identification error. The precision of NLPWIN
0.281955
HMM on our task is about 77%. Taking this into account, we know our precision score has actually been 0.360531
(Unseg) reduced by this rate. In Figure 4, this causes the 0.471464
upper bound of precision to be 77%.
0.291859
IBM 4 Segmentation 4.3.2 Comparison with Results with Word 0.253219
(Seg) propagations from word segmentation and To justify that our approach of NE alignment
0.223062 without word segmentation really reduces the error
0.292804
IBM 4 thereafter NER, we also perform all the experiments upon the data set with word 0.30695
(Unseg) 0.251185 segmentation. The segmented data is directly taken
0.394541
</table>
<figure confidence="0.967100827586207">
IBM Model
HMM
MaxEnt
Upper Bound
IBM Model
HMM
MaxEnt
1
0.8
precision
0.6
0.4
0.2
recall
0.6
0.4
F-score 0.8 IBM Model
0.7 HMM
0.6 MaxEnt
0.5
0.4
0.3
0.2
0
5k 10k 15k 20k 25k 30k data size
ing Curve with F-score
S
4.3.1 Results without Word Segmentation
Based on the testing strategies discussed in
</figure>
<figureCaption confidence="0.995283">
Figure 4. Learn
</figureCaption>
<page confidence="0.755362">
0.1
</page>
<figureCaption confidence="0.964624">
Figure 6. Learn
</figureCaption>
<bodyText confidence="0.45806">
fr
</bodyText>
<tableCaption confidence="0.993903">
Table 1. Results Comparison
</tableCaption>
<bodyText confidence="0.891423578947369">
Table 1 gives the comparison of precision, recall,
and F-score for the experiments with word
segmentation and without word segmentation
when the size of the data set is 30k sentences.
For HMM and IBM Model 4, performance
without word segmentation is always better than
with word segmentation. For maximum entropy
model, the scores without word segmentation are
always 6 to 9 percent better than those with word
segmentation. This owes to the reduction of error
propagation from word segmentation and NER.
For example, in the following sentence pair with
word segmentation, the English NE
can no longer be correctly aligned to
“UnitedStates”
“美国”. Since in the Chinese sentence, the incorrect
segmentation takes “访问美国” as one unit. But if
we conduct alignment without word segmentation,
“美国” can be correctly aligned.
</bodyText>
<listItem confidence="0.998786666666667">
• Greek Prime Minister Costas Simitis visits
[United States] .
• 希腊 总理 希米 蒂 斯 访问美国 .
</listItem>
<bodyText confidence="0.99972075">
Similar situations exist when HMM and IBM
Model 4 are used for NE alignment. When
compared with IBM Model 4 and HMM with word
segmentation, our approach with word
segmentation also has a much better performance
than them. This demonstrates that in any case our
approach outperforms IBM Model 4 and HMM
significantly.
</bodyText>
<sectionHeader confidence="0.76082" genericHeader="method">
4.3.3 Discussion
</sectionHeader>
<bodyText confidence="0.999960242424242">
Huang et al.’s (2003) approach investigated
transliteration cost and translation cost, based on
IBM Model 1, and NE tagging cost by an NE
identifier. In our approach, we do not have an NE
tagging cost. We use a different type of translation
and transliteration score, and add a distortion score
that is important to distinguish identical NEs in the
same sentence.
Experimental results prove that in our approach
the selected features that characterize NE
translations from English to Chinese help much for
NE alignment. The co-occurrence score uses the
knowledge from the whole corpus to help NE
alignment. And the transliteration score addresses
the problem of data sparseness. For example,
English person name “Mostafizur Rahman” only
appears once in the data set. But with the
transliteration score, we get it aligned to the
Chinese NE “穆斯塔菲兹拉赫曼” correctly.
Since in ME training we use iterative
bootstrapping to help supervised learning, the
training data is not completely clean and brings
some errors into the final results. But it avoids the
acquisition of large annotated training set and the
performance is still much better than traditional
alignment models. The performance is also
impaired by the English NER tool. Another
possible reason for alignment errors is the
inconsistency of NE translation in English and
Chinese. For example, usually only the last name
of foreigners is translated into Chinese and the first
name is ignored. This brings some trouble for the
alignment of person names.
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999907714285714">
Traditional word alignment approaches cannot
come up with satisfactory results for Named Entity
alignment. In this paper, we propose a novel
approach using a maximum entropy model for NE
alignment. To ease the training of the MaxEnt
model, bootstrapping is used to help supervised
learning. Unlike previous work reported in the
literature, our work conducts bilingual Named
Entity alignment without word segmentation for
Chinese, and its performance is much better than
with word segmentation. When compared with
IBM and HMM alignment models, experimental
results show that our approach outperforms IBM
Model 4 and HMM significantly.
Due to the inconsistency of NE translation, some
NE pairs can not be aligned correctly. We may
need some manually-generated rules to fix this. We
also notice that NER performance over the source
language can be improved using bilingual
knowledge. These problems will be investigated in
the future.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999484428571429">
Thanks to Hang Li, Changning Huang, Yunbo
Cao, and John Chen for their valuable comments
on this work. Also thank Kevin Knight for his
checking of the English of this paper. Special
thanks go to Eduard Hovy for his continuous
support and encouragement while the first author
was visiting MSRA.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833061728395">
Al-Onaizan, Y. and Knight, K. 2002. Translating
Named Entities Using Monolingual and
Bilingual Resources. ACL 2002, pp. 400-408.
Philadelphia.
Berger, A. L.; Della Pietra, S. A.; and Della Pietra,
V. J. 1996. A Maximum Entropy Approach to
Natural Language Processing. Computational
Linguistics, vol. 22, no. 1, pp. 39-68.
Brew, C. and McKelvie, D. 1996. Word-pair
extraction for lexicography. The 2nd
International Conference on New Methods in
Language Processing, pp. 45–55. Ankara.
Brown, P. F.; Della Pietra, S. A.; Della Pietra, V. J.
;and Mercer, R. L. 1993. The Mathematics of
Statistical Machine Translation: Parameter
Estimation. Computational Linguistics,
19(2):263-311.
Cherry, C. and Lin, D. 2003. A Probability Model
to Improve Word Alignment. ACL 2003.
Sapporo, Japan.
Darroch, J. N. and Ratcliff, D. 1972. Generalized
Iterative Scaling for Log-linear Models. Annals
of Mathematical Statistics, 43:1470-1480.
Heidorn, G. 2000. Intelligent Writing Assistant. A
Handbook of Natural Language Processing:
Techniques and Applications for the Processing
of Language as Text. Marcel Dekker.
Hobbs, J. et al. 1996. FASTUS: A Cascaded Finite-
State Transducer for Extracting Information
from Natural Language Text, MIT Press.
Cambridge, MA.
Huang, F.; Vogel, S. and Waibel, A. 2003.
Automatic Extraction of Named Entity
Translingual Equivalence Based on Multi-
Feature Cost Minimization. ACL 2003 Workshop
on Multilingual and Mixed-language NER.
Sapporo, Japan.
Huang, J. and Choi, K. 2000. Chinese-Korean
Word Alignment Based on Linguistic
Comparison. ACL-2000. Hongkong.
Ker, S. J. and Chang, J. S. 1997. A Class-based
Approach to Word Alignment. Computational
Linguistics, 23(2):313-343.
Knight, K. and Graehl, J. 1997. Machine
Transliteration. ACL 1997, pp. 128-135.
Koehn, P.; Och, F. J. and Marcu, D. 2003.
Statistical Phrase-Based Translation.
HLT/NAACL 2003. Edmonton, Canada.
Lee, C. and Chang, J. S. 2003. Acquisition of
English-Chinese Transliterated Word Pairs from
Parallel-Aligned Texts, HLT-NAACL 2003
Workshop on Data Driven MT, pp. 96-103.
Marcu, D. 2001. Towards a Unified Approach to
Memory- and Statistical-Based Machine
Translation. ACL 2001, pp. 378-385. Toulouse,
France.
Melamed, I. D. 2000. Models of Translation
Equivalence among Words. Computational
Linguistics, 26(2): 221-249.
Moore, R. C. 2003. Learning Translations of
Named-Entity Phrases from Parallel Corpora.
EACL-2003. Budapest, Hungary.
Och, F. J. and Ney, H. 2003. A Systematic
Comparison of Various Statistical Alignment
Models, Computational Linguistics, volume 29,
number 1, pp. 19-51.
Och, F. J. and Ney, H. 2002. Discriminative
Training and Maximum Entropy Models for
Statistical Machine Translation. ACL 2002, pp.
295-302.
Och, F. J. and Ney, H. 2000. Improved Statistical
Alignment Models. ACL 2000, pp: 440-447.
Probst, K. and Brown, R. 2002. Using Similarity
Scoring to Improve the Bilingual Dictionary for
Word Alignment. ACL-2002, pp: 409-416.
Vogel, S.; Ney, H. and Tillmann, C. 1996. HMM-
Based Word Alignment in Statistical Translation.
COLING’96, pp. 836-841.
Wang, W.; Zhou, M.; Huang, J. and Huang, C.
2002. Structural Alignment using Bilingual
Chunking. COLING-2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316002">
<title confidence="0.987629">A New Approach for English-Chinese Named Entity Alignment</title>
<affiliation confidence="0.9117115">Information Sciences University of Southern</affiliation>
<address confidence="0.8574915">4676 Admiralty Way, Suite Marina Del Rey, CA, U.S.A,</address>
<email confidence="0.999647">donghui@isi.edu</email>
<author confidence="0.915431">Ming</author>
<affiliation confidence="0.93338">Research</affiliation>
<address confidence="0.8322905">5F Sigma Center, No.49 Zhichun Road, Beijing, China,</address>
<email confidence="0.99967">t-yjlv@microsoft.com</email>
<email confidence="0.99967">mingzhou@microsoft.com</email>
<abstract confidence="0.993297823529412">Traditional word alignment approaches cannot come up with satisfactory results for Named Entities. In this paper, we propose a novel approach using a maximum entropy model for named entity alignment. To ease the training of the maximum entropy model, bootstrapping is used to help supervised learning. Unlike previous work reported in the literature, our work conducts bilingual Named Entity alignment without word segmentation for Chinese and its performance is much better than that with word segmentation. When compared with IBM and HMM alignment models, experimental results show that our approach outperforms IBM Model 4 and HMM significantly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating Named Entities Using Monolingual and Bilingual Resources. ACL</title>
<date>2002</date>
<pages>400--408</pages>
<location>Philadelphia.</location>
<contexts>
<context position="10219" citStr="Al-Onaizan and Knight, 2002" startWordPosition="1666" endWordPosition="1669">rded. The scores between the candidate Chinese NEs and the source English NE are calculated via this formula as the value of this feature. 3.1.2 Transliteration Score Although in theory, translation scores can build up relations within correct NE alignments, in practice this is not always the case, due to the characteristics of the corpus. This is more obvious when we have sparse data. For example, most of the person names in Named Entities are sparsely distributed in the corpus and not repeated regularly. Besides that, some English NEs are translated via transliteration (Lee and Chang, 2003; Al-Onaizan and Knight, 2002; Knight and Graehl, 1997) instead of semantic translation. Therefore, it is fairly important to make transliteration models. Given an English Named Entity e, e = {e1, e2 ...en }, the procedure of transliterating e into a Chinese Named Entity c, c = {c 1, c2 ... cm } , can be described with Formula (3.4) (For simplicity of denotation, we here use e and c to represent English NE and Chinese NE instead of nee and nec ). ) c arg = max P(c |e) c According to Bayes’ Rule, it can be transformed to: c) arg = max P(c) * P(e |c) c Since there are more than 6k common-used Chinese characters, we need a v</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002. Translating Named Entities Using Monolingual and Bilingual Resources. ACL 2002, pp. 400-408. Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<pages>39--68</pages>
<contexts>
<context position="7243" citStr="Berger et al., 1996" startWordPosition="1133" endWordPosition="1136">nce we only have NEs identified on the source side, and there is no extra knowledge from the target side. Considering the inherent characteristics of NE translation, we can find several features that can help NE alignment; therefore, we use a maximum entropy model to integrate these features and carry out NE alignment. 3 NE Alignment with a Maximum Entropy Model Without relying on syntactic knowledge from either the English side or the Chinese side, we find there are several valuable features that can be used for Named Entity alignment. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to handle our problem. Suppose the source English NE nee, nee = {e1,e2 ... en}, consists of n English words and the candidate Chinese NE nec , nec = {c1,c2 ... cm}, is composed of m Chinese characters. Suppose also that we have M feature functions hm (nec, nee), m =1,... , M. For each feature function, we have a model parameter λm , m =1,... , M. The alignment probability can be defined as follows (Och and Ney, 2002): P(nec |ne e) = p,M (nec |nee) h ne ne m ( , c e ) ] The decision rule to choose the most probable aligned target </context>
</contexts>
<marker>Berger, Pietra, A, Pietra, J, 1996</marker>
<rawString>Berger, A. L.; Della Pietra, S. A.; and Della Pietra, V. J. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, vol. 22, no. 1, pp. 39-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
<author>D McKelvie</author>
</authors>
<title>Word-pair extraction for lexicography.</title>
<date>1996</date>
<booktitle>The 2nd International Conference on New Methods in Language Processing,</booktitle>
<pages>45--55</pages>
<location>Ankara.</location>
<contexts>
<context position="13166" citStr="Brew and McKelvie, 1996" startWordPosition="2211" endWordPosition="2214">se characters directly. This is because when transliterating English NEs into Chinese, it is very flexible to choose which character to simulate the pronunciation, but the PinYin string is relatively fixed. For every English word, there exist several ways to partition it into syllables, so here we adopt a dynamic programming algorithm to decode the English word into a Chinese PinYin sequence. Based on the transliteration string of the English (3.4) (3.5) (3.6) (3.7) i = NE and the PinYin string of the original candidate Chinese NE, we can calculate their similarity with the XDice coefficient (Brew and McKelvie, 1996). This is a variant of Dice coefficient which allows “extended bigrams”. An extended bigram (xbig) is formed by deleting the middle letter from any three-letter substring of the word in addition to the original bigrams. Suppose the transliteration string of the English NE and the PinYin string of the candidate Chinese NE are etl and cpy , respectively. The XDice coefficient is calculated via the following formula: XDice ( etl , cpy ) = English strings are exactly Chinese person strings. To deal with the two situations, let denote the surface English string, the final transliteration score is d</context>
</contexts>
<marker>Brew, McKelvie, 1996</marker>
<rawString>Brew, C. and McKelvie, D. 1996. Word-pair extraction for lexicography. The 2nd International Conference on New Methods in Language Processing, pp. 45–55. Ankara.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="3719" citStr="Brown et al., 1993" startWordPosition="571" endWordPosition="574">ease the training of the maximum entropy model, bootstrapping is used to help supervised learning. On the other hand, to avoid error propagations from word segmentation and NER, we directly extract Chinese NEs and make the alignment from plain text without word segmentation. It is unlike previous work reported in the literature. Although this makes the task more difficult, it greatly reduces the chance of errors introduced by previous steps and therefore produces much better performance on our task. To justify our approach, we adopt traditional alignment approaches, in particular IBM Model 4 (Brown et al., 1993) and HMM (Vogel et al., 1996), to carry out NE alignment as our baseline systems. Experimental results show that in this task our approach outperforms IBM Model 4 and HMM significantly. Furthermore, the performance 1 We only discuss NEs of three categories: Person Name (PN), Location Name (LN), and Organization Name (ON). without word segmentation is much better than that with word segmentation. The rest of this paper is organized as follows: In section 2, we discuss related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature func</context>
<context position="5091" citStr="Brown et al., 1993" startWordPosition="791" endWordPosition="794">Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment pos</context>
<context position="9286" citStr="Brown et al., 1993" startWordPosition="1518" endWordPosition="1521">se an NE pair is based on translation probabilities. Supposing the source English NE nee consists of n English words, nee = {e1, e2...en } and the candidate Chinese NE nec is composed of m Chinese characters, nec = {c1, c2...cm } , we can get the translation score of these two bilingual NEs based on the translation probability between ei and cj: m n S nee nec ( , ) = ∑∑ p cj ei ( |) (3.3) j=1 i=1 Given a parallel corpus aligned at the sentence level, we can achieve the translation probability between each English word and each Chinese character p(cj |ei ) via word alignments with IBM Model 1 (Brown et al., 1993). Without word segmentation, we have to calculate every possible candidate to determine the most probable alignment, which will make the search space very large. Therefore, we conduct pruning upon the whole search space. If there is a score jump between two adjacent characters, the candidate will be discarded. The scores between the candidate Chinese NEs and the source English NE are calculated via this formula as the value of this feature. 3.1.2 Transliteration Score Although in theory, translation scores can build up relations within correct NE alignments, in practice this is not always the </context>
<context position="19779" citStr="Brown et al., 1993" startWordPosition="3320" endWordPosition="3323">e system’s performance as more and more data are added into the data set. Initially, we take 300 2 http://www.isi.edu/~och/YASMET.html sentences as the standard testing set, and we repeatedly add 5k more sentences into the data set and process the new data. After iterative re-ranking, the performance of alignment models over the 300 sentence pairs is calculated. The learning curves are drawn from 5k through 30k sentences with the step as 5k every time. 4.2 Baseline System A translated Chinese NE may appear at a different position from the corresponding English NE in the sentence. IBM Model 4 (Brown et al., 1993) integrates a distortion probability, which is complete enough to account for this tendency. The HMM model (Vogel et al., 1996) conducts word alignment with a strong tendency to preserve localization from one language to another. Therefore we extract NE alignments based on the results of these two models as our baseline systems. For the alignments of IBM Model 4 and HMM, we use the published software package, GIZA++ 3 (Och and Ney, 2003) for processing. Some recent research has proposed to extract phrase translations based on the results from IBM Model (Koehn et al., 2003). We extract EnglishC</context>
</contexts>
<marker>Brown, Pietra, A, Pietra, J, 1993</marker>
<rawString>Brown, P. F.; Della Pietra, S. A.; Della Pietra, V. J. ;and Mercer, R. L. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>D Lin</author>
</authors>
<title>A Probability Model to Improve Word Alignment. ACL</title>
<date>2003</date>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4787" citStr="Cherry and Lin, 2003" startWordPosition="740" endWordPosition="743">n section 2, we discuss related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the s</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Cherry, C. and Lin, D. 2003. A Probability Model to Improve Word Alignment. ACL 2003. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-linear Models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch, J. N. and Ratcliff, D. 1972. Generalized Iterative Scaling for Log-linear Models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Intelligent Writing Assistant. A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text.</title>
<date>2000</date>
<publisher>Marcel Dekker.</publisher>
<contexts>
<context position="16747" citStr="Heidorn, 2000" startWordPosition="2812" endWordPosition="2813">hinese relative position 0 &lt;_ pose, &lt;_ 1. The distortion score is defined with the following formula: (3.13) where ABS means the absolute value. If there are multiple identical candidate Chinese NEs at different positions in the target language, the one with the largest distortion score will win. 3.2 Bootstrapping with the MaxEnt Model To apply the maximum entropy model for NE alignment, we process in two steps: selecting the NE candidates and training the maximum entropy model parameters. 3.2.1 NE Candidate Selection To get an NE alignment with our maximum entropy model, we first use NLPWIN (Heidorn, 2000) to identify Named Entities in English. For each word in the recognized NE, we find all the possible translation characters in Chinese through the translation table acquired from IBM Model 1. Finally, we have all the selected characters as the data. With an open-ended window for each seed, all the possible sequences located within the window are considered as possible candidates for NE alignment. Their lengths range from 1 to the empirically determined length of the window. During the can NE’s posc , posc Dist ne c ne e ( , )1 = −ABS pose posc ( − ) “seed” didate selection, the pruning strateg</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>Heidorn, G. 2000. Intelligent Writing Assistant. A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. Marcel Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>FASTUS: A Cascaded FiniteState Transducer for Extracting Information from Natural Language Text,</title>
<date>1996</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<marker>Hobbs, 1996</marker>
<rawString>Hobbs, J. et al. 1996. FASTUS: A Cascaded FiniteState Transducer for Extracting Information from Natural Language Text, MIT Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Automatic Extraction of Named Entity Translingual Equivalence Based on MultiFeature Cost Minimization.</title>
<date>2003</date>
<booktitle>ACL 2003 Workshop on Multilingual and Mixed-language NER.</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2399" citStr="Huang et al., 2003" startWordPosition="362" endWordPosition="365">LIR). A Named Entity alignment, however, is not easy to obtain. It requires both Named Entity Recognition (NER) and alignment be handled correctly. NEs may not be well recognized, or only * The work was done while the first author was visiting Microsoft Research Asia. parts of them may be recognized during NER. When aligning bilingual NEs in different languages, we need to handle many-to-many alignments. And the inconsistency of NE translation and NER in different languages is also a big problem. Specifically, in Chinese NE processing, since Chinese is not a tokenized language, previous work (Huang et al., 2003) normally conducts word segmentation and identifies Named Entities in turn. This involves several problems for Chinese NEs, such as word segmentation error, the identification of Chinese NE boundaries, and the mis-tagging of Chinese NEs. For example, “国防部长” in Chinese is really one unit and should not be segmented as [ON 国防 部]/长. The errors from word segmentation and NER will propagate into NE alignment. In this paper, we propose a novel approach using a maximum entropy model to carry out EnglishChinese Named Entity1 alignment. NEs in English are first recognized by NER tools. We then investig</context>
<context position="5801" citStr="Huang et al. (2003)" startWordPosition="904" endWordPosition="907">y-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of cost models. However, this approach greatly relies on linguistic information, such as a string repeated on both sides, and clues from capital letters that are not suitable for language pairs not belonging to the same family. Also, there are already complete lexical compounds identified on the target side, which represent a big part </context>
<context position="11486" citStr="Huang et al., 2003" startWordPosition="1900" endWordPosition="1903">irectly between English words and Chinese characters. We adopt a romanization system, Chinese PinYin, to ease the transformation. Each Chinese character corresponds to a Chinese PinYin string. And the probability from a Chinese character to PinYin string is P(r |c) ≈1 , except for polyphonous characters. Thus we have: c) = arg max ( ) * ( |) * ( |) P c P r c P e r c Our problem is: Given both English NE and candidate Chinese NEs, finding the most probable alignment, instead of finding the most probable Chinese translation of the English NE. Therefore unlike previous work (Lee and Chang, 2003; Huang et al., 2003) in English-Chinese transliteration models, we transform each candidate Chinese NE to Chinese PinYin strings and directly train a PinYin-based language model with a separate English-Chinese name list consisting of 1258 name pairs to decode the most probable PinYin string from English NE. To find the most probable PinYin string from English NE, we rewrite Formula (3.5) as the following: r) arg = max P(r) * P(e |r) r where r represents the romanization (PinYin string), r = {r 1, r2 ... rm }. For each of the factor, we have m P e r ( |) = ∏ P(ei |ri) (3.8) i=1 m P r P r P r r P r i r i r ( ) ( ) </context>
</contexts>
<marker>Huang, Vogel, Waibel, 2003</marker>
<rawString>Huang, F.; Vogel, S. and Waibel, A. 2003. Automatic Extraction of Named Entity Translingual Equivalence Based on MultiFeature Cost Minimization. ACL 2003 Workshop on Multilingual and Mixed-language NER. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>K Choi</author>
</authors>
<title>Chinese-Korean Word Alignment Based on Linguistic Comparison.</title>
<date>2000</date>
<tech>ACL-2000. Hongkong.</tech>
<contexts>
<context position="4931" citStr="Huang and Choi, 2000" startWordPosition="765" endWordPosition="768">ure functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word</context>
</contexts>
<marker>Huang, Choi, 2000</marker>
<rawString>Huang, J. and Choi, K. 2000. Chinese-Korean Word Alignment Based on Linguistic Comparison. ACL-2000. Hongkong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Ker</author>
<author>J S Chang</author>
</authors>
<title>A Class-based Approach to Word Alignment.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--2</pages>
<contexts>
<context position="4953" citStr="Ker and Chang, 1997" startWordPosition="769" endWordPosition="772">strapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs</context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Ker, S. J. and Chang, J. S. 1997. A Class-based Approach to Word Alignment. Computational Linguistics, 23(2):313-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine Transliteration. ACL</booktitle>
<pages>128--135</pages>
<contexts>
<context position="10245" citStr="Knight and Graehl, 1997" startWordPosition="1670" endWordPosition="1673">candidate Chinese NEs and the source English NE are calculated via this formula as the value of this feature. 3.1.2 Transliteration Score Although in theory, translation scores can build up relations within correct NE alignments, in practice this is not always the case, due to the characteristics of the corpus. This is more obvious when we have sparse data. For example, most of the person names in Named Entities are sparsely distributed in the corpus and not repeated regularly. Besides that, some English NEs are translated via transliteration (Lee and Chang, 2003; Al-Onaizan and Knight, 2002; Knight and Graehl, 1997) instead of semantic translation. Therefore, it is fairly important to make transliteration models. Given an English Named Entity e, e = {e1, e2 ...en }, the procedure of transliterating e into a Chinese Named Entity c, c = {c 1, c2 ... cm } , can be described with Formula (3.4) (For simplicity of denotation, we here use e and c to represent English NE and Chinese NE instead of nee and nec ). ) c arg = max P(c |e) c According to Bayes’ Rule, it can be transformed to: c) arg = max P(c) * P(e |c) c Since there are more than 6k common-used Chinese characters, we need a very large training corpus </context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Knight, K. and Graehl, J. 1997. Machine Transliteration. ACL 1997, pp. 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation. HLT/NAACL</title>
<date>2003</date>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5468" citStr="Koehn et al., 2003" startWordPosition="856" endWordPosition="859"> Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of c</context>
<context position="20358" citStr="Koehn et al., 2003" startWordPosition="3415" endWordPosition="3418">ntence. IBM Model 4 (Brown et al., 1993) integrates a distortion probability, which is complete enough to account for this tendency. The HMM model (Vogel et al., 1996) conducts word alignment with a strong tendency to preserve localization from one language to another. Therefore we extract NE alignments based on the results of these two models as our baseline systems. For the alignments of IBM Model 4 and HMM, we use the published software package, GIZA++ 3 (Och and Ney, 2003) for processing. Some recent research has proposed to extract phrase translations based on the results from IBM Model (Koehn et al., 2003). We extract EnglishChinese NE alignments based on the results from IBM Model 4 and HMM. The extraction strategy takes each of the continuous aligned segments as one possible candidate, and finally the one with the highest frequency in the whole corpus wins. Figure 3. Example of Extraction Strategy Figure 3 gives an example of the extraction strategy. “China” here is aligned to either “中国” or “中”. Finally the one with a higher frequency in the whole corpus, say, “中国”, will be viewed as the final alignment for “China”. 4.3 Results Analysis Our approach first uses NLPWIN to conduct NER. Suppose </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P.; Och, F. J. and Marcu, D. 2003. Statistical Phrase-Based Translation. HLT/NAACL 2003. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lee</author>
<author>J S Chang</author>
</authors>
<title>Acquisition of English-Chinese Transliterated Word Pairs from Parallel-Aligned Texts,</title>
<date>2003</date>
<booktitle>HLT-NAACL 2003 Workshop on Data Driven MT,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="10190" citStr="Lee and Chang, 2003" startWordPosition="1662" endWordPosition="1665">ndidate will be discarded. The scores between the candidate Chinese NEs and the source English NE are calculated via this formula as the value of this feature. 3.1.2 Transliteration Score Although in theory, translation scores can build up relations within correct NE alignments, in practice this is not always the case, due to the characteristics of the corpus. This is more obvious when we have sparse data. For example, most of the person names in Named Entities are sparsely distributed in the corpus and not repeated regularly. Besides that, some English NEs are translated via transliteration (Lee and Chang, 2003; Al-Onaizan and Knight, 2002; Knight and Graehl, 1997) instead of semantic translation. Therefore, it is fairly important to make transliteration models. Given an English Named Entity e, e = {e1, e2 ...en }, the procedure of transliterating e into a Chinese Named Entity c, c = {c 1, c2 ... cm } , can be described with Formula (3.4) (For simplicity of denotation, we here use e and c to represent English NE and Chinese NE instead of nee and nec ). ) c arg = max P(c |e) c According to Bayes’ Rule, it can be transformed to: c) arg = max P(c) * P(e |c) c Since there are more than 6k common-used Ch</context>
<context position="11465" citStr="Lee and Chang, 2003" startWordPosition="1896" endWordPosition="1899">o build the mapping directly between English words and Chinese characters. We adopt a romanization system, Chinese PinYin, to ease the transformation. Each Chinese character corresponds to a Chinese PinYin string. And the probability from a Chinese character to PinYin string is P(r |c) ≈1 , except for polyphonous characters. Thus we have: c) = arg max ( ) * ( |) * ( |) P c P r c P e r c Our problem is: Given both English NE and candidate Chinese NEs, finding the most probable alignment, instead of finding the most probable Chinese translation of the English NE. Therefore unlike previous work (Lee and Chang, 2003; Huang et al., 2003) in English-Chinese transliteration models, we transform each candidate Chinese NE to Chinese PinYin strings and directly train a PinYin-based language model with a separate English-Chinese name list consisting of 1258 name pairs to decode the most probable PinYin string from English NE. To find the most probable PinYin string from English NE, we rewrite Formula (3.5) as the following: r) arg = max P(r) * P(e |r) r where r represents the romanization (PinYin string), r = {r 1, r2 ... rm }. For each of the factor, we have m P e r ( |) = ∏ P(ei |ri) (3.8) i=1 m P r P r P r r</context>
</contexts>
<marker>Lee, Chang, 2003</marker>
<rawString>Lee, C. and Chang, J. S. 2003. Acquisition of English-Chinese Transliterated Word Pairs from Parallel-Aligned Texts, HLT-NAACL 2003 Workshop on Data Driven MT, pp. 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Towards a Unified Approach to Memory- and Statistical-Based Machine Translation.</title>
<date>2001</date>
<pages>378--385</pages>
<publisher>ACL</publisher>
<location>Toulouse, France.</location>
<contexts>
<context position="5482" citStr="Marcu, 2001" startWordPosition="860" endWordPosition="861">l et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of cost models. Ho</context>
</contexts>
<marker>Marcu, 2001</marker>
<rawString>Marcu, D. 2001. Towards a Unified Approach to Memory- and Statistical-Based Machine Translation. ACL 2001, pp. 378-385. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of Translation Equivalence among Words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<pages>221--249</pages>
<contexts>
<context position="4864" citStr="Melamed, 2000" startWordPosition="756" endWordPosition="757">amework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., </context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>Melamed, I. D. 2000. Models of Translation Equivalence among Words. Computational Linguistics, 26(2): 221-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<date>2003</date>
<booktitle>Learning Translations of Named-Entity Phrases from Parallel Corpora. EACL-2003.</booktitle>
<location>Budapest, Hungary.</location>
<marker>Moore, 2003</marker>
<rawString>Moore, R. C. 2003. Learning Translations of Named-Entity Phrases from Parallel Corpora. EACL-2003. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models,</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>pp.</pages>
<contexts>
<context position="20220" citStr="Och and Ney, 2003" startWordPosition="3393" endWordPosition="3396">5k every time. 4.2 Baseline System A translated Chinese NE may appear at a different position from the corresponding English NE in the sentence. IBM Model 4 (Brown et al., 1993) integrates a distortion probability, which is complete enough to account for this tendency. The HMM model (Vogel et al., 1996) conducts word alignment with a strong tendency to preserve localization from one language to another. Therefore we extract NE alignments based on the results of these two models as our baseline systems. For the alignments of IBM Model 4 and HMM, we use the published software package, GIZA++ 3 (Och and Ney, 2003) for processing. Some recent research has proposed to extract phrase translations based on the results from IBM Model (Koehn et al., 2003). We extract EnglishChinese NE alignments based on the results from IBM Model 4 and HMM. The extraction strategy takes each of the continuous aligned segments as one possible candidate, and finally the one with the highest frequency in the whole corpus wins. Figure 3. Example of Extraction Strategy Figure 3 gives an example of the extraction strategy. “China” here is aligned to either “中国” or “中”. Finally the one with a higher frequency in the whole corpus, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and Ney, H. 2003. A Systematic Comparison of Various Statistical Alignment Models, Computational Linguistics, volume 29, number 1, pp. 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. ACL</title>
<date>2002</date>
<pages>295--302</pages>
<contexts>
<context position="7728" citStr="Och and Ney, 2002" startWordPosition="1223" endWordPosition="1226">e features that can be used for Named Entity alignment. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to handle our problem. Suppose the source English NE nee, nee = {e1,e2 ... en}, consists of n English words and the candidate Chinese NE nec , nec = {c1,c2 ... cm}, is composed of m Chinese characters. Suppose also that we have M feature functions hm (nec, nee), m =1,... , M. For each feature function, we have a model parameter λm , m =1,... , M. The alignment probability can be defined as follows (Och and Ney, 2002): P(nec |ne e) = p,M (nec |nee) h ne ne m ( , c e ) ] The decision rule to choose the most probable aligned target NE of the English NE is (Och and Ney, 2002): arg max{P(nec |nee )} nec  ∑=  m 1 M λ ( , ) m m h ne ne c e (3.2) In our approach, considering the characteristics of NE translation, we adopt 4 features: translation score, transliteration score, the source NE and target NE’s co-occurrence score, and distortion score for distinguishing identical NEs in the same sentence. Next, we discuss these four features in detail. 3.1 Feature Functions 3.1.1 Translation Score It is important</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J. and Ney, H. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. ACL 2002, pp. 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<pages>440--447</pages>
<publisher>ACL</publisher>
<contexts>
<context position="4849" citStr="Och and Ney, 2000" startWordPosition="752" endWordPosition="755">ives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, F. J. and Ney, H. 2000. Improved Statistical Alignment Models. ACL 2000, pp: 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Probst</author>
<author>R Brown</author>
</authors>
<title>Using Similarity Scoring to Improve the Bilingual Dictionary for Word Alignment. ACL-2002,</title>
<date>2002</date>
<pages>409--416</pages>
<contexts>
<context position="4811" citStr="Probst and Brown, 2002" startWordPosition="744" endWordPosition="747">s related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to corres</context>
</contexts>
<marker>Probst, Brown, 2002</marker>
<rawString>Probst, K. and Brown, R. 2002. Using Similarity Scoring to Improve the Bilingual Dictionary for Word Alignment. ACL-2002, pp: 409-416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMBased Word Alignment</title>
<date>1996</date>
<booktitle>in Statistical Translation. COLING’96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="3748" citStr="Vogel et al., 1996" startWordPosition="577" endWordPosition="580">mum entropy model, bootstrapping is used to help supervised learning. On the other hand, to avoid error propagations from word segmentation and NER, we directly extract Chinese NEs and make the alignment from plain text without word segmentation. It is unlike previous work reported in the literature. Although this makes the task more difficult, it greatly reduces the chance of errors introduced by previous steps and therefore produces much better performance on our task. To justify our approach, we adopt traditional alignment approaches, in particular IBM Model 4 (Brown et al., 1993) and HMM (Vogel et al., 1996), to carry out NE alignment as our baseline systems. Experimental results show that in this task our approach outperforms IBM Model 4 and HMM significantly. Furthermore, the performance 1 We only discuss NEs of three categories: Person Name (PN), Location Name (LN), and Organization Name (ON). without word segmentation is much better than that with word segmentation. The rest of this paper is organized as follows: In section 2, we discuss related work on NE alignment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping proce</context>
<context position="5628" citStr="Vogel et al., 1996" startWordPosition="879" endWordPosition="882"> of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in the target language (Koehn et al., 2003; Marcu, 2001). Therefore they can not handle many-to-many word alignments within NEs well. Another well-known word alignment approach, HMM (Vogel et al., 1996), makes the alignment probabilities depend on the alignment position of the previous word. It does not explicitly consider many-to-many alignment either. Huang et al. (2003) proposed to extract Named Entity translingual equivalences based on the minimization of a linearly combined multi-feature cost. But they require Named Entity Recognition on both the source side and the target side. Moore’s (2003) approach is based on a sequence of cost models. However, this approach greatly relies on linguistic information, such as a string repeated on both sides, and clues from capital letters that are no</context>
<context position="19906" citStr="Vogel et al., 1996" startWordPosition="3340" endWordPosition="3343">ET.html sentences as the standard testing set, and we repeatedly add 5k more sentences into the data set and process the new data. After iterative re-ranking, the performance of alignment models over the 300 sentence pairs is calculated. The learning curves are drawn from 5k through 30k sentences with the step as 5k every time. 4.2 Baseline System A translated Chinese NE may appear at a different position from the corresponding English NE in the sentence. IBM Model 4 (Brown et al., 1993) integrates a distortion probability, which is complete enough to account for this tendency. The HMM model (Vogel et al., 1996) conducts word alignment with a strong tendency to preserve localization from one language to another. Therefore we extract NE alignments based on the results of these two models as our baseline systems. For the alignments of IBM Model 4 and HMM, we use the published software package, GIZA++ 3 (Och and Ney, 2003) for processing. Some recent research has proposed to extract phrase translations based on the results from IBM Model (Koehn et al., 2003). We extract EnglishChinese NE alignments based on the results from IBM Model 4 and HMM. The extraction strategy takes each of the continuous aligne</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Vogel, S.; Ney, H. and Tillmann, C. 1996. HMMBased Word Alignment in Statistical Translation. COLING’96, pp. 836-841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M Zhou</author>
<author>J Huang</author>
<author>C Huang</author>
</authors>
<title>Structural Alignment using Bilingual Chunking.</title>
<date>2002</date>
<tech>COLING-2002.</tech>
<contexts>
<context position="4830" citStr="Wang et al., 2002" startWordPosition="748" endWordPosition="751">gnment. Section 3 gives the overall framework of NE alignment with our maximum entropy model. Feature functions and bootstrapping procedures are also explained in this section. We show experimental results and compare them with baseline systems in Section 4. Section 5 concludes the paper and discusses ongoing future work. 2 Related Work Translation knowledge can be acquired via word and phrase alignment. So far a lot of research has been conducted in the field of machine translation and knowledge acquisition, including both statistical approaches (Cherry and Lin, 2003; Probst and Brown, 2002; Wang et al., 2002; Och and Ney, 2000; Melamed, 2000; Vogel et al., 1996) and symbolic approaches (Huang and Choi, 2000; Ker and Chang, 1997). However, these approaches do not work well on the task of NE alignment. Traditional approaches following IBM Models (Brown et al., 1993) are not able to produce satisfactory results due to their inherent inability to handle many-to-many alignments. They only carry out the alignment between words and do not consider the case of complex phrases like some multi-word NEs. On the other hand, IBM Models allow at most one word in the source language to correspond to a word in t</context>
</contexts>
<marker>Wang, Zhou, Huang, Huang, 2002</marker>
<rawString>Wang, W.; Zhou, M.; Huang, J. and Huang, C. 2002. Structural Alignment using Bilingual Chunking. COLING-2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>