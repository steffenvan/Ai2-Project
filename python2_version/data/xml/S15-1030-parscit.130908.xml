<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.990831">
Discovering Hypernymy Relations using Text Layout
</title>
<author confidence="0.911411">
Jean-Philippe Fauconnier
</author>
<affiliation confidence="0.696727333333333">
Institut de Recherche en
Informatique de Toulouse
118, Route de Narbonne
</affiliation>
<address confidence="0.966564">
31062 Toulouse, France
</address>
<email confidence="0.998685">
faucon@irit.fr
</email>
<author confidence="0.883989">
Mouna Kamel
</author>
<affiliation confidence="0.672541666666667">
Institut de Recherche en
Informatique de Toulouse
118, Route de Narbonne
</affiliation>
<address confidence="0.966163">
31062 Toulouse, France
</address>
<email confidence="0.999183">
kamel@irit.fr
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995">
Hypernymy relation acquisition has been
widely investigated, especially because tax-
onomies, which often constitute the backbone
structure of semantic resources are structured
using this type of relations. Although lots
of approaches have been dedicated to this
task, most of them analyze only the written
text. However relations between not necessar-
ily contiguous textual units can be expressed,
thanks to typographical or dispositional mark-
ers. Such relations, which are out of reach
of standard NLP tools, have been investigated
in well specified layout contexts. Our aim is
to improve the relation extraction task consid-
ering both the plain text and the layout. We
are proposing here a method which combines
layout, discourse and terminological analyses,
and performs a structured prediction. We fo-
cused on textual structures which correspond
to a well defined discourse structure and which
often bear hypernymy relations. This type of
structure encompasses titles and sub-titles, or
enumerative structures. The results achieve a
precision of about 60%.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951571428571">
The hypernymy relation acquisition task is a widely
studied problem, especially because taxonomies,
which often constitute the backbone structure of
semantic resources like ontologies, are structured
using this type of relations. Although this task
has been addressed in literature, most of the
publications report analyses based on the written
text only, usually at the phrase or sentence level.
However, a written text is not merely a set of words
or sentences. When producing a document, a writer
may use various layout means, in addition to strictly
linguistics devices such as syntactic arrangement or
rhetorical forms. Relations between textual units
that are not necessarily contiguous can thus be
expressed thanks to typographical or dispositional
markers. Such relations, which are out of reach of
standard NLP tools, have been studied within some
specific layout contexts. Our aim is to improve
the relation extraction task by considering both the
plain text and the layout. This means (1) identifying
hierarchical structures within the text using only
layout, (2) identifying relations carried by these
structures, using both lexico-syntactic and layout
features.
Such an approach is deemed novel for at least
two reasons. It combines layout, discourse and
terminological analyses to bridge the gap be-
tween the document layout and lexical resources.
Moreover, it makes a structured prediction of the
whole hierarchical structure according to the set of
visual and discourse properties, rather than making
decisions only based on parts of this structure, as
usually performed.
The main strength of our approach is its ap-
plicability to different document formats as well
to several domains. It should be highlighted that
encyclopedic, technical or scientific documents,
which are often analyzed for building semantic
resources, are most of the time strongly structured.
Our approach has been implemented for the French
language, for which only few resources are currently
available. In this paper we focus on specific textual
</bodyText>
<page confidence="0.982973">
249
</page>
<note confidence="0.953166">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 249–258,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.996869692307692">
structures which share the same discourse properties
and that are expected to bear hypernymy relations.
They encompass for instance titles/sub-titles, or
enumerative structures.
The paper is organized as follows. Some
related works about hypernymy relation identifica-
tion are reported in section 2. Section 3 presents
the theoretical framework on which the proposed
approach is based. Sections 4 and 5 respectively
describe transitions from the text layout to its
discourse representation and from this discourse
structure to the terminological structure. Finally we
draw conclusions and propose some perspectives.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="introduction">
2 Related works
</sectionHeader>
<bodyText confidence="0.99996272368421">
The task of extracting hypernymy relations (it
may also be denoted as generic/specific, taxo-
nomic, is-a or instance-of relations) is critical
for building semantic resources and for semantic
content authoring. Several parameters concerning
corpora may affect the methods used for this task:
the natural language quality (carefully written or
informal), the textual genre (scientific, technical
documents, newspapers, etc.), technical properties
(corpus size, format), the level of precision of the
resource (thesaurus, lightweight or full-fledged
ontology), the degree of structuring, etc. This task
may be carried out by using the proper text and/or
external pre-existing resources. Various methods
for exploiting plain text exist using techniques
such as regular expressions (also known as lexico-
syntactic patterns) (Hearst, 1992), classification
using supervised or unsupervised learning (Snow
et al., 2004; Alfonseca and Manandhar, 2002),
distributional analysis (Lenci and Benotto, 2012) or
Formal Concepts Analysis (Cimiano et al., 2005).
In the Information Retrieval area, the relevant terms
are extracted from documents and organized into
hierarchies (S´anchez and Moreno, 2005).
Works on the document structure and on the
discourse relations that it conveys have been carried
out by the NLP community. Among these are
the Document Structure Theory (Power et al.,
2003), and the DArtbio system (Bateman et al.,
2001). These approaches offer strong theoretical
frameworks, but they were only implemented from
a text generation point of view.
With regard to the relation extraction task
using layout, two categories of approaches may
be distinguished. The first one encompasses ap-
proaches exploiting documents written in a markup
language. The semantics of these tags and their
nested structure is used to build semantic resources.
For instance, collection of XML documents have
been analyzed to build ontologies (Kamel and
Aussenac-Gilles, 2009), while collection of HTML
or MediaWiki documents have been exploited to
build taxonomies (Sumida and Torisawa, 2008).
The second category gathers approaches ex-
ploiting specific documents or parts of documents,
for which the semantics of the layout is strictly
defined. Let us mention dictionaries and thesaurus
(Jannink and Wiederhold, 1999) or specific and well
localized textual structures such as category field
(Chernov et al., 2006; Suchanek et al., 2007) or
infoboxes (Auer et al., 2007) from Wikipedia pages.
In some cases, these specific textual structures are
also expressed thanks to a markup language. All
these works implement symbolic as well as machine
learning techniques.
Our approach is similar to the one followed
by Sumida and Torisawa (2008) which analyzes
a structured text according to the following steps:
(1) they represent the document structure from a
limited set of tags (headings, bulleted lists, ordered
lists and definition lists), (2) they link two tagged
strings when the first one is in the scope of the
second one, and (3) they use lexico-syntactic and
layout features for selecting hypernymy relations,
with the help of a machine learning algorithm.
Some attempts have been made for improving these
results (Oh et al., 2009; Yamada et al., 2009).
However our work differs in two points: we aimed
to be more generic by proposing a discourse struc-
ture of layout that can be inferred from different
document formats, and we propose to find out the
relation arguments (hypernym-hyponym term pairs)
by analyzing propositional contents. Prior to de-
scribing the implemented processes, the underlying
principles of our approach will be reported in the
next section.
</bodyText>
<page confidence="0.995897">
250
</page>
<sectionHeader confidence="0.918183" genericHeader="method">
3 Underlying principles of our approach
</sectionHeader>
<bodyText confidence="0.999958333333333">
We rely on principles of discourse theories and on
knowledge models for respectively formalizing text
layout and identifying hypernymy relations.
</bodyText>
<subsectionHeader confidence="0.999913">
3.1 Discourse analysis of the layout
</subsectionHeader>
<bodyText confidence="0.999398837837838">
Several discourse theories exist. Their starting point
lies in the idea that a text is not just a collection
of sentences, but it also includes relations between
all these sentences that ensure its coherence (Mann
and Thompson, 1988; Asher and Lascarides, 2003).
Discourse analysis aims at observing the discourse
coherence from a rhetorical point of view (the
intention of the author) or from a semantic point
of view (the description of the world). A discourse
analysis is a three step process: splitting the text
into Discourse Units (DU), ensuring the attachment
between DUs, and then labeling links between DUs
with discourse relations. Discourse relations may
be divided into two categories: nucleus-satellite
(or subordinate) relations which link an important
argument to an argument supporting background
information, and multi-nuclear (or coordinate)
relations which link arguments of equal importance.
Most of discourse theories acknowledge that a
discourse is hierarchically structured thanks to
discourse relations.
Text layout supports a large part of seman-
tics and participates to the coherence of the text; it
thus contributes to the elaboration of the discourse.
Therefore, we adapted the discourse analysis to treat
the layout, according to the following principles:
- a DU corresponds to a visual unit (a bloc);
- two units sharing the same role (title, para-
graph, etc.) and the same typographic and
dispositional markers are linked with a multi-
nuclear relation; otherwise, they are linked with
a nuclear-satellite relation.
An example1 of document from Wikipedia and the
tree which results from the discourse analysis of its
layout is given (Figure 1). In the following figures,
we represent nucleus-satellite relations with solid
lines and multi-nuclear relations with dashed lines.
</bodyText>
<footnote confidence="0.961854">
1http://fr.wikipedia.org/wiki/Red´ecentralisation d’Internet
</footnote>
<bodyText confidence="0.993326125">
We are currently interested in discourse structures
displaying the following properties:
- n DUs are linked with multi-nuclear relations;
- one of these coordinated DU is linked to an-
other DU with a nucleus-satellite relation.
Figure 2 gives a representation of such a discourse
structure according to the Rhetorical Structure The-
ory (Mann and Thompson, 1988).
</bodyText>
<figureCaption confidence="0.902523">
Figure 2: Rhetorical representation of the discourse
structure of interest
</figureCaption>
<bodyText confidence="0.999447538461538">
Although there is only one explicit nucleus-
satellite relation, this kind of structure involves n
implicit nucleus-satellite relations (between DUO
and DUi (2 G i G n)). Indeed, from a discourse
point of view, if a DUB is subordinated to a DUi,
then all DUk coordinated to DUB, are subordinated
to DUi. As mentioned above, this kind of dis-
course structure encompasses textual structures such
as titles/sub-titles and enumerative structures which
are frequent in structured documents, and which of-
ten convey hypernymy relation. In that context,
the hypernym is borne by the DUO and each DUi
(1 G i G n) bears at least one hyponym.
</bodyText>
<subsectionHeader confidence="0.842132">
3.2 Knowledge models for hypernymy relation
identification
</subsectionHeader>
<bodyText confidence="0.9999755">
Hypernymy relation identification is carried out in
two stages: specifying if the relation is hypernymic
and, if appropriate, identifying its arguments. The
first stage relies on linguistic regularities denoting
a hypernymy relation, regularities which are ex-
pressed thanks to lexical, syntactic, typographical
and dispositional clues.
The second stage is based on a graph represen-
tation. Rather than independently identifying links
between the hypernym and each potential hyponym,
we take advantage from the fact that writers use the
same syntactic and visual skills (recognized by a tex-
tual parallelism) for expressing knowledge units of
equal rhetorical importance. Generally, these salient
units are semantically linked and belong to a same
lexical field.
</bodyText>
<page confidence="0.980545">
251
</page>
<figure confidence="0.380355">
text
[3 Solutions ]_1
[La décentralisation d&apos;Internet peut se faire via : ]_2
</figure>
<listItem confidence="0.898751153846154">
● [l&apos;autohébergement de son serveur grâce aux projets : ]_3
● [YunoHost ; ]_4
● [ArkOS ; ]_5
● [aux réseaux pair à pair et aux protocoles de communication
interopérables et libres comme : ]_6
● [le courrier électronique : SMTP, IMAP ; ]_7
● [la messagerie instantanée : XMPP ; ]_8
● [le partage de fichiers en pair à pair avec par exemple le
protocole BitTorrent ; ]_9
● [le tchat en salons (permettant plus de deux personnes) avec
des logiciels tels que RetroShare, Marabunta ; ]_10
● [aux moteurs de recherche décentralisés comme YaCy, Seeks ; ]_11
● [aux architectures distribuées. ]_12
</listItem>
<figure confidence="0.984806166666667">
text
[1] title (level 1)
[2] paragraph
[3] item [5] item [7] item [8] item [9] item [10] item
[4] item [6] item [11] item
[12] item
</figure>
<figureCaption confidence="0.999997">
Figure 1: Example of a discourse analysis of text layout
</figureCaption>
<bodyText confidence="0.9999397">
Thus, we represent each discourse structure of in-
terest bearing a hypernymy relation as a directed
acyclic graph (DAG), where the nodes are terms and
the edges are possible relations between them. This
DAG is decomposed into layers, each layer i gath-
ering nodes corresponding to terms of a given DUi
(0 &lt; i &lt; n). Each node of a layer i (0 &lt; i &lt;
(n − 1)) is connected by directed edges to all nodes
of the layer i + 1. A root node is added on the top of
the DAG. Figure 3 presents an example of this DAG.
</bodyText>
<figureCaption confidence="0.996753">
Figure 3: Example of a DAG
</figureCaption>
<bodyText confidence="0.9999772">
We weight the edges according to the inverse sim-
ilarity of terms they link. Thus, the terms in the
lower-cost path starting from the root and ending at
the last layer are maximally cohesive. A flatter rep-
resentation does not allow this structured prediction.
</bodyText>
<sectionHeader confidence="0.8844505" genericHeader="method">
4 From text layout to its discourse
representation
</sectionHeader>
<bodyText confidence="0.999991266666667">
To elicit discourse structures from text layout, the
system detects visuals units and labels them with
their role (paragraph, title, footnote, etc.) in the text.
Then, it links the labeled units using discourse rela-
tions (nucleus-satellite or multi-nuclear) in order to
produce a discourse tree.
We are currently able to process two types of doc-
uments: documents written in a markup language
and documents in PDF format. It is obvious that
tags of markup languages both delimit blocs and
give their role. Getting the visual structure is thus
straightforward. Conversely, PDF documents do not
benefit from such tags. So we used the LAPDF-Text
tool (Ramakrishnan et al., 2012) which is based on a
geometric analysis for detecting blocs, and we have
implemented a machine learning method for label-
ing these blocs. The features include typographical
markers (size of fonts, emphasis markers, etc.) and
dispositional one (margins, position in page, etc.).
For labeling relations, we used an adapted version
of the shift-reduce algorithm as (Marcu, 1999) did.
We thus obtain a dependency tree representing the
discourse structure of the text layout. We evaluate
this process on a corpus of PDF documents (docu-
ments written in a markup language pose no prob-
lem). Results are good since we obtain an accuracy
of 80.46% for labeling blocs, and an accuracy of
97.23% for labeling discourse relations (Fauconnier
et al., 2014). The whole process has been imple-
mented in the LaToe2 tool.
</bodyText>
<footnote confidence="0.695808">
2 http://github.com/fauconnier/LaToe
</footnote>
<figure confidence="0.982719380952381">
root
layer 0
réseaux
pair à pair
protocoles de com-
munication interopérables
layer 1
courrier SMTP IMAP
électronique
layer 2
messagerie
XMPP
instantanée
layer 3
partage de
fichiers en pair à pair
protocole
BitTorrent
layer 4
tchat
en salons personnes logiciels RetroShare Marabunta
</figure>
<page confidence="0.984664">
252
</page>
<bodyText confidence="0.9472604">
Finally, the extraction of discourse structures of
interest may be done easily by means of tree patterns
(Levy and Andrew, 2006).
5 From layout discourse structure to
terminological structure
We wish to elicit possible hypernymy relations from
identified discourse structures of interest. This task
involves a two-step process. The first step consists in
specifying the nature of the relation borne by these
structures. The second step aims at identifying the
related terms (the relation arguments). These steps
have been independently evaluated on an annotated
corpus, while the whole system has been evaluated
on another not annotated corpus. Corpora and eval-
uation protocols are described in the next section.
</bodyText>
<subsectionHeader confidence="0.960607">
5.1 Corpora and evaluation protocols
</subsectionHeader>
<bodyText confidence="0.999995172413793">
The annotated corpus includes 166 French
Wikipedia pages corresponding to urban and
environmental planning. 745 discourse structures of
interest were annotated by 3 annotators (2 students
in Linguistics, and an expert in knowledge engi-
neering) according to a guideline. The annotation
task for each discourse structure of interest has
consisted in annotating the nucleus-satellite relation
as hypernymy or not, and when required, in anno-
tating the terms involved in the relation. For the first
stage, we have calculated a degree of inter-annotator
agreement (Fleiss et al., 1979) and obtained a kappa
of 0.54. The second stage was evaluated as a named
entity recognition task (Tateisi et al., 2000) for
which we have obtained an F-measure of 79.44.
From this dataset, 80% of the discourse structures
of interest were randomly chosen to constitute
the development set, and the remaining 20% were
used for the test set. The tasks described below
were tuned on the development set using a k-10
cross-validation. The evaluation is done using the
precision, the recall and the F-measure metrics.
A second evaluation for the entire system was
led on two corpora respectively made of Wikipedia
pages from two domains: Transport and Computer
Science. For each domain, we have randomly
selected 400 pages from a French Wikipedia Dump
(2014-09-28). Since those copora are not manually
annotated, we have only reported the precision.
</bodyText>
<subsectionHeader confidence="0.999207">
5.2 Qualifying the nucleus-satellite relation
</subsectionHeader>
<bodyText confidence="0.99976936">
Hypernymy relations present lexical, syntactic, ty-
pographical and dispositional regularities in the
text. The recognition of these relations is thus
based on the analysis of these regularities within
the two DUs explicitly linked by the nucleus-
satellite relation. We consider this problem as
a binary classification one: each discourse struc-
ture is assigned to either the Hypernymy-Structure
class or the nonHypernymy-Structure class. The
Hypernymy-Structure class encompasses discourse
structures with a nucleus-satellite relation bearing
a hypernymy, whereas the nonHypernymy-Structure
one gathers all others discourse structures. In the ex-
ample given in figure 1, the discourse structures con-
stituted of DUs {3,4,5} and {6,7,8,9,10} would be
classified as Hypernymy-Structure, while this con-
stituted of DUs {2,3,6,11,12} would be assigned to
the nonHypernymy-Structure class.
For this purpose, we applied feature functions
(summarized in table 1) in order to map the two DUs
linked by the explicit nucleus-satellite relation into
a numerical vector which is submitted to a classi-
fier. The feature functions were defined according
to background knowledge and were selected on the
basis of a Pearson’s correlation.
</bodyText>
<tableCaption confidence="0.982618">
Table 1: Main features for qualifying the relation
</tableCaption>
<bodyText confidence="0.98363725">
We have compared two types of classifiers: a
linear one which generalizes well, but may pro-
duce more misclassifications when data distribution
presents a large spread, and a non-linear one which
may lead to a model separating well the training set
but with an overfitting risk. We respectively used
a Maximum Entropy classifier (MaxEnt) (Berger et
al., 1996) and a Support Vector Machine (SVM)
with a Gaussian kernel (Cortes and Vapnik, 1995).
Features
Description
Unigrams of parts of speech
Position of a token in a DU
Boolean indicating whether a token be-
longs to a predefined lexicon
Boolean indicating whether the last sen-
tence of a DU shows a syntactic hole
Returns the last punctuation of a DU
Number of tokens in a DU
Number of sentences in a DU
</bodyText>
<figure confidence="0.987136285714286">
POS
Position
Markers
Gram
Punc
NbToken
NbSent
</figure>
<page confidence="0.998071">
253
</page>
<bodyText confidence="0.9999471">
The morphological and lexical information used
were obtained from the French dependency parser
Talismane (Urieli, 2013). For the classifiers, we
have used the OpenNLP3 library for the MaxEnt
and the LIBSVM implementation of the SVM4. This
task has been evaluated against a majority baseline
which better reflects the reality because of the asym-
metry of the relation distribution. Table 2 presents
the results. The two supervised strategies outper-
form significantly the baseline (p-values&lt;0.01)5.
</bodyText>
<table confidence="0.99925825">
Strategies Prec. Rec. F1
MaxEnt 78.01 84.78 81.25
SVM 74.77 90.22 81.77
Baseline 63.01 100.0 77.31
</table>
<tableCaption confidence="0.996869">
Table 2: Results for qualifying the relation
</tableCaption>
<bodyText confidence="0.999762142857143">
Regarding the F-measure metric, the difference
between the MaxEnt and the SVM is not significant.
We observe that the MaxEnt achieves the best preci-
sion, while the SVM reaches the best recall. These
results are not surprising since the SVM decision
boundary seems to be biased by outliers, thus in-
creasing the false positive rate on unseen data.
</bodyText>
<subsectionHeader confidence="0.79462">
5.3 Identifying the terms linked by the
hypernymy relation
</subsectionHeader>
<bodyText confidence="0.999322941176471">
We have now to identify terms linked by the hyper-
nymy relation. As previously mentioned we build a
DAG reflecting all possible relations between terms
of the DUs, to find the lower-cost path which repre-
sents the most cohesive sequence of terms.
If we consider the discourse structure consti-
tuted of DUs {6,7,8,9,10} in figure 1, the retrieved
path from the corresponding DAG (figure 3) would
be [“protocoles de communication interop´erables”
(interoperable communication protocols), “courrier
´electronique” (email), “messagerie instantan´ee” (in-
stant messaging), “partage de fichiers en pair a` pair”
(peer-to-peer file sharing), “tchat en salons” (chat
room)]. Then, an example of hypernymy relation
would be “courrier ´electronique” (email) is a kind of
“protocoles de communication interop´erables” (in-
teroperable communication protocols).
</bodyText>
<footnote confidence="0.868624">
3 http://opennlp.apache.org/
4 http://www.csie.ntu.edu.tw/∼cjlin/libsvm/
5 The p-values are calculated using a paired t-test.
</footnote>
<bodyText confidence="0.96406575">
The cost of an edge is defined using the following
function:
cost(&lt; Tij , Tki+1 &gt;) = 1 − p(y|Tij , T ki+1)
where Tij is the j-th term of DUi. The probability
assigned to the outcome y measures the likeliness
that both terms are linked. This probability is condi-
tioned by lexical and dispositional clues. Since it is
expected that terms involved in the relation share the
same lexical field, we also consider the cosine sim-
ilarity between the term vectors. All those clues are
mapped into a numerical vector using feature func-
tions summarized in table 3.
</bodyText>
<tableCaption confidence="0.953707">
Table 3: Main features for the terms recognition
</tableCaption>
<bodyText confidence="0.977182266666667">
We built two models based on supervised prob-
abilistic classifiers since characteristics of links
between a hypernym and a hyponym are different
from those between two hyponyms. The first model
considers only the edges between layer 0 and layer
1 (hypernym-hyponym link), whereas the second
one is dedicated to the edges of remaining layers
(hyponym-hyponym link).
For this step, we used ACABIT (Daille, 1996)
and YaTeA (Aubin and Hamon, 2006) for extracting
terms. The cosine similarity is based on a distri-
butional model constructed with the word2vec tool
(Mikolov et al., 2013) and the French corpus FrWac
(Baroni et al., 2009). We have learned the models
using a Maximum Entropy classifier.
</bodyText>
<figure confidence="0.818564">
Features
Description
Context of a term (bigrams and unigrams
of parts of speech)
Parts of speech of a term
Role of a DU
</figure>
<bodyText confidence="0.9340425">
Boolean indicating whether a pair of
terms share the same visual properties
Value indicating a term position
Position of a DU in the whole document
For a DU, presence of coordinated DUs
For a DU, presence of subordinated DUs
Value indicating the level of a DU in the
structure of document
</bodyText>
<figure confidence="0.918444529411765">
Returns the last punctuation of a DU
Number of tokens in a DU
Number of sentences in a DU
Cosine similarity for a pair of terms
POS c
POS t
Role
Visual
Position t
Position d
Coord
Sub
Level
Punc
NbToken
NbSent
COS
</figure>
<page confidence="0.998027">
254
</page>
<bodyText confidence="0.999972">
For computing the lower-cost path, we use
an A* search algorithm because it can handle large
search space with an admissible heuristic. The
estimated cost of a path P, a sequence of edges
from the root to a given term, is defined by:
</bodyText>
<equation confidence="0.997072">
f(P) = g(P) + h(P)
</equation>
<bodyText confidence="0.989806">
The function g(P) calculates the real cost along the
path P and it is defined by:
</bodyText>
<equation confidence="0.9749495">
g(P) = � cost(&lt; Tij, Tki+1 &gt;)
&lt;Tij ,Tki+,&gt; ∈ P
</equation>
<bodyText confidence="0.982412666666667">
The heuristic h(P) is a greedy function which picks
a new path with the minimal cost over d layers and
returns its cost:
</bodyText>
<equation confidence="0.991658">
h(P) = g(ld(P))
</equation>
<bodyText confidence="0.9184146">
The function ld(P) is defined recursively: l0(P) is
the empty path. Assume ld(P) is defined and Tjd
id is
the last node reached on the path formed by the con-
catenation of P and ld(P), then we define:
</bodyText>
<equation confidence="0.965147">
ld+1(P) = ld(P). &lt; Tjd
id , Tmid+1 &gt;
</equation>
<bodyText confidence="0.999808">
where m is the index of the term with the lower cost
edge and belonging to the layer id + 1:
</bodyText>
<equation confidence="0.8802045">
cost(&lt; T jd
id , Tkid+1 &gt;)
</equation>
<bodyText confidence="0.99976275">
This heuristic is admissible by definition. We
set d=3 because it is a good tradeoff between the
number of operations and the number of iterations
during the A* search.
In order to evaluate this task, we compare it to
a baseline and two vector-based approaches. The
baseline works on the assumption that two related
terms belong to a same window of words; then it
takes the last term of the layer 0 as hypernym, and
the first term of each layer i (1 &lt; i &lt; n) as hy-
ponym. The two other strategies use a cosine sim-
ilarity (calculated with respectively 200- and 500-
dimensional vectors) for the costs estimation. Table
4 presents the results.
The MaxEnt achieves the best F-measure and
outperforms the others proposed strategies. The
</bodyText>
<table confidence="0.9991858">
Strategies Prec. Rec. F1
MaxEnt 78.98 69.09 73.71
w2v-200 66.52 30.10 41.45
w2v-500 83.71 30.10 44.28
Baseline 48.37 69.09 56.91
</table>
<tableCaption confidence="0.99829">
Table 4: Results for terms recognition
</tableCaption>
<bodyText confidence="0.9995976">
vector-based strategies present interesting preci-
sions, which seems to confirm a correlation between
the lexical cohesion of terms and their likelihood of
being involved in a relation.
To lead additional evaluations we define the score
of a path as the mean of its costs, and we select re-
sults using a list of threshold values: only the paths
with a score lower than a given threshold are re-
turned. Figure 4 shows the Precision-Recall curves
using the whole list of threshold values.
</bodyText>
<equation confidence="0.610686">
0 10 20 30 40 50 60 70 80 90 100
recall [%]
</equation>
<figureCaption confidence="0.9854095">
Figure 4: Comparison between the baseline, the
vector-based strategies and the MaxEnt
</figureCaption>
<subsectionHeader confidence="0.992444">
5.4 Evaluation of the whole system
</subsectionHeader>
<bodyText confidence="0.9999624">
In this section, we report the results for the whole
process applied on two corpora made of Wikipedia
pages from two domains: Transport and Com-
puter Science. For each of them, we applied a
discourse analysis of the layout, and we extracted
the hypernym-hyponym pairs. This extraction was
done with a Maximum Entropy classifier which has
shown a good precision for the two tasks described
before. The retrieved pairs were ranked according
to the score of the path they belong to. Finally, we
</bodyText>
<figure confidence="0.94079206122449">
100
90
80
70
30
maxent
w2v−200
w2v−500
baseline
0
precision [%]
60
50
40
20
10
m = argmin
k&lt;|layer id+1|
255
Transport
number of hypernym−hyponym pairs
Computer Science
number of hypernym−hyponym pairs
0 50 100 150 200 250 300
precision [%]
100
90
80
60
50
40
20
70
30
10
0
0 50 100 150 200 250 300
precision [%]
100
90
80
60
50
40
20
70
30
10
0
</figure>
<figureCaption confidence="0.999968">
Figure 5: Precision curves for two domains of Wikipedia
</figureCaption>
<bodyText confidence="0.9997865">
manually checked the first 500 pairs. The curves
in figure 5 indicate the precision. For the two do-
mains, around 300 pairs were retrieved with a preci-
sion of about 60% for the highest threshold. Table 5
presents examples of extracted relations. The terms
noted with a symbol ‘*’ are considered as errors.
</bodyText>
<table confidence="0.992862">
hypernyms hyponyms
transporteurs STEF, transporteur*, Groupe De-
frigorifiques lanchy, Norbert Dentressangle,
(refrigerated Groupe Malherbe, Madrias
transporters)
pˆoles Gare de la Part Dieu, Centre in-
d’´echanges termodal d’´echanges de Limo-
(interchange ges, Union Station a` Toronto
stations)
transmission Courte distance*, Moyenne dis-
(transmis- tance*, Longue distance*
sion)
</table>
<tableCaption confidence="0.999369">
Table 5: Examples of extracted relations
</tableCaption>
<bodyText confidence="0.999947590909091">
We have identified the main sources of error. The
most common arises from nested discourse struc-
tures. In this case, intermediate DUs often specify
contexts, and therefore do not contain the searched
hyponyms. This is the case in the last example of
table 5 where the retrieved hyponyms for “transmis-
sion” (transmission) are “Courte distance” (Short
distance), “Moyenne distance” (Medium distance)
and “Longue distance” (Long distance).
Another error comes from a confusion between
hypernymy and meronymy relations, which are both
hierarchical. The fact that these two relations share
the same linguistic properties may explain this con-
fusion (Ittoo and Bouma, 2009). Furthermore we are
still faced with classical linguistic problems which
are out of the scope of this paper: anaphora, ellipse,
coreference, etc.
Finally, we ignore cases where the hypernymy re-
lation is reversed, i.e. when the hyponym is local-
ized into the nucleus DU and its hypernym into a
satellite DU. Clues that we use are not enough dis-
criminating at this level.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9987692">
In this paper we investigate a new way for extract-
ing hypernymy relations, exploiting the text layout
which expresses hierarchical relations and for which
standard NLP tools are not suitable.
The system implements a two steps process:
</bodyText>
<listItem confidence="0.949206333333333">
(1) a discourse analysis of the text layout, and
(2) a hypernymy relation identification within spe-
cific discourse structures. We first evaluate each
</listItem>
<bodyText confidence="0.980466625">
module independently (discourse analysis of the
layout, identification of the nature of the relation,
and identification of arguments of the relation), and
we obtain accuracies of about 80% and 97% for
the discourse analysis, and F-measures of about
81% and 73% for the relation extraction. We
then evaluate the whole process and we obtain a
precision of about 60%.
</bodyText>
<page confidence="0.992282">
256
</page>
<bodyText confidence="0.999962833333333">
One way to improve this work is to extend this
analysis to other hierarchical relations. We plan
to investigate more advanced techniques offered by
distributional semantic models in order to discrimi-
nate hypernymy relation from meronymy ones.
Another way is to extend the scope of investiga-
tion of the layout to take into account new discursive
structures. Moreover, a subsequent step to this work
is its large scale application on collections of struc-
tured web documents (such as Wikipedia pages) in
order to build semantic resources and to share them
with the community.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999152597701149">
Enrique Alfonseca and Suresh Manandhar. 2002. Im-
proving an ontology refinement method with hy-
ponymy patterns. cell, 4081:0–0087.
N. Asher and A. Lascarides. 2003. Logics of conversa-
tion. Cambridge University Press.
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In Ad-
vances in Natural Language Processing, pages 380–
387. Springer.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In The
semantic web, pages 722–735. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209–226.
John Bateman, Thomas Kamps, J¨org Kleinz, and Klaus
Reichenberger. 2001. Towards constructive text, dia-
gram, and layout generation for information presenta-
tion. Computational Linguistics, 27(3):409–449.
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational lin-
guistics, 22(1):39–71.
Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and
Xuan Zhou. 2006. Extracting semantics relationships
between wikipedia categories. SemWiki, 206.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text cor-
pora using formal concept analysis. J. Artif. Intell.
Res.(JAIR), 24:305–339.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
B´eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. The balancing act: Combining symbolic
and statistical approaches to language, 1:49–66.
Jean-Philippe Fauconnier, Laurent Sorin, Mouna Kamel,
Mustapha Mojahid, and Nathalie Aussenac-Gilles.
2014. D´etection automatique de la structure organ-
isationnelle de documents a` partir de marqueurs vi-
suels et lexicaux. In Actes de la 21e Conf´erence
sur le TraitementAutomatique des Langues Naturelles
(TALN 2014), pages 340–351.
Joseph L Fleiss, John C Nee, and J Richard Landis. 1979.
Large sample variance of kappa in the case of different
sets of raters. Psychological Bulletin, 86(5):974–977.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics, vol-
ume 2, pages 539–545. Association for Computational
Linguistics.
Ashwin Ittoo and Gosse Bouma. 2009. Semantic selec-
tional restrictions for disambiguating meronymy rela-
tions. In proceedings of CLIN09: The 19th Compu-
tational Linguistics in the Netherlands meeting, to ap-
pear.
Jan Jannink and Gio Wiederhold. 1999. Thesaurus entry
extraction from an on-line dictionary. In Proceedings
of Fusion, volume 99. Citeseer.
Mouna Kamel and Nathalie Aussenac-Gilles. 2009.
How can document structure improve ontology learn-
ing? In Workshop on Semantic Annotation and
Knowledge Markup collocated with K-CAP.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation, pages 75–79. Associa-
tion for Computational Linguistics.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In Proceedings of the fifth international
conference on Language Resources and Evaluation,
pages 2231–2234. Citeseer.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.
Daniel Marcu. 1999. A decision-based approach to
rhetorical parsing. In Proceedings of the 37th annual
meeting of the Association for Computational Linguis-
tics on Computational Linguistics, pages 365–372. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.959791">
257
</page>
<reference confidence="0.9992974">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Bilingual co-training for monolingual
hyponymy-relation acquisition. In Proceedings of the
47th Annual Meeting of the ACL and the 4th IJCNLP
of the AFNL, pages 432–440. Association for Compu-
tational Linguistics.
Richard Power, Donia Scott, and Nadjet Bouayad-Agha.
2003. Document structure. Computational Linguis-
tics, 29(2):211–260.
Cartic Ramakrishnan, Abhishek Patnia, Eduard H Hovy,
Gully APC Burns, et al. 2012. Layout-aware text ex-
traction from full-text pdf of scientific articles. Source
code for biology and medicine, 7(1):7.
David S´anchez and Antonio Moreno. 2005. Web-scale
taxonomy learning. In Proceedings of Workshop on
Extending and Learning Lexical Ontologies using Ma-
chine Learning (ICML 2005), pages 53–60, Bonn,
Germany.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, volume 17.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697–706. ACM.
Asuka Sumida and Kentaro Torisawa. 2008. Hacking
wikipedia for hyponymy relation acquisition. In IJC-
NLP, volume 8, pages 883–888. Citeseer.
Yuka Tateisi, Tomoko Ohta, Nigel Collier, Chikashi No-
bata, and Jun-ichi Tsujii. 2000. Building an annotated
corpus in the molecular-biology domain. In Proceed-
ings of the COLING-2000 Workshop on Semantic An-
notation and Intelligent Content, pages 28–36. Asso-
ciation for Computational Linguistics.
Assaf Urieli. 2013. Robust French syntax analysis: rec-
onciling statistical methods and linguistic knowledge
in the Talismane toolkit. Ph.D. thesis, Universit´e de
Toulouse.
Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama, Kow
Kuroda, Masaki Murata, Stijn De Saeger, Francis
Bond, and Asuka Sumida. 2009. Hypernym discov-
ery based on distributional similarity and hierarchical
structures. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2-Volume 2, pages 929–937. Association for
Computational Linguistics.
</reference>
<page confidence="0.996121">
258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411591">
<title confidence="0.991658">Discovering Hypernymy Relations using Text Layout</title>
<author confidence="0.957239">Jean-Philippe</author>
<affiliation confidence="0.9943945">Institut de Recherche Informatique de</affiliation>
<address confidence="0.9865235">118, Route de 31062 Toulouse,</address>
<email confidence="0.999062">faucon@irit.fr</email>
<author confidence="0.469032">Mouna</author>
<affiliation confidence="0.993305">Institut de Recherche Informatique de</affiliation>
<address confidence="0.993805">118, Route de 31062 Toulouse,</address>
<email confidence="0.998844">kamel@irit.fr</email>
<abstract confidence="0.9990368">Hypernymy relation acquisition has been widely investigated, especially because taxonomies, which often constitute the backbone structure of semantic resources are structured using this type of relations. Although lots of approaches have been dedicated to this task, most of them analyze only the written text. However relations between not necessarily contiguous textual units can be expressed, thanks to typographical or dispositional markers. Such relations, which are out of reach of standard NLP tools, have been investigated in well specified layout contexts. Our aim is to improve the relation extraction task considering both the plain text and the layout. We are proposing here a method which combines layout, discourse and terminological analyses, and performs a structured prediction. We focused on textual structures which correspond to a well defined discourse structure and which often bear hypernymy relations. This type of structure encompasses titles and sub-titles, or enumerative structures. The results achieve a precision of about 60%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Suresh Manandhar</author>
</authors>
<title>Improving an ontology refinement method with hyponymy patterns.</title>
<date>2002</date>
<pages>4081--0</pages>
<contexts>
<context position="5144" citStr="Alfonseca and Manandhar, 2002" startWordPosition="747" endWordPosition="750"> quality (carefully written or informal), the textual genre (scientific, technical documents, newspapers, etc.), technical properties (corpus size, format), the level of precision of the resource (thesaurus, lightweight or full-fledged ontology), the degree of structuring, etc. This task may be carried out by using the proper text and/or external pre-existing resources. Various methods for exploiting plain text exist using techniques such as regular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point o</context>
</contexts>
<marker>Alfonseca, Manandhar, 2002</marker>
<rawString>Enrique Alfonseca and Suresh Manandhar. 2002. Improving an ontology refinement method with hyponymy patterns. cell, 4081:0–0087.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8337" citStr="Asher and Lascarides, 2003" startWordPosition="1241" endWordPosition="1244">l contents. Prior to describing the implemented processes, the underlying principles of our approach will be reported in the next section. 250 3 Underlying principles of our approach We rely on principles of discourse theories and on knowledge models for respectively formalizing text layout and identifying hypernymy relations. 3.1 Discourse analysis of the layout Several discourse theories exist. Their starting point lies in the idea that a text is not just a collection of sentences, but it also includes relations between all these sentences that ensure its coherence (Mann and Thompson, 1988; Asher and Lascarides, 2003). Discourse analysis aims at observing the discourse coherence from a rhetorical point of view (the intention of the author) or from a semantic point of view (the description of the world). A discourse analysis is a three step process: splitting the text into Discourse Units (DU), ensuring the attachment between DUs, and then labeling links between DUs with discourse relations. Discourse relations may be divided into two categories: nucleus-satellite (or subordinate) relations which link an important argument to an argument supporting background information, and multi-nuclear (or coordinate) r</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophie Aubin</author>
<author>Thierry Hamon</author>
</authors>
<title>Improving term extraction with terminological resources.</title>
<date>2006</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>380--387</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="22674" citStr="Aubin and Hamon, 2006" startWordPosition="3499" endWordPosition="3502">larity between the term vectors. All those clues are mapped into a numerical vector using feature functions summarized in table 3. Table 3: Main features for the terms recognition We built two models based on supervised probabilistic classifiers since characteristics of links between a hypernym and a hyponym are different from those between two hyponyms. The first model considers only the edges between layer 0 and layer 1 (hypernym-hyponym link), whereas the second one is dedicated to the edges of remaining layers (hyponym-hyponym link). For this step, we used ACABIT (Daille, 1996) and YaTeA (Aubin and Hamon, 2006) for extracting terms. The cosine similarity is based on a distributional model constructed with the word2vec tool (Mikolov et al., 2013) and the French corpus FrWac (Baroni et al., 2009). We have learned the models using a Maximum Entropy classifier. Features Description Context of a term (bigrams and unigrams of parts of speech) Parts of speech of a term Role of a DU Boolean indicating whether a pair of terms share the same visual properties Value indicating a term position Position of a DU in the whole document For a DU, presence of coordinated DUs For a DU, presence of subordinated DUs Val</context>
</contexts>
<marker>Aubin, Hamon, 2006</marker>
<rawString>Sophie Aubin and Thierry Hamon. 2006. Improving term extraction with terminological resources. In Advances in Natural Language Processing, pages 380– 387. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In The semantic web,</booktitle>
<pages>722--735</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6649" citStr="Auer et al., 2007" startWordPosition="974" endWordPosition="977">esources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a markup language. All these works implement symbolic as well as machine learning techniques. Our approach is similar to the one followed by Sumida and Torisawa (2008) which analyzes a structured text according to the following steps: (1) they represent the document structure from a limited set of tags (headings, bulleted lists, ordered lists and definition lists), (2) they link two tagged strings when the first one is in the scope of the second one, and (3) they use lexico-syntactic and layout</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="22861" citStr="Baroni et al., 2009" startWordPosition="3530" endWordPosition="3533">two models based on supervised probabilistic classifiers since characteristics of links between a hypernym and a hyponym are different from those between two hyponyms. The first model considers only the edges between layer 0 and layer 1 (hypernym-hyponym link), whereas the second one is dedicated to the edges of remaining layers (hyponym-hyponym link). For this step, we used ACABIT (Daille, 1996) and YaTeA (Aubin and Hamon, 2006) for extracting terms. The cosine similarity is based on a distributional model constructed with the word2vec tool (Mikolov et al., 2013) and the French corpus FrWac (Baroni et al., 2009). We have learned the models using a Maximum Entropy classifier. Features Description Context of a term (bigrams and unigrams of parts of speech) Parts of speech of a term Role of a DU Boolean indicating whether a pair of terms share the same visual properties Value indicating a term position Position of a DU in the whole document For a DU, presence of coordinated DUs For a DU, presence of subordinated DUs Value indicating the level of a DU in the structure of document Returns the last punctuation of a DU Number of tokens in a DU Number of sentences in a DU Cosine similarity for a pair of term</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language resources and evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
<author>Thomas Kamps</author>
<author>J¨org Kleinz</author>
<author>Klaus Reichenberger</author>
</authors>
<title>Towards constructive text, diagram, and layout generation for information presentation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="5627" citStr="Bateman et al., 2001" startWordPosition="821" endWordPosition="824">ctic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task using layout, two categories of approaches may be distinguished. The first one encompasses approaches exploiting documents written in a markup language. The semantics of these tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to</context>
</contexts>
<marker>Bateman, Kamps, Kleinz, Reichenberger, 2001</marker>
<rawString>John Bateman, Thomas Kamps, J¨org Kleinz, and Klaus Reichenberger. 2001. Towards constructive text, diagram, and layout generation for information presentation. Computational Linguistics, 27(3):409–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="19168" citStr="Berger et al., 1996" startWordPosition="2946" endWordPosition="2949">eus-satellite relation into a numerical vector which is submitted to a classifier. The feature functions were defined according to background knowledge and were selected on the basis of a Pearson’s correlation. Table 1: Main features for qualifying the relation We have compared two types of classifiers: a linear one which generalizes well, but may produce more misclassifications when data distribution presents a large spread, and a non-linear one which may lead to a model separating well the training set but with an overfitting risk. We respectively used a Maximum Entropy classifier (MaxEnt) (Berger et al., 1996) and a Support Vector Machine (SVM) with a Gaussian kernel (Cortes and Vapnik, 1995). Features Description Unigrams of parts of speech Position of a token in a DU Boolean indicating whether a token belongs to a predefined lexicon Boolean indicating whether the last sentence of a DU shows a syntactic hole Returns the last punctuation of a DU Number of tokens in a DU Number of sentences in a DU POS Position Markers Gram Punc NbToken NbSent 253 The morphological and lexical information used were obtained from the French dependency parser Talismane (Urieli, 2013). For the classifiers, we have used</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Chernov</author>
<author>Tereza Iofciu</author>
<author>Wolfgang Nejdl</author>
<author>Xuan Zhou</author>
</authors>
<title>Extracting semantics relationships between wikipedia categories.</title>
<date>2006</date>
<journal>SemWiki,</journal>
<pages>206</pages>
<contexts>
<context position="6592" citStr="Chernov et al., 2006" startWordPosition="964" endWordPosition="967">tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a markup language. All these works implement symbolic as well as machine learning techniques. Our approach is similar to the one followed by Sumida and Torisawa (2008) which analyzes a structured text according to the following steps: (1) they represent the document structure from a limited set of tags (headings, bulleted lists, ordered lists and definition lists), (2) they link two tagged strings when the first one is in the scope of the</context>
</contexts>
<marker>Chernov, Iofciu, Nejdl, Zhou, 2006</marker>
<rawString>Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and Xuan Zhou. 2006. Extracting semantics relationships between wikipedia categories. SemWiki, 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text corpora using formal concept analysis.</title>
<date>2005</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<pages>24--305</pages>
<contexts>
<context position="5246" citStr="Cimiano et al., 2005" startWordPosition="761" endWordPosition="764">, technical properties (corpus size, format), the level of precision of the resource (thesaurus, lightweight or full-fledged ontology), the degree of structuring, etc. This task may be carried out by using the proper text and/or external pre-existing resources. Various methods for exploiting plain text exist using techniques such as regular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task using layout, two categories of approaches may be </context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Philipp Cimiano, Andreas Hotho, and Steffen Staab. 2005. Learning concept hierarchies from text corpora using formal concept analysis. J. Artif. Intell. Res.(JAIR), 24:305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="19252" citStr="Cortes and Vapnik, 1995" startWordPosition="2960" endWordPosition="2963">r. The feature functions were defined according to background knowledge and were selected on the basis of a Pearson’s correlation. Table 1: Main features for qualifying the relation We have compared two types of classifiers: a linear one which generalizes well, but may produce more misclassifications when data distribution presents a large spread, and a non-linear one which may lead to a model separating well the training set but with an overfitting risk. We respectively used a Maximum Entropy classifier (MaxEnt) (Berger et al., 1996) and a Support Vector Machine (SVM) with a Gaussian kernel (Cortes and Vapnik, 1995). Features Description Unigrams of parts of speech Position of a token in a DU Boolean indicating whether a token belongs to a predefined lexicon Boolean indicating whether the last sentence of a DU shows a syntactic hole Returns the last punctuation of a DU Number of tokens in a DU Number of sentences in a DU POS Position Markers Gram Punc NbToken NbSent 253 The morphological and lexical information used were obtained from the French dependency parser Talismane (Urieli, 2013). For the classifiers, we have used the OpenNLP3 library for the MaxEnt and the LIBSVM implementation of the SVM4. This</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B´eatrice Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology. The balancing act: Combining symbolic and statistical approaches to language,</title>
<date>1996</date>
<pages>1--49</pages>
<contexts>
<context position="22640" citStr="Daille, 1996" startWordPosition="3495" endWordPosition="3496"> consider the cosine similarity between the term vectors. All those clues are mapped into a numerical vector using feature functions summarized in table 3. Table 3: Main features for the terms recognition We built two models based on supervised probabilistic classifiers since characteristics of links between a hypernym and a hyponym are different from those between two hyponyms. The first model considers only the edges between layer 0 and layer 1 (hypernym-hyponym link), whereas the second one is dedicated to the edges of remaining layers (hyponym-hyponym link). For this step, we used ACABIT (Daille, 1996) and YaTeA (Aubin and Hamon, 2006) for extracting terms. The cosine similarity is based on a distributional model constructed with the word2vec tool (Mikolov et al., 2013) and the French corpus FrWac (Baroni et al., 2009). We have learned the models using a Maximum Entropy classifier. Features Description Context of a term (bigrams and unigrams of parts of speech) Parts of speech of a term Role of a DU Boolean indicating whether a pair of terms share the same visual properties Value indicating a term position Position of a DU in the whole document For a DU, presence of coordinated DUs For a DU</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>B´eatrice Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. The balancing act: Combining symbolic and statistical approaches to language, 1:49–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Philippe Fauconnier</author>
<author>Laurent Sorin</author>
<author>Mouna Kamel</author>
<author>Mustapha Mojahid</author>
<author>Nathalie Aussenac-Gilles</author>
</authors>
<title>D´etection automatique de la structure organisationnelle de documents a` partir de marqueurs visuels et lexicaux.</title>
<date>2014</date>
<booktitle>In Actes de la 21e Conf´erence sur le TraitementAutomatique des Langues Naturelles (TALN 2014),</booktitle>
<pages>340--351</pages>
<contexts>
<context position="14941" citStr="Fauconnier et al., 2014" startWordPosition="2306" endWordPosition="2309">beling these blocs. The features include typographical markers (size of fonts, emphasis markers, etc.) and dispositional one (margins, position in page, etc.). For labeling relations, we used an adapted version of the shift-reduce algorithm as (Marcu, 1999) did. We thus obtain a dependency tree representing the discourse structure of the text layout. We evaluate this process on a corpus of PDF documents (documents written in a markup language pose no problem). Results are good since we obtain an accuracy of 80.46% for labeling blocs, and an accuracy of 97.23% for labeling discourse relations (Fauconnier et al., 2014). The whole process has been implemented in the LaToe2 tool. 2 http://github.com/fauconnier/LaToe root layer 0 réseaux pair à pair protocoles de communication interopérables layer 1 courrier SMTP IMAP électronique layer 2 messagerie XMPP instantanée layer 3 partage de fichiers en pair à pair protocole BitTorrent layer 4 tchat en salons personnes logiciels RetroShare Marabunta 252 Finally, the extraction of discourse structures of interest may be done easily by means of tree patterns (Levy and Andrew, 2006). 5 From layout discourse structure to terminological structure We wish to elicit possibl</context>
</contexts>
<marker>Fauconnier, Sorin, Kamel, Mojahid, Aussenac-Gilles, 2014</marker>
<rawString>Jean-Philippe Fauconnier, Laurent Sorin, Mouna Kamel, Mustapha Mojahid, and Nathalie Aussenac-Gilles. 2014. D´etection automatique de la structure organisationnelle de documents a` partir de marqueurs visuels et lexicaux. In Actes de la 21e Conf´erence sur le TraitementAutomatique des Langues Naturelles (TALN 2014), pages 340–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
<author>John C Nee</author>
<author>J Richard Landis</author>
</authors>
<title>Large sample variance of kappa in the case of different sets of raters.</title>
<date>1979</date>
<journal>Psychological Bulletin,</journal>
<volume>86</volume>
<issue>5</issue>
<contexts>
<context position="16653" citStr="Fleiss et al., 1979" startWordPosition="2563" endWordPosition="2566">on. 5.1 Corpora and evaluation protocols The annotated corpus includes 166 French Wikipedia pages corresponding to urban and environmental planning. 745 discourse structures of interest were annotated by 3 annotators (2 students in Linguistics, and an expert in knowledge engineering) according to a guideline. The annotation task for each discourse structure of interest has consisted in annotating the nucleus-satellite relation as hypernymy or not, and when required, in annotating the terms involved in the relation. For the first stage, we have calculated a degree of inter-annotator agreement (Fleiss et al., 1979) and obtained a kappa of 0.54. The second stage was evaluated as a named entity recognition task (Tateisi et al., 2000) for which we have obtained an F-measure of 79.44. From this dataset, 80% of the discourse structures of interest were randomly chosen to constitute the development set, and the remaining 20% were used for the test set. The tasks described below were tuned on the development set using a k-10 cross-validation. The evaluation is done using the precision, the recall and the F-measure metrics. A second evaluation for the entire system was led on two corpora respectively made of Wi</context>
</contexts>
<marker>Fleiss, Nee, Landis, 1979</marker>
<rawString>Joseph L Fleiss, John C Nee, and J Richard Landis. 1979. Large sample variance of kappa in the case of different sets of raters. Psychological Bulletin, 86(5):974–977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics,</booktitle>
<volume>2</volume>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5035" citStr="Hearst, 1992" startWordPosition="735" endWordPosition="736">arameters concerning corpora may affect the methods used for this task: the natural language quality (carefully written or informal), the textual genre (scientific, technical documents, newspapers, etc.), technical properties (corpus size, format), the level of precision of the resource (thesaurus, lightweight or full-fledged ontology), the degree of structuring, etc. This task may be carried out by using the proper text and/or external pre-existing resources. Various methods for exploiting plain text exist using techniques such as regular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics, volume 2, pages 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwin Ittoo</author>
<author>Gosse Bouma</author>
</authors>
<title>Semantic selectional restrictions for disambiguating meronymy relations.</title>
<date>2009</date>
<booktitle>In proceedings of CLIN09: The 19th Computational Linguistics in the Netherlands meeting,</booktitle>
<note>to appear.</note>
<contexts>
<context position="28259" citStr="Ittoo and Bouma, 2009" startWordPosition="4474" endWordPosition="4477">. The most common arises from nested discourse structures. In this case, intermediate DUs often specify contexts, and therefore do not contain the searched hyponyms. This is the case in the last example of table 5 where the retrieved hyponyms for “transmission” (transmission) are “Courte distance” (Short distance), “Moyenne distance” (Medium distance) and “Longue distance” (Long distance). Another error comes from a confusion between hypernymy and meronymy relations, which are both hierarchical. The fact that these two relations share the same linguistic properties may explain this confusion (Ittoo and Bouma, 2009). Furthermore we are still faced with classical linguistic problems which are out of the scope of this paper: anaphora, ellipse, coreference, etc. Finally, we ignore cases where the hypernymy relation is reversed, i.e. when the hyponym is localized into the nucleus DU and its hypernym into a satellite DU. Clues that we use are not enough discriminating at this level. 6 Conclusion In this paper we investigate a new way for extracting hypernymy relations, exploiting the text layout which expresses hierarchical relations and for which standard NLP tools are not suitable. The system implements a t</context>
</contexts>
<marker>Ittoo, Bouma, 2009</marker>
<rawString>Ashwin Ittoo and Gosse Bouma. 2009. Semantic selectional restrictions for disambiguating meronymy relations. In proceedings of CLIN09: The 19th Computational Linguistics in the Netherlands meeting, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Jannink</author>
<author>Gio Wiederhold</author>
</authors>
<title>Thesaurus entry extraction from an on-line dictionary.</title>
<date>1999</date>
<booktitle>In Proceedings of Fusion,</booktitle>
<volume>99</volume>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6497" citStr="Jannink and Wiederhold, 1999" startWordPosition="949" endWordPosition="952">st one encompasses approaches exploiting documents written in a markup language. The semantics of these tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a markup language. All these works implement symbolic as well as machine learning techniques. Our approach is similar to the one followed by Sumida and Torisawa (2008) which analyzes a structured text according to the following steps: (1) they represent the document structure from a limited set of tags (headings, bulleted lists, ordered lists an</context>
</contexts>
<marker>Jannink, Wiederhold, 1999</marker>
<rawString>Jan Jannink and Gio Wiederhold. 1999. Thesaurus entry extraction from an on-line dictionary. In Proceedings of Fusion, volume 99. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mouna Kamel</author>
<author>Nathalie Aussenac-Gilles</author>
</authors>
<title>How can document structure improve ontology learning?</title>
<date>2009</date>
<booktitle>In Workshop on Semantic Annotation and Knowledge Markup collocated with K-CAP.</booktitle>
<contexts>
<context position="6155" citStr="Kamel and Aussenac-Gilles, 2009" startWordPosition="899" endWordPosition="902">se are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task using layout, two categories of approaches may be distinguished. The first one encompasses approaches exploiting documents written in a markup language. The semantics of these tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a mar</context>
</contexts>
<marker>Kamel, Aussenac-Gilles, 2009</marker>
<rawString>Mouna Kamel and Nathalie Aussenac-Gilles. 2009. How can document structure improve ontology learning? In Workshop on Semantic Annotation and Knowledge Markup collocated with K-CAP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>75--79</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5195" citStr="Lenci and Benotto, 2012" startWordPosition="753" endWordPosition="756">re (scientific, technical documents, newspapers, etc.), technical properties (corpus size, format), the level of precision of the resource (thesaurus, lightweight or full-fledged ontology), the degree of structuring, etc. This task may be carried out by using the proper text and/or external pre-existing resources. Various methods for exploiting plain text exist using techniques such as regular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 75–79. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation,</booktitle>
<pages>2231--2234</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="15452" citStr="Levy and Andrew, 2006" startWordPosition="2384" endWordPosition="2387">80.46% for labeling blocs, and an accuracy of 97.23% for labeling discourse relations (Fauconnier et al., 2014). The whole process has been implemented in the LaToe2 tool. 2 http://github.com/fauconnier/LaToe root layer 0 réseaux pair à pair protocoles de communication interopérables layer 1 courrier SMTP IMAP électronique layer 2 messagerie XMPP instantanée layer 3 partage de fichiers en pair à pair protocole BitTorrent layer 4 tchat en salons personnes logiciels RetroShare Marabunta 252 Finally, the extraction of discourse structures of interest may be done easily by means of tree patterns (Levy and Andrew, 2006). 5 From layout discourse structure to terminological structure We wish to elicit possible hypernymy relations from identified discourse structures of interest. This task involves a two-step process. The first step consists in specifying the nature of the relation borne by these structures. The second step aims at identifying the related terms (the relation arguments). These steps have been independently evaluated on an annotated corpus, while the whole system has been evaluated on another not annotated corpus. Corpora and evaluation protocols are described in the next section. 5.1 Corpora and</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the fifth international conference on Language Resources and Evaluation, pages 2231–2234. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="8308" citStr="Mann and Thompson, 1988" startWordPosition="1237" endWordPosition="1240">by analyzing propositional contents. Prior to describing the implemented processes, the underlying principles of our approach will be reported in the next section. 250 3 Underlying principles of our approach We rely on principles of discourse theories and on knowledge models for respectively formalizing text layout and identifying hypernymy relations. 3.1 Discourse analysis of the layout Several discourse theories exist. Their starting point lies in the idea that a text is not just a collection of sentences, but it also includes relations between all these sentences that ensure its coherence (Mann and Thompson, 1988; Asher and Lascarides, 2003). Discourse analysis aims at observing the discourse coherence from a rhetorical point of view (the intention of the author) or from a semantic point of view (the description of the world). A discourse analysis is a three step process: splitting the text into Discourse Units (DU), ensuring the attachment between DUs, and then labeling links between DUs with discourse relations. Discourse relations may be divided into two categories: nucleus-satellite (or subordinate) relations which link an important argument to an argument supporting background information, and mu</context>
<context position="10299" citStr="Mann and Thompson, 1988" startWordPosition="1531" endWordPosition="1534">h results from the discourse analysis of its layout is given (Figure 1). In the following figures, we represent nucleus-satellite relations with solid lines and multi-nuclear relations with dashed lines. 1http://fr.wikipedia.org/wiki/Red´ecentralisation d’Internet We are currently interested in discourse structures displaying the following properties: - n DUs are linked with multi-nuclear relations; - one of these coordinated DU is linked to another DU with a nucleus-satellite relation. Figure 2 gives a representation of such a discourse structure according to the Rhetorical Structure Theory (Mann and Thompson, 1988). Figure 2: Rhetorical representation of the discourse structure of interest Although there is only one explicit nucleussatellite relation, this kind of structure involves n implicit nucleus-satellite relations (between DUO and DUi (2 G i G n)). Indeed, from a discourse point of view, if a DUB is subordinated to a DUi, then all DUk coordinated to DUB, are subordinated to DUi. As mentioned above, this kind of discourse structure encompasses textual structures such as titles/sub-titles and enumerative structures which are frequent in structured documents, and which often convey hypernymy relatio</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>A decision-based approach to rhetorical parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>365--372</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14574" citStr="Marcu, 1999" startWordPosition="2246" endWordPosition="2247">hat tags of markup languages both delimit blocs and give their role. Getting the visual structure is thus straightforward. Conversely, PDF documents do not benefit from such tags. So we used the LAPDF-Text tool (Ramakrishnan et al., 2012) which is based on a geometric analysis for detecting blocs, and we have implemented a machine learning method for labeling these blocs. The features include typographical markers (size of fonts, emphasis markers, etc.) and dispositional one (margins, position in page, etc.). For labeling relations, we used an adapted version of the shift-reduce algorithm as (Marcu, 1999) did. We thus obtain a dependency tree representing the discourse structure of the text layout. We evaluate this process on a corpus of PDF documents (documents written in a markup language pose no problem). Results are good since we obtain an accuracy of 80.46% for labeling blocs, and an accuracy of 97.23% for labeling discourse relations (Fauconnier et al., 2014). The whole process has been implemented in the LaToe2 tool. 2 http://github.com/fauconnier/LaToe root layer 0 réseaux pair à pair protocoles de communication interopérables layer 1 courrier SMTP IMAP électronique layer 2 messagerie </context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. A decision-based approach to rhetorical parsing. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 365–372. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="22811" citStr="Mikolov et al., 2013" startWordPosition="3521" endWordPosition="3524">: Main features for the terms recognition We built two models based on supervised probabilistic classifiers since characteristics of links between a hypernym and a hyponym are different from those between two hyponyms. The first model considers only the edges between layer 0 and layer 1 (hypernym-hyponym link), whereas the second one is dedicated to the edges of remaining layers (hyponym-hyponym link). For this step, we used ACABIT (Daille, 1996) and YaTeA (Aubin and Hamon, 2006) for extracting terms. The cosine similarity is based on a distributional model constructed with the word2vec tool (Mikolov et al., 2013) and the French corpus FrWac (Baroni et al., 2009). We have learned the models using a Maximum Entropy classifier. Features Description Context of a term (bigrams and unigrams of parts of speech) Parts of speech of a term Role of a DU Boolean indicating whether a pair of terms share the same visual properties Value indicating a term position Position of a DU in the whole document For a DU, presence of coordinated DUs For a DU, presence of subordinated DUs Value indicating the level of a DU in the structure of document Returns the last punctuation of a DU Number of tokens in a DU Number of sent</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Bilingual co-training for monolingual hyponymy-relation acquisition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNL,</booktitle>
<pages>432--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7414" citStr="Oh et al., 2009" startWordPosition="1097" endWordPosition="1100">olic as well as machine learning techniques. Our approach is similar to the one followed by Sumida and Torisawa (2008) which analyzes a structured text according to the following steps: (1) they represent the document structure from a limited set of tags (headings, bulleted lists, ordered lists and definition lists), (2) they link two tagged strings when the first one is in the scope of the second one, and (3) they use lexico-syntactic and layout features for selecting hypernymy relations, with the help of a machine learning algorithm. Some attempts have been made for improving these results (Oh et al., 2009; Yamada et al., 2009). However our work differs in two points: we aimed to be more generic by proposing a discourse structure of layout that can be inferred from different document formats, and we propose to find out the relation arguments (hypernym-hyponym term pairs) by analyzing propositional contents. Prior to describing the implemented processes, the underlying principles of our approach will be reported in the next section. 250 3 Underlying principles of our approach We rely on principles of discourse theories and on knowledge models for respectively formalizing text layout and identify</context>
</contexts>
<marker>Oh, Uchimoto, Torisawa, 2009</marker>
<rawString>Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Bilingual co-training for monolingual hyponymy-relation acquisition. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNL, pages 432–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Power</author>
<author>Donia Scott</author>
<author>Nadjet Bouayad-Agha</author>
</authors>
<date>2003</date>
<booktitle>Document structure. Computational Linguistics,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="5580" citStr="Power et al., 2003" startWordPosition="813" endWordPosition="816">egular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task using layout, two categories of approaches may be distinguished. The first one encompasses approaches exploiting documents written in a markup language. The semantics of these tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTM</context>
</contexts>
<marker>Power, Scott, Bouayad-Agha, 2003</marker>
<rawString>Richard Power, Donia Scott, and Nadjet Bouayad-Agha. 2003. Document structure. Computational Linguistics, 29(2):211–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cartic Ramakrishnan</author>
<author>Abhishek Patnia</author>
<author>Eduard H Hovy</author>
<author>Gully APC Burns</author>
</authors>
<title>Layout-aware text extraction from full-text pdf of scientific articles. Source code for biology and medicine,</title>
<date>2012</date>
<pages>7--1</pages>
<contexts>
<context position="14200" citStr="Ramakrishnan et al., 2012" startWordPosition="2187" endWordPosition="2190"> the system detects visuals units and labels them with their role (paragraph, title, footnote, etc.) in the text. Then, it links the labeled units using discourse relations (nucleus-satellite or multi-nuclear) in order to produce a discourse tree. We are currently able to process two types of documents: documents written in a markup language and documents in PDF format. It is obvious that tags of markup languages both delimit blocs and give their role. Getting the visual structure is thus straightforward. Conversely, PDF documents do not benefit from such tags. So we used the LAPDF-Text tool (Ramakrishnan et al., 2012) which is based on a geometric analysis for detecting blocs, and we have implemented a machine learning method for labeling these blocs. The features include typographical markers (size of fonts, emphasis markers, etc.) and dispositional one (margins, position in page, etc.). For labeling relations, we used an adapted version of the shift-reduce algorithm as (Marcu, 1999) did. We thus obtain a dependency tree representing the discourse structure of the text layout. We evaluate this process on a corpus of PDF documents (documents written in a markup language pose no problem). Results are good s</context>
</contexts>
<marker>Ramakrishnan, Patnia, Hovy, Burns, 2012</marker>
<rawString>Cartic Ramakrishnan, Abhishek Patnia, Eduard H Hovy, Gully APC Burns, et al. 2012. Layout-aware text extraction from full-text pdf of scientific articles. Source code for biology and medicine, 7(1):7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S´anchez</author>
<author>Antonio Moreno</author>
</authors>
<title>Web-scale taxonomy learning.</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Extending and Learning Lexical Ontologies using Machine Learning (ICML</booktitle>
<pages>53--60</pages>
<location>Bonn, Germany.</location>
<marker>S´anchez, Moreno, 2005</marker>
<rawString>David S´anchez and Antonio Moreno. 2005. Web-scale taxonomy learning. In Proceedings of Workshop on Extending and Learning Lexical Ontologies using Machine Learning (ICML 2005), pages 53–60, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>17</volume>
<contexts>
<context position="5112" citStr="Snow et al., 2004" startWordPosition="743" endWordPosition="746">he natural language quality (carefully written or informal), the textual genre (scientific, technical documents, newspapers, etc.), technical properties (corpus size, format), the level of precision of the resource (thesaurus, lightweight or full-fledged ontology), the degree of structuring, etc. This task may be carried out by using the proper text and/or external pre-existing resources. Various methods for exploiting plain text exist using techniques such as regular expressions (also known as lexicosyntactic patterns) (Hearst, 1992), classification using supervised or unsupervised learning (Snow et al., 2004; Alfonseca and Manandhar, 2002), distributional analysis (Lenci and Benotto, 2012) or Formal Concepts Analysis (Cimiano et al., 2005). In the Information Retrieval area, the relevant terms are extracted from documents and organized into hierarchies (S´anchez and Moreno, 2005). Works on the document structure and on the discourse relations that it conveys have been carried out by the NLP community. Among these are the Document Structure Theory (Power et al., 2003), and the DArtbio system (Bateman et al., 2001). These approaches offer strong theoretical frameworks, but they were only implemente</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Advances in Neural Information Processing Systems, volume 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6616" citStr="Suchanek et al., 2007" startWordPosition="968" endWordPosition="971">structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a markup language. All these works implement symbolic as well as machine learning techniques. Our approach is similar to the one followed by Sumida and Torisawa (2008) which analyzes a structured text according to the following steps: (1) they represent the document structure from a limited set of tags (headings, bulleted lists, ordered lists and definition lists), (2) they link two tagged strings when the first one is in the scope of the second one, and (3) the</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697–706. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asuka Sumida</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Hacking wikipedia for hyponymy relation acquisition.</title>
<date>2008</date>
<booktitle>In IJCNLP,</booktitle>
<volume>8</volume>
<pages>883--888</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6272" citStr="Sumida and Torisawa, 2008" startWordPosition="916" endWordPosition="919">offer strong theoretical frameworks, but they were only implemented from a text generation point of view. With regard to the relation extraction task using layout, two categories of approaches may be distinguished. The first one encompasses approaches exploiting documents written in a markup language. The semantics of these tags and their nested structure is used to build semantic resources. For instance, collection of XML documents have been analyzed to build ontologies (Kamel and Aussenac-Gilles, 2009), while collection of HTML or MediaWiki documents have been exploited to build taxonomies (Sumida and Torisawa, 2008). The second category gathers approaches exploiting specific documents or parts of documents, for which the semantics of the layout is strictly defined. Let us mention dictionaries and thesaurus (Jannink and Wiederhold, 1999) or specific and well localized textual structures such as category field (Chernov et al., 2006; Suchanek et al., 2007) or infoboxes (Auer et al., 2007) from Wikipedia pages. In some cases, these specific textual structures are also expressed thanks to a markup language. All these works implement symbolic as well as machine learning techniques. Our approach is similar to t</context>
</contexts>
<marker>Sumida, Torisawa, 2008</marker>
<rawString>Asuka Sumida and Kentaro Torisawa. 2008. Hacking wikipedia for hyponymy relation acquisition. In IJCNLP, volume 8, pages 883–888. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuka Tateisi</author>
<author>Tomoko Ohta</author>
<author>Nigel Collier</author>
<author>Chikashi Nobata</author>
<author>Jun-ichi Tsujii</author>
</authors>
<title>Building an annotated corpus in the molecular-biology domain.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING-2000 Workshop on Semantic Annotation and Intelligent Content,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16772" citStr="Tateisi et al., 2000" startWordPosition="2584" endWordPosition="2587">an and environmental planning. 745 discourse structures of interest were annotated by 3 annotators (2 students in Linguistics, and an expert in knowledge engineering) according to a guideline. The annotation task for each discourse structure of interest has consisted in annotating the nucleus-satellite relation as hypernymy or not, and when required, in annotating the terms involved in the relation. For the first stage, we have calculated a degree of inter-annotator agreement (Fleiss et al., 1979) and obtained a kappa of 0.54. The second stage was evaluated as a named entity recognition task (Tateisi et al., 2000) for which we have obtained an F-measure of 79.44. From this dataset, 80% of the discourse structures of interest were randomly chosen to constitute the development set, and the remaining 20% were used for the test set. The tasks described below were tuned on the development set using a k-10 cross-validation. The evaluation is done using the precision, the recall and the F-measure metrics. A second evaluation for the entire system was led on two corpora respectively made of Wikipedia pages from two domains: Transport and Computer Science. For each domain, we have randomly selected 400 pages fr</context>
</contexts>
<marker>Tateisi, Ohta, Collier, Nobata, Tsujii, 2000</marker>
<rawString>Yuka Tateisi, Tomoko Ohta, Nigel Collier, Chikashi Nobata, and Jun-ichi Tsujii. 2000. Building an annotated corpus in the molecular-biology domain. In Proceedings of the COLING-2000 Workshop on Semantic Annotation and Intelligent Content, pages 28–36. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Assaf Urieli</author>
</authors>
<title>Robust French syntax analysis: reconciling statistical methods and linguistic knowledge in the Talismane toolkit.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e de Toulouse.</institution>
<contexts>
<context position="19733" citStr="Urieli, 2013" startWordPosition="3044" endWordPosition="3045">ntropy classifier (MaxEnt) (Berger et al., 1996) and a Support Vector Machine (SVM) with a Gaussian kernel (Cortes and Vapnik, 1995). Features Description Unigrams of parts of speech Position of a token in a DU Boolean indicating whether a token belongs to a predefined lexicon Boolean indicating whether the last sentence of a DU shows a syntactic hole Returns the last punctuation of a DU Number of tokens in a DU Number of sentences in a DU POS Position Markers Gram Punc NbToken NbSent 253 The morphological and lexical information used were obtained from the French dependency parser Talismane (Urieli, 2013). For the classifiers, we have used the OpenNLP3 library for the MaxEnt and the LIBSVM implementation of the SVM4. This task has been evaluated against a majority baseline which better reflects the reality because of the asymmetry of the relation distribution. Table 2 presents the results. The two supervised strategies outperform significantly the baseline (p-values&lt;0.01)5. Strategies Prec. Rec. F1 MaxEnt 78.01 84.78 81.25 SVM 74.77 90.22 81.77 Baseline 63.01 100.0 77.31 Table 2: Results for qualifying the relation Regarding the F-measure metric, the difference between the MaxEnt and the SVM i</context>
</contexts>
<marker>Urieli, 2013</marker>
<rawString>Assaf Urieli. 2013. Robust French syntax analysis: reconciling statistical methods and linguistic knowledge in the Talismane toolkit. Ph.D. thesis, Universit´e de Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ichiro Yamada</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Stijn De Saeger</author>
<author>Francis Bond</author>
<author>Asuka Sumida</author>
</authors>
<title>Hypernym discovery based on distributional similarity and hierarchical structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>929--937</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yamada, Torisawa, Kazama, Kuroda, Murata, De Saeger, Bond, Sumida, 2009</marker>
<rawString>Ichiro Yamada, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, Masaki Murata, Stijn De Saeger, Francis Bond, and Asuka Sumida. 2009. Hypernym discovery based on distributional similarity and hierarchical structures. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 929–937. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>