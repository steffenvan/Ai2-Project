<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.963796">
Weight pushing and binarization for fixed-grammar parsing
</title>
<author confidence="0.998251">
Matt Post and Daniel Gildea
</author>
<affiliation confidence="0.998498">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.337921">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976684" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960888888889">
We apply the idea of weight pushing
(Mohri, 1997) to CKY parsing with fixed
context-free grammars. Applied after
rule binarization, weight pushing takes the
weight from the original grammar rule and
pushes it down across its binarized pieces,
allowing the parser to make better prun-
ing decisions earlier in the parsing pro-
cess. This process can be viewed as gen-
eralizing weight pushing from transduc-
ers to hypergraphs. We examine its ef-
fect on parsing efficiency with various bi-
narization schemes applied to tree sub-
stitution grammars from previous work.
We find that weight pushing produces dra-
matic improvements in efficiency, espe-
cially with small amounts of time and with
large grammars.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999209796296297">
Fixed grammar-parsing refers to parsing that em-
ploys grammars comprising a finite set of rules
that is fixed before inference time. This is in
contrast to markovized grammars (Collins, 1999;
Charniak, 2000), variants of tree-adjoining gram-
mars (Chiang, 2000), or grammars with wildcard
rules (Bod, 2001), all of which allow the con-
struction and use of rules not seen in the training
data. Fixed grammars must be binarized (either
explicitly or implicitly) in order to maintain the
O(n3|G|) (n the sentence length, |G |the grammar
size) complexity of algorithms such as the CKY
algorithm.
Recently, Song et al. (2008) explored different
methods of binarization of a PCFG read directly
from the Penn Treebank (the Treebank PCFG),
showing that binarization has a significant effect
on both the number of rules and new nontermi-
nals introduced, and subsequently on parsing time.
This variation occurs because different binariza-
tion schemes produce different amounts of shared
rules, which are rules produced during the bina-
rization process from more than one rule in the
original grammar. Increasing sharing reduces the
amount of state that the parser must explore. Bina-
rization has also been investigated in the context of
parsing-based approaches to machine translation,
where it has been shown that paying careful atten-
tion to the binarization scheme can produce much
faster decoders (Zhang et al., 2006; Huang, 2007;
DeNero et al., 2009).
The choice of binarization scheme will not af-
fect parsing results if the parser is permitted to ex-
plore the whole search space. In practice, how-
ever, this space is too large, so parsers use prun-
ing to discard unlikely hypotheses. This presents
a problem for bottom-up parsing algorithms be-
cause of the way the probability of a rule is dis-
tributed among its binarized pieces: The standard
approach is to place all of that probability on the
top-level binarized rule, and to set the probabilities
of lower binarized pieces to 1.0. Because these
rules are reconstructed from the bottom up, prun-
ing procedures do not have a good estimate of the
complete cost of a rule until the entire original rule
has been reconstructed. It is preferable to have this
information earlier on, especially for larger rules.
In this paper we adapt the technique of weight
pushing for finite state transducers (Mohri, 1997)
to arbitrary binarizations of context-free grammar
rules. Weight pushing takes the probability (or,
more generally, the weight) of a rule in the origi-
nal grammar and pushes it down across the rule’s
binarized pieces. This helps the parser make bet-
</bodyText>
<page confidence="0.997211">
89
</page>
<note confidence="0.877707">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999855705882353">
ter pruning decisions, and to make them earlier in
the bottom-up parsing process. We investigate this
algorithm with different binarization schemes and
grammars, and find that it improves the time vs.
accuracy tradeoff for parsers roughly proportion-
ally to the size of the grammar being binarized.
This paper extends the work of Song et al.
(2008) in three ways. First, weight pushing fur-
ther reduces the amount of time required for pars-
ing. Second, we apply these techniques to Tree
Substitution Grammars (TSGs) learned from the
Treebank, which are both larger and more accu-
rate than the context-free grammar read directly
from the Treebank.1 Third, we examine the inter-
action between binarization schemes and the in-
exact search heuristic of beam-based and k-best
pruning.
</bodyText>
<sectionHeader confidence="0.974353" genericHeader="method">
2 Weight pushing
</sectionHeader>
<subsectionHeader confidence="0.819399">
2.1 Binarization
</subsectionHeader>
<bodyText confidence="0.990421103448276">
Not all binarization schemes are equivalent in
terms of efficiency of representation. Consider the
grammar in the lefthand column of Figure 1 (rules
1 and 2). If this grammar is right-binarized or
left-binarized, it will produce seven rules, whereas
the optimal binarization (depicted) produces only
5 rules due to the fact that two of them are shared.
Since the complexity of parsing with CKY is a
function of the grammar size as well as the input
sentence length, and since in practice parsing re-
quires significant pruning, having a smaller gram-
mar with maximal shared substructure among the
rules is desirable.
We investigate two kinds of binarization in this
paper. The first is right binarization, in which non-
terminal pairs are collapsed beginning from the
two rightmost children and moving leftward. The
second is a greedy binarization, similar to that of
Schmid (2004), in which the most frequently oc-
curring (grammar-wide) nonterminal pair is col-
lapsed in turn, according to the algorithm given in
Figure 2.
Binarization must ensure that the product of the
probabilities of the binarized pieces is the same as
that of the original rule. The easiest way to do
this is to assign each newly-created binarized rule
a probability of 1.0, and give the top-level rule the
complete probability of the original rule. In the
following subsection, we describe a better way.
</bodyText>
<footnote confidence="0.996165">
1The mean rule rank in a Treebank PCFG is 2.14, while
the mean rank in our sampled TSG is 8.51. See Table 1.
</footnote>
<figureCaption confidence="0.998837">
Figure 1: A two-rule grammar. The greedy
</figureCaption>
<bodyText confidence="0.969726333333333">
binarization algorithm produces the binarization
shown, with the shared structure highlighted. Bi-
narized rules A, B, and C are initially assigned
a probability of 1.0, while rules D and E are as-
signed the original probabilities of rules 2 and 1,
respectively.
</bodyText>
<subsectionHeader confidence="0.999739">
2.2 Weight pushing
</subsectionHeader>
<bodyText confidence="0.9999972">
Spreading the weight of an original rule across
its binarized pieces is complicated by sharing,
because of the constraint that the probability of
shared binarized pieces must be set so that the
product of their probabilities is the same as the
original rule, for each rule the shared piece partici-
pates in. Mohri (1997) introduced weight pushing
as a step in the minimization of weighted finite-
state transducers (FSTs), which addressed a sim-
ilar problem for tasks employing finite-state ma-
chinery. At a high level, weight pushing moves
the weight of a path towards the initial state, sub-
ject to the constraint that the weight of each path
in the FST is unchanged. To do weight pushing,
one first computes for each state q in the trans-
ducer the shortest distance d(q) to any final state.
Let σ(q, a) be the state transition function, deter-
ministically transitioning on input a from state q to
state σ(q, a). Pushing adjusts the weight of each
edge w(e) according to the following formula:
</bodyText>
<equation confidence="0.999688">
w′(e) = d(q)−1 x w(e) x d(σ(q, a)) (1)
</equation>
<bodyText confidence="0.994946333333333">
Mohri (1997, §3.7) and Mohri and Riley (2001)
discuss how these operations can be applied us-
ing various semirings; in this paper we use the
(max, x) semiring. The important observation for
our purposes is that pushing can be thought of as a
sequence of local operations on individual nodes
</bodyText>
<figure confidence="0.998832181818182">
NP
NP
1
Rule
Rule 2
a 77 NN NN PP
the 77 NN NN
NP
D
E
the
A
((77:NN):NN)
(77:NN) NN
77 NN
NP
E
C
PP
I
a
(a:((77:NN):NN))
</figure>
<page confidence="0.968192">
90
</page>
<listItem confidence="0.99563425">
1: function GREEDYBINARIZE(P)
2: while RANK(P) &gt; 2 do
3: K := UPDATECOUNTS(P)
4: for each rule X —* x1x2 · · · xr do
5: b := argmaxiE(2···r) K[xi−1, xi]
6: l := (xb−1 : xb)
7: add l —* xb−1xb to P
8: replace xb−1xb with l in rule
9: function UPDATECOUNTS(P)
10: K := {} D a dictionary
11: for each rule X —* x1x2 · · · xr E P do
12: for i E (2···r) do
</listItem>
<figure confidence="0.398426">
13: K[xi−1, xij++
return K
</figure>
<figureCaption confidence="0.975066">
Figure 2: A greedy binarization algorithm. The
</figureCaption>
<bodyText confidence="0.957408371428571">
rank of a grammar is the rank of its largest rule.
Our implementation updates the counts in K more
efficiently, but we present it this way for clarity.
q, shifting a constant amount of weight d(q)−1
from q’s outgoing edges to its incoming edges.
Klein and Manning (2003) describe an encod-
ing of context-free grammar rule binarization that
permits weight pushing to be applied. Their ap-
proach, however, works only with left or right bi-
narizations whose rules can be encoded as an FST.
We propose a form of weight pushing that works
for arbitrary binarizations. Weight pushing across
a grammar can be viewed as generalizing push-
ing from weighted transducers to a certain kind of
weighted hypergraph. To begin, we use the fol-
lowing definition of a hypergraph:
Definition. A hypergraph H is a tuple
(V, E, F, R), where V is a set of nodes, E is a
set of hyperedges, F C V is a set of final nodes,
and R is a set of permissible weights on the hy-
peredges. Each hyperedge e E E is a triple
(T(e), h(e), w(e)), where h(e) E V is its head
node, T (e) is a sequence of tail nodes, and w(e) is
its weight.
We can arrange the binarized rules of Figure 1
into a shared hypergraph forest (Figure 3), with
nodes as nonterminals and binarized rules as hy-
peredges. We distinguish between final and non-
final nodes and hyperedges. Nonfinal nodes are
those in V −F. Nonfinal hyperdges ENF are those
in {e : h(e) E V − F}, that is, all hyperedges
whose head is a nonfinal node. Because all nodes
introduced by our binarization procedure expand
deterministically, each nonfinal node is the head
of no more than one such hyperedge. Initially, all
</bodyText>
<figure confidence="0.620882">
JJ NN
</figure>
<figureCaption confidence="0.937229333333333">
Figure 3: The binarized rules of Figure 1 arranged
in a shared hypergraph forest. Each hyperedge is
labeled with its weight before/after pushing.
</figureCaption>
<bodyText confidence="0.999928684210526">
nonfinal hyperedges have a probability of 1, and fi-
nal hyperedges have a probability equal to the that
of the original unbinarized rule. Each path through
the forest exactly identifies a binarization of a rule
in the original grammar, and hyperpaths overlap
where binarized rules are shared.
Weight pushing in this hypergraph is similar to
weight pushing in a transducer. We consider each
nonfinal node v in the graph and execute a local
operation that moves weight in some way from the
set of edges {e : v E T(e)} (v’s outgoing hyper-
edges) to the edge eh for which v = h(e) (v’s
incoming hyperedge).
A critical difference from pushing in trans-
ducers is that a node in a hyperpath may be
used more than once. Consider adding the rule
NP—*JJ NN JJ NN to the binarized two-rule gram-
mar we have been considering. Greedy binariza-
tion could2 binarize it in the following manner
</bodyText>
<equation confidence="0.972362">
NP —* (JJ:NN) (JJ:NN)
(JJ:NN) —* JJ NN
</equation>
<bodyText confidence="0.9997766">
which would yield the hypergraph in Figure 4. In
order to maintain hyperpath weights, a pushing
procedure at the (JJ:NN) node must pay attention
to the number of times it appears in the set of tail
nodes of each outgoing hyperedge.
</bodyText>
<figureCaption confidence="0.58069975">
2Depending on the order in which the argmax variable i
of Line 5 from the algorithm in Figure 2 is considered. This
particular binarization would not have been produced if the
values 2 ... r were tested sequentially.
</figureCaption>
<figure confidence="0.998680769230769">
((JJ:NN):NN)
1.0/1.0
(JJ:NN) NN
1.0/0.6
0.4/0.67
the
�
NP
a
(a:((JJ:NN):NN))
1.0/1.0
0.6/1.0
PP
</figure>
<page confidence="0.798553">
91
</page>
<figureCaption confidence="0.78215625">
Figure 4: A hypergraph containing a hyperpath
representing a rule using the same binarized piece
twice. Hyperedge weights are again shown be-
fore/after pushing.
</figureCaption>
<bodyText confidence="0.999949777777778">
With these similarities and differences in mind,
we can define the local weight pushing procedure.
For each nonfinal node v in the hypergraph, we
define eh as the edge for which h(e) = v (as be-
fore), P = {e : v E T(e)} (the set of outgo-
ing hyperedges), and c(v, T (e)) as the number of
times v appears in the sequence of tail nodes T (e).
The minimum amount of probability available for
pushing is then
</bodyText>
<equation confidence="0.515915">
�
max{ c(v,T (e)) w(e) : e E P}
</equation>
<bodyText confidence="0.9999806875">
This amount can then be multiplied into w(eh) and
divided out of each edge e E P. This max is a
lower bound because we have to ensure that the
amount of probability we divide out of the weight
of each outgoing hyperedge is at least as large as
that of the maximum weight.
While finite state transducers each have a
unique equivalent transducer on which no further
pushing is possible, defined by Equation 1, this is
not the case when operating on hypergraphs. In
this generalized setting, the choice of which tail
nodes to push weight across can result in differ-
ent final solutions. We must define a strategy for
choosing among sequences of pushing operations,
and for this we now turn to a discussion of the
specifics of our algorithm.
</bodyText>
<subsectionHeader confidence="0.995587">
2.3 Algorithm
</subsectionHeader>
<bodyText confidence="0.9999085">
We present two variants. Maximal pushing, analo-
gous to weight pushing in weighted FSTs, pushes
the original rule’s weight down as far as pos-
sible. Analysis of interactions between pruning
</bodyText>
<listItem confidence="0.985479142857143">
1: function DIFFUSEWEIGHTS(PBIN, H)
2: R := bottom-up sort of PBIN
3: for each rule r E R do
√p.pr : p E H(r)}
4: r.pr := max{ c(r,p)
5: for each rule p E H(r) do
6: p.pr := p.pr/r.prc(r,p)
</listItem>
<bodyText confidence="0.976011058823529">
Figure 6: Maximal weight pushing algorithm ap-
plied to a binarized grammar, PBIN. H is a dictio-
nary mapping from an internal binary rule to a list
of top-level binary rules that it appeared under.
and maximal pushing discovered situations where
maximal pushing resulted in search error (see
§4.2). To address this, we also discuss nthroot
pushing, which attempts to distribute the weight
more evenly across its pieces, by taking advantage
of the fact that Equation 2 is a lower bound on the
amount of probability available for pushing.
The algorithm for maximal pushing is listed
in Figure 6, and works in the following manner.
When binarizing we maintain, for each binarized
piece, a list of all the original rules that share
it. We then distribute that original rule’s weight
by considering each of these binarized pieces in
bottom-up topological order and setting the prob-
ability of the piece to the maximum (remaining)
probability of these parents. This amount is then
divided out of each of the parents, and the process
continues. See Figure 5 for a depiction of this pro-
cess. Note that, although we defined pushing as a
local operation between adjacent hyperedges, it is
safe to move probability mass from the top-level
directly to the bottom (as we do here). Intuitively,
we can imagine this as a series of local pushing
operations on all intervening nodes; the end result
is the same.
For nthroot pushing, we need to maintain a dic-
tionary S which records, for each binary piece, the
rank (number of items on the rule’s righthand side)
of the original rule it came from. This is accom-
plished by replacing line 4 in Figure 6 with
</bodyText>
<equation confidence="0.554597">
r.pr := max{ (a(p)−1)·c(r,p)√p.pr : p E H(r)}
</equation>
<bodyText confidence="0.9998845">
Applying weight pushing to a binarized PCFG
results in a grammar that is not a PCFG, be-
cause rule probabilities for each lefthand side
no longer sum to one. However, the tree dis-
tribution, as well as the conditional distribution
P(tree|string) (which are what matter for parsing)
are unchanged. To show this, we argue from
the algorithm in Figure 6, demonstrating that, for
</bodyText>
<figure confidence="0.968851333333333">
(JJ:NN) NN
1.0/0.6
JJ
NN
0.1/0.27
�
the
0.3/0.5
((JJ:NN):NN)
NP
a
(a:((JJ:NN):NN))
1.0/1.0
1.0/1.0
0.6/1.0
PP
(2)
92
step A B C D E
0 1.0 1.0 1.0 x y
1
</figure>
<equation confidence="0.991339375">
max(x, y)
x y
· ·max(x,y) max(x,y)
· max z z z1,D z1,D
( 1,D, 1,E) max(z1,D,z1,E) max(z1,D,z1,E)
z2,D z2,E
· ·max(z2,D, z2,E) max(z2,D,z2,E) max(z2,D,z2,E)
· · · · ·
</equation>
<page confidence="0.348719">
2
</page>
<figure confidence="0.743252">
3
4
</figure>
<figureCaption confidence="0.9414905">
Figure 5: Stepping through the maximal weight pushing algorithm for the binarized grammar in Figure 1.
Rule labels A through E were chosen so that the binarized pieces are sorted in topological order. A (·)
indicates a rule whose value has not changed from the previous step, and the value zr,c denotes the value
in row r column c.
</figureCaption>
<bodyText confidence="0.995775142857143">
each rule in the original grammar, its probability
is equal to the product of the probabilities of its
pieces in the binarized grammar. This invariant
holds at the start of the algorithm (because the
probability of each original rule was placed en-
tirely at the top-level rule, and all other pieces re-
ceived a probability of 1.0) and is also true at the
end of each iteration of the outer loop. Consider
this loop. Each iteration considers a single binary
piece (line 3), determines the amount of probabil-
ity to claim from the parents that share it (line 4),
and then removes this amount of weight from each
of its parents (lines 5 and 6). There are two impor-
tant considerations.
</bodyText>
<listItem confidence="0.634484066666667">
1. A binarized rule piece may be used more than
once in the reconstruction of an original rule;
this is important because we are assigning
probabilities to binarized rule types, but rule
reconstruction makes use of binarized rule to-
kens.
2. Multiplying together two probabilities results
in a lower number: when we shift weight p
from the parent rule to (n instances of) a bi-
narized piece beneath it, we are creating a
new set of probabilities pc and pp such that
pnc · pp = p, where pc is the weight placed on
the binarized rule type, and pp is the weight
we leave at the parent. This means that we
must choose pc from the range [p, 1.0].3
</listItem>
<bodyText confidence="0.99633525">
In light of these considerations, the weight re-
moved from each parent rule in line 6 must be
greater than or equal to each parent sharing the
binarized rule piece. To ensure this, line 4 takes
</bodyText>
<footnote confidence="0.539932">
3The upper bound of 1.0 is set to avoid assigning a nega-
tive weight to a rule.
</footnote>
<bodyText confidence="0.999599">
the maximum of the c(r, p)th root of each parent’s
probability, where c(r, p) is the number of times
binarized rule token r appears in the binarization
of p.
Line 4 breaks the invariant, but line 6 restores it
for each parent rule the current piece takes part in.
From this it can be seen that weight pushing does
not change the product of the probabilities of the
binarized pieces for each rule in the grammar, and
hence the tree distribution is also unchanged.
We note that, although Figures 3 and 4 show
only one final node, any number of final nodes can
appear if binarized pieces are shared across differ-
ent top-level nonterminals (which our implemen-
tation permits and which does indeed occur).
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.990517">
We present results from four different grammars:
</bodyText>
<listItem confidence="0.9839904375">
1. The standard Treebank probabilistic context-
free grammar (PCFG).
2. A “spinal” tree substitution grammar (TSG),
produced by extracting n lexicalized subtrees
from each length n sentence in the training
data. Each subtree is defined as the sequence
of CFG rules from leaf upward all sharing the
same lexical head, according to the Mager-
man head-selection rules (Collins, 1999). We
detach the top-level unary rule, and add in
counts from the Treebank CFG rules.
3. A “minimal subset” TSG, extracted and then
refined according to the process defined in
Bod (2001). For each height h, 2 ≤ h ≤ 14,
400,000 subtrees are randomly sampled from
the trees in the training data, and the counts
</listItem>
<page confidence="0.989522">
93
</page>
<table confidence="0.916828666666667">
rank
grammar # rules median mean max
PCFG 46K 1 2.14 51
spinal 190K 3 3.36 51
sampled 804K 8 8.51 70
minimal 2,566K 10 10.22 62
PP
DT JJ NN NN
a
</table>
<tableCaption confidence="0.741751">
Table 1: Grammar statistics. A rule’s rank is the Figure 7: Rule 1 in Figure 1 was produced by
number of symbols on its right-hand side.
</tableCaption>
<table confidence="0.9978338">
grammar unbinarized right greedy
PCFG 46K 56K 51K
spinal 190K 309K 235K
sampled 804K 3,296K 1,894K
minimal 2,566K 15,282K 7,981K
</table>
<tableCaption confidence="0.8213705">
Table 2: Number of rules in each of the complete
grammars before and after binarization.
</tableCaption>
<bodyText confidence="0.984487979591837">
are summed. From these counts we remove
(a) all unlexicalized subtrees of height greater
than six and (b) all lexicalized subtrees con-
taining more than twelve terminals on their
frontier, and we add all subtrees of height one
(i.e., the Treebank PCFG).
4. A sampled TSG produced by inducing
derivations on the training data using a
Dirichlet Process prior (described below).
The sampled TSG was produced by inducing a
TSG derivation on each of the trees in the train-
ing data, from which subtree counts were read di-
rectly. These derivations were induced using a
collapsed Gibbs sampler, which sampled from the
posterior of a Dirichlet process (DP) defined over
the subtree rewrites of each nonterminal. The DP
describes a generative process that prefers small
subtrees but occasionally produces larger ones;
when used for inference, it essentially discovers
TSG derivations that contain larger subtrees only
if they are frequent in the training data, which dis-
courages model overfitting. See Post and Gildea
(2009) for more detail. We ran the sampler for 100
iterations with a stop probability of 0.7 and the DP
parameter α = 100, accumulating subtree counts
from the derivation state at the end of all the itera-
tions, which corresponds to the (100, 0.7, ≤ 100)
grammar from that paper.
All four grammar were learned from all sen-
tences in sections 2 to 21 of the Wall Street Journal
portion of the Penn Treebank. All trees were pre-
processed to remove empty nodes and nontermi-
flattening this rule from the sampled grammar.
nal annotations. Punctuation was retained. Statis-
tics for these grammars can be found in Table 1.
We present results on sentences with no more than
forty words from section 23.
Our parser is a Perl implementation of the CKY
algorithm.4 For the larger grammars, memory lim-
itations require us to remove from consideration
all grammar rules that could not possibly take part
in a parse of the current sentence, which we do by
matching the rule’s frontier lexicalization pattern
against the words in the sentence. All unlexical-
ized rules are kept. This preprocessing time is not
included in the parsing times reported in the next
section.
For pruning, we group edges into equivalence
classes according to the following features:
</bodyText>
<listItem confidence="0.999887">
• span (s, t) of the input
• level of binarization (0,1,2+)
</listItem>
<bodyText confidence="0.999284888888889">
The level of binarization refers to the height of a
nonterminal in the subtree created by binarizing a
CFG rule (with the exception that the root of this
tree has a binarization level of 0). The naming
scheme used to create new nonterminals in line 6
of Figure 2 means we can determine this level by
counting the number of left-angle brackets in the
nonterminal’s name. In Figure 1, binarized rules
D and E have level 0, C has level 3, B has level 2,
and A has level 1.
Within each bin, only the Q highest-weight
items are kept, where Q E (1, 5, 10, 25, 50) is apa-
rameter that we vary during our experiments. Ties
are broken arbitrarily. Additionally, we maintain a
beam within each bin, and an edge is pruned if its
score is not within a factor of 10−5 of the highest-
scoring edge in the bin. Pruning takes place when
the edge is added and then again at the end of each
</bodyText>
<footnote confidence="0.991916">
4It is available from http://www.cs.rochester.
edu/˜post/.
</footnote>
<page confidence="0.999503">
94
</page>
<bodyText confidence="0.999942833333333">
span in the CKY algorithm (but before applying
unary rules).
In order to binarize TSG subtrees, we follow
Bod (2001) in first flattening each subtree to a
depth-one PCFG rule that shares the subtree’s root
nonterminal and leaves, as depicted in Figure 7.
Afterward, this transformation is reversed to pro-
duce the parse tree for scoring. If multiple TSG
subtrees have identical mappings, we take only the
most probable one. Table 2 shows how grammar
size is affected by binarization scheme.
We note two differences in our work that ex-
plain the large difference between the scores re-
ported for the “minimal subset” grammar in Bod
(2001) and here. First, we did not implement the
smoothed “mismatch parsing”, which introduces
new subtrees into the grammar at parsing time by
allowing lexical leaves of subtrees to act as wild-
cards. This technique reportedly makes a large
difference in parsing scores (Bod, 2009). Second,
we approximate the most probable parse with the
single most probable derivation instead of the top
1,000 derivations, which Bod also reports as hav-
ing a large impact (Bod, 2003, §4.2).
</bodyText>
<sectionHeader confidence="0.999938" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.998035631578947">
Figure 8 displays search time vs. model score for
the PCFG and the sampled grammar. Weight
pushing has a significant impact on search effi-
ciency, particularly for the larger sampled gram-
mar. The spinal and minimal graphs are similar to
the PCFG and sampled graphs, respectively, which
suggests that the technique is more effective for
the larger grammars.
For parsing, we are ultimately interested in ac-
curacy as measured by F1 score.5 Figure 9 dis-
plays graphs of time vs. accuracy for parses with
each of the grammars, alongside the numerical
scores used to generate them. We begin by noting
that the improved search efficiency from Figure 8
carries over to the time vs. accuracy curves for
the PCFG and sampled grammars, as we expect.
Once again, we note that the difference is less pro-
nounced for the two smaller grammars than for the
two larger ones.
</bodyText>
<subsectionHeader confidence="0.998569">
4.1 Model score vs. accuracy
</subsectionHeader>
<bodyText confidence="0.999971333333333">
The tables in Figure 9 show that parser accuracy
is not always a monotonic function of time; some
of the runs exhibited peak performance as early
</bodyText>
<equation confidence="0.931312">
5F1 = 2·P ·R
P +R , where P is precision and R recall.
</equation>
<figureCaption confidence="0.850720333333333">
Figure 8: Time vs. model score for the PCFG (top)
and the sampled grammar (bottom). Note that the
y-axis range differs between plots.
</figureCaption>
<bodyText confidence="0.999848">
as at a bin size of β = 10, and then saw drops
in scores when given more time. We examined
a number of instances where the F1 score for a
sentence was lower at a higher bin setting, and
found that they can be explained as modeling (as
opposed to search) errors. With the PCFG, these
errors were standard parser difficulties, such as PP
attachment, which require more context to resolve.
TSG subtrees, which have more context, are able
to correct some of these issues, but introduce a dif-
ferent set of problems. In many situations, larger
bin settings permitted erroneous analyses to re-
main in the chart, which later led to the parser’s
discovery of a large TSG fragment. Because these
fragments often explain a significant portion of the
sentence more cheaply than multiple smaller rules
multiplied together, the parser prefers them. More
often than not, they are useful, but sometimes they
are overfit to the training data, and result in an in-
correct analysis despite a higher model score.
Interestingly, these dips occur most frequently
for the heuristically extracted TSGs (four of six
</bodyText>
<figure confidence="0.996640423076923">
1 5 10 25 50
1 5 10 25 50
mean time per sentence (s)
model score (thousands)
-320
-322
-324
-326
-328
-330
-332
-334
-336
-338
-340
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
model score (thousands)
-290
-300
-310
-320
-330
-340
-350
-360
-370
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
95
PCFG
run 1 5 10 25 50
0 (g,m) 66.44 72.45 72.54 72.54 72.51
• (g,n) 65.44 72.21 72.47 72.45 72.47
A (g,-) 63.91 71.91 72.48 72.51 72.51
❑ (r,m) 67.30 72.45 72.61 72.47 72.49
O (r,n) 64.09 71.78 72.33 72.45 72.47
A (r,-) 61.82 71.00 72.18 72.42 72.41
spinal 1 5 10 25 50
run
0 (g,m) 68.33 78.35 79.21 79.25 79.24
• (g,n) 64.67 78.46 79.04 79.07 79.09
A (g,-) 61.44 77.73 78.94 79.11 79.20
❑ (r,m) 69.92 79.07 79.18 79.25 79.05
O (r,n) 67.76 78.46 79.07 79.04 79.04
A (r,-) 65.27 77.34 78.64 78.94 78.90
sampled 1 5 10 25 50
run
0 (g,m) 63.75 80.65 81.86 82.40 82.41
• (g,n) 61.87 79.88 81.35 82.10 82.17
A (g,-) 53.88 78.68 80.48 81.72 81.98
❑ (r,m) 72.98 81.66 82.37 82.49 82.40
O (r,n) 65.53 79.01 80.81 81.91 82.13
A (r,-) 61.82 77.33 79.72 81.13 81.70
minimal
run 1 5 10 25 50
0 (g,m) 59.75 77.28 77.77 78.47 78.52
• (g,n) 57.54 77.12 77.82 78.35 78.36
A (g,-) 51.00 75.52 77.21 78.30 78.13
❑ (r,m) 65.29 76.14 77.33 78.34 78.13
O (r,n) 61.63 75.08 76.80 77.97 78.31
A (r,-) 59.10 73.42 76.34 77.88 77.91
85
80
75
accuracy
70
65
60
55
50
(greedy,max)
(greedy,none)
(right,max)
(right,none)
1 5 10 25 50
mean time per sentence (s)
1 5 10 25 50
mean time per sentence (s)
1 5 10 25 50
mean time per sentence (s)
1 5 10 25 50
mean time per sentence (s)
85
80
75
accuracy
70
65
60
55
50
(greedy,max)
(greedy,none)
(right,max)
(right,none)
85
80
75
accuracy
70
65
60
55
50
(greedy,max)
(greedy,none)
(right,max)
(right,none)
85
80
75
accuracy
70
65
60
55
50
(greedy,max)
(greedy,none)
(right,max)
(right,none)
</figure>
<figureCaption confidence="0.992984333333333">
Figure 9: Plots of parsing time vs. accuracy for each of the grammars. Each plot contains four sets of five
points (Q E (1, 5, 10, 25, 50)), varying the binarization strategy (right (r) or greedy (g)) and the weight
pushing technique (maximal (m) or none (-)). The tables also include data from nthroot (n) pushing.
</figureCaption>
<page confidence="0.969707">
96
</page>
<bodyText confidence="0.854263111111111">
Figure 10: Time vs. accuracy (Fl) for the sampled
grammar, broken down by binarization (right on
top, greedy on bottom).
runs for the spinal grammar, and two for the min-
imal grammar) and for the PCFG (four), and least
often for the model-based sampled grammar (just
once). This may suggest that rules selected by our
sampling procedure are less prone to overfitting on
the training data.
</bodyText>
<subsectionHeader confidence="0.977466">
4.2 Pushing
</subsectionHeader>
<bodyText confidence="0.999982296296296">
Figure 10 compares the nthroot and maximal
pushing techniques for both binarizations of the
sampled grammar. We can see from this figure
that there is little difference between the two tech-
niques for the greedy binarization and a large dif-
ference for the right binarization. Our original mo-
tivation in developing nthroot pushing came as a
result of analysis of certain sentences where max-
imal pushing and greedy binarization resulted in
the parser producing a lower model score than
with right binarization with no pushing. One such
example was binarized fragment A from Fig-
ure 1; when parsing a particular sentence in the
development set, the correct analysis required the
rule from Figure 7, but greedy binarization and
maximal pushing resulted in this piece getting
pruned early in the search procedure. This pruning
happened because maximal pushing allowed too
much weight to shift down for binarized pieces of
competing analyses relative to the correct analy-
sis. Using nthroot pushing solved the search prob-
lem in that instance, but in the aggregate it does
not appear to be helpful in improving parser effi-
ciency as much as maximal pushing. This demon-
strates some of the subtle interactions between bi-
narization and weight pushing when inexact prun-
ing heuristics are applied.
</bodyText>
<subsectionHeader confidence="0.997124">
4.3 Binarization
</subsectionHeader>
<bodyText confidence="0.9999905">
Song et al. (2008, Table 4) showed that CKY pars-
ing efficiency is not a monotonic function of the
number of constituents produced; that is, enumer-
ating fewer edges in the dynamic programming
chart does not always correspond with shorter run
times. We see here that efficiency does not al-
ways perfectly correlate with grammar size, ei-
ther. For all but the PCFG, right binarization
improves upon greedy binarization, regardless of
the pushing technique, despite the fact that the
right-binarized grammars are always larger than
the greedily-binarized ones.
Weight pushing and greedy binarization both in-
crease parsing efficiency, and the graphs in Fig-
ures 8 and 9 suggest that they are somewhat com-
plementary. We also investigated left binarization,
but discontinued that exploration because the re-
sults were nearly identical to that of right bina-
rization. Another popular binarization approach
is head-outward binarization. Based on the anal-
ysis above, we suspect that its performance will
fall somewhere among the binarizations presented
here, and that pushing will improve it as well. We
hope to investigate this in future work.
</bodyText>
<sectionHeader confidence="0.999083" genericHeader="conclusions">
5 Summary
</sectionHeader>
<bodyText confidence="0.999871636363637">
Weight pushing increases parser efficiency, espe-
cially for large grammars. Most notably, it im-
proves parser efficiency for the Gibbs-sampled
tree substitution grammar of Post and Gildea
(2009).
We believe this approach could alo bene-
fit syntax-based machine translation. Zhang et
al. (2006) introduced a synchronous binariza-
tion technique that improved decoding efficiency
and accuracy by ensuring that rule binarization
avoided gaps on both the source and target sides
</bodyText>
<figure confidence="0.999260259259259">
1 5 10 25 50
1 5 10 25 50
mean time per sentence (s)
accuracy
85
80
75
70
65
60
55
50
(right,max)
(right,nthroot)
(right,none)
accuracy
85
80
75
70
65
60
55
50
(greedy,max)
(greedy,nthroot)
(greedy,none)
</figure>
<page confidence="0.997063">
97
</page>
<bodyText confidence="0.9999703">
(for rules where this was possible). Their binariza-
tion was designed to share binarized pieces among
rules, but their approach to distributing weight was
the default (nondiffused) case found in this paper
to be least efficient: The entire weight of the orig-
inal rule is placed at the top binarized rule and all
internal rules are assigned a probability of 1.0.
Finally, we note that the weight pushing algo-
rithm described in this paper began with a PCFG
and ensured that the tree distribution was not
changed. However, weight pushing need not be
limited to a probabilistic interpretation, but could
be used to spread weights for grammars with dis-
criminatively trained features as well, with neces-
sary adjustments to deal with positively and nega-
tively weighted rules.
Acknowledgments We thank the anonymous
reviewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and
ITR-0428020.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99972015625">
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of the 39th Annual Conference of the As-
sociation for Computational Linguistics (ACL-01),
Toulouse, France.
Rens Bod. 2003. Do all fragments count? Natural
Language Engineering, 9(4):307–323.
Rens Bod. 2009. Personal communication.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Meet-
ing of the North American chapter of the Association
for Computational Linguistics (NAACL-00), Seattle,
Washington.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Conference of the
Association for Computational Linguistics (ACL-
00), Hong Kong.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proceedings of the 2009 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-09), Boulder, Col-
orado.
Liang Huang. 2007. Binarization, synchronous bi-
narization, and target-side binarization. In North
American chapter of the Association for Computa-
tional Linguistics Workshop on Syntax and Struc-
ture in Statistical Translation (NAACL-SSST-07),
Rochester, NY.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03), Edmonton, Alberta.
Mehryar Mohri and Michael Riley. 2001. A weight
pushing algorithm for large vocabulary speech
recognition. In European Conference on Speech
Communication and Technology, pages 1603–1606.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269–311.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-09), Suntec, Singapore.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-08), Honolulu, Hawaii.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the 2006 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-06),
New York, NY.
</reference>
<page confidence="0.996246">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969215">
<title confidence="0.996296">Weight pushing and binarization for fixed-grammar parsing</title>
<author confidence="0.989598">Post</author>
<affiliation confidence="0.9998135">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.999139263157895">apply the idea of pushing (Mohri, 1997) to CKY parsing with fixed context-free grammars. Applied after rule binarization, weight pushing takes the weight from the original grammar rule and pushes it down across its binarized pieces, allowing the parser to make better pruning decisions earlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>What is the minimal set of fragments that achieves maximal parse accuracy?</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01),</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="1180" citStr="Bod, 2001" startWordPosition="181" endWordPosition="182">rs to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3|G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both the number of rules and new nonterminals introduced, and subsequently on parsing time. This variation occurs because</context>
<context position="18701" citStr="Bod (2001)" startWordPosition="3253" endWordPosition="3254">e present results from four different grammars: 1. The standard Treebank probabilistic contextfree grammar (PCFG). 2. A “spinal” tree substitution grammar (TSG), produced by extracting n lexicalized subtrees from each length n sentence in the training data. Each subtree is defined as the sequence of CFG rules from leaf upward all sharing the same lexical head, according to the Magerman head-selection rules (Collins, 1999). We detach the top-level unary rule, and add in counts from the Treebank CFG rules. 3. A “minimal subset” TSG, extracted and then refined according to the process defined in Bod (2001). For each height h, 2 ≤ h ≤ 14, 400,000 subtrees are randomly sampled from the trees in the training data, and the counts 93 rank grammar # rules median mean max PCFG 46K 1 2.14 51 spinal 190K 3 3.36 51 sampled 804K 8 8.51 70 minimal 2,566K 10 10.22 62 PP DT JJ NN NN a Table 1: Grammar statistics. A rule’s rank is the Figure 7: Rule 1 in Figure 1 was produced by number of symbols on its right-hand side. grammar unbinarized right greedy PCFG 46K 56K 51K spinal 190K 309K 235K sampled 804K 3,296K 1,894K minimal 2,566K 15,282K 7,981K Table 2: Number of rules in each of the complete grammars befor</context>
<context position="22681" citStr="Bod (2001)" startWordPosition="3953" endWordPosition="3954">has level 2, and A has level 1. Within each bin, only the Q highest-weight items are kept, where Q E (1, 5, 10, 25, 50) is aparameter that we vary during our experiments. Ties are broken arbitrarily. Additionally, we maintain a beam within each bin, and an edge is pruned if its score is not within a factor of 10−5 of the highestscoring edge in the bin. Pruning takes place when the edge is added and then again at the end of each 4It is available from http://www.cs.rochester. edu/˜post/. 94 span in the CKY algorithm (but before applying unary rules). In order to binarize TSG subtrees, we follow Bod (2001) in first flattening each subtree to a depth-one PCFG rule that shares the subtree’s root nonterminal and leaves, as depicted in Figure 7. Afterward, this transformation is reversed to produce the parse tree for scoring. If multiple TSG subtrees have identical mappings, we take only the most probable one. Table 2 shows how grammar size is affected by binarization scheme. We note two differences in our work that explain the large difference between the scores reported for the “minimal subset” grammar in Bod (2001) and here. First, we did not implement the smoothed “mismatch parsing”, which intr</context>
</contexts>
<marker>Bod, 2001</marker>
<rawString>Rens Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy? In Proceedings of the 39th Annual Conference of the Association for Computational Linguistics (ACL-01), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Do all fragments count?</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="23660" citStr="Bod, 2003" startWordPosition="4115" endWordPosition="4116">me. We note two differences in our work that explain the large difference between the scores reported for the “minimal subset” grammar in Bod (2001) and here. First, we did not implement the smoothed “mismatch parsing”, which introduces new subtrees into the grammar at parsing time by allowing lexical leaves of subtrees to act as wildcards. This technique reportedly makes a large difference in parsing scores (Bod, 2009). Second, we approximate the most probable parse with the single most probable derivation instead of the top 1,000 derivations, which Bod also reports as having a large impact (Bod, 2003, §4.2). 4 Results Figure 8 displays search time vs. model score for the PCFG and the sampled grammar. Weight pushing has a significant impact on search efficiency, particularly for the larger sampled grammar. The spinal and minimal graphs are similar to the PCFG and sampled graphs, respectively, which suggests that the technique is more effective for the larger grammars. For parsing, we are ultimately interested in accuracy as measured by F1 score.5 Figure 9 displays graphs of time vs. accuracy for parses with each of the grammars, alongside the numerical scores used to generate them. We begi</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. Do all fragments count? Natural Language Engineering, 9(4):307–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<date>2009</date>
<tech>Personal communication.</tech>
<contexts>
<context position="23474" citStr="Bod, 2009" startWordPosition="4084" endWordPosition="4085">o produce the parse tree for scoring. If multiple TSG subtrees have identical mappings, we take only the most probable one. Table 2 shows how grammar size is affected by binarization scheme. We note two differences in our work that explain the large difference between the scores reported for the “minimal subset” grammar in Bod (2001) and here. First, we did not implement the smoothed “mismatch parsing”, which introduces new subtrees into the grammar at parsing time by allowing lexical leaves of subtrees to act as wildcards. This technique reportedly makes a large difference in parsing scores (Bod, 2009). Second, we approximate the most probable parse with the single most probable derivation instead of the top 1,000 derivations, which Bod also reports as having a large impact (Bod, 2003, §4.2). 4 Results Figure 8 displays search time vs. model score for the PCFG and the sampled grammar. Weight pushing has a significant impact on search efficiency, particularly for the larger sampled grammar. The spinal and minimal graphs are similar to the PCFG and sampled graphs, respectively, which suggests that the technique is more effective for the larger grammars. For parsing, we are ultimately interest</context>
</contexts>
<marker>Bod, 2009</marker>
<rawString>Rens Bod. 2009. Personal communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-00),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="1083" citStr="Charniak, 2000" startWordPosition="167" endWordPosition="168">rlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3|G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both the number of rules</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 2000 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-00), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL00),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="1135" citStr="Chiang, 2000" startWordPosition="174" endWordPosition="175">ed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3|G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both the number of rules and new nonterminals introduced, and subsequently o</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Conference of the Association for Computational Linguistics (ACL00), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="1066" citStr="Collins, 1999" startWordPosition="165" endWordPosition="166">ng decisions earlier in the parsing process. This process can be viewed as generalizing weight pushing from transducers to hypergraphs. We examine its effect on parsing efficiency with various binarization schemes applied to tree substitution grammars from previous work. We find that weight pushing produces dramatic improvements in efficiency, especially with small amounts of time and with large grammars. 1 Introduction Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3|G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both th</context>
<context position="18516" citStr="Collins, 1999" startWordPosition="3221" endWordPosition="3222">umber of final nodes can appear if binarized pieces are shared across different top-level nonterminals (which our implementation permits and which does indeed occur). 3 Experimental setup We present results from four different grammars: 1. The standard Treebank probabilistic contextfree grammar (PCFG). 2. A “spinal” tree substitution grammar (TSG), produced by extracting n lexicalized subtrees from each length n sentence in the training data. Each subtree is defined as the sequence of CFG rules from leaf upward all sharing the same lexical head, according to the Magerman head-selection rules (Collins, 1999). We detach the top-level unary rule, and add in counts from the Treebank CFG rules. 3. A “minimal subset” TSG, extracted and then refined according to the process defined in Bod (2001). For each height h, 2 ≤ h ≤ 14, 400,000 subtrees are randomly sampled from the trees in the training data, and the counts 93 rank grammar # rules median mean max PCFG 46K 1 2.14 51 spinal 190K 3 3.36 51 sampled 804K 8 8.51 70 minimal 2,566K 10 10.22 62 PP DT JJ NN NN a Table 1: Grammar statistics. A rule’s rank is the Figure 7: Rule 1 in Figure 1 was produced by number of symbols on its right-hand side. grammar</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09),</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="2313" citStr="DeNero et al., 2009" startWordPosition="360" endWordPosition="363">onterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning pro</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proceedings of the 2009 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-09), Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Binarization, synchronous binarization, and target-side binarization.</title>
<date>2007</date>
<booktitle>In North American chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation (NAACL-SSST-07),</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="2291" citStr="Huang, 2007" startWordPosition="358" endWordPosition="359">les and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the </context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Liang Huang. 2007. Binarization, synchronous binarization, and target-side binarization. In North American chapter of the Association for Computational Linguistics Workshop on Syntax and Structure in Statistical Translation (NAACL-SSST-07), Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03),</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="8376" citStr="Klein and Manning (2003)" startWordPosition="1408" endWordPosition="1411">TS(P) 4: for each rule X —* x1x2 · · · xr do 5: b := argmaxiE(2···r) K[xi−1, xi] 6: l := (xb−1 : xb) 7: add l —* xb−1xb to P 8: replace xb−1xb with l in rule 9: function UPDATECOUNTS(P) 10: K := {} D a dictionary 11: for each rule X —* x1x2 · · · xr E P do 12: for i E (2···r) do 13: K[xi−1, xij++ return K Figure 2: A greedy binarization algorithm. The rank of a grammar is the rank of its largest rule. Our implementation updates the counts in K more efficiently, but we present it this way for clarity. q, shifting a constant amount of weight d(q)−1 from q’s outgoing edges to its incoming edges. Klein and Manning (2003) describe an encoding of context-free grammar rule binarization that permits weight pushing to be applied. Their approach, however, works only with left or right binarizations whose rules can be encoded as an FST. We propose a form of weight pushing that works for arbitrary binarizations. Weight pushing across a grammar can be viewed as generalizing pushing from weighted transducers to a certain kind of weighted hypergraph. To begin, we use the following definition of a hypergraph: Definition. A hypergraph H is a tuple (V, E, F, R), where V is a set of nodes, E is a set of hyperedges, F C V is</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-03), Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>A weight pushing algorithm for large vocabulary speech recognition.</title>
<date>2001</date>
<booktitle>In European Conference on Speech Communication and Technology,</booktitle>
<pages>1603--1606</pages>
<contexts>
<context position="7311" citStr="Mohri and Riley (2001)" startWordPosition="1192" endWordPosition="1195">lem for tasks employing finite-state machinery. At a high level, weight pushing moves the weight of a path towards the initial state, subject to the constraint that the weight of each path in the FST is unchanged. To do weight pushing, one first computes for each state q in the transducer the shortest distance d(q) to any final state. Let σ(q, a) be the state transition function, deterministically transitioning on input a from state q to state σ(q, a). Pushing adjusts the weight of each edge w(e) according to the following formula: w′(e) = d(q)−1 x w(e) x d(σ(q, a)) (1) Mohri (1997, §3.7) and Mohri and Riley (2001) discuss how these operations can be applied using various semirings; in this paper we use the (max, x) semiring. The important observation for our purposes is that pushing can be thought of as a sequence of local operations on individual nodes NP NP 1 Rule Rule 2 a 77 NN NN PP the 77 NN NN NP D E the A ((77:NN):NN) (77:NN) NN 77 NN NP E C PP I a (a:((77:NN):NN)) 90 1: function GREEDYBINARIZE(P) 2: while RANK(P) &gt; 2 do 3: K := UPDATECOUNTS(P) 4: for each rule X —* x1x2 · · · xr do 5: b := argmaxiE(2···r) K[xi−1, xi] 6: l := (xb−1 : xb) 7: add l —* xb−1xb to P 8: replace xb−1xb with l in rule 9</context>
</contexts>
<marker>Mohri, Riley, 2001</marker>
<rawString>Mehryar Mohri and Michael Riley. 2001. A weight pushing algorithm for large vocabulary speech recognition. In European Conference on Speech Communication and Technology, pages 1603–1606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="3215" citStr="Mohri, 1997" startWordPosition="516" endWordPosition="517">ecause of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstructed from the bottom up, pruning procedures do not have a good estimate of the complete cost of a rule until the entire original rule has been reconstructed. It is preferable to have this information earlier on, especially for larger rules. In this paper we adapt the technique of weight pushing for finite state transducers (Mohri, 1997) to arbitrary binarizations of context-free grammar rules. Weight pushing takes the probability (or, more generally, the weight) of a rule in the original grammar and pushes it down across the rule’s binarized pieces. This helps the parser make bet89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, Paris, October 2009. c�2009 Association for Computational Linguistics ter pruning decisions, and to make them earlier in the bottom-up parsing process. We investigate this algorithm with different binarization schemes and grammars, and find that it improv</context>
<context position="6558" citStr="Mohri (1997)" startWordPosition="1061" endWordPosition="1062">r. The greedy binarization algorithm produces the binarization shown, with the shared structure highlighted. Binarized rules A, B, and C are initially assigned a probability of 1.0, while rules D and E are assigned the original probabilities of rules 2 and 1, respectively. 2.2 Weight pushing Spreading the weight of an original rule across its binarized pieces is complicated by sharing, because of the constraint that the probability of shared binarized pieces must be set so that the product of their probabilities is the same as the original rule, for each rule the shared piece participates in. Mohri (1997) introduced weight pushing as a step in the minimization of weighted finitestate transducers (FSTs), which addressed a similar problem for tasks employing finite-state machinery. At a high level, weight pushing moves the weight of a path towards the initial state, subject to the constraint that the weight of each path in the FST is unchanged. To do weight pushing, one first computes for each state q in the transducer the shortest distance d(q) to any final state. Let σ(q, a) be the state transition function, deterministically transitioning on input a from state q to state σ(q, a). Pushing adju</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):269–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09),</booktitle>
<location>Suntec, Singapore.</location>
<contexts>
<context position="20340" citStr="Post and Gildea (2009)" startWordPosition="3533" endWordPosition="3536">was produced by inducing a TSG derivation on each of the trees in the training data, from which subtree counts were read directly. These derivations were induced using a collapsed Gibbs sampler, which sampled from the posterior of a Dirichlet process (DP) defined over the subtree rewrites of each nonterminal. The DP describes a generative process that prefers small subtrees but occasionally produces larger ones; when used for inference, it essentially discovers TSG derivations that contain larger subtrees only if they are frequent in the training data, which discourages model overfitting. See Post and Gildea (2009) for more detail. We ran the sampler for 100 iterations with a stop probability of 0.7 and the DP parameter α = 100, accumulating subtree counts from the derivation state at the end of all the iterations, which corresponds to the (100, 0.7, ≤ 100) grammar from that paper. All four grammar were learned from all sentences in sections 2 to 21 of the Wall Street Journal portion of the Penn Treebank. All trees were preprocessed to remove empty nodes and nontermiflattening this rule from the sampled grammar. nal annotations. Punctuation was retained. Statistics for these grammars can be found in Tab</context>
<context position="31219" citStr="Post and Gildea (2009)" startWordPosition="5403" endWordPosition="5406"> also investigated left binarization, but discontinued that exploration because the results were nearly identical to that of right binarization. Another popular binarization approach is head-outward binarization. Based on the analysis above, we suspect that its performance will fall somewhere among the binarizations presented here, and that pushing will improve it as well. We hope to investigate this in future work. 5 Summary Weight pushing increases parser efficiency, especially for large grammars. Most notably, it improves parser efficiency for the Gibbs-sampled tree substitution grammar of Post and Gildea (2009). We believe this approach could alo benefit syntax-based machine translation. Zhang et al. (2006) introduced a synchronous binarization technique that improved decoding efficiency and accuracy by ensuring that rule binarization avoided gaps on both the source and target sides 1 5 10 25 50 1 5 10 25 50 mean time per sentence (s) accuracy 85 80 75 70 65 60 55 50 (right,max) (right,nthroot) (right,none) accuracy 85 80 75 70 65 60 55 50 (greedy,max) (greedy,nthroot) (greedy,none) 97 (for rules where this was possible). Their binarization was designed to share binarized pieces among rules, but the</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL-09), Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5314" citStr="Schmid (2004)" startWordPosition="849" endWordPosition="850">inarization (depicted) produces only 5 rules due to the fact that two of them are shared. Since the complexity of parsing with CKY is a function of the grammar size as well as the input sentence length, and since in practice parsing requires significant pruning, having a smaller grammar with maximal shared substructure among the rules is desirable. We investigate two kinds of binarization in this paper. The first is right binarization, in which nonterminal pairs are collapsed beginning from the two rightmost children and moving leftward. The second is a greedy binarization, similar to that of Schmid (2004), in which the most frequently occurring (grammar-wide) nonterminal pair is collapsed in turn, according to the algorithm given in Figure 2. Binarization must ensure that the product of the probabilities of the binarized pieces is the same as that of the original rule. The easiest way to do this is to assign each newly-created binarized rule a probability of 1.0, and give the top-level rule the complete probability of the original rule. In the following subsection, we describe a better way. 1The mean rule rank in a Treebank PCFG is 2.14, while the mean rank in our sampled TSG is 8.51. See Tabl</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinying Song</author>
<author>Shilin Ding</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Better binarization for the CKY parsing.</title>
<date>2008</date>
<booktitle>In 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1493" citStr="Song et al. (2008)" startWordPosition="232" endWordPosition="235">n Fixed grammar-parsing refers to parsing that employs grammars comprising a finite set of rules that is fixed before inference time. This is in contrast to markovized grammars (Collins, 1999; Charniak, 2000), variants of tree-adjoining grammars (Chiang, 2000), or grammars with wildcard rules (Bod, 2001), all of which allow the construction and use of rules not seen in the training data. Fixed grammars must be binarized (either explicitly or implicitly) in order to maintain the O(n3|G|) (n the sentence length, |G |the grammar size) complexity of algorithms such as the CKY algorithm. Recently, Song et al. (2008) explored different methods of binarization of a PCFG read directly from the Penn Treebank (the Treebank PCFG), showing that binarization has a significant effect on both the number of rules and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context </context>
<context position="3977" citStr="Song et al. (2008)" startWordPosition="631" endWordPosition="634">e original grammar and pushes it down across the rule’s binarized pieces. This helps the parser make bet89 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89–98, Paris, October 2009. c�2009 Association for Computational Linguistics ter pruning decisions, and to make them earlier in the bottom-up parsing process. We investigate this algorithm with different binarization schemes and grammars, and find that it improves the time vs. accuracy tradeoff for parsers roughly proportionally to the size of the grammar being binarized. This paper extends the work of Song et al. (2008) in three ways. First, weight pushing further reduces the amount of time required for parsing. Second, we apply these techniques to Tree Substitution Grammars (TSGs) learned from the Treebank, which are both larger and more accurate than the context-free grammar read directly from the Treebank.1 Third, we examine the interaction between binarization schemes and the inexact search heuristic of beam-based and k-best pruning. 2 Weight pushing 2.1 Binarization Not all binarization schemes are equivalent in terms of efficiency of representation. Consider the grammar in the lefthand column of Figure</context>
<context position="29904" citStr="Song et al. (2008" startWordPosition="5199" endWordPosition="5202">reedy binarization and maximal pushing resulted in this piece getting pruned early in the search procedure. This pruning happened because maximal pushing allowed too much weight to shift down for binarized pieces of competing analyses relative to the correct analysis. Using nthroot pushing solved the search problem in that instance, but in the aggregate it does not appear to be helpful in improving parser efficiency as much as maximal pushing. This demonstrates some of the subtle interactions between binarization and weight pushing when inexact pruning heuristics are applied. 4.3 Binarization Song et al. (2008, Table 4) showed that CKY parsing efficiency is not a monotonic function of the number of constituents produced; that is, enumerating fewer edges in the dynamic programming chart does not always correspond with shorter run times. We see here that efficiency does not always perfectly correlate with grammar size, either. For all but the PCFG, right binarization improves upon greedy binarization, regardless of the pushing technique, despite the fact that the right-binarized grammars are always larger than the greedily-binarized ones. Weight pushing and greedy binarization both increase parsing e</context>
</contexts>
<marker>Song, Ding, Lin, 2008</marker>
<rawString>Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better binarization for the CKY parsing. In 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-06),</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="2278" citStr="Zhang et al., 2006" startWordPosition="354" endWordPosition="357">oth the number of rules and new nonterminals introduced, and subsequently on parsing time. This variation occurs because different binarization schemes produce different amounts of shared rules, which are rules produced during the binarization process from more than one rule in the original grammar. Increasing sharing reduces the amount of state that the parser must explore. Binarization has also been investigated in the context of parsing-based approaches to machine translation, where it has been shown that paying careful attention to the binarization scheme can produce much faster decoders (Zhang et al., 2006; Huang, 2007; DeNero et al., 2009). The choice of binarization scheme will not affect parsing results if the parser is permitted to explore the whole search space. In practice, however, this space is too large, so parsers use pruning to discard unlikely hypotheses. This presents a problem for bottom-up parsing algorithms because of the way the probability of a rule is distributed among its binarized pieces: The standard approach is to place all of that probability on the top-level binarized rule, and to set the probabilities of lower binarized pieces to 1.0. Because these rules are reconstruc</context>
<context position="31317" citStr="Zhang et al. (2006)" startWordPosition="5418" endWordPosition="5421">ly identical to that of right binarization. Another popular binarization approach is head-outward binarization. Based on the analysis above, we suspect that its performance will fall somewhere among the binarizations presented here, and that pushing will improve it as well. We hope to investigate this in future work. 5 Summary Weight pushing increases parser efficiency, especially for large grammars. Most notably, it improves parser efficiency for the Gibbs-sampled tree substitution grammar of Post and Gildea (2009). We believe this approach could alo benefit syntax-based machine translation. Zhang et al. (2006) introduced a synchronous binarization technique that improved decoding efficiency and accuracy by ensuring that rule binarization avoided gaps on both the source and target sides 1 5 10 25 50 1 5 10 25 50 mean time per sentence (s) accuracy 85 80 75 70 65 60 55 50 (right,max) (right,nthroot) (right,none) accuracy 85 80 75 70 65 60 55 50 (greedy,max) (greedy,nthroot) (greedy,none) 97 (for rules where this was possible). Their binarization was designed to share binarized pieces among rules, but their approach to distributing weight was the default (nondiffused) case found in this paper to be le</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the 2006 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-06), New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>