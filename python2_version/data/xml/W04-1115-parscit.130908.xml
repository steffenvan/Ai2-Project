<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.103109">
<title confidence="0.996713">
Combining Prosodic and Text Features for Segmentation of Mandarin
Broadcast News
</title>
<author confidence="0.988144">
Gina-Anne Levow
</author>
<affiliation confidence="0.99525">
University of Chicago
</affiliation>
<email confidence="0.995503">
levow@cs.uchicago.edu
</email>
<sectionHeader confidence="0.993793" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999603027777778">
Automatic topic segmentation, separation of a dis-
course stream into its constituent stories or topics,
is a necessary preprocessing step for applications
such as information retrieval, anaphora resolution,
and summarization. While significant progress has
been made in this area for text sources and for En-
glish audio sources, little work has been done in
automatic, acoustic feature-based segmentation of
other languages. In this paper, we consider exploit-
ing both prosodic and text-based features for topic
segmentation of Mandarin Chinese. As a tone lan-
guage, Mandarin presents special challenges for ap-
plicability of intonation-based techniques, since the
pitch contour is also used to establish lexical iden-
tity. We demonstrate that intonational cues such as
reduction in pitch and intensity at topic boundaries
and increase in duration and pause still provide sig-
nificant contrasts in Mandarin Chinese. We build
a decision tree classifier that, based only on word
and local context prosodic information without ref-
erence to term similarity, cue phrase, or sentence-
level information, achieves boundary classification
accuracy of 84.6-95.6% on a balanced test set. We
contrast these results with classification using text-
based features, exploiting both text similarity and
n-gram cues, to achieve accuracies between 77-
95.6%, if silence features are used. Finally we in-
tegrate prosody, text, and silence features using a
voting strategy to combine decision tree classifiers
for each feature subset individually and all subsets
jointly. This voted decision tree classifier yields
an overall classification accuracy of 96.85%, with
2.8% miss and 3.15% false alarm rates on a repre-
sentative corpus sample, demonstrating synergistic
combination of prosodic and text features for topic
segmentation.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927444444445">
Natural spoken discourse is composed of a sequence
of utterances, not independently generated or ran-
domly strung together, but rather organized accord-
ing to basic structural principles. This structure in
turn guides the interpretation of individual utter-
ances and the discourse as a whole. Formal writ-
ten discourse signals a hierarchical, tree-based dis-
course structure explicitly by the division of the text
into chapters, sections, paragraphs, and sentences.
This structure, in turn, identifies domains for in-
terpretation; many systems for anaphora resolution
rely on some notion of locality (Grosz and Sidner,
1986). Similarly, this structure represents topical
organization, and thus would be useful in informa-
tion retrieval to select documents where the primary
sections are on-topic, and, for summarization, to se-
lect information covering the different aspects of the
topic.
Unfortunately, spoken discourse does not include
the orthographic conventions that signal structural
organization in written discourse. Instead, one must
infer the hierarchical structure of spoken discourse
from other cues. Prior research (Nakatani et al.,
1995; Swerts, 1997) has shown that human label-
ers can more sharply, consistently, and confidently
identify discourse structure in a word-level tran-
scription when an original audio recording is avail-
able than they can on the basis of the transcribed
text alone. This finding indicates that substantial
additional information about the structure of the
discourse is encoded in the acoustic-prosodic fea-
tures of the utterance. Given the often errorful tran-
scriptions available for large speech corpora, we
choose to focus here on fully exploiting the prosodic
cues to discourse structure present in the original
speech. We then compare the effectiveness of a pure
prosodic classification to text-based and mixed text
and prosodic based classification.
In the current set of experiments, we concentrate
on sequential segmentation of news broadcasts into
individual stories. While a richer hierarchical seg-
mentation is ultimately desirable, sequential story
segmentation provides a natural starting point. This
level of segmentation can also be most reliably per-
formed by human labelers and thus can be consid-
ered most robust, and segmented data sets are pub-
licly available.
Furthermore, we apply prosodic-based segmenta-
tion to Mandarin Chinese, in addition to textual fea-
tures. Not only is the use of prosodic cues to topic
segmentation much less well-studied in general than
is the use of text cues, but the use of prosodic cues
has been largely limited to English and other Euro-
pean languages.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999950727272727">
Most prior research on automatic topic segmenta-
tion has been applied to clean text only and thus
used textual features. Text-based segmentation ap-
proaches have utilized term-based similarity mea-
sures computed across candidate segments (Hearst,
1994) and also discourse markers to identify dis-
course structure (Marcu, 2000).
The Topic Detection and Tracking (TDT) eval-
uations focused on segmentation of both text and
speech sources. This framework introduced new
challenges in dealing with errorful automatic tran-
scriptions as well as new opportunities to exploit
cues in the original speech. The most successful
approach (Beeferman et al., 1999) produced auto-
matic segmentations that yielded retrieval results
approaching those with manual segmentations, us-
ing text and silence features. (Tur et al., 2001) ap-
plied both a prosody-only and a mixed text-prosody
model to segmentation of TDT English broadcast
news, with the best results combining text and
prosodic features. (Hirschberg and Nakatani, 1998)
also examined automatic topic segmentation based
on prosodic cues, in the domain of English broad-
cast news, while (Hirschberg et al., 2001) applied
similar cues to segmentation of voicemail.
Work in discourse analysis (Nakatani et al., 1995;
Swerts, 1997) in both English and Dutch has iden-
tified features such as changes in pitch range, in-
tensity, and speaking rate associated with seg-
ment boundaries and with boundaries of different
strengths. They also demonstrated that access to
acoustic cues improves the ease and quality of hu-
man labeling.
</bodyText>
<sectionHeader confidence="0.96838" genericHeader="method">
3 Prosody and Mandarin
</sectionHeader>
<bodyText confidence="0.996539666666667">
In this paper we focus on topic segmentation in
Mandarin Chinese broadcast news. Mandarin Chi-
nese is a tone language in which lexical identity is
determined by a pitch contour - or tone - associ-
ated with each syllable. This additional use of pitch
raises the question of the cross-linguistic applicabil-
ity of the prosodic cues, especially pitch cues, iden-
tified for non-tone languages. Specifically, do we
find intonational cues in tone languages? The fact
that emphasis is marked intonationally by expansion
of pitch range even in the presence of Mandarin lex-
ical tone (Shen, 1989) suggests encouragingly that
prosodic, intonational cues to other aspects of in-
formation structure might also prove robust in tone
languages.
</bodyText>
<sectionHeader confidence="0.974297" genericHeader="method">
4 Data Set
</sectionHeader>
<bodyText confidence="0.999892947368421">
We utilize the Topic Detection and Tracking (TDT)
3 (Wayne, 2000) collection Mandarin Chinese
broadcast news audio corpus as our data set. Story
segmentation in Mandarin and English broadcast
news and newswire text was one of the TDT tasks
and also an enabling technology for other retrieval
tasks. We use the segment boundaries provided with
the corpus as our gold standard labeling. Our col-
lection comprises 3014 news stories drawn from ap-
proximately 113 hours over three months (October-
December 1998) of news broadcasts from the Voice
of America (VOA) in Mandarin Chinese, with 800
regions of other program material including musi-
cal interludes and teasers. The transcriptions span
approximately 750,000 words. Stories average ap-
proximately 250 words in length to span a full story.
No subtopic segmentation is performed. The audio
is stored in NIST Sphere format sampled at 16KHz
with 16-bit linear encoding.
</bodyText>
<sectionHeader confidence="0.982441" genericHeader="method">
5 Prosodic Features
</sectionHeader>
<bodyText confidence="0.999993">
We consider four main classes of prosodic features
for our analysis and classification: pitch, intensity,
silence and duration. Pitch, as represented by f0 in
Hertz was computed by the “To pitch” function of
the Praat system (Boersma, 2001). We selected the
highest ranked pitch candidate value in each voiced
region. We then applied a 5-point median filter to
smooth out local instabilities in the signal such as
vocal fry or small regions of spurious doubling or
halving. Analogously, we computed the intensity
in decibels for each 10ms frame with the Praat “To
intensity” function, followed by similar smoothing.
For consistency and to allow comparability, we
compute all figures for word-based units, using
the automatic speech recognition transcriptions pro-
vided with the TDT Mandarin data. The words are
used to establish time spans for computing pitch
or intensity mean or maximum values, to enable
durational normalization and the pairwise compar-
isons reported below, and to identify silence or non-
speech duration.
It is well-established (Ross and Ostendorf, 1996)
that for robust analysis pitch and intensity should
be normalized by speaker, since, for example, aver-
age pitch is largely incomparable for male and fe-
male speakers. In the absence of speaker identifica-
tion software, we approximate speaker normaliza-
tion with story-based normalization, computed as
, assuming one speaker per topic1. For du-
ration, we consider both absolute and normalized
word duration, where average word duration is used
as the mean in the calculation above.
</bodyText>
<sectionHeader confidence="0.966591" genericHeader="method">
6 Prosodic Analysis
</sectionHeader>
<bodyText confidence="0.999375243902439">
To evaluate the potential applicability of prosodic
features to story segmentation in Mandarin Chinese,
we performed some initial data analysis to deter-
mine if words in story-final position differed from
the same words used throughout the story in news
stories. This lexical match allows direct pairwise
comparison. We anticipated that since words in
Mandarin varied not only in phoneme sequence but
also in tone sequence, a direct comparison might be
particularly important to eliminate sources of vari-
ability. Features that differed significantly would
form the basis of our classifier feature set.
We found significant differences for each of the
features we considered. Specifically, word dura-
tion, normalized mean pitch, and normalized mean
intensity all differed significantly for words in topic-
final position relative to occurrences throughout the
story (paired t-test, two-tailed,
, respectively) . Word dura-
tion increased, while both pitch and intensity de-
creased. A small side experiment using 15 hours
of English broadcast news from the TDT collection
shows similar trends, though the magnitude of the
change in intensity is smaller than that observed for
the Chinese. Furthermore, comparison of average
pitch and average intensity for 1, 5, and 10 word
windows at the beginning and end of news stories
finds that pitch and intensity are both significantly
higher (✍ ) at the start of stories than at the
end.
These contrasts are consistent with, though in
some cases stronger than, those identified for En-
glish (Nakatani et al., 1995) and Dutch (Swerts,
1997). The relatively large size of the corpus en-
hances the salience of these effects. We find, im-
portantly, that reduction in pitch as a signal of
topic finality is robust across the typological con-
trast of tone and non-tone languages. These findings
demonstrate highly significant intonational effects
even in tone languages and suggest that prosodic
cues may be robust across wide ranges of languages.
</bodyText>
<footnote confidence="0.964756666666667">
1This is an imperfect approximation as some stories include
off-site interviews, but seems a reasonable choice in the ab-
sence of automatic speaker identification.
</footnote>
<sectionHeader confidence="0.993569" genericHeader="method">
7 Classification
</sectionHeader>
<subsectionHeader confidence="0.998769">
7.1 Prosodic Feature Set
</subsectionHeader>
<bodyText confidence="0.999981666666667">
The results above indicate that duration, pitch, and
intensity should be useful for automatic prosody-
based identification of topic boundaries. To facili-
tate cross-speaker comparisons, we use normalized
representations of average pitch, average intensity,
and word duration. We also include absolute word
duration. These features form a word-level context-
independent feature set.
Since segment boundaries and their cues exist
to contrastively signal the separation between top-
ics, we augment these features with local context-
dependent measures. Specifically, we add features
that measure the change between the current word
and the next word.2 This contextualization adds
four contextual features: change in normalized av-
erage pitch, change in normalized average intensity,
change in normalized word duration, and duration
of following silence or non-speech region.
</bodyText>
<subsectionHeader confidence="0.996782">
7.2 Text Feature Set
</subsectionHeader>
<bodyText confidence="0.999979846153846">
In addition to the prosodic features which are our
primary interest, we also consider a set of features
that exploit textual similarity to identify segment
boundaries. Motivated by text and topic similarity
measures in the vector space model of information
retrieval (Salton, 1989), we compute a vector rep-
resentation of the words in 50 word windows pre-
ceding and following the current potential bound-
ary position. We compute the cosine similarity of
these two vectors. We employ a weight-
ing; each term is weighted by the product of its
frequency in the window ( ) and its inverse doc-
ument frequency ( ) as a measure of topicality.
We also consider the same similarity measure com-
puted across 30 word windows. The final text sim-
ilarity measure we consider is simple word overlap,
counting the number of words that appear in both 50
word windows defined above. We did not remove
stopwords, and used the word-based units from the
ASR transcription directly as our term units. We ex-
pect that these measures will be minimized at topic
boundaries where changes in topic are accompanied
by changes in topical terminology.
Finally we identified a small set of word unigram
features occuring within a ten-word window imme-
diately preceding or following a story boundary that
</bodyText>
<figureCaption confidence="0.802352833333333">
2We have posed the task of boundary detection as the task
of finding segment-final words, so the technique incorporates a
single-word lookahead. We could also repose the task as iden-
tification of topic-initial words and avoid the lookahead to have
a more on-line process. This is an area for future research.
Figure 1: Prosody-based decision tree classifier labeling words as segment-final or non-segment-final
</figureCaption>
<bodyText confidence="0.973881833333333">
were indicative of such a boundary.3 These fea-
tures include the Mandarin Chinese words for “au-
dience”, “reporting”, and “Voice of America.” We
used a boolean feature for each such word corre-
sponding to its presence or absence in the current
word’s environment in the classifier formulation.
</bodyText>
<subsectionHeader confidence="0.9692045">
7.3 Classifier Training and Testing
Configuration
</subsectionHeader>
<bodyText confidence="0.999962333333333">
We employed Quinlan’s C4.5 (Quinlan, 1992) deci-
sion tree classifier to provide a readily interpretable
classifier. Now, the vast majority of word positions
in our collection are non-segment-final. So, in or-
der to focus training and test on segment boundary
identification and to assess the discriminative capa-
bility of the classifier, we downsample our corpus
to produce a 50/50 split of segment-final and non-
final words. We train on 3500 segment-final words4
and 3500 non-final words, not matched in any way,
drawn randomly from the full corpus. We test on a
similarly balanced test set of 500 instances.
</bodyText>
<subsectionHeader confidence="0.953355">
7.4 Classifier Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.614912">
7.4.1 Prosody-only classification
</subsubsectionHeader>
<bodyText confidence="0.999927">
The resulting classifier achieved 95.6% accuracy,
with 2% missed boundaries and 7% false alarms.
This effectiveness is a substantial improvement over
the sample baseline of 50%. A portion of the deci-
sion tree is reproduced in Figure 1. Inspection of the
tree indicates the key role of silence as well as the
use of both contextual and purely local features of
pitch, intensity, and duration. The classifier relies
</bodyText>
<footnote confidence="0.99687675">
3We used the ngram functionality to Boostexter (Schapire
and Singer, 2000) to identify these units.
4We excluded a small proportion of words for which the
pitch tracker returned no results.
</footnote>
<bodyText confidence="0.999945833333333">
on the theoretically and empirically grounded fea-
tures of pitch, intensity, duration, and silence, where
it has been suggested that higher pitch and wider
range are associated with topic initiation and lower
pitch or narrower range is associated with topic fi-
nality.
We performed a set of contrastive experiments to
explore the impact of different lexical tones on clas-
sification accuracy. We grouped words based on the
lexical tone of the initial syllable into high, rising,
low, and falling. We found no tone-based differ-
ences in classification with all groups achieving 94-
96% accuracy. Since the magnitude of the differ-
ence in pitch based on discourse position is compa-
rable to that based on lexical tone identity, and the
overlap between pitch values in non-final and final
positions is relatively small, we obtain consistent re-
sults.
</bodyText>
<subsectionHeader confidence="0.814879">
7.4.2 Text and Silence-based Classification
</subsectionHeader>
<bodyText confidence="0.999795636363636">
In a comparable experiment, we employed only the
text similarity, text unigram, and silence duration
features to train and test the classifier. These fea-
tures similarly achieved a 95.6% overall classifi-
cation accuracy, with 3.6% miss and 5.2% false
alarms. Here the best classification accuracy was
achieved by the weighted 50 word window
based text similarity measure. The text unigram fea-
tures also contributed to overall classifier effective-
ness. A portion of the decision tree classifier using
text-based features is reproduced in Figure 2.
</bodyText>
<subsectionHeader confidence="0.754821">
7.4.3 Combined Prosody and Text
Classification
</subsectionHeader>
<bodyText confidence="0.999806166666667">
Finally we built a combined classifier integrating all
prosodic, textual, and silence features. This classi-
fier yielded an accuracy of 96.4%, somewhat bet-
ter effectiveness, still with more than twice as many
false alarms as missed detections. The decision tree
utilized all prosodic features. weighted co-
sine similarity alone performed as well as any of the
other text similarity or overlap measures. The text
unigram features also contributed to overall classi-
fier effectiveness. A portion of the decision tree
classifier using prosodic, textual, and silence fea-
tures is reproduced in Figure 3.
</bodyText>
<subsectionHeader confidence="0.994132">
7.5 Feature Comparison
</subsectionHeader>
<bodyText confidence="0.999967916666667">
We also performed a set of contrastive experiments
with different subsets of available features to as-
sess the dependence on these features.5 We grouped
features into 5 sets: pitch, intensity, duration, si-
lence, and text-similarity. For each of the prosody-
only, text-only, and combined prosody and text-
based classifiers, we successively removed the fea-
ture class at the root of the decision tree and re-
trained with the remaining features (Table 1).
We observe that although silence duration plays
a very significant role in story boundary identifi-
cation for all feature sets, the richer prosodic and
mixed text-prosodic classifiers are much more ro-
bust to the absence of silence information. Further
we observe that intensity and then pitch play the
next most important roles in classification. This be-
havior can be explained by the observation that, like
silence or non-speech regions, pitch and intensity
changes provide sharp, local cues to topic finality or
initiation. Thus the prosodic features provide some
measure of redundancy for the silence feature. In
contrast, the text similarity measures apply to rel-
atively wide regions, comparing pairs of 50 word
windows.
</bodyText>
<subsectionHeader confidence="0.997272">
7.6 Further Feature Integration
</subsectionHeader>
<bodyText confidence="0.999706285714286">
Finally we considered effectiveness on a representa-
tive test sampling of the full data set, rather than the
downsampled, balanced set, adding a proportional
number of unseen non-final words to the test set.
We observed that although the overall classification
accuracy was 95.6% and we were missing only 2%
of the story boundaries, we produced a high level
of false alarms (4.4%), accounting for most of the
observed classification errors. Given the large pre-
dominance of non-boundary positions in the real-
word distribution, we sought to better understand
and reduce this false alarm rate, and hopefully re-
duce the overall error rates. At the same time, we
hoped to avoid a dramatic increase in the miss rate.
</bodyText>
<footnote confidence="0.997262333333333">
5For example, VOA Mandarin has been observed stylis-
tically to make idiosyncratically large use of silence at story
boundaries. (personal communication, James Allan).
</footnote>
<bodyText confidence="0.999965653846154">
To explore this question, we considered the con-
tribution of each of the three main feature types -
prosody, text, and silence - and their combined ef-
fects on false alarms. We constructed independent
feature-set-specific decision tree classifiers for each
of the feature types and compared their independent
classifications to those of the integrated classifier.
We found that while there was substantial agree-
ment across the different feature-based classifiers in
the cases of correct classification, erroneous clas-
sifications often occured when the assignment was
a minority decision. Specifically, one-third of the
false alarms were based on a minority assignemnt,
where only the fully integrated classifier deemed the
position a boundary or where it agreed with only
one of the feature-set-specific classifiers.
Based on these observations, we completed our
multi-feature integration by augmenting the deci-
sion tree based classification with a voting mech-
anism. In this configuration, a boundary was only
assigned in cases where the integrated classifier
agreed with at least two of the feature-set-specific
classifiers. This approach reduced the false alarm
rate by one-third, to 3.15%, while the miss rate rose
only to 2.8%. The overall accuracy on a representa-
tive sample distribution reached 96.85%.
</bodyText>
<sectionHeader confidence="0.975129" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981">
We have demonstrated the utility of prosody-only,
text-only, and mixed text-prosody features for auto-
matic topic segmentation of Mandarin Chinese. We
have demonstrated the applicability of intonational
prosodic features, specifically pitch, intensity, pause
and duration, to the identification of topic bound-
aries in a tone language. We find highly signifi-
cant decreases in pitch and intensity at topic final
positions, and significant increases in word dura-
tion. Furthermore, these features in both local form
and contextualized form provide the basis for an ef-
fective decision tree classifier of boundary positions
that does not use term similarity or cue phrase infor-
mation, but only prosodic features.
We observe similar effectiveness for all feature
sets when all features are available, with slightly
better classification accuracy for the text and hybrid
text-prosody approach. We further observe that the
prosody-only and hybrid feature sets are much less
sensitive to the absence of individual features, and,
in particular, to silence features, as pitch and inten-
sity provide comparable sharp cues to the position
of topic boundaries. These findings indicate that
prosodic features are robust cues to topic bound-
aries, both with and without textual cues.
Finally, we demonstrate the joint utility of the dif-
</bodyText>
<table confidence="0.999262833333333">
Prosody-only Text Silence Text Prosody
Accuracy Pct. Change Accuracy Pct. Change Accuracy Pct. Change
All 95.6% 0 95.6% 0 96.4% 0
Silence 84.6% -11.5% 77.4% -19% 89.6% -6.9%
Intensity 80.4% -15.9% 85.4% -11.4%
Pitch 63.6% -33.4% 78.6% -18.5%
</table>
<tableCaption confidence="0.8670365">
Table 1: Reduction in classification accuracy with removal of features. Each row is labeled with the feature
that is newly removed from the set of available features.
</tableCaption>
<bodyText confidence="0.999412333333333">
ferent feature sets - prosodic, textual, and silence.
The use of a simple voting mechanism exploits the
different contributions of each of the feature-set-
specific classifiers in conjunction with the integrated
classifier. This final combination allows a substan-
tial reduction of the false alarm rate, reduction in
the overall error rate, and only a small increase in
the miss rate. Further tuning of relative miss and
false alarm rates is certainly possible, but should be
tied to a specific task application.
There is still substantial work to be done. We
would like to integrate speaker identification for
normalization and speaker change detection. We
also plan to explore the integration of text and
prosodic features for the identification of more fine-
grained sub-topic structure, to provide more focused
units for information retrieval, summarization, and
anaphora resolution. We also plan to explore the in-
teraction of prosodic and textual features with cues
from other modalities, such as gaze and gesture, for
robust segmentation of varied multi-modal data.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999578649122807">
D. Beeferman, A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Ma-
chine Learning, 34((1-3)):177–210.
P. Boersma. 2001. Praat, a system for doing pho-
netics by computer. Glot International, 5(9–
10):341–345.
B. Grosz and C. Sidner. 1986. Attention, inten-
tion, and the structure of discourse. Computa-
tional Linguistics, 12(3):175–204.
M. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of the 32nd An-
nual Meeting of the Association for Computa-
tional Linguistics.
Julia Hirschberg and Christine Nakatani. 1998.
Acoustic indicators of topic segmentation. In
Proceedings on ICSLP-98.
J. Hirschberg, M. Bacchiani, D. Hindel, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker,
and G. Zamchick. 2001. Scanmail: Browsing
and searching speech data by content. In Pro-
ceedings ofEUROSPEECH 2001.
D. Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
C. H. Nakatani, J. Hirschberg, and B. J. Grosz.
1995. Discourse structure in spoken language:
Studies on speech corpora. In Working Notes of
the AAAI Spring Symposium on Empirical Meth-
ods in Discourse Interpretation and Generation,
pages 106–112.
J.R. Quinlan. 1992. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
K. Ross and M. Ostendorf. 1996. Prediction of
abstract labels for speech synthesis. Computer
Speech and Language, 10:155–185.
Gerard Salton. 1989. Automatic Text Processing:
The Transformation, Analysis and Retrieval ofIn-
formation by Computer. Addison-Wesley, Read-
ing, MA.
Robert E. Schapire and Yoram Singer. 2000. Boos-
Texter: A boosting-based system for text catego-
rization. Machine Learning, 39((2/3)):135–168.
X.-N. Shen. 1989. The Prosody of Mandarin Chi-
nese, volume 118 of University of California
Publications in Linguistics. University of Cali-
fornia Press.
Marc Swerts. 1997. Prosodic features at discourse
boundaries of different strength. Journal of the
Acoustical Society ofAmerica, 101(1):514–521.
G. Tur, D. Hakkani-Tur, A. Stolcke, and
E. Shriberg. 2001. Integrating prosodic and
lexical cues for automatic topic segmentation.
Computational Linguistics, 27(1):31–57.
C. Wayne. 2000. Multilingual topic detection and
tracking: Successful research enabled by cor-
pora and evaluation. In Language Resources
and Evaluation Conference (LREC) 2000, pages
1487–1494.
</reference>
<figureCaption confidence="0.991212">
Figure 2: Text-feature-based decision tree classifier labeling words as segment-final or non-segment-final
Figure 3: Prosody, text, and silence based decision tree classifier labeling words as segment-final or non-
segment-final
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799737">
<title confidence="0.990383">Combining Prosodic and Text Features for Segmentation of Broadcast News</title>
<author confidence="0.910141">Gina-Anne</author>
<affiliation confidence="0.999808">University of Chicago</affiliation>
<email confidence="0.999814">levow@cs.uchicago.edu</email>
<abstract confidence="0.996962054054054">Automatic topic segmentation, separation of a discourse stream into its constituent stories or topics, is a necessary preprocessing step for applications such as information retrieval, anaphora resolution, and summarization. While significant progress has been made in this area for text sources and for English audio sources, little work has been done in automatic, acoustic feature-based segmentation of other languages. In this paper, we consider exploiting both prosodic and text-based features for topic segmentation of Mandarin Chinese. As a tone language, Mandarin presents special challenges for applicability of intonation-based techniques, since the pitch contour is also used to establish lexical identity. We demonstrate that intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide significant contrasts in Mandarin Chinese. We build a decision tree classifier that, based only on word and local context prosodic information without reference to term similarity, cue phrase, or sentencelevel information, achieves boundary classification accuracy of 84.6-95.6% on a balanced test set. We contrast these results with classification using textbased features, exploiting both text similarity and n-gram cues, to achieve accuracies between 77- 95.6%, if silence features are used. Finally we integrate prosody, text, and silence features using a voting strategy to combine decision tree classifiers for each feature subset individually and all subsets jointly. This voted decision tree classifier yields an overall classification accuracy of 96.85%, with 2.8% miss and 3.15% false alarm rates on a representative corpus sample, demonstrating synergistic combination of prosodic and text features for topic segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="5255" citStr="Beeferman et al., 1999" startWordPosition="778" endWordPosition="781"> automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34((1-3)):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
</authors>
<title>Praat, a system for doing phonetics by computer.</title>
<date>2001</date>
<journal>Glot International,</journal>
<volume>5</volume>
<issue>9</issue>
<pages>10--341</pages>
<contexts>
<context position="8093" citStr="Boersma, 2001" startWordPosition="1229" endWordPosition="1230">andarin Chinese, with 800 regions of other program material including musical interludes and teasers. The transcriptions span approximately 750,000 words. Stories average approximately 250 words in length to span a full story. No subtopic segmentation is performed. The audio is stored in NIST Sphere format sampled at 16KHz with 16-bit linear encoding. 5 Prosodic Features We consider four main classes of prosodic features for our analysis and classification: pitch, intensity, silence and duration. Pitch, as represented by f0 in Hertz was computed by the “To pitch” function of the Praat system (Boersma, 2001). We selected the highest ranked pitch candidate value in each voiced region. We then applied a 5-point median filter to smooth out local instabilities in the signal such as vocal fry or small regions of spurious doubling or halving. Analogously, we computed the intensity in decibels for each 10ms frame with the Praat “To intensity” function, followed by similar smoothing. For consistency and to allow comparability, we compute all figures for word-based units, using the automatic speech recognition transcriptions provided with the TDT Mandarin data. The words are used to establish time spans f</context>
</contexts>
<marker>Boersma, 2001</marker>
<rawString>P. Boersma. 2001. Praat, a system for doing phonetics by computer. Glot International, 5(9– 10):341–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>C Sidner</author>
</authors>
<title>Attention, intention, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="2583" citStr="Grosz and Sidner, 1986" startWordPosition="374" endWordPosition="377">tion Natural spoken discourse is composed of a sequence of utterances, not independently generated or randomly strung together, but rather organized according to basic structural principles. This structure in turn guides the interpretation of individual utterances and the discourse as a whole. Formal written discourse signals a hierarchical, tree-based discourse structure explicitly by the division of the text into chapters, sections, paragraphs, and sentences. This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>B. Grosz and C. Sidner. 1986. Attention, intention, and the structure of discourse. Computational Linguistics, 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4859" citStr="Hearst, 1994" startWordPosition="720" endWordPosition="721">ts are publicly available. Furthermore, we apply prosodic-based segmentation to Mandarin Chinese, in addition to textual features. Not only is the use of prosodic cues to topic segmentation much less well-studied in general than is the use of text cues, but the use of prosodic cues has been largely limited to English and other European languages. 2 Related Work Most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed te</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>M. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Christine Nakatani</author>
</authors>
<title>Acoustic indicators of topic segmentation.</title>
<date>1998</date>
<booktitle>In Proceedings on ICSLP-98.</booktitle>
<contexts>
<context position="5614" citStr="Hirschberg and Nakatani, 1998" startWordPosition="831" endWordPosition="834"> focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. They also demonstrated that access to acoustic cues improves the ease and quality of human labeling. 3 Prosody and Mandarin In this paper we focus on top</context>
</contexts>
<marker>Hirschberg, Nakatani, 1998</marker>
<rawString>Julia Hirschberg and Christine Nakatani. 1998. Acoustic indicators of topic segmentation. In Proceedings on ICSLP-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>M Bacchiani</author>
<author>D Hindel</author>
<author>P Isenhour</author>
<author>A Rosenberg</author>
<author>L Stark</author>
<author>L Stead</author>
<author>S Whittaker</author>
<author>G Zamchick</author>
</authors>
<title>Scanmail: Browsing and searching speech data by content.</title>
<date>2001</date>
<booktitle>In Proceedings ofEUROSPEECH</booktitle>
<contexts>
<context position="5754" citStr="Hirschberg et al., 2001" startWordPosition="853" endWordPosition="856">ns as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. They also demonstrated that access to acoustic cues improves the ease and quality of human labeling. 3 Prosody and Mandarin In this paper we focus on topic segmentation in Mandarin Chinese broadcast news. Mandarin Chinese is a tone language in which lexical identity is determined by a pitch c</context>
</contexts>
<marker>Hirschberg, Bacchiani, Hindel, Isenhour, Rosenberg, Stark, Stead, Whittaker, Zamchick, 2001</marker>
<rawString>J. Hirschberg, M. Bacchiani, D. Hindel, P. Isenhour, A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and G. Zamchick. 2001. Scanmail: Browsing and searching speech data by content. In Proceedings ofEUROSPEECH 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4932" citStr="Marcu, 2000" startWordPosition="731" endWordPosition="732">on to Mandarin Chinese, in addition to textual features. Not only is the use of prosodic cues to topic segmentation much less well-studied in general than is the use of text cues, but the use of prosodic cues has been largely limited to English and other European languages. 2 Related Work Most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Nakatani</author>
<author>J Hirschberg</author>
<author>B J Grosz</author>
</authors>
<title>Discourse structure in spoken language: Studies on speech corpora.</title>
<date>1995</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>106--112</pages>
<contexts>
<context position="3107" citStr="Nakatani et al., 1995" startWordPosition="447" endWordPosition="450">on; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently, and confidently identify discourse structure in a word-level transcription when an original audio recording is available than they can on the basis of the transcribed text alone. This finding indicates that substantial additional information about the structure of the discourse is encoded in the acoustic-prosodic features of the utterance. Given the often errorful transcriptions available for large speech corpora, we choose to focus here on fully exploiting the prosodic cues to discourse structure present in the orig</context>
<context position="5855" citStr="Nakatani et al., 1995" startWordPosition="868" endWordPosition="871">eferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. They also demonstrated that access to acoustic cues improves the ease and quality of human labeling. 3 Prosody and Mandarin In this paper we focus on topic segmentation in Mandarin Chinese broadcast news. Mandarin Chinese is a tone language in which lexical identity is determined by a pitch contour - or tone - associated with each syllable. This additional use of pitch raises the question of</context>
<context position="10969" citStr="Nakatani et al., 1995" startWordPosition="1676" endWordPosition="1679">reased, while both pitch and intensity decreased. A small side experiment using 15 hours of English broadcast news from the TDT collection shows similar trends, though the magnitude of the change in intensity is smaller than that observed for the Chinese. Furthermore, comparison of average pitch and average intensity for 1, 5, and 10 word windows at the beginning and end of news stories finds that pitch and intensity are both significantly higher (✍ ) at the start of stories than at the end. These contrasts are consistent with, though in some cases stronger than, those identified for English (Nakatani et al., 1995) and Dutch (Swerts, 1997). The relatively large size of the corpus enhances the salience of these effects. We find, importantly, that reduction in pitch as a signal of topic finality is robust across the typological contrast of tone and non-tone languages. These findings demonstrate highly significant intonational effects even in tone languages and suggest that prosodic cues may be robust across wide ranges of languages. 1This is an imperfect approximation as some stories include off-site interviews, but seems a reasonable choice in the absence of automatic speaker identification. 7 Classifica</context>
</contexts>
<marker>Nakatani, Hirschberg, Grosz, 1995</marker>
<rawString>C. H. Nakatani, J. Hirschberg, and B. J. Grosz. 1995. Discourse structure in spoken language: Studies on speech corpora. In Working Notes of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages 106–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1992</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14545" citStr="Quinlan, 1992" startWordPosition="2237" endWordPosition="2238">entification of topic-initial words and avoid the lookahead to have a more on-line process. This is an area for future research. Figure 1: Prosody-based decision tree classifier labeling words as segment-final or non-segment-final were indicative of such a boundary.3 These features include the Mandarin Chinese words for “audience”, “reporting”, and “Voice of America.” We used a boolean feature for each such word corresponding to its presence or absence in the current word’s environment in the classifier formulation. 7.3 Classifier Training and Testing Configuration We employed Quinlan’s C4.5 (Quinlan, 1992) decision tree classifier to provide a readily interpretable classifier. Now, the vast majority of word positions in our collection are non-segment-final. So, in order to focus training and test on segment boundary identification and to assess the discriminative capability of the classifier, we downsample our corpus to produce a 50/50 split of segment-final and nonfinal words. We train on 3500 segment-final words4 and 3500 non-final words, not matched in any way, drawn randomly from the full corpus. We test on a similarly balanced test set of 500 instances. 7.4 Classifier Evaluation 7.4.1 Pros</context>
</contexts>
<marker>Quinlan, 1992</marker>
<rawString>J.R. Quinlan. 1992. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ross</author>
<author>M Ostendorf</author>
</authors>
<title>Prediction of abstract labels for speech synthesis. Computer Speech and Language,</title>
<date>1996</date>
<pages>10--155</pages>
<contexts>
<context position="8925" citStr="Ross and Ostendorf, 1996" startWordPosition="1356" endWordPosition="1359">rious doubling or halving. Analogously, we computed the intensity in decibels for each 10ms frame with the Praat “To intensity” function, followed by similar smoothing. For consistency and to allow comparability, we compute all figures for word-based units, using the automatic speech recognition transcriptions provided with the TDT Mandarin data. The words are used to establish time spans for computing pitch or intensity mean or maximum values, to enable durational normalization and the pairwise comparisons reported below, and to identify silence or nonspeech duration. It is well-established (Ross and Ostendorf, 1996) that for robust analysis pitch and intensity should be normalized by speaker, since, for example, average pitch is largely incomparable for male and female speakers. In the absence of speaker identification software, we approximate speaker normalization with story-based normalization, computed as , assuming one speaker per topic1. For duration, we consider both absolute and normalized word duration, where average word duration is used as the mean in the calculation above. 6 Prosodic Analysis To evaluate the potential applicability of prosodic features to story segmentation in Mandarin Chinese</context>
</contexts>
<marker>Ross, Ostendorf, 1996</marker>
<rawString>K. Ross and M. Ostendorf. 1996. Prediction of abstract labels for speech synthesis. Computer Speech and Language, 10:155–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis and Retrieval ofInformation by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="12766" citStr="Salton, 1989" startWordPosition="1945" endWordPosition="1946">ly, we add features that measure the change between the current word and the next word.2 This contextualization adds four contextual features: change in normalized average pitch, change in normalized average intensity, change in normalized word duration, and duration of following silence or non-speech region. 7.2 Text Feature Set In addition to the prosodic features which are our primary interest, we also consider a set of features that exploit textual similarity to identify segment boundaries. Motivated by text and topic similarity measures in the vector space model of information retrieval (Salton, 1989), we compute a vector representation of the words in 50 word windows preceding and following the current potential boundary position. We compute the cosine similarity of these two vectors. We employ a weighting; each term is weighted by the product of its frequency in the window ( ) and its inverse document frequency ( ) as a measure of topicality. We also consider the same similarity measure computed across 30 word windows. The final text similarity measure we consider is simple word overlap, counting the number of words that appear in both 50 word windows defined above. We did not remove sto</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic Text Processing: The Transformation, Analysis and Retrieval ofInformation by Computer. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="15657" citStr="Schapire and Singer, 2000" startWordPosition="2412" endWordPosition="2415">m the full corpus. We test on a similarly balanced test set of 500 instances. 7.4 Classifier Evaluation 7.4.1 Prosody-only classification The resulting classifier achieved 95.6% accuracy, with 2% missed boundaries and 7% false alarms. This effectiveness is a substantial improvement over the sample baseline of 50%. A portion of the decision tree is reproduced in Figure 1. Inspection of the tree indicates the key role of silence as well as the use of both contextual and purely local features of pitch, intensity, and duration. The classifier relies 3We used the ngram functionality to Boostexter (Schapire and Singer, 2000) to identify these units. 4We excluded a small proportion of words for which the pitch tracker returned no results. on the theoretically and empirically grounded features of pitch, intensity, duration, and silence, where it has been suggested that higher pitch and wider range are associated with topic initiation and lower pitch or narrower range is associated with topic finality. We performed a set of contrastive experiments to explore the impact of different lexical tones on classification accuracy. We grouped words based on the lexical tone of the initial syllable into high, rising, low, and</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39((2/3)):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X-N Shen</author>
</authors>
<title>The Prosody of Mandarin Chinese,</title>
<date>1989</date>
<volume>118</volume>
<institution>University of California Publications in Linguistics. University of California Press.</institution>
<contexts>
<context position="6767" citStr="Shen, 1989" startWordPosition="1020" endWordPosition="1021">ling. 3 Prosody and Mandarin In this paper we focus on topic segmentation in Mandarin Chinese broadcast news. Mandarin Chinese is a tone language in which lexical identity is determined by a pitch contour - or tone - associated with each syllable. This additional use of pitch raises the question of the cross-linguistic applicability of the prosodic cues, especially pitch cues, identified for non-tone languages. Specifically, do we find intonational cues in tone languages? The fact that emphasis is marked intonationally by expansion of pitch range even in the presence of Mandarin lexical tone (Shen, 1989) suggests encouragingly that prosodic, intonational cues to other aspects of information structure might also prove robust in tone languages. 4 Data Set We utilize the Topic Detection and Tracking (TDT) 3 (Wayne, 2000) collection Mandarin Chinese broadcast news audio corpus as our data set. Story segmentation in Mandarin and English broadcast news and newswire text was one of the TDT tasks and also an enabling technology for other retrieval tasks. We use the segment boundaries provided with the corpus as our gold standard labeling. Our collection comprises 3014 news stories drawn from approxim</context>
</contexts>
<marker>Shen, 1989</marker>
<rawString>X.-N. Shen. 1989. The Prosody of Mandarin Chinese, volume 118 of University of California Publications in Linguistics. University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Swerts</author>
</authors>
<title>Prosodic features at discourse boundaries of different strength.</title>
<date>1997</date>
<journal>Journal of the Acoustical Society ofAmerica,</journal>
<volume>101</volume>
<issue>1</issue>
<contexts>
<context position="3122" citStr="Swerts, 1997" startWordPosition="451" endWordPosition="452">aphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently, and confidently identify discourse structure in a word-level transcription when an original audio recording is available than they can on the basis of the transcribed text alone. This finding indicates that substantial additional information about the structure of the discourse is encoded in the acoustic-prosodic features of the utterance. Given the often errorful transcriptions available for large speech corpora, we choose to focus here on fully exploiting the prosodic cues to discourse structure present in the original speech. We</context>
<context position="5870" citStr="Swerts, 1997" startWordPosition="872" endWordPosition="873">roduced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. They also demonstrated that access to acoustic cues improves the ease and quality of human labeling. 3 Prosody and Mandarin In this paper we focus on topic segmentation in Mandarin Chinese broadcast news. Mandarin Chinese is a tone language in which lexical identity is determined by a pitch contour - or tone - associated with each syllable. This additional use of pitch raises the question of the cross-ling</context>
<context position="10994" citStr="Swerts, 1997" startWordPosition="1682" endWordPosition="1683">sity decreased. A small side experiment using 15 hours of English broadcast news from the TDT collection shows similar trends, though the magnitude of the change in intensity is smaller than that observed for the Chinese. Furthermore, comparison of average pitch and average intensity for 1, 5, and 10 word windows at the beginning and end of news stories finds that pitch and intensity are both significantly higher (✍ ) at the start of stories than at the end. These contrasts are consistent with, though in some cases stronger than, those identified for English (Nakatani et al., 1995) and Dutch (Swerts, 1997). The relatively large size of the corpus enhances the salience of these effects. We find, importantly, that reduction in pitch as a signal of topic finality is robust across the typological contrast of tone and non-tone languages. These findings demonstrate highly significant intonational effects even in tone languages and suggest that prosodic cues may be robust across wide ranges of languages. 1This is an imperfect approximation as some stories include off-site interviews, but seems a reasonable choice in the absence of automatic speaker identification. 7 Classification 7.1 Prosodic Feature</context>
</contexts>
<marker>Swerts, 1997</marker>
<rawString>Marc Swerts. 1997. Prosodic features at discourse boundaries of different strength. Journal of the Acoustical Society ofAmerica, 101(1):514–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>D Hakkani-Tur</author>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Integrating prosodic and lexical cues for automatic topic segmentation.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="5416" citStr="Tur et al., 2001" startWordPosition="801" endWordPosition="804">y measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries</context>
</contexts>
<marker>Tur, Hakkani-Tur, Stolcke, Shriberg, 2001</marker>
<rawString>G. Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg. 2001. Integrating prosodic and lexical cues for automatic topic segmentation. Computational Linguistics, 27(1):31–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wayne</author>
</authors>
<title>Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation.</title>
<date>2000</date>
<booktitle>In Language Resources and Evaluation Conference (LREC)</booktitle>
<pages>1487--1494</pages>
<contexts>
<context position="6985" citStr="Wayne, 2000" startWordPosition="1054" endWordPosition="1055">associated with each syllable. This additional use of pitch raises the question of the cross-linguistic applicability of the prosodic cues, especially pitch cues, identified for non-tone languages. Specifically, do we find intonational cues in tone languages? The fact that emphasis is marked intonationally by expansion of pitch range even in the presence of Mandarin lexical tone (Shen, 1989) suggests encouragingly that prosodic, intonational cues to other aspects of information structure might also prove robust in tone languages. 4 Data Set We utilize the Topic Detection and Tracking (TDT) 3 (Wayne, 2000) collection Mandarin Chinese broadcast news audio corpus as our data set. Story segmentation in Mandarin and English broadcast news and newswire text was one of the TDT tasks and also an enabling technology for other retrieval tasks. We use the segment boundaries provided with the corpus as our gold standard labeling. Our collection comprises 3014 news stories drawn from approximately 113 hours over three months (OctoberDecember 1998) of news broadcasts from the Voice of America (VOA) in Mandarin Chinese, with 800 regions of other program material including musical interludes and teasers. The </context>
</contexts>
<marker>Wayne, 2000</marker>
<rawString>C. Wayne. 2000. Multilingual topic detection and tracking: Successful research enabled by corpora and evaluation. In Language Resources and Evaluation Conference (LREC) 2000, pages 1487–1494.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>