<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.986615">
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
</title>
<author confidence="0.99931">
Stephan Oepen44, Marco Kuhlmann°, Yusuke Miyao♦, Daniel Zeman&apos; ,
Dan Flickinger&apos;, Jan Hajiˇc&apos;, Angelina Ivanova4, and Yi Zhang*
</author>
<affiliation confidence="0.968300142857143">
• University of Oslo, Department of Informatics
&apos;t&apos; Potsdam University, Department of Linguistics
° Linköping University, Department of Computer and Information Science
0 National Institute of Informatics, Tokyo
° Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
� Stanford University, Center for the Study of Language and Information
* Nuance Communications Aachen GmbH
</affiliation>
<email confidence="0.976735">
sdp-organizers@emmtee.net
</email>
<sectionHeader confidence="0.990312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9922226">
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate–argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
</bodyText>
<sectionHeader confidence="0.873354" genericHeader="categories and subject descriptors">
1 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999964285714286">
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &amp;
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
</bodyText>
<note confidence="0.5481915">
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
</note>
<bodyText confidence="0.999915035714286">
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),1 seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
</bodyText>
<listItem confidence="0.793972">
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
</listItem>
<bodyText confidence="0.99988025">
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
</bodyText>
<footnote confidence="0.92135">
1See http://alt.qcri.org/semeval2014/
</footnote>
<note confidence="0.529904">
task8/ for further technical details, information on how to
obtain the data, and official results.
</note>
<page confidence="0.594772">
63
</page>
<note confidence="0.9951885">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63–72,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.9796566">
A1 A2
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
(a) Partial semantic dependencies in PropBank and NomBank.
implicit_conj
_and_c
BV
ARG1
ARG2 ARG3 ARG1
ARG1
ARG2
ARG1
ARG1
mwe
top
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
(b) DELPH-IN Minimal Recursion Semantics–derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
(c) Enju Predicate–Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
</figure>
<figureCaption confidence="0.993813">
Figure 1: Sample semantic dependency graphs for Example (1).
</figureCaption>
<figure confidence="0.998789628571429">
ARG1 ARG2
ARG2
ARG2
ARG1
ARG1
ARG1
ARG1
ARG1
ARG1
top
ARG2
ARG1
ARG1
ARG1
ARG2
ARG1
ARG1 ARG2
ARG1
ARG2
RSTR
top
ACT
PAT
EXT
PAT
ADDR
ADDR
ADDR
CONJ.m
ADDR
APPS.m
CONJ.m
RSTR
APPS.m
CONJ.m
</figure>
<bodyText confidence="0.999875022727273">
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea &amp; Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena—
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction—typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ‘semantic parsing’,
which Kate and Wong (2010) define as “the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.”
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ‘completeness’ in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
</bodyText>
<sectionHeader confidence="0.962652" genericHeader="method">
2 Target Representations
</sectionHeader>
<bodyText confidence="0.999869714285714">
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
</bodyText>
<page confidence="0.926215">
64
</page>
<bodyText confidence="0.981450183673469">
we provide some background on the ‘pedigree’ and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00–
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource—
dubbed DeepBank by Flickinger et al. (2012)—
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ‘lossy’ conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen &amp; Lønning, 2006), then to ‘pure’
bi-lexical form—projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.3 The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate–argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate–argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Hajiˇc et al., 2012)4
is a set of parallel dependency trees over the WSJ
2Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
</bodyText>
<footnote confidence="0.3941525">
3Seehttp://kmcs.nii.ac.jp/enju/.
4Seehttp://ufal.mff.cuni.cz/pcedt2.0/.
</footnote>
<table confidence="0.778063285714286">
id form lemma pos top pred arg1 arg2
#2020000
1 Ms. Ms. NNP − _
2 Haag Haag NNP − − compound ARG1
3 plays play VBZ + + _
4 Elianti Elianti NNP − − _ ARG2
5 . . . − − _
</table>
<page confidence="0.606710666666667">
_
_
_
</page>
<tableCaption confidence="0.997524">
Table 1: Tabular SDP data format (showing DM).
</tableCaption>
<bodyText confidence="0.99998">
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
</bodyText>
<sectionHeader confidence="0.980611" genericHeader="method">
3 Graph Representation
</sectionHeader>
<bodyText confidence="0.999926034482759">
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
xi, ... , xn is a structure G = (V, E, EV, EE) where
V = 11, ... , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E C_ V x V is a set of edges; and EV
and EE are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label EV (i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label EE(i —* j) of an edge i —* j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Hajiˇc et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
</bodyText>
<page confidence="0.874649">
65
</page>
<bodyText confidence="0.999970884615385">
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate–argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg2
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
</bodyText>
<table confidence="0.999281454545455">
DM PAS PCEDT
# labels 51 42 68
% singletons 22.62 4.49 35.79
# edge density 0.96 1.02 0.99
%g trees 2.35 1.30 56.58
%g projective 3.05 1.71 53.29
%g fragmented 6.71 0.23 0.56
%n reentrancies 27.35 29.40 9.27
%g topless 0.28 0.02 0.00
# top nodes 0.9972 0.9998 1.1237
%n non-top roots 44.71 55.92 4.36
</table>
<tableCaption confidence="0.8824575">
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
</tableCaption>
<sectionHeader confidence="0.985728" genericHeader="method">
4 Data Sets
</sectionHeader>
<bodyText confidence="0.999607487179487">
All three target representations are annotations of
the same text, Sections 00–21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like (G.m.b, H.), (S.p, A.), and
(U.S., .), and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00–20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.5 In terms of distinctions
5These statistics are obtained using the ‘official’ SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ‘%n’ denotes (non-singleton) node percentages, and
‘%g’ percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ‘covered’, i.e. for any root j there is no edge i → k
</bodyText>
<table confidence="0.800514666666667">
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM − .6425 .2612 − .6719 .5675
PAS .6688 − .2963 .6993 − .5490
PCEDT .2636 .2963 − .5743 .5630 −
</table>
<tableCaption confidence="0.921671">
Table 3: Pairwise F1 similarities, including punctu-
ation (upper right diagonals) or not (lower left).
</tableCaption>
<bodyText confidence="0.988663905263158">
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ‘tree-oriented’ than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ‘top’ predicate, and the whole graph should
‘hang together’. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F1 (dubbed
OF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i &lt; j &lt; k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;6 while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ‘local’ top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ‘upstairs’ predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ‘vanilla’
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel’ˇcukian analysis of coordinate structures
(Mel’ˇcuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ‘standing in’ for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
</bodyText>
<figure confidence="0.9476852">
67
compound compound compound
ARG1
ARG1
ACT
ARG1
PAT REG
employee stock investment plans
employee stock investment plans
employee stock investment plans
</figure>
<figureCaption confidence="0.999869">
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively.
</figureCaption>
<bodyText confidence="0.999947471428571">
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase–internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...I investments
in stock for employees, i.e. grouping the concept
7As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ‘shared’ modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ‘common stock’).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
‘sentence’ (or ‘conventional’) meaning, on the one
hand, and ‘speaker’ (or ‘occasion’) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
</bodyText>
<sectionHeader confidence="0.992577" genericHeader="method">
5 Task Setup
</sectionHeader>
<bodyText confidence="0.998226045454546">
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit—including
graph input, basic statistics, and scoring—were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ‘companion’
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
</bodyText>
<page confidence="0.846462">
68
</page>
<table confidence="0.999893411764706">
LF DM PAS PCEDT
LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen- 80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malmö
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Linköping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
LF DM PAS PCEDT
LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
</table>
<tableCaption confidence="0.9830835">
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
</tableCaption>
<bodyText confidence="0.9887568">
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate–role–argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ‘virtual’ dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and Fi: LP, LR, LF; (b) unlabeled precision,
recall, and F1: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ‘official’ ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency Fi scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example—provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ‘companion’ syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
</bodyText>
<sectionHeader confidence="0.926259" genericHeader="method">
6 Submissions and Results
</sectionHeader>
<bodyText confidence="0.999884666666667">
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
</bodyText>
<page confidence="0.853349">
69
</page>
<table confidence="0.9760075">
Team Track Approach Resources
Linköping C extension of Eisner’s algorithm for DAGs, edge-factored —
structured perceptron
Potsdam C &amp; O graph-to-tree transformation, Mate companion
Priberam C &amp; O model with second-order features, decoding with dual decom- companion
position, MIRA companion,
Turku O cascade of SVM classifiers (dependency recognition, label syntactic n-grams,
classification, top recognition) word2vec
Alpage C &amp; O transition-based parsing for DAGs, logistic regression, struc- companion,
tured perceptron Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma- —
tion, parser ensemble companion
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
Copenhagen-Malmö C graph-to-tree transformation, Mate —
In-House O existing parsers developed by the organizers grammars
</table>
<tableCaption confidence="0.999405">
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
</tableCaption>
<bodyText confidence="0.999982368421053">
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
</bodyText>
<sectionHeader confidence="0.835754" genericHeader="method">
7 Overview of Approaches
</sectionHeader>
<bodyText confidence="0.99934492">
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &amp;
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning–oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
</bodyText>
<page confidence="0.679188">
70
</page>
<bodyText confidence="0.999978066666667">
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS &gt; DM &gt; PCEDT, which can be taken as
a tentative indicator of relative levels of ‘parsabil-
ity’. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
‘mere’ sentence meaning) in at least some construc-
tions in PCEDT.
</bodyText>
<sectionHeader confidence="0.989829" genericHeader="conclusions">
8 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999993666666667">
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate–
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ‘complete’ predications
(in contrast to more lenient per-dependency F1 used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ‘gold’ track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
</bodyText>
<sectionHeader confidence="0.995577" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999990333333333">
We are grateful to Željko Agi´c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
</bodyText>
<sectionHeader confidence="0.995745" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999325175">
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89–97). Beijing, China.
Bohnet, B., &amp; Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 –1465). Jeju
Island, Korea.
Buchholz, S., &amp; Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149–164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., &amp; Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281–332.
de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449–454). Genoa, Italy.
Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 – 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 – 28.
Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 – 96). Lisbon, Portugal: Edições Colibri.
Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
</reference>
<page confidence="0.542335">
71
</page>
<reference confidence="0.999064432098766">
245–288.
Grice, H. P. (1968). Utterer’s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 – 242.
Hajiˇc, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Martí, M. A., Màrquez, L., ... Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1–18). Boulder, CO, USA.
Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar,
O., Cinková, S., ... Žabokrtský, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 – 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 –
11). Jeju, Republic of Korea.
Kate, R. J., &amp; Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 – 330.
Mel’ˇcuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., &amp; Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 –
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291–330.
Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson,
J., Riedel, S., &amp; Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 – 932). Prague,
Czech Republic.
Oepen, S., &amp; Lønning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250–1255). Genoa, Italy.
Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71–106.
Petrov, S., Barrett, L., Thibaux, R., &amp; Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 –440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., &amp; Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 – 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 –1567).
Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240–247). Prague, Czech Re-
public.
</reference>
<page confidence="0.908665">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9152855">SemEval 2014 Task 8: Broad-Coverage Semantic Dependency Parsing</title>
<author confidence="0.9337075">Marco Yusuke Daniel Jan Angelina</author>
<author confidence="0.9337075">Yi</author>
<affiliation confidence="0.9864275">University of Oslo, Department of Informatics University, Department of Linguistics University, Department of Computer and Information Science Institute of Informatics, Tokyo University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics University, Center for the Study of Language and Information</affiliation>
<address confidence="0.603725">Communications Aachen GmbH</address>
<email confidence="0.981767">sdp-organizers@emmtee.net</email>
<abstract confidence="0.972264528089888">8 at SemEval 2014 defines Broad- Coverage Semantic Dependency Parsas the problem of recovering sentence-internal predicate–argument relafor content i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz &amp; Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and the footer are added by the organizers: Unfortunately, tree-oriented parsers are ill-suited for producing meaning representations, i.e. moving from the analysis of grammatical structure to sentence semantics. Even if syntactic parsing arguably can be limited to tree structures, this is not the case in semantic analysis, where a node will often be the argument of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Task 8 at SemEval 2014, Dependency Parsing to stimulate the dependency parsing community to move towards more general graph processing, thus enable a more direct analysis of did to Whom? English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. is dependent on the determiner (the quantificational locus), the modand the predicate Conversely, predicative copula, infinitival and the vacfurther technical details, information on how to obtain the data, and official results. 63 of the 8th International Workshop on Semantic Evaluation (SemEval pages Dublin, Ireland, August 23-24, 2014. A1 A2 A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice (a) Partial semantic dependencies in PropBank and NomBank. implicit_conj _and_c BV ARG1 ARG2 ARG3 ARG1 ARG1 ARG2 ARG1 ARG1 mwe top A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. (b) DELPH-IN Minimal Recursion Semantics–derived bi-lexical dependencies (DM).</abstract>
<note confidence="0.91209052173913">A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice (c) Enju Predicate–Argument Structures (PAS). A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice . (d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT). Figure 1: Sample semantic dependency graphs for Example (1). ARG1 ARG2 ARG2 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 ARG1 top ARG2 ARG1 ARG1 ARG1 ARG2 ARG1 ARG1 ARG2 ARG1</note>
<abstract confidence="0.974148147916667">ARG2 RSTR top ACT PAT EXT PAT ADDR ADDR ADDR CONJ.m ADDR APPS.m CONJ.m RSTR APPS.m CONJ.m preposition marking the deep object of apbe argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be taskand domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called PAS, dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. In the following paragraphs, 64 we provide some background on the ‘pedigree’ and linguistic characterization of these representations. DM: DELPH-IN MRS-Derived Bi-Lexical Desemantic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) adverb Figure PAS: Enju Predicate-Argument Structures The Enju parsing system is an HPSG-based parser grammar and the disambiguation model of this parser are derived from the Enju HPSG treebank, which is automatically converted from the phrase structure and predicate–argument annotations of the PTB. The set is extracted from the WSJ portion of the Enju HPSG treebank. While the Enju treebank is annotated with full HPSG-style structures, only its predicate–argument structures are converted into the SDP data format for use in this task. Top nodes in this representation denote semantic heads. Again, the system description of Miyao et al. (2014) provides more technical detail on the conversion. PCEDT: Prague Tectogrammatical Bi-Lexical Prague Czech-English De- Treebank (PCEDT; Hajiˇc et al., is a set of parallel dependency trees over the WSJ however, that non-scopal adverbs act as mere inmodifiers, e.g. a predicate in DM, but the verb provides the top node in structures like sang loudly. id form lemma pos top pred arg1 arg2 1 Ms. Ms. NNP − _ 2 Haag Haag NNP − − compound ARG1 3 plays play VBZ + + _ 4 Elianti Elianti NNP − − _ ARG2 5 . . . − − _ _ _ _ 1: Tabular SDP data format (showing texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, are two layers of syntactic annotation: anand dependencies in this task have been extracted from the t-trees. The specifics of are best observed in the procedure that converts the original PCEDT data to the SDP data format; see Miyao et al. (2014). Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence. 3 Graph Representation The SDP target representations can be characteras labeled, directed graphs. Formally, a sedependency graph a sentence ... , a structure E, ... , a set of are in one-to-one correspondence with the tokens of the a set of and mappings that assign some finite alphabet) to nodes and edges, respectively. specifically for this task, the label a a tuple consisting of four components: its word form, lemma, part of speech, and a Boolean flag indicating whether the corresponding token a for the specific sentence. label an edge a semanrelation that holds between The exact definition of what constitutes a top node and what semantic relations are available differs among our three target representations, but note that top nodes can have incoming edges. All data provided for the task uses a columnfile format (dubbed the data similar to the one of the 2009 CoNLL Shared Task (Hajiˇc et al., 2009). As in that task, we assume goldstandard sentence and token segmentation. For ease of reference, each sentence is prefixed by a line with just a unique identifier, using the scheme with a constant leading two-digit sectwo-digit (within each 65 and three-digit (within each For example, identifier denotes the second sentence in the first file of PTB 02, the classic Haag plays Elianti. annotation of this sentence is shown in Table 1. With one exception, our fields (i.e. columns in the tab-separated matrix) are a subset of the CoNLL inventory: (1) (2) (3) and the current token, with token identifiers starting from 1 within each sentence. Besides the lemma and part-of-speech information, in the closed track of our task, there is no explicit analysis of syntax. Across the three target representations in the task, fields (1) and (2) are aligned and uniform, i.e. all representations annotate exactly the same text. On the other hand, fields (3) and (4) are representation-specific, i.e. there are different conventions for lemmatization, and part-of-speech assignments can vary (but all representations use the same PTB inventory of PoS tags). The bi-lexical semantic dependency graph over tokens is represented by two or more columns starting with the obligatory, binary-valued fields (5) (6) A positive value in the column indicates that the node corresponding to token is a (see Section 2 below). The is a simplification of the corresponding field in earlier tasks, indicating whether or not this token represents a predicate, i.e. a node with outgoing dependency edges. With these minor differences to the CoNLL tradition, our file format can represent general, directed graphs, with designated top nodes. For example, there can be singleton nodes not connected to other parts of the graph, and in principle there can be multiple tops, or a non-predicate top node. To designate predicate–argument relations, there are as many additional columns as there are predin the graph (i.e. tokens marked the these additional columns are called (8) etc. These colums contain roles relative to the predicate, i.e. a value in column that the current token is an argument of the (linearly) first predicate in the sentence. In this format, graph reentrancies will lead to a token receiving argument for multiple predicates (i.e. non-empty values in the same row). All tokens of the same sentence must always have all argument columns filled in, even on non-predicate words; in other words, all lines making up one block of tokens will have same number fields, but differ across DM PAS PCEDT % singletons 22.62 4.49 35.79 2.35 1.30 56.58 3.05 1.71 53.29 6.71 0.23 0.56 27.35 29.40 9.27 0.28 0.02 0.00 roots 44.71 55.92 4.36 Table 2: Contrastive high-level graph statistics. sentences, depending on the count of graph nodes. 4 Data Sets All three target representations are annotations of the same text, Sections 00–21 of the WSJ Corpus. For this task, we have synchronized these resources at the sentence and tokenization levels and excluded from the SDP 2014 training and testing data any sentences for which (a) one or more of the treebanks lacked a gold-standard analysis; (b) a one-to-one alignment of tokens could not be established across all three representations; or (c) at least one of the graphs was cyclic. Of the 43,746 sentences in these 22 first sections of WSJ text, Deep- Bank lacks analyses for close to 15%, and the Enju Treebank has gaps for a little more than four percent. Some 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB like and and introducing a few new ones (Fares et al., 2013). Finally, 232 of the graphs obtained through the above conversions were cyclic. In total, we were left with 34,004 sentences (or 745,543 tokens) as training data (Sections 00–20), and 1348 testing sentences (29,808 tokens), from Section 21. Comparison a first attempt at contrasting our three target representations, Table 2 shows some high-level statistics of the graphs comthe training terms of distinctions statistics are obtained using the ‘official’ SDP toolkit. We refer to nodes that have neither incoming nor edges and are not marked as top nodes as these nodes are ignored in subsequent statistics, e.g. when determining the proportion of edges per node (3) or the percentages of rooted trees (4) and fragmented graphs (6). The denotes (non-singleton) node percentages, and percentages over all graphs. We consider a any node that has no incoming edges; nodes have at least two incoming edges. Following Sagae and (2008), we consider a graph there are no crossing edges (in a left-to-right rendering of nodes) and no are ‘covered’, i.e. for any root is no edge → k 66 Directed Undirected DM PAS PCEDT DM PAS PCEDT DM − .6425 .2612 − .6719 .5675 PAS − .2963 .6993 − .5490 PCEDT .2963 − .5743 .5630 − 3: Pairwise including punctuation (upper right diagonals) or not (lower left). drawn in dependency labels (1), there are clear difbetween the representations, with linguistically most fine-grained, and showing the smallest label inventory. Unattached singleton nodes (2) in our setup correspond to tokens analyzed as semantically vacuous, which (as seen in Figure 1) include most punctuation in not Furtherthe other two) analyzes some high-frequency determiners as semantically vacu- Conversely, average has more edges per (non-singleton) nodes than the other two (3), which likely reflects its approach to the analysis of functional words (see below). Judging from both the percentage of actual trees (4), the proportions of projective graphs (5), and the of reentrant nodes (7), much more ‘tree-oriented’ than the other two, which at least in part reflects its approach to the analysis of modifiers and determiners (again, see below). We view the small percentages of graphs without at least one top node (8) and of graphs with at least two non-singleton components that are not interconnected (6) as tentative indicators of general well-formedness. Intuitively, there should always be a ‘top’ predicate, and the whole graph should together’. Only non-trivial (if small) degrees of topless and fragmented graphs, and these may indicate imperfections in the Deep- Bank annotations or room for improvement in the conversion from full MRSs to bi-lexical dependencies, but possibly also exceptions to our intuitions about semantic dependency graphs. Finally, in Table 3 we seek to quantify pairwise structural similarity between the three representain terms of unlabeled dependency Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, structurally much closer to each than either of the two is to more that &lt; j &lt; so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either suggests that directionality of semantic dependencies is a major source of diversion the one hand, and on the other hand. Comparison other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 2 above, we observed that from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking introducing the argument of most punctuation while these (and other functional elements, e.g. complementizers) analyzed as semantically vacuous in function as predicates in do not always serve as ‘local’ top nodes (i.e. the semantic head of the corresponding sub-graph): For example, the infinitival marker in Figure 1 takes the verb as its argument, but the ‘upstairs’ predicate directly to the verb, rather than to the infinitival marker as an intermediate. the same time, alike in their approach to modifiers, e.g. attributive adjectives, adverbs, and prepositional phrases. Unin common syntactic dependency schemes), these are analyzed as semantic predicates and, thus, contribute to higher degrees of node reentrancy and non-top (structural) roots. Roughly the same holds for determiners, but here of Prague tectogrammatical trees onto bi-lexical dependencies leaves ‘vanilla’ (like as singleton nodes. The analysis of coordination is distinct in the three representations, as also evident in Figure 1. design, for what is often called the Mel’ˇcukian analysis of coordinate structures (Mel’ˇcuk, 1988), with a chain of dependencies rooted at the first conjunct (which is thus considered the head, ‘standing in’ for the structure at in the coordinating conjunctions are not integrated with the graph but rather different types of dependencies. In the final coordinating conjunction is the head of the all formats, punctuation marks like dashes, colons, and sometimes commas can be contentful, i.e. at times occur as both predicates, arguments, and top nodes. 67 compound compound compound ARG1 ARG1 ACT ARG1 PAT REG employee stock investment plans employee stock investment plans employee stock investment plans 2: Analysis of nominal compounding in PAS, structure and each coordinating conjunction (or intervening punctuation mark that acts like one) is a two-place predicate, taking left and right conjuncts its arguments. Conversely, in last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected the A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Fig- 2, thus, at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound eithe other hand, annotates both the bracketing and the asof roles, e.g. making In this spirit, the annotations could be directly paraphrased along the lines by employees for investment in stocks. middle position between the other two, disambiguates the bracketing but, by design, merely assigns an underspecified, construction-specific detype; its then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a prepoas in the above paraphrase). The of this specific example happen to diverge in their bracketing decisions, where corresponds to investments stock for i.e. grouping the concept detailed by Miyao et al. (2014), individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a ‘shared’ modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PCEDT. stock contrast to ‘common stock’). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent of 5 Task Setup Training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in early December 2013. In mid-January, a minor update to the training data and optional syntactic ‘companion’ analyses (see below) were provided, and in early February the description and evaluation of a simple baseline system (using tree approximations and the parser of Bohnet, 2010). Towards the end of March, an input-only version of the test data was released, with just columns (1) to (4) pre-filled; participants then had one week to run their systems on these inputs, fill in columns (5), (6), and upwards, and submit their results (from up to two different runs) for scoring. Upon completion of the testing phase, we have shared the gold-standard test data, official scores, and system results for all submissions with participants and are currently preparing all data for general release through the Linguistic</abstract>
<note confidence="0.577702526315789">Data Consortium. 68 LF DM PAS PCEDT LP LR LF LM LP LR LF LM LP LR LF LM Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05 Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42 Malmö 80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01 Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19 Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82 Linköping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01 LF DM PAS PCEDT LP LR LF LM LP LR LF LM LP LR LF LM Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68 CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12 Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82 Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42 Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60 In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30 4: Results of the closed (top) and open tracks (bottom). For each system, the second column</note>
<abstract confidence="0.994509469194312">the averaged across all target representations), which was used to rank the systems. participating in the task were evaluated based on the accuracy with which they can produce semantic dependency graphs for previously unseen text, measured relative to the gold-standard testing data. The key measures for this evaluation were labeled and unlabeled precision and recall with respect to predicted dependencies (predicate–role–argument triples) and labeled and unlabeled exact match with respect to complete graphs. In both contexts, identification of the top node(s) of a graph was considered as the identification of additional, ‘virtual’ dependencies from an artificial root node (at position 0). Below we abbreviate these metrics as (a) labeled precision, and (b) unlabeled precision, and and (c) labeled and exact match: The ‘official’ ranking of participating systems, in both the closed and the open tracks, is determined based on the arithmetic mean of the labeled depenscores (i.e. the geometric mean of labeled precision and labeled recall) on the three target repand Thus, to be considered for the final ranking, a system had to submit semantic dependencies for all three target representations. vs. Open Tracks task was subinto a and an where systems in the closed track could only be trained on the gold-standard semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentenceand token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in 4. The ranking is based on the average score across all three target representations, which given in the In cases where a team submitted two runs to a track, only the highestranked score is included in the table. 69 Team Track Approach Resources Linköping C extension of Eisner’s algorithm for DAGs, edge-factored — structured perceptron Potsdam C &amp; O graph-to-tree transformation, Mate companion C &amp; O model with second-order features, decoding with dual decomcompanion position, MIRA companion, syntactic n-grams, word2vec companion, Brown clusters Turku O cascade of SVM classifiers (dependency recognition, — classification, top recognition) companion C &amp; O transition-based parsing for DAGs, logistic regression, structured perceptron C transition-based parsing for DAGs, graph-to-tree transformation, parser ensemble O edge classification by logistic regression, edge-factored tured SVM Copenhagen-Malmö C graph-to-tree transformation, Mate — In-House O existing parsers developed by the organizers grammars Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any). the closed track, the average across target representations range from 85.91 to 72.20. Comparing the results for different target representhe average across systems are for for 70.17 for The scores for labeled exact match show a much larger variation across both target representations In the open track, we see very similar trends. average across target representations range from 86.27 to 75.89 and the corresponding across systems are 88.64 for 67.52 for these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not two points 7 Overview of Approaches Table 5 shows a summary of the systems that submitted final results. Most of the systems took a strategy to use some algorithm to process (restricted types of) graph structures, and apply machine learning like structured perceptrons. The methods for processing graph structures are classified into three types. One is to transform graphs into trees in the preprocessing stage, and apply conventional dependency parsing systems (e.g. Mate; Bohnet, 2010) to the converted trees. Some systems simply output the result of dependency pars- (which means they inherently lose some depensee the task web page at the address indicated above for full labeled and unlabeled scores. dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae &amp; Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency parsing was successful; for example, Peking built an ensemble of existing transition-based and graph-based dependency parsers, and Priberam extended an existing dependency parser. As we indicated in the task description, a novel feature of this task is that we have to compute graph structures, and cannot assume well-known properties like projectivity and lack of reentrancies. However, many of the participants found that our representations are mostly tree-like, and this fact motivated them to apply methods that have been well studied in the field of syntactic dependency parsing. Finally, we observe that three teams participated in both the closed and open tracks, and all of them reported that adding external resources improved accuracy by a little more than one point. Systems with (only) open submissions extensively use syntactic features (e.g. dependency paths) from external resources, and they are shown effective even 70 with simple machine learning models. Pre-existing, tree-oriented dependency parsers are relatively effective, especially when combined with graph-totree transformation. Comparing across our three target representations, system scores show a tencan be taken as a tentative indicator of relative levels of ‘parsability’. As suggested in Section 4, this variation most likely correlates at least in part with diverging design decisions, e.g. the inclusion of relatively local and deterministic dependencies involving function in the decision to annotate contextually determined speaker meaning (rather than ‘mere’ sentence meaning) in at least some construcin 8 Conclusions and Outlook We have described the motivation, design, and outcomes of the SDP 2014 task on semantic dependency parsing, i.e. retrieving bi-lexical predicate– argument relations between all content words within an English sentence. We have converted to common format three existing annotations over the same text and have put this to use for the first time in training and testing data-driven semantic dependency parsers. Building on strong community interest already to date and our belief that graph-oriented dependency parsing will further gain importance in the years to come, we are preparing a similar (slightly modified) task for SemEval 2015. Candidate modifications and extensions will include cross-domain testing and evaluation at the level of ‘complete’ predications contrast to more lenient per-dependency this year). As optional new sub-tasks, we plan on offering cross-linguistic variation and predicate (i.e. semantic frame) disambiguation for at least some of the target representations. To further probe the role of syntax in the recovery of semantic dependency relations, we will make available to participants a wider selection of syntactic analyses, as well as add a third (idealized) ‘gold’ track, where syntactic dependencies are provided directly from available syntactic annotations of the underlying treebanks. Acknowledgements We are grateful to Željko Agi´c and Bernd Bohnet for consultation and assistance in preparing our baseline and companion parses, to the Linguistic Data Consortium (LDC) for support in distributing the SDP data to participants, as well as to Emily M. Bender and two anonymous reviewers for feedback on this manuscript. Data preparation was supported access to the computing facilities at the University of Oslo, and we acknowledge the Scientific Computing staff at UiO, the Norwegian Metacenter for Computational Science, and the Norwegian tax payers. Part of this work has been supported by the infrastructural funding by the Ministry of Education, Youth and Sports of the Czech Republic (CEP ID LM2010013).</abstract>
<note confidence="0.793787666666667">References Bohnet, B. (2010). Top accuracy and fast depenparsing is not a contradiction. In of the 23rd International Conference on Computa- Linguistics 89–97). Beijing, China. Bohnet, B., &amp; Nivre, J. (2012). A transition-based</note>
<abstract confidence="0.7256184">system for joint part-of-speech tagging and labeled dependency parsing. In of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Language Learning 1455 –1465). Jeju</abstract>
<address confidence="0.630214">Island, Korea. Buchholz, S., &amp; Marsi, E. (2006). CoNLL-X shared</address>
<author confidence="0.313707">In Pro-</author>
<affiliation confidence="0.584283">ceedings of the 10th Conference on Natural Lan-</affiliation>
<address confidence="0.807369">Learning 149–164). New York, NY, USA.</address>
<abstract confidence="0.862874352941176">Copestake, A., Flickinger, D., Pollard, C., &amp; Sag, I. A. (2005). Minimal Recursion Semantics. An introducon Language and 281–332. de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D. (2006). Generating typed dependency parses from structure parses. In of the 5th International Conference on Language Resources and 449–454). Genoa, Italy. Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine learning for high-quality tokenization. Replicating tokenization schemes. In linand intelligent text processing 231 – 244). Springer. Flickinger, D. (2000). On building a more efficient by exploiting types. Language En- 15 – 28.</abstract>
<note confidence="0.849719611111111">Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). Deep- Bank. A dynamically annotated treebank of the Wall Journal. In of the 11th International Workshop on Treebanks and Linguistic Theo- 85 – 96). Lisbon, Portugal: Edições Colibri. Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling semantic roles. 71 245–288. Grice, H. P. (1968). Utterer’s meaning, sentenceand word-meaning. of Lan- 225 – 242. Hajiˇc, J., Ciaramita, M., Johansson, R., Kawahara, D., Martí, M. A., Màrquez, L., ... Zhang, Y. (2009). The CoNLL-2009 Shared Task. syntactic and semandependencies in multiple languages. In Proceedings of the 13th Conference on Natural Language 1–18). Boulder, CO, USA. Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... Žabokrtský, Z. (2012). Announcing Prague Czech-English Dependency Tree- 2.0. In of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160). Istanbul, Turkey. Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D. (2012). Who did what to whom? A contrastive study syntacto-semantic dependencies. In the Sixth Linguistic Annotation Workshop 2 – 11). Jeju, Republic of Korea. Kate, R. J., &amp; Wong, Y. W. (2010). Semantic parsing. The task, the state of the art and the future. In Tutorial abstracts of the 20th Meeting of the Associfor Computational Linguistics 6). Uppsala, Sweden. Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpora of En- The Penn Treebank. Linguis- 313 – 330. I. (1988). syntax. Theory and Albany, NY, USA: SUNY Press. Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., &amp; Grishman, R. (2004). Annotating noun argument structure for NomBank. of the 4th International Conference Language Resources and Evaluation 803 – 806). Lisbon, Portugal. Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-house: An ensemble of pre-existing off-the-shelf parsers. In Proceedings of the 8th International Workshop on Evaluation. Ireland. Nakov, P. (2013). On the interpretation of noun com- Syntax, semantics, and entailment. 291–330. Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J., Riedel, S., &amp; Yuret, D. (2007). The CoNLL 2007 task on dependency parsing. In of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Language Learning 915 – 932). Prague, Czech Republic. Oepen, S., &amp; Lønning, J. T. (2006). Discriminant- MRS banking. In of the 5th International Conference on Language Resources and 1250–1255). Genoa, Italy. Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The Proposition Bank. A corpus annotated with semantic 71–106. Petrov, S., Barrett, L., Thibaux, R., &amp; Klein, D. (2006). Learning accurate, compact, and interpretable tree In of the 21st International Conference on Computational Linguistics and the 44th Meeting of the Association for Computational 433 –440). Sydney, Australia. W. V. O. (1960). and Cambridge, MA, USA: MIT press. Sagae, K., &amp; Tsujii, J. (2008). Shift-reduce depen- DAG parsing. In of the 22nd International Conference on Computational Linguis- 753 – 760). Manchester, UK. Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G. (2009). Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st International Joint Conferon Artifical Intelligence 1562 –1567). Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase to the Penn Treebank. In of the 45th Meeting of the Association for Computa- Linguistics 240–247). Prague, Czech Republic. 72</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (p.</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="26415" citStr="Bohnet, 2010" startWordPosition="4204" endWordPosition="4205"> above could be construed as part of a subsequent task of interpretation. 5 Task Setup Training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in early December 2013. In mid-January, a minor update to the training data and optional syntactic ‘companion’ analyses (see below) were provided, and in early February the description and evaluation of a simple baseline system (using tree approximations and the parser of Bohnet, 2010). Towards the end of March, an input-only version of the test data was released, with just columns (1) to (4) pre-filled; participants then had one week to run their systems on these inputs, fill in columns (5), (6), and upwards, and submit their results (from up to two different runs) for scoring. Upon completion of the testing phase, we have shared the gold-standard test data, official scores, and system results for all submissions with participants and are currently preparing all data for general release through the Linguistic Data Consortium. 68 LF DM PAS PCEDT LP LR LF LM LP LR LF LM LP L</context>
<context position="33630" citStr="Bohnet, 2010" startWordPosition="5371" endWordPosition="5372">acks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 Overview of Approaches Table 5 shows a summary of the systems that submitted final results. Most of the systems took a strategy to use some algorithm to process (restricted types of) graph structures, and apply machine learning like structured perceptrons. The methods for processing graph structures are classified into three types. One is to transform graphs into trees in the preprocessing stage, and apply conventional dependency parsing systems (e.g. Mate; Bohnet, 2010) to the converted trees. Some systems simply output the result of dependency parsing (which means they inherently lose some depen8Please see the task web page at the address indicated above for full labeled and unlabeled scores. dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae &amp; Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target repr</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bohnet, B. (2010). Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (p. 89–97). Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>J Nivre</author>
</authors>
<title>A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 1455 –1465). Jeju Island,</booktitle>
<contexts>
<context position="30718" citStr="Bohnet and Nivre (2012)" startWordPosition="4909" endWordPosition="4912">ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bohnet, B., &amp; Nivre, J. (2012). A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 1455 –1465). Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Natural Language Learning (p.</booktitle>
<pages>149--164</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="1592" citStr="Buchholz &amp; Marsi, 2006" startWordPosition="222" endWordPosition="225">em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz &amp; Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and the proceedings footer are added by the organizers: http:// creativecommons.org/licenses/by/4.0/. Unfortunately, tr</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Buchholz, S., &amp; Marsi, E. (2006). CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the 10th Conference on Natural Language Learning (p. 149–164). New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>281--332</pages>
<contexts>
<context position="8185" citStr="Copestake et al., 2005" startWordPosition="1238" endWordPosition="1241">paragraphs, 64 we provide some background on the ‘pedigree’ and linguistic characterization of these representations. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost i</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Copestake, A., Flickinger, D., Pollard, C., &amp; Sag, I. A. (2005). Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4), 281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (p.</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., MacCartney, B., &amp; Manning, C. D. (2006). Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 449–454). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fares</author>
<author>S Oepen</author>
<author>Y Zhang</author>
</authors>
<title>Machine learning for high-quality tokenization. Replicating variable tokenization schemes.</title>
<date>2013</date>
<booktitle>In Computational linguistics and intelligent text processing (p. 231 – 244).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="16078" citStr="Fares et al., 2013" startWordPosition="2561" endWordPosition="2564">nd testing data any sentences for which (a) one or more of the treebanks lacked a gold-standard analysis; (b) a one-to-one alignment of tokens could not be established across all three representations; or (c) at least one of the graphs was cyclic. Of the 43,746 sentences in these 22 first sections of WSJ text, DeepBank lacks analyses for close to 15%, and the Enju Treebank has gaps for a little more than four percent. Some 500 sentences show tokenization mismatches, most owing to DeepBank correcting PTB idiosyncrasies like (G.m.b, H.), (S.p, A.), and (U.S., .), and introducing a few new ones (Fares et al., 2013). Finally, 232 of the graphs obtained through the above conversions were cyclic. In total, we were left with 34,004 sentences (or 745,543 tokens) as training data (Sections 00–20), and 1348 testing sentences (29,808 tokens), from Section 21. Quantitative Comparison As a first attempt at contrasting our three target representations, Table 2 shows some high-level statistics of the graphs comprising the training data.5 In terms of distinctions 5These statistics are obtained using the ‘official’ SDP toolkit. We refer to nodes that have neither incoming nor outgoing edges and are not marked as top </context>
</contexts>
<marker>Fares, Oepen, Zhang, 2013</marker>
<rawString>Fares, M., Oepen, S., &amp; Zhang, Y. (2013). Machine learning for high-quality tokenization. Replicating variable tokenization schemes. In Computational linguistics and intelligent text processing (p. 231 – 244). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>15--28</pages>
<contexts>
<context position="7940" citStr="Flickinger, 2000" startWordPosition="1206" endWordPosition="1207">running example (Figure 1), showing what are called the DM, PAS, and PCEDT semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. In the following paragraphs, 64 we provide some background on the ‘pedigree’ and linguistic characterization of these representations. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBan</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Flickinger, D. (2000). On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1), 15 – 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>Y Zhang</author>
<author>V Kordoni</author>
</authors>
<title>DeepBank. A dynamically annotated treebank of the Wall Street Journal.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96).</booktitle>
<location>Lisbon, Portugal: Edições Colibri.</location>
<contexts>
<context position="8045" citStr="Flickinger et al. (2012)" startWordPosition="1219" endWordPosition="1222">there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. In the following paragraphs, 64 we provide some background on the ‘pedigree’ and linguistic characterization of these representations. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this tar</context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Flickinger, D., Zhang, Y., &amp; Kordoni, V. (2012). DeepBank. A dynamically annotated treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96). Lisbon, Portugal: Edições Colibri.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>245--288</pages>
<contexts>
<context position="5565" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="846" endWordPosition="849"> ARG1 ARG1 ARG1 ARG1 ARG1 ARG1 top ARG2 ARG1 ARG1 ARG1 ARG2 ARG1 ARG1 ARG2 ARG1 ARG2 RSTR top ACT PAT EXT PAT ADDR ADDR ADDR CONJ.m ADDR APPS.m CONJ.m RSTR APPS.m CONJ.m uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contr</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D., &amp; Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics, 28, 245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Utterer’s meaning, sentencemeaning, and word-meaning.</title>
<date>1968</date>
<journal>Foundations of Language,</journal>
<volume>4</volume>
<issue>3</issue>
<pages>225--242</pages>
<contexts>
<context position="25545" citStr="Grice, 1968" startWordPosition="4070" endWordPosition="4071">nd indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation. 5 Task Setup Training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in early Decembe</context>
</contexts>
<marker>Grice, 1968</marker>
<rawString>Grice, H. P. (1968). Utterer’s meaning, sentencemeaning, and word-meaning. Foundations of Language, 4(3), 225 – 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Martí</author>
<author>L Màrquez</author>
</authors>
<title>The CoNLL-2009 Shared Task. syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Natural Language Learning (p. 1–18).</booktitle>
<location>Boulder, CO, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martí, Màrquez, 2009</marker>
<rawString>Hajiˇc, J., Ciaramita, M., Johansson, R., Kawahara, D., Martí, M. A., Màrquez, L., ... Zhang, Y. (2009). The CoNLL-2009 Shared Task. syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Natural Language Learning (p. 1–18). Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>E Hajiˇcová</author>
<author>J Panevová</author>
<author>P Sgall</author>
<author>O Bojar</author>
<author>S Cinková</author>
</authors>
<title>Announcing Prague Czech-English Dependency Treebank 2.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160).</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Hajiˇc, Hajiˇcová, Panevová, Sgall, Bojar, Cinková, 2012</marker>
<rawString>Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... Žabokrtský, Z. (2012). Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160). Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ivanova</author>
<author>S Oepen</author>
<author>L Øvrelid</author>
<author>D Flickinger</author>
</authors>
<title>Who did what to whom? A contrastive study of syntacto-semantic dependencies.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Linguistic Annotation Workshop (p. 2 – 11). Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="8485" citStr="Ivanova et al., 2012" startWordPosition="1278" endWordPosition="1281">yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 PAS: Enju Predicate-Argument Structures The Enju parsing system is an HPSG-based parser for English.3 The grammar and the disambiguation model of this parser are derived from the Enju HPSG treebank, which is automatically converted from the phrase structure and predicate–argument struct</context>
<context position="20045" citStr="Ivanova et al. (2012)" startWordPosition="3201" endWordPosition="3204">and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PCEDT, even more such that i &lt; j &lt; k. so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PCEDT. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PCEDT on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 2 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;6 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PCEDT, they function as predicates in PAS, though do not always serve as ‘lo</context>
</contexts>
<marker>Ivanova, Oepen, Øvrelid, Flickinger, 2012</marker>
<rawString>Ivanova, A., Oepen, S., Øvrelid, L., &amp; Flickinger, D. (2012). Who did what to whom? A contrastive study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop (p. 2 – 11). Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
</authors>
<title>Semantic parsing. The task, the state of the art and the future.</title>
<date>2010</date>
<booktitle>In Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics (p. 6).</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="6746" citStr="Kate and Wong (2010)" startWordPosition="1023" endWordPosition="1026">ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure</context>
</contexts>
<marker>Kate, Wong, 2010</marker>
<rawString>Kate, R. J., &amp; Wong, Y. W. (2010). Semantic parsing. The task, the state of the art and the future. In Tutorial abstracts of the 20th Meeting of the Association for Computational Linguistics (p. 6). Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpora of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="3115" citStr="Marcus et al., 1993" startWordPosition="456" endWordPosition="459">t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modif</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a large annotated corpora of English: The Penn Treebank. Computational Linguistics, 19, 313 – 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
</authors>
<title>Dependency syntax. Theory and practice.</title>
<date>1988</date>
<publisher>SUNY Press.</publisher>
<location>Albany, NY, USA:</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Mel’ˇcuk, I. (1988). Dependency syntax. Theory and practice. Albany, NY, USA: SUNY Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>Annotating noun argument structure for NomBank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (p. 803 – 806).</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="5718" citStr="Meyers et al., 2004" startWordPosition="869" endWordPosition="873">PPS.m CONJ.m uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., &amp; Grishman, R. (2004). Annotating noun argument structure for NomBank. In Proceedings of the 4th International Conference on Language Resources and Evaluation (p. 803 – 806). Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>S Oepen</author>
<author>D Zeman</author>
</authors>
<title>In-house: An ensemble of pre-existing off-the-shelf parsers.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation.</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="8631" citStr="Miyao et al. (2014)" startWordPosition="1302" endWordPosition="1305">pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 PAS: Enju Predicate-Argument Structures The Enju parsing system is an HPSG-based parser for English.3 The grammar and the disambiguation model of this parser are derived from the Enju HPSG treebank, which is automatically converted from the phrase structure and predicate–argument structure annotations of the PTB. The PAS data set is extracted from the WSJ portion of the Enju HPSG treebank. While the Enju treebank is annotated wit</context>
<context position="10627" citStr="Miyao et al. (2014)" startWordPosition="1624" endWordPosition="1627">1 Ms. Ms. NNP − _ 2 Haag Haag NNP − − compound ARG1 3 plays play VBZ + + _ 4 Elianti Elianti NNP − − _ ARG2 5 . . . − − _ _ _ _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCEDT representations are best observed in the procedure that converts the original PCEDT data to the SDP data format; see Miyao et al. (2014). Top nodes are derived from t-tree roots; i.e. they mostly correspond to main verbs. In case of coordinate clauses, there are multiple top nodes per sentence. 3 Graph Representation The SDP target representations can be characterized as labeled, directed graphs. Formally, a semantic dependency graph for a sentence x = xi, ... , xn is a structure G = (V, E, EV, EE) where V = 11, ... , n} is a set of nodes (which are in one-to-one correspondence with the tokens of the sentence); E C_ V x V is a set of edges; and EV and EE are mappings that assign labels (from some finite alphabet) to nodes and </context>
<context position="24367" citStr="Miyao et al. (2014)" startWordPosition="3881" endWordPosition="3884">the bracketing but, by design, merely assigns an underspecified, construction-specific dependency type; its compound dependency, then, is to be interpreted as the most general type of dependency that can hold between the elements of this construction (i.e. to a first approximation either an argument role or a relation parallel to a preposition, as in the above paraphrase). The DM and PCEDT annotations of this specific example happen to diverge in their bracketing decisions, where the DM analysis corresponds to [...I investments in stock for employees, i.e. grouping the concept 7As detailed by Miyao et al. (2014), individual conjuncts can be (and usually are) arguments of other predicates, whereas the topmost conjunction only has incoming edges in nested coordinate structures. Similarly, a ‘shared’ modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PCEDT. employee stock (in contrast to ‘common stock’). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much prev</context>
</contexts>
<marker>Miyao, Oepen, Zeman, 2014</marker>
<rawString>Miyao, Y., Oepen, S., &amp; Zeman, D. (2014). In-house: An ensemble of pre-existing off-the-shelf parsers. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
</authors>
<title>On the interpretation of noun compounds: Syntax, semantics, and entailment.</title>
<date>2013</date>
<journal>Natural Language Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>291--330</pages>
<contexts>
<context position="25088" citStr="Nakov, 2013" startWordPosition="4000" endWordPosition="4001"> only has incoming edges in nested coordinate structures. Similarly, a ‘shared’ modifier of the coordinate structure as a whole would take as its argument the local top node of the coordination in DM or PAS (i.e. the first conjunct or final conjunction, respectively), whereas it would depend as an argument on all conjuncts in PCEDT. employee stock (in contrast to ‘common stock’). Without context and expert knowledge, these decisions are hard to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation f</context>
</contexts>
<marker>Nakov, 2013</marker>
<rawString>Nakov, P. (2013). On the interpretation of noun compounds: Syntax, semantics, and entailment. Natural Language Engineering, 19(3), 291–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kübler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 915 – 932).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1612" citStr="Nivre et al., 2007" startWordPosition="226" endWordPosition="229">r sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz &amp; Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and the proceedings footer are added by the organizers: http:// creativecommons.org/licenses/by/4.0/. Unfortunately, tree-oriented parsers </context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J., Riedel, S., &amp; Yuret, D. (2007). The CoNLL 2007 shared task on dependency parsing. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 915 – 932). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J T Lønning</author>
</authors>
<title>Discriminantbased MRS banking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (p.</booktitle>
<pages>1250--1255</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="8360" citStr="Oepen &amp; Lønning, 2006" startWordPosition="1262" endWordPosition="1265">antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen &amp; Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 PAS: Enju Predicate-Argument Structures The Enju parsing system is an HPSG-based parser for English.3 The grammar and the disambiguation model of this parser are </context>
</contexts>
<marker>Oepen, Lønning, 2006</marker>
<rawString>Oepen, S., &amp; Lønning, J. T. (2006). Discriminantbased MRS banking. In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 1250–1255). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank. A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--106</pages>
<contexts>
<context position="5696" citStr="Palmer et al., 2005" startWordPosition="865" endWordPosition="868"> APPS.m CONJ.m RSTR APPS.m CONJ.m uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea &amp; Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., &amp; Kingsbury, P. (2005). The Proposition Bank. A corpus annotated with semantic roles. Computational Linguistics, 31(1), 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Meeting of the Association for Computational Linguistics (p. 433 –440).</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="30599" citStr="Petrov et al. (2006)" startWordPosition="4889" endWordPosition="4892">e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Petrov, S., Barrett, L., Thibaux, R., &amp; Klein, D. (2006). Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Meeting of the Association for Computational Linguistics (p. 433 –440). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W V O Quine</author>
</authors>
<title>Word and object.</title>
<date>1960</date>
<publisher>MIT press.</publisher>
<location>Cambridge, MA, USA:</location>
<contexts>
<context position="25531" citStr="Quine, 1960" startWordPosition="4068" endWordPosition="4069">rd to call, and indeed there has been much previous work seeking to identify and annotate the relations that hold between members of a nominal compound (see Nakov, 2013, for a recent overview). To what degree the bracketing and role disambiguation in this example are determined by the linguistic signal (rather than by context and world knowledge, say) can be debated, and thus the observed differences among our representations in this example relate to the classic contrast between ‘sentence’ (or ‘conventional’) meaning, on the one hand, and ‘speaker’ (or ‘occasion’) meaning, on the other hand (Quine, 1960; Grice, 1968). In turn, we acknowledge different plausible points of view about which level of semantic representation should be the target representation for data-driven parsing (i.e. structural analysis guided by the grammatical system), and which refinements like the above could be construed as part of a subsequent task of interpretation. 5 Task Setup Training data for the task, providing all columns in the file format sketched in Section 3 above, together with a first version of the SDP toolkit—including graph input, basic statistics, and scoring—were released to candidate participants in</context>
</contexts>
<marker>Quine, 1960</marker>
<rawString>Quine, W. V. O. (1960). Word and object. Cambridge, MA, USA: MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (p. 753 – 760).</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="17132" citStr="Sagae and Tsujii (2008)" startWordPosition="2725" endWordPosition="2728"> distinctions 5These statistics are obtained using the ‘official’ SDP toolkit. We refer to nodes that have neither incoming nor outgoing edges and are not marked as top nodes as singletons; these nodes are ignored in subsequent statistics, e.g. when determining the proportion of edges per node (3) or the percentages of rooted trees (4) and fragmented graphs (6). The notation ‘%n’ denotes (non-singleton) node percentages, and ‘%g’ percentages over all graphs. We consider a root node any (non-singleton) node that has no incoming edges; reentrant nodes have at least two incoming edges. Following Sagae and Tsujii (2008), we consider a graph projective when there are no crossing edges (in a left-to-right rendering of nodes) and no roots are ‘covered’, i.e. for any root j there is no edge i → k 66 Directed Undirected DM PAS PCEDT DM PAS PCEDT DM − .6425 .2612 − .6719 .5675 PAS .6688 − .2963 .6993 − .5490 PCEDT .2636 .2963 − .5743 .5630 − Table 3: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). drawn in dependency labels (1), there are clear differences between the representations, with PCEDT appearing linguistically most fine-grained, and PAS showing the smallest la</context>
<context position="34072" citStr="Sagae &amp; Tsujii, 2008" startWordPosition="5442" endWordPosition="5445">res are classified into three types. One is to transform graphs into trees in the preprocessing stage, and apply conventional dependency parsing systems (e.g. Mate; Bohnet, 2010) to the converted trees. Some systems simply output the result of dependency parsing (which means they inherently lose some depen8Please see the task web page at the address indicated above for full labeled and unlabeled scores. dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae &amp; Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Sagae, K., &amp; Tsujii, J. (2008). Shift-reduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics (p. 753 – 760). Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
<author>P Merlo</author>
<author>G Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artifical Intelligence</booktitle>
<pages>1562--1567</pages>
<contexts>
<context position="34093" citStr="Titov et al., 2009" startWordPosition="5446" endWordPosition="5449">o three types. One is to transform graphs into trees in the preprocessing stage, and apply conventional dependency parsing systems (e.g. Mate; Bohnet, 2010) to the converted trees. Some systems simply output the result of dependency parsing (which means they inherently lose some depen8Please see the task web page at the address indicated above for full labeled and unlabeled scores. dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae &amp; Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency parsing was successful; </context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Titov, I., Henderson, J., Merlo, P., &amp; Musillo, G. (2009). Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st International Joint Conference on Artifical Intelligence (p. 1562 –1567).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vadas</author>
<author>J Curran</author>
</authors>
<title>Adding Noun Phrase Structure to the Penn Treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Meeting of the Association for Computational Linguistics (p.</booktitle>
<pages>240--247</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23189" citStr="Vadas and Curran (2007)" startWordPosition="3688" endWordPosition="3691">ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>Vadas, D., &amp; Curran, J. (2007). Adding Noun Phrase Structure to the Penn Treebank. In Proceedings of the 45th Meeting of the Association for Computational Linguistics (p. 240–247). Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>