<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.973709">
LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic
Analysis, ESA and Information Retrieval based Features
</title>
<author confidence="0.7281175">
Davide Buscaldi, Joseph Le Roux,
Jorge J. Garcia Flores
</author>
<affiliation confidence="0.513652">
Laboratoire d’Informatique de Paris Nord,
</affiliation>
<address confidence="0.803070666666667">
CNRS, (UMR 7030)
Universit´e Paris 13, Sorbonne Paris Cit´e,
F-93430, Villetaneuse, France
</address>
<email confidence="0.733423">
lbuscaldi,joseph.le-roux,jgflores}
@lipn.univ-paris13.fr
</email>
<sectionHeader confidence="0.99539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993364727272727">
This paper describes the system used by the
LIPN team in the Semantic Textual Similarity
task at *SEM 2013. It uses a support vector re-
gression model, combining different text sim-
ilarity measures that constitute the features.
These measures include simple distances like
Levenshtein edit distance, cosine, Named En-
tities overlap and more complex distances like
Explicit Semantic Analysis, WordNet-based
similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994872">
The Semantic Textual Similarity task (STS) at
*SEM 2013 requires systems to grade the degree of
similarity between pairs of sentences. It is closely
related to other well known tasks in NLP such as tex-
tual entailment, question answering or paraphrase
detection. However, as noticed in (B¨ar et al., 2012),
the major difference is that STS systems must give a
graded, as opposed to binary, answer.
One of the most successful systems in *SEM
2012 STS, (B¨ar et al., 2012), managed to grade pairs
of sentences accurately by combining focused mea-
sures, either simple ones based on surface features
(ie n-grams), more elaborate ones based on lexical
semantics, or measures requiring external corpora
such as Explicit Semantic Analysis, into a robust
measure by using a log-linear regression model.
The LIPN-CORE system is built upon this idea of
combining simple measures with a regression model
to obtain a robust and accurate measure of tex-
tual similarity, using the individual measures as fea-
</bodyText>
<note confidence="0.8124926">
Adrian Popescu
CEA, LIST,
Vision &amp; Content
Engineering Laboratory
F-91190 Gif-sur-Yvette, France
</note>
<email confidence="0.951261">
adrian.popescu@cea.fr
</email>
<bodyText confidence="0.999874909090909">
tures for the global system. These measures include
simple distances like Levenshtein edit distance, co-
sine, Named Entities overlap and more complex dis-
tances like Explicit Semantic Analysis, WordNet-
based similarity, IR-based similarity, and a similar-
ity measure based on syntactic dependencies.
The paper is organized as follows. Measures are
presented in Section 2. Then the regression model,
based on Support Vector Machines, is described in
Section 3. Finally we discuss the results of the sys-
tem in Section 4.
</bodyText>
<sectionHeader confidence="0.965907" genericHeader="method">
2 Text Similarity Measures
</sectionHeader>
<subsectionHeader confidence="0.6458175">
2.1 WordNet-based Conceptual Similarity
(Proxigenea)
</subsectionHeader>
<bodyText confidence="0.999811944444444">
First of all, sentences p and q are analysed in or-
der to extract all the included WordNet synsets. For
each WordNet synset, we keep noun synsets and put
into the set of synsets associated to the sentence, Cp
and Cq, respectively. If the synsets are in one of the
other POS categories (verb, adjective, adverb) we
look for their derivationally related forms in order
to find a related noun synset: if there is one, we put
this synsets in Cp (or Cq). For instance, the word
“playing” can be associated in WordNet to synset
(v)play#2, which has two derivationally related
forms corresponding to synsets (n)play#5 and
(n)play#6: these are the synsets that are added
to the synset set of the sentence. No disambiguation
process is carried out, so we take all possible mean-
ings into account.
Given Cp and Cq as the sets of concepts contained
in sentences p and q, respectively, with |Cp |&gt; |Cq|,
</bodyText>
<page confidence="0.968173">
162
</page>
<subsubsectionHeader confidence="0.242443">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.716307666666667">
and the Shared Task, pages 162–168, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
the conceptual similarity between p and q is calcu-
lated as:
</bodyText>
<equation confidence="0.991950333333333">
s(c1,c2)
(1)
|Cp|
</equation>
<bodyText confidence="0.9998605">
where s(c1, c2) is a conceptual similarity measure.
Concept similarity can be calculated by different
ways. For the participation in the 2013 Seman-
tic Textual Similarity task, we used a variation of
the Wu-Palmer formula (Wu and Palmer, 1994)
named “ProxiGenea” (from the french Proximit´e
G´en´ealogique, genealogical proximity), introduced
by (Dudognon et al., 2010), which is inspired by the
analogy between a family tree and the concept hi-
erarchy in WordNet. Among the different formula-
tions proposed by (Dudognon et al., 2010), we chose
the ProxiGenea3 variant, already used in the STS
2012 task by the IRIT team (Buscaldi et al., 2012).
The ProxiGenea3 measure is defined as:
</bodyText>
<equation confidence="0.999058">
1
s(c1,c2) = 1 + d(c1) + d(c2) − 2 • d(c0) (2)
</equation>
<bodyText confidence="0.99981525">
where c0 is the most specific concept that is present
both in the synset path of c1 and c2 (that is, the Least
Common Subsumer or LCS). The function returning
the depth of a concept is noted with d.
</bodyText>
<subsectionHeader confidence="0.995089">
2.2 IC-based Similarity
</subsectionHeader>
<bodyText confidence="0.99886075">
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik’s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity metric:
</bodyText>
<equation confidence="0.94410975">
1
sjc(c1, c2) = IC(c1) + IC(c2) − 2 • IC(c0) (3)
where IC is the information content introduced by
(Resnik, 1995) as IC(c) = − log P(c).
</equation>
<bodyText confidence="0.5442785">
The similarity between two text segments T1 and
T2 is therefore determined as:
</bodyText>
<equation confidence="0.9764316">
( P max ws(w, w2) * idf (w)
1 wE{T1} w2E{T2}
sim(T1, T2) = P
2idf(w)wE{T1}
The seman
tic similarity between words is calculated
as:
ws(wi,wj) = max
ciEWi,cjinWj sjc(ci,cj). (5)
163 Web 1T (Brants and Franz, 2006) frequency counts.
</equation>
<bodyText confidence="0.967452">
Given the sets of parsed dependencies
and Dq,
for sentence p and q, a dependency d E
is a
triple (l, h, t) where l is the dependency label (for in-
stance, dobj or prep), h the governor and t the depen-
dant. We define the following similarity measure be-
tween two syntactic dependencies
</bodyText>
<equation confidence="0.9750709">
=
Dp
Dx
d1
(l1,h1,t1)
(l2, h2, t2):
dsim(d1, d2) = Lev(l1, l2)
idfh * sWN(h1, h2) + idft * sWN(t1, t2)
*
2
</equation>
<bodyText confidence="0.98838">
where idfh =
and
=
are the inverse document fre-
quencies calculated on Google Web 1T for the gov-
ernors and the dependants (we retain the maximum
for each pair), an
</bodyText>
<equation confidence="0.985309666666667">
max(idf(h1),idf(h2))
idft
max(idf(t1),idf(t2))
</equation>
<bodyText confidence="0.8929775">
d sWN is calculated using formula
2, with two differences:
</bodyText>
<equation confidence="0.931089">
ss(p, q) =
E
c1ECp
max
c2ECy
ws(w, w1) * idf(w)
ed score is 0;
1http://www.d.umn.edu/˜tpederse/similarity.html
+
⎞
⎟⎠(4)
P idf(w)
wE{T2}
P
wE{T2}
max
w1E{T1}
</equation>
<bodyText confidence="0.847786666666667">
where
is calculated as the inverse document
fr
idf(w)
equency of word w, taking into account Google
2
https://github.com/CNGLdlab/LORG-Release
where Wi and Wj are the sets containing all synsets
in WordNet corresponding to word wi and wj, re-
spectively. The IC values used are those calcu-
lated by Ted Pedersen (Pedersen et al., 2004) on the
Bri
</bodyText>
<subsectionHeader confidence="0.7546275">
tish National Corpus1.
2.3 Syntactic Dependencies
</subsectionHeader>
<bodyText confidence="0.962499583333333">
We also wanted for our systems to take syntac-
tic similarity into account. As our measures are
lexically grounded, we chose to use dependen-
cies rather than constituents. Previous experiments
showed that converting constituents to dependen-
cies still achieved best results on out-of-domain
texts (Le Roux et al., 2012), so we decided to use
a 2-step architecture to obtain syntactic dependen-
cies. First we parsed pairs of sentences with the
LORG parser2. Second we converted the resulting
parse trees to Stanford dependencies3.
and d2 =
</bodyText>
<listItem confidence="0.910275875">
(6)
• if the two words to be compared are antonyms,
then the return
3We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
• if one of the words to be compared is not in
WordNet, their similarity is calculated using
the Levenshtein distance.
</listItem>
<bodyText confidence="0.9651005">
The similarity score between p and q, is then cal-
culated as:
</bodyText>
<figure confidence="0.778872714285714">
E max dsim(di, dj)
diEDp djinDq
,
|Dp|
dsim(di, dj)
|Dq|
(7)
</figure>
<subsectionHeader confidence="0.983058">
2.4 Information Retrieval-based Similarity
</subsectionHeader>
<bodyText confidence="0.99979675">
Let us consider two texts p and q, an Information Re-
trieval (IR) system S and a document collection D
indexed by S. This measure is based on the assump-
tion that p and q are similar if the documents re-
trieved by S for the two texts, used as input queries,
are ranked similarly.
Let be Lp = {dp1,..., dpK} and Lq =
{dq1,... , dqK}, dxi E D the sets of the top K docu-
ments retrieved by S for texts p and q, respectively.
Let us define sp(d) and sq(d) the scores assigned by
S to a document d for the query p and q, respectively.
Then, the similarity score is calculated as:
</bodyText>
<equation confidence="0.7633968">
Y (sp(d)−sq(d))�
max(sp(d),sq(d))
(8)
|Lp n Lq|
if |Lp n Lq |=� 0, 0 otherwise.
</equation>
<bodyText confidence="0.999880166666667">
For the participation in this task we indexed a
collection composed by the AQUAINT-24 and the
English NTCIR-85 document collections, using the
Lucene6 4.2 search engine with BM25 similarity.
The K value was empirically set to 20 after some
tests on the STS 2012 data.
</bodyText>
<subsectionHeader confidence="0.60018">
2.5 ESA
</subsectionHeader>
<bodyText confidence="0.5047745">
Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007) represents meaning as a
</bodyText>
<footnote confidence="0.9997415">
4http://www.nist.gov/tac/data/data_desc.html#AQUAINT-2
5http://metadata.berkeley.edu/NTCIR-GeoTime/
ntcir-8-databases.php
6http://lucene.apache.org/core
</footnote>
<bodyText confidence="0.991224">
weighted vector of Wikipedia concepts. Weights
are supposed to quantify the strength of the relation
between a word and each Wikipedia concept using
the tf-idf measure. A text is then represented as a
high-dimensional real valued vector space spanning
all along the Wikipedia database. For this particular
task we adapt the research-esa implementation
(Sorg and Cimiano, 2008)7 to our own home-made
weighted vectors corresponding to a Wikipedia
snapshot of February 4th, 2013.
</bodyText>
<subsectionHeader confidence="0.996135">
2.6 N-gram based Similarity
</subsectionHeader>
<bodyText confidence="0.9996694">
This feature is based on the Clustered Keywords Po-
sitional Distance (CKPD) model proposed in (Bus-
caldi et al., 2009) for the passage retrieval task.
The similarity between a text fragment p and an-
other text fragment q is calculated as:
</bodyText>
<equation confidence="0.8908405">
simngrams(p, q) = E h(x, P) 1 (9)
dxEQ d(x, xmax)
�En
i=1 wi
</equation>
<bodyText confidence="0.995589666666667">
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible n-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
</bodyText>
<listItem confidence="0.590106">
• wi calculates the weight of the term tI as:
</listItem>
<equation confidence="0.999343666666667">
log(ni)
wi = 1 − (10)
1 + log(N)
</equation>
<bodyText confidence="0.998659">
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
</bodyText>
<listItem confidence="0.9064645">
• the function h(x, P) measures the weight of
each n-gram and is defined as:
</listItem>
<figure confidence="0.9310815">
h(x, Pj) Ejk=1 wk if x E Pj
{ 0 otherwise
(11)
7http://code.google.com/p/research-esa/
�
�
sSD(p, q) = max �
E
diEDq
max
djinDp
�
� �
simIR(p, q) = 1
�
dELpnLq
</figure>
<page confidence="0.995251">
164
</page>
<bodyText confidence="0.999310333333333">
Where wk is the weight of the k-th term (see
Equation 10) and j is the number of terms that
compose the n-gram x;
</bodyText>
<listItem confidence="0.7061195">
• d(x�x�ax) is a distance factor which reduces the
1
</listItem>
<bodyText confidence="0.9982022">
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one:
</bodyText>
<equation confidence="0.59363">
d(x, xmax) = 1 + k• ln(1 + L) (12)
</equation>
<bodyText confidence="0.999975166666667">
where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 11).
In our experiments, k was set to 0.1, the default
value in the original model.
</bodyText>
<subsectionHeader confidence="0.98529">
2.7 Other measures
</subsectionHeader>
<bodyText confidence="0.9999215">
In addition to the above text similarity measures, we
used also the following common measures:
</bodyText>
<subsectionHeader confidence="0.586817">
2.7.1 Cosine
</subsectionHeader>
<bodyText confidence="0.99916125">
Given p = (wp,, ... , wpn) and q =
(wql, ... , wqn) the vectors of tf.idf weights asso-
ciated to sentences p and q, the cosine distance is
calculated as:
</bodyText>
<equation confidence="0.995228">
n
wpi X wqi
simcos(p, q) = � n n
2=1 2=1
2=1
wpi 2 X � wqi2 (13)
</equation>
<bodyText confidence="0.97004">
The idf value was calculated on Google Web 1T.
</bodyText>
<subsectionHeader confidence="0.686961">
2.7.2 Edit Distance
</subsectionHeader>
<bodyText confidence="0.999229">
This similarity measure is calculated using the
Levenshtein distance as:
</bodyText>
<equation confidence="0.885405666666667">
Lev(p, q)
simED(p, q) = 1 − (14)
max(|p|, |q|)
</equation>
<bodyText confidence="0.999913">
where Lev(p, q) is the Levenshtein distance be-
tween the two sentences, taking into account the
characters.
</bodyText>
<subsubsectionHeader confidence="0.519758">
2.7.3 Named Entity Overlap
</subsubsectionHeader>
<bodyText confidence="0.999361142857143">
We used the Stanford Named Entity Recognizer
by (Finkel et al., 2005), with the 7 class model
trained for MUC: Time, Location, Organization,
Person, Money, Percent, Date. Then we calculated a
per-class overlap measure (in this way, “France” as
an Organization does not match “France” as a Loca-
tion):
</bodyText>
<equation confidence="0.997125333333333">
2 * |Np n Nq|
ONER(p, q) = (15)
|Np |+ |Nq|
</equation>
<bodyText confidence="0.9999885">
where Np and Nq are the sets of NEs found, respec-
tively, in sentences p and q.
</bodyText>
<sectionHeader confidence="0.986613" genericHeader="method">
3 Integration of Similarity Measures
</sectionHeader>
<bodyText confidence="0.9999135">
The integration has been carried out using the
v-Support Vector Regression model (v-SVR)
(Sch¨olkopf et al., 1999) implementation provided
by LIBSVM (Chang and Lin, 2011), with a radial
basis function kernel with the standard parameters
(v = 0.5).
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999615">
In order to evaluate the impact of the different fea-
tures, we carried out an ablation test, removing one
feature at a time and training a new model with the
reduced set of features. In Table 2 we show the re-
sults of the ablation test for each subset of the *SEM
2013 test set; in Table 1 we show the same test on the
whole test set. Note: the results have been calculated
as the Pearson correlation test on the whole test set
and not as an average of the correlation scores cal-
culated over the composing test sets.
</bodyText>
<table confidence="0.998581">
Feature Removed Pearson Loss
None 0.597 0
N-grams 0.596 0.10%
WordNet 0.563 3.39%
SyntDeps 0.602 −0.43%
Edit 0.584 1.31%
Cosine 0.596 0.10%
NE Overlap 0.603 −0.53%
IC-based 0.598 −0.10%
IR-Similarity 0.510 8.78%
ESA 0.601 −0.38%
</table>
<tableCaption confidence="0.9961225">
Table 1: Ablation test for the different features on the
whole 2013 test set.
</tableCaption>
<page confidence="0.943863">
165
</page>
<table confidence="0.9998975">
FNWN Headlines OnWN SMT
Feature Pearson Loss Pearson Loss Pearson Loss Pearson Loss
None 0.404 0 0.706 0 0.694 0 0.301 0
N-grams 0.379 2.49% 0.705 0.12% 0.698 −0.44% 0.289 1.16%
WordNet 0.376 2.80% 0.695 1.09% 0.682 1.17% 0.278 2.28%
SyntDeps 0.403 0.08% 0.699 0.70% 0.679 1.49% 0.284 1.62%
Edit 0.402 0.19% 0.689 1.70% 0.667 2.72% 0.286 1.50%
Cosine 0.393 1.03% 0.683 2.38% 0.676 1.80% 0.303 −0.24%
NE Overlap 0.410 −0.61% 0.700 0.67% 0.680 1.37% 0.285 1.58%
IC-based 0.391 1.26% 0.699 0.75% 0.669 2.50% 0.283 1.76%
IR-Similarity 0.426 −2.21% 0.633 7.33% 0.589 10.46% 0.249 5.19%
ESA 0.391 1.22% 0.691 1.57% 0.702 −0.81% 0.275 2.54%
</table>
<tableCaption confidence="0.998965">
Table 2: Ablation test for the different features on the different parts of the 2013 test set.
</tableCaption>
<table confidence="0.9999247">
FNWN Headlines OnWN SMT ALL
N-grams 0.285 0.532 0.459 0.280 0.336
WordNet 0.395 0.606 0.552 0.282 0.477
SyntDeps 0.233 0.409 0.345 0.323 0.295
Edit 0.220 0.536 0.089 0.355 0.230
Cosine 0.306 0.573 0.541 0.244 0.382
NE Overlap 0.000 0.216 0.000 0.013 0.020
IC-based 0.413 0.540 0.642 0.285 0.421
IR-based 0.067 0.598 0.628 0.241 0.541
ESA 0.328 0.546 0.322 0.289 0.390
</table>
<tableCaption confidence="0.999954">
Table 3: Pearson correlation calculated on individual features.
</tableCaption>
<bodyText confidence="0.99998">
The ablation test show that the IR-based feature
showed up to be the most effective one, especially
for the headlines subset (as expected), and, quite sur-
prisingly, on the OnWN data. In Table 3 we show
the correlation between each feature and the result
(feature values normalised between 0 and 5): from
this table we can also observe that, on average, IR-
based similarity was better able to capture the se-
mantic similarity between texts. The only exception
was the FNWN test set: the IR-based similarity re-
turned a 0 score 178 times out of 189 (94.1%), indi-
cating that the indexed corpus did not fit the content
of the FNWN sentences. This result shows also the
limits of the IR-based similarity score which needs
a large corpus to achieve enough coverage.
</bodyText>
<subsectionHeader confidence="0.99496">
4.1 Shared submission with INAOE-UPV
</subsectionHeader>
<bodyText confidence="0.9999215">
One of the files submitted by INAOE-UPV,
INAOE-UPV-run3 has been produced using seven
features produced by different teams: INAOE, LIPN
and UMCC-DLSI. We contributed to this joint sub-
mission with the IR-based, WordNet and cosine fea-
tures.
</bodyText>
<sectionHeader confidence="0.980781" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.998048">
In this paper we introduced the LIPN-CORE sys-
tem, which combines semantic, syntactic an lexi-
cal measures of text similarity in a linear regression
model. Our system was among the best 15 runs for
the STS task. According to the ablation test, the best
performing feature was the IR-based one, where a
sentence is considered as a query and its meaning
represented as a set of documents indexed by an IR
system. The second and third best-performing mea-
sures were WordNet similarity and Levenshtein’s
edit distance. On the other hand, worst perform-
ing similarity measures were Named Entity Over-
lap, Syntactic Dependencies and ESA. However, a
correlation analysis calculated on the features taken
one-by-one shows that the contribution of a feature
</bodyText>
<page confidence="0.995995">
166
</page>
<bodyText confidence="0.999989956521739">
on the overall regression result does not correspond
to the actual capability of the measure to represent
the semantic similarity between the two texts. These
results raise the methodological question of how to
combine semantic, syntactic and lexical similarity
measures in order to estimate the impact of the dif-
ferent strategies used on each dataset.
Further work will include richer similarity mea-
sures, like quasi-synchronous grammars (Smith and
Eisner, 2006) and random walks (Ramage et al.,
2009). Quasi-synchronous grammars have been
used successfully for paraphrase detection (Das and
Smith, 2009), as they provide a fine-grained model-
ing of the alignment of syntactic structures, in a very
flexible way, enabling partial alignments and the in-
clusion of external features, like Wordnet lexical re-
lations for example. Random walks have been used
effectively for paraphrase recognition and as a fea-
ture for recognizing textual entailment. Finally, we
will continue analyzing the question of how to com-
bine a wide variety of similarity measures in such a
way that they tackle the semantic variations of each
dataset.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998325">
We would like to thank the Quaero project and the
LabEx EFL8 for their support to this work.
</bodyText>
<sectionHeader confidence="0.998152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981305597222222">
[B¨ar et al.2012] Daniel B¨ar, Chris Biemann, Iryna
Gurevych, and Torsten Zesch. 2012. Ukp: Computing
semantic textual similarity by combining multiple
content similarity measures. In Proceedings of the
6th International Workshop on Semantic Evaluation,
held in conjunction with the 1st Joint Conference
on Lexical and Computational Semantics, pages
435–440, Montreal, Canada, June.
[Brants and Franz2006] Thorsten Brants and Alex Franz.
2006. Web 1t 5-gram corpus version 1.1.
[Buscaldi et al.2009] Davide Buscaldi, Paolo Rosso,
Jos´e Manuel G´omez, and Emilio Sanchis. 2009. An-
swering questions with an n-gram based passage re-
trieval engine. Journal of Intelligent Information Sys-
tems (JIIS), 34(2):113–134.
[Buscaldi et al.2012] Davide Buscaldi, Ronan Tournier,
Nathalie Aussenac-Gilles, and Josiane Mothe. 2012.
8http://www.labex-efl.org
Irit: Textual similarity combining conceptual simi-
larity with an n-gram comparison method. In Pro-
ceedings of the 6th International Workshop on Se-
mantic Evaluation (SemEval 2012), Montreal, Que-
bec, Canada.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector
machines. ACM Transactions on Intelligent Systems
and Technology, 2:27:1–27:27. Software available
at http://www.csie.ntu.edu.tw/˜cjlin/
libsvm.
[Das and Smith2009] Dipanjan Das and Noah A. Smith.
2009. Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proc. ofACL-IJCNLP.
[Dudognon et al.2010] Damien Dudognon, Gilles Hubert,
and Bachelin Jhonn Victorino Ralalason. 2010.
Proxig´en´ea : Une mesure de similarit´e conceptuelle.
In Proceedings of the Colloque Veille Strat´egique Sci-
entifique et Technologique (VSST 2010).
[Finkel et al.2005] Jenny Rose Finkel, Trond Grenager,
and Christopher Manning. 2005. Incorporating non-
local information into information extraction systems
by gibbs sampling. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 363–370, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit semantic
analysis. In Proceedings of the 20th international joint
conference on Artifical intelligence, IJCAI’07, pages
1606–1611, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
[Jiang and Conrath1997] J.J. Jiang and D.W. Conrath.
1997. Semantic similarity based on corpus statistics
and lexical taxonomy. In Proc. of the Int’l. Conf. on
Research in Computational Linguistics, pages 19–33.
[Le Roux et al.2012] Joseph Le Roux, Jennifer Foster,
Joachim Wagner, Rasul Samad Zadeh Kaljahi, and
Anton Bryl. 2012. DCU-Paris13 Systems for the
SANCL 2012 Shared Task. In The NAACL 2012 First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL), pages 1–4, Montr´eal, Canada,
June.
[Mihalcea et al.2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence - Volume 1, AAAI’06, pages 775–
780. AAAI Press.
[Pedersen et al.2004] Ted Pedersen, Siddharth Patward-
han, and Jason Michelizzi. 2004. Wordnet::similarity:
measuring the relatedness of concepts. In Demon-
stration Papers at HLT-NAACL 2004, HLT-NAACL–
</reference>
<page confidence="0.975634">
167
</page>
<reference confidence="0.999647914285714">
Demonstrations ’04, pages 38–41, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Ramage et al.2009] Daniel Ramage, Anna N. Rafferty,
and Christopher D. Manning. 2009. Random walks
for text semantic similarity. In Proceedings of the
2009 Workshop on Graph-based Methods for Natural
Language Processing, pages 23–31. The Association
for Computer Linguistics.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI’95,
pages 448–453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Sch¨olkopf et al.1999] Bernhard Sch¨olkopf, Peter
Bartlett, Alex Smola, and Robert Williamson. 1999.
Shrinking the tube: a new support vector regression
algorithm. In Proceedings of the 1998 conference on
Advances in neural information processing systems II,
pages 330–336, Cambridge, MA, USA. MIT Press.
[Smith and Eisner2006] David A. Smith and Jason Eisner.
2006. Quasi-synchronous grammars: Alignment by
soft projection of syntactic dependencies. In Proceed-
ings of the HLT-NAACL Workshop on Statistical Ma-
chine Translation, pages 23–30, New York, June.
[Sorg and Cimiano2008] Philipp Sorg and Philipp Cimi-
ano. 2008. Cross-lingual Information Retrieval with
Explicit Semantic Analysis. In Working Notes for the
CLEF 2008 Workshop.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting on Association
for Computational Linguistics, ACL ’94, pages 133–
138, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.9973">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.644866">
<title confidence="0.9913415">LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Analysis, ESA and Information Retrieval based Features</title>
<author confidence="0.998769">Davide Buscaldi</author>
<author confidence="0.998769">Joseph Le_Jorge J Garcia</author>
<affiliation confidence="0.859772">Laboratoire d’Informatique de Paris CNRS, (UMR Universit´e Paris 13, Sorbonne Paris</affiliation>
<address confidence="0.985693">F-93430, Villetaneuse,</address>
<email confidence="0.981855">@lipn.univ-paris13.fr</email>
<abstract confidence="0.9983005">This paper describes the system used by the LIPN team in the Semantic Textual Similarity task at *SEM 2013. It uses a support vector regression model, combining different text similarity measures that constitute the features. These measures include simple distances like Levenshtein edit distance, cosine, Named Entities overlap and more complex distances like Explicit Semantic Analysis, WordNet-based similarity, IR-based similarity, and a similarity measure based on syntactic dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<location>Montreal, Canada,</location>
<marker>[B¨ar et al.2012]</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the 6th International Workshop on Semantic Evaluation, held in conjunction with the 1st Joint Conference on Lexical and Computational Semantics, pages 435–440, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<marker>[Brants and Franz2006]</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
<author>Jos´e Manuel G´omez</author>
<author>Emilio Sanchis</author>
</authors>
<title>Answering questions with an n-gram based passage retrieval engine.</title>
<date>2009</date>
<journal>Journal of Intelligent Information Systems (JIIS),</journal>
<volume>34</volume>
<issue>2</issue>
<marker>[Buscaldi et al.2009]</marker>
<rawString>Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and Emilio Sanchis. 2009. Answering questions with an n-gram based passage retrieval engine. Journal of Intelligent Information Systems (JIIS), 34(2):113–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Ronan Tournier</author>
<author>Nathalie Aussenac-Gilles</author>
<author>Josiane Mothe</author>
</authors>
<title>8http://www.labex-efl.org Irit: Textual similarity combining conceptual similarity with an n-gram comparison method.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012),</booktitle>
<location>Montreal, Quebec, Canada.</location>
<marker>[Buscaldi et al.2012]</marker>
<rawString>Davide Buscaldi, Ronan Tournier, Nathalie Aussenac-Gilles, and Josiane Mothe. 2012. 8http://www.labex-efl.org Irit: Textual similarity combining conceptual similarity with an n-gram comparison method. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</note>
<marker>[Chang and Lin2011]</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http://www.csie.ntu.edu.tw/˜cjlin/ libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasisynchronous recognition.</title>
<date>2009</date>
<booktitle>In Proc. ofACL-IJCNLP.</booktitle>
<marker>[Das and Smith2009]</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasisynchronous recognition. In Proc. ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Dudognon</author>
<author>Gilles Hubert</author>
<author>Bachelin Jhonn Victorino Ralalason</author>
</authors>
<title>Proxig´en´ea : Une mesure de similarit´e conceptuelle.</title>
<date>2010</date>
<booktitle>In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST</booktitle>
<marker>[Dudognon et al.2010]</marker>
<rawString>Damien Dudognon, Gilles Hubert, and Bachelin Jhonn Victorino Ralalason. 2010. Proxig´en´ea : Une mesure de similarit´e conceptuelle. In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating nonlocal information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Finkel et al.2005]</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating nonlocal information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>[Gabrilovich and Markovitch2007]</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–1611, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>[Jiang and Conrath1997]</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Anton Bryl</author>
</authors>
<date>2012</date>
<booktitle>DCU-Paris13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),</booktitle>
<pages>1--4</pages>
<location>Montr´eal, Canada,</location>
<marker>[Le Roux et al.2012]</marker>
<rawString>Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCU-Paris13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), pages 1–4, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, AAAI’06,</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<marker>[Mihalcea et al.2006]</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, AAAI’06, pages 775– 780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL– Demonstrations ’04,</booktitle>
<pages>38--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Pedersen et al.2004]</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL– Demonstrations ’04, pages 38–41, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Random walks for text semantic similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>23--31</pages>
<institution>The Association for Computer Linguistics.</institution>
<marker>[Ramage et al.2009]</marker>
<rawString>Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning. 2009. Random walks for text semantic similarity. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, pages 23–31. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>[Resnik1995]</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95, pages 448–453, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Peter Bartlett</author>
<author>Alex Smola</author>
<author>Robert Williamson</author>
</authors>
<title>Shrinking the tube: a new support vector regression algorithm.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1998 conference on Advances in neural information processing systems II,</booktitle>
<pages>330--336</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA.</location>
<marker>[Sch¨olkopf et al.1999]</marker>
<rawString>Bernhard Sch¨olkopf, Peter Bartlett, Alex Smola, and Robert Williamson. 1999. Shrinking the tube: a new support vector regression algorithm. In Proceedings of the 1998 conference on Advances in neural information processing systems II, pages 330–336, Cambridge, MA, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<location>New York,</location>
<marker>[Smith and Eisner2006]</marker>
<rawString>David A. Smith and Jason Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation, pages 23–30, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Sorg</author>
<author>Philipp Cimiano</author>
</authors>
<title>Cross-lingual Information Retrieval with Explicit Semantic Analysis.</title>
<date>2008</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<note>Workshop.</note>
<marker>[Sorg and Cimiano2008]</marker>
<rawString>Philipp Sorg and Philipp Cimiano. 2008. Cross-lingual Information Retrieval with Explicit Semantic Analysis. In Working Notes for the CLEF 2008 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Wu and Palmer1994]</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133– 138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>