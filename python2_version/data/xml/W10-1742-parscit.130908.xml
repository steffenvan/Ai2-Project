<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000265">
<title confidence="0.9994805">
An Augmented Three-Pass System Combination Framework:
DCU Combination System for WMT 2010
</title>
<author confidence="0.99968">
Jinhua Du, Pavel Pecina, Andy Way
</author>
<affiliation confidence="0.9851705">
CNGL, School of Computing
Dublin City University
</affiliation>
<address confidence="0.857782">
Dublin 9, Ireland
</address>
<email confidence="0.991427">
{jdu,ppecina,away}@computing.dcu.ie
</email>
<page confidence="0.992965">
290
</page>
<sectionHeader confidence="0.982535" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999751">
This paper describes the augmented three-
pass system combination framework of
the Dublin City University (DCU) MT
group for the WMT 2010 system combi-
nation task. The basic three-pass frame-
work includes building individual confu-
sion networks (CNs), a super network, and
a modified Minimum Bayes-risk (mCon-
MBR) decoder. The augmented parts for
WMT2010 tasks include 1) a rescoring
component which is used to re-rank the
N-best lists generated from the individual
CNs and the super network, 2) a new hy-
pothesis alignment metric – TERp – that
is used to carry out English-targeted hy-
pothesis alignment, and 3) more differ-
ent backbone-based CNs which are em-
ployed to increase the diversity of the
mConMBR decoding phase. We took
part in the combination tasks of English-
to-Czech and French-to-English. Exper-
imental results show that our proposed
combination framework achieved 2.17 ab-
solute points (13.36 relative points) and
1.52 absolute points (5.37 relative points)
in terms of BLEU score on English-to-
Czech and French-to-English tasks re-
spectively than the best single system. We
also achieved better performance on hu-
man evaluation.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993632771929825">
In several recent years, system combination has
become not only a research focus, but also a pop-
ular evaluation task due to its help in improving
machine translation quality. Generally, most com-
bination approaches are based on a confusion net-
work (CN) which can effectively re-shuffle the
translation hypotheses and generate a new target
sentence. A CN is essentially a directed acyclic
graph built from a set of translation hypotheses
against a reference or “backbone”. Each arc be-
tween two nodes in the CN denotes a word or to-
ken, possibly a null item, with an associated pos-
terior probability.
Typically, the dominant CN is constructed at the
word level by a state-of-the-art framework: firstly,
a minimum Bayes-risk (MBR) decoder (Kumar
and Byrne, 2004) is utilised to choose the back-
bone from a merged set of hypotheses, and then
the remaining hypotheses are aligned against the
backbone by a specific alignment approach. Cur-
rently, most research in system combination has
focused on hypothesis alignment due to its signif-
icant influence on combination quality.
A multiple CN or “super-network” framework
was firstly proposed in Rosti et al. (2007) who
used each of all individual system results as the
backbone to build CNs based on the same align-
ment metric, TER (Snover et al., 2006). A consen-
sus network MBR (ConMBR) approach was pre-
sented in (Sim et al., 2007), where MBR decod-
ing is employed to select the best hypothesis with
the minimum cost from the original single system
outputs compared to the consensus output.
Du and Way (2009) proposed a combination
strategy that employs MBR, super network, and
a modified ConMBR (mConMBR) approach to
construct a three-pass system combination frame-
work which can effectively combine different hy-
pothesis alignment results and easily be extended
to more alignment metrics. Firstly, a number of
individual CNs are built based on different back-
bones and different kinds of alignment metrics.
Each network generates a 1-best output. Secondly,
a super network is constructed combining all the
individual networks, and a consensus is generated
based on a weighted search model. In the third
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 290–295,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
pass, all the 1-best hypotheses coming from sin-
gle MT systems, individual networks, and the su-
per network are combined to select the final result
using the mConMBR decoder.
In the system combination task of WMT 2010,
we adopted an augmented framework by extend-
ing the strategy in (Du and Way, 2009). In addi-
tion to the basic three-pass architecture, we aug-
ment our combination system as follows:
</bodyText>
<listItem confidence="0.956271">
• We add a rescoring component in Pass 1 and
Pass 2.
• We introduce the TERp (Snover et al., 2009)
alignment metric for the English-targeted
combination.
• We employ different backbones and hypothe-
sis alignment metrics to increase the diversity
of candidates for our mConMBR decoding.
</listItem>
<bodyText confidence="0.997553818181818">
The remainder of this paper is organised as fol-
lows. In Section 2, we introduce the three hy-
pothesis alignment methods used in our frame-
work. Section 3 details the steps for building our
augmented three-pass combination framework. In
Section 4, a rescoring model with rich features
is described. Then, Sections 5 and 6 respec-
tively report the experimental settings and exper-
imental results on English-to-Czech and French-
to-English combination tasks. Section 7 gives our
conclusions.
</bodyText>
<sectionHeader confidence="0.960259" genericHeader="introduction">
2 Hypothesis Alignment Methods
</sectionHeader>
<bodyText confidence="0.997472444444444">
Hypothesis alignment plays a vital role in the CN,
as the backbone sentence determines the skeleton
and the word order of the consensus output.
In the combination evaluation task, we inte-
grated TER (Snover et al., 2006), HMM (Ma-
tusov et al., 2006) and TERp (Snover et al.,
2009) into our augmented three-pass combination
framework. In this section, we briefly describe
these three methods.
</bodyText>
<subsectionHeader confidence="0.907245">
2.1 TER
</subsectionHeader>
<bodyText confidence="0.977984304347826">
The TER (Translation Edit Rate) metric measures
the ratio of the number of edit operations between
the hypothesis E0 and the reference Eb to the total
number of words in Eb. Here the backbone Eb is
assumed to be the reference. The allowable edits
include insertions (Ins), deletions (Del), substitu-
tions (Sub), and phrase shifts (Shft). The TER of
E0 compared to Eb is computed as in (1):
TER(E&apos;, Eb) = Ins + Del +Sub + Shft x 100% (1)
where Nb is the total number of words in Eb. The
difference between TER and Levenshtein edit dis-
tance (or WER) is the sequence shift operation al-
lowing phrasal shifts in the output to be captured.
The phrase shift edit is carried out by a greedy
algorithm and restricted by three constraints: 1)
The shifted words must exactly match the refer-
ence words in the destination position. 2) The
word sequence of the hypothesis in the original
position and the corresponding reference words
must not exactly match. 3) The word sequence
of the reference that corresponds to the desti-
nation position must be misaligned before the
shift (Snover et al., 2006).
</bodyText>
<subsectionHeader confidence="0.985611">
2.2 HMM
</subsectionHeader>
<bodyText confidence="0.970584666666667">
The hypothesis alignment model based on HMM
(Hidden Markov Model) considers the align-
ment between the backbone and the hypoth-
esis as a hidden variable in the conditional
probability Pr(E0|Eb). Given the backbone
Eb = {e1, ... , eI} and the hypothesis E0 =
{e01, ... , e0J}, which are both in the same lan-
guage, the probability Pr(E0|Eb) is defined as in
(2):
</bodyText>
<equation confidence="0.953337">
Pr(E0|Eb) = � Pr(E0, A|Eb) (2)
A
</equation>
<bodyText confidence="0.999969571428571">
where the alignemnt A C_ {(j, i) : 1 G j G
J;1 G i G I}, i and j represent the word po-
sition in Eb and E0 respectively. Hence, the align-
ment issue is to seek the optimum alignment A�
such that:
where p(aj|aj−1, I) is the alignment probability
and p(e0j|ei) is the translation probability.
</bodyText>
<subsectionHeader confidence="0.999376">
2.3 TER-Plus
</subsectionHeader>
<bodyText confidence="0.971535714285714">
TER-Plus (TERp) is an extension of TER that
aligns words in the hypothesis and reference not
only when they are exact matches but also when
the words share a stem or are synonyms (Snover
et al., 2009). In addition, it uses probabilistic
phrasal substitutions to align phrases in the hy-
pothesis and reference. In contrast to the use of
</bodyText>
<page confidence="0.745359">
291
</page>
<equation confidence="0.6083705">
A� = arg max P(A|eI1, e0J1 ) (3)
A
</equation>
<bodyText confidence="0.915413">
For the HMM-based model, equation (2) can be
represented as in (4):
</bodyText>
<equation confidence="0.995389666666667">
J
Pr(E0|Eb) = � Ip(aj|aj−1, I) &apos; p(e0j|ea,)] (4)
a� j=1
</equation>
<page confidence="0.985147">
292
</page>
<bodyText confidence="0.99994525">
the constant edit cost for all operations such as
shifts, insertion, deleting or substituting in TER,
all edit costs in TERp are optimized to maximize
correlation with human judgments.
TERp uses all the edit operations of TER –
matches, insertions, deletions, substitutions, and
shifts – as well as three new edit operations:
stem matches, synonym matches, and phrase sub-
stitutions (Snover et al., 2009). TERp employs
the Porter stemming algorithm (Porter, 1980) and
WordNet (Fellbaum, 1998) to perform the “stem
match” and “synonym match” respectively. Se-
quences of words in the reference are considered
to be paraphrases of a sequence of words in the
hypothesis if that phrase pair occurs in the TERp
phrase table (Snover et al., 2009).
In our experiments, TERp was used for the
French-English system combination task, and we
used the default configuration of optimised edit
costs.
</bodyText>
<sectionHeader confidence="0.9341375" genericHeader="method">
3 Augmented Three-Pass Combination
Framework
</sectionHeader>
<bodyText confidence="0.9982195">
The construction of the augmented three-pass
combination framework is shown in Figure 1.
</bodyText>
<figure confidence="0.9330485">
NSingle MT
Systems
</figure>
<figureCaption confidence="0.999943">
Figure 1: Three-Pass Combination Framework
</figureCaption>
<bodyText confidence="0.9999435">
In Figure 1, the dashed boxes labeled “TERp”
indicate that the TERp alignment is only appli-
cable for English-targeted hypothesis alignment.
The lines with arrows pointing to “mConMBR”
represent adding outputs into the mConMBR de-
coding component. “Top M Single” indicates that
the 1-best results from the best M individual MT
systems are also used as backbones to build in-
dividual CNs under different alignment metrics.
The three dashed boxes represent Pass 1, Pass 2
and Pass 3 respectively. The steps can be sum-
marised as follows:
</bodyText>
<listItem confidence="0.972744186046512">
Pass 1: Specific Metric-based Single Networks
1. Merge all the 1-best hypotheses from single
MT systems into a new N-best set N3.
2. Utilise the standard MBR decoder to se-
lect one from the N3 as the backbone given
some specific loss function such as TER,
BLEU (Papineni et al., 2002) and TERp; Ad-
ditionally, in order to increase the diversity
of candidates used for Pass 2 and Pass 3, we
also use the 1-best hypotheses from the top
M single MT systems as the backbone. Add
the backbones generated by MBR into N3.
3. Perform the word alignment between the dif-
ferent backbones and the other hypotheses
via the TER, HMM, TERp (only for English)
metrics.
4. Carry out word reordering based on word
alignment (TER and TERp have completed
the reordering in the process of scoring) and
build individual CNs (Rosti et al., 2007);
5. Decode the single networks and export the 1-
best outputs and the N-best lists separately.
Add these 1-best outputs into N3.
Pass 2: Super-Network
1. Connect the single networks using a start
node and an end node to form a super-
network based on multiple hypothesis align-
ment and different backbones. In this evalu-
ation, we set uniform weights for these dif-
ferent individual networks when building the
super network(Du and Way, 2009).
2. Decode the super network and generate a
consensus output as well as the N-best list.
Add the 1-best result into N3.
3. Rescore the N-best lists from all individual
networks and super network and add the new
1-best results into N3.
Pass 3: mConMBR
1. Rename the set N3 as a new set N,,,,,;
2. Use mConMBR decoding to search for the
best final result from N,,,,,. In this step, we
set a uniform distribution between the candi-
dates in N,,,,,.
</listItem>
<figure confidence="0.997956571428571">
BLEU
MBR
TER
Nbest
Re-ranking
HMM
TERp
Hypotheses Set
Individual CNs
mConMBR
Alignment
TER
Super CN Networks
BLEU
TERp
Top M Single
Pass 3
TER
Pass 1
TERp
Pass 2
</figure>
<sectionHeader confidence="0.972727" genericHeader="method">
4 Rescoring Model
</sectionHeader>
<bodyText confidence="0.999835666666667">
We adapted our previous rescoring model (Du
et al., 2009) to larger-scale data. The features we
used are as follows:
</bodyText>
<listItem confidence="0.997989894736842">
• Direct and inverse IBM model;
• 4-gram and 5-gram target language model;
• 3, 4, and 5-gram Part-of-Speech (POS) lan-
guage model (Schmid, 1994; Ratnaparkhi,
1996);
• Sentence-length posterior probability (Zens
and Ney, 2006);
• N-gram posterior probabilities within the N-
best list (Zens and Ney, 2006);
• Minimum Bayes Risk cost. This process is
similar to the calculation of the MBR decod-
ing in which we take the current hypothesis
in the N-best list as the “backbone”, and then
calculate and sum up all the Bayes risk cost
between the backbone and each of the rest of
the N-best list using BLEU metric as the loss
function;
• Length ratio between source and target sen-
tence.
</listItem>
<bodyText confidence="0.996789">
The weights are optimized via the MERT algo-
rithm (Och, 2003).
</bodyText>
<sectionHeader confidence="0.997746" genericHeader="method">
5 Experimental Settings
</sectionHeader>
<bodyText confidence="0.9998382">
We participated in the English–Czech and
French–English system combination tasks.
In our system combination framework, we use
a large-scale monolingual data to train language
models and carry out POS-tagging.
</bodyText>
<subsectionHeader confidence="0.9760755">
5.1 English-Czech
Training Data
</subsectionHeader>
<bodyText confidence="0.9985145">
The statistics of the data used for language models
training are shown in Table 1.
</bodyText>
<table confidence="0.997216833333333">
Corpus Monolingual Number of
tokens (Cz) sentences
News-Comm 2,214,757 84,706
CzEng 81,161,278 8,027,391
News 205,600,053 13,042,040
Total 288,976,088 21,154,137
</table>
<tableCaption confidence="0.999892">
Table 1: Statistics of data in the En–Cz task
</tableCaption>
<bodyText confidence="0.999773181818182">
“CzEng” is the Czech–English corpus v0.9 (Bo-
jar and ˇZabokrtsk´y, 2009). “News” is the Czech
monolingual News corpus.
As to our CN and rescoring components,
we use “News-Comm+CzEng” to train a
4-gram language model and use “News-
Comm+CzEng+News” to train a 5-gram
language model. Additionally, we per-
form POS tagging (Hajiˇc, 2004) for ‘News-
Comm+CzEng+News” data, and train 3-gram,
4-gram, and 5-gram POS-tag language models.
</bodyText>
<subsectionHeader confidence="0.557471">
Devset and Testset
</subsectionHeader>
<bodyText confidence="0.9999295">
The devset includes 455 sentences and the testset
contains 2,034 sentences. Both data sets are pro-
vided by the workshop organizers. Each source
sentence has only one reference. There are 11 MT
systems in the En-Cz track and we use all of them
in our combination experiments.
</bodyText>
<subsectionHeader confidence="0.9778395">
5.2 French-English
Training Data
</subsectionHeader>
<bodyText confidence="0.999349">
The statistics of the data used for language models
training and POS tagging are shown in Table 2.
</bodyText>
<table confidence="0.998371166666667">
Corpus Monolingual Number of
tokens (En) sentences
News-Comm 2,973,711 125,879
Europarl 50,738,215 1,843,035
News 1,131,527,255 48,648,160
Total 1,184,234,384 50,617,074
</table>
<tableCaption confidence="0.999501">
Table 2: Statistics of data in the Fr–En task
</tableCaption>
<bodyText confidence="0.999904875">
“News” is the English monolingual News
corpus. We use “News-Comm+Europarl” to
train a 4-gram language model and use “News-
Comm+Europarl+News” to train a 5-gram lan-
guage model. We also perform POS tagging (Rat-
naparkhi, 1996) for all available data, and train
3-gram, 4-gram and, 5-gram POS-tag language
models.
</bodyText>
<subsectionHeader confidence="0.557262">
Devset and Testset
</subsectionHeader>
<bodyText confidence="0.999928">
We also use all the 1-best results to carry out sys-
tem combination. There are 14 MT systems in the
Fr-En track and we use all of them in our combi-
nation experiments.
</bodyText>
<sectionHeader confidence="0.997829" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.995138">
In this section, all the results are reported on de-
vsets in terms of BLEU and NIST scores.
</bodyText>
<footnote confidence="0.69141575">
All the data are provided by the workshop 6.1 English–Czech
organisers. 1 In Table 1, “News-Comm” indi- In this task, we only used one hypothesis align-
cates the data set of News-Commentary v1.0 and 293ment method – TER – to carry out hypothesis
1http://www.statmt.org/wmt10/translation-task.html
</footnote>
<bodyText confidence="0.985438222222222">
alignment. However, in order to increase diversity
for our 3-pass framework, in addition to using the
output from MBR decoding as the backbone, we
also separately selected the top 4 individual sys-
tems (SYS1, SYS4, SYS6, and SYS11 in our sys-
tem set) in terms of BLEU scores on the devset as
the backbones so that we can build multiple indi-
vidual CNs for the super network. All the results
are shown in Table 3.
</bodyText>
<table confidence="0.9997696">
SYS BLEU4 NIST
Worst 9.09 3.83
Best 17.28 4.99
SYS1 15.11 4.76
SYS4 12.67 4.40
SYS6 17.28 4.99
SYS11 15.75 4.81
CN-SYS1 17.36 5.12
CN-SYS4 16.94 5.10
CN-SYS6 17.91 5.13
CN-SYS11 17.45 5.09
CN-MBR 18.29 5.15
SuperCN 18.44 5.17
mConMBR-BAS 18.60 5.18
mConMBR-New 18.84 5.11
</table>
<tableCaption confidence="0.8676215">
Table 3: Automatic evaluation of the combination
results on the En-Cz devset.
</tableCaption>
<bodyText confidence="0.9996551">
“Worst” indicates the 1-best hypothesis from
the worst single system, the “Best” is the 1-best
hypothesis from the best single system (SYS11)).
“CN-SYSX” denotes that we use SYSX (X =
1, 4,6, 11 and MBR) as the backbone to build an
individual CN. “mConMBR-BAS” stands for the
original three-pass combination framework with-
out rescoring component, while “mConMBR-
New” indicates the proposed augmented combina-
tion framework. It can be seen from Table 3 that 1)
in all individual CNs, the CN-MBR achieved the
best performance; 2) SuperCN and mConMBR-
New improved by 1.16 (6.71% relative) and 1.56
(9.03% relative) absolute BLEU points compared
to the best single MT system. 3) our new
three-pass combination framework achieved the
improvement of 0.24 absolute (1.29% relative)
BLEU points than the original framework.
The final results on the test set are shown in Ta-
ble 4.
</bodyText>
<table confidence="0.99506825">
SYS BLEU4 human eval.(%win)
Best 16.24 70.38
mConMBR-BAS 17.91 -
mConMBR-New 18.41 2 75.17
</table>
<tableCaption confidence="0.904638">
Table 4: Evaluation of the combination results on
the En-Cz testset.
</tableCaption>
<bodyText confidence="0.991962571428571">
It can be seen that our “mConMBR-New”
framework performs better than the best single
system and our original framework “mConMBR-
BAS” in terms of automatic BLEU scores and hu-
man evaluation for the English-to-Czech task. In
this task campaign, we achieved top 1 in terms of
the human evaluation.
</bodyText>
<subsectionHeader confidence="0.999913">
6.2 French–English
</subsectionHeader>
<bodyText confidence="0.999880166666667">
We used three hypothesis alignment methods –
TER, TERp and HMM – to carry out word align-
ment between the backbone and the rest of the
hypotheses. Apart from the backbone generated
from MBR, we separately select the top 5 individ-
ual systems (SYS1, SYS10, SYS11, SYS12, and
SYS13 in our system set) respectively as the back-
bones using HMM, TER and TERp to carry out
hypothesis alignment so that we can build more
individual CNs for the super network to increase
the diversity of candidates for mConMBR. The re-
sults are shown in Table 5.3
</bodyText>
<table confidence="0.99955775">
SYS BLEU4(%) NIST
Worst 15.04 4.97
Best 28.88 6.71
CN-SYS1-TER 29.56 6.78
CN-SYS1-HMM 29.60 6.84
CN-SYS1-TERp 29.77 6.83
CN-MBR-TER 30.16 6.91
CN-MBR-HMM 30.19 6.92
CN-MBR-TERp 30.27 6.92
SuperCN 30.58 6.90
mConMBR-BAS 30.74 7.01
mConMBR-New 31.02 6.96
</table>
<tableCaption confidence="0.800281">
Table 5: Automatic evaluation of the combination
results on the Fr-En devset.
</tableCaption>
<bodyText confidence="0.999719153846154">
“CN-MBR-X” represents the different possi-
ble hypothesis alignment methods (X = {TER,
HMM, TERp}) which are used to build indi-
vidual CNs using the output from MBR de-
coding as the backbone. We can see that the
SuperCN and mConMBR-New respectively im-
proved by 1.7 absolute (5.89% relative) and 2.88
absolute (9.97% relative) BLEU points compared
to the best single system. Furthermore, our aug-
mented framework “mConMBR-New” achieved
the improvement of 0.28 absolute (0.91% relative)
BLEU points than the original three-pass frame-
work as well.
</bodyText>
<footnote confidence="0.998362333333333">
2This score was measured in-house on the refer-
ence provided by the organizer using metric mteval-v13
(ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl).
3In this Table, we take SYS1 as an example to show the
results using a single MT system as the backbone under the
294three alignment metrics.
</footnote>
<bodyText confidence="0.671321">
The final results on the test set are shown in Ta-
ble 6.
</bodyText>
<table confidence="0.9984025">
SYS BLEU4 human eval.(%win)
Best 28.30 66.84
mConMBR-BAS 29.21 -
mConMBR-New 29.82 2 72.15
</table>
<tableCaption confidence="0.9435465">
Table 6: Evaluation of the combination results on
Fr-En test set.
</tableCaption>
<bodyText confidence="0.9820916">
It can be seen that our “mConMBR-New”
framework performs the best than the best single
system and our original framework “mConMBR-
BAS” in terms of automatic BLEU scores and hu-
man evaluation for the French–English task.
</bodyText>
<sectionHeader confidence="0.998051" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999949375">
We proposed an augmented three-pass mul-
tiple system combination framework for the
WMT2010 system combination shared task. The
augmented parts include 1) a rescoring model to
select the potential 1-best result from the indi-
vidual CNs and super network to increase the di-
versity for “mConMBR” decoding; 2) a new hy-
pothesis alignment metric “TERp” for English-
targeted alignment; 3) 1-best results from the top
M individual systems employed to build CNs
to augment the “mConMBR” decoding. We
took part in the English-to-Czech and French-to-
English tasks. Experimental results reported on
test set of these two tasks showed that our aug-
mented framework performed better than the best
single system in terms of BLEU scores and hu-
man evaluation. Furthermore, the proposed aug-
mented framework achieved better results than our
basic three-pass combination framework (Du and
Way, 2009) as well in terms of automatic evalua-
tion scores. In the released preliminary results, we
achieved top 1 and top 3 for the English-to-Czech
and French-to-English tasks respectively in terms
of human evaluation.
As for future work, firstly we plan to do further
experiments using automatic weight-tuning algo-
rithm to tune our framework. Secondly, we plan
to examine how the differences between the hy-
pothesis alignment metrics impact on the accuracy
of the super network. We also intend to integrate
more alignment metrics to the networks and verify
on the other language pairs.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998647444444445">
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University
and has been partially funded by PANACEA, a 7th Frame-
work Research Programme of the European Union (contract
number: 7FP-ITC-248064) as well as partially supported by
the project GA405/09/0278 of the Grant Agency of the Czech
Republic. Thanks also to the reviewers for their insightful
comments.
</bodyText>
<sectionHeader confidence="0.996403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999366881355932">
Bojar, O. and ˇZabokrtsk´y, Z. (2009). CzEng0.9: Large Par-
allel Treebank with Rich Annotation. Prague Bulletin of
Mathematical Linguistics, 92.
Du, J., He, Y., Penkale, S., and Way, A. (2009). MaTrEx:
The DCU MT System for WMT2009. In Proceedings of
the EACL-WMT 2009, pages 95–99, Athens, Greece.
Du, J. and Way, A. (2009). A Three-pass System Com-
bination Framework by Combining Multiple Hypothesis
Alignment Methods. In Proceedings of the International
Conference on Asian Language Processing (IALP), pages
172–176, Singapore.
Fellbaum, C., editor (1998). WordNet: an electronic lexical
database. MIT Press.
Hajiˇc, J. (2004). Disambiguation of Rich Inflection (Compu-
tational Morphology of Czech), volume 1. Charles Uni-
versity Press, Prague.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL 2004, pages 169–176, Boston,
MA.
Matusov, E., Ueffing, N., and Ney, H. (2006). Computing
consensus translation from multiple machine translation
systems using enhanced hypotheses alignment. In Pro-
ceedings of EACL’06, pages 33–40.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160–167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the ACL-02, pages 311–
318, Philadelphia, PA.
Porter, M. F. (1980). An algorithm for suffix stripping, pro-
gram.
Ratnaparkhi, A. (1996). A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
EMNLP’96, pages 133–142, Philadelphia, PA.
Rosti, A., Matsoukas, S., and Schwartz, R. (2007). Improved
Word-Level System Combination for Machine Transla-
tion. In Proceedings of ACL’07, pages 312–319.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44–49, Manchester, UK.
Sim, K., Byrne, W., Gales, M., Sahbi, H., and Woodland, P.
(2007). Consensus network decoding for statistical ma-
chine translation system combination. In Proceedings of
the ICASSP’07, pages 105–108.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate
with targeted human annotation. In Proceedings of the
AMTA’06), pages 223–231, Cambridge, MA.
Snover, M., Madnani, N., J.Dorr, B., and Schwartz, R.
(2009). Fluency, adequacy, or HTER? Exploring different
human judgments with a tunable MT metric. In Proceed-
ings of the WMT’09, pages 259–268, Athens, Greece.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
295 HLT-NAACL’06), pages 72–77, New York, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270527">
<title confidence="0.7109055">An Augmented Three-Pass System Combination DCU Combination System for WMT 2010</title>
<author confidence="0.986326">Jinhua Du</author>
<author confidence="0.986326">Pavel Pecina</author>
<author confidence="0.986326">Andy Way</author>
<affiliation confidence="0.979952">CNGL, School of Dublin City</affiliation>
<address confidence="0.829398">Dublin 9, Ireland 290</address>
<abstract confidence="0.997814483870968">This paper describes the augmented threepass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combination task. The basic three-pass framework includes building individual confusion networks (CNs), a super network, and a modified Minimum Bayes-risk (mCon- MBR) decoder. The augmented parts for WMT2010 tasks include 1) a rescoring component which is used to re-rank the lists generated from the individual CNs and the super network, 2) a new hypothesis alignment metric – TERp – that is used to carry out English-targeted hypothesis alignment, and 3) more different backbone-based CNs which are employed to increase the diversity of the mConMBR decoding phase. We took part in the combination tasks of Englishto-Czech and French-to-English. Experimental results show that our proposed combination framework achieved 2.17 absolute points (13.36 relative points) and 1.52 absolute points (5.37 relative points) in terms of BLEU score on English-to- Czech and French-to-English tasks respectively than the best single system. We also achieved better performance on human evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>Z ˇZabokrtsk´y</author>
</authors>
<title>CzEng0.9: Large Parallel Treebank with Rich Annotation.</title>
<date>2009</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>92</volume>
<marker>Bojar, ˇZabokrtsk´y, 2009</marker>
<rawString>Bojar, O. and ˇZabokrtsk´y, Z. (2009). CzEng0.9: Large Parallel Treebank with Rich Annotation. Prague Bulletin of Mathematical Linguistics, 92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Du</author>
<author>Y He</author>
<author>S Penkale</author>
<author>A Way</author>
</authors>
<title>MaTrEx: The DCU MT System for WMT2009.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL-WMT 2009,</booktitle>
<pages>95--99</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="11149" citStr="Du et al., 2009" startWordPosition="1857" endWordPosition="1860">ell as the N-best list. Add the 1-best result into N3. 3. Rescore the N-best lists from all individual networks and super network and add the new 1-best results into N3. Pass 3: mConMBR 1. Rename the set N3 as a new set N,,,,,; 2. Use mConMBR decoding to search for the best final result from N,,,,,. In this step, we set a uniform distribution between the candidates in N,,,,,. BLEU MBR TER Nbest Re-ranking HMM TERp Hypotheses Set Individual CNs mConMBR Alignment TER Super CN Networks BLEU TERp Top M Single Pass 3 TER Pass 1 TERp Pass 2 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N-gram posterior probabilities within the Nbest list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N-best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the ba</context>
</contexts>
<marker>Du, He, Penkale, Way, 2009</marker>
<rawString>Du, J., He, Y., Penkale, S., and Way, A. (2009). MaTrEx: The DCU MT System for WMT2009. In Proceedings of the EACL-WMT 2009, pages 95–99, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Du</author>
<author>A Way</author>
</authors>
<title>A Three-pass System Combination Framework by Combining Multiple Hypothesis Alignment Methods.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Asian Language Processing (IALP),</booktitle>
<pages>172--176</pages>
<contexts>
<context position="2933" citStr="Du and Way (2009)" startWordPosition="467" endWordPosition="470">rrently, most research in system combination has focused on hypothesis alignment due to its significant influence on combination quality. A multiple CN or “super-network” framework was firstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim et al., 2007), where MBR decoding is employed to select the best hypothesis with the minimum cost from the original single system outputs compared to the consensus output. Du and Way (2009) proposed a combination strategy that employs MBR, super network, and a modified ConMBR (mConMBR) approach to construct a three-pass system combination framework which can effectively combine different hypothesis alignment results and easily be extended to more alignment metrics. Firstly, a number of individual CNs are built based on different backbones and different kinds of alignment metrics. Each network generates a 1-best output. Secondly, a super network is constructed combining all the individual networks, and a consensus is generated based on a weighted search model. In the third Procee</context>
<context position="10467" citStr="Du and Way, 2009" startWordPosition="1732" endWordPosition="1735">) metrics. 4. Carry out word reordering based on word alignment (TER and TERp have completed the reordering in the process of scoring) and build individual CNs (Rosti et al., 2007); 5. Decode the single networks and export the 1- best outputs and the N-best lists separately. Add these 1-best outputs into N3. Pass 2: Super-Network 1. Connect the single networks using a start node and an end node to form a supernetwork based on multiple hypothesis alignment and different backbones. In this evaluation, we set uniform weights for these different individual networks when building the super network(Du and Way, 2009). 2. Decode the super network and generate a consensus output as well as the N-best list. Add the 1-best result into N3. 3. Rescore the N-best lists from all individual networks and super network and add the new 1-best results into N3. Pass 3: mConMBR 1. Rename the set N3 as a new set N,,,,,; 2. Use mConMBR decoding to search for the best final result from N,,,,,. In this step, we set a uniform distribution between the candidates in N,,,,,. BLEU MBR TER Nbest Re-ranking HMM TERp Hypotheses Set Individual CNs mConMBR Alignment TER Super CN Networks BLEU TERp Top M Single Pass 3 TER Pass 1 TERp </context>
<context position="19619" citStr="Du and Way, 2009" startWordPosition="3227" endWordPosition="3230">rease the diversity for “mConMBR” decoding; 2) a new hypothesis alignment metric “TERp” for Englishtargeted alignment; 3) 1-best results from the top M individual systems employed to build CNs to augment the “mConMBR” decoding. We took part in the English-to-Czech and French-toEnglish tasks. Experimental results reported on test set of these two tasks showed that our augmented framework performed better than the best single system in terms of BLEU scores and human evaluation. Furthermore, the proposed augmented framework achieved better results than our basic three-pass combination framework (Du and Way, 2009) as well in terms of automatic evaluation scores. In the released preliminary results, we achieved top 1 and top 3 for the English-to-Czech and French-to-English tasks respectively in terms of human evaluation. As for future work, firstly we plan to do further experiments using automatic weight-tuning algorithm to tune our framework. Secondly, we plan to examine how the differences between the hypothesis alignment metrics impact on the accuracy of the super network. We also intend to integrate more alignment metrics to the networks and verify on the other language pairs. Acknowledgments This r</context>
</contexts>
<marker>Du, Way, 2009</marker>
<rawString>Du, J. and Way, A. (2009). A Three-pass System Combination Framework by Combining Multiple Hypothesis Alignment Methods. In Proceedings of the International Conference on Asian Language Processing (IALP), pages 172–176, Singapore.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Fellbaum, C., editor</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Fellbaum, C., editor (1998). WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
</authors>
<date>2004</date>
<journal>Disambiguation of Rich Inflection (Computational Morphology of Czech),</journal>
<volume>1</volume>
<publisher>Charles University Press,</publisher>
<location>Prague.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Hajiˇc, J. (2004). Disambiguation of Rich Inflection (Computational Morphology of Czech), volume 1. Charles University Press, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004,</booktitle>
<pages>169--176</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2144" citStr="Kumar and Byrne, 2004" startWordPosition="333" endWordPosition="336"> improving machine translation quality. Generally, most combination approaches are based on a confusion network (CN) which can effectively re-shuffle the translation hypotheses and generate a new target sentence. A CN is essentially a directed acyclic graph built from a set of translation hypotheses against a reference or “backbone”. Each arc between two nodes in the CN denotes a word or token, possibly a null item, with an associated posterior probability. Typically, the dominant CN is constructed at the word level by a state-of-the-art framework: firstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research in system combination has focused on hypothesis alignment due to its significant influence on combination quality. A multiple CN or “super-network” framework was firstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim </context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of the HLT-NAACL 2004, pages 169–176, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL’06,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="5157" citStr="Matusov et al., 2006" startWordPosition="818" endWordPosition="822">. Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and Frenchto-English combination tasks. Section 7 gives our conclusions. 2 Hypothesis Alignment Methods Hypothesis alignment plays a vital role in the CN, as the backbone sentence determines the skeleton and the word order of the consensus output. In the combination evaluation task, we integrated TER (Snover et al., 2006), HMM (Matusov et al., 2006) and TERp (Snover et al., 2009) into our augmented three-pass combination framework. In this section, we briefly describe these three methods. 2.1 TER The TER (Translation Edit Rate) metric measures the ratio of the number of edit operations between the hypothesis E0 and the reference Eb to the total number of words in Eb. Here the backbone Eb is assumed to be the reference. The allowable edits include insertions (Ins), deletions (Del), substitutions (Sub), and phrase shifts (Shft). The TER of E0 compared to Eb is computed as in (1): TER(E&apos;, Eb) = Ins + Del +Sub + Shft x 100% (1) where Nb is t</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Matusov, E., Ueffing, N., and Ney, H. (2006). Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proceedings of EACL’06, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="11947" citStr="Och, 2003" startWordPosition="1998" endWordPosition="1999">l (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N-gram posterior probabilities within the Nbest list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N-best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N-best list using BLEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 Experimental Settings We participated in the English–Czech and French–English system combination tasks. In our system combination framework, we use a large-scale monolingual data to train language models and carry out POS-tagging. 5.1 English-Czech Training Data The statistics of the data used for language models training are shown in Table 1. Corpus Monolingual Number of tokens (Cz) sentences News-Comm 2,214,757 84,706 CzEng 81,161,278 8,027,391 News 205,600,053 13,042,040 Total 288,976,088 21,154,137 Table 1: Statistics of data in the En–Cz task “CzEng” is the Czech–English corpus v0.9 (</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. (2003). Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="9493" citStr="Papineni et al., 2002" startWordPosition="1563" endWordPosition="1566"> adding outputs into the mConMBR decoding component. “Top M Single” indicates that the 1-best results from the best M individual MT systems are also used as backbones to build individual CNs under different alignment metrics. The three dashed boxes represent Pass 1, Pass 2 and Pass 3 respectively. The steps can be summarised as follows: Pass 1: Specific Metric-based Single Networks 1. Merge all the 1-best hypotheses from single MT systems into a new N-best set N3. 2. Utilise the standard MBR decoder to select one from the N3 as the backbone given some specific loss function such as TER, BLEU (Papineni et al., 2002) and TERp; Additionally, in order to increase the diversity of candidates used for Pass 2 and Pass 3, we also use the 1-best hypotheses from the top M single MT systems as the backbone. Add the backbones generated by MBR into N3. 3. Perform the word alignment between the different backbones and the other hypotheses via the TER, HMM, TERp (only for English) metrics. 4. Carry out word reordering based on word alignment (TER and TERp have completed the reordering in the process of scoring) and build individual CNs (Rosti et al., 2007); 5. Decode the single networks and export the 1- best outputs </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the ACL-02, pages 311– 318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping,</title>
<date>1980</date>
<pages>program.</pages>
<contexts>
<context position="8060" citStr="Porter, 1980" startWordPosition="1332" endWordPosition="1333"> P(A|eI1, e0J1 ) (3) A For the HMM-based model, equation (2) can be represented as in (4): J Pr(E0|Eb) = � Ip(aj|aj−1, I) &apos; p(e0j|ea,)] (4) a� j=1 292 the constant edit cost for all operations such as shifts, insertion, deleting or substituting in TER, all edit costs in TERp are optimized to maximize correlation with human judgments. TERp uses all the edit operations of TER – matches, insertions, deletions, substitutions, and shifts – as well as three new edit operations: stem matches, synonym matches, and phrase substitutions (Snover et al., 2009). TERp employs the Porter stemming algorithm (Porter, 1980) and WordNet (Fellbaum, 1998) to perform the “stem match” and “synonym match” respectively. Sequences of words in the reference are considered to be paraphrases of a sequence of words in the hypothesis if that phrase pair occurs in the TERp phrase table (Snover et al., 2009). In our experiments, TERp was used for the French-English system combination task, and we used the default configuration of optimised edit costs. 3 Augmented Three-Pass Combination Framework The construction of the augmented three-pass combination framework is shown in Figure 1. NSingle MT Systems Figure 1: Three-Pass Comb</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F. (1980). An algorithm for suffix stripping, program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP’96,</booktitle>
<pages>133--142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11372" citStr="Ratnaparkhi, 1996" startWordPosition="1896" endWordPosition="1897">,,,,,; 2. Use mConMBR decoding to search for the best final result from N,,,,,. In this step, we set a uniform distribution between the candidates in N,,,,,. BLEU MBR TER Nbest Re-ranking HMM TERp Hypotheses Set Individual CNs mConMBR Alignment TER Super CN Networks BLEU TERp Top M Single Pass 3 TER Pass 1 TERp Pass 2 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N-gram posterior probabilities within the Nbest list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N-best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N-best list using BLEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 Experimental Settings</context>
<context position="13794" citStr="Ratnaparkhi, 1996" startWordPosition="2276" endWordPosition="2278">use all of them in our combination experiments. 5.2 French-English Training Data The statistics of the data used for language models training and POS tagging are shown in Table 2. Corpus Monolingual Number of tokens (En) sentences News-Comm 2,973,711 125,879 Europarl 50,738,215 1,843,035 News 1,131,527,255 48,648,160 Total 1,184,234,384 50,617,074 Table 2: Statistics of data in the Fr–En task “News” is the English monolingual News corpus. We use “News-Comm+Europarl” to train a 4-gram language model and use “NewsComm+Europarl+News” to train a 5-gram language model. We also perform POS tagging (Ratnaparkhi, 1996) for all available data, and train 3-gram, 4-gram and, 5-gram POS-tag language models. Devset and Testset We also use all the 1-best results to carry out system combination. There are 14 MT systems in the Fr-En track and we use all of them in our combination experiments. 6 Experimental Results In this section, all the results are reported on devsets in terms of BLEU and NIST scores. All the data are provided by the workshop 6.1 English–Czech organisers. 1 In Table 1, “News-Comm” indi- In this task, we only used one hypothesis aligncates the data set of News-Commentary v1.0 and 293ment method –</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the EMNLP’96, pages 133–142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="2540" citStr="Rosti et al. (2007)" startWordPosition="396" endWordPosition="399">ossibly a null item, with an associated posterior probability. Typically, the dominant CN is constructed at the word level by a state-of-the-art framework: firstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research in system combination has focused on hypothesis alignment due to its significant influence on combination quality. A multiple CN or “super-network” framework was firstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim et al., 2007), where MBR decoding is employed to select the best hypothesis with the minimum cost from the original single system outputs compared to the consensus output. Du and Way (2009) proposed a combination strategy that employs MBR, super network, and a modified ConMBR (mConMBR) approach to construct a three-pass system combination framework which can effectively combine different hypot</context>
<context position="10030" citStr="Rosti et al., 2007" startWordPosition="1658" endWordPosition="1661">ckbone given some specific loss function such as TER, BLEU (Papineni et al., 2002) and TERp; Additionally, in order to increase the diversity of candidates used for Pass 2 and Pass 3, we also use the 1-best hypotheses from the top M single MT systems as the backbone. Add the backbones generated by MBR into N3. 3. Perform the word alignment between the different backbones and the other hypotheses via the TER, HMM, TERp (only for English) metrics. 4. Carry out word reordering based on word alignment (TER and TERp have completed the reordering in the process of scoring) and build individual CNs (Rosti et al., 2007); 5. Decode the single networks and export the 1- best outputs and the N-best lists separately. Add these 1-best outputs into N3. Pass 2: Super-Network 1. Connect the single networks using a start node and an end node to form a supernetwork based on multiple hypothesis alignment and different backbones. In this evaluation, we set uniform weights for these different individual networks when building the super network(Du and Way, 2009). 2. Decode the super network and generate a consensus output as well as the N-best list. Add the 1-best result into N3. 3. Rescore the N-best lists from all indiv</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Rosti, A., Matsoukas, S., and Schwartz, R. (2007). Improved Word-Level System Combination for Machine Translation. In Proceedings of ACL’07, pages 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="11352" citStr="Schmid, 1994" startWordPosition="1894" endWordPosition="1895">as a new set N,,,,,; 2. Use mConMBR decoding to search for the best final result from N,,,,,. In this step, we set a uniform distribution between the candidates in N,,,,,. BLEU MBR TER Nbest Re-ranking HMM TERp Hypotheses Set Individual CNs mConMBR Alignment TER Super CN Networks BLEU TERp Top M Single Pass 3 TER Pass 1 TERp Pass 2 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N-gram posterior probabilities within the Nbest list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N-best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N-best list using BLEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 E</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sim</author>
<author>W Byrne</author>
<author>M Gales</author>
<author>H Sahbi</author>
<author>P Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proceedings of the ICASSP’07,</booktitle>
<pages>105--108</pages>
<contexts>
<context position="2757" citStr="Sim et al., 2007" startWordPosition="437" endWordPosition="440">004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research in system combination has focused on hypothesis alignment due to its significant influence on combination quality. A multiple CN or “super-network” framework was firstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim et al., 2007), where MBR decoding is employed to select the best hypothesis with the minimum cost from the original single system outputs compared to the consensus output. Du and Way (2009) proposed a combination strategy that employs MBR, super network, and a modified ConMBR (mConMBR) approach to construct a three-pass system combination framework which can effectively combine different hypothesis alignment results and easily be extended to more alignment metrics. Firstly, a number of individual CNs are built based on different backbones and different kinds of alignment metrics. Each network generates a 1</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Sim, K., Byrne, W., Gales, M., Sahbi, H., and Woodland, P. (2007). Consensus network decoding for statistical machine translation system combination. In Proceedings of the ICASSP’07, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciula</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the AMTA’06),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="2678" citStr="Snover et al., 2006" startWordPosition="422" endWordPosition="425">the-art framework: firstly, a minimum Bayes-risk (MBR) decoder (Kumar and Byrne, 2004) is utilised to choose the backbone from a merged set of hypotheses, and then the remaining hypotheses are aligned against the backbone by a specific alignment approach. Currently, most research in system combination has focused on hypothesis alignment due to its significant influence on combination quality. A multiple CN or “super-network” framework was firstly proposed in Rosti et al. (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al., 2006). A consensus network MBR (ConMBR) approach was presented in (Sim et al., 2007), where MBR decoding is employed to select the best hypothesis with the minimum cost from the original single system outputs compared to the consensus output. Du and Way (2009) proposed a combination strategy that employs MBR, super network, and a modified ConMBR (mConMBR) approach to construct a three-pass system combination framework which can effectively combine different hypothesis alignment results and easily be extended to more alignment metrics. Firstly, a number of individual CNs are built based on different</context>
<context position="5129" citStr="Snover et al., 2006" startWordPosition="813" endWordPosition="816">thods used in our framework. Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and Frenchto-English combination tasks. Section 7 gives our conclusions. 2 Hypothesis Alignment Methods Hypothesis alignment plays a vital role in the CN, as the backbone sentence determines the skeleton and the word order of the consensus output. In the combination evaluation task, we integrated TER (Snover et al., 2006), HMM (Matusov et al., 2006) and TERp (Snover et al., 2009) into our augmented three-pass combination framework. In this section, we briefly describe these three methods. 2.1 TER The TER (Translation Edit Rate) metric measures the ratio of the number of edit operations between the hypothesis E0 and the reference Eb to the total number of words in Eb. Here the backbone Eb is assumed to be the reference. The allowable edits include insertions (Ins), deletions (Del), substitutions (Sub), and phrase shifts (Shft). The TER of E0 compared to Eb is computed as in (1): TER(E&apos;, Eb) = Ins + Del +Sub + S</context>
<context position="6392" citStr="Snover et al., 2006" startWordPosition="1033" endWordPosition="1036"> of words in Eb. The difference between TER and Levenshtein edit distance (or WER) is the sequence shift operation allowing phrasal shifts in the output to be captured. The phrase shift edit is carried out by a greedy algorithm and restricted by three constraints: 1) The shifted words must exactly match the reference words in the destination position. 2) The word sequence of the hypothesis in the original position and the corresponding reference words must not exactly match. 3) The word sequence of the reference that corresponds to the destination position must be misaligned before the shift (Snover et al., 2006). 2.2 HMM The hypothesis alignment model based on HMM (Hidden Markov Model) considers the alignment between the backbone and the hypothesis as a hidden variable in the conditional probability Pr(E0|Eb). Given the backbone Eb = {e1, ... , eI} and the hypothesis E0 = {e01, ... , e0J}, which are both in the same language, the probability Pr(E0|Eb) is defined as in (2): Pr(E0|Eb) = � Pr(E0, A|Eb) (2) A where the alignemnt A C_ {(j, i) : 1 G j G J;1 G i G I}, i and j represent the word position in Eb and E0 respectively. Hence, the alignment issue is to seek the optimum alignment A� such that: wher</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciula, Makhoul, 2006</marker>
<rawString>Snover, M., Dorr, B., Schwartz, R., Micciula, L., and Makhoul, J. (2006). A study of translation edit rate with targeted human annotation. In Proceedings of the AMTA’06), pages 223–231, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the WMT’09,</booktitle>
<pages>259--268</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="4209" citStr="Snover et al., 2009" startWordPosition="670" endWordPosition="673">ranslation and MetricsMATR, pages 290–295, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics pass, all the 1-best hypotheses coming from single MT systems, individual networks, and the super network are combined to select the final result using the mConMBR decoder. In the system combination task of WMT 2010, we adopted an augmented framework by extending the strategy in (Du and Way, 2009). In addition to the basic three-pass architecture, we augment our combination system as follows: • We add a rescoring component in Pass 1 and Pass 2. • We introduce the TERp (Snover et al., 2009) alignment metric for the English-targeted combination. • We employ different backbones and hypothesis alignment metrics to increase the diversity of candidates for our mConMBR decoding. The remainder of this paper is organised as follows. In Section 2, we introduce the three hypothesis alignment methods used in our framework. Section 3 details the steps for building our augmented three-pass combination framework. In Section 4, a rescoring model with rich features is described. Then, Sections 5 and 6 respectively report the experimental settings and experimental results on English-to-Czech and</context>
<context position="7296" citStr="Snover et al., 2009" startWordPosition="1202" endWordPosition="1205">which are both in the same language, the probability Pr(E0|Eb) is defined as in (2): Pr(E0|Eb) = � Pr(E0, A|Eb) (2) A where the alignemnt A C_ {(j, i) : 1 G j G J;1 G i G I}, i and j represent the word position in Eb and E0 respectively. Hence, the alignment issue is to seek the optimum alignment A� such that: where p(aj|aj−1, I) is the alignment probability and p(e0j|ei) is the translation probability. 2.3 TER-Plus TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms (Snover et al., 2009). In addition, it uses probabilistic phrasal substitutions to align phrases in the hypothesis and reference. In contrast to the use of 291 A� = arg max P(A|eI1, e0J1 ) (3) A For the HMM-based model, equation (2) can be represented as in (4): J Pr(E0|Eb) = � Ip(aj|aj−1, I) &apos; p(e0j|ea,)] (4) a� j=1 292 the constant edit cost for all operations such as shifts, insertion, deleting or substituting in TER, all edit costs in TERp are optimized to maximize correlation with human judgments. TERp uses all the edit operations of TER – matches, insertions, deletions, substitutions, and shifts – as well as</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Snover, M., Madnani, N., J.Dorr, B., and Schwartz, R. (2009). Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the WMT’09, pages 259–268, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-gram Posterior Probabilities for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 295 HLT-NAACL’06),</booktitle>
<pages>72--77</pages>
<location>New York, USA.</location>
<contexts>
<context position="11434" citStr="Zens and Ney, 2006" startWordPosition="1902" endWordPosition="1905">esult from N,,,,,. In this step, we set a uniform distribution between the candidates in N,,,,,. BLEU MBR TER Nbest Re-ranking HMM TERp Hypotheses Set Individual CNs mConMBR Alignment TER Super CN Networks BLEU TERp Top M Single Pass 3 TER Pass 1 TERp Pass 2 4 Rescoring Model We adapted our previous rescoring model (Du et al., 2009) to larger-scale data. The features we used are as follows: • Direct and inverse IBM model; • 4-gram and 5-gram target language model; • 3, 4, and 5-gram Part-of-Speech (POS) language model (Schmid, 1994; Ratnaparkhi, 1996); • Sentence-length posterior probability (Zens and Ney, 2006); • N-gram posterior probabilities within the Nbest list (Zens and Ney, 2006); • Minimum Bayes Risk cost. This process is similar to the calculation of the MBR decoding in which we take the current hypothesis in the N-best list as the “backbone”, and then calculate and sum up all the Bayes risk cost between the backbone and each of the rest of the N-best list using BLEU metric as the loss function; • Length ratio between source and target sentence. The weights are optimized via the MERT algorithm (Och, 2003). 5 Experimental Settings We participated in the English–Czech and French–English syste</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities for Statistical Machine Translation. In Proceedings of the 295 HLT-NAACL’06), pages 72–77, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>