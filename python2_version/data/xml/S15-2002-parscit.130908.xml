<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048256">
<title confidence="0.954775">
MITRE: Seven Systems for Semantic Similarity in Tweets
</title>
<author confidence="0.76399">
Guido Zarrella, John Henderson, Elizabeth M. Merkhofer and Laura Strickhart
</author>
<affiliation confidence="0.542225">
The MITRE Corporation
</affiliation>
<address confidence="0.745924">
202 Burlington Road
Bedford, MA 01730-1420, USA
</address>
<email confidence="0.980189">
{jzarrella,jhndrsn,emerkhofer,lstrickhart}@mitre.org
</email>
<sectionHeader confidence="0.993513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998565">
This paper describes MITRE’s participation
in the Paraphrase and Semantic Similar-
ity in Twitter task (SemEval-2015 Task 1).
This effort placed first in Semantic Similar-
ity and second in Paraphrase Identification
with scores of Pearson’s r of 61.9%, F1 of
66.7%, and maxF1 of 72.4%. We detail the
approaches we explored including mixtures
of string matching metrics, alignments us-
ing tweet-specific distributed word represen-
tations, recurrent neural networks for model-
ing similarity with those alignments, and dis-
tance measurements on pooled latent semantic
features. Logistic regression is used to tie the
systems together into the ensembles submitted
for evaluation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999478">
Paraphrase identification is the task of judging if two
texts express the same or very similar meaning. Au-
tomatic identification of paraphrases has practical
applications for a range of domains, including news
summarization, information retrieval, essay grading,
and evaluation of machine translation outputs. Fur-
thermore, work on paraphrase detection tends to ad-
vance the state of art in modeling semantics and se-
mantic similarity in natural language in general.
Current approaches to paraphrase detection vary
widely. The Microsoft Research Paraphrase Corpus,
with pairs of sentences from newswire text, serves as
a benchmark for the task (Dolan et al., 2004). One
top result on this dataset uses features from surface
characteristics of text (Madnani et al., 2012). An-
other system with comparable results models sen-
tences as hierarchical compositions of distributed
word embeddings (Socher et al., 2011). SemEval-
2015 Task 1 (Xu et al., 2015), with a corpus drawn
from Twitter, offers an opportunity to test paraphrase
</bodyText>
<page confidence="0.980628">
12
</page>
<bodyText confidence="0.972676382978723">
systems in a domain with an expanded vocabulary
and informal grammar.
Our contribution builds upon the recent success
of distributed representations of language (Mikolov
et al., 2013a; Pennington et al., 2014). We further
aim to minimize reliance on language- and domain-
dependent tools. However we do not possess enough
labeled paraphrase data to train a generalized model
of word composition. Instead we explore models
that examine low-dimensional relationships between
individual pairs of aligned words, and combine the
above with string similarity features that generalize
well to out-of-vocabulary terms.
In the remainder of this paper, we describe our
high-performing system for modeling semantic sim-
ilarity between two tweets. In Section 2 we describe
the data, task, and evaluation. In Section 3 we dis-
cuss details of systems we built to solve the semantic
similarity task. We describe our experiments on dif-
ferent parameterizations in Section 4. In Section 5
we present performance results for our ensembles
and all subsystems, and in Section 6 we summarize
our findings.
2 Task, data and evaluation
Paraphrase and Semantic Similarity in Twitter was a
shared task organized within SemEval-2015.
The task organizers released 18,762 pairs of
English-language tweets with a 70/25/5 split for
train, development, and test sets. The organizers re-
moved URLs, deleted non-alphanumeric characters,
and provided part of speech tags. Tweet pairs were
judged by five human annotators to be a paraphrase
(e.g. Amber alert gave me a damn heart attack and
That Amber alert scared the crap out of me) or not
(e.g. My phone is annoying me with these amber
alert and Am I the only one who dont get Amber
alert). Approximately 35% of provided pairs are
paraphrases. For each pair, task participants predict
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 12–17,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
word count of the longer string. LPSOLVE was em-
ployed to find the assignment maximizing these cri-
teria (Berkelaar et al., 2004).
a binary label and optionally provide a confidence
score. Systems were evaluated by F1 measure, F1 at
the best confidence threshold, and Pearson correla-
tion with expert annotation.
</bodyText>
<sectionHeader confidence="0.938076" genericHeader="method">
3 System overview
</sectionHeader>
<bodyText confidence="0.99995">
We created an ensemble of seven systems which
each independently predicted a semantic similarity
score. Some features were reused among the compo-
nents, including word embeddings and alignments.
</bodyText>
<subsectionHeader confidence="0.997878">
3.1 Twitter Word Embeddings
</subsectionHeader>
<bodyText confidence="0.99986625">
We used word2vec to learn distributed representa-
tions of words and phrases from an unlabeled cor-
pus of 330.3 million tweets sampled in 2013 from
Twitter’s public streaming API. Retweets and non-
English messages were not included in the sam-
ple. Text was lowercased and processed to mimic
the style of the task data. We applied word2phrase
(Mikolov et al., 2013b) twice consecutively to iden-
tify phrases comprised of up to four words. We then
trained a skip-gram model of size 256 for the 1.87
million vocabulary items which appeared at least 25
times, using a context window of 10 words and 15
negative samples per positive example. These hy-
perparameters were selected based on our prior ex-
perience in training embeddings for identification of
word analogies.
</bodyText>
<subsectionHeader confidence="0.998952">
3.2 Alignment
</subsectionHeader>
<bodyText confidence="0.999987588235294">
Comparing semantics in two tweets can be imagined
as a tallying process. One finds some semantic atom
on the left hand side and searches for it in the right
hand side. If found, it gets crossed off. Otherwise,
that atom contributes to a difference. Repeat on the
other side. This idealized process is reminiscent of
finding translation equivalences for training machine
translation systems (Al-Onaizan et al., 1999).
To this end, we built an alignment system on top
of word embeddings. Each tweet was converted into
a bag of words, and two different alignments were
created. The min alignment maximized the cosine
similarity of aligned pairs under the constraint that
no word could be aligned more than once. The max
alignment was constrained such that each word must
be paired with at least one other, and the total num-
ber of edges in the alignment can be no more than
</bodyText>
<subsectionHeader confidence="0.99975">
3.3 Seven Systems
</subsectionHeader>
<bodyText confidence="0.999929666666667">
Random Projection The random projection fam-
ily of Locality Sensitive Hashing algorithms is a
probabilistic technique for reducing high dimen-
sional inputs to a fixed-length low dimensional
sketch (Charikar, 2002), in which similar inputs
yield similar hashes. This characteristic is useful
for approximate nearest neighbor search and online
clustering (Petrovi´c et al., 2010), but we use it here
to obtain an unsupervised similarity metric that iden-
tifies string overlap at many levels of granularity.
Concretely, we extract the set of all word unigrams,
word bigrams, and character n-grams of lengths 2
through 5. These features are input to 2048 inde-
pendent binary classifiers with random weights, and
each classifier contributes a single bit to the resulting
hash. We assess similarity of two tweets by measur-
ing the Hamming distance between their bit vectors.
Recurrent Neural Network One common ap-
proach to paraphrase detection is to construct a
model of each sentence before learning a distance
function over these representations. We chose to
sidestep this global semantics modeling problem
and instead directly measured the relationships be-
tween embedded lexical items.
In particular, we used a Recurrent Neural Net-
work to examine the sequence of aligned word pairs
obtained from the min alignment process described
in section 3.2. For each aligned pair, we computed
descriptive statistics that were used as input to the
network: cosine similarity and Euclidean distance
of the aligned word embeddings, the magnitudes
of each word’s vector, and the relative position of
each word in the sentence. These features enabled
the network to consider the quality of the alignment
without introducing sparsity by including the word
vectors themselves. The RNN also received two
global features at each time step: the ratio of sen-
tence lengths and the normalized Hamming distance
computed via random projection as described above.
The RNN contained 8 input features, 16 hid-
den units, and a single output, composed as an
Elman network (Elman, 1990) with tied weights.
</bodyText>
<page confidence="0.994506">
13
</page>
<bodyText confidence="0.994490966666667">
We unfolded it using backpropagation through
time (Williams and Zipser, 1990) to create a deep
network with as many hidden layers as there were
lexical units in the shorter sentence. We trained
the RNN with stochastic gradient descent and a for-
mulation of dropout (Hinton et al., 2012) that ran-
domly removed a single word pair from each train-
ing sequence. Parameters were tuned on the devel-
opment set, including a minibatch of 20, a learning
rate of 0.05 or 0.06, hyperbolic tangent activation
functions, and early stopping after about 2000 iter-
ations. Two RNNs were used in the final ensemble,
each trained with different learning rates.
Paris: String Similarity MITRE entered a sys-
tem based on string similarity metrics in the 2004
Pascal RTE competition (Bayer et al., 2005). We re-
vivified the code base (called libparis) and up-
dated it for this evaluation. Eight different string
similarity and machine translation evaluation ap-
proaches are implemented in this package; mea-
sures include an implementation of the MT evalu-
ation BLEU (Papineni et al., 2002); WER, a com-
mon speech recognition word error rate based on
Levenshtein distance (Levenshtein, 1966); WER-g,
an error rate similar to WER, but with denomina-
tor based on the min edit traceback (Foster et al.,
2003); the MT evaluation ROUGE (Lin and Och,
2004); a simple position-independent error rate sim-
ilar to PER as described in Leusch et al. (2003); both
global and local similarity metrics often used for bi-
ological string comparison as described in Gusfield
(1997). Finally, there are precision and recall mea-
sures based on bags of all substrings (or n-grams in
word tokenization).
In total we computed 22 metrics for a pair of
strings. The metrics were run on both lowercased
and original versions as well as on word tokens
and characters, yielding 88 string similarity features.
Some of the metrics are not symmetric, so they were
run both forward and reversed based on presentation
in the dataset yielding 176 features. Finally, for each
feature value x, log(x) was added as a feature, pro-
ducing a final count of 352 string similarity features.
We used LIBLINEAR with these features to build a
L1-regularized logistic regression model.
Simple Alignment Measures Section 3.2 de-
scribes methods we used for aligning two strings.
We built one component that computed similarity
between tweets using simple metrics applied only to
the aligned word pairs. Mean vectors and pooled
component-wise min and max vectors were com-
puted for both sides of the two different types of
alignments. Those six pairs of vectors were com-
pared using cosine distance, Manhattan distance,
and Euclidean distance, resulting in eighteen fea-
tures. Separately, the alignments were traversed and
pairs of word vectors were compared using the three
distance functions. The means of those comparisons
produced six more features. L2-regularized logistic
regression combined these 24 features into a single
measure of semantic similarity.
Similarity Matrices, Averaged and Min/Max
Two subsystems drew upon a similarity matrix and
dynamic pooling technique presented in Socher et
al. (2011). This method considers distance between
all syntactically meaningful subunits of two sen-
tences. First, a representation is induced for each
node of the parse tree of two sentences, starting from
word embeddings at leaf nodes. Then a similarity
matrix is created from measurements of Euclidean
distance between every pair of nodes. Finally, a dy-
namic pooling scheme reduces this to a fixed-size
representation that is used as input to a logistic re-
gression classifier. For one subsystem in MITRE’s
contribution, nodes were represented as averages of
their child nodes; for another, nodes were repre-
sented as the concatenation of the minimum and
maximum of the child nodes.
Normalized Averages This subsystem computed
an unsupervised distance metric based on semantic
features. We first replaced each word in the tweet
with its synonym from the Twitter normalization
lexicon (Han and Baldwin, 2011), for example con-
verting tv to television. The embeddings of these
words were used in experiments on weighted aver-
aging and pooling, folding of part-of-speech tags,
and various distance and similarity metrics. The best
F1 score on the development set was achieved by av-
eraging the word vectors and computing Euclidean
distance between the two tweets’ resulting vectors.
</bodyText>
<subsectionHeader confidence="0.781935">
3.4 Ensembles
</subsectionHeader>
<bodyText confidence="0.9996835">
The predictors described above were selected for in-
clusion in a larger ensemble on the basis of their
</bodyText>
<page confidence="0.99692">
14
</page>
<table confidence="0.999814739130435">
System Pearson F1 maxF1
MITRE 61.9 66.7 71.6
RTM-DCU 57.0 54.0 69.1
HLTC-UST 56.3 65.1 67.6
ASOBEK 50.4 67.2 66.3
MITRE components
RNN 60.8 71.8
Paris 58.7 68.2
RandProj 54.9 64.6
SimMat_avg 54.6 64.7
SimMat_minmax 53.5 62.8
Aligner 51.8 61.9
NormalizedAvg 45.8 61.1
Name Factored Ablated
BLEU 61.5 64.6
ROUGE 60.2 63.8
PER 60.0 64.4
substring bags 58.7 63.5
WER 58.0 63.9
WER-g 57.9 63.9
global sim 57.7 64.1
local sim 55.9 63.1
none − 63.9
</table>
<tableCaption confidence="0.997805">
Table 1: Dev set F1 scores for string similarities.
</tableCaption>
<bodyText confidence="0.999850181818182">
performance on the development set. Each compo-
nent’s semantic similarity score contributed to the
final prediction with a weighting determined by L2-
regularized logistic regression. Binary paraphrase
labels were assigned by choosing an ensemble score
threshold that optimized development set F1.
The ensemble described in this paper was submit-
ted for scoring under the name MITRE IKR. A sec-
ond submission was identical with one exception:
its supervised subsystems were retrained on the con-
catenation of the train and development data.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999870952380952">
In all experiments, systems were trained while omit-
ting debatable examples with scores of 2 as sug-
gested by the task organizers. The development set
was used both to fit the hyperparameters (ablations,
lambdas) and the eventual ensemble.
String Similarity Ablations The MT evaluation
metrics and string similarities contributed varying
amounts to that system. In Table 1 we show the
score achieved by the logistic regression system
built using just that one measure (in the Factored
column) as well as the F1 achieved by the logistic
regression when only that one measure is left out
(Ablated column). BLEU was omitted from the sub-
system as a result of this analysis.
Ensemble Construction We focused our ensem-
bles only on the output of our individual compo-
nents, ignoring the features from the original data
they attempt to model. Table 3 shows the weights of
these components. Note that NormalizedAvg pro-
duced larger outputs than the rest; as a result its co-
efficient is about 10 times smaller than its effect.
</bodyText>
<tableCaption confidence="0.994966">
Table 2: Test scores of Semantic Similarity Systems (%).
</tableCaption>
<sectionHeader confidence="0.999487" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999939615384615">
The evaluation of our components on the compe-
tition test set is shown in Table 2, along with a
sample of top-scoring competitors. Our best en-
semble achieves 0.619 Pearson correlation with ex-
pert judgments, a state-of-the-art result. In contrast,
the correlation of crowdsourced annotations with ex-
pert ratings is 0.735 (Xu et al., 2015). Our sys-
tem’s F1 on the binary paraphrase judgment task was
0.667, with a maximum F1 of 0.716 using an opti-
mal threshold. Additionally several individual com-
ponents performed well in isolation. The recurrent
neural network alone achieved Pearson of 0.608 and
a max F1 of 0.718.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999180214285714">
Seven models of semantic similarity were combined
for paraphrase detection in Twitter. This ensemble
placed first in the Semantic Similarity competition
organized within SemEval-2015 Task 1. The simi-
larity judgments showed 0.619 correlation with ex-
pert judgment, a relative improvement of 8.6% over
other published results (Xu et al., 2015).
Our best performing single system represents a
novel departure from existing paraphrase detection
approaches. The recurrent neural network makes
use of the relationships between aligned word pairs,
an approach which we feel is well-suited to informal
contexts where explicit models of syntax face addi-
tional challenges.
</bodyText>
<page confidence="0.986569">
15
</page>
<table confidence="0.999587833333333">
Component Φ Component Φ
RNN1 −1.89 SimMat_minmax 0.84
RNN2 −1.11 Aligner 0.28
Paris −1.81 NormalizedAvg −0.034
SimMat_avg −1.28 bias 0.91
RandProj 1.11
</table>
<tableCaption confidence="0.9975875">
Table 3: Final MITRE component coefficients in the en-
semble logistic regression.
</tableCaption>
<sectionHeader confidence="0.997094" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988162">
This work was funded under the MITRE Innova-
tion Program. Many thanks to John Burger for his
comments on machine translation alignments. Ap-
proved for Public Release; Distribution Unlimited:
Case Number 15-0811.
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999378717647059">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, I. Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation: Fi-
nal report. Technical report, JHU Center for Language
and Speech Processing.
Samuel Bayer, John Burger, Lisa Ferro, John Henderson,
and Alexander Yeh. 2005. MITRE’s submissions to
the EU Pascal RTE challenge. In Proceedings of the
Pattern Analysis, Statistical Modelling, and Compu-
tational Learning (PASCAL) Challenges Workshop on
Recognising Textual Entailment.
Michel Berkelaar, Kjell Eikland, and Peter Notebaert.
2004. lp_solve 5.5, open source (mixed-integer) lin-
ear programming system. Software. Available at
http://lpsolve.sourceforge.net/5.5/.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the Thirty-fourth Annual ACM Symposium on Theory
of Computing, STOC ’02, pages 380–388.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics, COLING ’04.
Jeffrey L. Elman. 1990. Finding structure in time. COG-
NITIVE SCIENCE, 14(2):179–211.
George Foster, Simona Gandrabur, Cyril Goutte, Erin
Fitzgerald, Alberto Sanchis, Nicola Ueffing, John
Blatz, and Alex Kulesza. 2003. Confidence estima-
tion for machine translation. Technical report, JHU
Center for Language and Speech Processing.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational Bi-
ology.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 368–
378.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov.
2012. Improving neural networks by prevent-
ing co-adaptation of feature detectors. CoRR,
http://arxiv.org/abs/1207.0580.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003.
A novel string-to-string distance measure with applica-
tions to machine translation evaluation. In Proc. of the
Ninth MT Summit, pages 240–247.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions and reversals.
Soviet Physics Doklady, 10(8):707–710.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE:
a method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING 2004), August.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 182–190.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. In International Conference on
Learning Representations Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global vectors for word rep-
resentation. Proceedings of the Empiricial Methods in
Natural Language Processing (EMNLP 2014), 12.
Saša Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to Twitter. In Human Language Technologies: The
</reference>
<page confidence="0.971103">
16
</page>
<reference confidence="0.999279">
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 181–189.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Christo-
pher D. Manning, and Andrew Y. Ng. 2011. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Advances in Neural Information
Processing Systems, pages 801–809.
Ronald J. Williams and David Zipser. 1990. Gradient-
based learning algorithms for recurrent connectionist
networks. pages 433–486.
Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015.
Semeval-2015 task 1: Paraphrase and semantic sim-
ilarity in twitter. In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation (SemEval
2015).
</reference>
<page confidence="0.999399">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727108">
<title confidence="0.999724">MITRE: Seven Systems for Semantic Similarity in Tweets</title>
<author confidence="0.9975">John Henderson Zarrella</author>
<author confidence="0.9975">Elizabeth M Merkhofer</author>
<affiliation confidence="0.925966">The MITRE</affiliation>
<address confidence="0.995092">202 Burlington Bedford, MA 01730-1420,</address>
<email confidence="0.99749">jzarrella@mitre.org</email>
<email confidence="0.99749">jhndrsn@mitre.org</email>
<email confidence="0.99749">emerkhofer@mitre.org</email>
<email confidence="0.99749">lstrickhart@mitre.org</email>
<abstract confidence="0.983901647058823">This paper describes MITRE’s participation in the Paraphrase and Semantic Similarity in Twitter task (SemEval-2015 Task 1). This effort placed first in Semantic Similarity and second in Paraphrase Identification scores of Pearson’s 61.9%, F1 of 66.7%, and maxF1 of 72.4%. We detail the approaches we explored including mixtures of string matching metrics, alignments using tweet-specific distributed word representations, recurrent neural networks for modeling similarity with those alignments, and distance measurements on pooled latent semantic features. Logistic regression is used to tie the systems together into the ensembles submitted for evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>I Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation: Final report.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>JHU Center for Language and Speech Processing.</institution>
<contexts>
<context position="5685" citStr="Al-Onaizan et al., 1999" startWordPosition="872" endWordPosition="875">xt window of 10 words and 15 negative samples per positive example. These hyperparameters were selected based on our prior experience in training embeddings for identification of word analogies. 3.2 Alignment Comparing semantics in two tweets can be imagined as a tallying process. One finds some semantic atom on the left hand side and searches for it in the right hand side. If found, it gets crossed off. Otherwise, that atom contributes to a difference. Repeat on the other side. This idealized process is reminiscent of finding translation equivalences for training machine translation systems (Al-Onaizan et al., 1999). To this end, we built an alignment system on top of word embeddings. Each tweet was converted into a bag of words, and two different alignments were created. The min alignment maximized the cosine similarity of aligned pairs under the constraint that no word could be aligned more than once. The max alignment was constrained such that each word must be paired with at least one other, and the total number of edges in the alignment can be no more than 3.3 Seven Systems Random Projection The random projection family of Locality Sensitive Hashing algorithms is a probabilistic technique for reduci</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, I. Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation: Final report. Technical report, JHU Center for Language and Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Bayer</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>John Henderson</author>
<author>Alexander Yeh</author>
</authors>
<title>MITRE’s submissions to the EU Pascal RTE challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the Pattern Analysis, Statistical Modelling, and Computational Learning (PASCAL) Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="9004" citStr="Bayer et al., 2005" startWordPosition="1410" endWordPosition="1413">in the shorter sentence. We trained the RNN with stochastic gradient descent and a formulation of dropout (Hinton et al., 2012) that randomly removed a single word pair from each training sequence. Parameters were tuned on the development set, including a minibatch of 20, a learning rate of 0.05 or 0.06, hyperbolic tangent activation functions, and early stopping after about 2000 iterations. Two RNNs were used in the final ensemble, each trained with different learning rates. Paris: String Similarity MITRE entered a system based on string similarity metrics in the 2004 Pascal RTE competition (Bayer et al., 2005). We revivified the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as </context>
</contexts>
<marker>Bayer, Burger, Ferro, Henderson, Yeh, 2005</marker>
<rawString>Samuel Bayer, John Burger, Lisa Ferro, John Henderson, and Alexander Yeh. 2005. MITRE’s submissions to the EU Pascal RTE challenge. In Proceedings of the Pattern Analysis, Statistical Modelling, and Computational Learning (PASCAL) Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Berkelaar</author>
<author>Kjell Eikland</author>
<author>Peter Notebaert</author>
</authors>
<title>lp_solve 5.5, open source (mixed-integer) linear programming system. Software. Available at http://lpsolve.sourceforge.net/5.5/.</title>
<date>2004</date>
<contexts>
<context position="4075" citStr="Berkelaar et al., 2004" startWordPosition="613" endWordPosition="616">o be a paraphrase (e.g. Amber alert gave me a damn heart attack and That Amber alert scared the crap out of me) or not (e.g. My phone is annoying me with these amber alert and Am I the only one who dont get Amber alert). Approximately 35% of provided pairs are paraphrases. For each pair, task participants predict Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 12–17, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics word count of the longer string. LPSOLVE was employed to find the assignment maximizing these criteria (Berkelaar et al., 2004). a binary label and optionally provide a confidence score. Systems were evaluated by F1 measure, F1 at the best confidence threshold, and Pearson correlation with expert annotation. 3 System overview We created an ensemble of seven systems which each independently predicted a semantic similarity score. Some features were reused among the components, including word embeddings and alignments. 3.1 Twitter Word Embeddings We used word2vec to learn distributed representations of words and phrases from an unlabeled corpus of 330.3 million tweets sampled in 2013 from Twitter’s public streaming API. </context>
</contexts>
<marker>Berkelaar, Eikland, Notebaert, 2004</marker>
<rawString>Michel Berkelaar, Kjell Eikland, and Peter Notebaert. 2004. lp_solve 5.5, open source (mixed-integer) linear programming system. Software. Available at http://lpsolve.sourceforge.net/5.5/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Thirty-fourth Annual ACM Symposium on Theory of Computing, STOC ’02,</booktitle>
<pages>380--388</pages>
<contexts>
<context position="6369" citStr="Charikar, 2002" startWordPosition="989" endWordPosition="990">. Each tweet was converted into a bag of words, and two different alignments were created. The min alignment maximized the cosine similarity of aligned pairs under the constraint that no word could be aligned more than once. The max alignment was constrained such that each word must be paired with at least one other, and the total number of edges in the alignment can be no more than 3.3 Seven Systems Random Projection The random projection family of Locality Sensitive Hashing algorithms is a probabilistic technique for reducing high dimensional inputs to a fixed-length low dimensional sketch (Charikar, 2002), in which similar inputs yield similar hashes. This characteristic is useful for approximate nearest neighbor search and online clustering (Petrovi´c et al., 2010), but we use it here to obtain an unsupervised similarity metric that identifies string overlap at many levels of granularity. Concretely, we extract the set of all word unigrams, word bigrams, and character n-grams of lengths 2 through 5. These features are input to 2048 independent binary classifiers with random weights, and each classifier contributes a single bit to the resulting hash. We assess similarity of two tweets by measu</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the Thirty-fourth Annual ACM Symposium on Theory of Computing, STOC ’02, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</booktitle>
<contexts>
<context position="1607" citStr="Dolan et al., 2004" startWordPosition="226" endWordPosition="229">udging if two texts express the same or very similar meaning. Automatic identification of paraphrases has practical applications for a range of domains, including news summarization, information retrieval, essay grading, and evaluation of machine translation outputs. Furthermore, work on paraphrase detection tends to advance the state of art in modeling semantics and semantic similarity in natural language in general. Current approaches to paraphrase detection vary widely. The Microsoft Research Paraphrase Corpus, with pairs of sentences from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>COGNITIVE SCIENCE,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="8204" citStr="Elman, 1990" startWordPosition="1278" endWordPosition="1279">rk: cosine similarity and Euclidean distance of the aligned word embeddings, the magnitudes of each word’s vector, and the relative position of each word in the sentence. These features enabled the network to consider the quality of the alignment without introducing sparsity by including the word vectors themselves. The RNN also received two global features at each time step: the ratio of sentence lengths and the normalized Hamming distance computed via random projection as described above. The RNN contained 8 input features, 16 hidden units, and a single output, composed as an Elman network (Elman, 1990) with tied weights. 13 We unfolded it using backpropagation through time (Williams and Zipser, 1990) to create a deep network with as many hidden layers as there were lexical units in the shorter sentence. We trained the RNN with stochastic gradient descent and a formulation of dropout (Hinton et al., 2012) that randomly removed a single word pair from each training sequence. Parameters were tuned on the development set, including a minibatch of 20, a learning rate of 0.05 or 0.06, hyperbolic tangent activation functions, and early stopping after about 2000 iterations. Two RNNs were used in th</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time. COGNITIVE SCIENCE, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Erin Fitzgerald</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
<author>John Blatz</author>
<author>Alex Kulesza</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>JHU Center for Language and Speech Processing.</institution>
<contexts>
<context position="9498" citStr="Foster et al., 2003" startWordPosition="1491" endWordPosition="1494">g Similarity MITRE entered a system based on string similarity metrics in the 2004 Pascal RTE competition (Bayer et al., 2005). We revivified the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we computed 22 metrics for a pair of strings. The metrics were run on both lowercased and original versions as well as on word tokens and characters, yielding 88 string similarity features. Some of the metrics are no</context>
</contexts>
<marker>Foster, Gandrabur, Goutte, Fitzgerald, Sanchis, Ueffing, Blatz, Kulesza, 2003</marker>
<rawString>George Foster, Simona Gandrabur, Cyril Goutte, Erin Fitzgerald, Alberto Sanchis, Nicola Ueffing, John Blatz, and Alex Kulesza. 2003. Confidence estimation for machine translation. Technical report, JHU Center for Language and Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<date>1997</date>
<booktitle>Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology.</booktitle>
<contexts>
<context position="9755" citStr="Gusfield (1997)" startWordPosition="1535" endWordPosition="1536">ation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we computed 22 metrics for a pair of strings. The metrics were run on both lowercased and original versions as well as on word tokens and characters, yielding 88 string similarity features. Some of the metrics are not symmetric, so they were run both forward and reversed based on presentation in the dataset yielding 176 features. Finally, for each feature value x, log(x) was added as a feature, producing a final count of 352 string similarity features. We used LIBLINEA</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>368--378</pages>
<contexts>
<context position="12246" citStr="Han and Baldwin, 2011" startWordPosition="1923" endWordPosition="1926">ments of Euclidean distance between every pair of nodes. Finally, a dynamic pooling scheme reduces this to a fixed-size representation that is used as input to a logistic regression classifier. For one subsystem in MITRE’s contribution, nodes were represented as averages of their child nodes; for another, nodes were represented as the concatenation of the minimum and maximum of the child nodes. Normalized Averages This subsystem computed an unsupervised distance metric based on semantic features. We first replaced each word in the tweet with its synonym from the Twitter normalization lexicon (Han and Baldwin, 2011), for example converting tv to television. The embeddings of these words were used in experiments on weighted averaging and pooling, folding of part-of-speech tags, and various distance and similarity metrics. The best F1 score on the development set was achieved by averaging the word vectors and computing Euclidean distance between the two tweets’ resulting vectors. 3.4 Ensembles The predictors described above were selected for inclusion in a larger ensemble on the basis of their 14 System Pearson F1 maxF1 MITRE 61.9 66.7 71.6 RTM-DCU 57.0 54.0 69.1 HLTC-UST 56.3 65.1 67.6 ASOBEK 50.4 67.2 66</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 368– 378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<location>CoRR, http://arxiv.org/abs/1207.0580.</location>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, http://arxiv.org/abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>A novel string-to-string distance measure with applications to machine translation evaluation.</title>
<date>2003</date>
<booktitle>In Proc. of the Ninth MT Summit,</booktitle>
<pages>240--247</pages>
<contexts>
<context position="9637" citStr="Leusch et al. (2003)" startWordPosition="1515" endWordPosition="1518">ed the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we computed 22 metrics for a pair of strings. The metrics were run on both lowercased and original versions as well as on word tokens and characters, yielding 88 string similarity features. Some of the metrics are not symmetric, so they were run both forward and reversed based on presentation in the dataset yielding 176 features. Finally, for each featu</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A novel string-to-string distance measure with applications to machine translation evaluation. In Proc. of the Ninth MT Summit, pages 240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="9385" citStr="Levenshtein, 1966" startWordPosition="1472" endWordPosition="1473"> iterations. Two RNNs were used in the final ensemble, each trained with different learning rates. Paris: String Similarity MITRE entered a system based on string similarity metrics in the 2004 Pascal RTE competition (Bayer et al., 2005). We revivified the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we computed 22 metrics for a pair of strings. The metrics were run on both lowercased and original vers</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="9543" citStr="Lin and Och, 2004" startWordPosition="1499" endWordPosition="1502">ring similarity metrics in the 2004 Pascal RTE competition (Bayer et al., 2005). We revivified the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we computed 22 metrics for a pair of strings. The metrics were run on both lowercased and original versions as well as on word tokens and characters, yielding 88 string similarity features. Some of the metrics are not symmetric, so they were run both forward an</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="1713" citStr="Madnani et al., 2012" startWordPosition="243" endWordPosition="246">s practical applications for a range of domains, including news summarization, information retrieval, essay grading, and evaluation of machine translation outputs. Furthermore, work on paraphrase detection tends to advance the state of art in modeling semantics and semantic similarity in natural language in general. Current approaches to paraphrase detection vary widely. The Microsoft Research Paraphrase Corpus, with pairs of sentences from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In International Conference on Learning Representations Workshop.</booktitle>
<contexts>
<context position="2152" citStr="Mikolov et al., 2013" startWordPosition="311" endWordPosition="314">from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data to train a generalized model of word composition. Instead we explore models that examine low-dimensional relationships between individual pairs of aligned words, and combine the above with string similarity features that generalize well to out-of-vocabulary terms. In the remainder of this paper, we describe our high-performing system for modeling semantic similarity between two tweets. In Section 2 we describe the data, task, and eval</context>
<context position="4856" citStr="Mikolov et al., 2013" startWordPosition="737" endWordPosition="740">th expert annotation. 3 System overview We created an ensemble of seven systems which each independently predicted a semantic similarity score. Some features were reused among the components, including word embeddings and alignments. 3.1 Twitter Word Embeddings We used word2vec to learn distributed representations of words and phrases from an unlabeled corpus of 330.3 million tweets sampled in 2013 from Twitter’s public streaming API. Retweets and nonEnglish messages were not included in the sample. Text was lowercased and processed to mimic the style of the task data. We applied word2phrase (Mikolov et al., 2013b) twice consecutively to identify phrases comprised of up to four words. We then trained a skip-gram model of size 256 for the 1.87 million vocabulary items which appeared at least 25 times, using a context window of 10 words and 15 negative samples per positive example. These hyperparameters were selected based on our prior experience in training embeddings for identification of word analogies. 3.2 Alignment Comparing semantics in two tweets can be imagined as a tallying process. One finds some semantic atom on the left hand side and searches for it in the right hand side. If found, it gets </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In International Conference on Learning Representations Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2152" citStr="Mikolov et al., 2013" startWordPosition="311" endWordPosition="314">from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data to train a generalized model of word composition. Instead we explore models that examine low-dimensional relationships between individual pairs of aligned words, and combine the above with string similarity features that generalize well to out-of-vocabulary terms. In the remainder of this paper, we describe our high-performing system for modeling semantic similarity between two tweets. In Section 2 we describe the data, task, and eval</context>
<context position="4856" citStr="Mikolov et al., 2013" startWordPosition="737" endWordPosition="740">th expert annotation. 3 System overview We created an ensemble of seven systems which each independently predicted a semantic similarity score. Some features were reused among the components, including word embeddings and alignments. 3.1 Twitter Word Embeddings We used word2vec to learn distributed representations of words and phrases from an unlabeled corpus of 330.3 million tweets sampled in 2013 from Twitter’s public streaming API. Retweets and nonEnglish messages were not included in the sample. Text was lowercased and processed to mimic the style of the task data. We applied word2phrase (Mikolov et al., 2013b) twice consecutively to identify phrases comprised of up to four words. We then trained a skip-gram model of size 256 for the 1.87 million vocabulary items which appeared at least 25 times, using a context window of 10 words and 15 negative samples per positive example. These hyperparameters were selected based on our prior experience in training embeddings for identification of word analogies. 3.2 Alignment Comparing semantics in two tweets can be imagined as a tallying process. One finds some semantic atom on the left hand side and searches for it in the right hand side. If found, it gets </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9285" citStr="Papineni et al., 2002" startWordPosition="1455" endWordPosition="1458">rning rate of 0.05 or 0.06, hyperbolic tangent activation functions, and early stopping after about 2000 iterations. Two RNNs were used in the final ensemble, each trained with different learning rates. Paris: String Similarity MITRE entered a system based on string similarity metrics in the 2004 Pascal RTE competition (Bayer et al., 2005). We revivified the code base (called libparis) and updated it for this evaluation. Eight different string similarity and machine translation evaluation approaches are implemented in this package; measures include an implementation of the MT evaluation BLEU (Papineni et al., 2002); WER, a common speech recognition word error rate based on Levenshtein distance (Levenshtein, 1966); WER-g, an error rate similar to WER, but with denominator based on the min edit traceback (Foster et al., 2003); the MT evaluation ROUGE (Lin and Och, 2004); a simple position-independent error rate similar to PER as described in Leusch et al. (2003); both global and local similarity metrics often used for biological string comparison as described in Gusfield (1997). Finally, there are precision and recall measures based on bags of all substrings (or n-grams in word tokenization). In total we </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="2179" citStr="Pennington et al., 2014" startWordPosition="315" endWordPosition="318">ves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data to train a generalized model of word composition. Instead we explore models that examine low-dimensional relationships between individual pairs of aligned words, and combine the above with string similarity features that generalize well to out-of-vocabulary terms. In the remainder of this paper, we describe our high-performing system for modeling semantic similarity between two tweets. In Section 2 we describe the data, task, and evaluation. In Section 3 we dis</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saša Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to Twitter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>181--189</pages>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saša Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to Twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 181–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="1852" citStr="Socher et al., 2011" startWordPosition="263" endWordPosition="266">ne translation outputs. Furthermore, work on paraphrase detection tends to advance the state of art in modeling semantics and semantic similarity in natural language in general. Current approaches to paraphrase detection vary widely. The Microsoft Research Paraphrase Corpus, with pairs of sentences from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data to train a generalized model of word composition. Instead we explore models that examine low-dimensional relationships between individual </context>
<context position="11349" citStr="Socher et al. (2011)" startWordPosition="1783" endWordPosition="1786">es of the two different types of alignments. Those six pairs of vectors were compared using cosine distance, Manhattan distance, and Euclidean distance, resulting in eighteen features. Separately, the alignments were traversed and pairs of word vectors were compared using the three distance functions. The means of those comparisons produced six more features. L2-regularized logistic regression combined these 24 features into a single measure of semantic similarity. Similarity Matrices, Averaged and Min/Max Two subsystems drew upon a similarity matrix and dynamic pooling technique presented in Socher et al. (2011). This method considers distance between all syntactically meaningful subunits of two sentences. First, a representation is induced for each node of the parse tree of two sentences, starting from word embeddings at leaf nodes. Then a similarity matrix is created from measurements of Euclidean distance between every pair of nodes. Finally, a dynamic pooling scheme reduces this to a fixed-size representation that is used as input to a logistic regression classifier. For one subsystem in MITRE’s contribution, nodes were represented as averages of their child nodes; for another, nodes were represe</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Christopher D. Manning, and Andrew Y. Ng. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Williams</author>
<author>David Zipser</author>
</authors>
<title>Gradientbased learning algorithms for recurrent connectionist networks.</title>
<date>1990</date>
<pages>433--486</pages>
<contexts>
<context position="8304" citStr="Williams and Zipser, 1990" startWordPosition="1291" endWordPosition="1294">tudes of each word’s vector, and the relative position of each word in the sentence. These features enabled the network to consider the quality of the alignment without introducing sparsity by including the word vectors themselves. The RNN also received two global features at each time step: the ratio of sentence lengths and the normalized Hamming distance computed via random projection as described above. The RNN contained 8 input features, 16 hidden units, and a single output, composed as an Elman network (Elman, 1990) with tied weights. 13 We unfolded it using backpropagation through time (Williams and Zipser, 1990) to create a deep network with as many hidden layers as there were lexical units in the shorter sentence. We trained the RNN with stochastic gradient descent and a formulation of dropout (Hinton et al., 2012) that randomly removed a single word pair from each training sequence. Parameters were tuned on the development set, including a minibatch of 20, a learning rate of 0.05 or 0.06, hyperbolic tangent activation functions, and early stopping after about 2000 iterations. Two RNNs were used in the final ensemble, each trained with different learning rates. Paris: String Similarity MITRE entered</context>
</contexts>
<marker>Williams, Zipser, 1990</marker>
<rawString>Ronald J. Williams and David Zipser. 1990. Gradientbased learning algorithms for recurrent connectionist networks. pages 433–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>Bill Dolan</author>
</authors>
<title>Semeval-2015 task 1: Paraphrase and semantic similarity in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1890" citStr="Xu et al., 2015" startWordPosition="271" endWordPosition="274">on paraphrase detection tends to advance the state of art in modeling semantics and semantic similarity in natural language in general. Current approaches to paraphrase detection vary widely. The Microsoft Research Paraphrase Corpus, with pairs of sentences from newswire text, serves as a benchmark for the task (Dolan et al., 2004). One top result on this dataset uses features from surface characteristics of text (Madnani et al., 2012). Another system with comparable results models sentences as hierarchical compositions of distributed word embeddings (Socher et al., 2011). SemEval2015 Task 1 (Xu et al., 2015), with a corpus drawn from Twitter, offers an opportunity to test paraphrase 12 systems in a domain with an expanded vocabulary and informal grammar. Our contribution builds upon the recent success of distributed representations of language (Mikolov et al., 2013a; Pennington et al., 2014). We further aim to minimize reliance on language- and domaindependent tools. However we do not possess enough labeled paraphrase data to train a generalized model of word composition. Instead we explore models that examine low-dimensional relationships between individual pairs of aligned words, and combine th</context>
<context position="15196" citStr="Xu et al., 2015" startWordPosition="2405" endWordPosition="2408">nal data they attempt to model. Table 3 shows the weights of these components. Note that NormalizedAvg produced larger outputs than the rest; as a result its coefficient is about 10 times smaller than its effect. Table 2: Test scores of Semantic Similarity Systems (%). 5 Results The evaluation of our components on the competition test set is shown in Table 2, along with a sample of top-scoring competitors. Our best ensemble achieves 0.619 Pearson correlation with expert judgments, a state-of-the-art result. In contrast, the correlation of crowdsourced annotations with expert ratings is 0.735 (Xu et al., 2015). Our system’s F1 on the binary paraphrase judgment task was 0.667, with a maximum F1 of 0.716 using an optimal threshold. Additionally several individual components performed well in isolation. The recurrent neural network alone achieved Pearson of 0.608 and a max F1 of 0.718. 6 Conclusion Seven models of semantic similarity were combined for paraphrase detection in Twitter. This ensemble placed first in the Semantic Similarity competition organized within SemEval-2015 Task 1. The similarity judgments showed 0.619 correlation with expert judgment, a relative improvement of 8.6% over other pub</context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and Bill Dolan. 2015. Semeval-2015 task 1: Paraphrase and semantic similarity in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>