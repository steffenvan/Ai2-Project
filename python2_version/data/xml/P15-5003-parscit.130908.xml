<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.277752">
<title confidence="0.942468">
Sentiment and Belief:
How to Think about, Represent, and Annotate Private States
A Tutorial
</title>
<author confidence="0.998753">
Owen Rambow Janyce Wiebe
</author>
<affiliation confidence="0.999983">
Columbia University University of Pittsburgh
</affiliation>
<email confidence="0.99647">
rambow@ccls.columbia.edu wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.963288" genericHeader="method">
1 Tutotial Description
</sectionHeader>
<subsectionHeader confidence="0.680667">
1.1 Introduction
</subsectionHeader>
<bodyText confidence="0.999977085106383">
Over the last ten years, there has been an explosion
in interest in sentiment analysis, with many interest-
ing and impressive results. For example, the first
twenty publications on Google Scholar returned for
the Query “sentiment analysis” all date from 2003
or later, and have a total citation count of 12,140.
The total number of publications is in the thousands.
Partly, this interest is driven by the immediate com-
mercial applications of sentiment analysis.
Sentiment is a “private state” (Wiebe, 1990).
However, it is not the only private state that has re-
ceived attention in the computational literature; oth-
ers include belief and intention. In this tutorial, we
propose to provide a deeper understanding of what
a private state is. We will concentrate on sentiment
and belief. We will provide background that will al-
low the tutorial participants to understand the notion
of a private state as a cognitive phenomenon, which
can be manifested linguistically in communication
in various ways. We will explain the formalization
in terms of a triple of state, source, and target. We
will discuss how to model the source and the tar-
get. We will then explain in some detail the annota-
tions that have been made. The issue of annotation
is crucial for private states: while the MPQA corpus
(Wiebe et al., 2005; Wilson, 2007) has been around
for some time, most research using it does not make
use of many of its features. We believe this is be-
cause the MPQA annotation is quite complex and
requires a deeper understanding of the phenomenon
of “private state”, which is what the annotation is
getting at. Furthermore, there are currently several
efforts underway of creating new versions of anno-
tations, which we will also present.
The larger goal of this tutorial is to allow the tu-
torial participants to gain a deeper understanding of
the role of private states in human communication,
and to encourage them to use this deeper under-
standing in their computational work. The imme-
diate goal of this tutorial is to allow the participants
to make more complete use of available annotated
resources. We propose to achieve these goals by
concentrating on annotated corpora, since this will
allow participants to both understand the underlying
content (achieving the larger goal) and the technical
details of the annotations (achieving the immediate
goal).
</bodyText>
<subsectionHeader confidence="0.984793">
1.2 Current Work on Annotating Sentiment
</subsectionHeader>
<bodyText confidence="0.9999246875">
To date, the computational analyses of sentiment are
often fairly superficial. Much work in sentiment
analysis and opinion mining is at the document level
(Pang et al., 2002). There is increasing interest in
more fine-grained levels: sentence-level (McDonald
et al., 2007), phrase-level (Choi and Cardie, 2008;
Agarwal et al., 2009), aspect-level (Hu and Liu,
2004; Titov and McDonald, 2008), etc. Sentiments
toward entities and events (“eTargets”) expressed in
blogs, newswire, editorials, etc. are particularly im-
portant. A system that could recognize sentiments
toward entities and events would be valuable in an
application such as Automatic Question Answer-
ing, to support answering questions such as “Toward
whom/what is X negative/positive?” “Who is neg-
ative/positive toward X?” (Stoyanov et al., 2005).
</bodyText>
<page confidence="0.996545">
7
</page>
<note confidence="0.7921685">
Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999938205128205">
Or, to augment an automatic wikification system
(Ratinov et al., 2011), which could include informa-
tion about whom or what the subject supports or op-
poses. A recent NIST evaluation – The Knowledge
Base Population (KBP) Sentiment track1 — aims at
using corpora to collect information regarding sen-
timents expressed toward or by named entities. An-
notated corpora of reviews (Hu and Liu, 2004; Titov
and McDonald, 2008), widely used in NLP, often in-
clude annotations of targets that are aspects of prod-
ucts or services. As such, they are somewhat limited,
excluding, e.g., events or agents of events.
A widely used corpus is Version 2 of the MPQA
opinion annotated corpus (Wiebe et al., 2005; Wil-
son, 2007). It is entirely span-based, and contains
no eTarget annotations. However, it provides an in-
frastructure for sentiment annotation that is not pro-
vided by other sentiment NLP corpora, and is much
more varied in topic, genre, and publication source.
MPQA 3.0 (Deng and Wiebe, 2015), which was re-
cently created, adds entity- and event-target (eTar-
get) annotations to the MPQA 2.0 annotations (Wil-
son, 2007).2 The MPQA annotations consist of pri-
vate states, states of a source holding an attitude,
optionally toward a target. An important property
of sources is that they are nested, reflecting the fact
that private states and speech events are often em-
bedded in one another. There are several types of at-
titudes included in MPQA 2.0, including sentiment,
arguing, and intention. The tutorial will focus on
sentiments (while also discussing the others), which
are defined in (Wilson, 2007) as positive and nega-
tive evaluations, emotions, and judgements. In the
future, eTargets may be added to private states with
other types of attitudes.
This tutorial will present the original MPQA an-
notation scheme (V2) and its recent extension to in-
clude eTarget annotations (V3), which we believe is
a valuable new resource for the community.
</bodyText>
<subsectionHeader confidence="0.998663">
1.3 Belief Annotations
</subsectionHeader>
<bodyText confidence="0.99998325">
Compared to sentiment, belief has received far less
attention in the computational community. There
have been several efforts at annotating belief re-
cently. The most complete is FactBank (Sauriand
</bodyText>
<footnote confidence="0.99993">
1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html
2Available at http://mpqa.cs.pitt.edu
</footnote>
<bodyText confidence="0.9997964">
Pustejovsky, 2009), which represents the source of
the belief, the target, the strength, and the polar-
ity (using a system of 10 tags which cover strength
and polarity). Following (Wiebe et al., 2005), the
sources are nested, reflecting the same nesting of
private states we also observe for sentiment. Fact-
Bank is a rich and complex annotation; the so-called
LU corpus of Diab et al. (2009) was created inde-
pendently, and represents a subset of the annotations
of FactBank. The LU corpus annotates only the
writer’s belief in the propositions in the text, only
distinguishes 3 types of belief, but does clearly rep-
resent the target. Unlike FactBank, which is anno-
tated on top of the Penn Treebank, the LU corpus
represents a diverse set of texts. The recent annota-
tions at the LDC for the DARPA DEFT project fol-
low the simplicity of the LU corpus annotations, but
extend the tagset of the LU corpus to four tags. An
annotation effort in the spring of 2015 will include
the source of the belief. The LDC effort is impor-
tant since it covers a new domain – web discussion
forums. Its size is an order of magnitude larger than
that of FactBank or the LU corpus (about 800,000
words). This tutorial will discuss these resources
and compare the annotations.
</bodyText>
<subsectionHeader confidence="0.969656">
1.4 Integration Issues
</subsectionHeader>
<bodyText confidence="0.999945210526316">
Sentiment and belief are very similar: most impor-
tantly, they are both private states. They both in-
volve a holder and a target, and within the broad
categories of sentiment and belief there are subdi-
visions, which can affect the strength of the private
state. There is an important difference though: while
the target of a sentiment can be an entity or an event
(state of affairs), belief can only target a state of af-
fairs. In addition to being similar types of phenom-
ena, the same linguistic means can convey sentiment
and belief at the same time: the utterance I regret
that I am leaving tomorrow reveals both the utterer’s
sentiment and belief towards the leaving event. De-
spite these interactions between sentiment and be-
lief, there has been no attempt to jointly annotate or
predict sentiment and belief. The tutorial will use
examples to show the interaction between sentiment
and belief, and discuss some issues that arise in joint
annotation and tagging.
</bodyText>
<page confidence="0.996179">
8
</page>
<sectionHeader confidence="0.963605" genericHeader="method">
2 Tutorial Contents
</sectionHeader>
<listItem confidence="0.99469568">
1. Introduction: an overview over the issue of pri-
vate states, and how they relate to other well-
known concepts such as the BDI (belief-desire-
intention) model (Bratman, 1999 1987), related
work in NLP (such as RST (Mann and Thomp-
son, 1987) and dialog act tagging), linguistic
semantics (for example, the notion of veridicity
(Karttunen, 1971) and modality), and cognitive
science. (45 minutes)
2. Representing sentiment: a presentation of early
work, of MPQA V2 (with nested sources, and
attitude, expressive-subjective element, and tar-
get span annotations), and of MPQA Version 3
(extension of MPQA V2 to eTargets). (45 min-
utes)
3. Break (15 minutes)
4. Representing belief: a presentation of Fact-
Bank, the LU corpus, and the ongoing LDC
annotation under the DARPA DEFT program.
(30 minutes)
5. Integration and looking forward: a discussion
of how sentiment and belief interact, and how
we can integrate their annotations, including a
discussion of a General Modality Annotation
Scheme. (45 minutes)
</listItem>
<sectionHeader confidence="0.994241" genericHeader="method">
3 Tutorial Instructors
</sectionHeader>
<subsectionHeader confidence="0.996663">
3.1 Owen Rainbow
</subsectionHeader>
<bodyText confidence="0.99970832">
Owen Rainbow is a Senior Research Scientist at
the Center for Computational Learning Systems at
Columbia University. He is also the co-chair of the
Center for New Media at the Data Science Institute
at Columbia University. He has been interested in
modeling cognitive states in relation to language for
a long time, initially in the context of natural lan-
guage generation (Rambow, 1993; Walker and Ram-
bow, 1994). More recently, he has studied belief in
the context of recognizing beliefs in language (Diab
et al., 2009; Prabhakaran et al., 2010; Danlos and
Rambow, 2011; Prabhakaran et al., 2012). He is cur-
rently involved in the DARPA DEFT Belief group,
working with other researchers and with the LDC to
define annotation standards and evaluations. He has
recently led the pilot evaluation for belief recogni-
tion (in English) in the DARPA DEFT program.
He has been the PI or co-PI on many other Gov-
ernment grants from the NSF, DARPA, and IARPA.
He has been the Chair of the North American Chap-
ter of the Association for Computational Linguis-
tics. He has been on the editorial board of Com-
putational Linguistics, and has served as chair or
area chair for several major conferences. http:
//www.cs.columbia.edu/-rambow
</bodyText>
<subsectionHeader confidence="0.999388">
3.2 Janyce Wiebe
</subsectionHeader>
<bodyText confidence="0.997213695652174">
Janyce Wiebe is Professor of Computer Science
and Professor and Co-Director of the Intelligent Sys-
tems at the University of Pittsburgh. She has worked
on issues related to private states for some time,
originally in the context of tracking point of view in
narrative (Wiebe, 1994), and later in the context of
recognizing sentiment in other genres such as news
articles (Wilson et al., 2005). She has approached
the area from the perspective of corpus annotation
(Wiebe et al., 2005; Deng et al., 2013), lexical se-
mantics (Wiebe and Mihalcea, 2006), and discourse
(Somasundaran et al., 2009). In addition to contin-
uing these lines of research, she has recently begun
investigating implicatures in opinion analysis (Deng
and Wiebe, 2014).
She has received funding for her research from
NSF, NIH, DARPA, ONR, NSA, ARDA, and Home-
land Security. She was Program Chair of NAACL
2000 and Program Co-Chair of ACL-IJCNLP 2009.
She has been on the editorial board of Computa-
tional Linguistics and is currently an action editor
for Transactions of the ACL. http://people.
cs.pitt.edu/-wiebe/
</bodyText>
<page confidence="0.997702">
9
</page>
<sectionHeader confidence="0.98265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998977">
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mcke-
own. 2009. Contextual phrase-level polarity analy-
sis using lexical affect scoring and syntactic N-grams.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the ACL (EACL 2009), pages 24–
32, Athens, Greece, March. Association for Compu-
tational Linguistics.
Michael E. Bratman. 1999 [1987]. Intention, Plans, and
Practical Reason. CSLI Publications.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 793–801. Association for Compu-
tational Linguistics.
Laurence Danlos and Owen Rambow. 2011. Discourse
relations and propositional attitudes. In Proceedings
of CID 2011 - Fourth International Workshop on Con-
straints in Discourse.
Lingjia Deng and Janyce Wiebe. 2014. Sentiment prop-
agation via implicature constraints. In Proceedings
of the 14th Conference of the European Chapter of
the Association for Computational Linguistics, pages
377–385, Gothenburg, Sweden, April. Association for
Computational Linguistics.
Lingjia Deng and Janyce Wiebe. 2015. Entity/event-
level sentiment annotation. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (short paper). Associa-
tion for Computational Linguistics, May.
Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013.
Benefactive/malefactive event and writer attitude an-
notation. In ACL 2013 (short paper). Association for
Computational Linguistics.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68–73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Lauri Karttunen. 1971. Some observations on factivity.
Research on Language &amp; Social Interaction, 4(1):55–
69.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text organi-
zation. Technical Report ISI/RS-87-190, ISI.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Annual Meeting-
Association For Computational Linguistics, page 432.
Citeseer.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014–1022, Beijing,
China, August. Coling 2010 Organizing Committee.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012. Sta-
tistical modality tagging from rule-based annotations
and crowdsourcing. In Proceedings of the Workshop
on Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, pages 57–64, Jeju, Republic of
Korea, July. Association for Computational Linguis-
tics.
Owen Rambow. 1993. Rhetoric as knowledge. In Pro-
ceedings of the ACL Workshop on Intentionality and
Structure in Discourse Relations, Columbus, OH.
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1375–1384. Association for Computational
Linguistics.
Roser Saur´ı and James Pustejovsky. 2009. Fact-
bank: a corpus annotated with event factuality.
Language Resources and Evaluation, 43:227–268.
10.1007/s10579-009-9089-9.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 170–179, Singapore,
August. Association for Computational Linguistics.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-Perspective Question Answering us-
ing the OpQA corpus. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 923–930, Vancouver,
Canada.
Ivan Titov and Ryan T McDonald. 2008. A joint model
</reference>
<page confidence="0.952774">
10
</page>
<reference confidence="0.995312411764706">
of text and aspect ratings for sentiment summarization.
In ACL, volume 8, pages 308–316. Citeseer.
Marilyn Walker and Owen Rambow. 1994. The role
of cognitive modeling in achieving communicative in-
tentions. In Proceedings of the Seventh International
Workshop on Natural Language Generation, Kenneb-
unkport, ME.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1065–1072, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language ann. Language Resources and Evaluation,
39(2/3):164–210.
Janyce M. Wiebe. 1990. Identifying subjective char-
acters in narrative. In Proceedings of the 13th In-
ternational Conference on Computational Linguistics
(COLING’90).
Janyce Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233–287.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347–354, Vancouver,
Canada.
Theresa Wilson. 2007. Fine-grained Subjectivity and
Sentiment Analysis: Recognizing the Intensity, Polar-
ity, and Attitudes ofprivate states. Ph.D. thesis, Intel-
ligent Systems Program, University of Pittsburgh.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958058">
<title confidence="0.992725666666667">Sentiment and Belief: How to Think about, Represent, and Annotate Private States A Tutorial</title>
<author confidence="0.993199">Owen Rambow Janyce Wiebe</author>
<affiliation confidence="0.999958">Columbia University University of Pittsburgh</affiliation>
<email confidence="0.996644">rambow@ccls.columbia.eduwiebe@cs.pitt.edu</email>
<intro confidence="0.984593">1 Tutotial Description</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen Mckeown</author>
</authors>
<title>Contextual phrase-level polarity analysis using lexical affect scoring and syntactic N-grams.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>24--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="2966" citStr="Agarwal et al., 2009" startWordPosition="470" endWordPosition="473">ose to achieve these goals by concentrating on annotated corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, Beijing, China, July </context>
</contexts>
<marker>Agarwal, Biadsy, Mckeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown. 2009. Contextual phrase-level polarity analysis using lexical affect scoring and syntactic N-grams. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 24– 32, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Bratman</author>
</authors>
<title>Intention, Plans, and Practical Reason.</title>
<date>1999</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="8307" citStr="Bratman, 1999" startWordPosition="1343" endWordPosition="1344">ime: the utterance I regret that I am leaving tomorrow reveals both the utterer’s sentiment and belief towards the leaving event. Despite these interactions between sentiment and belief, there has been no attempt to jointly annotate or predict sentiment and belief. The tutorial will use examples to show the interaction between sentiment and belief, and discuss some issues that arise in joint annotation and tagging. 8 2 Tutorial Contents 1. Introduction: an overview over the issue of private states, and how they relate to other wellknown concepts such as the BDI (belief-desireintention) model (Bratman, 1999 1987), related work in NLP (such as RST (Mann and Thompson, 1987) and dialog act tagging), linguistic semantics (for example, the notion of veridicity (Karttunen, 1971) and modality), and cognitive science. (45 minutes) 2. Representing sentiment: a presentation of early work, of MPQA V2 (with nested sources, and attitude, expressive-subjective element, and target span annotations), and of MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) 3. Break (15 minutes) 4. Representing belief: a presentation of FactBank, the LU corpus, and the ongoing LDC annotation under the DARPA DEFT pr</context>
</contexts>
<marker>Bratman, 1999</marker>
<rawString>Michael E. Bratman. 1999 [1987]. Intention, Plans, and Practical Reason. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>793--801</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2943" citStr="Choi and Cardie, 2008" startWordPosition="466" endWordPosition="469">ated resources. We propose to achieve these goals by concentrating on annotated corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 793–801. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Danlos</author>
<author>Owen Rambow</author>
</authors>
<title>Discourse relations and propositional attitudes.</title>
<date>2011</date>
<booktitle>In Proceedings of CID 2011 - Fourth International Workshop on Constraints in Discourse.</booktitle>
<contexts>
<context position="9744" citStr="Danlos and Rambow, 2011" startWordPosition="1570" endWordPosition="1573"> (45 minutes) 3 Tutorial Instructors 3.1 Owen Rainbow Owen Rainbow is a Senior Research Scientist at the Center for Computational Learning Systems at Columbia University. He is also the co-chair of the Center for New Media at the Data Science Institute at Columbia University. He has been interested in modeling cognitive states in relation to language for a long time, initially in the context of natural language generation (Rambow, 1993; Walker and Rambow, 1994). More recently, he has studied belief in the context of recognizing beliefs in language (Diab et al., 2009; Prabhakaran et al., 2010; Danlos and Rambow, 2011; Prabhakaran et al., 2012). He is currently involved in the DARPA DEFT Belief group, working with other researchers and with the LDC to define annotation standards and evaluations. He has recently led the pilot evaluation for belief recognition (in English) in the DARPA DEFT program. He has been the PI or co-PI on many other Government grants from the NSF, DARPA, and IARPA. He has been the Chair of the North American Chapter of the Association for Computational Linguistics. He has been on the editorial board of Computational Linguistics, and has served as chair or area chair for several major</context>
</contexts>
<marker>Danlos, Rambow, 2011</marker>
<rawString>Laurence Danlos and Owen Rambow. 2011. Discourse relations and propositional attitudes. In Proceedings of CID 2011 - Fourth International Workshop on Constraints in Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Sentiment propagation via implicature constraints.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>377--385</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<marker>Deng, Wiebe, 2014</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2014. Sentiment propagation via implicature constraints. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 377–385, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Janyce Wiebe</author>
</authors>
<title>Entity/eventlevel sentiment annotation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (short paper). Association for Computational Linguistics,</booktitle>
<contexts>
<context position="4610" citStr="Deng and Wiebe, 2015" startWordPosition="732" endWordPosition="735">of reviews (Hu and Liu, 2004; Titov and McDonald, 2008), widely used in NLP, often include annotations of targets that are aspects of products or services. As such, they are somewhat limited, excluding, e.g., events or agents of events. A widely used corpus is Version 2 of the MPQA opinion annotated corpus (Wiebe et al., 2005; Wilson, 2007). It is entirely span-based, and contains no eTarget annotations. However, it provides an infrastructure for sentiment annotation that is not provided by other sentiment NLP corpora, and is much more varied in topic, genre, and publication source. MPQA 3.0 (Deng and Wiebe, 2015), which was recently created, adds entity- and event-target (eTarget) annotations to the MPQA 2.0 annotations (Wilson, 2007).2 The MPQA annotations consist of private states, states of a source holding an attitude, optionally toward a target. An important property of sources is that they are nested, reflecting the fact that private states and speech events are often embedded in one another. There are several types of attitudes included in MPQA 2.0, including sentiment, arguing, and intention. The tutorial will focus on sentiments (while also discussing the others), which are defined in (Wilson</context>
</contexts>
<marker>Deng, Wiebe, 2015</marker>
<rawString>Lingjia Deng and Janyce Wiebe. 2015. Entity/eventlevel sentiment annotation. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (short paper). Association for Computational Linguistics, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingjia Deng</author>
<author>Yoonjung Choi</author>
<author>Janyce Wiebe</author>
</authors>
<title>Benefactive/malefactive event and writer attitude annotation.</title>
<date>2013</date>
<booktitle>In ACL</booktitle>
<marker>Deng, Choi, Wiebe, 2013</marker>
<rawString>Lingjia Deng, Yoonjung Choi, and Janyce Wiebe. 2013. Benefactive/malefactive event and writer attitude annotation. In ACL 2013 (short paper). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Owen Rambow</author>
<author>Vinodkumar Prabhakaran</author>
<author>Weiwei Guo</author>
</authors>
<title>Committed belief annotation and tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>68--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6264" citStr="Diab et al. (2009)" startWordPosition="988" endWordPosition="991">in the computational community. There have been several efforts at annotating belief recently. The most complete is FactBank (Sauriand 1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 2Available at http://mpqa.cs.pitt.edu Pustejovsky, 2009), which represents the source of the belief, the target, the strength, and the polarity (using a system of 10 tags which cover strength and polarity). Following (Wiebe et al., 2005), the sources are nested, reflecting the same nesting of private states we also observe for sentiment. FactBank is a rich and complex annotation; the so-called LU corpus of Diab et al. (2009) was created independently, and represents a subset of the annotations of FactBank. The LU corpus annotates only the writer’s belief in the propositions in the text, only distinguishes 3 types of belief, but does clearly represent the target. Unlike FactBank, which is annotated on top of the Penn Treebank, the LU corpus represents a diverse set of texts. The recent annotations at the LDC for the DARPA DEFT project follow the simplicity of the LU corpus annotations, but extend the tagset of the LU corpus to four tags. An annotation effort in the spring of 2015 will include the source of the bel</context>
<context position="9693" citStr="Diab et al., 2009" startWordPosition="1562" endWordPosition="1565">sion of a General Modality Annotation Scheme. (45 minutes) 3 Tutorial Instructors 3.1 Owen Rainbow Owen Rainbow is a Senior Research Scientist at the Center for Computational Learning Systems at Columbia University. He is also the co-chair of the Center for New Media at the Data Science Institute at Columbia University. He has been interested in modeling cognitive states in relation to language for a long time, initially in the context of natural language generation (Rambow, 1993; Walker and Rambow, 1994). More recently, he has studied belief in the context of recognizing beliefs in language (Diab et al., 2009; Prabhakaran et al., 2010; Danlos and Rambow, 2011; Prabhakaran et al., 2012). He is currently involved in the DARPA DEFT Belief group, working with other researchers and with the LDC to define annotation standards and evaluations. He has recently led the pilot evaluation for belief recognition (in English) in the DARPA DEFT program. He has been the PI or co-PI on many other Government grants from the NSF, DARPA, and IARPA. He has been the Chair of the North American Chapter of the Association for Computational Linguistics. He has been on the editorial board of Computational Linguistics, and </context>
</contexts>
<marker>Diab, Levin, Mitamura, Rambow, Prabhakaran, Guo, 2009</marker>
<rawString>Mona Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In Proceedings of the Third Linguistic Annotation Workshop, pages 68–73, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2998" citStr="Hu and Liu, 2004" startWordPosition="475" endWordPosition="478">rating on annotated corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, Beijing, China, July 26-31, 2015. c�2015 Association </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Some observations on factivity.</title>
<date>1971</date>
<journal>Research on Language &amp; Social Interaction,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>69</pages>
<contexts>
<context position="8476" citStr="Karttunen, 1971" startWordPosition="1370" endWordPosition="1371">entiment and belief, there has been no attempt to jointly annotate or predict sentiment and belief. The tutorial will use examples to show the interaction between sentiment and belief, and discuss some issues that arise in joint annotation and tagging. 8 2 Tutorial Contents 1. Introduction: an overview over the issue of private states, and how they relate to other wellknown concepts such as the BDI (belief-desireintention) model (Bratman, 1999 1987), related work in NLP (such as RST (Mann and Thompson, 1987) and dialog act tagging), linguistic semantics (for example, the notion of veridicity (Karttunen, 1971) and modality), and cognitive science. (45 minutes) 2. Representing sentiment: a presentation of early work, of MPQA V2 (with nested sources, and attitude, expressive-subjective element, and target span annotations), and of MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) 3. Break (15 minutes) 4. Representing belief: a presentation of FactBank, the LU corpus, and the ongoing LDC annotation under the DARPA DEFT program. (30 minutes) 5. Integration and looking forward: a discussion of how sentiment and belief interact, and how we can integrate their annotations, including a discus</context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>Lauri Karttunen. 1971. Some observations on factivity. Research on Language &amp; Social Interaction, 4(1):55– 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190, ISI.</tech>
<contexts>
<context position="8373" citStr="Mann and Thompson, 1987" startWordPosition="1353" endWordPosition="1357">reveals both the utterer’s sentiment and belief towards the leaving event. Despite these interactions between sentiment and belief, there has been no attempt to jointly annotate or predict sentiment and belief. The tutorial will use examples to show the interaction between sentiment and belief, and discuss some issues that arise in joint annotation and tagging. 8 2 Tutorial Contents 1. Introduction: an overview over the issue of private states, and how they relate to other wellknown concepts such as the BDI (belief-desireintention) model (Bratman, 1999 1987), related work in NLP (such as RST (Mann and Thompson, 1987) and dialog act tagging), linguistic semantics (for example, the notion of veridicity (Karttunen, 1971) and modality), and cognitive science. (45 minutes) 2. Representing sentiment: a presentation of early work, of MPQA V2 (with nested sources, and attitude, expressive-subjective element, and target span annotations), and of MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) 3. Break (15 minutes) 4. Representing belief: a presentation of FactBank, the LU corpus, and the ongoing LDC annotation under the DARPA DEFT program. (30 minutes) 5. Integration and looking forward: a discussi</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical Structure Theory: A theory of text organization. Technical Report ISI/RS-87-190, ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kerry Hannan</author>
<author>Tyler Neylon</author>
<author>Mike Wells</author>
<author>Jeff Reynar</author>
</authors>
<title>Structured models for fine-to-coarse sentiment analysis.</title>
<date>2007</date>
<booktitle>In Annual MeetingAssociation For Computational Linguistics,</booktitle>
<pages>432</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2906" citStr="McDonald et al., 2007" startWordPosition="461" endWordPosition="464">e more complete use of available annotated resources. We propose to achieve these goals by concentrating on annotated corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of t</context>
</contexts>
<marker>McDonald, Hannan, Neylon, Wells, Reynar, 2007</marker>
<rawString>Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells, and Jeff Reynar. 2007. Structured models for fine-to-coarse sentiment analysis. In Annual MeetingAssociation For Computational Linguistics, page 432. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2808" citStr="Pang et al., 2002" startWordPosition="448" endWordPosition="451">ir computational work. The immediate goal of this tutorial is to allow the participants to make more complete use of available annotated resources. We propose to achieve these goals by concentrating on annotated corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Automatic committed belief tagging.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1014--1022</pages>
<location>Beijing, China,</location>
<contexts>
<context position="9719" citStr="Prabhakaran et al., 2010" startWordPosition="1566" endWordPosition="1569">odality Annotation Scheme. (45 minutes) 3 Tutorial Instructors 3.1 Owen Rainbow Owen Rainbow is a Senior Research Scientist at the Center for Computational Learning Systems at Columbia University. He is also the co-chair of the Center for New Media at the Data Science Institute at Columbia University. He has been interested in modeling cognitive states in relation to language for a long time, initially in the context of natural language generation (Rambow, 1993; Walker and Rambow, 1994). More recently, he has studied belief in the context of recognizing beliefs in language (Diab et al., 2009; Prabhakaran et al., 2010; Danlos and Rambow, 2011; Prabhakaran et al., 2012). He is currently involved in the DARPA DEFT Belief group, working with other researchers and with the LDC to define annotation standards and evaluations. He has recently led the pilot evaluation for belief recognition (in English) in the DARPA DEFT program. He has been the PI or co-PI on many other Government grants from the NSF, DARPA, and IARPA. He has been the Chair of the North American Chapter of the Association for Computational Linguistics. He has been on the editorial board of Computational Linguistics, and has served as chair or are</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2010</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2010. Automatic committed belief tagging. In Coling 2010: Posters, pages 1014–1022, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Michael Bloodgood</author>
<author>Mona Diab</author>
<author>Bonnie Dorr</author>
<author>Lori Levin</author>
<author>Christine D Piatko</author>
<author>Owen Rambow</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Statistical modality tagging from rule-based annotations and crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,</booktitle>
<pages>57--64</pages>
<institution>of Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic</location>
<marker>Prabhakaran, Bloodgood, Diab, Dorr, Levin, Piatko, Rambow, Van Durme, 2012</marker>
<rawString>Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowdsourcing. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 57–64, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>Rhetoric as knowledge.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL Workshop on Intentionality and Structure in Discourse Relations,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="9560" citStr="Rambow, 1993" startWordPosition="1541" endWordPosition="1542">oking forward: a discussion of how sentiment and belief interact, and how we can integrate their annotations, including a discussion of a General Modality Annotation Scheme. (45 minutes) 3 Tutorial Instructors 3.1 Owen Rainbow Owen Rainbow is a Senior Research Scientist at the Center for Computational Learning Systems at Columbia University. He is also the co-chair of the Center for New Media at the Data Science Institute at Columbia University. He has been interested in modeling cognitive states in relation to language for a long time, initially in the context of natural language generation (Rambow, 1993; Walker and Rambow, 1994). More recently, he has studied belief in the context of recognizing beliefs in language (Diab et al., 2009; Prabhakaran et al., 2010; Danlos and Rambow, 2011; Prabhakaran et al., 2012). He is currently involved in the DARPA DEFT Belief group, working with other researchers and with the LDC to define annotation standards and evaluations. He has recently led the pilot evaluation for belief recognition (in English) in the DARPA DEFT program. He has been the PI or co-PI on many other Government grants from the NSF, DARPA, and IARPA. He has been the Chair of the North Ame</context>
</contexts>
<marker>Rambow, 1993</marker>
<rawString>Owen Rambow. 1993. Rhetoric as knowledge. In Proceedings of the ACL Workshop on Intentionality and Structure in Discourse Relations, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1375--1384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3698" citStr="Ratinov et al., 2011" startWordPosition="579" endWordPosition="582">ets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Or, to augment an automatic wikification system (Ratinov et al., 2011), which could include information about whom or what the subject supports or opposes. A recent NIST evaluation – The Knowledge Base Population (KBP) Sentiment track1 — aims at using corpora to collect information regarding sentiments expressed toward or by named entities. Annotated corpora of reviews (Hu and Liu, 2004; Titov and McDonald, 2008), widely used in NLP, often include annotations of targets that are aspects of products or services. As such, they are somewhat limited, excluding, e.g., events or agents of events. A widely used corpus is Version 2 of the MPQA opinion annotated corpus (</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1375–1384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: a corpus annotated with event factuality. Language Resources and Evaluation,</title>
<date>2009</date>
<volume>43</volume>
<pages>10--1007</pages>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language Resources and Evaluation, 43:227–268. 10.1007/s10579-009-9089-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Galileo Namata</author>
<author>Janyce Wiebe</author>
<author>Lise Getoor</author>
</authors>
<title>Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>170--179</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<marker>Somasundaran, Namata, Wiebe, Getoor, 2009</marker>
<rawString>Swapna Somasundaran, Galileo Namata, Janyce Wiebe, and Lise Getoor. 2009. Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 170–179, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multi-Perspective Question Answering using the OpQA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005),</booktitle>
<pages>923--930</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="3442" citStr="Stoyanov et al., 2005" startWordPosition="539" endWordPosition="542">ncreasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Or, to augment an automatic wikification system (Ratinov et al., 2011), which could include information about whom or what the subject supports or opposes. A recent NIST evaluation – The Knowledge Base Population (KBP) Sentiment track1 — aims at using corpora to collect information regarding sentiments expressed toward or by named entities. Annotated corpora of reviews (Hu and Liu, 2004; Titov and McDonald, 200</context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-Perspective Question Answering using the OpQA corpus. In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005), pages 923–930, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan T McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>8</volume>
<pages>308--316</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3025" citStr="Titov and McDonald, 2008" startWordPosition="479" endWordPosition="482">d corpora, since this will allow participants to both understand the underlying content (achieving the larger goal) and the technical details of the annotations (achieving the immediate goal). 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguisti</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan T McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In ACL, volume 8, pages 308–316. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Owen Rambow</author>
</authors>
<title>The role of cognitive modeling in achieving communicative intentions.</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventh International Workshop on Natural Language Generation,</booktitle>
<location>Kennebunkport, ME.</location>
<contexts>
<context position="9586" citStr="Walker and Rambow, 1994" startWordPosition="1543" endWordPosition="1547"> a discussion of how sentiment and belief interact, and how we can integrate their annotations, including a discussion of a General Modality Annotation Scheme. (45 minutes) 3 Tutorial Instructors 3.1 Owen Rainbow Owen Rainbow is a Senior Research Scientist at the Center for Computational Learning Systems at Columbia University. He is also the co-chair of the Center for New Media at the Data Science Institute at Columbia University. He has been interested in modeling cognitive states in relation to language for a long time, initially in the context of natural language generation (Rambow, 1993; Walker and Rambow, 1994). More recently, he has studied belief in the context of recognizing beliefs in language (Diab et al., 2009; Prabhakaran et al., 2010; Danlos and Rambow, 2011; Prabhakaran et al., 2012). He is currently involved in the DARPA DEFT Belief group, working with other researchers and with the LDC to define annotation standards and evaluations. He has recently led the pilot evaluation for belief recognition (in English) in the DARPA DEFT program. He has been the PI or co-PI on many other Government grants from the NSF, DARPA, and IARPA. He has been the Chair of the North American Chapter of the Assoc</context>
</contexts>
<marker>Walker, Rambow, 1994</marker>
<rawString>Marilyn Walker and Owen Rambow. 1994. The role of cognitive modeling in achieving communicative intentions. In Proceedings of the Seventh International Workshop on Natural Language Generation, Kennebunkport, ME.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1065--1072</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1065–1072, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1551" citStr="Wiebe et al., 2005" startWordPosition="244" endWordPosition="247"> deeper understanding of what a private state is. We will concentrate on sentiment and belief. We will provide background that will allow the tutorial participants to understand the notion of a private state as a cognitive phenomenon, which can be manifested linguistically in communication in various ways. We will explain the formalization in terms of a triple of state, source, and target. We will discuss how to model the source and the target. We will then explain in some detail the annotations that have been made. The issue of annotation is crucial for private states: while the MPQA corpus (Wiebe et al., 2005; Wilson, 2007) has been around for some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is getting at. Furthermore, there are currently several efforts underway of creating new versions of annotations, which we will also present. The larger goal of this tutorial is to allow the tutorial participants to gain a deeper understanding of the role of private states in human communication, and to encourage them </context>
<context position="4316" citStr="Wiebe et al., 2005" startWordPosition="684" endWordPosition="687">, which could include information about whom or what the subject supports or opposes. A recent NIST evaluation – The Knowledge Base Population (KBP) Sentiment track1 — aims at using corpora to collect information regarding sentiments expressed toward or by named entities. Annotated corpora of reviews (Hu and Liu, 2004; Titov and McDonald, 2008), widely used in NLP, often include annotations of targets that are aspects of products or services. As such, they are somewhat limited, excluding, e.g., events or agents of events. A widely used corpus is Version 2 of the MPQA opinion annotated corpus (Wiebe et al., 2005; Wilson, 2007). It is entirely span-based, and contains no eTarget annotations. However, it provides an infrastructure for sentiment annotation that is not provided by other sentiment NLP corpora, and is much more varied in topic, genre, and publication source. MPQA 3.0 (Deng and Wiebe, 2015), which was recently created, adds entity- and event-target (eTarget) annotations to the MPQA 2.0 annotations (Wilson, 2007).2 The MPQA annotations consist of private states, states of a source holding an attitude, optionally toward a target. An important property of sources is that they are nested, refle</context>
<context position="6073" citStr="Wiebe et al., 2005" startWordPosition="955" endWordPosition="958">ension to include eTarget annotations (V3), which we believe is a valuable new resource for the community. 1.3 Belief Annotations Compared to sentiment, belief has received far less attention in the computational community. There have been several efforts at annotating belief recently. The most complete is FactBank (Sauriand 1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html 2Available at http://mpqa.cs.pitt.edu Pustejovsky, 2009), which represents the source of the belief, the target, the strength, and the polarity (using a system of 10 tags which cover strength and polarity). Following (Wiebe et al., 2005), the sources are nested, reflecting the same nesting of private states we also observe for sentiment. FactBank is a rich and complex annotation; the so-called LU corpus of Diab et al. (2009) was created independently, and represents a subset of the annotations of FactBank. The LU corpus annotates only the writer’s belief in the propositions in the text, only distinguishes 3 types of belief, but does clearly represent the target. Unlike FactBank, which is annotated on top of the Penn Treebank, the LU corpus represents a diverse set of texts. The recent annotations at the LDC for the DARPA DEFT</context>
<context position="10893" citStr="Wiebe et al., 2005" startWordPosition="1762" endWordPosition="1765"> Linguistics, and has served as chair or area chair for several major conferences. http: //www.cs.columbia.edu/-rambow 3.2 Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is currently an action editor for Transactions of the ACL. http://people. cs.pitt.edu/-wiebe/ 9</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language ann. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Identifying subjective characters in narrative.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING’90).</booktitle>
<contexts>
<context position="750" citStr="Wiebe, 1990" startWordPosition="109" endWordPosition="110">niversity of Pittsburgh rambow@ccls.columbia.edu wiebe@cs.pitt.edu 1 Tutotial Description 1.1 Introduction Over the last ten years, there has been an explosion in interest in sentiment analysis, with many interesting and impressive results. For example, the first twenty publications on Google Scholar returned for the Query “sentiment analysis” all date from 2003 or later, and have a total citation count of 12,140. The total number of publications is in the thousands. Partly, this interest is driven by the immediate commercial applications of sentiment analysis. Sentiment is a “private state” (Wiebe, 1990). However, it is not the only private state that has received attention in the computational literature; others include belief and intention. In this tutorial, we propose to provide a deeper understanding of what a private state is. We will concentrate on sentiment and belief. We will provide background that will allow the tutorial participants to understand the notion of a private state as a cognitive phenomenon, which can be manifested linguistically in communication in various ways. We will explain the formalization in terms of a triple of state, source, and target. We will discuss how to m</context>
</contexts>
<marker>Wiebe, 1990</marker>
<rawString>Janyce M. Wiebe. 1990. Identifying subjective characters in narrative. In Proceedings of the 13th International Conference on Computational Linguistics (COLING’90).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Tracking point of view in narrative.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="10691" citStr="Wiebe, 1994" startWordPosition="1730" endWordPosition="1731">ent grants from the NSF, DARPA, and IARPA. He has been the Chair of the North American Chapter of the Association for Computational Linguistics. He has been on the editorial board of Computational Linguistics, and has served as chair or area chair for several major conferences. http: //www.cs.columbia.edu/-rambow 3.2 Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and</context>
</contexts>
<marker>Wiebe, 1994</marker>
<rawString>Janyce Wiebe. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005),</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="10802" citStr="Wilson et al., 2005" startWordPosition="1747" endWordPosition="1750">ssociation for Computational Linguistics. He has been on the editorial board of Computational Linguistics, and has served as chair or area chair for several major conferences. http: //www.cs.columbia.edu/-rambow 3.2 Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is c</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-2005), pages 347–354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes ofprivate states.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Intelligent Systems Program, University of Pittsburgh.</institution>
<contexts>
<context position="1566" citStr="Wilson, 2007" startWordPosition="248" endWordPosition="249">g of what a private state is. We will concentrate on sentiment and belief. We will provide background that will allow the tutorial participants to understand the notion of a private state as a cognitive phenomenon, which can be manifested linguistically in communication in various ways. We will explain the formalization in terms of a triple of state, source, and target. We will discuss how to model the source and the target. We will then explain in some detail the annotations that have been made. The issue of annotation is crucial for private states: while the MPQA corpus (Wiebe et al., 2005; Wilson, 2007) has been around for some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is getting at. Furthermore, there are currently several efforts underway of creating new versions of annotations, which we will also present. The larger goal of this tutorial is to allow the tutorial participants to gain a deeper understanding of the role of private states in human communication, and to encourage them to use this dee</context>
<context position="4331" citStr="Wilson, 2007" startWordPosition="688" endWordPosition="690">e information about whom or what the subject supports or opposes. A recent NIST evaluation – The Knowledge Base Population (KBP) Sentiment track1 — aims at using corpora to collect information regarding sentiments expressed toward or by named entities. Annotated corpora of reviews (Hu and Liu, 2004; Titov and McDonald, 2008), widely used in NLP, often include annotations of targets that are aspects of products or services. As such, they are somewhat limited, excluding, e.g., events or agents of events. A widely used corpus is Version 2 of the MPQA opinion annotated corpus (Wiebe et al., 2005; Wilson, 2007). It is entirely span-based, and contains no eTarget annotations. However, it provides an infrastructure for sentiment annotation that is not provided by other sentiment NLP corpora, and is much more varied in topic, genre, and publication source. MPQA 3.0 (Deng and Wiebe, 2015), which was recently created, adds entity- and event-target (eTarget) annotations to the MPQA 2.0 annotations (Wilson, 2007).2 The MPQA annotations consist of private states, states of a source holding an attitude, optionally toward a target. An important property of sources is that they are nested, reflecting the fact </context>
</contexts>
<marker>Wilson, 2007</marker>
<rawString>Theresa Wilson. 2007. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes ofprivate states. Ph.D. thesis, Intelligent Systems Program, University of Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>