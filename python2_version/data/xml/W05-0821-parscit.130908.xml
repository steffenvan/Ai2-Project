<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022842">
<title confidence="0.997057">
Improved Language Modeling for Statistical Machine Translation
</title>
<author confidence="0.997579">
Katrin Kirchhoff and Mei Yang
</author>
<affiliation confidence="0.999374">
Department of Electrical Engineering
University of Washington, Seattle, WA, 98195
</affiliation>
<email confidence="0.998857">
{katrin,yangmei}@ee.washington.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999436722222222">
Statistical machine translation systems
use a combination of one or more transla-
tion models and a language model. While
there is a significant body of research ad-
dressing the improvement of translation
models, the problem of optimizing lan-
guage models for a specific translation
task has not received much attention. Typ-
ically, standard word trigram models are
used as an out-of-the-box component in
a statistical machine translation system.
In this paper we apply language model-
ing techniques that have proved benefi-
cial in automatic speech recognition to the
ACL05 machine translation shared data
task and demonstrate improvements over a
baseline system with a standard language
model.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998785666666667">
Statistical machine translation (SMT) makes use of
a noisy channel model where a sentence e� in the de-
sired language can be conceived of as originating as
a sentence f in a source language. The goal is to
find, for every input utterance 1, the best hypothesis
e* such that
</bodyText>
<equation confidence="0.8671845">
e* = argmaxeP(e |f) = argmaxeP( f|e)P(e)
(1)
</equation>
<bodyText confidence="0.97088">
P(�f|e) is the translation model expressing proba-
bilistic constraints on the association of source and
target strings. P(e) is a language model specifying
the probability of target language strings. Usually, a
standard word trigram model of the form
</bodyText>
<equation confidence="0.779397666666667">
l
P(e1, ..., el) � ri P(ei|ei−1, ei−2) (2)
i=3
</equation>
<bodyText confidence="0.996520894736842">
is used, where e� = e1, ..., el. Each word is predicted
based on a history of two preceding words.
Most work in SMT has concentrated on develop-
ing better translation models, decoding algorithms,
or minimum error rate training for SMT. Compara-
tively little effort has been spent on language mod-
eling for machine translation. In other fields, partic-
ularly in automatic speech recognition (ASR), there
exists a large body of work on statistical language
modeling, addressing e.g. the use of word classes,
language model adaptation, or alternative probabil-
ity estimation techniques. The goal of this study was
to use some of the language modeling techniques
that have proved beneficial for ASR in the past and
to investigate whether they transfer to statistical ma-
chine translation. In particular, this includes lan-
guage models that make use of morphological and
part-of-speech information, so-called factored lan-
guage models.
</bodyText>
<sectionHeader confidence="0.975145" genericHeader="method">
2 Factored Language Models
</sectionHeader>
<bodyText confidence="0.999934571428571">
A factored language model (FLM) (Bilmes and
Kirchhoff, 2003) is based on a representation of
words as feature vectors and can utilize a variety of
additional information sources in addition to words,
such as part-of-speech (POS) information, morpho-
logical information, or semantic features, in a uni-
fied and principled framework. Assuming that each
</bodyText>
<page confidence="0.976569">
125
</page>
<note confidence="0.8340845">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125–128,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.965764">
word w can be decomposed into k features, i.e. w �
f1:K, a trigram model can be defined as
</bodyText>
<equation confidence="0.947664">
p(f1:K
1 , f1:K
2 , ..., f1:K
T ) �
(3)
</equation>
<bodyText confidence="0.993645285714286">
Each word is dependent not only on a single stream
of temporally preceding words, but also on addi-
tional parallel streams of features. This represen-
tation can be used to provide more robust probabil-
ity estimates when a particular word n-gram has not
been observed in the training data but its correspond-
ing feature combinations (e.g. stem or tag trigrams)
has been observed. FLMs are therefore designed to
exploit sparse training data more effectively. How-
ever, even when a sufficient amount of training data
is available, a language model utilizing morpholog-
ical and POS information may bias the system to-
wards selecting more fluent translations, by boost-
ing the score of hypotheses with e.g. frequent POS
combinations. In FLMs, word feature information
is integrated via a new generalized parallel back-
off technique. In standard Katz-style backoff, the
maximum-likelihood estimate of an n-gram with too
few observations in the training data is replaced with
a probability derived from the lower-order (n − 1)-
gram and a backoff weight as follows:
</bodyText>
<equation confidence="0.92957725">
pBO(wt|wt−1, wt−2) (4)
f dcpML(wt|wt−1, wt−2) if c &gt; T
α(wt−1,wt−2)pBO(wt|wt−1)
otherwise
</equation>
<figureCaption confidence="0.981223333333333">
Figure 1: Standard backoff path for a 4-gram lan-
guage model over words (left) and backoff graph
over word features (right).
</figureCaption>
<bodyText confidence="0.99992176744186">
is also possible to choose multiple paths and com-
bine their probability estimates. This is achieved by
replacing the backed-off probability pBO in Equa-
tion 2 by a general function g, which can be any
non-negative function applied to the counts of the
lower-order n-gram. Several different g functions
can be chosen, e.g. the mean, weighted mean, prod-
uct, minimum or maximum of the smoothed prob-
ability distributions over all subsets of conditioning
factors. In addition to different choices for g, dif-
ferent discounting parameters can be selected at dif-
ferent levels in the backoff graph. One difficulty in
training FLMs is the choice of the best combination
of conditioning factors, backoff path(s) and smooth-
ing options. Since the space of different combina-
tions is too large to be searched exhaustively, we use
a guided search procedure based on Genetic Algo-
rithms (Duh and Kirchhoff, 2004), which optimizes
the FLM structure with respect to the desired crite-
rion. In ASR, this is usually the perplexity of the
language model on a held-out dataset; here, we use
the BLEU scores of the oracle 1-best hypotheses on
the development set, as described below. FLMs have
previously shown significant improvements in per-
plexity and word error rate on several ASR tasks
(e.g. (Vergyri et al., 2004)).
where c is the count of (wt, wt−1, wt−2), pML
denotes the maximum-likelihood estimate, T is a
count threshold, dc is a discounting factor and
α(wt−1, wt−2) is a normalization factor. During
standard backoff, the most distant conditioning vari-
able (in this case wt−2) is dropped first, followed
by the second most distant variable etc., until the
unigram is reached. This can be visualized as a
backoff path (Figure 1(a)). If additional condition-
ing variables are used which do not form a tempo-
ral sequence, it is not immediately obvious in which
order they should be eliminated. In this case, sev-
eral backoff paths are possible, which can be sum-
marized in a backoff graph (Figure 1(b)). Paths in
this graph can be chosen in advance based on lin-
guistic knowledge, or at run-time based on statis-
tical criteria such as counts in the training set. It
</bodyText>
<sectionHeader confidence="0.995793" genericHeader="method">
3 Baseline System
</sectionHeader>
<bodyText confidence="0.995957">
We used a fairly simple baseline system trained us-
ing standard tools, i.e. GIZA++ (Och and Ney, 2000)
for training word alignments and Pharaoh (Koehn,
2004) for phrase-based decoding. The training data
</bodyText>
<figure confidence="0.65757505">
F F� F� F�
(a) (b)
Wt Wt−1 Wt−2 Wt−3
Wt Wt−1 Wt−2
Wt Wt−1
Wt
F F� F�
F F�
F F� F�
F F�
F
F F� F�
F F�
p(f1:K
t |f1:K
t−1 , f1:K
t−2 )
T
ri
t=3
</figure>
<page confidence="0.978861">
126
</page>
<bodyText confidence="0.999977942857143">
was that provided on the ACL05 Shared MT task
website for 4 different language pairs (translation
from Finnish, Spanish, French into English); no
additional data was used. Preprocessing consisted
of lowercasing the data and filtering out sentences
with a length ratio greater than 9. The total num-
ber of training sentences and words per language
pair ranged between 11.3M words (Finnish-English)
and 15.7M words (Spanish-English). The develop-
ment data consisted of the development sets pro-
vided on the website (2000 sentences each). We
trained our own word alignments, phrase table, lan-
guage model, and model combination weights. The
language model was a trigram model trained us-
ing the SRILM toolkit, with modified Kneser-Ney
smoothing and interpolation of higher- and lower-
order ngrams. Combination weights were trained
using the minimum error weight optimization pro-
cedure provided by Pharaoh. We use a two-pass de-
coding approach: in the first pass, Pharaoh is run
in N-best mode to produce N-best lists with 2000
hypotheses per sentence. Seven different compo-
nent model scores are collected from the outputs,
including the distortion model score, the first-pass
language model score, word and phrase penalties,
and bidirectional phrase and word translation scores,
as used in Pharaoh (Koehn, 2004). In the second
pass, the N-best lists are rescored with additional
language models. The resulting scores are then com-
bined with the above scores in a log-linear fashion.
The combination weights are optimized on the de-
velopment set to maximize the BLEU score. The
weighted combined scores are then used to select
the final 1-best hypothesis. The individual rescoring
steps are described in more detail below.
</bodyText>
<sectionHeader confidence="0.996607" genericHeader="method">
4 Language Models
</sectionHeader>
<bodyText confidence="0.999993027027027">
We trained two additional language models to be
used in the second pass, one word-based 4-gram
model, and a factored trigram model. Both were
trained on the same training set as the baseline sys-
tem. The 4-gram model uses modified Kneser-
Ney smoothing and interpolation of higher-order
and lower-order n-gram probabilities. The potential
advantage of this model is that it models n-grams
up to length 4; since the BLEU score is a combina-
tion of n-gram precision scores up to length 4, the
integration of a 4-gram language model might yield
better results. Note that this can only be done in a
rescoring framework since the first-pass decoder can
only use a trigram language model.
For the factored language models, a feature-based
word representation was obtained by tagging the text
with Rathnaparki’s maximum-entropy tagger (Rat-
naparkhi, 1996) and by stemming words using the
Porter stemmer (Porter, 1980). Thus, the factored
language models use two additional features per
word. A word history of up to 2 was considered (3-
gram FLMs). Rather than optimizing the FLMs on
the development set references, they were optimized
to achieve a low perplexity on the oracle 1-best hy-
potheses (the hypotheses with the best individual
BLEU scores) from the first decoding pass. This is
done to avoid optimizing the model on word combi-
nations that might never be hypothesized by the first-
pass decoder, and to bias the model towards achiev-
ing a high BLEU score. Since N-best lists differ for
different language pairs, a separate FLM was trained
for each language pair. While both the 4-gram lan-
guage model and the FLMs achieved a 8-10% reduc-
tion in perplexity on the dev set references compared
to the baseline language model, their perplexities on
the oracle 1-best hypotheses were not significantly
different from that of the baseline model.
</bodyText>
<sectionHeader confidence="0.967518" genericHeader="method">
5 N-best List Rescoring
</sectionHeader>
<bodyText confidence="0.999555941176471">
For N-best list rescoring, the original seven model
scores are combined with the scores of the second-
pass language models using the framework of dis-
criminative model combination (Beyerlein, 1998).
This approach aims at an optimal (with respect to
a given error criterion) integration of different infor-
mation sources in a log-linear model, whose com-
bination weights are trained discriminatively. This
combination technique has been used successfully
in ASR, where weights are typically optimized to
minimize the empirical word error count on a held-
out set. In this case, we use the BLEU score of
the N-best hypothesis as an optimization criterion.
Optimization is performed using a simplex downhill
method known as amoeba search (Nelder and Mead,
1965), which is available as part of the SRILM
toolkit.
</bodyText>
<page confidence="0.988261">
127
</page>
<table confidence="0.9993118">
Language pair 1st pass oracle
Fi-En 21.8 29.8
Fr-En 28.9 34.4
De-En 23.9 31.0
Es-En 30.8 37.4
</table>
<tableCaption confidence="0.9087355">
Table 1: First-pass (left column) and oracle results
(right column) on the dev set (% BLEU).
</tableCaption>
<table confidence="0.9998724">
Language pair 4-gram FLM both
Fi-En 22.2 22.2 22.3
Fr-En 30.2 30.2 30.4
De-En 24.6 24.2 24.6
Es-En 31.4 31.0 31.3
</table>
<tableCaption confidence="0.814457">
Table 2: Second-pass rescoring results (% BLEU)
on the dev set for 4-gram LM, 3-gram FLM, and
their combination.
</tableCaption>
<sectionHeader confidence="0.999892" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999866764705882">
The results from the first decoding pass on the de-
velopment set are shown in Table 1. The second
column in Table 1 lists the oracle BLEU scores for
the N-best lists, i.e. the scores obtained by always
selecting the hypothesis known to have the highest
individual BLEU score. We see that considerable
improvements can in principle be obtained by a bet-
ter second-pass selection of hypotheses. The lan-
guage model rescoring results are shown in Table 2,
for both types of second-pass language models indi-
vidually, and for their combination. In both cases we
obtain small improvements in BLEU score, with the
4-gram providing larger gains than the 3-gram FLM.
Since their combination only yielded negligible ad-
ditional improvements, only 4-grams were used for
processing the final evaluation sets. The evaluation
results are shown in Table 3.
</bodyText>
<table confidence="0.9990754">
Language pair baseline 4-gram
Fi-En 21.6 22.0
Fr-En 29.3 30.3
De-En 24.2 24.8
Es-En 30.5 31.0
</table>
<tableCaption confidence="0.9971615">
Table 3: Second-pass rescoring results (% BLEU)
on the evaluation set.
</tableCaption>
<sectionHeader confidence="0.995147" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999991263157895">
We have demonstrated improvements in BLEU
score by utilizing more complex language models
in the rescoring pass of a two-pass SMT system.
We noticed that FLMs performed worse than word-
based 4-gram models. However, only trigram FLM
were used in the present experiments; larger im-
provements might be obtained by 4-gram FLMs.
The weights assigned to the second-pass language
models during weight optimization were larger than
those assigned to the first-pass language model, sug-
gesting that both the word-based model and the FLM
provide more useful scores than the baseline lan-
guage model. Finally, we observed that the overall
improvement represents only a small portion of the
possible increase in BLEU score as indicated by the
oracle results, suggesting that better language mod-
els do not have a significant effect on the overall sys-
tem performance unless the translation model is im-
proved as well.
</bodyText>
<sectionHeader confidence="0.99476" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998804666666667">
This work was funded by the National Science
Foundation, Grant no. IIS-0308297. We are grate-
ful to Philip Koehn for assistance with Pharaoh.
</bodyText>
<sectionHeader confidence="0.999215" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999032523809524">
P. Beyerlein. 1998. Discriminative model combination. In
Proc. ICASSP, pages 481–484.
J.A. Bilmes and K. Kirchhoff. 2003. Factored language mod-
els and generalized parallel backoff. In Proceedings of
HLT/NAACL, pages 4–6.
K. Duh and K. Kirchhoff. 2004. Automatic learning of lan-
guage model structure. In Proceedings of COLING.
P. Koehn. 2004. Pharaoh: a beam search decoder for phrase-
based statistical machine translation models. In Proceedings
ofAMTA.
J.A. Nelder and R. Mead. 1965. A simplex method for function
minimization. Computing Journal, 7(4):308–313.
F.J. Och and H. Ney. 2000. Giza++: Training of sta-
tistical translation models. http://www-i6.informatik.rwth-
aachen.de/ och/software/GIZA++.html.
M.F. Porter. 1980. An algorithm for suffix stripping. Program,
14(3):130–137.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Proceedings EMNLP, pages 133–141.
D. Vergyri et al. 2004. Morphology-based language modeling
for Arabic speech recognition. In Proceedings ofICSLP.
</reference>
<page confidence="0.996669">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957419">
<title confidence="0.99984">Improved Language Modeling for Statistical Machine Translation</title>
<author confidence="0.997077">Katrin Kirchhoff</author>
<author confidence="0.997077">Mei</author>
<affiliation confidence="0.997518">Department of Electrical University of Washington, Seattle, WA,</affiliation>
<abstract confidence="0.998156157894737">Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Beyerlein</author>
</authors>
<title>Discriminative model combination. In</title>
<date>1998</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>481--484</pages>
<contexts>
<context position="10680" citStr="Beyerlein, 1998" startWordPosition="1741" endWordPosition="1742">gh BLEU score. Since N-best lists differ for different language pairs, a separate FLM was trained for each language pair. While both the 4-gram language model and the FLMs achieved a 8-10% reduction in perplexity on the dev set references compared to the baseline language model, their perplexities on the oracle 1-best hypotheses were not significantly different from that of the baseline model. 5 N-best List Rescoring For N-best list rescoring, the original seven model scores are combined with the scores of the secondpass language models using the framework of discriminative model combination (Beyerlein, 1998). This approach aims at an optimal (with respect to a given error criterion) integration of different information sources in a log-linear model, whose combination weights are trained discriminatively. This combination technique has been used successfully in ASR, where weights are typically optimized to minimize the empirical word error count on a heldout set. In this case, we use the BLEU score of the N-best hypothesis as an optimization criterion. Optimization is performed using a simplex downhill method known as amoeba search (Nelder and Mead, 1965), which is available as part of the SRILM t</context>
</contexts>
<marker>Beyerlein, 1998</marker>
<rawString>P. Beyerlein. 1998. Discriminative model combination. In Proc. ICASSP, pages 481–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="2547" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="393" endWordPosition="396">on (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques. The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation. In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models. 2 Factored Language Models A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. Assuming that each 125 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 125–128, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 word w can be decomposed into k features, i.e. w � f1:K, a trigram model can be defined as p(f1:K 1 , f1:K 2 , ..., f1:K T ) � (3) Each word is depend</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J.A. Bilmes and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proceedings of HLT/NAACL, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic learning of language model structure.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5285" citStr="Duh and Kirchhoff, 2004" startWordPosition="839" endWordPosition="842">. Several different g functions can be chosen, e.g. the mean, weighted mean, product, minimum or maximum of the smoothed probability distributions over all subsets of conditioning factors. In addition to different choices for g, different discounting parameters can be selected at different levels in the backoff graph. One difficulty in training FLMs is the choice of the best combination of conditioning factors, backoff path(s) and smoothing options. Since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure based on Genetic Algorithms (Duh and Kirchhoff, 2004), which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004)). where c is the count of (wt, wt−1, wt−2), pML denotes the maximum-likelihood estimate, T is a count threshold, dc is a discounting factor and α(wt−1, wt−2) is a normalization factor. During standard</context>
</contexts>
<marker>Duh, Kirchhoff, 2004</marker>
<rawString>K. Duh and K. Kirchhoff. 2004. Automatic learning of language model structure. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrasebased statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="6708" citStr="Koehn, 2004" startWordPosition="1084" endWordPosition="1085">igure 1(a)). If additional conditioning variables are used which do not form a temporal sequence, it is not immediately obvious in which order they should be eliminated. In this case, several backoff paths are possible, which can be summarized in a backoff graph (Figure 1(b)). Paths in this graph can be chosen in advance based on linguistic knowledge, or at run-time based on statistical criteria such as counts in the training set. It 3 Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data F F� F� F� (a) (b) Wt Wt−1 Wt−2 Wt−3 Wt Wt−1 Wt−2 Wt Wt−1 Wt F F� F� F F� F F� F� F F� F F F� F� F F� p(f1:K t |f1:K t−1 , f1:K t−2 ) T ri t=3 126 was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair ranged between 11.3M words (Finnish-English) and 15.7M w</context>
<context position="8202" citStr="Koehn, 2004" startWordPosition="1335" endWordPosition="1336">ILM toolkit, with modified Kneser-Ney smoothing and interpolation of higher- and lowerorder ngrams. Combination weights were trained using the minimum error weight optimization procedure provided by Pharaoh. We use a two-pass decoding approach: in the first pass, Pharaoh is run in N-best mode to produce N-best lists with 2000 hypotheses per sentence. Seven different component model scores are collected from the outputs, including the distortion model score, the first-pass language model score, word and phrase penalties, and bidirectional phrase and word translation scores, as used in Pharaoh (Koehn, 2004). In the second pass, the N-best lists are rescored with additional language models. The resulting scores are then combined with the above scores in a log-linear fashion. The combination weights are optimized on the development set to maximize the BLEU score. The weighted combined scores are then used to select the final 1-best hypothesis. The individual rescoring steps are described in more detail below. 4 Language Models We trained two additional language models to be used in the second pass, one word-based 4-gram model, and a factored trigram model. Both were trained on the same training se</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Pharaoh: a beam search decoder for phrasebased statistical machine translation models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Nelder</author>
<author>R Mead</author>
</authors>
<title>A simplex method for function minimization.</title>
<date>1965</date>
<journal>Computing Journal,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="11237" citStr="Nelder and Mead, 1965" startWordPosition="1827" endWordPosition="1830">e framework of discriminative model combination (Beyerlein, 1998). This approach aims at an optimal (with respect to a given error criterion) integration of different information sources in a log-linear model, whose combination weights are trained discriminatively. This combination technique has been used successfully in ASR, where weights are typically optimized to minimize the empirical word error count on a heldout set. In this case, we use the BLEU score of the N-best hypothesis as an optimization criterion. Optimization is performed using a simplex downhill method known as amoeba search (Nelder and Mead, 1965), which is available as part of the SRILM toolkit. 127 Language pair 1st pass oracle Fi-En 21.8 29.8 Fr-En 28.9 34.4 De-En 23.9 31.0 Es-En 30.8 37.4 Table 1: First-pass (left column) and oracle results (right column) on the dev set (% BLEU). Language pair 4-gram FLM both Fi-En 22.2 22.2 22.3 Fr-En 30.2 30.2 30.4 De-En 24.6 24.2 24.6 Es-En 31.4 31.0 31.3 Table 2: Second-pass rescoring results (% BLEU) on the dev set for 4-gram LM, 3-gram FLM, and their combination. 6 Results The results from the first decoding pass on the development set are shown in Table 1. The second column in Table 1 lists </context>
</contexts>
<marker>Nelder, Mead, 1965</marker>
<rawString>J.A. Nelder and R. Mead. 1965. A simplex method for function minimization. Computing Journal, 7(4):308–313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<note>http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html.</note>
<contexts>
<context position="6653" citStr="Och and Ney, 2000" startWordPosition="1074" endWordPosition="1077">igram is reached. This can be visualized as a backoff path (Figure 1(a)). If additional conditioning variables are used which do not form a temporal sequence, it is not immediately obvious in which order they should be eliminated. In this case, several backoff paths are possible, which can be summarized in a backoff graph (Figure 1(b)). Paths in this graph can be chosen in advance based on linguistic knowledge, or at run-time based on statistical criteria such as counts in the training set. It 3 Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data F F� F� F� (a) (b) Wt Wt−1 Wt−2 Wt−3 Wt Wt−1 Wt−2 Wt Wt−1 Wt F F� F� F F� F F� F� F F� F F F� F� F F� p(f1:K t |f1:K t−1 , f1:K t−2 ) T ri t=3 126 was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair r</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Giza++: Training of statistical translation models. http://www-i6.informatik.rwthaachen.de/ och/software/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="9534" citStr="Porter, 1980" startWordPosition="1551" endWordPosition="1552">r-order n-gram probabilities. The potential advantage of this model is that it models n-grams up to length 4; since the BLEU score is a combination of n-gram precision scores up to length 4, the integration of a 4-gram language model might yield better results. Note that this can only be done in a rescoring framework since the first-pass decoder can only use a trigram language model. For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparki’s maximum-entropy tagger (Ratnaparkhi, 1996) and by stemming words using the Porter stemmer (Porter, 1980). Thus, the factored language models use two additional features per word. A word history of up to 2 was considered (3- gram FLMs). Rather than optimizing the FLMs on the development set references, they were optimized to achieve a low perplexity on the oracle 1-best hypotheses (the hypotheses with the best individual BLEU scores) from the first decoding pass. This is done to avoid optimizing the model on word combinations that might never be hypothesized by the firstpass decoder, and to bias the model towards achieving a high BLEU score. Since N-best lists differ for different language pairs,</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings EMNLP,</booktitle>
<pages>133--141</pages>
<contexts>
<context position="9472" citStr="Ratnaparkhi, 1996" startWordPosition="1540" endWordPosition="1542">fied KneserNey smoothing and interpolation of higher-order and lower-order n-gram probabilities. The potential advantage of this model is that it models n-grams up to length 4; since the BLEU score is a combination of n-gram precision scores up to length 4, the integration of a 4-gram language model might yield better results. Note that this can only be done in a rescoring framework since the first-pass decoder can only use a trigram language model. For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparki’s maximum-entropy tagger (Ratnaparkhi, 1996) and by stemming words using the Porter stemmer (Porter, 1980). Thus, the factored language models use two additional features per word. A word history of up to 2 was considered (3- gram FLMs). Rather than optimizing the FLMs on the development set references, they were optimized to achieve a low perplexity on the oracle 1-best hypotheses (the hypotheses with the best individual BLEU scores) from the first decoding pass. This is done to avoid optimizing the model on word combinations that might never be hypothesized by the firstpass decoder, and to bias the model towards achieving a high BLEU </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tagger. In Proceedings EMNLP, pages 133–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
</authors>
<title>Morphology-based language modeling for Arabic speech recognition.</title>
<date>2004</date>
<booktitle>In Proceedings ofICSLP.</booktitle>
<marker>Vergyri, 2004</marker>
<rawString>D. Vergyri et al. 2004. Morphology-based language modeling for Arabic speech recognition. In Proceedings ofICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>