<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004715">
<title confidence="0.9274995">
Recession Segmentation: Simpler Online Word Segmentation Using
Limited Resources*
</title>
<author confidence="0.998877">
Constantine Lignos, Charles Yang
</author>
<affiliation confidence="0.9982905">
Dept. of Computer and Information Science, Dept. of Linguistics
University of Pennsylvania
</affiliation>
<email confidence="0.998471">
lignos@cis.upenn.edu,charles.yang@ling.upenn.edu
</email>
<sectionHeader confidence="0.993935" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999276315789474">
In this paper we present a cognitively plau-
sible approach to word segmentation that
segments in an online fashion using only
local information and a lexicon of pre-
viously segmented words. Unlike popu-
lar statistical optimization techniques, the
learner uses structural information of the
input syllables rather than distributional
cues to segment words. We develop a
memory model for the learner that like a
child learner does not recall previously hy-
pothesized words perfectly. The learner at-
tains an F-score of 86.69% in ideal condi-
tions and 85.05% when word recall is un-
reliable and stress in the input is reduced.
These results demonstrate the power that a
simple learner can have when paired with
appropriate structural constraints on its hy-
potheses.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999755941176471">
The problem of word segmentation presents an
important challenge in language acquisition. The
child learner must segment a continuous stream of
sounds into words without knowing what the in-
dividual words are until the stream has been seg-
mented. Computational models present an op-
portunity to test the potentially innate constraints,
structures, and algorithms that a child may be us-
ing to guide her acquisition. In this work we de-
velop a segmentation model from the constraints
suggested by Yang (2004) and evaluate it in ideal-
ized conditions and conditions that better approx-
imate the environment of a child learner. We seek
to determine how these limitations in the learner’s
input and memory affect the learner’s performance
and to demonstrate that the presented learner is ro-
bust even under non-ideal conditions.
</bodyText>
<note confidence="0.201359">
*Portions of this work were adapted from an earlier
manuscript, Word Segmentation: Quick But Not Dirty.
</note>
<sectionHeader confidence="0.999322" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999816974358974">
Most recent work in word segmentation of child-
directed speech has operated within statistical op-
timization frameworks, particularly Bayesian ap-
proaches (Goldwater et al., 2009; Johnson and
Goldwater, 2009). These models have established
the state-of-the-art for the task of selecting appro-
priate word boundaries from a stream of unstruc-
tured phonemes. But while these models deliver
excellent performance, it is not clear how they in-
form the process of acquisition.
Trying to find cognitive insight from these types
of models is difficult because of the inherent mis-
match in the quality and types of hypotheses they
maintain during learning. Children are incremen-
tal learners (Brown, 1973), and learners relying
on statistical optimization are generally not. A
child’s competence grows gradually as she hears
and produces more and more utterances, going
through predictable changes to her working gram-
mar (Marcus et al., 1992) that statistical optimiza-
tion techniques typically do not go through and do
not intend to replicate.
Statistical models provide excellent information
about the features, distributional cues, and priors
that can be used in learning, but provide little in-
formation about how a child learner can use this
information and how her knowledge of language
develops as the learning process evolves. Previ-
ous simulations in word segmentation using the
same type of distributional information as many
statistical optimization-based learners but without
an optimization model suggest that statistics alone
are not sufficient for learning to succeed in a com-
putationally efficient online manner; further con-
straints on the search space are needed (Yang,
2004).
Previous computational models have demanded
tremendous memory and computational capacity
from human learners. For example, the algorithm
</bodyText>
<page confidence="0.98896">
88
</page>
<note confidence="0.9552555">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88–97,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999924411764706">
of Brent &amp; Cartwright (1996) produces a set of
possible lexicons that describe the learning cor-
pus, each of which is evaluated as the learner it-
erates until no further improvement is possible. It
is unlikely that an algorithm of this type is some-
thing a human learner is capable of using given the
requirement to remember at the very least a long
history of recent utterances encountered and con-
stantly reanalyze them to find a optimal segmenta-
tion. Work in this tradition makes no claims, how-
ever, that these methods are actually the ones used
by human learners.
On the other hand, previous computational
models often underestimate the human learner’s
knowledge of linguistic representations. Most of
these models are “synthetic” in the sense of Brent
(1999): the raw material for segmentation is a
stream of segments, which are then successively
grouped into larger units and eventually, conjec-
tured words. This assumption may make the child
learner’s job unnecessarily hard; since syllables
are hierarchical structures consisting of segments,
treating the linguistic data as unstructured segment
sequences makes the problem harder than it actu-
ally is. For a given utterance, there are fewer sylla-
bles than segments, and hence fewer segmentation
possibilities.
Modeling the corpus using hierarchical gram-
mars that can model the input at varying levels
(word collocations, words, syllables, onsets, etc.)
provide the learner the most flexibility, allowing
the learner to build structure from the individual
phonemes and apply distributions at each level of
abstraction (Johnson and Goldwater, 2009). While
this results in state-of-the-art performance for seg-
mentation performed at the phoneme level, this
approach requires significant computational re-
sources as each additional level of representation
increases the complexity of learning. In addition,
it is not clear that some of the intermediate levels
in such an approach, such as word level colloca-
tions which are not syntactic constituents, would
have any linguistic or psychological reality to a
human learner.
A number of psychologically-motivated mod-
els of word segmentation rely on the use of syl-
labic transitional probabilities (TPs), basing the
use of TPs on experimental work in artificial lan-
guage learning (Saffran et al., 1996a; Saffran et
al., 1996b) and in corpus studies (Swingley, 2005).
The identification of the syllable as the basic unit
of segmentation is supported research in experi-
mental psychology using infants as young as 4-
days-old (Bijeljac-Babic et al., 1993), but when
syllable transitional probabilities are evaluated in
online learning procedures that only use local in-
formation (Yang, 2004), the results are surpris-
ingly poor, even under the assumption that the
learner has already syllabified the input perfectly.
Precision is 41.6%, and recall is 23.3%, which
we will show is worse than a simple baseline of
assuming every syllable is a word. The below-
baseline performance is unsurprising given that in
order for this type of model to posit a word bound-
ary, a transitional probability between syllables
must be lower than its neighbors. This condition
cannot be met if the input is a sequence of mono-
syllabic words for which a boundary must be pos-
tulated for every syllable; it is impossible to treat
every boundary as a local minimum.
While the pseudo-words used in infant stud-
ies measuring the ability to use transitional prob-
ability information are uniformly three-syllables
long, much of child-directed English consists of
sequences of monosyllabic words. Corpus statis-
tics reveal that on average a monosyllabic word
is followed by another monosyllabic word 85%
of time (Yang, 2004), and thus learners that use
only local transitional probabilities without any
global optimization are unlikely to succeed. This
problem does not affect online approaches that
use global information, such as computing the
maximum likelihood of the corpus incrementally
(Venkataraman, 2001). Since these approaches do
not require each boundary be a local minimum,
they are able to correctly handle a sequence of
monosyllable words.
We believe that the computational modeling of
psychological processes, with special attention to
concrete mechanisms and quantitative evaluations,
can play an important role in identifying the con-
straints and structures relevant to children’s acqui-
sition of language. Rather than using a prior which
guides the learner to a desired distribution, we ex-
amine learning with respect to a model in which
the hypothesis space is constrained by structural
requirements.
In this paper we take a different approach than
statistical optimization approaches by exploring
how well a learner can perform while processing
a corpus in an online fashion with only local in-
formation and a lexicon of previously segmented
</bodyText>
<page confidence="0.999587">
89
</page>
<bodyText confidence="0.999962714285714">
words. We present a simple, efficient approach
to word segmentation that uses structural informa-
tion rather than distributional cues in the input to
segment words. We seek to demonstrate that even
in the face of impoverished input and limited re-
sources, a simple learner can succeed when it op-
erates with the appropriate constraints.
</bodyText>
<sectionHeader confidence="0.899792" genericHeader="method">
3 Constraining the Learning Space
</sectionHeader>
<bodyText confidence="0.999635119565218">
Modern machine learning research (Gold, 1967;
Valiant, 1984; Vapnik, 2000) suggests that con-
straints on the learning space and the learning
algorithm are essential for realistically efficient
learning. If a domain-neutral learning model fails
on a specific task where children succeed, it is
likely that children are equipped with knowledge
and constraints specific to the task at hand. It
is important to identify such constraints to see to
what extent they complement, or even replace, do-
main neutral learning mechanisms.
A particularly useful constraint for word seg-
mentation, introduced to the problem of word
segmentation by Yang (2004) but previously dis-
cussed by Halle and Vergnaud (1987), is as fol-
lows:
Unique Stress Constraint (USC): A word can
bear at most one primary stress.
A simple example of how adult learners might
use the USC is upon hearing novel names or
words. Taking Star Wars characters as an exam-
ple, it is clear that chewbacca is one word but
darthvader cannot be as the latter bears two pri-
mary stresses.
The USC could give the learner many isolated
words for free. If the learner hears an utterance
that contains exactly one primary stress, it is likely
it is a single word. Moreover, the segmenta-
tion for a multiple word utterance can be equally
straightforward under USC. Consider a sequence
W1S1S2S3W2, where W stands for a weak sylla-
ble and S stands for a strong syllable. A learner
equipped with USC will immediately know that
the sequence consists of three words: specifically,
W1S1, S2, and S2W2.
The USC can also constrain the use of other
learning techniques. For example, the syllable
consequence S1W1W2W3S2 cannot be segmented
by USC alone, but it may still provide cues that
facilitate the application of other segmentation
strategies. For instance, the learner knows that the
sequence consists of at least two words, as indi-
cated by two strong syllables. Moreover, it also
knows that in the window between S1 and S2 there
must be one or more word boundaries.
Yang (2004) evaluates the effectiveness of the
USC in conjunction with a simple approach to us-
ing transitional probabilities. The performance of
the approach presented there improves dramati-
cally if the learner is equipped with the assump-
tion that each word can have only one primary
stress. If the learner knows this, then it may
limit the search for local minima to only the win-
dow between two syllables that both bear primary
stress, e.g., between the two a’s in the sequence
languageacquisition. This assumption is plau-
sible given that 7.5-month-old infants are sensi-
tive to strong/weak prosodic distinctions (Jusczyk,
1999). Yang’s stress-delimited algorithm achieves
the precision of 73.5% and recall of 71.2%, a sig-
nificant improvement over using TPs alone, but
still below the baseline presented in our results.
The improvement of the transitional
probability-based approach when provided
with a simple linguistic constraint suggests
that structural constraints can be powerful in
narrowing the hypothesis space so that even
sparse, local information can prove useful and
simple segmentation strategies can become more
effective.
It should be noted that the classification of every
syllable as “weak” or “strong” is a significant sim-
plification. Stress is better organized into hierar-
chical patterns constructed on top of syllables that
vary in relative prominence based on the domain
of each level of the hierarchy, and generally lan-
guages avoid adjacent strong syllables (Liberman
and Prince, 1977). We later discuss a manipula-
tion of the corpus used by Yang (2004) to address
this concern.
Additionally, there are significant challenges
in reconstructing stress from an acoustic signal
(Van Kuijk and Boves, 1999). For a child learner
to use the algorithm presented here, she would
need to have mechanisms for detecting stress in
the speech signal and categorizing the gradient
stress in utterances into a discrete level for each
syllable. These mechanisms are not addressed in
this work; our focus is on an algorithm that can
succeed given discrete stress information for each
syllable. Given the evidence that infants can dis-
tinguish weak and strong syllables and use that in-
</bodyText>
<page confidence="0.988817">
90
</page>
<bodyText confidence="0.9999672">
formation to detect word boundaries (Jusczyk et
al., 1999), we believe that it is reasonable to as-
sume that identifying syllabic stress is a task an
infant learner can perform at the developmental
stage of word segmentation.
</bodyText>
<sectionHeader confidence="0.923198" genericHeader="method">
4 A Simple Algorithm for Word
Segmentation
</sectionHeader>
<bodyText confidence="0.996175727272727">
We now present a simple algebraic approach to
word segmentation based on the constraints sug-
gested by Yang (2004). The learner we present is
algebraic in that it has a lexicon which stores pre-
viously segmented words and identifies the input
as a combination of words already in the lexicon
and novel words. No transitional probabilities or
any distributional data are calculated from the in-
put. The learner operates in an online fashion, seg-
menting each utterance in a primarily left-to-right
fashion and updating its lexicon as it segments.
The USC is used in two ways by the learner.
First, if the current syllable has primary stress and
the next syllable also has primary stress, a word
boundary is placed between the current and next
syllable. Second, whenever the algorithm is faced
with the choice of accepting a novel word into the
lexicon and outputting it as a word, the learner
“abstains” from doing so if the word violates USC,
that is if it contains more than one primary stress.
Since not all words are stressed, if a word contains
no primary stresses it is considered an acceptable
word; only a word with more that one primary
stress is prohibited. If a sequence of syllables has
more than one primary stress and cannot be seg-
mented further, the learner does not include that
sequence in its segmentation of the utterance and
does not add it to the lexicon as it cannot be a valid
word.
The algorithm is as follows, with each step ex-
plained in further detail in the following para-
graphs.
For each utterance in the corpus, do the following:
</bodyText>
<listItem confidence="0.960859266666667">
1. As each syllable is encountered, use Initial
Subtraction and USC Segmentation to seg-
ment words from the beginning of the utter-
ance if possible.
2. If unsegmented syllables still remain, apply
Final Subtraction, segmenting words itera-
tively from the end of the utterance if pos-
sible.
3. If unsegmented syllables still remain, if those
syllables constitute a valid word under the
USC, segment them as a single word and add
them to the lexicon. Otherwise, abstain, and
do not include these syllables in the segmen-
tation of the sentence and do not add them to
the lexicon.
</listItem>
<bodyText confidence="0.995148285714286">
Initial Subtraction. If the syllables of the utter-
ance from the last segmentation (or the start of the
utterance) up to this point matches a word in the
lexicon but adding one more syllable would result
in it not being a known word, segment off the rec-
ognized word and increase its frequency. This iter-
atively segments the longest prefix word from the
utterance.
USC Segmentation. If the current and next syl-
lables have primary stress, place a word bound-
ary after the current syllable, treating all syllables
from the last segmentation point up to and and in-
cluding the current syllable as a potential word. If
these syllables form a valid word under the USC,
segment them as a word and add them to the lex-
icon. Otherwise, abstain, not including these syl-
lables in the segmentation of the sentence and not
adding them to the lexicon.
Final Subtraction. After initial subtraction and
USC Segmentation have been maximally applied
to the utterance, the learner is often left with a
sequence of syllables that is not prefixed by any
known word and does not have any adjacent pri-
mary stresses. In this situation the learner works
from right to left on the remaining utterance, iter-
atively removing words from the end of the utter-
ance if possible. Similar to the approach used in
Initial Subtraction, the longest word that is a suf-
fix word of the remaining syllables is segmented
off, and this is repeated until the entire utterance is
segmented or syllables remain that are not suffixed
by any known word.
The ability to abstain is a significant difference
between this learner and most recent work on this
task. Because the learner has a structural descrip-
tion for a word, the USC, it is able to reject any
hypothesized words that do not meet the descrip-
tion. This improves the learner’s precision and
recall because it reduces the number of incorrect
predictions the learner makes. The USC also al-
lows the learner keep impossible words out of its
lexicon.
</bodyText>
<page confidence="0.985444">
91
</page>
<figure confidence="0.934186">
Count
</figure>
<figureCaption confidence="0.998942">
Figure 1: The selected probabilistic memory func-
</figureCaption>
<bodyText confidence="0.99736525">
tion for α = 0.05. The dashed line at 0.05 rep-
resents the threshold above which a word is more
likely than not to be recalled, occurring at a count
of approximately 14.
</bodyText>
<sectionHeader confidence="0.996142" genericHeader="method">
5 A Probabilistic Lexicon
</sectionHeader>
<bodyText confidence="0.99993325">
To simulate the imperfect memory of a child
learner, we use a simple exponential function to
generate the probability with which a word is re-
trieved from the lexicon:
</bodyText>
<equation confidence="0.951123">
pr(word) = 1.0 − e−α,(,ord)
</equation>
<bodyText confidence="0.999980413043478">
pr(word) is the probability of a word being re-
trieved, α is a constant, and c(word) is the number
of times the word has been identified in segmen-
tations thus far. This type of memory function is
a simplified representation of models of humans’
memory recall capabilities (Anderson et al., 1998;
Gillund and Shiffrin, 1984). This memory func-
tion for the value of α = 0.05, the value used in
our experiments, is given in Figure 1. We later
show that the choice of α has little impact on the
learner’s segmentation performance, and thus the
more or less arbitrary selection of a value for α is
of little consequence.
When the algorithm attempts to subtract words
from the beginning or end of an utterance, it may
miss words in the lexicon due to this probabilis-
tic retrieval. The learner only has one opportu-
nity to recall a word in a given utterance. For ex-
ample, in the utterance P.EH1.N S.AH0.L (pencil),
if the learner has P.EH1.N and P.EH1.N S.AH0.L
in its lexicon but P.EH1.N is more frequent, it
may fail to recall P.EH1.N S.AH0.L when exam-
ining the second syllable but succeed in recog-
nizing P.EH1.N in the first. Thus it will break
off P.EH1.N instead of P.EH1.N S.AH0.L. This
means the learner may fail to reliably break off
the longest words, instead breaking off the longest
word that is successfully recalled.
While probabilistic memory means that the
learner will fail to recognize words it has seen be-
fore, potentially decreasing recall, it also provides
the learner the benefit of probabilistically failing
to repeat previous mistakes if they occur rarely.
Probabilistic word recall results in a “rich
get richer” phenomenon as the learner segments;
words that are used more often in segmentations
are more likely to be reused in later segmentations.
While recent work from Bayesian approaches has
used a Dirichlet Process to generate these distri-
butions (Goldwater et al., 2006), in this learner the
reuse of frequent items in learning is a result of
the memory model rather than an explicit process
of reusing old outcomes or generating new ones.
This growth is an inherent property of the cogni-
tive model of memory used here rather than an ex-
ternally imposed computational technique.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999972">
Our computational model is designed to process
child-directed speech. The corpus we use to eval-
uate it is the same corpus used by Yang (2004).
Adult utterances were extracted from the Brown
(1973) data in the CHILDES corpus (MacWhin-
ney, 2000), consisting of three children’s data:
Adam, Eve, and Sarah. We obtained the pho-
netic transcriptions of words from the Carnegie
Mellon Pronouncing Dictionary (CMUdict) Ver-
sion 0.6 (Weide, 1998), using the first pronunci-
ation of each word. In CMUdict, lexical stress
information is preserved by numbers: 0 for un-
stressed, 1 for primary stress, 2 for secondary
stress. For instance, cat is represented as K.AE1.T,
catalog is K.AE1.T.AH0.L.AO0.G, and catapult is
K.AE1.T.AH0.P.AH2.L.T. We treat primary stress
as “strong” and secondary or unstressed syllables
as “weak.”
For each word, the phonetic segments were
grouped into syllables. This process is straightfor-
ward by the use of the principle “Maximize On-
set,” which maximizes the length of the onset as
long as it is valid consonant cluster of English, i.e.,
</bodyText>
<figure confidence="0.9951442">
0
20 40
60 80 100
0.0 0.2 0.4 0.6 0.8 1.0
Probability of Recall
</figure>
<page confidence="0.956681">
92
</page>
<bodyText confidence="0.99992662962963">
it conforms to the phonotactic constraints of En-
glish. For example, Einstein is AY1.N.S.T.AY0.N
as segments and parsed into AY1.N S.T.AY0.N as
syllables: this is because /st/ is the longest valid
onset for the second syllable containing AY0 while
/nst/ is longer but violates English phonotac-
tics. While we performed syllabification as a pre-
processing step outside of learning, a child learner
would presumably learn the required phonotac-
tics as a part of learning to segment words. 9-
month old infants are believed to have learned
some phonotactic constraints of their native lan-
guage (Mattys and Jusczyk, 2001), and learning
these constraints can be done with only minimal
exposure (Onishi et al., 2002).
Finally, spaces and punctuation between
words were removed, but the boundaries be-
tween utterances–as indicated by line breaks in
CHILDES–are retained. Altogether, there are
226,178 words, consisting of 263,660 syllables.
The learning material is a list of unsegmented
syllable sequences grouped into utterances, and
the learner’s task is to find word boundaries that
group substrings of syllables together, building a
lexicon of words as it segments.
We evaluated the learner’s performance to ad-
dress these questions:
</bodyText>
<listItem confidence="0.987103875">
• How does probabilistic memory affect
learner performance?
• How much does degrading stress information
relied on by USC segmentation reduce per-
formance?
• What is the interaction between the proba-
bilistic lexicon and non-idealized stress infor-
mation?
</listItem>
<bodyText confidence="0.999581523076924">
To evaluate the learner, we tested configurations
that used a probabilistic lexicon and ones with per-
fect memory in two scenarios: Dictionary Stress,
and Reduced Stress. We create the Reduced Stress
condition in order to simulate that stress is of-
ten reduced in casual speech, and that language-
specific stress rules may cause reductions or shifts
in stress that prevent two strong syllables from oc-
curring in sequence. The difference between the
scenarios is defined as follows:
Dictionary Stress. The stress information is
given to the learner as it was looked up in CMU-
dict. For example, the first utterance from the
Adam corpus would be B.IH1.G D.R.AH1.M (big
drum), an utterance with two stressed monosyl-
lables (SS). In most languages, however, condi-
tions where two stressed syllables are in sequence
are handled by reducing the stress of one syllable.
This is simulated in the reduced stress condition.
Reduced Stress. The stress information ob-
tained from CMUdict is post-processed in the con-
text of each utterance. For any two adjacent pri-
mary stressed syllables, the first syllable is re-
duced from a strong syllable to a weak one. This is
applied iteratively from left to right, so for any se-
quence of n adjacent primary-stress syllables, only
the nth syllable retains primary stress; all others
are reduced. This removes the most valuable clue
as to where utterances can be segmented, as USC
segmentation no longer applies. This simulates the
stress retraction effect found in real speech, which
tries to avoid adjacent primary stresses.
Learners that use probabilistic memory were al-
lowed to iterate over the input two times with ac-
cess to the lexicon developed over previous iter-
ations but no access to previous segmentations.
This simulates a child hearing many of the same
words and utterances again, and reduces the effect
of the small corpus size used on the learning pro-
cess. Because the probabilistic memory reduces
the algorithm’s ability to build a lexicon, perfor-
mance in a single iteration is lower than perfect
memory conditions. In all other conditions, the
learner is allowed only a single pass over the cor-
pus.
The precision and recall metrics are calculated
for the segmentation that the learner outputs and
the lexicon itself. For an utterance, each word
in the learner’s segmentation that also appears in
the gold standard segmentation is counted as cor-
rect, and each word in the learner’s segmentation
not present in the gold standard segmentation is
a false alarm. F-score is computed using equally
balanced precision and recall (FO). The correct
words, false words, and number of words in the
gold standard are summed over the output in each
iteration to produce performance measures for that
iteration.
Precision, recall, and F-score are similarly com-
puted for the lexicon; every word in the learner’s
lexicon present in the gold standard is counted as
correct, and every word in the learner’s lexicon not
present in the gold standard is a false alarm. These
computations are performed over word types in
the lexicon, thus all words in the lexicon are of
</bodyText>
<page confidence="0.982541">
93
</page>
<bodyText confidence="0.979625666666667">
F−Score
equal weight in computing performance regard-
less of their frequency. In the probabilistic mem-
ory conditions, however, the memory function de-
fines the probability of each word being recalled
(and thus being considered a part of the lexicon)
at evaluation time.
In addition to evaluating the learner, we also im-
plemented three baseline approaches to compare
the learner against. The Utterance baseline seg-
menter assumes every utterance is a single word.
The Monosyllabic baseline segmenter assumes ev-
ery syllable is a single word. The USC segmenter
inserts word boundaries between all adjacent syl-
lables with primary stress in the corpus.
</bodyText>
<subsectionHeader confidence="0.804497">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.999832342857143">
The performance of the learner and baseline seg-
menters is given in Table 1. While the Utterance
segmenter provides expectedly poor performance,
the Monosyllabic segmenter sets a relatively high
baseline for the task. Because of the impoverished
morphology of English and the short words that
tend to be used in child-directed speech, assuming
each syllable is a word proves to be an excellent
heuristic. It is unlikely that this heuristic will per-
form as well in other languages. Because the USC
segmenter only creates segmentation points where
there are words of adjacent primary stress, it is
prone to attaching unstressed monosyllabic func-
tion words to content words, causing very low lex-
icon precision (13.56%).
With both perfect memory and dictionary stress
information, the learner attains an F-score of
86.69%, with precision (83.78%) lower than re-
call (89.81%). First, we consider the effects of
probabilistic memory on the learner. In the Dictio-
nary Stress condition, using probabilistic memory
decreases Fo by 1.15%, a relatively small impact
given that with the setting of α = 0.05 the learner
must use a word approximately 14 times before it
can retrieve it with 50% reliability and 45 times
before it can retrieve it with 90% reliability. In the
first iteration over the data set, 17.87% of lexicon
lookups for words that have been hypothesized be-
fore fail. The impact on FO is caused by a drop in
recall, as would be expected for a such a memory
model.
To examine the effect of the α parameter for
probabilistic memory on learner performance, we
plot the utterance and lexicon FO after the learner
iterates over the corpus once in the Probabilistic
</bodyText>
<figure confidence="0.707593">
Memory Parameter
</figure>
<figureCaption confidence="0.949866">
Figure 2: Learner utterance and lexicon F-scores
after two iterations when α is varied in the Proba-
bilistic Memory, Dictionary Stress condition
</figureCaption>
<table confidence="0.9996595">
Perfect Perfect
Memory, Memory,
Dictionary Reduced
Stress Stress
USC Seg. 114,333 0
Initial Sub. 65,800 164,989
Final Sub. 5,690 14,813
Total 185,823 179,802
</table>
<tableCaption confidence="0.923410666666667">
Table 2: Number of segmentations performed by
each operation: USC Segmentation, Initial Sub-
traction, and Final Subtraction.
</tableCaption>
<bodyText confidence="0.990904833333333">
Memory, Dictionary Stress condition. As Figure 2
shows, the choice of α has little effect on the ut-
terance FO through most of a broad range from
0.01 to 0.9. Because the setting of α determines
the number of times a word must be hypothesized
before it can reliably be recalled, it expectedly
has a significant effect on lexicon FO. The selec-
tion of α = 0.05 for our experiments is thus un-
likely to have had any significant bearing on the
utterance segmentation performance, although for
lower values of α precision is favored while for
larger values recall is favored. Larger values of
α imply the learner is able to recall items after
fewer exposures. While a larger value of α would
have yielded higher performance in lexicon per-
formance, it also assumes much more about the
learner’s memory capabilities.
The Reduced Stress condition also has only a
</bodyText>
<figure confidence="0.998724428571429">
0.0 0.2 0.4
0.6 0.8
1.0
0.0 0.2 0.4 0.6 0.8 1.0
● ● ● ● ● ● ● ● ● ● ●
● Utterance
Lexicon
</figure>
<page confidence="0.992592">
94
</page>
<table confidence="0.999271555555556">
Utterances Lexicon
Segmenter Precision Recall Fo Precision Recall Fo
Utterance 18.61% 4.67% 7.47% 3.57% 30.35% 6.39%
Monosyllabic 73.29% 85.44% 78.90% 55.41% 43.88% 48.97%
USC 81.06% 61.52% 69.95% 13.56% 66.97% 22.55%
Perfect Memory, Dictionary Stress 83.78% 89.81% 86.69% 67.72% 58.60% 62.83%
Perfect Memory, Reduced Stress 82.32% 85.81% 84.03% 39.18% 50.08% 43.97%
Prob. Memory, Dictionary Stress 84.05% 87.07% 85.54% 72.34% 30.01% 42.42%
Prob. Memory, Reduced Stress 84.85% 85.24% 85.05% 41.13% 22.91% 29.43%
</table>
<tableCaption confidence="0.7833465">
Table 1: Baseline and Learner Performance. Performance is reported after two iterations over the corpus
for probabilistic memory learners and after a single iteration for all other learners.
</tableCaption>
<bodyText confidence="0.99525">
small impact on utterance segmentation perfor-
mance. This suggests that the USC’s primary
value to the learner is in constraining the contents
of the lexicon and identifying words in isolation as
good candidates for the lexicon. In the Reduced
Stress condition where the USC is not directly re-
sponsible for any segmentations as there are no
adjacent primary-stressed syllables, the learner re-
lies much more heavily on subtractive techniques.
Table 2 gives the number of segmentations per-
formed using each segmentation operation. The
total number of segmentations is very similar be-
tween the Dictionary and Reduced Stress condi-
tions, but because USC Segmentation is not effec-
tive on Reduced Stress input, Initial and Final Sub-
traction are used much more heavily.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999994056603773">
The design of the segmenter presented here sug-
gests that both the quality of memory and the
structural purity of the input would be critical fac-
tors in the learner’s success. Our results suggest,
however, that using probabilistic memory and a
less idealized version of stress in natural language
have little impact on the performance of the pre-
sented learner. They do cause the learner to learn
much more slowly, causing the learner to need to
be presented with more material and resulting in
worse performance in the lexicon evaluation. But
this slower learning is unlikely to be a concern for
a child learner who would be exposed to much
larger amounts of data than the corpora here pro-
vide.
Cognitive literature suggests that limited mem-
ory during learning may be essential to a learner in
its early stages (Elman, 1993). But we do not see
any notable improvement as a result of the prob-
abilistic memory model used in our experiments,
although the learner does do better in the Reduced
Stress condition with Probabilistic Memory than
Perfect Memory. This should not be interpreted
as a negative result as we only analyze a single
learner and memory model. Adding decay to the
model such that among words of equal frequency
those that have not been used in segmentation re-
cently are less likely to be remembered may be
sufficient to create the desired effect.
The success of this learner suggests that the
type of “bootstrapping” approaches can succeed
in word segmentation. The learner presented uses
USC to identify utterances that are likely to be
lone words, seeding the lexicon with initial infor-
mation. Even if these first items in the lexicon are
of relatively low purity, often combining function
words and content words into one, the learner is
able to expand its lexicon by using these hypothe-
sized words to segment new input. As the learner
segments more, these hypotheses become more re-
liable, allowing the learner to build a lexicon of
good quality.
The subtraction approaches presented in this
work provide a basic algorithm for to handling
segmentation of incoming data in an online fash-
ion. The subtractive heuristics used here are of
course not guaranteed to result in a perfect seg-
mentation even with a perfect lexicon; they are
presented to show how a simple model of pro-
cessing incoming data can be paired with struc-
tural constraints on the hypothesis space to learn
word segmentation in a computationally efficient
and cognitively plausible online fashion.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999855333333333">
The learner’s strong performance using minimal
computational resources and unreliable memory
suggest that simple learners can succeed in un-
</bodyText>
<page confidence="0.996131">
95
</page>
<bodyText confidence="0.999972842105263">
supervised tasks as long as they take advantage
of domain-specific knowledge to constrain the hy-
pothesis space. Our results show that, even in ad-
versarial conditions, structural constraints remain
powerful tools for simple learning algorithms in
difficult tasks.
Future work in this area should focus on learn-
ers that can take advantage of the benefits of a
probabilistic lexicon and memory models suited
to them. Also, a more complex model of the type
of stress variation present in natural speech would
help better determine a learner that uses USC’s
ability to handle realistic variation in the input.
Our model of stress reduction is a worst-case sce-
nario for USC segmentation but is unlikely to be
an accurate model of real speech. Future work
should adopt a more naturalistic model to deter-
mine whether the robustness found in our results
holds true in more realistic stress permutations.
</bodyText>
<sectionHeader confidence="0.997601" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999006">
We thank Kyle Gorman, Josef Fruehwald, and
Dan Swingley for their helpful discussions regard-
ing this work. We are grateful to and Mitch Mar-
cus and Jana Beck for their feedback on earlier
versions of this paper.
</bodyText>
<sectionHeader confidence="0.999257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999653662162162">
J.R. Anderson, D. Bothell, C. Lebiere, and M. Matessa.
1998. An integrated theory of list memory. Journal
of Memory and Language, 38(4):341–380.
R. Bijeljac-Babic, J. Bertoncini, and J. Mehler. 1993.
How do 4-day-old infants categorize multisyllabic
utterances? Developmental Psychology, 29:711–
711.
M.R. Brent and T.A. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61(1-2):93–125.
M.R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1):71–105.
R. Brown. 1973. A First Language: The Early
Stages. Harvard Univ. Press, Cambridge, Mas-
sachusetts 02138.
J.L. Elman. 1993. Learning and development in neural
networks: The importance of starting small. Cogni-
tion, 48(1):71–99.
G. Gillund and R.M. Shiffrin. 1984. A retrieval model
for both recognition and recall. Psychological Re-
view, 91(1):1–67.
E.M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447–474.
S. Goldwater, T. Griffiths, and M. Johnson. 2006. In-
terpolating between types and tokens by estimating
power-law generators. Advances in Neural Informa-
tion Processing Systems, 18:459.
S. Goldwater, T.L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition.
M. Halle and J.R. Vergnaud. 1987. An essay on stress.
MIT Press.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 317–325. Association for
Computational Linguistics.
P.W. Jusczyk, D.M. Houston, and M. Newsome. 1999.
The Beginnings of Word Segmentation in English-
Learning Infants. Cognitive Psychology, 39(3-
4):159–207.
P.W. Jusczyk. 1999. How infants begin to extract
words from speech. Trends in Cognitive Sciences,
3(9):323–328.
M. Liberman and A. Prince. 1977. On stress and lin-
guistic rhythm. Linguistic Inquiry, 8(2):249–336.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates.
G.F. Marcus, S. Pinker, M. Ullman, M. Hollander, T.J.
Rosen, F. Xu, and H. Clahsen. 1992. Overregular-
ization in language acquisition. Monographs of the
Society for Research in Child Development, 57(4).
S.L. Mattys and P.W. Jusczyk. 2001. Phonotactic cues
for segmentation of fluent speech by infants. Cogni-
tion, 78(2):91–121.
K.H. Onishi, K.E. Chambers, and C. Fisher. 2002.
Learning phonotactic constraints from brief auditory
experience. Cognition, 83(1):B13–B23.
J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996a.
Statistical Learning by 8-month-old Infants. Sci-
ence, 274(5294):1926.
J.R. Saffran, E.L. Newport, and R.N. Aslin. 1996b.
Word Segmentation: The Role of Distributional
Cues. Journal of Memory and Language,
35(4):606–621.
D. Swingley. 2005. Statistical clustering and the con-
tents of the infant vocabulary. Cognitive Psychol-
ogy, 50(1):86–132.
LG Valiant. 1984. A theory of the learnable. Commu-
nications of the ACM, 27(11):1142.
</reference>
<page confidence="0.972293">
96
</page>
<reference confidence="0.999252166666667">
D. Van Kuijk and L. Boves. 1999. Acoustic char-
acteristics of lexical stress in continuous telephone
speech. Speech Communication, 27(2):95–111.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351–372.
R.L. Weide. 1998. The Carnegie Mellon Pronouncing
Dictionary [cmudict. 0.6].
C.D. Yang. 2004. Universal Grammar, statistics or
both? Trends in Cognitive Sciences, 8(10):451–456.
</reference>
<page confidence="0.999692">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.804084">
<title confidence="0.997989">Recession Segmentation: Simpler Online Word Segmentation</title>
<author confidence="0.98158">Constantine Lignos</author>
<author confidence="0.98158">Charles</author>
<affiliation confidence="0.997035">Dept. of Computer and Information Science, Dept. of University of</affiliation>
<abstract confidence="0.9911695">In this paper we present a cognitively plausible approach to word segmentation that segments in an online fashion using only local information and a lexicon of previously segmented words. Unlike popular statistical optimization techniques, the learner uses structural information of the input syllables rather than distributional cues to segment words. We develop a memory model for the learner that like a child learner does not recall previously hypothesized words perfectly. The learner attains an F-score of 86.69% in ideal conditions and 85.05% when word recall is unreliable and stress in the input is reduced. These results demonstrate the power that a simple learner can have when paired with appropriate structural constraints on its hypotheses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>D Bothell</author>
<author>C Lebiere</author>
<author>M Matessa</author>
</authors>
<title>An integrated theory of list memory.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="18427" citStr="Anderson et al., 1998" startWordPosition="2962" endWordPosition="2965">shold above which a word is more likely than not to be recalled, occurring at a count of approximately 14. 5 A Probabilistic Lexicon To simulate the imperfect memory of a child learner, we use a simple exponential function to generate the probability with which a word is retrieved from the lexicon: pr(word) = 1.0 − e−α,(,ord) pr(word) is the probability of a word being retrieved, α is a constant, and c(word) is the number of times the word has been identified in segmentations thus far. This type of memory function is a simplified representation of models of humans’ memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984). This memory function for the value of α = 0.05, the value used in our experiments, is given in Figure 1. We later show that the choice of α has little impact on the learner’s segmentation performance, and thus the more or less arbitrary selection of a value for α is of little consequence. When the algorithm attempts to subtract words from the beginning or end of an utterance, it may miss words in the lexicon due to this probabilistic retrieval. The learner only has one opportunity to recall a word in a given utterance. For example, in the utterance P.EH1.N S.AH0.</context>
</contexts>
<marker>Anderson, Bothell, Lebiere, Matessa, 1998</marker>
<rawString>J.R. Anderson, D. Bothell, C. Lebiere, and M. Matessa. 1998. An integrated theory of list memory. Journal of Memory and Language, 38(4):341–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bijeljac-Babic</author>
<author>J Bertoncini</author>
<author>J Mehler</author>
</authors>
<title>How do 4-day-old infants categorize multisyllabic utterances? Developmental Psychology,</title>
<date>1993</date>
<pages>29--711</pages>
<contexts>
<context position="6510" citStr="Bijeljac-Babic et al., 1993" startWordPosition="986" endWordPosition="989">n approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner. A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005). The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4- days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly. Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word. The belowbaseline performance is unsurprising given that in order for this type of model to posit a word boundary, a transitional probability between syllables must be lower than its neighbors. This condition c</context>
</contexts>
<marker>Bijeljac-Babic, Bertoncini, Mehler, 1993</marker>
<rawString>R. Bijeljac-Babic, J. Bertoncini, and J. Mehler. 1993. How do 4-day-old infants categorize multisyllabic utterances? Developmental Psychology, 29:711– 711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
<author>T A Cartwright</author>
</authors>
<title>Distributional regularity and phonotactic constraints are useful for segmentation.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="3994" citStr="Brent &amp; Cartwright (1996)" startWordPosition="596" endWordPosition="599">formation as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004). Previous computational models have demanded tremendous memory and computational capacity from human learners. For example, the algorithm 88 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88–97, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics of Brent &amp; Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible. It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation. Work in this tradition makes no claims, however, that these methods are actually the ones used by human learners. On the other hand, previous computational models often underesti</context>
</contexts>
<marker>Brent, Cartwright, 1996</marker>
<rawString>M.R. Brent and T.A. Cartwright. 1996. Distributional regularity and phonotactic constraints are useful for segmentation. Cognition, 61(1-2):93–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4725" citStr="Brent (1999)" startWordPosition="720" endWordPosition="721">tes until no further improvement is possible. It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation. Work in this tradition makes no claims, however, that these methods are actually the ones used by human learners. On the other hand, previous computational models often underestimate the human learner’s knowledge of linguistic representations. Most of these models are “synthetic” in the sense of Brent (1999): the raw material for segmentation is a stream of segments, which are then successively grouped into larger units and eventually, conjectured words. This assumption may make the child learner’s job unnecessarily hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is. For a given utterance, there are fewer syllables than segments, and hence fewer segmentation possibilities. Modeling the corpus using hierarchical grammars that can model the input at varying levels (word</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>M.R. Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
</authors>
<title>A First Language: The Early Stages.</title>
<date>1973</date>
<publisher>Harvard Univ. Press,</publisher>
<location>Cambridge, Massachusetts</location>
<contexts>
<context position="2659" citStr="Brown, 1973" startWordPosition="405" endWordPosition="406">thin statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009). These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes. But while these models deliver excellent performance, it is not clear how they inform the process of acquisition. Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning. Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not. A child’s competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate. Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as th</context>
<context position="20543" citStr="Brown (1973)" startWordPosition="3323" endWordPosition="3324">pproaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones. This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique. 6 Evaluation Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004). Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children’s data: Adam, Eve, and Sarah. We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word. In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress. For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T. We treat primary stress as “strong” and secondary or unstressed syllabl</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>R. Brown. 1973. A First Language: The Early Stages. Harvard Univ. Press, Cambridge, Massachusetts 02138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Learning and development in neural networks: The importance of starting small.</title>
<date>1993</date>
<journal>Cognition,</journal>
<volume>48</volume>
<issue>1</issue>
<contexts>
<context position="32044" citStr="Elman, 1993" startWordPosition="5193" endWordPosition="5194">sing probabilistic memory and a less idealized version of stress in natural language have little impact on the performance of the presented learner. They do cause the learner to learn much more slowly, causing the learner to need to be presented with more material and resulting in worse performance in the lexicon evaluation. But this slower learning is unlikely to be a concern for a child learner who would be exposed to much larger amounts of data than the corpora here provide. Cognitive literature suggests that limited memory during learning may be essential to a learner in its early stages (Elman, 1993). But we do not see any notable improvement as a result of the probabilistic memory model used in our experiments, although the learner does do better in the Reduced Stress condition with Probabilistic Memory than Perfect Memory. This should not be interpreted as a negative result as we only analyze a single learner and memory model. Adding decay to the model such that among words of equal frequency those that have not been used in segmentation recently are less likely to be remembered may be sufficient to create the desired effect. The success of this learner suggests that the type of “bootst</context>
</contexts>
<marker>Elman, 1993</marker>
<rawString>J.L. Elman. 1993. Learning and development in neural networks: The importance of starting small. Cognition, 48(1):71–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gillund</author>
<author>R M Shiffrin</author>
</authors>
<title>A retrieval model for both recognition and recall.</title>
<date>1984</date>
<journal>Psychological Review,</journal>
<volume>91</volume>
<issue>1</issue>
<contexts>
<context position="18456" citStr="Gillund and Shiffrin, 1984" startWordPosition="2966" endWordPosition="2969">d is more likely than not to be recalled, occurring at a count of approximately 14. 5 A Probabilistic Lexicon To simulate the imperfect memory of a child learner, we use a simple exponential function to generate the probability with which a word is retrieved from the lexicon: pr(word) = 1.0 − e−α,(,ord) pr(word) is the probability of a word being retrieved, α is a constant, and c(word) is the number of times the word has been identified in segmentations thus far. This type of memory function is a simplified representation of models of humans’ memory recall capabilities (Anderson et al., 1998; Gillund and Shiffrin, 1984). This memory function for the value of α = 0.05, the value used in our experiments, is given in Figure 1. We later show that the choice of α has little impact on the learner’s segmentation performance, and thus the more or less arbitrary selection of a value for α is of little consequence. When the algorithm attempts to subtract words from the beginning or end of an utterance, it may miss words in the lexicon due to this probabilistic retrieval. The learner only has one opportunity to recall a word in a given utterance. For example, in the utterance P.EH1.N S.AH0.L (pencil), if the learner ha</context>
</contexts>
<marker>Gillund, Shiffrin, 1984</marker>
<rawString>G. Gillund and R.M. Shiffrin. 1984. A retrieval model for both recognition and recall. Psychological Review, 91(1):1–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit. Information and control,</title>
<date>1967</date>
<pages>10--5</pages>
<contexts>
<context position="9193" citStr="Gold, 1967" startWordPosition="1408" endWordPosition="1409">n statistical optimization approaches by exploring how well a learner can perform while processing a corpus in an online fashion with only local information and a lexicon of previously segmented 89 words. We present a simple, efficient approach to word segmentation that uses structural information rather than distributional cues in the input to segment words. We seek to demonstrate that even in the face of impoverished input and limited resources, a simple learner can succeed when it operates with the appropriate constraints. 3 Constraining the Learning Space Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning. If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand. It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms. A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) bu</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E.M. Gold. 1967. Language identification in the limit. Information and control, 10(5):447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>18--459</pages>
<contexts>
<context position="20026" citStr="Goldwater et al., 2006" startWordPosition="3233" endWordPosition="3236">ff the longest word that is successfully recalled. While probabilistic memory means that the learner will fail to recognize words it has seen before, potentially decreasing recall, it also provides the learner the benefit of probabilistically failing to repeat previous mistakes if they occur rarely. Probabilistic word recall results in a “rich get richer” phenomenon as the learner segments; words that are used more often in segmentations are more likely to be reused in later segmentations. While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones. This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique. 6 Evaluation Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004). Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children’s data</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>S. Goldwater, T. Griffiths, and M. Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. Advances in Neural Information Processing Systems, 18:459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T L Griffiths</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition. M. Halle</journal>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2145" citStr="Goldwater et al., 2009" startWordPosition="322" endWordPosition="325">y Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner. We seek to determine how these limitations in the learner’s input and memory affect the learner’s performance and to demonstrate that the presented learner is robust even under non-ideal conditions. *Portions of this work were adapted from an earlier manuscript, Word Segmentation: Quick But Not Dirty. 2 Related Work Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009). These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes. But while these models deliver excellent performance, it is not clear how they inform the process of acquisition. Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning. Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not. A child’s compet</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>S. Goldwater, T.L. Griffiths, and M. Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition. M. Halle and J.R. Vergnaud. 1987. An essay on stress. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2175" citStr="Johnson and Goldwater, 2009" startWordPosition="326" endWordPosition="329">te it in idealized conditions and conditions that better approximate the environment of a child learner. We seek to determine how these limitations in the learner’s input and memory affect the learner’s performance and to demonstrate that the presented learner is robust even under non-ideal conditions. *Portions of this work were adapted from an earlier manuscript, Word Segmentation: Quick But Not Dirty. 2 Related Work Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et al., 2009; Johnson and Goldwater, 2009). These models have established the state-of-the-art for the task of selecting appropriate word boundaries from a stream of unstructured phonemes. But while these models deliver excellent performance, it is not clear how they inform the process of acquisition. Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning. Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not. A child’s competence grows gradually as she he</context>
<context position="5565" citStr="Johnson and Goldwater, 2009" startWordPosition="841" endWordPosition="844"> hard; since syllables are hierarchical structures consisting of segments, treating the linguistic data as unstructured segment sequences makes the problem harder than it actually is. For a given utterance, there are fewer syllables than segments, and hence fewer segmentation possibilities. Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009). While this results in state-of-the-art performance for segmentation performed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning. In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner. A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), ba</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>M. Johnson and S. Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
<author>D M Houston</author>
<author>M Newsome</author>
</authors>
<date>1999</date>
<booktitle>The Beginnings of Word Segmentation in EnglishLearning Infants. Cognitive Psychology,</booktitle>
<pages>39--3</pages>
<contexts>
<context position="13398" citStr="Jusczyk et al., 1999" startWordPosition="2086" endWordPosition="2089">nificant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999). For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable. These mechanisms are not addressed in this work; our focus is on an algorithm that can succeed given discrete stress information for each syllable. Given the evidence that infants can distinguish weak and strong syllables and use that in90 formation to detect word boundaries (Jusczyk et al., 1999), we believe that it is reasonable to assume that identifying syllabic stress is a task an infant learner can perform at the developmental stage of word segmentation. 4 A Simple Algorithm for Word Segmentation We now present a simple algebraic approach to word segmentation based on the constraints suggested by Yang (2004). The learner we present is algebraic in that it has a lexicon which stores previously segmented words and identifies the input as a combination of words already in the lexicon and novel words. No transitional probabilities or any distributional data are calculated from the in</context>
</contexts>
<marker>Jusczyk, Houston, Newsome, 1999</marker>
<rawString>P.W. Jusczyk, D.M. Houston, and M. Newsome. 1999. The Beginnings of Word Segmentation in EnglishLearning Infants. Cognitive Psychology, 39(3-4):159–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
</authors>
<title>How infants begin to extract words from speech.</title>
<date>1999</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>3</volume>
<issue>9</issue>
<contexts>
<context position="11777" citStr="Jusczyk, 1999" startWordPosition="1836" endWordPosition="1837">aluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities. The performance of the approach presented there improves dramatically if the learner is equipped with the assumption that each word can have only one primary stress. If the learner knows this, then it may limit the search for local minima to only the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence languageacquisition. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinctions (Jusczyk, 1999). Yang’s stress-delimited algorithm achieves the precision of 73.5% and recall of 71.2%, a significant improvement over using TPs alone, but still below the baseline presented in our results. The improvement of the transitional probability-based approach when provided with a simple linguistic constraint suggests that structural constraints can be powerful in narrowing the hypothesis space so that even sparse, local information can prove useful and simple segmentation strategies can become more effective. It should be noted that the classification of every syllable as “weak” or “strong” is a si</context>
</contexts>
<marker>Jusczyk, 1999</marker>
<rawString>P.W. Jusczyk. 1999. How infants begin to extract words from speech. Trends in Cognitive Sciences, 3(9):323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liberman</author>
<author>A Prince</author>
</authors>
<title>On stress and linguistic rhythm.</title>
<date>1977</date>
<journal>Linguistic Inquiry,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="12657" citStr="Liberman and Prince, 1977" startWordPosition="1965" endWordPosition="1968"> when provided with a simple linguistic constraint suggests that structural constraints can be powerful in narrowing the hypothesis space so that even sparse, local information can prove useful and simple segmentation strategies can become more effective. It should be noted that the classification of every syllable as “weak” or “strong” is a significant simplification. Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977). We later discuss a manipulation of the corpus used by Yang (2004) to address this concern. Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999). For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable. These mechanisms are not addressed in this work; our focus is on an algorithm that can succeed given discrete stress information for each syllable. Given the</context>
</contexts>
<marker>Liberman, Prince, 1977</marker>
<rawString>M. Liberman and A. Prince. 1977. On stress and linguistic rhythm. Linguistic Inquiry, 8(2):249–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates.</title>
<date>2000</date>
<contexts>
<context position="20589" citStr="MacWhinney, 2000" startWordPosition="3330" endWordPosition="3332"> generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones. This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique. 6 Evaluation Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004). Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children’s data: Adam, Eve, and Sarah. We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word. In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress. For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T. We treat primary stress as “strong” and secondary or unstressed syllables as “weak.” For each word, the phonetic segm</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>B. MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Marcus</author>
<author>S Pinker</author>
<author>M Ullman</author>
<author>M Hollander</author>
<author>T J Rosen</author>
<author>F Xu</author>
<author>H Clahsen</author>
</authors>
<title>Overregularization in language acquisition.</title>
<date>1992</date>
<journal>Monographs of the Society for Research in Child Development,</journal>
<volume>57</volume>
<issue>4</issue>
<contexts>
<context position="2896" citStr="Marcus et al., 1992" startWordPosition="439" endWordPosition="442">aries from a stream of unstructured phonemes. But while these models deliver excellent performance, it is not clear how they inform the process of acquisition. Trying to find cognitive insight from these types of models is difficult because of the inherent mismatch in the quality and types of hypotheses they maintain during learning. Children are incremental learners (Brown, 1973), and learners relying on statistical optimization are generally not. A child’s competence grows gradually as she hears and produces more and more utterances, going through predictable changes to her working grammar (Marcus et al., 1992) that statistical optimization techniques typically do not go through and do not intend to replicate. Statistical models provide excellent information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves. Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are n</context>
</contexts>
<marker>Marcus, Pinker, Ullman, Hollander, Rosen, Xu, Clahsen, 1992</marker>
<rawString>G.F. Marcus, S. Pinker, M. Ullman, M. Hollander, T.J. Rosen, F. Xu, and H. Clahsen. 1992. Overregularization in language acquisition. Monographs of the Society for Research in Child Development, 57(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Mattys</author>
<author>P W Jusczyk</author>
</authors>
<title>Phonotactic cues for segmentation of fluent speech by infants.</title>
<date>2001</date>
<journal>Cognition,</journal>
<volume>78</volume>
<issue>2</issue>
<contexts>
<context position="22080" citStr="Mattys and Jusczyk, 2001" startWordPosition="3570" endWordPosition="3573">f Recall 92 it conforms to the phonotactic constraints of English. For example, Einstein is AY1.N.S.T.AY0.N as segments and parsed into AY1.N S.T.AY0.N as syllables: this is because /st/ is the longest valid onset for the second syllable containing AY0 while /nst/ is longer but violates English phonotactics. While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words. 9- month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002). Finally, spaces and punctuation between words were removed, but the boundaries between utterances–as indicated by line breaks in CHILDES–are retained. Altogether, there are 226,178 words, consisting of 263,660 syllables. The learning material is a list of unsegmented syllable sequences grouped into utterances, and the learner’s task is to find word boundaries that group substrings of syllables together, building a lexicon of words as it segments. We evaluated the learner’s performance to address thes</context>
</contexts>
<marker>Mattys, Jusczyk, 2001</marker>
<rawString>S.L. Mattys and P.W. Jusczyk. 2001. Phonotactic cues for segmentation of fluent speech by infants. Cognition, 78(2):91–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Onishi</author>
<author>K E Chambers</author>
<author>C Fisher</author>
</authors>
<title>Learning phonotactic constraints from brief auditory experience.</title>
<date>2002</date>
<journal>Cognition,</journal>
<volume>83</volume>
<issue>1</issue>
<contexts>
<context position="22173" citStr="Onishi et al., 2002" startWordPosition="3585" endWordPosition="3588">S.T.AY0.N as segments and parsed into AY1.N S.T.AY0.N as syllables: this is because /st/ is the longest valid onset for the second syllable containing AY0 while /nst/ is longer but violates English phonotactics. While we performed syllabification as a preprocessing step outside of learning, a child learner would presumably learn the required phonotactics as a part of learning to segment words. 9- month old infants are believed to have learned some phonotactic constraints of their native language (Mattys and Jusczyk, 2001), and learning these constraints can be done with only minimal exposure (Onishi et al., 2002). Finally, spaces and punctuation between words were removed, but the boundaries between utterances–as indicated by line breaks in CHILDES–are retained. Altogether, there are 226,178 words, consisting of 263,660 syllables. The learning material is a list of unsegmented syllable sequences grouped into utterances, and the learner’s task is to find word boundaries that group substrings of syllables together, building a lexicon of words as it segments. We evaluated the learner’s performance to address these questions: • How does probabilistic memory affect learner performance? • How much does degr</context>
</contexts>
<marker>Onishi, Chambers, Fisher, 2002</marker>
<rawString>K.H. Onishi, K.E. Chambers, and C. Fisher. 2002. Learning phonotactic constraints from brief auditory experience. Cognition, 83(1):B13–B23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical Learning by 8-month-old Infants.</title>
<date>1996</date>
<journal>Science,</journal>
<volume>274</volume>
<issue>5294</issue>
<contexts>
<context position="6259" citStr="Saffran et al., 1996" startWordPosition="947" endWordPosition="950">ormed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning. In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner. A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005). The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4- days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly. Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseli</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996a. Statistical Learning by 8-month-old Infants. Science, 274(5294):1926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>E L Newport</author>
<author>R N Aslin</author>
</authors>
<title>Word Segmentation: The Role of Distributional Cues.</title>
<date>1996</date>
<journal>Journal of Memory and Language,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="6259" citStr="Saffran et al., 1996" startWordPosition="947" endWordPosition="950">ormed at the phoneme level, this approach requires significant computational resources as each additional level of representation increases the complexity of learning. In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner. A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005). The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4- days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly. Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseli</context>
</contexts>
<marker>Saffran, Newport, Aslin, 1996</marker>
<rawString>J.R. Saffran, E.L. Newport, and R.N. Aslin. 1996b. Word Segmentation: The Role of Distributional Cues. Journal of Memory and Language, 35(4):606–621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Swingley</author>
</authors>
<title>Statistical clustering and the contents of the infant vocabulary.</title>
<date>2005</date>
<journal>Cognitive Psychology,</journal>
<volume>50</volume>
<issue>1</issue>
<contexts>
<context position="6323" citStr="Swingley, 2005" startWordPosition="959" endWordPosition="960">ational resources as each additional level of representation increases the complexity of learning. In addition, it is not clear that some of the intermediate levels in such an approach, such as word level collocations which are not syntactic constituents, would have any linguistic or psychological reality to a human learner. A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005). The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4- days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly. Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word. The belowbaseline perfo</context>
</contexts>
<marker>Swingley, 2005</marker>
<rawString>D. Swingley. 2005. Statistical clustering and the contents of the infant vocabulary. Cognitive Psychology, 50(1):86–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LG Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<issue>11</issue>
<contexts>
<context position="9208" citStr="Valiant, 1984" startWordPosition="1410" endWordPosition="1411">l optimization approaches by exploring how well a learner can perform while processing a corpus in an online fashion with only local information and a lexicon of previously segmented 89 words. We present a simple, efficient approach to word segmentation that uses structural information rather than distributional cues in the input to segment words. We seek to demonstrate that even in the face of impoverished input and limited resources, a simple learner can succeed when it operates with the appropriate constraints. 3 Constraining the Learning Space Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning. If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand. It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms. A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously di</context>
</contexts>
<marker>Valiant, 1984</marker>
<rawString>LG Valiant. 1984. A theory of the learnable. Communications of the ACM, 27(11):1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Van Kuijk</author>
<author>L Boves</author>
</authors>
<title>Acoustic characteristics of lexical stress in continuous telephone speech.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>Van Kuijk, Boves, 1999</marker>
<rawString>D. Van Kuijk and L. Boves. 1999. Acoustic characteristics of lexical stress in continuous telephone speech. Speech Communication, 27(2):95–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>2000</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9223" citStr="Vapnik, 2000" startWordPosition="1412" endWordPosition="1413">approaches by exploring how well a learner can perform while processing a corpus in an online fashion with only local information and a lexicon of previously segmented 89 words. We present a simple, efficient approach to word segmentation that uses structural information rather than distributional cues in the input to segment words. We seek to demonstrate that even in the face of impoverished input and limited resources, a simple learner can succeed when it operates with the appropriate constraints. 3 Constraining the Learning Space Modern machine learning research (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning. If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand. It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms. A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Hall</context>
</contexts>
<marker>Vapnik, 2000</marker>
<rawString>V.N. Vapnik. 2000. The nature of statistical learning theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="7932" citStr="Venkataraman, 2001" startWordPosition="1211" endWordPosition="1212">ds used in infant studies measuring the ability to use transitional probability information are uniformly three-syllables long, much of child-directed English consists of sequences of monosyllabic words. Corpus statistics reveal that on average a monosyllabic word is followed by another monosyllabic word 85% of time (Yang, 2004), and thus learners that use only local transitional probabilities without any global optimization are unlikely to succeed. This problem does not affect online approaches that use global information, such as computing the maximum likelihood of the corpus incrementally (Venkataraman, 2001). Since these approaches do not require each boundary be a local minimum, they are able to correctly handle a sequence of monosyllable words. We believe that the computational modeling of psychological processes, with special attention to concrete mechanisms and quantitative evaluations, can play an important role in identifying the constraints and structures relevant to children’s acquisition of language. Rather than using a prior which guides the learner to a desired distribution, we examine learning with respect to a model in which the hypothesis space is constrained by structural requireme</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>A. Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3):351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Weide</author>
</authors>
<title>The Carnegie Mellon Pronouncing Dictionary</title>
<date>1998</date>
<note>[cmudict. 0.6].</note>
<contexts>
<context position="20782" citStr="Weide, 1998" startWordPosition="3360" endWordPosition="3361">s or generating new ones. This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique. 6 Evaluation Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004). Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children’s data: Adam, Eve, and Sarah. We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word. In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress. For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T. We treat primary stress as “strong” and secondary or unstressed syllables as “weak.” For each word, the phonetic segments were grouped into syllables. This process is straightforward by the use of the principle “Maximize Onset,” which maximizes the length of the onset as long as it is valid consonant cluster </context>
</contexts>
<marker>Weide, 1998</marker>
<rawString>R.L. Weide. 1998. The Carnegie Mellon Pronouncing Dictionary [cmudict. 0.6].</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Yang</author>
</authors>
<title>Universal Grammar, statistics or both? Trends</title>
<date>2004</date>
<booktitle>in Cognitive Sciences,</booktitle>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="1536" citStr="Yang (2004)" startWordPosition="230" endWordPosition="231">imple learner can have when paired with appropriate structural constraints on its hypotheses. 1 Introduction The problem of word segmentation presents an important challenge in language acquisition. The child learner must segment a continuous stream of sounds into words without knowing what the individual words are until the stream has been segmented. Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition. In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner. We seek to determine how these limitations in the learner’s input and memory affect the learner’s performance and to demonstrate that the presented learner is robust even under non-ideal conditions. *Portions of this work were adapted from an earlier manuscript, Word Segmentation: Quick But Not Dirty. 2 Related Work Most recent work in word segmentation of childdirected speech has operated within statistical optimization frameworks, particularly Bayesian approaches (Goldwater et </context>
<context position="3643" citStr="Yang, 2004" startWordPosition="553" endWordPosition="554">nt information about the features, distributional cues, and priors that can be used in learning, but provide little information about how a child learner can use this information and how her knowledge of language develops as the learning process evolves. Previous simulations in word segmentation using the same type of distributional information as many statistical optimization-based learners but without an optimization model suggest that statistics alone are not sufficient for learning to succeed in a computationally efficient online manner; further constraints on the search space are needed (Yang, 2004). Previous computational models have demanded tremendous memory and computational capacity from human learners. For example, the algorithm 88 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 88–97, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics of Brent &amp; Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible. It is unlikely that an algorithm of this type is something a human learner is capabl</context>
<context position="6645" citStr="Yang, 2004" startWordPosition="1007" endWordPosition="1008"> A number of psychologically-motivated models of word segmentation rely on the use of syllabic transitional probabilities (TPs), basing the use of TPs on experimental work in artificial language learning (Saffran et al., 1996a; Saffran et al., 1996b) and in corpus studies (Swingley, 2005). The identification of the syllable as the basic unit of segmentation is supported research in experimental psychology using infants as young as 4- days-old (Bijeljac-Babic et al., 1993), but when syllable transitional probabilities are evaluated in online learning procedures that only use local information (Yang, 2004), the results are surprisingly poor, even under the assumption that the learner has already syllabified the input perfectly. Precision is 41.6%, and recall is 23.3%, which we will show is worse than a simple baseline of assuming every syllable is a word. The belowbaseline performance is unsurprising given that in order for this type of model to posit a word boundary, a transitional probability between syllables must be lower than its neighbors. This condition cannot be met if the input is a sequence of monosyllabic words for which a boundary must be postulated for every syllable; it is impossi</context>
<context position="9790" citStr="Yang (2004)" startWordPosition="1500" endWordPosition="1501">ch (Gold, 1967; Valiant, 1984; Vapnik, 2000) suggests that constraints on the learning space and the learning algorithm are essential for realistically efficient learning. If a domain-neutral learning model fails on a specific task where children succeed, it is likely that children are equipped with knowledge and constraints specific to the task at hand. It is important to identify such constraints to see to what extent they complement, or even replace, domain neutral learning mechanisms. A particularly useful constraint for word segmentation, introduced to the problem of word segmentation by Yang (2004) but previously discussed by Halle and Vergnaud (1987), is as follows: Unique Stress Constraint (USC): A word can bear at most one primary stress. A simple example of how adult learners might use the USC is upon hearing novel names or words. Taking Star Wars characters as an example, it is clear that chewbacca is one word but darthvader cannot be as the latter bears two primary stresses. The USC could give the learner many isolated words for free. If the learner hears an utterance that contains exactly one primary stress, it is likely it is a single word. Moreover, the segmentation for a multi</context>
<context position="11160" citStr="Yang (2004)" startWordPosition="1737" endWordPosition="1738">ble. A learner equipped with USC will immediately know that the sequence consists of three words: specifically, W1S1, S2, and S2W2. The USC can also constrain the use of other learning techniques. For example, the syllable consequence S1W1W2W3S2 cannot be segmented by USC alone, but it may still provide cues that facilitate the application of other segmentation strategies. For instance, the learner knows that the sequence consists of at least two words, as indicated by two strong syllables. Moreover, it also knows that in the window between S1 and S2 there must be one or more word boundaries. Yang (2004) evaluates the effectiveness of the USC in conjunction with a simple approach to using transitional probabilities. The performance of the approach presented there improves dramatically if the learner is equipped with the assumption that each word can have only one primary stress. If the learner knows this, then it may limit the search for local minima to only the window between two syllables that both bear primary stress, e.g., between the two a’s in the sequence languageacquisition. This assumption is plausible given that 7.5-month-old infants are sensitive to strong/weak prosodic distinction</context>
<context position="12724" citStr="Yang (2004)" startWordPosition="1980" endWordPosition="1981">aints can be powerful in narrowing the hypothesis space so that even sparse, local information can prove useful and simple segmentation strategies can become more effective. It should be noted that the classification of every syllable as “weak” or “strong” is a significant simplification. Stress is better organized into hierarchical patterns constructed on top of syllables that vary in relative prominence based on the domain of each level of the hierarchy, and generally languages avoid adjacent strong syllables (Liberman and Prince, 1977). We later discuss a manipulation of the corpus used by Yang (2004) to address this concern. Additionally, there are significant challenges in reconstructing stress from an acoustic signal (Van Kuijk and Boves, 1999). For a child learner to use the algorithm presented here, she would need to have mechanisms for detecting stress in the speech signal and categorizing the gradient stress in utterances into a discrete level for each syllable. These mechanisms are not addressed in this work; our focus is on an algorithm that can succeed given discrete stress information for each syllable. Given the evidence that infants can distinguish weak and strong syllables an</context>
<context position="20488" citStr="Yang (2004)" startWordPosition="3315" endWordPosition="3316">later segmentations. While recent work from Bayesian approaches has used a Dirichlet Process to generate these distributions (Goldwater et al., 2006), in this learner the reuse of frequent items in learning is a result of the memory model rather than an explicit process of reusing old outcomes or generating new ones. This growth is an inherent property of the cognitive model of memory used here rather than an externally imposed computational technique. 6 Evaluation Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004). Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children’s data: Adam, Eve, and Sarah. We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998), using the first pronunciation of each word. In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress. For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T. We treat primary</context>
</contexts>
<marker>Yang, 2004</marker>
<rawString>C.D. Yang. 2004. Universal Grammar, statistics or both? Trends in Cognitive Sciences, 8(10):451–456.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>