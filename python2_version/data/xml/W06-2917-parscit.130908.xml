<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000247">
<title confidence="0.995392">
Learning Auxiliary Fronting with Grammatical Inference
</title>
<author confidence="0.995217">
Alexander Clark
</author>
<affiliation confidence="0.9959305">
Department of Computer Science
Royal Holloway University of London
</affiliation>
<address confidence="0.856674">
Egham, Surrey TW20 0EX
</address>
<email confidence="0.996614">
alexc@cs.rhul.ac.uk
</email>
<note confidence="0.6676905">
R´emi Eyraud
EURISE
</note>
<address confidence="0.427515">
23, rue du Docteur Paul Michelon
42023 Saint-´Etienne Cedex 2
France
</address>
<email confidence="0.469326">
remi.eyraud@univ-st-etienne.fr
</email>
<sectionHeader confidence="0.983428" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974105263158">
We present a simple context-free gram-
matical inference algorithm, and prove
that it is capable of learning an inter-
esting subclass of context-free languages.
We also demonstrate that an implementa-
tion of this algorithm is capable of learn-
ing auxiliary fronting in polar interroga-
tives (AFIPI) in English. This has been
one of the most important test cases in
language acquisition over the last few
decades. We demonstrate that learning
can proceed even in the complete absence
of examples of particular constructions,
and thus that debates about the frequency
of occurrence of such constructions are ir-
relevant. We discuss the implications of
this on the type of innate learning biases
that must be hypothesized to explain first
language acquisition.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887295454545">
For some years, a particular set of examples has
been used to provide support for nativist theories
of first language acquisition (FLA). These exam-
ples, which hinge around auxiliary inversion in the
formation of questions in English, have been con-
sidered to provide a strong argument in favour of
the nativist claim: that FLA proceeds primarily
through innately specified domain specific mecha-
nisms or knowledge, rather than through the oper-
ation of general-purpose cognitive mechanisms. A
key point of empirical debate is the frequency of oc-
currence of the forms in question. If these are van-
ishingly rare, or non-existent in the primary linguis-
tic data, and yet children acquire the construction in
question, then the hypothesis that they have innate
knowledge would be supported. But this rests on the
assumption that examples of that specific construc-
tion are necessary for learning to proceed. In this
paper we show that this assumption is false: that this
particular construction can be learned without the
learner being exposed to any examples of that par-
ticular type. Our demonstration is primarily mathe-
matical/computational: we present a simple experi-
ment that demonstrates the applicability of this ap-
proach to this particular problem neatly, but the data
we use is not intended to be a realistic representation
of the primary linguistic data, nor is the particular
algorithm we use suitable for large scale grammar
induction.
We present a general purpose context-free gram-
matical algorithm that is provably correct under a
certain learning criterion. This algorithm incorpo-
rates no domain specific knowledge: it has no spe-
cific information about language; no knowledge of
X-bar schemas, no hidden sources of information to
reveal the structure. It operates purely on unanno-
tated strings of raw text. Obviously, as all learn-
ing algorithms do, it has an implicit learning bias.
This very simple algorithm has a particularly clear
bias, with a simple mathematical description, that al-
lows a remarkably simple characterisation of the set
of languages that it can learn. This algorithm does
not use a statistical learning paradigm that has to be
tested on large quantities of data. Rather it uses a
</bodyText>
<page confidence="0.979733">
125
</page>
<note confidence="0.865">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 125–132, New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999634666666667">
symbolic learning paradigm, that works efficiently
with very small quantities of data, while being very
sensitive to noise. We discuss this choice in some
depth below.
For reasons that were first pointed out by Chom-
sky (Chomsky, 1975, pages 129–137), algorithms
of this type are not capable of learning all of nat-
ural language. It turns out, however, that algorithms
based on this approach are sufficiently strong to
learn some key properties of language, such as the
correct rule for forming polar questions.
In the next section we shall describe the dispute
briefly; in the subsequent sections we will describe
the algorithm we use, and the experiments we have
performed.
</bodyText>
<sectionHeader confidence="0.976911" genericHeader="method">
2 The Dispute
</sectionHeader>
<bodyText confidence="0.987586681818182">
We will present the dispute in traditional terms,
though later we shall analyse some of the assump-
tions implicit in this description. In English, po-
lar interrogatives (yes/no questions) are formed by
fronting an auxiliary, and adding a dummy auxiliary
“do” if the main verb is not an auxiliary. For exam-
ple,
Example 1a The man is hungry.
Example 1b Is the man hungry?
When the subject NP has a relative clause that also
contains an auxiliary, the auxiliary that is moved is
not the auxiliary in the relative clause, but the one in
the main (matrix) clause.
Example 2a The man who is eating is hungry.
Example 2b Is the man who is eating hungry?
An alternative rule would be to move the first oc-
curring auxiliary, i.e. the one in the relative clause,
which would produce the form
Example 2c Is the man who eating is hungry?
In some sense, there is no reason that children
should favour the correct rule, rather than the in-
correct one, since they are both of similar com-
plexity and so on. Yet children do in fact, when
provided with the appropriate context, produce sen-
tences of the form of Example 2b, and rarely if ever
produce errors of the form Example 2c (Crain and
Nakayama, 1987). The problem is how to account
for this phenomenon.
Chomsky claimed first, that sentences of the type
in Example 2b are vanishingly rare in the linguis-
tic environment that children are exposed to, yet
when tested they unfailingly produce the correct
form rather than the incorrect Example 2c. This is
put forward as strong evidence in favour of innately
specified language specific knowledge: we shall re-
fer to this view as linguistic nativism.
In a special volume of the Linguistic Review, Pul-
lum and Scholz (Pullum and Scholz, 2002), showed
that in fact sentences of this type are not rare at all.
Much discussion ensued on this empirical question
and the consequences of this in the context of ar-
guments for linguistic nativism. These debates re-
volved around both the methodology employed in
the study, and also the consequences of such claims
for nativist theories. It is fair to say that in spite
of the strength of Pullum and Scholz’s arguments,
nativists remained completely unconvinced by the
overall argument.
(Reali and Christiansen, 2004) present a possible
solution to this problem. They claim that local statis-
tics, effectively n-grams, can be sufficient to indi-
cate to the learner which alternative should be pre-
ferred. However this argument has been carefully re-
butted by (Kam et al., 2005), who show that this ar-
gument relies purely on a phonological coincidence
in English. This is unsurprising since it is implausi-
ble that a flat, finite-state model should be powerful
enough to model a phenomenon that is clearly struc-
ture dependent in this way.
In this paper we argue that the discussion about
the rarity of sentences that exhibit this particular
structure is irrelevant: we show that simple gram-
matical inference algorithms can learn this property
even in the complete absence of sentences of this
particular type. Thus the issue as to how frequently
an infant child will see them is a moot point.
</bodyText>
<sectionHeader confidence="0.997211" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.997914428571429">
Context-free grammatical inference algorithms are
explored in two different communities: in gram-
matical inference and in NLP. The task in NLP is
normally taken to be one of recovering appropri-
ate annotations (Smith and Eisner, 2005) that nor-
mally represent constituent structure (strong learn-
ing), while in grammatical inference, researchers
</bodyText>
<page confidence="0.997771">
126
</page>
<bodyText confidence="0.9998854">
are more interested in merely identifying the lan-
guage (weak learning). In both communities, the
best performing algorithms that learn from raw posi-
tive data only 1, generally rely on some combination
of three heuristics: frequency, information theoretic
measures of constituency, and finally substitutabil-
ity. 2 The first rests on the observation that strings
of words generated by constituents are likely to oc-
cur more frequently than by chance. The second
heuristic looks for information theoretic measures
that may predict boundaries, such as drops in condi-
tional entropy. The third method which is the foun-
dation of the algorithm we use, is based on the distri-
butional analysis of Harris (Harris, 1954). This prin-
ciple has been appealed to by many researchers in
the field of grammatical inference, but these appeals
have normally been informal and heuristic (van Za-
anen, 2000).
In its crudest form we can define it as follows:
given two sentences “I saw a cat over there”, and “I
saw a dog over there” the learner will hypothesize
that “cat” and “dog” are similar, since they appear
in the same context “I saw a _ there”. Pairs of
sentences of this form can be taken as evidence that
two words, or strings of words are substitutable.
</bodyText>
<subsectionHeader confidence="0.994453">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.950699823529412">
We briefly define some notation.
An alphabet E is a finite nonempty set of sym-
bols called letters. A string w over E is a finite se-
quence w = a1a2 ... an of letters. Let |w |denote
the length of w. In the following, letters will be in-
dicated by a, b, c, ..., strings by u, v, ... , z, and the
empty string by A. Let E* be the set of all strings,
the free monoid generated by E. By a language we
mean any subset L ⊆ E*. The set of all substrings
of a language L is denoted Sub(L) = {u ∈ E+ :
∃l, r, lur ∈ L} (notice that the empty word does not
belong to Sub(L)). We shall assume an order ≺ or
� on E which we shall extend to E* in the normal
way by saying that u ≺ v if |u |&lt; |v |or |u |= |v|
and u is lexicographically before v.
A grammar is a quadruple G = hV, E, P, Si
where E is a finite alphabet of terminal symbols, V
</bodyText>
<footnote confidence="0.98895475">
1We do not consider in this paper the complex and con-
tentious issues around negative data.
2For completeness we should include lexical dependencies
or attraction.
</footnote>
<bodyText confidence="0.960205470588235">
is a finite alphabet of variables or non-terminals, P
is a finite set of production rules, and S ∈ V is a
start symbol.
If P ⊆ V × (E ∪ V )+ then the grammar is said to
be context-free (CF), and we will write the produc-
tions as T → w.
We will write uTv ⇒ uwv when T → w ∈ P.
⇒ is the reflexive and transitive closure of ⇒.
*
In general, the definition of a class L relies on
a class R of abstract machines, here called rep-
resentations, together with a function L from rep-
resentations to languages, that characterize all and
only the languages of L: (1) ∀R ∈ R, L(R) ∈ L
and (2) ∀L ∈ L, ∃R ∈ R such that L(R) = L.
Two representations R1 and R2 are equivalent iff
L(R1) = L(R2).
</bodyText>
<subsectionHeader confidence="0.998622">
3.2 Learning
</subsectionHeader>
<bodyText confidence="0.989890047619048">
We now define our learning criterion. This is identi-
fication in the limit from positive text (Gold, 1967),
with polynomial bounds on data and computation,
but not on errors of prediction (de la Higuera, 1997).
A learning algorithm A for a class of represen-
tations R, is an algorithm that computes a function
from a finite sequence of strings s1, ... , sn to R. We
define a presentation of a language L to be an infinite
sequence of elements of L such that every element
of L occurs at least once. Given a presentation, we
can consider the sequence of hypotheses that the al-
gorithm produces, writing Rn = A(s1, ... sn) for
the nth such hypothesis.
The algorithm A is said to identify the class R in
the limit if for every R ∈ R, for every presentation
of L(R), there is an N such that for all n &gt; N,
Rn = RN and L(R) = L(RN).
We further require that the algorithm needs only
polynomially bounded amounts of data and compu-
tation. We use the slightly weaker notion defined by
de la Higuera (de la Higuera, 1997).
</bodyText>
<listItem confidence="0.860597857142857">
Definition A representation class R is identifiable
in the limit from positive data with polynomial time
and data iff there exist two polynomials p(), q() and
an algorithm A such that S ⊆ L(R)
1. Given a positive sample S of size m A returns
a representation R ∈ R in time p(m), such that
2. For each representation R of size n there exists
</listItem>
<page confidence="0.991385">
127
</page>
<bodyText confidence="0.997733666666667">
a characteristic set CS of size less than q(n)
such that if CS C_ S, A returns a representation
R&apos; such that L(R) = L(R&apos;).
</bodyText>
<subsectionHeader confidence="0.998668">
3.3 Distributional learning
</subsectionHeader>
<bodyText confidence="0.994539459770115">
The key to the Harris approach for learning a lan-
guage L, is to look at pairs of strings u and v and to
see whether they occur in the same contexts; that is
to say, to look for pairs of strings of the form lur and
lvr that are both in L. This can be taken as evidence
that there is a non-terminal symbol that generates
both strings. In the informal descriptions of this that
appear in Harris’s work, there is an ambiguity be-
tween two ideas. The first is that they should appear
in all the same contexts; and the second is that they
should appear in some of the same contexts. We can
write the first criterion as follows:
bl, r lur E L if and only if lvr E L (1)
This has also been known in language theory by the
name syntactic congruence, and can be written u =L
v.
The second, weaker, criterion is
1l, r lur E L and lvr E L (2)
We call this weak substitutability and write it as
u =L v. Clearly u =L v implies u .
. =L v when u is
a substring of the language. Any two strings that do
not occur as substrings of the language are obviously
syntactically congruent but not weakly substitutable.
First of all, observe that syntactic congruence is a
purely language theoretic notion that makes no ref-
erence to the grammatical representation of the lan-
guage, but only to the set of strings that occur in
it. However there is an obvious problem: syntac-
tic congruence tells us something very useful about
the language, but all we can observe is weak substi-
tutability.
When working within a Gold-style identification
in the limit (IIL) paradigm, we cannot rely on statis-
tical properties of the input sample, since they will
in general not be generated by random draws from a
fixed distribution. This, as is well known, severely
limits the class of languages that can be learned un-
der this paradigm. However, the comparative sim-
plicity of the IIL paradigm in the form when there
are polynomial constraints on size of characteristic
sets and computation(de la Higuera, 1997) makes it
a suitable starting point for analysis.
Given these restrictions, one solution to this prob-
lem is simply to define a class of languages where
substitutability implies congruence. We call these
the substitutable languages: A language L is substi-
tutable if and only if for every pair of strings u, v,
u =L v implies u =L v. This rather radical so-
lution clearly rules out the syntax of natural lan-
guages, at least if we consider them as strings of
raw words, rather than as strings of lexical or syn-
tactic categories. Lexical ambiguity alone violates
this requirement: consider the sentences “The rose
died”, “The cat died” and “The cat rose from its bas-
ket”. A more serious problem is pairs of sentences
like “John is hungry” and “John is running”, where
it is not ambiguity in the syntactic category of the
word that causes the problem, but rather ambigu-
ity in the context. Using this assumption, whether
it is true or false, we can then construct a simple
algorithm for grammatical inference, based purely
on the idea that whenever we find a pair of strings
that are weakly substitutable, we can generalise the
hypothesized language so that they are syntactically
congruent.
The algorithm proceeds by constructing a graph
where every substring in the sample defines a node.
An arc is drawn between two nodes if and only if
the two nodes are weakly substitutable with respect
to the sample, i.e. there is an arc between u and v if
and only if we have observed in the sample strings
of the form lur and lvr. Clearly all of the strings in
the sample will form a clique in this graph (consider
when l and r are both empty strings). The connected
components of this graph can be computed in time
polynomial in the total size of the sample. If the
language is substitutable then each of these compo-
nents will correspond to a congruence class of the
language.
There are two ways of doing this: one way, which
is perhaps the purest involves defining a reduction
system or semi-Thue system which directly captures
this generalisation process. The second way, which
we present here, will be more familiar to computa-
tional linguists, and involves constructing a gram-
mar.
</bodyText>
<page confidence="0.99018">
128
</page>
<subsectionHeader confidence="0.866857">
3.4 Grammar construction
</subsectionHeader>
<bodyText confidence="0.987975666666667">
Simply knowing the syntactic congruence might not
appear to be enough to learn a context-free gram-
mar, but in fact it is. In fact given the syntactic con-
gruence, and a sample of the language, we can sim-
ply write down a grammar in Chomsky normal form,
and under quite weak assumptions this grammar will
converge to a correct grammar for the language.
This construction relies on a simple property of
the syntactic congruence, namely that is in fact a
congruence: i.e.,
u ≡L v implies ∀l, r lur ≡L lvr
We define the syntactic monoid to be the quo-
tient of the monoid E∗/ ≡L. The monoid operation
[u][v] = [uv] is well defined since if u ≡L u0 and
v ≡L v0 then uv ≡L u0v0.
We can construct a grammar in the following triv-
ial way, from a sample of strings where we are given
the syntactic congruence.
</bodyText>
<listItem confidence="0.995342222222222">
• The non-terminals of the grammar are iden-
tified with the congruence classes of the lan-
guage.
• For any string w = uv , we add a production
[w] → [u][v].
• For all strings a of length one (i.e. letters of E),
we add productions of the form [a] → a.
• The start symbol is the congruence class which
contains all the strings of the language.
</listItem>
<bodyText confidence="0.9999145">
This defines a grammar in CNF. At first sight, this
construction might appear to be completely vacu-
ous, and not to define any strings beyond those in
the sample. The situation where it generalises is
when two different strings are congruent: if uv =
w ≡ w0 = u0v0 then we will have two different rules
[w] → [u][v] and [w] → [u0][v0], since [w] is the
same non-terminal as [w0].
A striking feature of this algorithm is that it makes
no attempt to identify which of these congruence
classes correspond to non-terminals in the target
grammar. Indeed that is to some extent an ill-posed
question. There are many different ways of assign-
ing constituent structure to sentences, and indeed
some reputable theories of syntax, such as depen-
dency grammars, dispense with the notion of con-
stituent structure all together. De facto standards,
such as the Penn treebank annotations are a some-
what arbitrary compromise among many different
possible analyses. This algorithm instead relies on
the syntactic monoid, which expresses the combina-
torial structure of the language in its purest form.
</bodyText>
<subsectionHeader confidence="0.8596">
3.5 Proof
</subsectionHeader>
<bodyText confidence="0.998502540540541">
We will now present our main result, with an outline
proof. For a full proof the reader is referred to (Clark
and Eyraud, 2005).
Theorem 1 This algorithm polynomially identi-
fies in the limit the class of substitutable context-free
languages.
Proof (Sketch) We can assume without loss of
generality that the target grammar is in Chomsky
normal form. We first define a characteristic set, that
is to say a set of strings such that whenever the sam-
ple includes the characteristic set, the algorithm will
output a correct grammar.
We define w(α) ∈ E∗ to be the smallest word,
according to ≺, generated by α ∈ (E ∪ V )+. For
each non-terminal N ∈ V define c(N) to be the
smallest pair of terminal strings (l, r) (extending ≺
from E∗ to E∗ × E∗, in some way), such that S ∗ ⇒
lNr.
We can now define the characteristic set CS =
{lwr|(N → α) ∈ P, (l, r) = c(N), w = w(α)}.
The cardinality of this set is at most |P |which
is clearly polynomially bounded. We observe that
the computations involved can all be polynomially
bounded in the total size of the sample.
We next show that whenever the algorithm en-
counters a sample that includes this characteristic
set, it outputs the right grammar. We write G� for
the learned grammar. Suppose [u] ⇒∗G v. Then
we can see that u ≡L v by induction on the max-
imum length of the derivation of v. At each step
we must use some rule [u0] ⇒ [v0][w0]. It is easy
to see that every rule of this type preserves the syn-
tactic congruence of the left and right sides of the
rules. Intuitively, the algorithm will never generate
too large a language, since the languages are sub-
stitutable. Conversely, if we have a derivation of a
string u with respect to the target grammar G, by
</bodyText>
<page confidence="0.997408">
129
</page>
<bodyText confidence="0.9999431">
construction of the characteristic set, we will have,
for every production L → MN in the target gram-
mar, a production in the hypothesized grammar of
the form [w(L)] → [w(M)][w(N)], and for every
production of the form L → a we have a produc-
tion [w(L)] → a. A simple recursive argument
shows that the hypothesized grammar will generate
all the strings in the target language. Thus the gram-
mar will generate all and only the strings required
(QED).
</bodyText>
<sectionHeader confidence="0.764254" genericHeader="method">
3.6 Related work
</sectionHeader>
<bodyText confidence="0.9999495">
This is the first provably correct and efficient gram-
matical inference algorithm for a linguistically in-
teresting class of context-free grammars (but see for
example (Yokomori, 2003) on the class of very sim-
ple grammars). It can also be compared to An-
gluin’s famous work on reversible grammars (An-
gluin, 1982) which inspired a similar paper(Pilato
and Berwick, 1985).
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99993825">
We decided to see whether this algorithm without
modification could shed some light on the debate
discussed above. The experiments we present here
are not intended to be an exhaustive test of the learn-
ability of natural language. The focus is on deter-
mining whether learning can proceed in the absence
of positive samples, and given only a very weak gen-
eral purpose bias.
</bodyText>
<subsectionHeader confidence="0.991335">
4.1 Implementation
</subsectionHeader>
<bodyText confidence="0.984664666666667">
We have implemented the algorithm described
above. There are a number of algorithmic issues
that were addressed. First, in order to find which
pairs of strings are substitutable, the naive approach
would be to compare strings pairwise which would
be quadratic in the number of sentences. A more
efficient approach maintains a hashtable mapping
from contexts to congruence classes. Caching hash-
codes, and using a union-find algorithm for merging
classes allows an algorithm that is effectively linear
in the number of sentences.
In order to handle large data sets with thousands
of sentences, it was necessary to modify the al-
gorithm in various ways which slightly altered its
formal properties. However for the experiments
reported here we used a version which performs
the man who is hungry died.
the man ordered dinner.
the man died.
the man is hungry.
is the man hungry ?
the man is ordering dinner.
is the man who is hungry ordering dinner ?
∗is the man who hungry is ordering dinner ?
</bodyText>
<tableCaption confidence="0.860441">
Table 1: Auxiliary fronting data set. Examples
</tableCaption>
<bodyText confidence="0.7997692">
above the line were presented to the algorithm dur-
ing the training phase, and it was tested on examples
below the line.
exactly in line with the mathematical description
above.
</bodyText>
<subsectionHeader confidence="0.97533">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999964633333333">
For clarity of exposition, we have used extremely
small artificial data-sets, consisting only of sen-
tences of types that would indubitably occur in the
linguistic experience of a child.
Our first experiments were intended to determine
whether the algorithm could determine the correct
form of a polar question when the noun phrase had a
relative clause, even when the algorithm was not ex-
posed to any examples of that sort of sentence. We
accordingly prepared a small data set shown in Ta-
ble 1. Above the line is the training data that the
algorithm was trained on. It was then tested on all of
the sentences, including the ones below the line. By
construction the algorithm would generate all sen-
tences it has already seen, so it scores correctly on
those. The learned grammar also correctly generated
the correct form and did not generate the final form.
We can see how this happens quite easily since the
simple nature of the algorithm allows a straightfor-
ward analysis. We can see that in the learned gram-
mar “the man” will be congruent to “the man who
is hungry”, since there is a pair of sentences which
differ only by this. Similarly, “hungry” will be con-
gruent to “ordering dinner”. Thus the sentence “is
the man hungry ?” which is in the language, will be
congruent to the correct sentence.
One of the derivations for this sentence would be:
[is the man hungry ?] → [is the man hungry] [?] →
[is the man] [hungry] [?] → [is] [the man] [hungry]
[?] → [is] [the man][who is hungry] [hungry] [?] →
</bodyText>
<page confidence="0.991725">
130
</page>
<bodyText confidence="0.8546403">
it rains
it may rain
it may have rained
it may be raining
it has rained
it has been raining
it is raining
it may have been raining
*it may have been rained
*it may been have rain
*it may have been rain
Table 2: English auxiliary data. Training data above
the line, and testing data below.
[is] [the man][who is hungry] [ordering dinner] [?].
Our second data set is shown in Table 2, and is a
fragment of the English auxiliary system. This has
also been claimed to be evidence in favour of na-
tivism. This was discussed in some detail by (Pilato
and Berwick, 1985). Again the algorithm correctly
learns.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991971830986">
Chomsky was among the first to point out the limi-
tations of Harris’s approach, and it is certainly true
that the grammars produced from these toy exam-
ples overgenerate radically. On more realistic lan-
guage samples this algorithm would eventually start
to generate even the incorrect forms of polar ques-
tions.
Given the solution we propose it is worth look-
ing again and examining why nativists have felt that
AFIPI was such an important issue. It appears that
there are several different areas. First, the debate
has always focussed on how to construct the inter-
rogative from the declarative form. The problem
has been cast as finding which auxilary should be
“moved”. Implicit in this is the assumption that the
interrogative structure must be defined with refer-
ence to the declarative, one of the central assump-
tions of traditional transformational grammar. Now,
of course, given our knowledge of many differ-
ent formalisms which can correctly generate these
forms without movement we can see that this as-
sumption is false. There is of course a relation be-
tween these two sentences, a semantic one, but this
does not imply that there need be any particular syn-
tactic relation, and certainly not a “generative” rela-
tion.
Secondly, the view of learning algorithms is very
narrow. It is considered that only sentences of that
exact type could be relevant. We have demonstrated,
if nothing else, that that view is false. The distinction
can be learnt from a set of data that does not include
any example of the exact piece of data required: as
long as the various parts can be learned separately,
the combination will function in the natural way.
A more interesting question is the extent to which
the biases implicit in the learning algorithm are do-
main specific. Clearly the algorithm has a strong
bias. It overgeneralises massively. One of the advan-
tages of the algorithm for the purposes of this paper
is that its triviality allows a remarkably clear and ex-
plicit statement of its bias. But is this bias specific to
the domain of language? It in no way refers to any-
thing specific to the field of language, still less spe-
cific to human language – no references to parts of
speech, or phrases, or even hierarchical phrase struc-
ture. It is now widely recognised that this sort of re-
cursive structure is domain-general (Jackendoff and
Pinker, 2005).
We have selected for this demonstration an algo-
rithm from grammatical inference. A number of sta-
tistical models have been proposed over the last few
years by researchers such as (Klein and Manning,
2002; Klein and Manning, 2004) and (Solan et al.,
2005). These models impressively manage to ex-
tract significant structure from raw data. However,
for our purposes, neither of these models is suitable.
Klein and Manning’s model uses a variety of differ-
ent cues, which combine with some specific initial-
isation and smoothing, and an explicit constraint to
produce binary branching trees. Though very im-
pressive, the model is replete with domain-specific
biases and assumptions. Moreover, it does not learn
a language in the strict sense (a subset of the set of
all strings), though it would be a simple modification
to make it perform such a task. The model by Solan
et al. would be more suitable for this task, but again
the complexity of the algorithm, which has numer-
ous components and heuristics, and the lack of a the-
oretical justification for these heuristics again makes
the task of identifying exactly what these biases are,
and more importantly how domain specific they are,
</bodyText>
<page confidence="0.995449">
131
</page>
<bodyText confidence="0.9941976875">
a very significant problem.
In this model, the bias of the algorithm is com-
pletely encapsulated in the assumption u = v im-
plies u - v. It is worth pointing out that this does
not even need hierarchical structure – the model
could be implemented purely as a reduction system
or semi-Thue system. The disadvantage of using
that approach is that it is possible to construct some
bizarre examples where the number of reductions
can be exponential.
Using statistical properties of the set of strings,
it is possible to extend these learnability results to
a more substantial class of context free languages,
though it is unlikely that these methods could be ex-
tended to a class that properly contains all natural
languages.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999806071428571">
We have presented an analysis of the argument that
the acquisition of auxiliary fronting in polar inter-
rogatives supports linguistic nativism. Using a very
simple algorithm based on the ideas of Zellig Har-
ris, with a simple domain-general heuristic, we show
that the empirical question as to the frequency of oc-
currence of polar questions of a certain type in child-
directed speech is a moot point, since the distinction
in question can be learned even when no such sen-
tences occur.
Acknowledgements This work has been partially
supported by the EU funded PASCAL Network of
Excellence on Pattern Analysis, Statistical Mod-
elling and Computational Learning.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999716065573771">
D. Angluin. 1982. Inference of reversible languages.
Communications of the ACM, 29:741–765.
Noam Chomsky. 1975. The Logical Structure of Lin-
guistic Theory. University of Chicago Press.
Alexander Clark and Remi Eyraud. 2005. Identification
in the limit of substitutable context free languages. In
Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita,
editors, Proceedings of The 16th International Confer-
ence on Algorithmic Learning Theory, pages 283–296.
Springer-Verlag.
S. Crain and M. Nakayama. 1987. Structure dependence
in grammar formation. Language, 63(522-543).
C. de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
(27):125–138. Kluwer Academic Publishers. Manu-
factured in Netherland.
E. M. Gold. 1967. Language indentification in the limit.
Information and control, 10(5):447 – 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146–62.
Ray Jackendoff and Steven Pinker. 2005. The nature of
the language faculty and its implications for the evolu-
tion of language. Cognition, 97:211–225.
X. N. C. Kam, I. Stoyneshka, L. Tornyova, J. D. Fodor,
and W. G. Sakas. 2005. Non-robustness of syntax
acquisition from n-grams: A cross-linguistic perspec-
tive. In The 18th Annual CUNY Sentence Processing
Conference, April.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Dan Klein and Chris Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Proceedings of the 42nd Annual
Meeting of the ACL.
Samuel F. Pilato and Robert C. Berwick. 1985. Re-
versible automata and induction of the english auxil-
iary system. In Proceedings of the ACL, pages 70–75.
Geoffrey K. Pullum and Barbara C. Scholz. 2002. Em-
pirical assessment of stimulus poverty arguments. The
Linguistic Review, 19(1-2):9–50.
Florencia Reali and Morten H. Christiansen. 2004.
Structure dependence in language acquisition: Uncov-
ering the statistical richness of the stimulus. In Pro-
ceedings of the 26th Annual Conference of the Cogni-
tive Science Society, Mahwah, NJ. Lawrence Erlbaum.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 354–
362, Ann Arbor, Michigan, June.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proc. Natl. Acad. Sci., 102:11629–11634.
Menno van Zaanen. 2000. ABL: Alignment-based learn-
ing. In COLING 2000 - Proceedings of the 18th Inter-
national Conference on Computational Linguistics.
Takashi Yokomori. 2003. Polynomial-time identification
of very simple grammars from positive data. Theoret-
ical Computer Science, 298(1):179–206.
</reference>
<page confidence="0.997724">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.351394">
<title confidence="0.999861">Learning Auxiliary Fronting with Grammatical Inference</title>
<author confidence="0.998429">Alexander</author>
<affiliation confidence="0.949704">Department of Computer Royal Holloway University of</affiliation>
<address confidence="0.941318">Egham, Surrey TW20</address>
<email confidence="0.994354">alexc@cs.rhul.ac.uk</email>
<affiliation confidence="0.436906">R´emi</affiliation>
<address confidence="0.9180195">23, rue du Docteur Paul 42023 Saint-´Etienne Cedex</address>
<email confidence="0.979462">remi.eyraud@univ-st-etienne.fr</email>
<abstract confidence="0.99898315">We present a simple context-free grammatical inference algorithm, and prove that it is capable of learning an interesting subclass of context-free languages. We also demonstrate that an implementation of this algorithm is capable of learning auxiliary fronting in polar interrogatives (AFIPI) in English. This has been one of the most important test cases in language acquisition over the last few decades. We demonstrate that learning can proceed even in the complete absence of examples of particular constructions, and thus that debates about the frequency of occurrence of such constructions are irrelevant. We discuss the implications of this on the type of innate learning biases that must be hypothesized to explain first language acquisition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Inference of reversible languages.</title>
<date>1982</date>
<journal>Communications of the ACM,</journal>
<pages>29--741</pages>
<contexts>
<context position="20927" citStr="Angluin, 1982" startWordPosition="3696" endWordPosition="3698">w(L)] → [w(M)][w(N)], and for every production of the form L → a we have a production [w(L)] → a. A simple recursive argument shows that the hypothesized grammar will generate all the strings in the target language. Thus the grammar will generate all and only the strings required (QED). 3.6 Related work This is the first provably correct and efficient grammatical inference algorithm for a linguistically interesting class of context-free grammars (but see for example (Yokomori, 2003) on the class of very simple grammars). It can also be compared to Angluin’s famous work on reversible grammars (Angluin, 1982) which inspired a similar paper(Pilato and Berwick, 1985). 4 Experiments We decided to see whether this algorithm without modification could shed some light on the debate discussed above. The experiments we present here are not intended to be an exhaustive test of the learnability of natural language. The focus is on determining whether learning can proceed in the absence of positive samples, and given only a very weak general purpose bias. 4.1 Implementation We have implemented the algorithm described above. There are a number of algorithmic issues that were addressed. First, in order to find</context>
</contexts>
<marker>Angluin, 1982</marker>
<rawString>D. Angluin. 1982. Inference of reversible languages. Communications of the ACM, 29:741–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>The Logical Structure of Linguistic Theory.</title>
<date>1975</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="3698" citStr="Chomsky, 1975" startWordPosition="573" endWordPosition="574">y simple characterisation of the set of languages that it can learn. This algorithm does not use a statistical learning paradigm that has to be tested on large quantities of data. Rather it uses a 125 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 125–132, New York City, June 2006. c�2006 Association for Computational Linguistics symbolic learning paradigm, that works efficiently with very small quantities of data, while being very sensitive to noise. We discuss this choice in some depth below. For reasons that were first pointed out by Chomsky (Chomsky, 1975, pages 129–137), algorithms of this type are not capable of learning all of natural language. It turns out, however, that algorithms based on this approach are sufficiently strong to learn some key properties of language, such as the correct rule for forming polar questions. In the next section we shall describe the dispute briefly; in the subsequent sections we will describe the algorithm we use, and the experiments we have performed. 2 The Dispute We will present the dispute in traditional terms, though later we shall analyse some of the assumptions implicit in this description. In English,</context>
</contexts>
<marker>Chomsky, 1975</marker>
<rawString>Noam Chomsky. 1975. The Logical Structure of Linguistic Theory. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Remi Eyraud</author>
</authors>
<title>Identification in the limit of substitutable context free languages.</title>
<date>2005</date>
<booktitle>Proceedings of The 16th International Conference on Algorithmic Learning Theory,</booktitle>
<pages>283--296</pages>
<editor>In Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="18575" citStr="Clark and Eyraud, 2005" startWordPosition="3264" endWordPosition="3267">here are many different ways of assigning constituent structure to sentences, and indeed some reputable theories of syntax, such as dependency grammars, dispense with the notion of constituent structure all together. De facto standards, such as the Penn treebank annotations are a somewhat arbitrary compromise among many different possible analyses. This algorithm instead relies on the syntactic monoid, which expresses the combinatorial structure of the language in its purest form. 3.5 Proof We will now present our main result, with an outline proof. For a full proof the reader is referred to (Clark and Eyraud, 2005). Theorem 1 This algorithm polynomially identifies in the limit the class of substitutable context-free languages. Proof (Sketch) We can assume without loss of generality that the target grammar is in Chomsky normal form. We first define a characteristic set, that is to say a set of strings such that whenever the sample includes the characteristic set, the algorithm will output a correct grammar. We define w(α) ∈ E∗ to be the smallest word, according to ≺, generated by α ∈ (E ∪ V )+. For each non-terminal N ∈ V define c(N) to be the smallest pair of terminal strings (l, r) (extending ≺ from E∗</context>
</contexts>
<marker>Clark, Eyraud, 2005</marker>
<rawString>Alexander Clark and Remi Eyraud. 2005. Identification in the limit of substitutable context free languages. In Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita, editors, Proceedings of The 16th International Conference on Algorithmic Learning Theory, pages 283–296. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Crain</author>
<author>M Nakayama</author>
</authors>
<title>Structure dependence in grammar formation.</title>
<date>1987</date>
<journal>Language,</journal>
<pages>63--522</pages>
<contexts>
<context position="5337" citStr="Crain and Nakayama, 1987" startWordPosition="861" endWordPosition="864">ample 2a The man who is eating is hungry. Example 2b Is the man who is eating hungry? An alternative rule would be to move the first occurring auxiliary, i.e. the one in the relative clause, which would produce the form Example 2c Is the man who eating is hungry? In some sense, there is no reason that children should favour the correct rule, rather than the incorrect one, since they are both of similar complexity and so on. Yet children do in fact, when provided with the appropriate context, produce sentences of the form of Example 2b, and rarely if ever produce errors of the form Example 2c (Crain and Nakayama, 1987). The problem is how to account for this phenomenon. Chomsky claimed first, that sentences of the type in Example 2b are vanishingly rare in the linguistic environment that children are exposed to, yet when tested they unfailingly produce the correct form rather than the incorrect Example 2c. This is put forward as strong evidence in favour of innately specified language specific knowledge: we shall refer to this view as linguistic nativism. In a special volume of the Linguistic Review, Pullum and Scholz (Pullum and Scholz, 2002), showed that in fact sentences of this type are not rare at all.</context>
</contexts>
<marker>Crain, Nakayama, 1987</marker>
<rawString>S. Crain and M. Nakayama. 1987. Structure dependence in grammar formation. Language, 63(522-543).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de la Higuera</author>
</authors>
<title>Characteristic sets for polynomial grammatical inference.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>27--125</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<note>Manufactured in Netherland.</note>
<contexts>
<context position="10758" citStr="Higuera, 1997" startWordPosition="1839" endWordPosition="1840">reflexive and transitive closure of ⇒. * In general, the definition of a class L relies on a class R of abstract machines, here called representations, together with a function L from representations to languages, that characterize all and only the languages of L: (1) ∀R ∈ R, L(R) ∈ L and (2) ∀L ∈ L, ∃R ∈ R such that L(R) = L. Two representations R1 and R2 are equivalent iff L(R1) = L(R2). 3.2 Learning We now define our learning criterion. This is identification in the limit from positive text (Gold, 1967), with polynomial bounds on data and computation, but not on errors of prediction (de la Higuera, 1997). A learning algorithm A for a class of representations R, is an algorithm that computes a function from a finite sequence of strings s1, ... , sn to R. We define a presentation of a language L to be an infinite sequence of elements of L such that every element of L occurs at least once. Given a presentation, we can consider the sequence of hypotheses that the algorithm produces, writing Rn = A(s1, ... sn) for the nth such hypothesis. The algorithm A is said to identify the class R in the limit if for every R ∈ R, for every presentation of L(R), there is an N such that for all n &gt; N, Rn = RN a</context>
<context position="14021" citStr="Higuera, 1997" startWordPosition="2455" endWordPosition="2456">congruence tells us something very useful about the language, but all we can observe is weak substitutability. When working within a Gold-style identification in the limit (IIL) paradigm, we cannot rely on statistical properties of the input sample, since they will in general not be generated by random draws from a fixed distribution. This, as is well known, severely limits the class of languages that can be learned under this paradigm. However, the comparative simplicity of the IIL paradigm in the form when there are polynomial constraints on size of characteristic sets and computation(de la Higuera, 1997) makes it a suitable starting point for analysis. Given these restrictions, one solution to this problem is simply to define a class of languages where substitutability implies congruence. We call these the substitutable languages: A language L is substitutable if and only if for every pair of strings u, v, u =L v implies u =L v. This rather radical solution clearly rules out the syntax of natural languages, at least if we consider them as strings of raw words, rather than as strings of lexical or syntactic categories. Lexical ambiguity alone violates this requirement: consider the sentences “</context>
</contexts>
<marker>Higuera, 1997</marker>
<rawString>C. de la Higuera. 1997. Characteristic sets for polynomial grammatical inference. Machine Learning, (27):125–138. Kluwer Academic Publishers. Manufactured in Netherland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language indentification in the limit.</title>
<date>1967</date>
<journal>Information and control,</journal>
<volume>10</volume>
<issue>5</issue>
<pages>474</pages>
<contexts>
<context position="10655" citStr="Gold, 1967" startWordPosition="1822" endWordPosition="1823"> (CF), and we will write the productions as T → w. We will write uTv ⇒ uwv when T → w ∈ P. ⇒ is the reflexive and transitive closure of ⇒. * In general, the definition of a class L relies on a class R of abstract machines, here called representations, together with a function L from representations to languages, that characterize all and only the languages of L: (1) ∀R ∈ R, L(R) ∈ L and (2) ∀L ∈ L, ∃R ∈ R such that L(R) = L. Two representations R1 and R2 are equivalent iff L(R1) = L(R2). 3.2 Learning We now define our learning criterion. This is identification in the limit from positive text (Gold, 1967), with polynomial bounds on data and computation, but not on errors of prediction (de la Higuera, 1997). A learning algorithm A for a class of representations R, is an algorithm that computes a function from a finite sequence of strings s1, ... , sn to R. We define a presentation of a language L to be an infinite sequence of elements of L such that every element of L occurs at least once. Given a presentation, we can consider the sequence of hypotheses that the algorithm produces, writing Rn = A(s1, ... sn) for the nth such hypothesis. The algorithm A is said to identify the class R in the lim</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. M. Gold. 1967. Language indentification in the limit. Information and control, 10(5):447 – 474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="8322" citStr="Harris, 1954" startWordPosition="1348" endWordPosition="1349">he best performing algorithms that learn from raw positive data only 1, generally rely on some combination of three heuristics: frequency, information theoretic measures of constituency, and finally substitutability. 2 The first rests on the observation that strings of words generated by constituents are likely to occur more frequently than by chance. The second heuristic looks for information theoretic measures that may predict boundaries, such as drops in conditional entropy. The third method which is the foundation of the algorithm we use, is based on the distributional analysis of Harris (Harris, 1954). This principle has been appealed to by many researchers in the field of grammatical inference, but these appeals have normally been informal and heuristic (van Zaanen, 2000). In its crudest form we can define it as follows: given two sentences “I saw a cat over there”, and “I saw a dog over there” the learner will hypothesize that “cat” and “dog” are similar, since they appear in the same context “I saw a _ there”. Pairs of sentences of this form can be taken as evidence that two words, or strings of words are substitutable. 3.1 Preliminaries We briefly define some notation. An alphabet E is</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(2-3):146–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
<author>Steven Pinker</author>
</authors>
<title>The nature of the language faculty and its implications for the evolution of language.</title>
<date>2005</date>
<journal>Cognition,</journal>
<pages>97--211</pages>
<contexts>
<context position="27082" citStr="Jackendoff and Pinker, 2005" startWordPosition="4762" endWordPosition="4765">the learning algorithm are domain specific. Clearly the algorithm has a strong bias. It overgeneralises massively. One of the advantages of the algorithm for the purposes of this paper is that its triviality allows a remarkably clear and explicit statement of its bias. But is this bias specific to the domain of language? It in no way refers to anything specific to the field of language, still less specific to human language – no references to parts of speech, or phrases, or even hierarchical phrase structure. It is now widely recognised that this sort of recursive structure is domain-general (Jackendoff and Pinker, 2005). We have selected for this demonstration an algorithm from grammatical inference. A number of statistical models have been proposed over the last few years by researchers such as (Klein and Manning, 2002; Klein and Manning, 2004) and (Solan et al., 2005). These models impressively manage to extract significant structure from raw data. However, for our purposes, neither of these models is suitable. Klein and Manning’s model uses a variety of different cues, which combine with some specific initialisation and smoothing, and an explicit constraint to produce binary branching trees. Though very i</context>
</contexts>
<marker>Jackendoff, Pinker, 2005</marker>
<rawString>Ray Jackendoff and Steven Pinker. 2005. The nature of the language faculty and its implications for the evolution of language. Cognition, 97:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X N C Kam</author>
<author>I Stoyneshka</author>
<author>L Tornyova</author>
<author>J D Fodor</author>
<author>W G Sakas</author>
</authors>
<title>Non-robustness of syntax acquisition from n-grams: A cross-linguistic perspective.</title>
<date>2005</date>
<booktitle>In The 18th Annual CUNY Sentence Processing Conference,</booktitle>
<contexts>
<context position="6643" citStr="Kam et al., 2005" startWordPosition="1077" endWordPosition="1080">ntext of arguments for linguistic nativism. These debates revolved around both the methodology employed in the study, and also the consequences of such claims for nativist theories. It is fair to say that in spite of the strength of Pullum and Scholz’s arguments, nativists remained completely unconvinced by the overall argument. (Reali and Christiansen, 2004) present a possible solution to this problem. They claim that local statistics, effectively n-grams, can be sufficient to indicate to the learner which alternative should be preferred. However this argument has been carefully rebutted by (Kam et al., 2005), who show that this argument relies purely on a phonological coincidence in English. This is unsurprising since it is implausible that a flat, finite-state model should be powerful enough to model a phenomenon that is clearly structure dependent in this way. In this paper we argue that the discussion about the rarity of sentences that exhibit this particular structure is irrelevant: we show that simple grammatical inference algorithms can learn this property even in the complete absence of sentences of this particular type. Thus the issue as to how frequently an infant child will see them is </context>
</contexts>
<marker>Kam, Stoyneshka, Tornyova, Fodor, Sakas, 2005</marker>
<rawString>X. N. C. Kam, I. Stoyneshka, L. Tornyova, J. D. Fodor, and W. G. Sakas. 2005. Non-robustness of syntax acquisition from n-grams: A cross-linguistic perspective. In The 18th Annual CUNY Sentence Processing Conference, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="27286" citStr="Klein and Manning, 2002" startWordPosition="4796" endWordPosition="4799">llows a remarkably clear and explicit statement of its bias. But is this bias specific to the domain of language? It in no way refers to anything specific to the field of language, still less specific to human language – no references to parts of speech, or phrases, or even hierarchical phrase structure. It is now widely recognised that this sort of recursive structure is domain-general (Jackendoff and Pinker, 2005). We have selected for this demonstration an algorithm from grammatical inference. A number of statistical models have been proposed over the last few years by researchers such as (Klein and Manning, 2002; Klein and Manning, 2004) and (Solan et al., 2005). These models impressively manage to extract significant structure from raw data. However, for our purposes, neither of these models is suitable. Klein and Manning’s model uses a variety of different cues, which combine with some specific initialisation and smoothing, and an explicit constraint to produce binary branching trees. Though very impressive, the model is replete with domain-specific biases and assumptions. Moreover, it does not learn a language in the strict sense (a subset of the set of all strings), though it would be a simple mo</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of the 40th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="27312" citStr="Klein and Manning, 2004" startWordPosition="4800" endWordPosition="4803">and explicit statement of its bias. But is this bias specific to the domain of language? It in no way refers to anything specific to the field of language, still less specific to human language – no references to parts of speech, or phrases, or even hierarchical phrase structure. It is now widely recognised that this sort of recursive structure is domain-general (Jackendoff and Pinker, 2005). We have selected for this demonstration an algorithm from grammatical inference. A number of statistical models have been proposed over the last few years by researchers such as (Klein and Manning, 2002; Klein and Manning, 2004) and (Solan et al., 2005). These models impressively manage to extract significant structure from raw data. However, for our purposes, neither of these models is suitable. Klein and Manning’s model uses a variety of different cues, which combine with some specific initialisation and smoothing, and an explicit constraint to produce binary branching trees. Though very impressive, the model is replete with domain-specific biases and assumptions. Moreover, it does not learn a language in the strict sense (a subset of the set of all strings), though it would be a simple modification to make it perf</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Chris Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel F Pilato</author>
<author>Robert C Berwick</author>
</authors>
<title>Reversible automata and induction of the english auxiliary system.</title>
<date>1985</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>70--75</pages>
<contexts>
<context position="20984" citStr="Pilato and Berwick, 1985" startWordPosition="3703" endWordPosition="3706"> of the form L → a we have a production [w(L)] → a. A simple recursive argument shows that the hypothesized grammar will generate all the strings in the target language. Thus the grammar will generate all and only the strings required (QED). 3.6 Related work This is the first provably correct and efficient grammatical inference algorithm for a linguistically interesting class of context-free grammars (but see for example (Yokomori, 2003) on the class of very simple grammars). It can also be compared to Angluin’s famous work on reversible grammars (Angluin, 1982) which inspired a similar paper(Pilato and Berwick, 1985). 4 Experiments We decided to see whether this algorithm without modification could shed some light on the debate discussed above. The experiments we present here are not intended to be an exhaustive test of the learnability of natural language. The focus is on determining whether learning can proceed in the absence of positive samples, and given only a very weak general purpose bias. 4.1 Implementation We have implemented the algorithm described above. There are a number of algorithmic issues that were addressed. First, in order to find which pairs of strings are substitutable, the naive appr</context>
<context position="24684" citStr="Pilato and Berwick, 1985" startWordPosition="4354" endWordPosition="4357">hungry] [?] → [is] [the man][who is hungry] [hungry] [?] → 130 it rains it may rain it may have rained it may be raining it has rained it has been raining it is raining it may have been raining *it may have been rained *it may been have rain *it may have been rain Table 2: English auxiliary data. Training data above the line, and testing data below. [is] [the man][who is hungry] [ordering dinner] [?]. Our second data set is shown in Table 2, and is a fragment of the English auxiliary system. This has also been claimed to be evidence in favour of nativism. This was discussed in some detail by (Pilato and Berwick, 1985). Again the algorithm correctly learns. 5 Discussion Chomsky was among the first to point out the limitations of Harris’s approach, and it is certainly true that the grammars produced from these toy examples overgenerate radically. On more realistic language samples this algorithm would eventually start to generate even the incorrect forms of polar questions. Given the solution we propose it is worth looking again and examining why nativists have felt that AFIPI was such an important issue. It appears that there are several different areas. First, the debate has always focussed on how to const</context>
</contexts>
<marker>Pilato, Berwick, 1985</marker>
<rawString>Samuel F. Pilato and Robert C. Berwick. 1985. Reversible automata and induction of the english auxiliary system. In Proceedings of the ACL, pages 70–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
<author>Barbara C Scholz</author>
</authors>
<title>Empirical assessment of stimulus poverty arguments. The Linguistic Review,</title>
<date>2002</date>
<pages>19--1</pages>
<contexts>
<context position="5872" citStr="Pullum and Scholz, 2002" startWordPosition="950" endWordPosition="953">e 2b, and rarely if ever produce errors of the form Example 2c (Crain and Nakayama, 1987). The problem is how to account for this phenomenon. Chomsky claimed first, that sentences of the type in Example 2b are vanishingly rare in the linguistic environment that children are exposed to, yet when tested they unfailingly produce the correct form rather than the incorrect Example 2c. This is put forward as strong evidence in favour of innately specified language specific knowledge: we shall refer to this view as linguistic nativism. In a special volume of the Linguistic Review, Pullum and Scholz (Pullum and Scholz, 2002), showed that in fact sentences of this type are not rare at all. Much discussion ensued on this empirical question and the consequences of this in the context of arguments for linguistic nativism. These debates revolved around both the methodology employed in the study, and also the consequences of such claims for nativist theories. It is fair to say that in spite of the strength of Pullum and Scholz’s arguments, nativists remained completely unconvinced by the overall argument. (Reali and Christiansen, 2004) present a possible solution to this problem. They claim that local statistics, effec</context>
</contexts>
<marker>Pullum, Scholz, 2002</marker>
<rawString>Geoffrey K. Pullum and Barbara C. Scholz. 2002. Empirical assessment of stimulus poverty arguments. The Linguistic Review, 19(1-2):9–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florencia Reali</author>
<author>Morten H Christiansen</author>
</authors>
<title>Structure dependence in language acquisition: Uncovering the statistical richness of the stimulus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Conference of the Cognitive Science Society,</booktitle>
<location>Mahwah, NJ. Lawrence Erlbaum.</location>
<contexts>
<context position="6387" citStr="Reali and Christiansen, 2004" startWordPosition="1034" endWordPosition="1037"> as linguistic nativism. In a special volume of the Linguistic Review, Pullum and Scholz (Pullum and Scholz, 2002), showed that in fact sentences of this type are not rare at all. Much discussion ensued on this empirical question and the consequences of this in the context of arguments for linguistic nativism. These debates revolved around both the methodology employed in the study, and also the consequences of such claims for nativist theories. It is fair to say that in spite of the strength of Pullum and Scholz’s arguments, nativists remained completely unconvinced by the overall argument. (Reali and Christiansen, 2004) present a possible solution to this problem. They claim that local statistics, effectively n-grams, can be sufficient to indicate to the learner which alternative should be preferred. However this argument has been carefully rebutted by (Kam et al., 2005), who show that this argument relies purely on a phonological coincidence in English. This is unsurprising since it is implausible that a flat, finite-state model should be powerful enough to model a phenomenon that is clearly structure dependent in this way. In this paper we argue that the discussion about the rarity of sentences that exhibi</context>
</contexts>
<marker>Reali, Christiansen, 2004</marker>
<rawString>Florencia Reali and Morten H. Christiansen. 2004. Structure dependence in language acquisition: Uncovering the statistical richness of the stimulus. In Proceedings of the 26th Annual Conference of the Cognitive Science Society, Mahwah, NJ. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>354--362</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="7501" citStr="Smith and Eisner, 2005" startWordPosition="1219" endWordPosition="1222">e dependent in this way. In this paper we argue that the discussion about the rarity of sentences that exhibit this particular structure is irrelevant: we show that simple grammatical inference algorithms can learn this property even in the complete absence of sentences of this particular type. Thus the issue as to how frequently an infant child will see them is a moot point. 3 Algorithm Context-free grammatical inference algorithms are explored in two different communities: in grammatical inference and in NLP. The task in NLP is normally taken to be one of recovering appropriate annotations (Smith and Eisner, 2005) that normally represent constituent structure (strong learning), while in grammatical inference, researchers 126 are more interested in merely identifying the language (weak learning). In both communities, the best performing algorithms that learn from raw positive data only 1, generally rely on some combination of three heuristics: frequency, information theoretic measures of constituency, and finally substitutability. 2 The first rests on the observation that strings of words generated by constituents are likely to occur more frequently than by chance. The second heuristic looks for informa</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 354– 362, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zach Solan</author>
<author>David Horn</author>
<author>Eytan Ruppin</author>
<author>Shimon Edelman</author>
</authors>
<title>Unsupervised learning of natural languages.</title>
<date>2005</date>
<booktitle>Proc. Natl. Acad. Sci.,</booktitle>
<pages>102--11629</pages>
<contexts>
<context position="27337" citStr="Solan et al., 2005" startWordPosition="4805" endWordPosition="4808">bias. But is this bias specific to the domain of language? It in no way refers to anything specific to the field of language, still less specific to human language – no references to parts of speech, or phrases, or even hierarchical phrase structure. It is now widely recognised that this sort of recursive structure is domain-general (Jackendoff and Pinker, 2005). We have selected for this demonstration an algorithm from grammatical inference. A number of statistical models have been proposed over the last few years by researchers such as (Klein and Manning, 2002; Klein and Manning, 2004) and (Solan et al., 2005). These models impressively manage to extract significant structure from raw data. However, for our purposes, neither of these models is suitable. Klein and Manning’s model uses a variety of different cues, which combine with some specific initialisation and smoothing, and an explicit constraint to produce binary branching trees. Though very impressive, the model is replete with domain-specific biases and assumptions. Moreover, it does not learn a language in the strict sense (a subset of the set of all strings), though it would be a simple modification to make it perform such a task. The mode</context>
</contexts>
<marker>Solan, Horn, Ruppin, Edelman, 2005</marker>
<rawString>Zach Solan, David Horn, Eytan Ruppin, and Shimon Edelman. 2005. Unsupervised learning of natural languages. Proc. Natl. Acad. Sci., 102:11629–11634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>ABL: Alignment-based learning.</title>
<date>2000</date>
<booktitle>In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics.</booktitle>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. ABL: Alignment-based learning. In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Yokomori</author>
</authors>
<title>Polynomial-time identification of very simple grammars from positive data.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<volume>298</volume>
<issue>1</issue>
<contexts>
<context position="20800" citStr="Yokomori, 2003" startWordPosition="3673" endWordPosition="3674">tic set, we will have, for every production L → MN in the target grammar, a production in the hypothesized grammar of the form [w(L)] → [w(M)][w(N)], and for every production of the form L → a we have a production [w(L)] → a. A simple recursive argument shows that the hypothesized grammar will generate all the strings in the target language. Thus the grammar will generate all and only the strings required (QED). 3.6 Related work This is the first provably correct and efficient grammatical inference algorithm for a linguistically interesting class of context-free grammars (but see for example (Yokomori, 2003) on the class of very simple grammars). It can also be compared to Angluin’s famous work on reversible grammars (Angluin, 1982) which inspired a similar paper(Pilato and Berwick, 1985). 4 Experiments We decided to see whether this algorithm without modification could shed some light on the debate discussed above. The experiments we present here are not intended to be an exhaustive test of the learnability of natural language. The focus is on determining whether learning can proceed in the absence of positive samples, and given only a very weak general purpose bias. 4.1 Implementation We have i</context>
</contexts>
<marker>Yokomori, 2003</marker>
<rawString>Takashi Yokomori. 2003. Polynomial-time identification of very simple grammars from positive data. Theoretical Computer Science, 298(1):179–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>