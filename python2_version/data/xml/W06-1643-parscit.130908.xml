<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000341">
<title confidence="0.9996065">
A Skip-Chain Conditional Random Field for
Ranking Meeting Utterances by Importance*
</title>
<author confidence="0.996104">
Michel Galley
</author>
<affiliation confidence="0.99909">
Columbia University
Department of Computer Science
</affiliation>
<address confidence="0.987813">
New York, NY 10027, USA
</address>
<email confidence="0.999583">
galley@cs.columbia.edu
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776285714286">
We describe a probabilistic approach to content se-
lection for meeting summarization. We use skip-
chain Conditional Random Fields (CRF) to model
non-local pragmatic dependencies between paired
utterances such as QUESTION-ANSWER that typi-
cally appear together in summaries, and show that
these models outperform linear-chain CRFs and
Bayesian models in the task. We also discuss dif-
ferent approaches for ranking all utterances in a se-
quence using CRFs. Our best performing system
achieves 91.3% of human performance when evalu-
ated with the Pyramid evaluation metric, which rep-
resents a 3.9% absolute increase compared to our
most competitive non-sequential classifier.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963626984126984">
Summarization of meetings faces many challenges
not found in texts, i.e., high word error rates, ab-
sence of punctuation, and sometimes lack of gram-
maticality and coherent ordering. On the other
hand, meetings present a rich source of structural
and pragmatic information that makes summariza-
tion of multi-party speech quite unique. In par-
ticular, our analyses of patterns in the verbal ex-
change between participants found that adjacency
pairs (AP), a concept drawn from the conver-
sational analysis literature (Schegloff and Sacks,
1973), have particular relevance to summarization.
APs are pairs of utterances such as QUESTION-
ANSWER or OFFER-ACCEPT, in which the second
utterance is said to be conditionally relevant on the
first. We show that there is a strong correlation be-
tween the two elements of an AP in summariza-
tion, and that one is unlikely to be included if the
other element is not present in the summary.
Most current statistical sequence models in nat-
ural language processing (NLP), such as hidden
*This material is based on research supported in part by
the U.S. National Science Foundation (NSF) under Grants
No. IIS-0121396 and IIS-05-34871, and the Defense Ad-
vanced Research Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023. Any opinions, findings and con-
clusions or recommendations expressed in this material are
those of the author and do not necessarily reflect the views of
the NSF or DARPA.
Markov models (HMMs) (Rabiner, 1989), are lin-
ear chains that only encode local dependencies
between utterances to be labeled. In multi-party
speech, the two elements of an AP are gener-
ally arbitrarily distant, and such models can only
poorly account for dependencies underlying APs
in summarization. We use instead skip-chain se-
quence models (Sutton and McCallum, 2004),
which allow us to explicitly model dependencies
between distant utterances, and turn out to be par-
ticularly effective in the summarization task.
In this paper, we compare two types of network
structures—linear-chain and skip-chain—and two
types of network semantics—Bayesian Networks
(BNs) and Conditional Random Fields (CRFs).
We discuss the problem of estimating the class
posterior probability of each utterance in a se-
quence in order to extract the N most proba-
ble ones, and show that the cost assigned by a
CRF to each utterance needs to be locally nor-
malized in order to outperform BNs. After ana-
lyzing the predictive power of a large set of dura-
tional, acoustical, lexical, structural, and informa-
tion retrieval features, we perform feature selec-
tion to have a competitive set of predictors to test
the different models. Empirical evaluations using
two standard summarization metrics—the Pyra-
mid method (Nenkova and Passonneau, 2004b)
and ROUGE (Lin, 2004)—show that the best
performing system is a CRF incorporating both
order-2 Markov dependencies and skip-chain de-
pendencies, which achieves 91.3% of human per-
formance in Pyramid score, and outperforms our
best-performing non-sequential model by 3.9%.
</bodyText>
<sectionHeader confidence="0.993884" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999755833333333">
The work presented here was applied to the ICSI
Meeting Corpus (Janin et al., 2003), a corpus
of “naturally-occurring” meetings, i.e. meetings
that would have taken place anyway. Their style
is quite informal, and topics are primarily con-
cerned with speech, natural language, artificial
</bodyText>
<page confidence="0.981623">
364
</page>
<note confidence="0.854074">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364–372,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999972921568628">
intelligence, and networking research. The cor-
pus contains 75 meetings, which are 60 minutes
long on average, and involve a number of partic-
ipants ranging from 3 to 10 (6 on average). The
total number of unique speakers is 60, includ-
ing 26 non-native English speakers. Experiments
in this paper are based either on human ortho-
graphic transcriptions or automatic speech recog-
nition output, which were available for all meet-
ings. For automatic recognition, we used the ICSI-
SRI-UW speech recognition system (Mirghafori
et al., 2004), a state-of-the-art conversational tele-
phone speech (CTS) recognizer whose language
and acoustic models were adapted to the meeting
domain. It achieves 34.8% WER on the ICSI cor-
pus, which is indicative of the difficulty involved
in processing meetings automatically.
We also used additional annotation that has
been developed to support higher-level analyses of
meeting structure, in particular the ICSI Meeting
Recorder Dialog act (MRDA) corpus (Shriberg et
al., 2004). Dialog act (DA) labels describe the
pragmatic function of utterances, e.g. a STATE-
MENT or a BACKCHANNEL. This auxiliary cor-
pus consists of over 180,000 human-annotated
dialog act labels (n = .8), for which so-called
adjacency pair (AP) relations (e.g., APOLOGY-
DOWNPLAY) were also labeled. This latter anno-
tation was used to train an AP classifier that is in-
strumental in automatically determining the struc-
ture of our sequence models. Note that, in the case
of three or more speakers, adjacency pair is ad-
mittedly an unfortunate term, since labeled APs
are generally not adjacent (e.g., see Table 1), but
we will nevertheless use the same terminology to
enforce consistency with previous work.
To train and evaluate our summarizer, we used
a corpus of extractive summaries produced at the
University of Edinburgh (Murray et al., 2005). For
each of the 75 meetings, human judges were asked
to select transcription utterances segmented by DA
to include in summaries, resulting in an average
compression ratio of 6.26% (though no strict limit
was imposed). Inter-labeler agreement was mea-
sured using six meetings that were summarized by
multiple coders (average n = .323). While this
level of agreement is quite low, this situation is
not uncommon to summarization, since there may
be many good summaries for a given document;
a main challenge lies in using evaluation schemes
that properly accounts for this diversity.
</bodyText>
<sectionHeader confidence="0.942831" genericHeader="method">
3 Content selection
</sectionHeader>
<bodyText confidence="0.99980062">
State sequence Markov models such as hidden
Markov models (Rabiner, 1989) have been highly
successful in many speech and natural language
processing applications, including summarization.
Following an intuition that the probability of a
given sentence may be locally conditioned on the
previous one, Conroy (2004) built a HMM-based
summarizer that consistently ranked among the
top systems in recent Document Understanding
Conference (DUC) evaluations.
Inter-sentential influences become more com-
plex in the case of dialogues or correspondences,
especially when they involve multiple parties.
In the case of summarization of conversational
speech, Zechner (2002) found, for instance, that
a simple technique consisting of linking together
questions and answers in summaries—and thus
preventing the selection of orphan questions or
answers—significantly improved their readability
according to various human summary evaluations.
In email summarization (Rambow et al., 2004),
Shrestha and McKeown (2004) obtained good per-
formance in automatic detection of questions and
answers, which can help produce summaries that
highlight or focus on the question and answer ex-
change. In a combined chat and email summariza-
tion task, a technique (Zhou and Hovy, 2005) con-
sisting of identifying APs and appending any rele-
vant responses to topic initiating messages was in-
strumental in outperforming two competitive sum-
marization baselines.
The need to model pragmatic influences, such
as between a question and an answer, is also preva-
lent in meeting summarization. In fact, question-
answer pairs are not the only discourse relations
that we need to preserve in order to create co-
herent summaries, and, as we will see, most in-
stances of APs would need to be preserved to-
gether, either inside or outside the summary. Ta-
ble 1 displays an AP construction with one state-
ment (A part) and three respondents (B parts).
This example illustrates that the number of turns
between constituents of APs is variable and thus
difficult to model with standard sequence models.
This example also illustrates some of the predic-
tors investigated in this paper. First, many speak-
ers respond to A’s utterance, which is generally a
strong indicator that the A utterance should be in-
cluded. Secondly, while APs are generally char-
acterized in terms of pre-defined dialog acts, such
</bodyText>
<page confidence="0.996057">
365
</page>
<figure confidence="0.785038142857143">
Time Speaker AP
1480.85-1493.91 1 A
1489.71-1489.94 2
1493.95-1495.41 3 B
1494.31-1495.83 2 B
1495.1-1497.07 4 B
Transcript
</figure>
<construct confidence="0.584936">
are - are those d- delays adjustable? see a lot of people who actually build stuff
with human computer interfaces understand that delay, and - and so when you -
by the time you click it it’ll be right on because it’ll go back in time to put the -
yeah.
yeah, uh, not in this case.
it could do that, couldn’t it.
we could program that pretty easily , couldn’t we?
</construct>
<tableCaption confidence="0.998579">
Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B). Sentences in
italic are not present in the reference summary.
</tableCaption>
<bodyText confidence="0.99997125">
as OFFER-ACCEPT, we found that the type of di-
alog act has much less importance than the ex-
istence of the AP connection itself (APs in the
data represent a great variety of DA pairs, includ-
ing many that are not characterized as APs in the
litterature—e.g., STATEMENT-STATEMENT in the
table). Since DAs seem to matter less than adja-
cency pairs, the aim will be to build techniques to
automatically identify such relations and exploit
them in utterance selection.
In the current work, we use skip-chain sequence
models (Sutton and McCallum, 2004) to repre-
sent dependencies between both contiguous ut-
terances and paired utterances appearing in the
same AP constructions. The graphical represen-
tations of skip-chain models, such as the CRF rep-
resented in Figure 1, are composed of two types of
edges: linear-chain and skip-chain edges. The lat-
ter edges model AP links, which we represent as
a set of (s, d) index pairs (note that no more than
one AP may share the same second element d).
The intuition that the summarization labels (−1
or 1) are highly correlated with APs is confirmed
in Table 2. While contiguous labels yt−1 and yt
seem to seldom influence each other, the correla-
tion between AP elements ys and yd is particularly
strong, and they have a tendency to be either both
included or both excluded. Note that the second
table is not symmetric, because the data allows an
A part to be linked to multiple B parts, but not
vice-versa. While counts in Table 2 reflect hu-
man labels, we only use automatically predicted
(s, d) pairs in the experiments of the remaining
part of this paper. To find these pairs automati-
cally, we trained a non-sequential log-linear model
that achieves a .902 accuracy (Galley et al., 2004).
</bodyText>
<sectionHeader confidence="0.967491" genericHeader="method">
4 Skip-Chain Sequence Models
</sectionHeader>
<bodyText confidence="0.99306175">
In this paper, we investigate conditional models
for paired sequences of observations and labels. In
the case of utterance selection, the observation se-
quence x = x1:T = (x1, ... , xT) represents local
</bodyText>
<figureCaption confidence="0.996747">
Figure 1: A skip-chain CRF with pragmatic-level links.
</figureCaption>
<equation confidence="0.886019">
Linear-chain edges yt = 1 yt = −1
yt−1 = 1 529 7742
yt−1 = −1 7742 116040
Skip-chain edges yd = 1 yd = −1
y3 = 1 6792 2191
y3 = −1 1479 121591
</equation>
<tableCaption confidence="0.891289">
Table 2: Contingency tables: while the correlation between
</tableCaption>
<construct confidence="0.321099666666667">
adjacent labels yt−1 and yt is not significant (x2 = 2.3,
p &gt; .05), empirical evidence clearly shows that y3 and yd
influence each other (x2 = 78948, p &lt; .001).
</construct>
<bodyText confidence="0.984919370370371">
summarization predictors (see Section 6), and the
binary sequence y = y1:T = (y1, ... , yT) (where
yt E {−1,1}) determines which utterances must
be included in the summary. In a discriminative
framework, we concentrate our modeling effort on
estimating p(y|x) from data, and do not explicitly
model the prior probability p(x), since x is fixed
during testing anyway.
Many probabilistic approaches to modeling se-
quences have relied on directed graphical mod-
els, also known as Bayesian networks (BN),1 in
particular hidden Markov models (Rabiner, 1989)
and conditional Markov models (McCallum et al.,
2000). However, prominent recent approaches
have focused on undirected graphical models, in
particular conditional random fields (CRF) (Laf-
ferty et al., 2001), and provided state-of-the-art
performance in many NLP tasks. In our work, we
will provide empirical results for state sequence
models of both semantics, and we will now de-
1In the existing literature, sequence models that satisfy the
Markovian condition—i.e., the state of the system at time t
depend only on its immediate past t − k:t − 1 (typically just
t − 1)—are generally termed dynamic Bayesian networks
(DBN). Since the particular models under investigation, i.e.
skip-chain models, do not have this property, we will simply
refer to them as Bayesian networks.
</bodyText>
<page confidence="0.995061">
366
</page>
<bodyText confidence="0.99972425">
scribe skip-chain models for both BNs and CRFs.
In a BN, the probability of the sequence y fac-
torizes as a product of probabilities of local predic-
tions yt conditioned on their parents 7r(yt) (Equa-
tion 1). In a CRF, the probability of the sequence y
factorizes according to a set of clique potentials
{4bc}c∈C, where C is represents the cliques of the
underlying graphical model (Equation 2).
</bodyText>
<equation confidence="0.9927595">
pBN(y|x) = YT pBN(yt|x,7r(yt)) (1)
i=1
pCRF(y|x) a Y 4&apos;c(xc, yc) (2)
c∈C
</equation>
<bodyText confidence="0.999978565217391">
We parameterize these BNs and CRFs as log-
linear models, and factorize both BN’s local pre-
diction probabilities and CRF’s clique potentials
using two types of feature functions. Linear-chain
feature functions fj(yt−k:t, x, t) represent local
dependencies that are consistent with an order-k
Markov assumption. For instance, one such func-
tion could be a predicate that is true if and only if
yt−1 = 1, yt = −1, and (xt−1, xt) indicates that
both utterances are produced by the same speaker.
Given a set of skip edges S = {(st, t)} specifying
source and destination indices, skip-chain feature
functions gj(yst, yt, x, st, t) exploit dependencies
between variables that are arbitrarily distant in
the chain. For instance, the finding that OFFER-
REJECT pairs are often linked in summaries might
be encoded as a skip-chain feature predicate that
is true if and only if yst = 1, yt = 1, and the first
word of the t-th utterance is “no”.
Log-linear models for skip-chain sequence
models are defined in terms of weights {Ak} and
{µk}, one for each feature function. In the case of
BNs, we write:
</bodyText>
<equation confidence="0.9904795">
log pBN(yt|x,7r(yt)) a
J J&apos;
X Ajfj(x, yt−k:t, t) + X µjgj(x, yst, yt, st, t)
j=1 j=1
</equation>
<bodyText confidence="0.99982675">
We can reduce a particular skip-chain CRF to rep-
resent only the set of cliques along (yt−1, yt) adja-
cency edges and (yst, yt) skip edges, resulting in
only two potential functions:
</bodyText>
<equation confidence="0.9368974">
log 4bLIN(x, yt−k:t, t) = XJ Ajfj(x, yt−k:t, t)
j=1
J&apos;
log 4&apos;SKIP(x, yst, yt, t) = X µjgj(x, yst, yt, st, t)
j=1
</equation>
<subsectionHeader confidence="0.962444">
4.1 Inference and Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.99992594">
Our CRF and BN models were designed us-
ing MALLET (McCallum, 2002), which provides
tools for training log-linear models with L-BFGS
optimization techniques and maximize the log-
likelihood of our training data D = (x(i), y(i))Ni=1,
and provides probabilistic inference algorithms for
linear-chain BNs and CRFs.
Most previous work with CRFs containing non-
local dependencies used approximate probabilis-
tic inference techniques, including TRP (Sutton
and McCallum, 2004) and Gibbs sampling (Finkel
et al., 2005). Approximation is needed when
the junction tree of a graphical model is associ-
ated with prohibitively large cliques. For exam-
ple, the worse case reported in (Sutton and Mc-
Callum, 2004) is a clique of 61 nodes. In the
case of skip-chain models representing APs, the
inference problem is somewhat simpler: loops in
the graph are relatively short, 98% of AP edges
span no more than 5 time slices, and the maximum
clique size in the entire data is 5. While exact in-
ference might be possible in our case, we used the
simpler approach of adapting standard inference
algorithms for linear-chain models.
Specifically, to account for skip-edges, we used
a technique inspired by (Sha and Pereira, 2003),
in which multiple state dependencies, such as an
order-2 Markov model, are encoded using auxil-
iary tags. For instance, an order-2 Markov model
is parameterized using state triples yt−2:t, and each
possible triple is converted to a label zt = yt−2:t.
Using these auxiliary labels only, we can then
use the standard forward-backward algorithm for
computing marginal distributions in linear-chain
CRFs, and Viterbi decoding in linear-chain CRFs
and BNs. The only requirement is to ensure that
a transition between zt and zt+1 is forbidden if
the sub-states yt−1:t common to both states differ,
i.e., is assigned an infinite cost. This approach can
be extended to the case of skip-chain transitions.
For instance, an order-1 Markov model with skip-
edges can be constructed using zt = (yst, yt−1, yt)
triples, where the first element yst represents the
label at the source of the skip-edge. Similarly to
the case of order-2 Markov models, we need to
ensure that only valid sequences of labels are con-
sidered, which is trivial to enforce if we assume
that no skip edge ranges more than a predefined
threshold of k time slices.
While this approach is not exact, it still provides
</bodyText>
<page confidence="0.991943">
367
</page>
<bodyText confidence="0.995429333333333">
competitive performance as we will see in Sec-
tion 8. In future work, we plan to explore more
accurate probabilistic inference techniques.
</bodyText>
<sectionHeader confidence="0.949273" genericHeader="method">
5 Ranking Utterances by Importance
</sectionHeader>
<bodyText confidence="0.999983777777778">
As we will see in Section 8, using the actual
1−1, 11 label predictions of our BNs and CRFs
leads to significantly sub-optimal results, which
might be explained by the following reasons. First,
our models are optimized to maximize the condi-
tional log-likelihood of the training data, a mea-
sure that does not correlate well with utility mea-
sures generally used in retrieval oriented tasks
such as summarization, especially when faced
with a significant class imbalance (only 6.26%
of reference instances are positive). Second, the
MAP decision rule doesn’t give us the freedom to
select an arbitrary number of sentences in order
to satisfy any constraint on length. Instead of us-
ing actual predictions, it seems more reasonable
to compute the posterior probability of each lo-
cal prediction yt, and extract the N most probable
summary sentences (yr1, ... , yrk), where N may
depend on a length expressed in number of words,
as it is the case in our evaluation in Section 7.
BNs assign probability distributions over entire
sequences by estimating the probability of each in-
dividual instance yt in the sequence (Equation 1),
and seem thus particularly suited for ranking utter-
ances. A first approach is then to rank utterances
according to the cost of predicting yt = 1 at each
time step on the Viterbi path. While these costs
are well-formed (negative log) probabilities in the
case of BNs, they cannot be interpreted as such in
the case of CRFs, and turn out to produce poor re-
sults with CRFs. Indeed, the set of CRF potentials
associated with each time step have no immedi-
ate probabilistic interpretation, and cannot be used
directly to rank sentences. Since BNs and CRFs
are here parameterized as log-linear models and
rely on the same set of feature functions, a second
approach is to use CRF-trained model parameters
to build a BN classifier that assigns a probability
to each yt. Specifically, the CRF model is first
used to generate label predicitons y, from which
the locally-normalized model estimates the cost
of predicting yt = 1 given a label history y1:t_1.
This ensures that we have a well-formed probabil-
ity distribution at each time slice, while capitaliz-
ing on the good performance of CRF models.
</bodyText>
<table confidence="0.965946555555556">
Lexical features:
· n-grams (n &lt; 3)
· number of words
· number of digits
· number of consecutive repeats
Information retrieval features:
· max/sum/mean frequency of all terms in ut
· max/sum/mean idf score
· max/sum/mean tf ·idf score
</table>
<tableCaption confidence="0.785314">
· cosine similarity between word vector of ut with cen-
troid of of the meeting
· scores of LSA with 5, 10, 50, 100, 200, 300 concepts
</tableCaption>
<table confidence="0.8641059">
Acoustic features:
· seconds of silence before/during/after the turn
· speech rate
· min/max/mean/median/stddev/onset/outset f0 of utter-
ance t, and of first and last word
· min/max/mean/stddev energy
· .05, .25, .5, .75, .95 quantiles of f0 and energy
· pitch range
· f0 mean absolute slope
Durational and structural features:
</table>
<listItem confidence="0.913562545454545">
· duration of the previous/current/next utterance
· relative position within meeting (i.e., index t)
· relative position within speaker turn
· large number of structural predicates, i.e. “is the previ-
ous utterance of the same speaker?”
· number of APs initiated in yt
Discourse features:
· lexical cohesion score (for topic shifts) (Hearst, 1994)
· first and second word of utterance, if in cue word list
· number of pronouns
· number of fillers and fluency devices (e.g., “uh”, “um”)
</listItem>
<tableCaption confidence="0.8307886">
· number of backchannel and acknowledgment tokens
(e.g., “uh-huh”, “ok”, “right”)
Table 3: Features for extractive summarization. Unless oth-
erwise mentioned, we refer to features of utterance t whose
label yt we are trying to predict.
</tableCaption>
<sectionHeader confidence="0.995931" genericHeader="method">
6 Features for extractive summarization
</sectionHeader>
<bodyText confidence="0.999980888888889">
We started our analyses with a large collection
of features found to be good predictors in ei-
ther speech (Inoue et al., 2004; Maskey and
Hirschberg, 2005; Murray et al., 2005) or text
summarization (Mani and Maybury, 1999). Our
goal is to build a very competitive feature set that
capitalizes on recent advances in summarization of
both genres. Table 3 lists some important features.
There is strong evidence that lexical cues such
as “significant” and “great” are strong predictors
in many summarization tasks (Edmundson, 1968).
Such cues are admittedly quite genre specific,
so we did not want to commit ourselves to any
specific list, which may not carry over well to
our specific speech domain, and we automatically
selected a list of n-grams (n &lt; 3) using cross-
validation on the training data. More specifically,
we computed the mutual information of each n-
</bodyText>
<page confidence="0.9981">
368
</page>
<figureCaption confidence="0.999845">
Figure 2: Model, peer, and “optimal” summaries are all extracts taken from the same transcription.
</figureCaption>
<bodyText confidence="0.999952945945946">
gram with the class variable, and selected for each
n the 200 best scoring n-grams. Other lexical fea-
tures include: the number of digits, which is help-
ful for identifying sections of the meetings where
participants collect data by recording digits; the
number of repeats, which may indicate the kind of
hesitations and disfluencies that negatively corre-
lates with what is included in the summary.
The information retrieval feature set contains
many features that are generally found helpful in
summarization, in particular tf ·idf and scores de-
rived from centroid methods. In particular, we
used the latent semantic analysis (LSA) feature
discussed in (Murray et al., 2005), which attempts
to determine sentence importance through singu-
lar value decomposition, and whose resulting sin-
gular values and singular vectors can be exploited
to associate each utterance a degree of relevance to
one of the top-n concepts of the meetings (where n
represents the number of dimensions in the LSA).
We used the same scoring mechanism as (Mur-
ray et al., 2005), though we extracted features for
many different n values.
Acoustic features extracted with Praat
(Boersma and Weenink, 2006) were normal-
ized by channel and speaker, including many
raw features such as f0 and energy. Structural
features listed in the table are those computed
from the sequence model before decoding, e.g.,
the duration that separates the two elements
of an AP. Finally, discourse features represent
predictors that may substitute to DA labels. While
DA tagging is not directly our concern, it is
presumably helpful to capitalize on discourse
characteristics of utterances involved in adjacency
pairs, since different types of dialog acts may be
unequally likely to appear in a summary.
</bodyText>
<sectionHeader confidence="0.998038" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999971431818182">
Evaluating summarization is a difficult problem
and there is no broad consensus on how to best
perform this task. Two metrics have become
quite popular in multi-document summarization,
namely the Pyramid method (Nenkova and Pas-
sonneau, 2004b) and ROUGE (Lin, 2004). Pyra-
mid and ROUGE are techniques looking for con-
tent units repeated in different model summaries,
i.e., summary content units (SCUs) such as clauses
and noun phrases for the Pyramid method, and n-
grams for ROUGE. The underlying hypothesis is
that different model sentences, clauses, or phrases
may convey the same meaning, which is a reason-
able assumption when dealing with reference sum-
maries produced by different authors, since it is
quite unlikely that any two abstractors would use
the exact same words to convey the same idea.
Our situation is however quite different, since
all model summaries of a given document are ut-
terance extracts of that same document, as this can
been seen in the excerpt of Figure 2. In our own
annotation of three meetings with SCUs defined
as in (Nenkova and Passonneau, 2004a), we found
that repetitions and reformulation of the same in-
formation are particularly infrequent, and that tex-
tual units that express the same content among
model summaries are generally originating from
the same document sentence (e.g., in the figure,
the first sentence in model 1 and 2 emanate from
the same document sentence). Very short SCUs
(e.g., base noun phrases) sometimes appeared in
different locations of a meeting, but we think it is
problematic to assume that connections between
such short units are indicative of any similarity
of sentential meaning: the contexts are different,
and words may be uttered by different speakers,
which may lead to unrelated or conflicting prag-
matic forces. For instance, an SCU realized as
“DC offset” and “DC component” appears in two
different sentences in the figure, i.e. those iden-
tified as 1-13 and 31-41. However, the two sen-
tences have contradictory meanings, and it would
be unfortunate to increase the score of a peer sum-
mary containing the former sentence because the
</bodyText>
<page confidence="0.997883">
369
</page>
<bodyText confidence="0.996128679245283">
latter is included in some model summaries.
For all these reasons, we believe that sum-
marization evaluation in our case should rely on
the following restrictive matching: two summary
units should be considered equivalent if and only
if they are extracted from the same location in
the original document (e.g., the “DC” appearing
in models 1 and 2 is not the same as the “DC” in
the peer summary, since they are extracted from
different sentences). This constraint on the match-
ing is reflected in our Pyramid evaluation, and we
define an SCU as a word and its document po-
sition, which lets us distinguish (“DC”,11) from
(“DC”,33). While this restriction on SCUs forces
us to disregard scarcely occurring paraphrases and
repetitions of the same information, it provides the
benefit of automated evaluation.
Once all SCUs have been identified, the Pyra-
mid method is applied as in (Nenkova and Passon-
neau, 2004b): we compute a score D by adding for
each SCU present in the summary a score equal
to the number of model summaries in which that
SCU appears. The Pyramid score P is computed
by dividing D by the maximum D* value that is
obtainable given the constraint on length. For in-
stance, the peer summary in the figure gets a score
D = 9 (since the 9 SCUs in range 43-51 occur in
one model), and the maximum obtainable score is
D* = 44 (all SCUs of the optimal summary ap-
pear in exactly two model summaries), hence the
peer summary’s score is P = .204.
While our evaluation scheme is similar to com-
paring the binary predictions of model and peer
summaries—each prediction determining whether
a given transcription word is included or not—
and averaging precision scores over all peer-model
pairs, the Pyramid evaluation differs on an im-
portant point, which makes us prefer the Pyramid
evaluation method: the maximum possible Pyra-
mid score is always guaranteed to be 1, but av-
erage precision scores can become arbitrarily low
as the consensus between summary annotators de-
creases. For instance, the average precision score
of the optimal summary in the figure is PR = 3.2
2Precision scores of the optimal summary compared
against the the three model summaries are .5, 1, and .5, re-
spectively, and hence average 23. We can show that P =
PR/PR*, where PR* is the average precision of the op-
timal summary. Lack of space prevent us from providing a
proof, so we will just show that the equality holds in our ex-
ample: since the peer summary’s precision scores against the
three model summaries are respectively 922, 0, and 0, we have
PR/PR*= (9 66�/(2 3� = 944 = P.
</bodyText>
<table confidence="0.999498653846154">
FEATURE FQ=1
1 utterance duration .246
2 100-dimension LSA .268
3 duration of utterance t − 1 .275
4 time between utterances s and d = t .281
5 IDF mean .284
6 meeting position .286
7 number of APs initiated in t .288
8 duration of utterance t + 1 .288
9 number of fillers .289
10 .25-quantile of energy .290
11 number of lexical repeats .292
12 lexical cohesion score .294
13 f0 mean of last word of utterance t .294
14 LSA 50 dimensions .295
15 utterances (t,t + 1) by same speaker .298
16 speech rate .302
17 “is that” .303
18 “for the” .303
19 (ut_1,ut) by same speaker .305
20 “to try” .305
21 “meetings” .305
22 utterance starts with “and” .306
23 “we have” .306
24 “new” .307
25 utterance starts with “what” .307
</table>
<tableCaption confidence="0.99957">
Table 4: Forward feature selection.
</tableCaption>
<bodyText confidence="0.999860333333333">
In the case of the six test meetings, which all have
either 3 or 4 model summaries, the maximum pos-
sible average precision is .6405.
</bodyText>
<sectionHeader confidence="0.996751" genericHeader="evaluation">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99996752">
We follow (Murray et al., 2005) in using the same
six meetings as test data, since each of these meet-
ings has multiple reference summaries. The re-
maining 69 meetings were used for training, which
represent in total more than 103,000 training in-
stances (or DA units), of which 6,464 are posi-
tives (6.24%). The multi-reference test set con-
tains more than 28,000 instances.
The goal of a preliminary experiment was to de-
vise a set of useful predictors from a full set of
1171. We performed feature selection by incre-
mentally growing a log-linear model with order-
0 features f(x, yt) using a forward feature selec-
tion procedure similar to (Berger et al., 1996).
Probably due to the imbalance between positive
and negative samples, we found it more effective
to rank candidate features by gains in F-measure
(through 5-fold cross validation on the entire train-
ing set). The increase in Fi by adding new features
to the model is displayed in Table 4; this greedy
search resulted in a set S of 217 features.
We now analyze the performance of different
sequence models on our test set. The target length
of each summary was set to 12.7% of the number
of words of the full document, which is the aver-
</bodyText>
<page confidence="0.993598">
370
</page>
<bodyText confidence="0.999779823529412">
age on the entire training data (the average on the
test data is 12.9%). In Table 5, we use an order-0
CRF to compare S against all features and various
categorical groupings. Overall, we notice lexical
predictors and statistics derived from them (e.g.
LSA features) represent the most helpful feature
group (.497), though all other features combined
achieve a competitive performance (.476).
Table 6 displays performance for sequence
models incorporating linear-chain features of in-
creasing order k. Its second column indicates
what criterion was used to rank utterances. In the
case of ‘pred’, we used actual model 1−1, 11 pre-
dictions, which in all cases generated summaries
much shorted than the allowable length, and pro-
duced poor performance. ‘Costs’ and ‘norm-CRF’
refer to the two ranking criteria presented in Sec-
tion 5, and it is clear that the performance of CRFs
degrades with increasing orders without local nor-
malization. While the contingency counts in Ta-
ble 2 only hinted a limited benefit of linear-chain
features, empirical results show the contrary—
especially for order k = 2. However, the further
increase of k causes overfitting, and skip-chain
features seem a better way to capture non-local
dependencies while keeping the number of model
parameters relatively small. Overall, the addition
of skip-chain edges to linear-chain models provide
noticeable improvement in Pyramid scores. Our
system that performed best on cross-validation
data is an order-2 CRF with skip-chain transitions,
which achieves a Pyramid score of P = .554.
We now assess the significance of our results
by comparing our best system against: (1) a lead
summarizer that always selects the first N utter-
ances to match the predefined length; (2) human
performance, which is obtained by leave-one-out
comparisons among references (Table 7); (3) “op-
timal” summaries generated using the procedure
explained in (Nenkova and Passonneau, 2004b)
by ranking document utterances by the number of
model summaries in which they appear. It ap-
pears that our system is considerably better than
the baseline, and achieves 91.3% of human per-
formance in terms of Pyramid scores, and 83% if
using ASR transcription. This last result is partic-
ularly positive if we consider our strong reliance
on lexical features.
For completeness, we also included standard
ROUGE (1, 2, and L) scores in Table 7, which
were obtained using parameters defined for the
</bodyText>
<table confidence="0.999333">
FEATURE SET P
lexical .471
IR .415
lexical + IR .497
acoustic .407
structural/durational .478
acoustic + structural/durational .476
all features .507
selected features (S) .515
</table>
<tableCaption confidence="0.997256">
Table 5: Pyramid score for each feature set.
</tableCaption>
<table confidence="0.99992275">
RANKING k = 1 2 3
pred .241 .267 .269
costs .512 .519 .525
costs .543 .549 .542
pred .326 .36 .348
costs .508 .475 .447
norm-CRF .53 .548 .54
norm-CRF .541 .554 .559
</table>
<tableCaption confidence="0.9542854">
Table 6: Pyramid scores for different sequence models, where
k stands for the order of linear-chain features. The value in
bold is the performance of the model that was selected after
a 5-fold cross validation on the training data, which obtained
the highest Fl score.
</tableCaption>
<table confidence="0.999906333333333">
SUMMARIZER P R-1 R-2 R-L
baseline .188 .501 .210 .495
skip-chain CRF (transcript) .554 .715 .442 .709
skip-chain CRF (ASR) .504 .714 .42 .706
human .607 .720 .477 .715
optimal 1 .791 .648 .788
</table>
<tableCaption confidence="0.968262666666667">
Table 7: Pyramid, and average ROUGE scores for summaries
produces by a baseline (lead summarizer), our best system,
humans, and the optimal summarizer.
</tableCaption>
<bodyText confidence="0.98862108">
DUC-05 evaluation. Since system summaries
have on average approximately the same length
as references, we only report recall measures of
ROUGE (precision and F averages are within f
.002).3 It may come as a surprise that our best sys-
tem (both with ASR and true words) performs al-
most as well as humans; it seems more reasonable
to conclude that, in our case, ROUGE has trouble
discriminating between systems with moderately
close performance. This seems to confirm our im-
pression that content evaluation in our task should
be based on exact matches.
We performed a last experiment to compare our
best system against Murray et al. (2005), who used
the same test data, but constrained summary sizes
in terms of number of DA units instead of words.
In their experiments, 10% of DAs had to be se-
lected. Our system achieves .91 recall, .5 preci-
sion, and .64 F1 with the same length constraint.
3Human performance with ROUGE was assessed by
cross-validating reference summaries of each meeting (i.e.,
n references for a given meeting resulted in n evaluations
against the other references). We used the same leave-one-
out procedure with other summarizers, in order to get results
comparable to humans.
</bodyText>
<figure confidence="0.996672625">
MODEL
linear-chain BN
linear-chain BN
skip-chain BN
linear-chain CRF
linear-chain CRF
linear-chain CRF
skip-chain CRF
</figure>
<page confidence="0.993666">
371
</page>
<bodyText confidence="0.999989875">
The discrepancy between recall and precision is
largely due to the fact that generated summaries
are on average much longer than model summaries
(10% vs. 6.26% of DAs), which explains why our
precision is relatively low in this last evaluation.
The best ROUGE-1 measure reported in (Murray
et al., 2005) is .69 recall, which is significantly
lower than ours according to confidence intervals.
</bodyText>
<sectionHeader confidence="0.997522" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999992923076923">
An order-2 CRF with skip-chain dependencies de-
rived from the automatic analysis of participant
interaction was shown to outperform linear-chain
BNs and CRFs, despite the incorporation in all
cases of the same competitive set of predictors
resulting from cross-validated feature selection.
Compared to an order-0 CRF model, the absolute
increase in performance is 3.9% (7.5% relative in-
crease), which indicates that it is helpful to use
skip-chain sequence models in the summarization
task. Our best performing system reaches 91.3%
of human performance, and scales relatively well
on automatic speech recognition output.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999774">
This work has benefited greatly from suggestions
and advice from Kathleen McKeown. I also would
like to thank Jean Carletta, Steve Renals and
Gabriel Murray for giving me access to their sum-
marization corpus, Ani Nenkova for helpful dis-
cussions about summarization evaluation, Michael
Collins, Daniel Ellis, Julia Hirschberg, and Owen
Rambow for useful preliminary discussions, and
three anonymous reviewers for their insightful
comments on an earlier version of this paper.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999806119047619">
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A max-
imum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–72.
P. Boersma and D. Weenink. 2006. Praat: doing phonetics
by computer. http://www.praat.org/.
J. Conroy, J. Schlesinger, J. Goldstein, and D. O’Leary. 2004.
Left-brain/right-brain multi-document summarization. In
DUC 04 Conference Proceedings.
H.P. Edmundson. 1968. New methods in automatic extract-
ing. Journal of the ACM, 16(2):264–285.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
non-local information into information extraction systems
by gibbs sampling. In Proc. of ACL, pages 363–370.
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in conver-
sational speech: Use of bayesian networks to model prag-
matic dependencies. In Proc. of ACL, pages 669–676.
M. Hearst. 1994. Multi-paragraph segmentation of exposi-
tory text. In Proc. of ACL, pages 9–16.
A. Inoue, T. Mikami, and Y. Yamashita. 2004. Improvement
of speech summarization using prosodic information. In
Proc. of Speech Prosody.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-
gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and
C. Wooters. 2003. The ICSI meeting corpus. In Proc.
of ICASSP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. of ICML, pages 282–289.
C.-Y. Lin. 2004. ROUGE: a package for automatic evalua-
tion of summaries. In Proc. of workshop on text summa-
rization, ACL-04.
I. Mani and M. Maybury. 1999. Advances in Automatic Text
Summarization. MIT Press.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features for
speech summarization. In Proc. of Eurospeech.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy markov models for information extraction
and segmentation. In Proc. of ICML.
A. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
N. Mirghafori, A. Stolcke, C. Wooters, T. Pirinen, I. Bulyko,
D. Gelbart, M. Graciarena, S. Otterson, B. Peskin, and
M. Ostendorf. 2004. From switchboard to meetings: De-
velopment of the 2004 ICSI-SRI-UW meeting recognition
system. In Proc. of ICSLP.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Eval-
uating automatic summaries of meeting recordings. In
Proc. of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
A. Nenkova and R. Passonneau. 2004a. Evaluating con-
tent selection in human- or machine-generated summaries:
The pyramid scoring method. Technical Report CUCS-
025-03, Columbia University, CS Department.
A. Nenkova and R. Passonneau. 2004b. Evaluating con-
tent selection in summarization: The pyramid method. In
Proc. of HLT/NAACL, pages 145–152.
L. Rabiner. 1989. A tutorial on hidden markov models and
selected applications in speech recogntion. Proc. of the
IEEE, 77(2):257–286.
O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen. 2004.
Summarizing email threads. In Proc. of HLT-NAACL.
E. Schegloff and H. Sacks. 1973. Opening up closings.
Semiotica, 7-4:289–327.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134–141.
L. Shrestha and K. McKeown. 2004. Detection of question-
answer pairs in email conversations. In Proc. of COLING,
pages 889–895.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act (MRDA) cor-
pus. In SIGdial Workshop on Discourse and Dialogue,
pages 97–100.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information extrac-
tion. Technical Report TR # 04-49, University of Mas-
sachusetts.
K. Zechner. 2002. Automatic summarization of open domain
multi-party dialogues in diverse genres. Computational
Liguistics, 28(4):447–485.
L. Zhou and E. Hovy. 2005. Digesting virtual “geek” culture:
The summarization of technical internet relay chats. In
Proc. of ACL, pages 298–305.
</reference>
<page confidence="0.998367">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823449">
<title confidence="0.981043">A Skip-Chain Conditional Random Field for Meeting Utterances by</title>
<author confidence="0.995471">Michel</author>
<affiliation confidence="0.9515055">Columbia Department of Computer</affiliation>
<address confidence="0.973389">New York, NY 10027,</address>
<email confidence="0.99979">galley@cs.columbia.edu</email>
<abstract confidence="0.997786666666666">We describe a probabilistic approach to content sefor meeting summarization. We use skip- Conditional Random Fields to model non-local pragmatic dependencies between paired such as typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task. We also discuss different approaches for ranking all utterances in a sequence using CRFs. Our best performing system achieves 91.3% of human performance when evaluated with the Pyramid evaluation metric, which represents a 3.9% absolute increase compared to our most competitive non-sequential classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="30668" citStr="Berger et al., 1996" startWordPosition="5056" endWordPosition="5059">using the same six meetings as test data, since each of these meetings has multiple reference summaries. The remaining 69 meetings were used for training, which represent in total more than 103,000 training instances (or DA units), of which 6,464 are positives (6.24%). The multi-reference test set contains more than 28,000 instances. The goal of a preliminary experiment was to devise a set of useful predictors from a full set of 1171. We performed feature selection by incrementally growing a log-linear model with order0 features f(x, yt) using a forward feature selection procedure similar to (Berger et al., 1996). Probably due to the imbalance between positive and negative samples, we found it more effective to rank candidate features by gains in F-measure (through 5-fold cross validation on the entire training set). The increase in Fi by adding new features to the model is displayed in Table 4; this greedy search resulted in a set S of 217 features. We now analyze the performance of different sequence models on our test set. The target length of each summary was set to 12.7% of the number of words of the full document, which is the aver370 age on the entire training data (the average on the test data</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
<author>D Weenink</author>
</authors>
<title>Praat: doing phonetics by computer.</title>
<date>2006</date>
<note>http://www.praat.org/.</note>
<contexts>
<context position="23852" citStr="Boersma and Weenink, 2006" startWordPosition="3880" endWordPosition="3883">ed from centroid methods. In particular, we used the latent semantic analysis (LSA) feature discussed in (Murray et al., 2005), which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited to associate each utterance a degree of relevance to one of the top-n concepts of the meetings (where n represents the number of dimensions in the LSA). We used the same scoring mechanism as (Murray et al., 2005), though we extracted features for many different n values. Acoustic features extracted with Praat (Boersma and Weenink, 2006) were normalized by channel and speaker, including many raw features such as f0 and energy. Structural features listed in the table are those computed from the sequence model before decoding, e.g., the duration that separates the two elements of an AP. Finally, discourse features represent predictors that may substitute to DA labels. While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary. 7 Evaluation Evaluatin</context>
</contexts>
<marker>Boersma, Weenink, 2006</marker>
<rawString>P. Boersma and D. Weenink. 2006. Praat: doing phonetics by computer. http://www.praat.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Conroy</author>
<author>J Schlesinger</author>
<author>J Goldstein</author>
<author>D O’Leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In DUC 04 Conference Proceedings.</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’Leary, 2004</marker>
<rawString>J. Conroy, J. Schlesinger, J. Goldstein, and D. O’Leary. 2004. Left-brain/right-brain multi-document summarization. In DUC 04 Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="22242" citStr="Edmundson, 1968" startWordPosition="3621" endWordPosition="3622">ance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n &lt; 3) using crossvalidation on the training data. More specifically, we computed the mutual information of each n368 Figure 2: Model, peer, and “optimal” summaries are all extracts taken from the same transcription. gram with the class variable, and selected for each n the 200 best scoring n-grams. Other lexical features include: the number of digits, which is helpful for iden</context>
</contexts>
<marker>Edmundson, 1968</marker>
<rawString>H.P. Edmundson. 1968. New methods in automatic extracting. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="16003" citStr="Finkel et al., 2005" startWordPosition="2581" endWordPosition="2584">yt−k:t, t) j=1 J&apos; log 4&apos;SKIP(x, yst, yt, t) = X µjgj(x, yst, yt, st, t) j=1 4.1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i))Ni=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs. Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005). Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques. For example, the worse case reported in (Sutton and McCallum, 2004) is a clique of 61 nodes. In the case of skip-chain models representing APs, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of AP edges span no more than 5 time slices, and the maximum clique size in the entire data is 5. While exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models. Spe</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. of ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>J Hirschberg</author>
<author>E Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>669--676</pages>
<contexts>
<context position="11546" citStr="Galley et al., 2004" startWordPosition="1831" endWordPosition="1834">ntiguous labels yt−1 and yt seem to seldom influence each other, the correlation between AP elements ys and yd is particularly strong, and they have a tendency to be either both included or both excluded. Note that the second table is not symmetric, because the data allows an A part to be linked to multiple B parts, but not vice-versa. While counts in Table 2 reflect human labels, we only use automatically predicted (s, d) pairs in the experiments of the remaining part of this paper. To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al., 2004). 4 Skip-Chain Sequence Models In this paper, we investigate conditional models for paired sequences of observations and labels. In the case of utterance selection, the observation sequence x = x1:T = (x1, ... , xT) represents local Figure 1: A skip-chain CRF with pragmatic-level links. Linear-chain edges yt = 1 yt = −1 yt−1 = 1 529 7742 yt−1 = −1 7742 116040 Skip-chain edges yd = 1 yd = −1 y3 = 1 6792 2191 y3 = −1 1479 121591 Table 2: Contingency tables: while the correlation between adjacent labels yt−1 and yt is not significant (x2 = 2.3, p &gt; .05), empirical evidence clearly shows that y3 a</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg. 2004. Identifying agreement and disagreement in conversational speech: Use of bayesian networks to model pragmatic dependencies. In Proc. of ACL, pages 669–676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="21300" citStr="Hearst, 1994" startWordPosition="3469" endWordPosition="3470">the turn · speech rate · min/max/mean/median/stddev/onset/outset f0 of utterance t, and of first and last word · min/max/mean/stddev energy · .05, .25, .5, .75, .95 quantiles of f0 and energy · pitch range · f0 mean absolute slope Durational and structural features: · duration of the previous/current/next utterance · relative position within meeting (i.e., index t) · relative position within speaker turn · large number of structural predicates, i.e. “is the previous utterance of the same speaker?” · number of APs initiated in yt Discourse features: · lexical cohesion score (for topic shifts) (Hearst, 1994) · first and second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text su</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>M. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of ACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Inoue</author>
<author>T Mikami</author>
<author>Y Yamashita</author>
</authors>
<title>Improvement of speech summarization using prosodic information.</title>
<date>2004</date>
<booktitle>In Proc. of Speech Prosody.</booktitle>
<contexts>
<context position="21838" citStr="Inoue et al., 2004" startWordPosition="3557" endWordPosition="3560">t Discourse features: · lexical cohesion score (for topic shifts) (Hearst, 1994) · first and second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically sele</context>
</contexts>
<marker>Inoue, Mikami, Yamashita, 2004</marker>
<rawString>A. Inoue, T. Mikami, and Y. Yamashita. 2004. Improvement of speech summarization using prosodic information. In Proc. of Speech Prosody.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="3986" citStr="Janin et al., 2003" startWordPosition="612" endWordPosition="615">ctural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models. Empirical evaluations using two standard summarization metrics—the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)—show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. 2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al., 2003), a corpus of “naturally-occurring” meetings, i.e. meetings that would have taken place anyway. Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364–372, Sydney, July 2006. c�2006 Association for Computational Linguistics intelligence, and networking research. The corpus contains 75 meetings, which are 60 minutes long on average, and involve a number of participants ranging from 3 to 10 (6 on average). The total number of uniq</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="12954" citStr="Lafferty et al., 2001" startWordPosition="2067" endWordPosition="2071">nces must be included in the summary. In a discriminative framework, we concentrate our modeling effort on estimating p(y|x) from data, and do not explicitly model the prior probability p(x), since x is fixed during testing anyway. Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models (Rabiner, 1989) and conditional Markov models (McCallum et al., 2000). However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) (Lafferty et al., 2001), and provided state-of-the-art performance in many NLP tasks. In our work, we will provide empirical results for state sequence models of both semantics, and we will now de1In the existing literature, sequence models that satisfy the Markovian condition—i.e., the state of the system at time t depend only on its immediate past t − k:t − 1 (typically just t − 1)—are generally termed dynamic Bayesian networks (DBN). Since the particular models under investigation, i.e. skip-chain models, do not have this property, we will simply refer to them as Bayesian networks. 366 scribe skip-chain models fo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proc. of workshop on text summarization, ACL-04.</booktitle>
<contexts>
<context position="3645" citStr="Lin, 2004" startWordPosition="562" endWordPosition="563"> estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs. After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models. Empirical evaluations using two standard summarization metrics—the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)—show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. 2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al., 2003), a corpus of “naturally-occurring” meetings, i.e. meetings that would have taken place anyway. Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 Proceedings of the 2006 Conference on Empirical Met</context>
<context position="24707" citStr="Lin, 2004" startWordPosition="4016" endWordPosition="4017"> an AP. Finally, discourse features represent predictors that may substitute to DA labels. While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary. 7 Evaluation Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task. Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e., summary content units (SCUs) such as clauses and noun phrases for the Pyramid method, and ngrams for ROUGE. The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonable assumption when dealing with reference summaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea. Our situation is however quite different, since all model summarie</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.-Y. Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proc. of workshop on text summarization, ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Maybury</author>
</authors>
<title>Advances in Automatic Text Summarization.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="21936" citStr="Mani and Maybury, 1999" startWordPosition="3572" endWordPosition="3575">second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n &lt; 3) using crossvalidation on the training data. More specifically, we c</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>I. Mani and M. Maybury. 1999. Advances in Automatic Text Summarization. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Maskey</author>
<author>J Hirschberg</author>
</authors>
<title>Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization.</title>
<date>2005</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="21867" citStr="Maskey and Hirschberg, 2005" startWordPosition="3561" endWordPosition="3564">: · lexical cohesion score (for topic shifts) (Hearst, 1994) · first and second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n &lt; 3</context>
</contexts>
<marker>Maskey, Hirschberg, 2005</marker>
<rawString>S. Maskey and J. Hirschberg. 2005. Comparing lexial, acoustic/prosodic, discourse and structural features for speech summarization. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="12801" citStr="McCallum et al., 2000" startWordPosition="2047" endWordPosition="2050">78948, p &lt; .001). summarization predictors (see Section 6), and the binary sequence y = y1:T = (y1, ... , yT) (where yt E {−1,1}) determines which utterances must be included in the summary. In a discriminative framework, we concentrate our modeling effort on estimating p(y|x) from data, and do not explicitly model the prior probability p(x), since x is fixed during testing anyway. Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models (Rabiner, 1989) and conditional Markov models (McCallum et al., 2000). However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) (Lafferty et al., 2001), and provided state-of-the-art performance in many NLP tasks. In our work, we will provide empirical results for state sequence models of both semantics, and we will now de1In the existing literature, sequence models that satisfy the Markovian condition—i.e., the state of the system at time t depend only on its immediate past t − k:t − 1 (typically just t − 1)—are generally termed dynamic Bayesian networks (DBN). Since the particular models u</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="15563" citStr="McCallum, 2002" startWordPosition="2521" endWordPosition="2522">p-chain sequence models are defined in terms of weights {Ak} and {µk}, one for each feature function. In the case of BNs, we write: log pBN(yt|x,7r(yt)) a J J&apos; X Ajfj(x, yt−k:t, t) + X µjgj(x, yst, yt, st, t) j=1 j=1 We can reduce a particular skip-chain CRF to represent only the set of cliques along (yt−1, yt) adjacency edges and (yst, yt) skip edges, resulting in only two potential functions: log 4bLIN(x, yt−k:t, t) = XJ Ajfj(x, yt−k:t, t) j=1 J&apos; log 4&apos;SKIP(x, yst, yt, t) = X µjgj(x, yst, yt, st, t) j=1 4.1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i))Ni=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs. Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005). Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques. For example, the worse case reported in (S</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>A. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Mirghafori</author>
<author>A Stolcke</author>
<author>C Wooters</author>
<author>T Pirinen</author>
<author>I Bulyko</author>
<author>D Gelbart</author>
<author>M Graciarena</author>
<author>S Otterson</author>
<author>B Peskin</author>
<author>M Ostendorf</author>
</authors>
<title>From switchboard to meetings: Development of the 2004 ICSI-SRI-UW meeting recognition system.</title>
<date>2004</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="4907" citStr="Mirghafori et al., 2004" startWordPosition="752" endWordPosition="755">MNLP 2006), pages 364–372, Sydney, July 2006. c�2006 Association for Computational Linguistics intelligence, and networking research. The corpus contains 75 meetings, which are 60 minutes long on average, and involve a number of participants ranging from 3 to 10 (6 on average). The total number of unique speakers is 60, including 26 non-native English speakers. Experiments in this paper are based either on human orthographic transcriptions or automatic speech recognition output, which were available for all meetings. For automatic recognition, we used the ICSISRI-UW speech recognition system (Mirghafori et al., 2004), a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain. It achieves 34.8% WER on the ICSI corpus, which is indicative of the difficulty involved in processing meetings automatically. We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). Dialog act (DA) labels describe the pragmatic function of utterances, e.g. a STATEMENT or a BACKCHANNEL. This auxiliary corpus c</context>
</contexts>
<marker>Mirghafori, Stolcke, Wooters, Pirinen, Bulyko, Gelbart, Graciarena, Otterson, Peskin, Ostendorf, 2004</marker>
<rawString>N. Mirghafori, A. Stolcke, C. Wooters, T. Pirinen, I. Bulyko, D. Gelbart, M. Graciarena, S. Otterson, B. Peskin, and M. Ostendorf. 2004. From switchboard to meetings: Development of the 2004 ICSI-SRI-UW meeting recognition system. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
<author>J Moore</author>
</authors>
<title>Evaluating automatic summaries of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="6213" citStr="Murray et al., 2005" startWordPosition="957" endWordPosition="960">d adjacency pair (AP) relations (e.g., APOLOGYDOWNPLAY) were also labeled. This latter annotation was used to train an AP classifier that is instrumental in automatically determining the structure of our sequence models. Note that, in the case of three or more speakers, adjacency pair is admittedly an unfortunate term, since labeled APs are generally not adjacent (e.g., see Table 1), but we will nevertheless use the same terminology to enforce consistency with previous work. To train and evaluate our summarizer, we used a corpus of extractive summaries produced at the University of Edinburgh (Murray et al., 2005). For each of the 75 meetings, human judges were asked to select transcription utterances segmented by DA to include in summaries, resulting in an average compression ratio of 6.26% (though no strict limit was imposed). Inter-labeler agreement was measured using six meetings that were summarized by multiple coders (average n = .323). While this level of agreement is quite low, this situation is not uncommon to summarization, since there may be many good summaries for a given document; a main challenge lies in using evaluation schemes that properly accounts for this diversity. 3 Content selecti</context>
<context position="21889" citStr="Murray et al., 2005" startWordPosition="3565" endWordPosition="3568">or topic shifts) (Hearst, 1994) · first and second word of utterance, if in cue word list · number of pronouns · number of fillers and fluency devices (e.g., “uh”, “um”) · number of backchannel and acknowledgment tokens (e.g., “uh-huh”, “ok”, “right”) Table 3: Features for extractive summarization. Unless otherwise mentioned, we refer to features of utterance t whose label yt we are trying to predict. 6 Features for extractive summarization We started our analyses with a large collection of features found to be good predictors in either speech (Inoue et al., 2004; Maskey and Hirschberg, 2005; Murray et al., 2005) or text summarization (Mani and Maybury, 1999). Our goal is to build a very competitive feature set that capitalizes on recent advances in summarization of both genres. Table 3 lists some important features. There is strong evidence that lexical cues such as “significant” and “great” are strong predictors in many summarization tasks (Edmundson, 1968). Such cues are admittedly quite genre specific, so we did not want to commit ourselves to any specific list, which may not carry over well to our specific speech domain, and we automatically selected a list of n-grams (n &lt; 3) using crossvalidatio</context>
<context position="23352" citStr="Murray et al., 2005" startWordPosition="3800" endWordPosition="3803"> the 200 best scoring n-grams. Other lexical features include: the number of digits, which is helpful for identifying sections of the meetings where participants collect data by recording digits; the number of repeats, which may indicate the kind of hesitations and disfluencies that negatively correlates with what is included in the summary. The information retrieval feature set contains many features that are generally found helpful in summarization, in particular tf ·idf and scores derived from centroid methods. In particular, we used the latent semantic analysis (LSA) feature discussed in (Murray et al., 2005), which attempts to determine sentence importance through singular value decomposition, and whose resulting singular values and singular vectors can be exploited to associate each utterance a degree of relevance to one of the top-n concepts of the meetings (where n represents the number of dimensions in the LSA). We used the same scoring mechanism as (Murray et al., 2005), though we extracted features for many different n values. Acoustic features extracted with Praat (Boersma and Weenink, 2006) were normalized by channel and speaker, including many raw features such as f0 and energy. Structur</context>
<context position="30044" citStr="Murray et al., 2005" startWordPosition="4947" endWordPosition="4950">mber of lexical repeats .292 12 lexical cohesion score .294 13 f0 mean of last word of utterance t .294 14 LSA 50 dimensions .295 15 utterances (t,t + 1) by same speaker .298 16 speech rate .302 17 “is that” .303 18 “for the” .303 19 (ut_1,ut) by same speaker .305 20 “to try” .305 21 “meetings” .305 22 utterance starts with “and” .306 23 “we have” .306 24 “new” .307 25 utterance starts with “what” .307 Table 4: Forward feature selection. In the case of the six test meetings, which all have either 3 or 4 model summaries, the maximum possible average precision is .6405. 8 Experiments We follow (Murray et al., 2005) in using the same six meetings as test data, since each of these meetings has multiple reference summaries. The remaining 69 meetings were used for training, which represent in total more than 103,000 training instances (or DA units), of which 6,464 are positives (6.24%). The multi-reference test set contains more than 28,000 instances. The goal of a preliminary experiment was to devise a set of useful predictors from a full set of 1171. We performed feature selection by incrementally growing a log-linear model with order0 features f(x, yt) using a forward feature selection procedure similar </context>
<context position="35268" citStr="Murray et al. (2005)" startWordPosition="5814" endWordPosition="5817">n. Since system summaries have on average approximately the same length as references, we only report recall measures of ROUGE (precision and F averages are within f .002).3 It may come as a surprise that our best system (both with ASR and true words) performs almost as well as humans; it seems more reasonable to conclude that, in our case, ROUGE has trouble discriminating between systems with moderately close performance. This seems to confirm our impression that content evaluation in our task should be based on exact matches. We performed a last experiment to compare our best system against Murray et al. (2005), who used the same test data, but constrained summary sizes in terms of number of DA units instead of words. In their experiments, 10% of DAs had to be selected. Our system achieves .91 recall, .5 precision, and .64 F1 with the same length constraint. 3Human performance with ROUGE was assessed by cross-validating reference summaries of each meeting (i.e., n references for a given meeting resulted in n evaluations against the other references). We used the same leave-oneout procedure with other summarizers, in order to get results comparable to humans. MODEL linear-chain BN linear-chain BN ski</context>
</contexts>
<marker>Murray, Renals, Carletta, Moore, 2005</marker>
<rawString>G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. Evaluating automatic summaries of meeting recordings. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in human- or machine-generated summaries: The pyramid scoring method.</title>
<date>2004</date>
<tech>Technical Report CUCS025-03,</tech>
<institution>Columbia University, CS Department.</institution>
<contexts>
<context position="3621" citStr="Nenkova and Passonneau, 2004" startWordPosition="556" endWordPosition="559">m Fields (CRFs). We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs. After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models. Empirical evaluations using two standard summarization metrics—the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)—show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. 2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al., 2003), a corpus of “naturally-occurring” meetings, i.e. meetings that would have taken place anyway. Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 Proceedings of the 2006 Con</context>
<context position="24683" citStr="Nenkova and Passonneau, 2004" startWordPosition="4009" endWordPosition="4013">uration that separates the two elements of an AP. Finally, discourse features represent predictors that may substitute to DA labels. While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary. 7 Evaluation Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task. Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e., summary content units (SCUs) such as clauses and noun phrases for the Pyramid method, and ngrams for ROUGE. The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonable assumption when dealing with reference summaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea. Our situation is however quite different, </context>
<context position="27458" citStr="Nenkova and Passonneau, 2004" startWordPosition="4470" endWordPosition="4474">in the original document (e.g., the “DC” appearing in models 1 and 2 is not the same as the “DC” in the peer summary, since they are extracted from different sentences). This constraint on the matching is reflected in our Pyramid evaluation, and we define an SCU as a word and its document position, which lets us distinguish (“DC”,11) from (“DC”,33). While this restriction on SCUs forces us to disregard scarcely occurring paraphrases and repetitions of the same information, it provides the benefit of automated evaluation. Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): we compute a score D by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears. The Pyramid score P is computed by dividing D by the maximum D* value that is obtainable given the constraint on length. For instance, the peer summary in the figure gets a score D = 9 (since the 9 SCUs in range 43-51 occur in one model), and the maximum obtainable score is D* = 44 (all SCUs of the optimal summary appear in exactly two model summaries), hence the peer summary’s score is P = .204. While our evaluation scheme is similar to comparing the</context>
<context position="33134" citStr="Nenkova and Passonneau, 2004" startWordPosition="5454" endWordPosition="5457">the addition of skip-chain edges to linear-chain models provide noticeable improvement in Pyramid scores. Our system that performed best on cross-validation data is an order-2 CRF with skip-chain transitions, which achieves a Pyramid score of P = .554. We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) “optimal” summaries generated using the procedure explained in (Nenkova and Passonneau, 2004b) by ranking document utterances by the number of model summaries in which they appear. It appears that our system is considerably better than the baseline, and achieves 91.3% of human performance in terms of Pyramid scores, and 83% if using ASR transcription. This last result is particularly positive if we consider our strong reliance on lexical features. For completeness, we also included standard ROUGE (1, 2, and L) scores in Table 7, which were obtained using parameters defined for the FEATURE SET P lexical .471 IR .415 lexical + IR .497 acoustic .407 structural/durational .478 acoustic +</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004a. Evaluating content selection in human- or machine-generated summaries: The pyramid scoring method. Technical Report CUCS025-03, Columbia University, CS Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="3621" citStr="Nenkova and Passonneau, 2004" startWordPosition="556" endWordPosition="559">m Fields (CRFs). We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs. After analyzing the predictive power of a large set of durational, acoustical, lexical, structural, and information retrieval features, we perform feature selection to have a competitive set of predictors to test the different models. Empirical evaluations using two standard summarization metrics—the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)—show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. 2 Corpus The work presented here was applied to the ICSI Meeting Corpus (Janin et al., 2003), a corpus of “naturally-occurring” meetings, i.e. meetings that would have taken place anyway. Their style is quite informal, and topics are primarily concerned with speech, natural language, artificial 364 Proceedings of the 2006 Con</context>
<context position="24683" citStr="Nenkova and Passonneau, 2004" startWordPosition="4009" endWordPosition="4013">uration that separates the two elements of an AP. Finally, discourse features represent predictors that may substitute to DA labels. While DA tagging is not directly our concern, it is presumably helpful to capitalize on discourse characteristics of utterances involved in adjacency pairs, since different types of dialog acts may be unequally likely to appear in a summary. 7 Evaluation Evaluating summarization is a difficult problem and there is no broad consensus on how to best perform this task. Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). Pyramid and ROUGE are techniques looking for content units repeated in different model summaries, i.e., summary content units (SCUs) such as clauses and noun phrases for the Pyramid method, and ngrams for ROUGE. The underlying hypothesis is that different model sentences, clauses, or phrases may convey the same meaning, which is a reasonable assumption when dealing with reference summaries produced by different authors, since it is quite unlikely that any two abstractors would use the exact same words to convey the same idea. Our situation is however quite different, </context>
<context position="27458" citStr="Nenkova and Passonneau, 2004" startWordPosition="4470" endWordPosition="4474">in the original document (e.g., the “DC” appearing in models 1 and 2 is not the same as the “DC” in the peer summary, since they are extracted from different sentences). This constraint on the matching is reflected in our Pyramid evaluation, and we define an SCU as a word and its document position, which lets us distinguish (“DC”,11) from (“DC”,33). While this restriction on SCUs forces us to disregard scarcely occurring paraphrases and repetitions of the same information, it provides the benefit of automated evaluation. Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): we compute a score D by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears. The Pyramid score P is computed by dividing D by the maximum D* value that is obtainable given the constraint on length. For instance, the peer summary in the figure gets a score D = 9 (since the 9 SCUs in range 43-51 occur in one model), and the maximum obtainable score is D* = 44 (all SCUs of the optimal summary appear in exactly two model summaries), hence the peer summary’s score is P = .204. While our evaluation scheme is similar to comparing the</context>
<context position="33134" citStr="Nenkova and Passonneau, 2004" startWordPosition="5454" endWordPosition="5457">the addition of skip-chain edges to linear-chain models provide noticeable improvement in Pyramid scores. Our system that performed best on cross-validation data is an order-2 CRF with skip-chain transitions, which achieves a Pyramid score of P = .554. We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) “optimal” summaries generated using the procedure explained in (Nenkova and Passonneau, 2004b) by ranking document utterances by the number of model summaries in which they appear. It appears that our system is considerably better than the baseline, and achieves 91.3% of human performance in terms of Pyramid scores, and 83% if using ASR transcription. This last result is particularly positive if we consider our strong reliance on lexical features. For completeness, we also included standard ROUGE (1, 2, and L) scores in Table 7, which were obtained using parameters defined for the FEATURE SET P lexical .471 IR .415 lexical + IR .497 acoustic .407 structural/durational .478 acoustic +</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004b. Evaluating content selection in summarization: The pyramid method. In Proc. of HLT/NAACL, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recogntion.</title>
<date>1989</date>
<booktitle>Proc. of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="2347" citStr="Rabiner, 1989" startWordPosition="358" endWordPosition="359">ely to be included if the other element is not present in the summary. Most current statistical sequence models in natural language processing (NLP), such as hidden *This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grants No. IIS-0121396 and IIS-05-34871, and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF or DARPA. Markov models (HMMs) (Rabiner, 1989), are linear chains that only encode local dependencies between utterances to be labeled. In multi-party speech, the two elements of an AP are generally arbitrarily distant, and such models can only poorly account for dependencies underlying APs in summarization. We use instead skip-chain sequence models (Sutton and McCallum, 2004), which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task. In this paper, we compare two types of network structures—linear-chain and skip-chain—and two types of network semantics</context>
<context position="6889" citStr="Rabiner, 1989" startWordPosition="1066" endWordPosition="1067">ct transcription utterances segmented by DA to include in summaries, resulting in an average compression ratio of 6.26% (though no strict limit was imposed). Inter-labeler agreement was measured using six meetings that were summarized by multiple coders (average n = .323). While this level of agreement is quite low, this situation is not uncommon to summarization, since there may be many good summaries for a given document; a main challenge lies in using evaluation schemes that properly accounts for this diversity. 3 Content selection State sequence Markov models such as hidden Markov models (Rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization. Following an intuition that the probability of a given sentence may be locally conditioned on the previous one, Conroy (2004) built a HMM-based summarizer that consistently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations. Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties. In the case of summarization of conversational speech, Zechner (2002) found, for</context>
<context position="12747" citStr="Rabiner, 1989" startWordPosition="2041" endWordPosition="2042">ows that y3 and yd influence each other (x2 = 78948, p &lt; .001). summarization predictors (see Section 6), and the binary sequence y = y1:T = (y1, ... , yT) (where yt E {−1,1}) determines which utterances must be included in the summary. In a discriminative framework, we concentrate our modeling effort on estimating p(y|x) from data, and do not explicitly model the prior probability p(x), since x is fixed during testing anyway. Many probabilistic approaches to modeling sequences have relied on directed graphical models, also known as Bayesian networks (BN),1 in particular hidden Markov models (Rabiner, 1989) and conditional Markov models (McCallum et al., 2000). However, prominent recent approaches have focused on undirected graphical models, in particular conditional random fields (CRF) (Lafferty et al., 2001), and provided state-of-the-art performance in many NLP tasks. In our work, we will provide empirical results for state sequence models of both semantics, and we will now de1In the existing literature, sequence models that satisfy the Markovian condition—i.e., the state of the system at time t depend only on its immediate past t − k:t − 1 (typically just t − 1)—are generally termed dynamic </context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recogntion. Proc. of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>L Shrestha</author>
<author>J Chen</author>
<author>C Lauridsen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<journal>Semiotica,</journal>
<booktitle>In Proc. of HLT-NAACL. E. Schegloff</booktitle>
<pages>7--4</pages>
<contexts>
<context position="7788" citStr="Rambow et al., 2004" startWordPosition="1186" endWordPosition="1189">istently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations. Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties. In the case of summarization of conversational speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summaries—and thus preventing the selection of orphan questions or answers—significantly improved their readability according to various human summary evaluations. In email summarization (Rambow et al., 2004), Shrestha and McKeown (2004) obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange. In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines. The need to model pragmatic influences, such as between a question and an answer, is also prevalent in meeting summarization. In fact, questionansw</context>
</contexts>
<marker>Rambow, Shrestha, Chen, Lauridsen, 2004</marker>
<rawString>O. Rambow, L. Shrestha, J. Chen, and C. Lauridsen. 2004. Summarizing email threads. In Proc. of HLT-NAACL. E. Schegloff and H. Sacks. 1973. Opening up closings. Semiotica, 7-4:289–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="16696" citStr="Sha and Pereira, 2003" startWordPosition="2699" endWordPosition="2702">s associated with prohibitively large cliques. For example, the worse case reported in (Sutton and McCallum, 2004) is a clique of 61 nodes. In the case of skip-chain models representing APs, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of AP edges span no more than 5 time slices, and the maximum clique size in the entire data is 5. While exact inference might be possible in our case, we used the simpler approach of adapting standard inference algorithms for linear-chain models. Specifically, to account for skip-edges, we used a technique inspired by (Sha and Pereira, 2003), in which multiple state dependencies, such as an order-2 Markov model, are encoded using auxiliary tags. For instance, an order-2 Markov model is parameterized using state triples yt−2:t, and each possible triple is converted to a label zt = yt−2:t. Using these auxiliary labels only, we can then use the standard forward-backward algorithm for computing marginal distributions in linear-chain CRFs, and Viterbi decoding in linear-chain CRFs and BNs. The only requirement is to ensure that a transition between zt and zt+1 is forbidden if the sub-states yt−1:t common to both states differ, i.e., i</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of NAACL, pages 134–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shrestha</author>
<author>K McKeown</author>
</authors>
<title>Detection of questionanswer pairs in email conversations.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>889--895</pages>
<contexts>
<context position="7817" citStr="Shrestha and McKeown (2004)" startWordPosition="1190" endWordPosition="1193">the top systems in recent Document Understanding Conference (DUC) evaluations. Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties. In the case of summarization of conversational speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summaries—and thus preventing the selection of orphan questions or answers—significantly improved their readability according to various human summary evaluations. In email summarization (Rambow et al., 2004), Shrestha and McKeown (2004) obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange. In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines. The need to model pragmatic influences, such as between a question and an answer, is also prevalent in meeting summarization. In fact, questionanswer pairs are not the only dis</context>
</contexts>
<marker>Shrestha, McKeown, 2004</marker>
<rawString>L. Shrestha and K. McKeown. 2004. Detection of questionanswer pairs in email conversations. In Proc. of COLING, pages 889–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>J Ang</author>
<author>H Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>97--100</pages>
<contexts>
<context position="5377" citStr="Shriberg et al., 2004" startWordPosition="821" endWordPosition="824">ion output, which were available for all meetings. For automatic recognition, we used the ICSISRI-UW speech recognition system (Mirghafori et al., 2004), a state-of-the-art conversational telephone speech (CTS) recognizer whose language and acoustic models were adapted to the meeting domain. It achieves 34.8% WER on the ICSI corpus, which is indicative of the difficulty involved in processing meetings automatically. We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). Dialog act (DA) labels describe the pragmatic function of utterances, e.g. a STATEMENT or a BACKCHANNEL. This auxiliary corpus consists of over 180,000 human-annotated dialog act labels (n = .8), for which so-called adjacency pair (AP) relations (e.g., APOLOGYDOWNPLAY) were also labeled. This latter annotation was used to train an AP classifier that is instrumental in automatically determining the structure of our sequence models. Note that, in the case of three or more speakers, adjacency pair is admittedly an unfortunate term, since labeled APs are generally not adjacent (e.g., see Table 1</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In SIGdial Workshop on Discourse and Dialogue, pages 97–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Collective segmentation and labeling of distant entities in information extraction.</title>
<date>2004</date>
<tech>Technical Report TR # 04-49,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="2680" citStr="Sutton and McCallum, 2004" startWordPosition="408" endWordPosition="411">efense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the NSF or DARPA. Markov models (HMMs) (Rabiner, 1989), are linear chains that only encode local dependencies between utterances to be labeled. In multi-party speech, the two elements of an AP are generally arbitrarily distant, and such models can only poorly account for dependencies underlying APs in summarization. We use instead skip-chain sequence models (Sutton and McCallum, 2004), which allow us to explicitly model dependencies between distant utterances, and turn out to be particularly effective in the summarization task. In this paper, we compare two types of network structures—linear-chain and skip-chain—and two types of network semantics—Bayesian Networks (BNs) and Conditional Random Fields (CRFs). We discuss the problem of estimating the class posterior probability of each utterance in a sequence in order to extract the N most probable ones, and show that the cost assigned by a CRF to each utterance needs to be locally normalized in order to outperform BNs. After</context>
<context position="10368" citStr="Sutton and McCallum, 2004" startWordPosition="1625" endWordPosition="1628">three responses (B). Sentences in italic are not present in the reference summary. as OFFER-ACCEPT, we found that the type of dialog act has much less importance than the existence of the AP connection itself (APs in the data represent a great variety of DA pairs, including many that are not characterized as APs in the litterature—e.g., STATEMENT-STATEMENT in the table). Since DAs seem to matter less than adjacency pairs, the aim will be to build techniques to automatically identify such relations and exploit them in utterance selection. In the current work, we use skip-chain sequence models (Sutton and McCallum, 2004) to represent dependencies between both contiguous utterances and paired utterances appearing in the same AP constructions. The graphical representations of skip-chain models, such as the CRF represented in Figure 1, are composed of two types of edges: linear-chain and skip-chain edges. The latter edges model AP links, which we represent as a set of (s, d) index pairs (note that no more than one AP may share the same second element d). The intuition that the summarization labels (−1 or 1) are highly correlated with APs is confirmed in Table 2. While contiguous labels yt−1 and yt seem to seldom</context>
<context position="15962" citStr="Sutton and McCallum, 2004" startWordPosition="2574" endWordPosition="2577">unctions: log 4bLIN(x, yt−k:t, t) = XJ Ajfj(x, yt−k:t, t) j=1 J&apos; log 4&apos;SKIP(x, yst, yt, t) = X µjgj(x, yst, yt, st, t) j=1 4.1 Inference and Parameter Estimation Our CRF and BN models were designed using MALLET (McCallum, 2002), which provides tools for training log-linear models with L-BFGS optimization techniques and maximize the loglikelihood of our training data D = (x(i), y(i))Ni=1, and provides probabilistic inference algorithms for linear-chain BNs and CRFs. Most previous work with CRFs containing nonlocal dependencies used approximate probabilistic inference techniques, including TRP (Sutton and McCallum, 2004) and Gibbs sampling (Finkel et al., 2005). Approximation is needed when the junction tree of a graphical model is associated with prohibitively large cliques. For example, the worse case reported in (Sutton and McCallum, 2004) is a clique of 61 nodes. In the case of skip-chain models representing APs, the inference problem is somewhat simpler: loops in the graph are relatively short, 98% of AP edges span no more than 5 time slices, and the maximum clique size in the entire data is 5. While exact inference might be possible in our case, we used the simpler approach of adapting standard inferenc</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>C. Sutton and A. McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. Technical Report TR # 04-49, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Automatic summarization of open domain multi-party dialogues in diverse genres.</title>
<date>2002</date>
<journal>Computational Liguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="7478" citStr="Zechner (2002)" startWordPosition="1147" endWordPosition="1148">ov models (Rabiner, 1989) have been highly successful in many speech and natural language processing applications, including summarization. Following an intuition that the probability of a given sentence may be locally conditioned on the previous one, Conroy (2004) built a HMM-based summarizer that consistently ranked among the top systems in recent Document Understanding Conference (DUC) evaluations. Inter-sentential influences become more complex in the case of dialogues or correspondences, especially when they involve multiple parties. In the case of summarization of conversational speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summaries—and thus preventing the selection of orphan questions or answers—significantly improved their readability according to various human summary evaluations. In email summarization (Rambow et al., 2004), Shrestha and McKeown (2004) obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange. In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisti</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>K. Zechner. 2002. Automatic summarization of open domain multi-party dialogues in diverse genres. Computational Liguistics, 28(4):447–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>E Hovy</author>
</authors>
<title>Digesting virtual “geek” culture: The summarization of technical internet relay chats.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>298--305</pages>
<contexts>
<context position="8069" citStr="Zhou and Hovy, 2005" startWordPosition="1232" endWordPosition="1235">tional speech, Zechner (2002) found, for instance, that a simple technique consisting of linking together questions and answers in summaries—and thus preventing the selection of orphan questions or answers—significantly improved their readability according to various human summary evaluations. In email summarization (Rambow et al., 2004), Shrestha and McKeown (2004) obtained good performance in automatic detection of questions and answers, which can help produce summaries that highlight or focus on the question and answer exchange. In a combined chat and email summarization task, a technique (Zhou and Hovy, 2005) consisting of identifying APs and appending any relevant responses to topic initiating messages was instrumental in outperforming two competitive summarization baselines. The need to model pragmatic influences, such as between a question and an answer, is also prevalent in meeting summarization. In fact, questionanswer pairs are not the only discourse relations that we need to preserve in order to create coherent summaries, and, as we will see, most instances of APs would need to be preserved together, either inside or outside the summary. Table 1 displays an AP construction with one statemen</context>
</contexts>
<marker>Zhou, Hovy, 2005</marker>
<rawString>L. Zhou and E. Hovy. 2005. Digesting virtual “geek” culture: The summarization of technical internet relay chats. In Proc. of ACL, pages 298–305.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>