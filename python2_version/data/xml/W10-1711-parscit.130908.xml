<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025734">
<title confidence="0.964115">
The RWTH Aachen Machine Translation System for WMT 2010
</title>
<author confidence="0.9937005">
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor Leusch,
Saab Mansour, Daniel Stein and Hermann Ney
</author>
<affiliation confidence="0.860027">
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.993779">
surname@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.993826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999048375">
In this paper we describe the statisti-
cal machine translation system of the
RWTH Aachen University developed for
the translation task of the Fifth Workshop
on Statistical Machine Translation. State-
of-the-art phrase-based and hierarchical
statistical MT systems are augmented
with appropriate morpho-syntactic en-
hancements, as well as alternative phrase
training methods and extended lexicon
models. For some tasks, a system combi-
nation of the best systems was used to gen-
erate a final hypothesis. We participated
in the constrained condition of German-
English and French-English in each trans-
lation direction.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948333333333">
This paper describes the statistical MT system
used for our participation in the WMT 2010 shared
translation task. We used it as an opportunity to in-
corporate novel methods which have been investi-
gated at RWTH over the last year and which have
proven to be successful in other evaluations.
For all tasks we used standard alignment and
training tools as well as our in-house phrase-
based and hierarchical statistical MT decoders.
When German was involved, morpho-syntactic
preprocessing was applied. An alternative phrase-
training method and additional models were tested
and investigated with respect to their effect for the
different language pairs. For two of the language
pairs we could improve performance by system
combination.
An overview of the systems and models will fol-
low in Section 2 and 3, which describe the base-
line architecture, followed by descriptions of the
additional system components. Morpho-syntactic
analysis and other preprocessing issues are cov-
ered by Section 4. Finally, translation results for
the different languages and system variants are
presented in Section 5.
</bodyText>
<sectionHeader confidence="0.974507" genericHeader="method">
2 Translation Systems
</sectionHeader>
<bodyText confidence="0.999888">
For the WMT 2010 Evaluation we used stan-
dard phrase-based and hierarchical translation sys-
tems. Alignments were trained with a variant of
GIZA++. Target language models are 4-gram lan-
guage models trained with the SRI toolkit, using
Kneser-Ney discounting with interpolation.
</bodyText>
<subsectionHeader confidence="0.938372">
2.1 Phrase-Based System
</subsectionHeader>
<bodyText confidence="0.9999705">
Our phrase-based translation system is similar to
the one described in (Zens and Ney, 2008). Phrase
pairs are extracted from a word-aligned bilingual
corpus and their translation probability in both di-
rections is estimated by relative frequencies. Ad-
ditional models include a standard n-gram lan-
guage model, phrase-level IBM1, word-, phrase-
and distortion-penalties and a discriminative re-
ordering model as described in (Zens and Ney,
2006).
</bodyText>
<subsectionHeader confidence="0.999748">
2.2 Hierarchical System
</subsectionHeader>
<bodyText confidence="0.999945818181818">
Our hierarchical phrase-based system is similar to
the one described in (Chiang, 2007). It allows for
gaps in the phrases by employing a context-free
grammar and a CYK-like parsing during the de-
coding step. It has similar features as the phrase-
based system mentioned above. For some sys-
tems, we only allowed the non-terminals in hierar-
chical phrases to be substituted with initial phrases
as in (Iglesias et al., 2009), which gave better re-
sults on some language pairs. We will refer to this
as “shallow rules”.
</bodyText>
<subsectionHeader confidence="0.998852">
2.3 System Combination
</subsectionHeader>
<bodyText confidence="0.9970855">
The RWTH approach to MT system combination
of the French—*English systems as well as the
German—*English systems is a refined version of
the ROVER approach in ASR (Fiscus, 1997) with
</bodyText>
<page confidence="0.990632">
93
</page>
<note confidence="0.6937885">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93–97,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<table confidence="0.9989845">
German—*English French—*English English—*French
BLEU # Phrases BLEU # Phrases BLEU # Phrases
Standard 19.7 128M 25.5 225M 23.7 261M
FA 20.0 12M 25.9 35M 24.0 33M
</table>
<tableCaption confidence="0.889442">
Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For
German—*English and English—*French phrase table interpolation was applied.
</tableCaption>
<bodyText confidence="0.999380454545455">
additional steps to cope with reordering between
different hypotheses, and to use true casing infor-
mation from the input hypotheses. The basic con-
cept of the approach has been described by Ma-
tusov et al. (2006). Several improvements have
been added later (Matusov et al., 2008). This ap-
proach includes an enhanced alignment and re-
ordering framework. Alignments between the sys-
tems are learned by GIZA++, a one-to-one align-
ment is generated from the learned state occupa-
tion probabilities.
From these alignments, a confusion network
(CN) is then built using one of the hypotheses as
“skeleton” or “primary” hypothesis. We do not
make a hard decision on which of the hypothe-
ses to use for that, but instead combine all pos-
sible CNs into a single lattice. Majority voting on
the generated lattice is performed using the prior
probabilities for each system as well as other sta-
tistical models such as a special trigram language
model. This language model is also learned on
the input hypotheses. The intention is to favor
longer phrases contained in individual hypotheses.
The translation with the best total score within this
lattice is selected as consensus translation. Scal-
ing factors of these models are optimized similar
to MERT using the Downhill Simplex algorithm.
As the objective function for this optimization, we
selected a linear combination of BLEU and TER
with a weight of 2 on the former; a combination
that has proven to deliver stable results on sev-
eral MT evaluation measures in preceding experi-
ments.
In contrast to previous years, we now include a
separate consensus true casing step to exploit the
true casing capabilities of some of the input sys-
tems: After generating a (lower cased) consensus
translation from the CN, we sum up the counts of
different casing variants of each word in a sen-
tence over the input hypotheses, and use the ma-
jority casing over those. In previous experiments,
this showed to work significantly better than us-
ing a fixed non-consensus true caser, and main-
tains flexibility on the input systems.
</bodyText>
<sectionHeader confidence="0.965076" genericHeader="method">
3 New Additional Models
</sectionHeader>
<subsectionHeader confidence="0.991019">
3.1 Forced Alignment
</subsectionHeader>
<bodyText confidence="0.99998837037037">
For the German—*English, French—*English and
English—*French language tasks we applied a
forced alignment procedure to train the phrase
translation model with the EM algorithm, sim-
ilar to the one described in (DeNero et al.,
2006). Here, the phrase translation probabil-
ities are estimated from their relative frequen-
cies in the phrase-aligned training data. The
phrase alignment is produced by a modified
version of the translation decoder. In addi-
tion to providing a statistically well-founded
phrase model, this has the benefit of produc-
ing smaller phrase tables and thus allowing
more rapid experiments. For the language pairs
German—*English and English—*French the best
results were achieved by log-linear interpolation
of the standard phrase table with the generative
model. For French—*English we directly used the
model trained by forced alignment. A detailed
description of the training procedure is given in
(Wuebker et al., 2010). Table 1 shows the system
performances and phrase table sizes with the stan-
dard phrase table and the one trained with forced
alignment after the first EM iteration. We can see
that the generative model reduces the phrase table
size by 85-90% while increasing performance by
0.3% to 0.4% BLEU.
</bodyText>
<subsectionHeader confidence="0.998817">
3.2 Extended Lexicon Models
</subsectionHeader>
<bodyText confidence="0.999957">
In previous work, RWTH was able to show the
positive impact of extended lexicon models that
cope with lexical context beyond the limited hori-
zon of phrase pairs and n-gram language models.
Mauser et al. (2009) report improvements of
up to +1% in BLEU on large-scale systems for
Chinese—*English and Arabic—*English by incor-
porating discriminative and trigger-based lexicon
models into a state-of-the-art phrase-based de-
coder. They discuss how the two types of lexicon
</bodyText>
<page confidence="0.996544">
94
</page>
<bodyText confidence="0.999579966666667">
models help to select content words by capturing
long-distance effects.
The triplet model is a straightforward extension
of the IBM model 1 with a second trigger, and like
the former is trained iteratively using the EM al-
gorithm. In search, the triggers are usually on the
source side, i.e., p(e|f, f&apos;) is modeled. The path-
constrained triplet model restricts the first source
trigger to the aligned target word, whereas the sec-
ond trigger can move along the whole source sen-
tence. See (Hasan et al., 2008) for a detailed de-
scription and variants of the model and its training.
For the WMT 2010 evaluation, triplets mod-
eling p(e|f, f&apos;) were trained and applied di-
rectly in search for all relevant language pairs.
Path-constrained models were trained on the in-
domain news-commentary data only and on the
news-commentary plus the Europarl data. Al-
though experience from similar setups indicates
that triplet lexicon models can be beneficial for
machine translation between the languages En-
glish, French, and German, on this year’s WMT
translation tasks slight improvements on the devel-
opment sets did not or only partially carry over to
the held-out test sets. Nevertheless, systems with
triplets were used for system combination, as ex-
tended lexicon models often help to predict con-
tent words and to capture long-range dependen-
cies. Thus they can help to find a strong consensus
hypothesis.
</bodyText>
<subsectionHeader confidence="0.99801">
3.3 Unsupervised Training
</subsectionHeader>
<bodyText confidence="0.999850066666667">
Due to the small size of the English—*German re-
sources available for language modeling as well as
for lexicon extraction, we decided to apply the un-
supervised adaptation suggested in (Schwenk and
Senellart, 2009). We use a baseline SMT system to
translate in-domain monolingual source data, fil-
ter the translations according to a decoder score
normalized by sentence length, add this synthetic
bilingual data to the original one and rebuild the
SMT system from scratch.
The motivation behind the method is that the
phrase table will adapt to the genre, and thus
let phrases which are domain related have higher
probabilities. Two phenomena are observed from
phrase tables and the corresponding translations:
</bodyText>
<listItem confidence="0.984235">
• Phrase translation probabilities are changed,
making the system choose better phrase
translation candidates.
</listItem>
<table confidence="0.999593">
Running Words
English German
Bilingual 44.3M 43.4M
Dict. 1.4M 1.2M
AFP 610.7M
AFP unsup. 152.0M 157.3M
</table>
<tableCaption confidence="0.981617">
Table 2: Overview on data for unsupervised train-
ing.
</tableCaption>
<table confidence="0.9998682">
BLEU Test
Dev
baseline 15.0 14.7
+dict. 15.1 14.6
+unsup.+dict 15.4 14.9
</table>
<tableCaption confidence="0.995492">
Table 3: Results for unsupervised training method.
</tableCaption>
<listItem confidence="0.522978">
• Phrases which appear repeatedly in the do-
main get higher probabilities, so that the de-
coder can better segment the sentence.
</listItem>
<bodyText confidence="0.999903375">
To implement this idea, we translate the AFP part
of the English LDC Gigaword v4.0 and obtain the
synthetic data.
To decrease the number of OOV words, we use
dictionaries from the stardict directory as addi-
tional bilingual data to translate the AFP corpus.
We filter sentences with OOV words and sentences
longer than 100 tokens. A summary of the addi-
tional data used is shown in Table 2.
We tried to use the best 10%, 20% and 40% of
the synthetic data, where the 40% option worked
best. A summary of the results is given in Table 3.
Although this is our best result for the
English—*German task, it was not submitted, be-
cause the use of the dictionary is not allowed in
the constrained track.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="method">
4 Preprocessing
</sectionHeader>
<subsectionHeader confidence="0.999524">
4.1 Large Parallel Data
</subsectionHeader>
<bodyText confidence="0.999952555555556">
In addition to the provided parallel Europarl and
news-commentary corpora, also the large French-
English news corpus (about 22.5 Mio. sentence
pairs) and the French-English UN corpus (about
7.2 Mio. sentence pairs) were available. Since
model training and tuning with such large cor-
pora takes a very long time, we extracted about
2 Mio. sentence pairs of both of these corpora. We
filter sentences with the following properties:
</bodyText>
<page confidence="0.994435">
95
</page>
<listItem confidence="0.996437571428571">
• Only sentences of minimum length of 4 to-
kens were considered.
• At least 92% of the vocabulary of each sen-
tence occur in the development set.
• The ratio of the vocabulary size of a sen-
tence and the number of its tokens is mini-
mum 80%.
</listItem>
<subsectionHeader confidence="0.98527">
4.2 Morpho-Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.999945615384615">
German, as a flexible and morphologically rich
language, raises a couple of problems in machine
translation. We picked two major problems and
tackled them with morpho-syntactic pre- and post-
processing: compound splitting and long-range
verb reordering.
For the translation from German into English,
German compound words were split using the
frequency-based method described in (Koehn and
Knight, 2003). Thereby, we forbid certain words
and syllables to be split. For the other trans-
lation direction, the English text was first trans-
lated into the modified German language with
split compounds. The generated output was then
postprocessed by re-merging the previously gen-
erated components using the method described in
(Popovi´c et al., 2006).
Additionally, for the German—*English phrase-
based system, the long-range POS-based reorder-
ing rules described in (Popovi´c and Ney, 2006)
were applied on the training and test corpora as a
preprocessing step. Thereby, German verbs which
occur at the end of a clause, like infinitives and
past participles, are moved towards the beginning
of that clause. With this, we improved our baseline
phrase-based system by 0.6% BLEU.
</bodyText>
<sectionHeader confidence="0.987423" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.986871214285714">
For all translation directions, we used the provided
parallel corpora (Europarl, news) to train the trans-
lation models and the monolingual corpora to train
the language models. We improved the French-
English systems by enriching the data with parts of
the large addional data, extracted with the method
described in Section 4.1. Depending on the sys-
tem this gave an improvement of 0.2-0.7% BLEU.
We also made use of the large giga-news as well
as the LDC Gigaword corpora for the French and
English language models. All systems were opti-
mized for BLEU score on the development data,
newstest2008. The newstest2009 data is
used as a blind test set.
In the following, we will give the BLEU scores
for all language tasks of the baseline system and
the best setup for both, the phrase-based and the
hierarchical system. We will use the following
notations to indicate the several methods we used:
(+POS) POS-based verb reordering
(+mero) maximum entropy reordering
(+giga) including giga-news and
LDC Gigaword in LM
(fa) trained by forced alignment
(shallow) allow only shallow rules
We applied system combination of up to 6 sys-
tems with several setups. The submitted systems
are marked in tables 4-7.
</bodyText>
<sectionHeader confidence="0.995322" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999921538461539">
For the participation in the WMT 2010 shared
translation task, RWTH used state-of-the-art
phrase-based and hierarchical translation systems.
To deal with the rich morphology and word or-
der differences in German, compound splitting
and long range verb reordering were applied in a
preprocessing step. For the French-English lan-
guage pairs, RWTH extracted parts of the large
news corpus and the UN corpus as additional
training data. Further, training the phrase trans-
lation model with forced alignment yielded im-
provements in BLEU. To obtain the final hypothe-
sis for the French—*English and German—*English
</bodyText>
<figure confidence="0.940103545454545">
Test
BLEU
Dev
19.2
20.3
19.9
21.0
19.6
20.1
20.2
20.5
</figure>
<page confidence="0.9647865">
20.4
21.4
</page>
<figure confidence="0.997950625">
system combination
Test
BLEU
Dev
phrase-based baseline
phrase-based (+mero)
hierarchical baseline
hierarchical (shallow)
14.8 14.5
15.0 14.7
14.2 13.9
14.5 14.3
phrase-based baseline
phrase-based (+POS+mero+giga)
hierarchical baseline
hierarchical (+giga)
</figure>
<tableCaption confidence="0.999828">
Table 4: Results for the German—*English task.
Table 5: Results for the English—*German task.
</tableCaption>
<page confidence="0.994781">
96
</page>
<reference confidence="0.911617333333333">
chical Translation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 380–388.
P. Koehn and K. Knight. 2003. Empirical Methods for
Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187–194.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33–40.
</reference>
<figure confidence="0.99877448">
BLEU
Dev
Test
BLEU
Dev
Test
phrase-based baseline
phrase-based (fa+giga)
hierarchical baseline
hierarchical (shallow+giga)
system combination
21.8 25.1
23.0 26.1
21.9 25.0
22.7 25.6
23.1 26.1
Table 6: Results for the French-*English task.
20.9 23.2
23.0 24.6
20.6 22.5
22.4 24.3
phrase-based baseline
phrase-based (fa+mero+giga)
hierarchical baseline
hierarchical (shallow,+giga)
</figure>
<tableCaption confidence="0.930045">
Table 7: Results for the English-*French task.
</tableCaption>
<bodyText confidence="0.999650428571429">
language pairs, RWTH applied system combina-
tion. Altogether, by application of these meth-
ods RWTH was able to increase performance in
BLEU by 0.8% for German-*English, 0.2% for
English-*German, 1.0% for French-*English and
1.4% for English-*French on the test set over the
respective baseline systems.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999972666666667">
This work was realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997185">
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201–228.
J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006.
Why Generative Phrase Models Underperform Sur-
face Heuristics. In Proceedings of the Workshop on
Statistical Machine Translation, pages 31–38.
J.G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´es-
Ferrer. 2008. Triplet Lexicon Models for Statisti-
cal Machine Translation. In Proceedings of Emperi-
cal Methods of Natural Language Processing, pages
372–381.
G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne.
2009. Rule Filtering by Pattern for Efficient Hierar-
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222–1237.
A. Mauser, S. Hasan, and H. Ney. 2009. Extend-
ing Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 210–217.
M. Popovi´c and H. Ney. 2006. POS-based Word Re-
orderings for Statistical Machine Translation. In In-
ternational Conference on Language Resources and
Evaluation, pages 1278–1283.
M. Popovi´c, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Nat-
ural Language Processing, Springer Verlag, LNCS,
pages 616–624.
H. Schwenk and J. Senellart. 2009. Translation Model
Adaptation for an Arabic/French News Translation
System by Lightly-Supervised Training. In MT
Summit XII.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics. To ap-
pear.
R. Zens and H. Ney. 2006. Discriminative Reorder-
ing Models for Statistical Machine Translation. In
Workshop on Statistical Machine Translation, pages
55–63.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statis-
tical Machine Translation. In International Work-
shop on Spoken Language Translation.
</reference>
<page confidence="0.999688">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599181">
<title confidence="0.77135">The RWTH Aachen Machine Translation System for WMT 2010</title>
<author confidence="0.9717745">Carmen Heger</author>
<author confidence="0.9717745">Joern Wuebker</author>
<author confidence="0.9717745">Matthias Huck</author>
<author confidence="0.9717745">Gregor Saab Mansour</author>
<author confidence="0.9717745">Daniel Stein</author>
<author confidence="0.9717745">Hermann</author>
<affiliation confidence="0.978715">RWTH Aachen</affiliation>
<address confidence="0.905784">Aachen,</address>
<email confidence="0.998267">surname@cs.rwth-aachen.de</email>
<abstract confidence="0.994647764705882">In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the Fifth Workshop on Statistical Machine Translation. Stateof-the-art phrase-based and hierarchical statistical MT systems are augmented with appropriate morpho-syntactic enhancements, as well as alternative phrase training methods and extended lexicon models. For some tasks, a system combination of the best systems was used to generate a final hypothesis. We participated in the constrained condition of German- English and French-English in each translation direction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>chical Translation</author>
</authors>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>380--388</pages>
<marker>Translation, 2009</marker>
<rawString>chical Translation. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of European Chapter of the ACL (EACL</booktitle>
<pages>187--194</pages>
<contexts>
<context position="12272" citStr="Koehn and Knight, 2003" startWordPosition="1954" endWordPosition="1957">were considered. • At least 92% of the vocabulary of each sentence occur in the development set. • The ratio of the vocabulary size of a sentence and the number of its tokens is minimum 80%. 4.2 Morpho-Syntactic Analysis German, as a flexible and morphologically rich language, raises a couple of problems in machine translation. We picked two major problems and tackled them with morpho-syntactic pre- and postprocessing: compound splitting and long-range verb reordering. For the translation from German into English, German compound words were split using the frequency-based method described in (Koehn and Knight, 2003). Thereby, we forbid certain words and syllables to be split. For the other translation direction, the English text was first translated into the modified German language with split compounds. The generated output was then postprocessed by re-merging the previously generated components using the method described in (Popovi´c et al., 2006). Additionally, for the German—*English phrasebased system, the long-range POS-based reordering rules described in (Popovi´c and Ney, 2006) were applied on the training and test corpora as a preprocessing step. Thereby, German verbs which occur at the end of a</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>P. Koehn and K. Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of European Chapter of the ACL (EACL 2009), pages 187–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="4186" citStr="Matusov et al. (2006)" startWordPosition="637" endWordPosition="641">a, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics German—*English French—*English English—*French BLEU # Phrases BLEU # Phrases BLEU # Phrases Standard 19.7 128M 25.5 225M 23.7 261M FA 20.0 12M 25.9 35M 24.0 33M Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For German—*English and English—*French phrase table interpolation was applied. additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. Alignments between the systems are learned by GIZA++, a one-to-one alignment is generated from the learned state occupation probabilities. From these alignments, a confusion network (CN) is then built using one of the hypotheses as “skeleton” or “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible CNs into a single lattice. Majority voting on the generated lattice is performed usi</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>E. Matusov, N. Ueffing, and H. Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2821" citStr="Chiang, 2007" startWordPosition="421" endWordPosition="422">r-Ney discounting with interpolation. 2.1 Phrase-Based System Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (Zens and Ney, 2006). 2.2 Hierarchical System Our hierarchical phrase-based system is similar to the one described in (Chiang, 2007). It allows for gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step. It has similar features as the phrasebased system mentioned above. For some systems, we only allowed the non-terminals in hierarchical phrases to be substituted with initial phrases as in (Iglesias et al., 2009), which gave better results on some language pairs. We will refer to this as “shallow rules”. 2.3 System Combination The RWTH approach to MT system combination of the French—*English systems as well as the German—*English systems is a refined version of the ROVER appr</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why Generative Phrase Models Underperform Surface Heuristics.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="6293" citStr="DeNero et al., 2006" startWordPosition="985" endWordPosition="988">cased) consensus translation from the CN, we sum up the counts of different casing variants of each word in a sentence over the input hypotheses, and use the majority casing over those. In previous experiments, this showed to work significantly better than using a fixed non-consensus true caser, and maintains flexibility on the input systems. 3 New Additional Models 3.1 Forced Alignment For the German—*English, French—*English and English—*French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm, similar to the one described in (DeNero et al., 2006). Here, the phrase translation probabilities are estimated from their relative frequencies in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. For the language pairs German—*English and English—*French the best results were achieved by log-linear interpolation of the standard phrase table with the generative model. For French—*English we directly used the model tr</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>J. DeNero, D. Gillick, J. Zhang, and D. Klein. 2006. Why Generative Phrase Models Underperform Surface Heuristics. In Proceedings of the Workshop on Statistical Machine Translation, pages 31–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
</authors>
<title>A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER).</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="3447" citStr="Fiscus, 1997" startWordPosition="527" endWordPosition="528">or gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step. It has similar features as the phrasebased system mentioned above. For some systems, we only allowed the non-terminals in hierarchical phrases to be substituted with initial phrases as in (Iglesias et al., 2009), which gave better results on some language pairs. We will refer to this as “shallow rules”. 2.3 System Combination The RWTH approach to MT system combination of the French—*English systems as well as the German—*English systems is a refined version of the ROVER approach in ASR (Fiscus, 1997) with 93 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 93–97, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics German—*English French—*English English—*French BLEU # Phrases BLEU # Phrases BLEU # Phrases Standard 19.7 128M 25.5 225M 23.7 261M FA 20.0 12M 25.9 35M 24.0 33M Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For German—*English and English—*French phrase table interpolation was applied. additional steps to cope with reordering between different hypotheses, </context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J.G. Fiscus. 1997. A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER). In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>J Ganitkevitch</author>
<author>H Ney</author>
<author>J Andr´esFerrer</author>
</authors>
<title>Triplet Lexicon Models for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of Emperical Methods of Natural Language Processing,</booktitle>
<pages>372--381</pages>
<marker>Hasan, Ganitkevitch, Ney, Andr´esFerrer, 2008</marker>
<rawString>S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´esFerrer. 2008. Triplet Lexicon Models for Statistical Machine Translation. In Proceedings of Emperical Methods of Natural Language Processing, pages 372–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Iglesias</author>
<author>A de Gispert</author>
<author>E R Banga</author>
<author>W Byrne</author>
</authors>
<title>Rule Filtering by Pattern for Efficient Hierar-</title>
<date>2009</date>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>G. Iglesias, A. de Gispert, E.R. Banga, and W. Byrne. 2009. Rule Filtering by Pattern for Efficient Hierar-</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>R E Banchs</author>
<author>N Bertoldi</author>
<author>D Dechelotte</author>
<author>M Federico</author>
<author>M Kolss</author>
<author>Y-S Lee</author>
<author>J B Marino</author>
<author>M Paulik</author>
<author>S Roukos</author>
<author>H Schwenk</author>
<author>H Ney</author>
</authors>
<title>System Combination for Machine Translation of Spoken and Written Language.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<contexts>
<context position="4253" citStr="Matusov et al., 2008" startWordPosition="648" endWordPosition="651">nguistics German—*English French—*English English—*French BLEU # Phrases BLEU # Phrases BLEU # Phrases Standard 19.7 128M 25.5 225M 23.7 261M FA 20.0 12M 25.9 35M 24.0 33M Table 1: BLEU scores on Test and phrase table sizes with and without forced alignment (FA). For German—*English and English—*French phrase table interpolation was applied. additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. Alignments between the systems are learned by GIZA++, a one-to-one alignment is generated from the learned state occupation probabilities. From these alignments, a confusion network (CN) is then built using one of the hypotheses as “skeleton” or “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible CNs into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statist</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Dechelotte, Federico, Kolss, Lee, Marino, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee, J.B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System Combination for Machine Translation of Spoken and Written Language. IEEE Transactions on Audio, Speech and Language Processing, 16(7):1222–1237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mauser</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--217</pages>
<contexts>
<context position="7534" citStr="Mauser et al. (2009)" startWordPosition="1181" endWordPosition="1184">ment. A detailed description of the training procedure is given in (Wuebker et al., 2010). Table 1 shows the system performances and phrase table sizes with the standard phrase table and the one trained with forced alignment after the first EM iteration. We can see that the generative model reduces the phrase table size by 85-90% while increasing performance by 0.3% to 0.4% BLEU. 3.2 Extended Lexicon Models In previous work, RWTH was able to show the positive impact of extended lexicon models that cope with lexical context beyond the limited horizon of phrase pairs and n-gram language models. Mauser et al. (2009) report improvements of up to +1% in BLEU on large-scale systems for Chinese—*English and Arabic—*English by incorporating discriminative and trigger-based lexicon models into a state-of-the-art phrase-based decoder. They discuss how the two types of lexicon 94 models help to select content words by capturing long-distance effects. The triplet model is a straightforward extension of the IBM model 1 with a second trigger, and like the former is trained iteratively using the EM algorithm. In search, the triggers are usually on the source side, i.e., p(e|f, f&apos;) is modeled. The pathconstrained tri</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>A. Mauser, S. Hasan, and H. Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In Conference on Empirical Methods in Natural Language Processing, pages 210–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovi´c</author>
<author>H Ney</author>
</authors>
<title>POS-based Word Reorderings for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<pages>1278--1283</pages>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>M. Popovi´c and H. Ney. 2006. POS-based Word Reorderings for Statistical Machine Translation. In International Conference on Language Resources and Evaluation, pages 1278–1283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovi´c</author>
<author>D Stein</author>
<author>H Ney</author>
</authors>
<title>Statistical Machine Translation of German Compound Words.</title>
<date>2006</date>
<booktitle>In FinTAL - 5th International Conference on Natural Language Processing,</booktitle>
<pages>616--624</pages>
<publisher>Springer Verlag, LNCS,</publisher>
<marker>Popovi´c, Stein, Ney, 2006</marker>
<rawString>M. Popovi´c, D. Stein, and H. Ney. 2006. Statistical Machine Translation of German Compound Words. In FinTAL - 5th International Conference on Natural Language Processing, Springer Verlag, LNCS, pages 616–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>J Senellart</author>
</authors>
<title>Translation Model Adaptation for an Arabic/French News Translation System by Lightly-Supervised Training.</title>
<date>2009</date>
<booktitle>In MT Summit XII.</booktitle>
<contexts>
<context position="9423" citStr="Schwenk and Senellart, 2009" startWordPosition="1484" endWordPosition="1487">, and German, on this year’s WMT translation tasks slight improvements on the development sets did not or only partially carry over to the held-out test sets. Nevertheless, systems with triplets were used for system combination, as extended lexicon models often help to predict content words and to capture long-range dependencies. Thus they can help to find a strong consensus hypothesis. 3.3 Unsupervised Training Due to the small size of the English—*German resources available for language modeling as well as for lexicon extraction, we decided to apply the unsupervised adaptation suggested in (Schwenk and Senellart, 2009). We use a baseline SMT system to translate in-domain monolingual source data, filter the translations according to a decoder score normalized by sentence length, add this synthetic bilingual data to the original one and rebuild the SMT system from scratch. The motivation behind the method is that the phrase table will adapt to the genre, and thus let phrases which are domain related have higher probabilities. Two phenomena are observed from phrase tables and the corresponding translations: • Phrase translation probabilities are changed, making the system choose better phrase translation candi</context>
</contexts>
<marker>Schwenk, Senellart, 2009</marker>
<rawString>H. Schwenk and J. Senellart. 2009. Translation Model Adaptation for an Arabic/French News Translation System by Lightly-Supervised Training. In MT Summit XII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wuebker</author>
<author>A Mauser</author>
<author>H Ney</author>
</authors>
<title>Training Phrase Translation Models with Leaving-One-Out.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<note>To appear.</note>
<contexts>
<context position="7003" citStr="Wuebker et al., 2010" startWordPosition="1091" endWordPosition="1094">s in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. For the language pairs German—*English and English—*French the best results were achieved by log-linear interpolation of the standard phrase table with the generative model. For French—*English we directly used the model trained by forced alignment. A detailed description of the training procedure is given in (Wuebker et al., 2010). Table 1 shows the system performances and phrase table sizes with the standard phrase table and the one trained with forced alignment after the first EM iteration. We can see that the generative model reduces the phrase table size by 85-90% while increasing performance by 0.3% to 0.4% BLEU. 3.2 Extended Lexicon Models In previous work, RWTH was able to show the positive impact of extended lexicon models that cope with lexical context beyond the limited horizon of phrase pairs and n-gram language models. Mauser et al. (2009) report improvements of up to +1% in BLEU on large-scale systems for </context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>J. Wuebker, A. Mauser, and H. Ney. 2010. Training Phrase Translation Models with Leaving-One-Out. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="2709" citStr="Zens and Ney, 2006" startWordPosition="403" endWordPosition="406"> with a variant of GIZA++. Target language models are 4-gram language models trained with the SRI toolkit, using Kneser-Ney discounting with interpolation. 2.1 Phrase-Based System Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (Zens and Ney, 2006). 2.2 Hierarchical System Our hierarchical phrase-based system is similar to the one described in (Chiang, 2007). It allows for gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step. It has similar features as the phrasebased system mentioned above. For some systems, we only allowed the non-terminals in hierarchical phrases to be substituted with initial phrases as in (Iglesias et al., 2009), which gave better results on some language pairs. We will refer to this as “shallow rules”. 2.3 System Combination The RWTH approach to MT system combinat</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Workshop on Statistical Machine Translation, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Improvements in Dynamic Programming Beam Search for Phrase-based Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="2361" citStr="Zens and Ney, 2008" startWordPosition="352" endWordPosition="355">system components. Morpho-syntactic analysis and other preprocessing issues are covered by Section 4. Finally, translation results for the different languages and system variants are presented in Section 5. 2 Translation Systems For the WMT 2010 Evaluation we used standard phrase-based and hierarchical translation systems. Alignments were trained with a variant of GIZA++. Target language models are 4-gram language models trained with the SRI toolkit, using Kneser-Ney discounting with interpolation. 2.1 Phrase-Based System Our phrase-based translation system is similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. Additional models include a standard n-gram language model, phrase-level IBM1, word-, phraseand distortion-penalties and a discriminative reordering model as described in (Zens and Ney, 2006). 2.2 Hierarchical System Our hierarchical phrase-based system is similar to the one described in (Chiang, 2007). It allows for gaps in the phrases by employing a context-free grammar and a CYK-like parsing during the decoding step. It has similar feat</context>
</contexts>
<marker>Zens, Ney, 2008</marker>
<rawString>R. Zens and H. Ney. 2008. Improvements in Dynamic Programming Beam Search for Phrase-based Statistical Machine Translation. In International Workshop on Spoken Language Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>