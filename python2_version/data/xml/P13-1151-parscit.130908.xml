<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9988695">
Improving Text Simplification Language Modeling
Using Unsimplified Text Data
</title>
<author confidence="0.996253">
David Kauchak
</author>
<affiliation confidence="0.986501">
Middlebury College
</affiliation>
<address confidence="0.835422">
Middlebury, VT 05753
</address>
<email confidence="0.999088">
dkauchak@middlebury.edu
</email>
<sectionHeader confidence="0.993953" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999320636363637">
In this paper we examine language mod-
eling for text simplification. Unlike some
text-to-text translation tasks, text simplifi-
cation is a monolingual translation task al-
lowing for text in both the input and out-
put domain to be used for training the lan-
guage model. We explore the relation-
ship between normal English and simpli-
fied English and compare language mod-
els trained on varying amounts of text
from each. We evaluate the models intrin-
sically with perplexity and extrinsically
on the lexical simplification task from Se-
mEval 2012. We find that a combined
model using both simplified and normal
English data achieves a 23% improvement
in perplexity and a 24% improvement on
the lexical simplification task over a model
trained only on simple data. Post-hoc anal-
ysis shows that the additional unsimplified
data provides better coverage for unseen
and rare n-grams.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985555555556">
An important component of many text-to-text
translation systems is the language model which
predicts the likelihood of a text sequence being
produced in the output language. In some problem
domains, such as machine translation, the trans-
lation is between two distinct languages and the
language model can only be trained on data in
the output language. However, some problem do-
mains (e.g. text compression, text simplification
and summarization) can be viewed as monolingual
translation tasks, translating between text varia-
tions within a single language. In these monolin-
gual problems, text could be used from both the
input and output domain to train a language model.
In this paper, we investigate this possibility for text
simplification where both simplified English text
and normal English text are available for training
a simple English language model.
Table 1 shows the n-gram overlap proportions
in a sentence aligned data set of 137K sentence
pairs from aligning Simple English Wikipedia and
English Wikipedia articles (Coster and Kauchak,
2011a).1 The data highlights two conflicting
views: does the benefit of additional data out-
weigh the problem of the source of the data?
Throughout the rest of this paper we refer to
sentences/articles/text from English Wikipedia as
normal and sentences/articles/text from Simple
English Wikipedia as simple.
On the one hand, there is a strong correspon-
dence between the simple and normal data. At the
word level 96% of the simple words are found in
the normal corpus and even for n-grams as large as
5, more than half of the n-grams can be found in
the normal text. In addition, the normal text does
represent English text and contains many n-grams
not seen in the simple corpus. This extra informa-
tion may help with data sparsity, providing better
estimates for rare and unseen n-grams.
On the other hand, there is still only modest
overlap between the sentences for longer n-grams,
particularly given that the corpus is sentence-
aligned and that 27% of the sentence pairs in
this aligned data set are identical. If the word
distributions were very similar between simple
and normal text, then the overlap proportions be-
tween the two languages would be similar re-
gardless of which direction the comparison is
made. Instead, we see that the normal text has
more varied language and contains more n-grams.
Previous research has also shown other differ-
ences between simple and normal data sources
that could impact language model performance
including average number of syllables, reading
</bodyText>
<footnote confidence="0.980888">
1http://www.cs.middlebury.edu/˜dkauchak/simplification
</footnote>
<page confidence="0.866704">
1537
</page>
<note confidence="0.9408705">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537–1546,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<table confidence="0.658289">
n-gram size: 1 2 3 4 5
simple in normal 0.96 0.80 0.68 0.61 0.55
normal in simple 0.87 0.68 0.58 0.51 0.46
</table>
<tableCaption confidence="0.863601">
Table 1: The proportion of n-grams that overlap
</tableCaption>
<bodyText confidence="0.996737266666667">
in a corpus of 137K sentence-aligned pairs from
Simple English Wikipedia and English Wikipedia.
complexity, and grammatical complexity (Napoles
and Dredze, 2010; Zhu et al., 2010; Coster and
Kauchak, 2011b). In addition, for some monolin-
gual translation domains, it has been argued that it
is not appropriate to train a language model using
data from the input domain (Turner and Charniak,
2005).
Although this question arises in other monolin-
gual translation domains, text simplification rep-
resents an ideal problem area for analysis. First,
simplified text data is available in reasonable
quantities. Simple English Wikipedia contains
more than 60K articles written in simplified En-
glish. This is not the case for all monolingual
translation tasks (Knight and Marcu, 2002; Cohn
and Lapata, 2009). Second, the quantity of sim-
ple text data available is still limited. After pre-
processing, the 60K articles represents less than
half a million sentences which is orders of mag-
nitude smaller than the amount of normal English
data available (for example the English Gigaword
corpus (David Graff, 2003)). Finally, many recent
text simplification systems have utilized language
models trained only on simplified data (Zhu et al.,
2010; Woodsend and Lapata, 2011; Coster and
Kauchak, 2011a; Wubben et al., 2012); improve-
ments in simple language modeling could translate
into improvements for these systems.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999825215686275">
If we view the normal data as out-of-domain data,
then the problem of combining simple and nor-
mal data is similar to the language model do-
main adaption problem (Suzuki and Gao, 2005),
in particular cross-domain adaptation (Bellegarda,
2004) where a domain-specific model is improved
by incorporating additional general data. Adapta-
tion techniques have been shown to improve lan-
guage modeling performance based on perplexity
(Rosenfeld, 1996) and in application areas such
as speech transcription (Bacchiani and Roark,
2003) and machine translation (Zhao et al., 2004),
though no previous research has examined the lan-
guage model domain adaptation problem for text
simplification. Pan and Yang (2010) provide a sur-
vey on the related problem of domain adaptation
for machine learning (also referred to as “transfer
learning”), which utilizes similar techniques. In
this paper, we explore some basic adaptation tech-
niques, however this paper is not a comparison of
domain adaptation techniques for language mod-
eling. Our goal is more general: to examine the
relationship between simple and normal data and
determine whether normal data is helpful. Previ-
ous domain adaptation research is complementary
to our experiments and could be explored in the
future for additional performance improvements.
Simple language models play a role in a va-
riety of text simplification applications. Many
recent statistical simplification techniques build
upon models from machine translation and uti-
lize a simple language model during simplifica-
tion/decoding both in English (Zhu et al., 2010;
Woodsend and Lapata, 2011; Coster and Kauchak,
2011a; Wubben et al., 2012) and in other lan-
guages (Specia, 2010). Simple English language
models have also been used as predictive features
in other simplification sub-problems such as lexi-
cal simplification (Specia et al., 2012) and predict-
ing text simplicity (Eickhoff et al., 2010).
Due to data scarcity, little research has been
done on language modeling in other monolin-
gual translation domains. For text compression,
most systems are trained on uncompressed data
since the largest text compression data sets con-
tain only a few thousand sentences (Knight and
Marcu, 2002; Galley and McKeown, 2007; Cohn
and Lapata, 2009; Nomoto, 2009). Similarly for
summarization, systems that have employed lan-
guage models trained only on unsummarized text
(Banko et al., 2000; Daume and Marcu, 2002).
</bodyText>
<sectionHeader confidence="0.993695" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999902090909091">
We collected a data set from English Wikipedia
and Simple English Wikipedia with the former
representing normal English and the latter sim-
ple English. Simple English Wikipedia has been
previously used for many text simplification ap-
proaches (Zhu et al., 2010; Yatskar et al., 2010;
Biran et al., 2011; Coster and Kauchak, 2011a;
Woodsend and Lapata, 2011; Wubben et al., 2012)
and has been shown to be simpler than normal En-
glish Wikipedia by both automatic measures and
human perception (Coster and Kauchak, 2011b;
</bodyText>
<page confidence="0.985435">
1538
</page>
<table confidence="0.99846675">
simple normal
sentences 385K 2540K
words 7.15M 64.7M
vocab size 78K 307K
</table>
<tableCaption confidence="0.867832333333333">
Table 2: Summary counts for the simple-normal
article aligned data set consisting of 60K article
pairs.
</tableCaption>
<bodyText confidence="0.956846631578947">
Woodsend and Lapata, 2011). We downloaded all
articles from Simple English Wikipedia then re-
moved stubs, navigation pages and any article that
consisted of a single sentence, resulting in 60K
simple articles.
To partially normalize for content and source
differences we generated a document aligned cor-
pus for our experiments. We extracted the cor-
responding 60K normal articles from English
Wikipedia based on the article title to represent the
normal data. We held out 2K article pairs for use
as a testing set in our experiments. The extracted
data set is available for download online.2
Table 2 shows count statistics for the collected
data set. Although the simple and normal data
contain the same number of articles, because nor-
mal articles tend to be longer and contain more
content, the normal side is an order of magnitude
larger.
</bodyText>
<sectionHeader confidence="0.9565215" genericHeader="method">
4 Language Model Evaluation:
Perplexity
</sectionHeader>
<bodyText confidence="0.999979714285714">
To analyze the impact of data source on simple
English language modeling, we trained language
models on varying amounts of simple data, nor-
mal data, and a combination of the two. For our
first task, we evaluated these language models us-
ing perplexity based on how well they modeled the
simple side of the held-out data.
</bodyText>
<subsectionHeader confidence="0.990152">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999979222222222">
We used trigram language models with interpo-
lated Kneser-Kney discounting trained using the
SRI language modeling toolkit (Stolcke, 2002). To
ensure comparability, all models were closed vo-
cabulary with the same vocabulary set based on
the words that occurred in the simple side of the
training corpus, though similar results were seen
for other vocabulary choices. We generated differ-
ent models by varying the size and type of training
</bodyText>
<footnote confidence="0.523634">
2http://www.cs.middlebury.edu/˜dkauchak/simplification
</footnote>
<figure confidence="0.369744">
0.5M 1M 1.5M 2M 2.5M 3M
total number of sentences
</figure>
<figureCaption confidence="0.675840333333333">
Figure 1: Language model perplexities on the
held-out test data for models trained on increasing
amounts of data.
</figureCaption>
<bodyText confidence="0.587722">
data:
</bodyText>
<listItem confidence="0.7585286">
- simple-only: simple sentences only
- normal-only: normal sentences only
- simple-X+normal: X simple sentences com-
bined with a varying number of normal sen-
tences
</listItem>
<bodyText confidence="0.999989">
To evaluate the language models we calculated
the model perplexity (Chen et al., 1998) on the
simple side of the held-out data. The test set con-
sisted of 2K simple English articles with 7,799
simple sentences and 179K words. Perplexity
measures how likely a model finds a test set, with
lower scores indicating better performance.
</bodyText>
<subsectionHeader confidence="0.996727">
4.2 Perplexity Results
</subsectionHeader>
<bodyText confidence="0.99995835">
Figure 1 shows the language model perplexi-
ties for the three types of models for increasing
amounts of training data. As expected, when
trained on the same amount of data, the language
models trained on simple data perform signifi-
cantly better than language models trained on nor-
mal data. In addition, as we increase the amount of
data, the simple-only model improves more than
the normal-only model.
However, the results also show that the normal
data does have some benefit. The perplexity for
the simple-ALL+normal model, which starts with
all available simple data, continues to improve as
normal data is added resulting in a 23% improve-
ment over the model trained with only simple data
(from a perplexity of 129 down to 100). Even
by itself the normal data does have value. The
normal-only model achieves a slightly better per-
plexity than the simple-only model, though only
by utilizing an order of magnitude more data.
</bodyText>
<figure confidence="0.949967882352941">
simple-only
normal-only
simple-ALL+normal
perplexity 350
300
250
200
150
100
1539
perplexity
300
250
200
150
100
perplexity
130
125
120
115
110
105
100
95
simple-50k+normal
simple-100k+normal
simple-150k+normal
simple-200k+normal
simple-250k+normal
simple-300k+normal
simple-350k+normal
0.5M 1M 1.5M 2M 2.5M
number of additional normal sentences
</figure>
<figureCaption confidence="0.72091775">
Figure 2: Language model perplexities for com-
bined simple-normal models. Each line represents
a model trained on a different amount of simple
data as normal data is added.
</figureCaption>
<bodyText confidence="0.9998397">
To better understand how the amount of sim-
ple and normal data impacts perplexity, Figure 2
shows perplexity scores for models trained on
varying amounts of simple data as we add increas-
ing amounts of normal data. We again see that
normal data is beneficial; regardless of the amount
of simple data, adding normal data improves per-
plexity. This improvement is most beneficial when
simple data is limited. Models trained on less
simple data achieved larger performance increases
than those models trained on more simple data.
Figure 2 also shows again that simple data
is more valuable than normal data. For ex-
ample, the simple-only model trained on 250K
sentences achieves a perplexity of approximately
150. To achieve this same perplexity level start-
ing with 200K simple sentences requires an ad-
ditional 300K normal sentences, or starting with
100K simple sentences an additional 850K normal
sentences.
</bodyText>
<subsectionHeader confidence="0.995117">
4.3 Language Model Adaptation
</subsectionHeader>
<bodyText confidence="0.999958416666667">
In the experiments above, we generated the lan-
guage models by treating the simple and normal
data as one combined corpus. This approach has
the benefit of simplicity, however, better perfor-
mance for combining related corpora has been
seen by domain adaptation techniques which com-
bine the data in more structured ways (Bacchiani
and Roark, 2003). Our goal for this paper is not
to explore domain adaptation techniques, but to
determine if normal data is useful for the simple
language modeling task. However, to provide an-
other dimension for comparison and to understand
</bodyText>
<figure confidence="0.5004445">
simple-only 0.2 0.4 0.6 0.8 normal-only
lambda
</figure>
<figureCaption confidence="0.793043">
Figure 3: Perplexity scores for a linearly interpo-
lated model between the simple-only model and
the normal-only model for varying lambda values.
</figureCaption>
<bodyText confidence="0.9518382">
if domain adaptation techniques may be useful, we
also investigated a linearly interpolated language
model.
A linearly interpolated language model com-
bines the probabilities of two or more language
models as a weighted sum. In our case, the in-
terpolated model combines the simple model esti-
mate, ps(wi|wi−2, wi−1), and the normal model esti-
mate, pn(wi|wi−2, wi−1), linearly (Jelinek and Mer-
cer, 1980; Hsu, 2007):
</bodyText>
<equation confidence="0.9945005">
pinterpolated(wi|wi−2, wi−1) =
A pn(wi|wi−2, wi−1) + (1 − A) ps(wi|wi−2, wi−1)
</equation>
<bodyText confidence="0.999972857142857">
where 0 &gt; λ &gt; 1.
Figure 3 shows perplexity scores for vary-
ing lambda values ranging from the simple-only
model on the left with λ = 0 to the normal-only
model on the right with λ = 1. As with the pre-
vious experiments, adding normal data improves
improves perplexity. In fact, with a lambda of
0.5 (equal weight between the models) the per-
formance is slightly better than the aggregate ap-
proaches above with a perplexity of 98. The re-
sults also highlight the balance between simple
and normal data; normal data is not as good as
simple data and adding too much of it can cause
the results to degrade.
</bodyText>
<sectionHeader confidence="0.765034" genericHeader="method">
5 Language Model Evaluation:
Lexical Simplification
</sectionHeader>
<bodyText confidence="0.999948333333333">
Currently, no automated methods exist for eval-
uating sentence-level or document-level text sim-
plification systems and manual evaluation is time-
consuming, expensive and has not been vali-
dated. Because of these evaluation challenges, we
chose to evaluate the language models extrinsi-
</bodyText>
<page confidence="0.944557">
1540
</page>
<figure confidence="0.69193775">
Word: tight
Context: With the physical market as tight as it has been in memory, silver could fly at any time.
Candidates: constricted, pressurised, low, high-strung, tight
Human ranking: tight, low, constricted, pressurised, high-strung
</figure>
<figureCaption confidence="0.998859">
Figure 4: A lexical substitution example from the SemEval 2012 data set.
</figureCaption>
<bodyText confidence="0.998777615384615">
cally based on the lexical simplification task from
SemEval 2012 (Specia et al., 2012).
Lexical simplification is a sub-problem of the
general text simplification problem (Chandrasekar
and Srinivas, 1997); a sentence is simplified by
substituting words or phrases in the sentence with
“simpler” variations. Lexical simplification ap-
proaches have been shown to improve the read-
ability of texts (Urano, 2000; Leroy et al., 2012),
are useful in domains such as medical texts where
major content changes are restricted, and they may
be useful as a pre- or post-processing step for gen-
eral simplification systems.
</bodyText>
<subsectionHeader confidence="0.887934">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.988101655172414">
Examples from the lexical simplification data set
from SemEval 2012 consist of three parts: w, the
word to be simplified; s1, ..., si−1, w, si+1, ..., sn,
a sentence containing the word; and, r1, r2, ..., rm,
a list of candidate simplifications for w. The goal
of the task is to rank the candidate simplifications
according to their simplicity in the context of the
sentence. Figure 4 shows an example from the
data set. The data set contains a development set
of 300 examples and a test set of 1710 examples.3
For our experiments, we evaluated the models on
the test set.
Given a language model p(·) and a lexical sim-
plification example, we ranked the list of candi-
dates based on the probability the language model
assigns to the sentence with the candidate simplifi-
cation inserted in context. Specifically, we scored
each candidate simplification rj by
p(s1... si−1 rj si+1... sn)
and then ranked them based on this score. For ex-
ample, to calculate the ranking for the example in
Figure 4 we calculate the probability of each of:
With the physical market as constricted as it has been ...
With the physical market as pressurised as it has been ...
With the physical market as low as it has been ...
With the physical market as high-strung as it has been ...
With the physical market as tight as it has been ...
with the language model and then rank them by
their probability. We do not suggest this as a com-
</bodyText>
<equation confidence="0.451485">
3http://www.cs.york.ac.uk/semeval-2012/task1/
0.5M 1M 1.5M 2M 2.5M 3M
</equation>
<figureCaption confidence="0.767324">
total number of sentences
Figure 5: Kappa rank scores for the models trained
on increasing amounts of data.
</figureCaption>
<bodyText confidence="0.999063476190476">
plete lexical substitution system, but it was a com-
mon feature for many of the submitted systems, it
performs well relative to the other systems, and it
allows for a concrete comparison between the lan-
guage models on a simplification task.
To evaluate the rankings, we use the metric from
the SemEval 2012 task, the Cohen’s kappa coeffi-
cient (Landis and Koch, 1977) between the system
ranking and the human ranking, which we denote
the “kappa rank score”. See Specia et al. (2012)
for the full details of how the evaluation metric is
calculated.
We use the same setup for training the language
models as in the perplexity experiments except
the models are open vocabulary instead of closed.
Open vocabulary models allow for the language
models to better utilize the varying amounts of
data and since the lexical simplification problem
only requires a comparison of probabilities within
a given model to produce the final ranking, we do
not need the closed vocabulary requirement.
</bodyText>
<subsectionHeader confidence="0.997608">
5.2 Lexical Simplification Results
</subsectionHeader>
<bodyText confidence="0.999985142857143">
Figure 5 shows the kappa rank scores for the
simple-only, normal-only and combined models.
As with the perplexity results, for similar amounts
of data the simple-only model performs better than
the normal-only model. We also again see that the
performance difference between the two models
grows as the amount of data increases. However,
</bodyText>
<figure confidence="0.950750785714286">
kappa rank score
0.36
0.34
0.32
0.28
0.26
0.24
0.3
simple-only
normal-only
simple-ALL+normal
1541
0.5M 1M 1.5M 2M 2.5M
number of additional normal sentences
</figure>
<figureCaption confidence="0.987956">
Figure 6: Kappa rank scores for models trained
with varying amounts of simple data combined
with increasing amounts of normal data.
</figureCaption>
<bodyText confidence="0.999749222222222">
unlike the perplexity results, simply appending ad-
ditional normal data to the entire simple data set
does not improve the performance of the lexical
simplifier.
To determine if additional normal data im-
proves the performance for models trained on
smaller amounts of simple data, Figure 6 shows
the kappa rank scores for models trained on differ-
ent amounts of simple data as additional normal
data is added. For smaller amounts of simple data
adding normal data does improve the kappa rank
score. For example, a language model trained with
100K simple sentences achieves a score of 0.246
and is improved by almost 40% to 0.344 by adding
all of the additional normal data. Even the perfor-
mance of a model trained with 300K simple sen-
tences is increased by 3% (0.01 improvement in
kappa rank score) by adding normal data.
</bodyText>
<subsectionHeader confidence="0.974427">
5.3 Language Model Adaptation
</subsectionHeader>
<bodyText confidence="0.998424466666667">
The results in the previous section show that
adding normal data to a simple data set can im-
prove the lexical simplifier if the amount of simple
data is limited. To investigate this benefit further,
we again generated linearly interpolated language
models between the simple-only model and the
normal-only model. Figure 7 shows results for the
same experimental design as Figure 6 with vary-
ing amounts of simple and normal data, however,
rather than appending the normal data we trained
the models separately and created a linearly inter-
polated model as described in Section 4.3. The
best lambda was chosen based on a linear search
optimized on the SemEval 2012 development set.
For all starting amounts of simple data, interpo-
</bodyText>
<equation confidence="0.792585">
0.5M 1M 1.5M 2M 2.5M
</equation>
<bodyText confidence="0.644212">
number of additional normal sentences
</bodyText>
<figureCaption confidence="0.7767265">
Figure 7: Kappa rank scores for linearly inter-
polated models between simple-only and normal-
only models trained with varying amounts of sim-
ple and normal data.
</figureCaption>
<bodyText confidence="0.99997465">
lating the simple model with the normal model re-
sults in a large increase in the kappa rank score.
Combining the model trained on all the simple
data with the model trained on all the normal data
achieves a score of 0.419, an improvement of 23%
over the model trained on only simple data. Al-
though our goal was not to create the best lexical
simplification system, this approach would have
ranked 6th out of 11 submitted systems in the
SemEval 2012 competition (Specia et al., 2012).
Interestingly, although the performance of the
simple-only models varied based on the amount of
simple data, when these models are interpolated
with a model trained on normal data, the perfor-
mance tended to converge. This behavior is also
seen in Figure 6, though to a lesser extent. This
may indicate that for some tasks like lexical sim-
plification, only a modest amount of simple data is
required when combining with additional normal
data to achieve reasonable performance.
</bodyText>
<sectionHeader confidence="0.984863" genericHeader="method">
6 Why Does Unsimplified Data Help?
</sectionHeader>
<bodyText confidence="0.999951875">
For both the perplexity experiments and the lexi-
cal simplification experiments, utilizing additional
normal data resulted in large performance im-
provements; using all of the simple data available,
performance is still significantly improved when
combined with normal data. In this section, we
investigate why the additional normal data is ben-
eficial for simple language modeling.
</bodyText>
<subsectionHeader confidence="0.989377">
6.1 More n-grams
</subsectionHeader>
<bodyText confidence="0.997616">
Intuitively, adding normal data provides additional
English data to train on. Most language models are
</bodyText>
<figure confidence="0.982392620689655">
normal-only
simple-100k+normal
simple-150k+normal
simple-200k+normal
simple-250k+normal
simple-300k+normal
simple-350k+normal
simple-ALL+normal
simple-100k
simple-150k
simple-200k
simple-250k
simple-300k
simple-350k
simple-ALL
kappa rank score 0.36
0.34
0.32
0.3
0.28
0.26
0.24
kappa rank score 0.42
0.39
0.36
0.33
0.3
0.27
0.24
</figure>
<page confidence="0.980588">
1542
</page>
<table confidence="0.9994482">
Perplexity test data Lexical simplification
simple normal % inc. simple normal % inc.
1-grams 0.85 0.93 9.4% 0.74 0.78 6.2%
2-grams 0.66 0.82 24% 0.34 0.54 56%
3-grams 0.39 0.57 46% 0.088 0.19 117%
</table>
<tableCaption confidence="0.6021066">
Table 3: Proportion of n-grams in the test sets that
occur in the simple and normal training data sets.
trained using a smoothed version of the maximum
likelihood estimate for an n-gram. For trigrams,
this is:
</tableCaption>
<equation confidence="0.9945525">
count(abc)
p(a|bc) = count(bc)
</equation>
<bodyText confidence="0.999968394736842">
where count(·) is the number of times the n-gram
occurs in the training corpus. For interpolated
and backoff n-gram models, these counts are
smoothed based on the probabilities of lower or-
der n-gram models, which are in-turn calculated
based on counts from the corpus.
We hypothesize that the key benefit of addi-
tional normal data is access to more n-gram counts
and therefore better probability estimation, partic-
ularly for n-grams in the simple corpus that are
unseen or have low frequency. For n-grams that
have never been seen before, the normal data pro-
vides some estimate from English text. This is
particularly important for unigrams (i.e. words)
since there is no lower order model to gain infor-
mation from and most language models assume a
uniform prior on unseen words, treating them all
equally. For n-grams that have been seen but are
rare, the additional normal data can help provide
better probability estimates. Because frequencies
tend to follow a Zipfian distribution, these rare
n-grams make up a large portion of n-grams in
real data (Ha et al., 2003).
To partially validate this hypothesis, we exam-
ined the n-gram overlap between the n-grams in
the training data and the n-grams in the test sets
from the two tasks. Table 3 shows the percentage
of unigrams, bigrams and trigrams from the two
test sets that are found in the simple and normal
training data.
For all n-gram sizes the normal data contained
more test set n-grams than the simple data. Even
at the unigram level, the normal data contained
significantly more of the test set unigrams than the
simple data. On the perplexity data set, the 9.4%
increase in word occurrence between the simple
and normal data set represents an over 50% reduc-
tion in the number of out of vocabulary words. For
</bodyText>
<table confidence="0.9873125">
Perplexity test data simple + Lexical simplification
simple + % inc. over normal % inc. over
normal normal normal
1-grams 0.93 0.2% 0.78 0.0%
2-grams 0.83 0.8% 0.54 1.1%
3-grams 0.58 2.5% 0.20 2.6%
</table>
<tableCaption confidence="0.992614">
Table 4: Proportion of n-grams in the test sets that
</tableCaption>
<bodyText confidence="0.970066444444444">
occur in the combination of both the simple and
normal data.
larger n-grams, the difference between the simple
and normal data sets are even more pronounced.
On the lexical simplification data the normal data
contained more than twice as many test trigrams
as the simple data. These additional n-grams al-
low for better probability estimates on the test data
and therefore better performance on the two tasks.
</bodyText>
<subsectionHeader confidence="0.99955">
6.2 The Role of Normal Data
</subsectionHeader>
<bodyText confidence="0.99980246875">
Estimation of rare events is one component of lan-
guage model performance, but other factors also
impact performance. Table 4 shows the test set
n-gram overlap on the combined data set of simple
and normal data. Because the simple and normal
data come from the same content areas, the simple
data provides little additional coverage if the nor-
mal data is already used. For example, adding the
simple data to the normal data only increases the
number of seen unigrams by 0.2%, representing
only about 600 new words. However, the exper-
iments above showed the combined models per-
formed much better than models trained only on
normal data.
This discrepancy highlights the key problem
with normal data: it is out-of-domain data. While
it shares some characteristics with the simple data,
it represents a different distribution over the lan-
guage. To make this discrepancy more explicit,
we created a sentence aligned data set by align-
ing the simple and normal articles using the ap-
proach from Coster and Kauchak (2011b). This
approach has been previously used for aligning
English Wikipedia and Simple English Wikipedia
with reasonable accuracy. The resulting data set
contains 150K aligned simple-normal sentence
pairs.
Figure 8 shows the perplexity scores for lan-
guage models trained on this data set. Because
the data is aligned and therefore similar, we see
the perplexity curves run parallel to each other as
more data is added. However, even though these
</bodyText>
<page confidence="0.96646">
1543
</page>
<figure confidence="0.4667835">
25K 50K 75K 100K 125K 150K
number of sentences
</figure>
<figureCaption confidence="0.898388333333333">
Figure 8: Language model perplexities for mod-
els trained on increasing data sizes for a simple-
normal sentence aligned data set.
</figureCaption>
<bodyText confidence="0.998470333333333">
sentences represent the same content, the language
use is different between simple and normal and the
normal data performs consistently worse.
</bodyText>
<subsectionHeader confidence="0.998448">
6.3 A Balance Between Simple and Normal
</subsectionHeader>
<bodyText confidence="0.9999933">
Examining the optimal lambda values for the lin-
early interpolated models also helps understand
the role of the normal data. On the perplexity task,
the best perplexity results were obtained with a
lambda of 0.5, or an equal weighting between the
simple and normal models. Even though the nor-
mal data contained six times as many sentences
and nine times as many words, the best model-
ing performance balanced the quality of the simple
model with the coverage of the normal model.
For the simplification task, the optimal lambda
value determined on the development set was 0.98,
with a very strong bias towards the simple model.
Only when the simple model did not provide dif-
ferentiation between lexical choices will the nor-
mal model play a role in selecting the candidates.
For the lexical simplification task, the role of the
normal model is even more clear: to handle rare
occurrences not covered by the simple model and
to smooth the simple model estimates.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999886636363637">
In the experiments above we have shown that on
two different tasks utilizing additional normal data
improves the performance of simple English lan-
guage models. On the perplexity task, the com-
bined model achieved a performance improvement
of 23% over the simple-only model and on the
lexical simplification task, the combined model
achieved a 24% improvement. These improve-
ments are achieved over a simple-only model that
uses all simple English data currently available in
this domain.
For both tasks, the best improvements were
seen when using language model adaptation tech-
niques, however, the adaptation results also indi-
cated that the role of normal data is partially task
dependent. On the perplexity task, the best results
were achieved with an equal weighting between
the simple-only and normal-only model. How-
ever, on the lexical simplification task, the best
results were achieved with a very strong bias to-
wards the simple-only model. For other simplifi-
cation tasks, the optimal parameters will need to
be investigated.
For many of the experiments, combining a
smaller amount of simple data (50K-100K sen-
tences) with normal data achieved results that were
similar to larger simple data set sizes. For ex-
ample, on the lexical simplification task, when
using a linearly interpolated model, the model
combining 100K simple sentences with all the
normal data achieved comparable results to the
model combining all the simple sentences with all
the normal data. This is encouraging for other
monolingual domains such as text compression
or text simplification in non-English languages
where less data is available.
There are still a number of open research ques-
tions related to simple language modeling. First,
further experiments with larger normal data sets
are required to understand the limits of adding
out-of-domain data. Second, we have only uti-
lized data from Wikipedia for normal text. Many
other text sources are available and the impact of
not only size, but also of domain should be in-
vestigated. Third, it still needs to be determined
how language model performance will impact
sentence-level and document-level simplification
approaches. In machine translation, improved
language models have resulted in significant im-
provements in translation performance (Brants et
al., 2007). Finally, in this paper we only in-
vestigated linearly interpolated language models.
Many other domain adaptations techniques exist
and may produce language models with better per-
formance.
</bodyText>
<figure confidence="0.968746571428572">
simple-only-aligned
normal-only-aligned
perplexity 300
250
200
150
100
</figure>
<page confidence="0.991706">
1544
</page>
<sectionHeader confidence="0.988812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998342823529412">
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised language model adaptation. In Proceedings of
ICASSP.
Michele Banko, Vibhu Mittal, and Michael Witbrock.
2000. Headline generation based on statistical trans-
lation. In Proceedings of ACL.
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: Review and perspectives. Speech
Communication.
Or Biran, Samuel Brody, and Noe ´mie Elhadad. 2011.
Putting it simply: A context-aware approach to lexi-
cal simplification. In Proceedings of ACL.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge Based Systems.
Stanley Chen, Douglas Beeferman, and Ronald Rosen-
feld. 1998. Evaluation metrics for language mod-
els. In DARPA Broadcast News Transcription and
Understanding Workshop.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research.
William Coster and David Kauchak. 2011a. Learning
to simplify sentences using Wikipedia. In Proceed-
ings of Text-To-Text Generation.
William Coster and David Kauchak. 2011b. Simple
English Wikipedia: A new text simplification task.
In Proceedings of ACL.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings
of ACL.
Christopher Cieri David Graff. 2003. En-
glish gigaword. http://www.ldc.
upenn.edu/Catalog/CatalogEntry.
jsp?catalogId=LDC2003T05.
Carsten Eickhoff, Pavel Serdyukov, and Arjen P.
de Vries. 2010. Web page classification on child
suitability. In Proceedings of CIKM.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Proceedings of HLT-NAACL.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.
Smith. 2003. Extension of Zipf’s law to word and
character n-grams for English and Chinese. Compu-
tational Linguistics and Chinese Language Process-
ing.
Bo-June Hsu. 2007. Generalized linear interpolation
of language models. In IEEE Workshop on ASRU.
Frederick Jelinek and Robert Mercer. 1980. Interpo-
lated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
ter Recognition in Practice.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics.
Gondy Leroy, James E. Endicott, Obay Mouradi, David
Kauchak, and Melissa Just. 2012. Improving per-
ceived and actual text difficulty for health informa-
tion consumers using semi-automated methods. In
American Medical Informatics Association (AMIA)
Fall Symposium.
Courtney Napoles and Mark Dredze. 2010. Learn-
ing simple Wikipedia: A cogitation in ascertain-
ing abecedarian language. In Proceedings of
HLT/NAACL Workshop on Computation Linguistics
and Writing.
Tadashi Nomoto. 2009. A comparison of model free
versus model intensive approaches to sentence com-
pression. In Proceedings of EMNLP.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering.
Ronald Rosenfeld. 1996. A maximum entropy ap-
proach to adaptive statistical language modeling.
Computer, Speech and Language.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In Joint Conference on Lexical and
Computerational Semantics (*SEM).
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of Computational
Processing of the Portuguese Language.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Hisami Suzuki and Jianfeng Gao. 2005. A compara-
tive study on language model adaptation techniques.
In Proceedings of EMNLP.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of ACL.
Ken Urano. 2000. Lexical simplification and elabo-
ration: Sentence comprehension and incidental vo-
cabulary acquisition. Master’s thesis, University of
Hawaii.
</reference>
<page confidence="0.840787">
1545
</page>
<reference confidence="0.999251611111111">
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of EMNLP.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings ofACL.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Proceedings of NAACL.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of COLING.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of ICCL.
</reference>
<page confidence="0.994201">
1546
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526901">
<title confidence="0.998421">Improving Text Simplification Language Using Unsimplified Text Data</title>
<author confidence="0.728623">David</author>
<affiliation confidence="0.935775">Middlebury</affiliation>
<address confidence="0.946788">Middlebury, VT</address>
<email confidence="0.999347">dkauchak@middlebury.edu</email>
<abstract confidence="0.999862863636364">In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen</abstract>
<intro confidence="0.785234">rare</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Brian Roark</author>
</authors>
<title>Unsupervised language model adaptation.</title>
<date>2003</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="5928" citStr="Bacchiani and Roark, 2003" startWordPosition="920" endWordPosition="923">ements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning (also referred to as “transfer learning”), which utilizes similar techniques. In this paper, we explore some basic adaptation techniques, however this paper is not a comparison of domain adaptation techniques for language modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether no</context>
<context position="13648" citStr="Bacchiani and Roark, 2003" startWordPosition="2142" endWordPosition="2145"> sentences achieves a perplexity of approximately 150. To achieve this same perplexity level starting with 200K simple sentences requires an additional 300K normal sentences, or starting with 100K simple sentences an additional 850K normal sentences. 4.3 Language Model Adaptation In the experiments above, we generated the language models by treating the simple and normal data as one combined corpus. This approach has the benefit of simplicity, however, better performance for combining related corpora has been seen by domain adaptation techniques which combine the data in more structured ways (Bacchiani and Roark, 2003). Our goal for this paper is not to explore domain adaptation techniques, but to determine if normal data is useful for the simple language modeling task. However, to provide another dimension for comparison and to understand simple-only 0.2 0.4 0.6 0.8 normal-only lambda Figure 3: Perplexity scores for a linearly interpolated model between the simple-only model and the normal-only model for varying lambda values. if domain adaptation techniques may be useful, we also investigated a linearly interpolated language model. A linearly interpolated language model combines the probabilities of two o</context>
</contexts>
<marker>Bacchiani, Roark, 2003</marker>
<rawString>Michiel Bacchiani and Brian Roark. 2003. Unsupervised language model adaptation. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu Mittal</author>
<author>Michael Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7793" citStr="Banko et al., 2000" startWordPosition="1207" endWordPosition="1210"> simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.1</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu Mittal, and Michael Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: Review and perspectives. Speech Communication.</title>
<date>2004</date>
<contexts>
<context position="5645" citStr="Bellegarda, 2004" startWordPosition="882" endWordPosition="883">for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning (also referred to as “transfer learning”), which utilizes </context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: Review and perspectives. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>Noe ´mie Elhadad</author>
</authors>
<title>Putting it simply: A context-aware approach to lexical simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8128" citStr="Biran et al., 2011" startWordPosition="1262" endWordPosition="1265">est text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs. Woodsend and Lapata, 2011). We downloaded all articles from Simple English Wikipedia then removed stubs, navigation pages and any article that consisted of a single sentence, resulting in 60K simple art</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and Noe ´mie Elhadad. 2011. Putting it simply: A context-aware approach to lexical simplification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. Knowledge Based Systems.</title>
<date>1997</date>
<contexts>
<context position="16011" citStr="Chandrasekar and Srinivas, 1997" startWordPosition="2521" endWordPosition="2524">as not been validated. Because of these evaluation challenges, we chose to evaluate the language models extrinsi1540 Word: tight Context: With the physical market as tight as it has been in memory, silver could fly at any time. Candidates: constricted, pressurised, low, high-strung, tight Human ranking: tight, low, constricted, pressurised, high-strung Figure 4: A lexical substitution example from the SemEval 2012 data set. cally based on the lexical simplification task from SemEval 2012 (Specia et al., 2012). Lexical simplification is a sub-problem of the general text simplification problem (Chandrasekar and Srinivas, 1997); a sentence is simplified by substituting words or phrases in the sentence with “simpler” variations. Lexical simplification approaches have been shown to improve the readability of texts (Urano, 2000; Leroy et al., 2012), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems. 5.1 Experimental Setup Examples from the lexical simplification data set from SemEval 2012 consist of three parts: w, the word to be simplified; s1, ..., si−1, w, si+1, ..., sn, a sentence contai</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Raman Chandrasekar and Bangalore Srinivas. 1997. Automatic induction of rules for text simplification. Knowledge Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Douglas Beeferman</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Evaluation metrics for language models.</title>
<date>1998</date>
<booktitle>In DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="10659" citStr="Chen et al., 1998" startWordPosition="1666" endWordPosition="1669">g corpus, though similar results were seen for other vocabulary choices. We generated different models by varying the size and type of training 2http://www.cs.middlebury.edu/˜dkauchak/simplification 0.5M 1M 1.5M 2M 2.5M 3M total number of sentences Figure 1: Language model perplexities on the held-out test data for models trained on increasing amounts of data. data: - simple-only: simple sentences only - normal-only: normal sentences only - simple-X+normal: X simple sentences combined with a varying number of normal sentences To evaluate the language models we calculated the model perplexity (Chen et al., 1998) on the simple side of the held-out data. The test set consisted of 2K simple English articles with 7,799 simple sentences and 179K words. Perplexity measures how likely a model finds a test set, with lower scores indicating better performance. 4.2 Perplexity Results Figure 1 shows the language model perplexities for the three types of models for increasing amounts of training data. As expected, when trained on the same amount of data, the language models trained on simple data perform significantly better than language models trained on normal data. In addition, as we increase the amount of d</context>
</contexts>
<marker>Chen, Beeferman, Rosenfeld, 1998</marker>
<rawString>Stanley Chen, Douglas Beeferman, and Ronald Rosenfeld. 1998. Evaluation metrics for language models. In DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="4786" citStr="Cohn and Lapata, 2009" startWordPosition="745" endWordPosition="748">010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Second, the quantity of simple text data available is still limited. After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these syste</context>
<context position="7651" citStr="Cohn and Lapata, 2009" startWordPosition="1186" endWordPosition="1189">a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal E</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to simplify sentences using Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of Text-To-Text Generation.</booktitle>
<contexts>
<context position="2103" citStr="Coster and Kauchak, 2011" startWordPosition="321" endWordPosition="324">summarization) can be viewed as monolingual translation tasks, translating between text variations within a single language. In these monolingual problems, text could be used from both the input and output domain to train a language model. In this paper, we investigate this possibility for text simplification where both simplified English text and normal English text are available for training a simple English language model. Table 1 shows the n-gram overlap proportions in a sentence aligned data set of 137K sentence pairs from aligning Simple English Wikipedia and English Wikipedia articles (Coster and Kauchak, 2011a).1 The data highlights two conflicting views: does the benefit of additional data outweigh the problem of the source of the data? Throughout the rest of this paper we refer to sentences/articles/text from English Wikipedia as normal and sentences/articles/text from Simple English Wikipedia as simple. On the one hand, there is a strong correspondence between the simple and normal data. At the word level 96% of the simple words are found in the normal corpus and even for n-grams as large as 5, more than half of the n-grams can be found in the normal text. In addition, the normal text does repr</context>
<context position="4193" citStr="Coster and Kauchak, 2011" startWordPosition="653" endWordPosition="656">lables, reading 1http://www.cs.middlebury.edu/˜dkauchak/simplification 1537 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537–1546, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics n-gram size: 1 2 3 4 5 simple in normal 0.96 0.80 0.68 0.61 0.55 normal in simple 0.87 0.68 0.58 0.51 0.46 Table 1: The proportion of n-grams that overlap in a corpus of 137K sentence-aligned pairs from Simple English Wikipedia and English Wikipedia. complexity, and grammatical complexity (Napoles and Dredze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Secon</context>
<context position="7030" citStr="Coster and Kauchak, 2011" startWordPosition="1088" endWordPosition="1091">ge modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; </context>
<context position="8341" citStr="Coster and Kauchak, 2011" startWordPosition="1297" endWordPosition="1300">ployed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs. Woodsend and Lapata, 2011). We downloaded all articles from Simple English Wikipedia then removed stubs, navigation pages and any article that consisted of a single sentence, resulting in 60K simple articles. To partially normalize for content and source differences we generated a document aligned corpus for our experiments. We extracted the corresponding 60K normal articles from English Wikipedia based on the a</context>
<context position="27199" citStr="Coster and Kauchak (2011" startWordPosition="4373" endWordPosition="4376">g the simple data to the normal data only increases the number of seen unigrams by 0.2%, representing only about 600 new words. However, the experiments above showed the combined models performed much better than models trained only on normal data. This discrepancy highlights the key problem with normal data: it is out-of-domain data. While it shares some characteristics with the simple data, it represents a different distribution over the language. To make this discrepancy more explicit, we created a sentence aligned data set by aligning the simple and normal articles using the approach from Coster and Kauchak (2011b). This approach has been previously used for aligning English Wikipedia and Simple English Wikipedia with reasonable accuracy. The resulting data set contains 150K aligned simple-normal sentence pairs. Figure 8 shows the perplexity scores for language models trained on this data set. Because the data is aligned and therefore similar, we see the perplexity curves run parallel to each other as more data is added. However, even though these 1543 25K 50K 75K 100K 125K 150K number of sentences Figure 8: Language model perplexities for models trained on increasing data sizes for a simplenormal sen</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011a. Learning to simplify sentences using Wikipedia. In Proceedings of Text-To-Text Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Simple English Wikipedia: A new text simplification task.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2103" citStr="Coster and Kauchak, 2011" startWordPosition="321" endWordPosition="324">summarization) can be viewed as monolingual translation tasks, translating between text variations within a single language. In these monolingual problems, text could be used from both the input and output domain to train a language model. In this paper, we investigate this possibility for text simplification where both simplified English text and normal English text are available for training a simple English language model. Table 1 shows the n-gram overlap proportions in a sentence aligned data set of 137K sentence pairs from aligning Simple English Wikipedia and English Wikipedia articles (Coster and Kauchak, 2011a).1 The data highlights two conflicting views: does the benefit of additional data outweigh the problem of the source of the data? Throughout the rest of this paper we refer to sentences/articles/text from English Wikipedia as normal and sentences/articles/text from Simple English Wikipedia as simple. On the one hand, there is a strong correspondence between the simple and normal data. At the word level 96% of the simple words are found in the normal corpus and even for n-grams as large as 5, more than half of the n-grams can be found in the normal text. In addition, the normal text does repr</context>
<context position="4193" citStr="Coster and Kauchak, 2011" startWordPosition="653" endWordPosition="656">lables, reading 1http://www.cs.middlebury.edu/˜dkauchak/simplification 1537 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537–1546, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics n-gram size: 1 2 3 4 5 simple in normal 0.96 0.80 0.68 0.61 0.55 normal in simple 0.87 0.68 0.58 0.51 0.46 Table 1: The proportion of n-grams that overlap in a corpus of 137K sentence-aligned pairs from Simple English Wikipedia and English Wikipedia. complexity, and grammatical complexity (Napoles and Dredze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Secon</context>
<context position="7030" citStr="Coster and Kauchak, 2011" startWordPosition="1088" endWordPosition="1091">ge modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; </context>
<context position="8341" citStr="Coster and Kauchak, 2011" startWordPosition="1297" endWordPosition="1300">ployed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs. Woodsend and Lapata, 2011). We downloaded all articles from Simple English Wikipedia then removed stubs, navigation pages and any article that consisted of a single sentence, resulting in 60K simple articles. To partially normalize for content and source differences we generated a document aligned corpus for our experiments. We extracted the corresponding 60K normal articles from English Wikipedia based on the a</context>
<context position="27199" citStr="Coster and Kauchak (2011" startWordPosition="4373" endWordPosition="4376">g the simple data to the normal data only increases the number of seen unigrams by 0.2%, representing only about 600 new words. However, the experiments above showed the combined models performed much better than models trained only on normal data. This discrepancy highlights the key problem with normal data: it is out-of-domain data. While it shares some characteristics with the simple data, it represents a different distribution over the language. To make this discrepancy more explicit, we created a sentence aligned data set by aligning the simple and normal articles using the approach from Coster and Kauchak (2011b). This approach has been previously used for aligning English Wikipedia and Simple English Wikipedia with reasonable accuracy. The resulting data set contains 150K aligned simple-normal sentence pairs. Figure 8 shows the perplexity scores for language models trained on this data set. Because the data is aligned and therefore similar, we see the perplexity curves run parallel to each other as more data is added. However, even though these 1543 25K 50K 75K 100K 125K 150K number of sentences Figure 8: Language model perplexities for models trained on increasing data sizes for a simplenormal sen</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011b. Simple English Wikipedia: A new text simplification task. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7817" citStr="Daume and Marcu, 2002" startWordPosition="1211" endWordPosition="1214">problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K </context>
</contexts>
<marker>Daume, Marcu, 2002</marker>
<rawString>Hal Daume and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri David Graff</author>
</authors>
<date>2003</date>
<note>English gigaword. http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2003T05.</note>
<contexts>
<context position="5087" citStr="Graff, 2003" startWordPosition="796" endWordPosition="797">on represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Second, the quantity of simple text data available is still limited. After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>Christopher Cieri David Graff. 2003. English gigaword. http://www.ldc. upenn.edu/Catalog/CatalogEntry. jsp?catalogId=LDC2003T05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten Eickhoff</author>
<author>Pavel Serdyukov</author>
<author>Arjen P de Vries</author>
</authors>
<title>Web page classification on child suitability.</title>
<date>2010</date>
<booktitle>In Proceedings of CIKM.</booktitle>
<marker>Eickhoff, Serdyukov, de Vries, 2010</marker>
<rawString>Carsten Eickhoff, Pavel Serdyukov, and Arjen P. de Vries. 2010. Web page classification on child suitability. In Proceedings of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov grammars for sentence compression.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="7628" citStr="Galley and McKeown, 2007" startWordPosition="1182" endWordPosition="1185">; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to b</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov grammars for sentence compression. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Quan Ha</author>
<author>E I Sicilia-Garcia</author>
<author>Ji Ming</author>
<author>F J Smith</author>
</authors>
<title>Extension of Zipf’s law to word and character n-grams for English and Chinese. Computational Linguistics and Chinese Language Processing.</title>
<date>2003</date>
<contexts>
<context position="24801" citStr="Ha et al., 2003" startWordPosition="3964" endWordPosition="3967">that are unseen or have low frequency. For n-grams that have never been seen before, the normal data provides some estimate from English text. This is particularly important for unigrams (i.e. words) since there is no lower order model to gain information from and most language models assume a uniform prior on unseen words, treating them all equally. For n-grams that have been seen but are rare, the additional normal data can help provide better probability estimates. Because frequencies tend to follow a Zipfian distribution, these rare n-grams make up a large portion of n-grams in real data (Ha et al., 2003). To partially validate this hypothesis, we examined the n-gram overlap between the n-grams in the training data and the n-grams in the test sets from the two tasks. Table 3 shows the percentage of unigrams, bigrams and trigrams from the two test sets that are found in the simple and normal training data. For all n-gram sizes the normal data contained more test set n-grams than the simple data. Even at the unigram level, the normal data contained significantly more of the test set unigrams than the simple data. On the perplexity data set, the 9.4% increase in word occurrence between the simple</context>
</contexts>
<marker>Ha, Sicilia-Garcia, Ming, Smith, 2003</marker>
<rawString>Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J. Smith. 2003. Extension of Zipf’s law to word and character n-grams for English and Chinese. Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
</authors>
<title>Generalized linear interpolation of language models.</title>
<date>2007</date>
<booktitle>In IEEE Workshop on ASRU.</booktitle>
<contexts>
<context position="14477" citStr="Hsu, 2007" startWordPosition="2275" endWordPosition="2276">rstand simple-only 0.2 0.4 0.6 0.8 normal-only lambda Figure 3: Perplexity scores for a linearly interpolated model between the simple-only model and the normal-only model for varying lambda values. if domain adaptation techniques may be useful, we also investigated a linearly interpolated language model. A linearly interpolated language model combines the probabilities of two or more language models as a weighted sum. In our case, the interpolated model combines the simple model estimate, ps(wi|wi−2, wi−1), and the normal model estimate, pn(wi|wi−2, wi−1), linearly (Jelinek and Mercer, 1980; Hsu, 2007): pinterpolated(wi|wi−2, wi−1) = A pn(wi|wi−2, wi−1) + (1 − A) ps(wi|wi−2, wi−1) where 0 &gt; λ &gt; 1. Figure 3 shows perplexity scores for varying lambda values ranging from the simple-only model on the left with λ = 0 to the normal-only model on the right with λ = 1. As with the previous experiments, adding normal data improves improves perplexity. In fact, with a lambda of 0.5 (equal weight between the models) the performance is slightly better than the aggregate approaches above with a perplexity of 98. The results also highlight the balance between simple and normal data; normal data is not as</context>
</contexts>
<marker>Hsu, 2007</marker>
<rawString>Bo-June Hsu. 2007. Generalized linear interpolation of language models. In IEEE Workshop on ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Patter Recognition in Practice.</booktitle>
<contexts>
<context position="14465" citStr="Jelinek and Mercer, 1980" startWordPosition="2270" endWordPosition="2274">for comparison and to understand simple-only 0.2 0.4 0.6 0.8 normal-only lambda Figure 3: Perplexity scores for a linearly interpolated model between the simple-only model and the normal-only model for varying lambda values. if domain adaptation techniques may be useful, we also investigated a linearly interpolated language model. A linearly interpolated language model combines the probabilities of two or more language models as a weighted sum. In our case, the interpolated model combines the simple model estimate, ps(wi|wi−2, wi−1), and the normal model estimate, pn(wi|wi−2, wi−1), linearly (Jelinek and Mercer, 1980; Hsu, 2007): pinterpolated(wi|wi−2, wi−1) = A pn(wi|wi−2, wi−1) + (1 − A) ps(wi|wi−2, wi−1) where 0 &gt; λ &gt; 1. Figure 3 shows perplexity scores for varying lambda values ranging from the simple-only model on the left with λ = 0 to the normal-only model on the right with λ = 1. As with the previous experiments, adding normal data improves improves perplexity. In fact, with a lambda of 0.5 (equal weight between the models) the performance is slightly better than the aggregate approaches above with a perplexity of 98. The results also highlight the balance between simple and normal data; normal da</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Patter Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence.</journal>
<contexts>
<context position="4762" citStr="Knight and Marcu, 2002" startWordPosition="741" endWordPosition="744">dze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Second, the quantity of simple text data available is still limited. After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into impr</context>
<context position="7602" citStr="Knight and Marcu, 2002" startWordPosition="1178" endWordPosition="1181">oodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 201</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics.</journal>
<contexts>
<context position="18391" citStr="Landis and Koch, 1977" startWordPosition="2931" endWordPosition="2934"> model and then rank them by their probability. We do not suggest this as a com3http://www.cs.york.ac.uk/semeval-2012/task1/ 0.5M 1M 1.5M 2M 2.5M 3M total number of sentences Figure 5: Kappa rank scores for the models trained on increasing amounts of data. plete lexical substitution system, but it was a common feature for many of the submitted systems, it performs well relative to the other systems, and it allows for a concrete comparison between the language models on a simplification task. To evaluate the rankings, we use the metric from the SemEval 2012 task, the Cohen’s kappa coefficient (Landis and Koch, 1977) between the system ranking and the human ranking, which we denote the “kappa rank score”. See Specia et al. (2012) for the full details of how the evaluation metric is calculated. We use the same setup for training the language models as in the perplexity experiments except the models are open vocabulary instead of closed. Open vocabulary models allow for the language models to better utilize the varying amounts of data and since the lexical simplification problem only requires a comparison of probabilities within a given model to produce the final ranking, we do not need the closed vocabular</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gondy Leroy</author>
<author>James E Endicott</author>
<author>Obay Mouradi</author>
<author>David Kauchak</author>
<author>Melissa Just</author>
</authors>
<title>Improving perceived and actual text difficulty for health information consumers using semi-automated methods.</title>
<date>2012</date>
<booktitle>In American Medical Informatics Association (AMIA) Fall Symposium.</booktitle>
<contexts>
<context position="16233" citStr="Leroy et al., 2012" startWordPosition="2556" endWordPosition="2559">tes: constricted, pressurised, low, high-strung, tight Human ranking: tight, low, constricted, pressurised, high-strung Figure 4: A lexical substitution example from the SemEval 2012 data set. cally based on the lexical simplification task from SemEval 2012 (Specia et al., 2012). Lexical simplification is a sub-problem of the general text simplification problem (Chandrasekar and Srinivas, 1997); a sentence is simplified by substituting words or phrases in the sentence with “simpler” variations. Lexical simplification approaches have been shown to improve the readability of texts (Urano, 2000; Leroy et al., 2012), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems. 5.1 Experimental Setup Examples from the lexical simplification data set from SemEval 2012 consist of three parts: w, the word to be simplified; s1, ..., si−1, w, si+1, ..., sn, a sentence containing the word; and, r1, r2, ..., rm, a list of candidate simplifications for w. The goal of the task is to rank the candidate simplifications according to their simplicity in the context of the sentence. Figure 4 shows an </context>
</contexts>
<marker>Leroy, Endicott, Mouradi, Kauchak, Just, 2012</marker>
<rawString>Gondy Leroy, James E. Endicott, Obay Mouradi, David Kauchak, and Melissa Just. 2012. Improving perceived and actual text difficulty for health information consumers using semi-automated methods. In American Medical Informatics Association (AMIA) Fall Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Mark Dredze</author>
</authors>
<title>Learning simple Wikipedia: A cogitation in ascertaining abecedarian language.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT/NAACL Workshop on Computation Linguistics and Writing.</booktitle>
<contexts>
<context position="4149" citStr="Napoles and Dredze, 2010" startWordPosition="645" endWordPosition="648"> performance including average number of syllables, reading 1http://www.cs.middlebury.edu/˜dkauchak/simplification 1537 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537–1546, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics n-gram size: 1 2 3 4 5 simple in normal 0.96 0.80 0.68 0.61 0.55 normal in simple 0.87 0.68 0.58 0.51 0.46 Table 1: The proportion of n-grams that overlap in a corpus of 137K sentence-aligned pairs from Simple English Wikipedia and English Wikipedia. complexity, and grammatical complexity (Napoles and Dredze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight an</context>
</contexts>
<marker>Napoles, Dredze, 2010</marker>
<rawString>Courtney Napoles and Mark Dredze. 2010. Learning simple Wikipedia: A cogitation in ascertaining abecedarian language. In Proceedings of HLT/NAACL Workshop on Computation Linguistics and Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>A comparison of model free versus model intensive approaches to sentence compression.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7666" citStr="Nomoto, 2009" startWordPosition="1190" endWordPosition="1191"> and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedi</context>
</contexts>
<marker>Nomoto, 2009</marker>
<rawString>Tadashi Nomoto. 2009. A comparison of model free versus model intensive approaches to sentence compression. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering.</journal>
<contexts>
<context position="6104" citStr="Pan and Yang (2010)" startWordPosition="947" endWordPosition="950">simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning (also referred to as “transfer learning”), which utilizes similar techniques. In this paper, we explore some basic adaptation techniques, however this paper is not a comparison of domain adaptation techniques for language modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simpl</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language.</journal>
<contexts>
<context position="5846" citStr="Rosenfeld, 1996" startWordPosition="910" endWordPosition="911">nd Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning (also referred to as “transfer learning”), which utilizes similar techniques. In this paper, we explore some basic adaptation techniques, however this paper is not a comparison of domain adaptation techniques for language modeling. Our goal is more general: t</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay Kumar Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<date>2012</date>
<booktitle>Semeval-2012 task 1: English lexical simplification. In Joint Conference on Lexical and Computerational Semantics (*SEM).</booktitle>
<contexts>
<context position="7256" citStr="Specia et al., 2012" startWordPosition="1123" endWordPosition="1126">xplored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from</context>
<context position="15893" citStr="Specia et al., 2012" startWordPosition="2506" endWordPosition="2509">evel or document-level text simplification systems and manual evaluation is timeconsuming, expensive and has not been validated. Because of these evaluation challenges, we chose to evaluate the language models extrinsi1540 Word: tight Context: With the physical market as tight as it has been in memory, silver could fly at any time. Candidates: constricted, pressurised, low, high-strung, tight Human ranking: tight, low, constricted, pressurised, high-strung Figure 4: A lexical substitution example from the SemEval 2012 data set. cally based on the lexical simplification task from SemEval 2012 (Specia et al., 2012). Lexical simplification is a sub-problem of the general text simplification problem (Chandrasekar and Srinivas, 1997); a sentence is simplified by substituting words or phrases in the sentence with “simpler” variations. Lexical simplification approaches have been shown to improve the readability of texts (Urano, 2000; Leroy et al., 2012), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems. 5.1 Experimental Setup Examples from the lexical simplification data set from</context>
<context position="18506" citStr="Specia et al. (2012)" startWordPosition="2951" endWordPosition="2954">/task1/ 0.5M 1M 1.5M 2M 2.5M 3M total number of sentences Figure 5: Kappa rank scores for the models trained on increasing amounts of data. plete lexical substitution system, but it was a common feature for many of the submitted systems, it performs well relative to the other systems, and it allows for a concrete comparison between the language models on a simplification task. To evaluate the rankings, we use the metric from the SemEval 2012 task, the Cohen’s kappa coefficient (Landis and Koch, 1977) between the system ranking and the human ranking, which we denote the “kappa rank score”. See Specia et al. (2012) for the full details of how the evaluation metric is calculated. We use the same setup for training the language models as in the perplexity experiments except the models are open vocabulary instead of closed. Open vocabulary models allow for the language models to better utilize the varying amounts of data and since the lexical simplification problem only requires a comparison of probabilities within a given model to produce the final ranking, we do not need the closed vocabulary requirement. 5.2 Lexical Simplification Results Figure 5 shows the kappa rank scores for the simple-only, normal-</context>
<context position="21944" citStr="Specia et al., 2012" startWordPosition="3520" endWordPosition="3523">scores for linearly interpolated models between simple-only and normalonly models trained with varying amounts of simple and normal data. lating the simple model with the normal model results in a large increase in the kappa rank score. Combining the model trained on all the simple data with the model trained on all the normal data achieves a score of 0.419, an improvement of 23% over the model trained on only simple data. Although our goal was not to create the best lexical simplification system, this approach would have ranked 6th out of 11 submitted systems in the SemEval 2012 competition (Specia et al., 2012). Interestingly, although the performance of the simple-only models varied based on the amount of simple data, when these models are interpolated with a model trained on normal data, the performance tended to converge. This behavior is also seen in Figure 6, though to a lesser extent. This may indicate that for some tasks like lexical simplification, only a modest amount of simple data is required when combining with additional normal data to achieve reasonable performance. 6 Why Does Unsimplified Data Help? For both the perplexity experiments and the lexical simplification experiments, utiliz</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In Joint Conference on Lexical and Computerational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Translating from complex to simplified sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of Computational Processing of the Portuguese Language.</booktitle>
<contexts>
<context position="7091" citStr="Specia, 2010" startWordPosition="1101" endWordPosition="1102">en simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summariza</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>Lucia Specia. 2010. Translating from complex to simplified sentences. In Proceedings of Computational Processing of the Portuguese Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP.</booktitle>
<contexts>
<context position="9885" citStr="Stolcke, 2002" startWordPosition="1549" endWordPosition="1550"> tend to be longer and contain more content, the normal side is an order of magnitude larger. 4 Language Model Evaluation: Perplexity To analyze the impact of data source on simple English language modeling, we trained language models on varying amounts of simple data, normal data, and a combination of the two. For our first task, we evaluated these language models using perplexity based on how well they modeled the simple side of the held-out data. 4.1 Experimental Setup We used trigram language models with interpolated Kneser-Kney discounting trained using the SRI language modeling toolkit (Stolcke, 2002). To ensure comparability, all models were closed vocabulary with the same vocabulary set based on the words that occurred in the simple side of the training corpus, though similar results were seen for other vocabulary choices. We generated different models by varying the size and type of training 2http://www.cs.middlebury.edu/˜dkauchak/simplification 0.5M 1M 1.5M 2M 2.5M 3M total number of sentences Figure 1: Language model perplexities on the held-out test data for models trained on increasing amounts of data. data: - simple-only: simple sentences only - normal-only: normal sentences only -</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An extensible language modeling toolkit. In Proceedings of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisami Suzuki</author>
<author>Jianfeng Gao</author>
</authors>
<title>A comparative study on language model adaptation techniques.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5587" citStr="Suzuki and Gao, 2005" startWordPosition="874" endWordPosition="877">ude smaller than the amount of normal English data available (for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning </context>
</contexts>
<marker>Suzuki, Gao, 2005</marker>
<rawString>Hisami Suzuki and Jianfeng Gao. 2005. A comparative study on language model adaptation techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4384" citStr="Turner and Charniak, 2005" startWordPosition="685" endWordPosition="688">Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics n-gram size: 1 2 3 4 5 simple in normal 0.96 0.80 0.68 0.61 0.55 normal in simple 0.87 0.68 0.58 0.51 0.46 Table 1: The proportion of n-grams that overlap in a corpus of 137K sentence-aligned pairs from Simple English Wikipedia and English Wikipedia. complexity, and grammatical complexity (Napoles and Dredze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Second, the quantity of simple text data available is still limited. After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than t</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Urano</author>
</authors>
<title>Lexical simplification and elaboration: Sentence comprehension and incidental vocabulary acquisition. Master’s thesis,</title>
<date>2000</date>
<institution>University of Hawaii.</institution>
<contexts>
<context position="16212" citStr="Urano, 2000" startWordPosition="2554" endWordPosition="2555">time. Candidates: constricted, pressurised, low, high-strung, tight Human ranking: tight, low, constricted, pressurised, high-strung Figure 4: A lexical substitution example from the SemEval 2012 data set. cally based on the lexical simplification task from SemEval 2012 (Specia et al., 2012). Lexical simplification is a sub-problem of the general text simplification problem (Chandrasekar and Srinivas, 1997); a sentence is simplified by substituting words or phrases in the sentence with “simpler” variations. Lexical simplification approaches have been shown to improve the readability of texts (Urano, 2000; Leroy et al., 2012), are useful in domains such as medical texts where major content changes are restricted, and they may be useful as a pre- or post-processing step for general simplification systems. 5.1 Experimental Setup Examples from the lexical simplification data set from SemEval 2012 consist of three parts: w, the word to be simplified; s1, ..., si−1, w, si+1, ..., sn, a sentence containing the word; and, r1, r2, ..., rm, a list of candidate simplifications for w. The goal of the task is to rank the candidate simplifications according to their simplicity in the context of the sentenc</context>
</contexts>
<marker>Urano, 2000</marker>
<rawString>Ken Urano. 2000. Lexical simplification and elaboration: Sentence comprehension and incidental vocabulary acquisition. Master’s thesis, University of Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5245" citStr="Woodsend and Lapata, 2011" startWordPosition="817" endWordPosition="820">tains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Cohn and Lapata, 2009). Second, the quantity of simple text data available is still limited. After preprocessing, the 60K articles represents less than half a million sentences which is orders of magnitude smaller than the amount of normal English data available (for example the English Gigaword corpus (David Graff, 2003)). Finally, many recent text simplification systems have utilized language models trained only on simplified data (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012); improvements in simple language modeling could translate into improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996</context>
<context position="7004" citStr="Woodsend and Lapata, 2011" startWordPosition="1084" endWordPosition="1087">ation techniques for language modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; </context>
<context position="8552" citStr="Woodsend and Lapata, 2011" startWordPosition="1330" endWordPosition="1333">enting normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs. Woodsend and Lapata, 2011). We downloaded all articles from Simple English Wikipedia then removed stubs, navigation pages and any article that consisted of a single sentence, resulting in 60K simple articles. To partially normalize for content and source differences we generated a document aligned corpus for our experiments. We extracted the corresponding 60K normal articles from English Wikipedia based on the article title to represent the normal data. We held out 2K article pairs for use as a testing set in our experiments. The extracted data set is available for download online.2 Table 2 shows count statistics for t</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL.</booktitle>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="8108" citStr="Yatskar et al., 2010" startWordPosition="1258" endWordPosition="1261">ed data since the largest text compression data sets contain only a few thousand sentences (Knight and Marcu, 2002; Galley and McKeown, 2007; Cohn and Lapata, 2009; Nomoto, 2009). Similarly for summarization, systems that have employed language models trained only on unsummarized text (Banko et al., 2000; Daume and Marcu, 2002). 3 Corpus We collected a data set from English Wikipedia and Simple English Wikipedia with the former representing normal English and the latter simple English. Simple English Wikipedia has been previously used for many text simplification approaches (Zhu et al., 2010; Yatskar et al., 2010; Biran et al., 2011; Coster and Kauchak, 2011a; Woodsend and Lapata, 2011; Wubben et al., 2012) and has been shown to be simpler than normal English Wikipedia by both automatic measures and human perception (Coster and Kauchak, 2011b; 1538 simple normal sentences 385K 2540K words 7.15M 64.7M vocab size 78K 307K Table 2: Summary counts for the simple-normal article aligned data set consisting of 60K article pairs. Woodsend and Lapata, 2011). We downloaded all articles from Simple English Wikipedia then removed stubs, navigation pages and any article that consisted of a single sentence, resulti</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5972" citStr="Zhao et al., 2004" startWordPosition="927" endWordPosition="930">nto improvements for these systems. 2 Related Work If we view the normal data as out-of-domain data, then the problem of combining simple and normal data is similar to the language model domain adaption problem (Suzuki and Gao, 2005), in particular cross-domain adaptation (Bellegarda, 2004) where a domain-specific model is improved by incorporating additional general data. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al., 2004), though no previous research has examined the language model domain adaptation problem for text simplification. Pan and Yang (2010) provide a survey on the related problem of domain adaptation for machine learning (also referred to as “transfer learning”), which utilizes similar techniques. In this paper, we explore some basic adaptation techniques, however this paper is not a comparison of domain adaptation techniques for language modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adapta</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of ICCL.</booktitle>
<contexts>
<context position="4167" citStr="Zhu et al., 2010" startWordPosition="649" endWordPosition="652">rage number of syllables, reading 1http://www.cs.middlebury.edu/˜dkauchak/simplification 1537 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1537–1546, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics n-gram size: 1 2 3 4 5 simple in normal 0.96 0.80 0.68 0.61 0.55 normal in simple 0.87 0.68 0.58 0.51 0.46 Table 1: The proportion of n-grams that overlap in a corpus of 137K sentence-aligned pairs from Simple English Wikipedia and English Wikipedia. complexity, and grammatical complexity (Napoles and Dredze, 2010; Zhu et al., 2010; Coster and Kauchak, 2011b). In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). Although this question arises in other monolingual translation domains, text simplification represents an ideal problem area for analysis. First, simplified text data is available in reasonable quantities. Simple English Wikipedia contains more than 60K articles written in simplified English. This is not the case for all monolingual translation tasks (Knight and Marcu, 2002; Coh</context>
<context position="6977" citStr="Zhu et al., 2010" startWordPosition="1080" endWordPosition="1083">on of domain adaptation techniques for language modeling. Our goal is more general: to examine the relationship between simple and normal data and determine whether normal data is helpful. Previous domain adaptation research is complementary to our experiments and could be explored in the future for additional performance improvements. Simple language models play a role in a variety of text simplification applications. Many recent statistical simplification techniques build upon models from machine translation and utilize a simple language model during simplification/decoding both in English (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011a; Wubben et al., 2012) and in other languages (Specia, 2010). Simple English language models have also been used as predictive features in other simplification sub-problems such as lexical simplification (Specia et al., 2012) and predicting text simplicity (Eickhoff et al., 2010). Due to data scarcity, little research has been done on language modeling in other monolingual translation domains. For text compression, most systems are trained on uncompressed data since the largest text compression data sets contain only a few thousand sentence</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of ICCL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>