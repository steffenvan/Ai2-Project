<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031878">
<title confidence="0.913271">
MANA for the Ageing
</title>
<author confidence="0.88533">
David M W Powers, Martin H Luerssen, Trent W Lewis, Richard E Leibbrandt,
</author>
<note confidence="0.551813333333333">
Marissa Milne, John Pashalis and Kenneth Treharne
AI Lab, School of Computer Science, Engineering and Mathematics,
Flinders University, South Australia
</note>
<email confidence="0.994172">
David.Powers@flinders.edu.au
</email>
<sectionHeader confidence="0.993792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999463944444444">
We present a family of Embodied Conversa-
tional Agents (ECAs) using Talking Head
technology, along with a program of associ-
ated research and user trials. Whilst antece-
dents of our current ECAs include “chatbots”
desgined to pass the Turing Test (TT) or win
a Loebner Prize (LP), our current agents are
task-oriented Teaching Agents and Social
Companions. The current focus for our re-
search includes the role of emotion, expres-
sion and gesture in our agents/companions,
the explicit teaching of such social skills as
recognizing and displaying appropriate ex-
pressions/gestures, and the integration of
template/database-based dialogue managers
with more conversational TT/LP systems as
well as with audio-visual speech/gesture rec-
ognition/synthesis technologies.
</bodyText>
<sectionHeader confidence="0.998972" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996486">
Embodied Conversational Agents (ECAs) are
animated or robotic agents that engage users in
real-time dialogue. As a development of the
Chatterbot TT/LP system, they address a funda-
mental criticism of the Turing Test (TT) as
incarnated in the Loebner Prize (LP), viz. the
lack of understanding of the world, the lack of
understanding people, the lack of personality
(Harnad,1992; Shapiro,1992). This has in fact
been acknowledge by Loebner who has insisted
that more than “pen pal” conversation is neces-
sary to win his $100K prize and Gold medal, and
arranged design of a multimodal test [3]. At a
technological level ECAs are a showcase for a
large variety of language and human interface
technologies including speech and face recogni-
tion and synthesis, speech understanding and
generation, and dialogue management. How-
ever, at a deeper level they are a platform for
exploring affect – the effect of multimodal fea-
tures, including in particular expression and ges-
ture on the human user.
Our aim is not to pass the Turing Test, al-
though perhaps some descendant of our system
will eventually do so. Rather our focus is to pro-
vide an effective agent for specific tasks where
the limitations of current conversational compan-
ions, or dialog technologies, serve to match
rather than conflict with the application con-
straints. Whereas limiting the topic was seen as
a trick and a cheat in the Loebner Prize, our aim
is to demonstrate and develop useful technolo-
gies and we are not interested in philosophical
debates about intelligence. For these naturally
constrained applications human level grammati-
cal and syntactic understanding is not required,
and the simple ELIZA-like approach of template
matching is perfectly adequate as a first step
(Weizenbaum, 1966).
Our initial Talking Head was based around the
Stelarc Prosthetic Head1 which combines multi-
ple off-the-shelf components: keyboard input to a
chatbot (AliceBot2) is linked to speech synthesis
(IBM ViaVoice3) and 3D face rendering (Eye-
matic4). More recently we have adopted Head X5
which is capable of generating a continuous,
synchronized, optionally subtitled audiovisual
speech stream in many different languages, with
the ability to switch and modify voices and
morph different faces at the same time as inter-
acting with the user. The system is designed to
be able to use different speech and face tech-
nologies, and we in general use Microsoft’s
SAPI6 for speech recognition and generation plus
the FaceGen face generation technology7.
</bodyText>
<footnote confidence="0.999182142857143">
1 http://www.stelarc.va.com.au/prosthetichead/
2 http://www.alicebot.org/about.html
3 http://www.ibm.com/software/pervasive/viavoice.html
4 http://google.about.com/od/n/g/nevenvisiondef.htm
5 http://csem.flinders.edu.au/research/programs/th/
6 http://msdn.microsoft.com/speech
7 http://www.facegen.com
</footnote>
<page confidence="0.994257">
7
</page>
<note confidence="0.8297395">
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 7–12,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.95252" genericHeader="method">
2 Teaching ECA Applications
</sectionHeader>
<bodyText confidence="0.999988555555556">
We have been predominantly exploring the ap-
plication of our Talking Head as a virtual tutor of
various subject areas. Initially our focus was lan-
guage teaching/learning, but more recently de-
mand for assistance with social teaching and as-
sistant/companion applications has redirected our
efforts.
The Talking Head has been extended for
teaching and environmental/social interaction
purposes with intelligent software that integrates
inputs from various input sources such as cam-
eras, microphones, touch sensors, and the like. A
situational model is constructed that represents
the physical environment in which encounters
with the user take place. A teaching application
can monitor a student’s spoken utterances using
both audio and video, can try to identify the stu-
dent’s facial expressions, and can make reference
to physical objects in the surroundings (including
specially-devised teaching ‘props’).
In addition to spoken utterances (the principal
mode of output used in these applications), the
Head may make use of audiovisual content pre-
sented on additional computer monitors and pro-
vide non-linguistic output that involves other
sensory modalities, e.g. by making use of haptic
devices. The multimodal capabilities of our ECA
Teaching Agent are particularly valuable as they
allow tutor and student to ground their interac-
tion in a shared physical and social environment.
Another invaluable aspect of our ECA for lan-
guage teaching is the ability to model a student
speaking the target language with a correct ac-
cent and authentic facial expression and gestures,
with their own face and voice.
It is important in teaching, and in particular in
language teaching, not to give the student any
examples of incorrect or poor grammar, accent,
etc. In a classroom context, students are held
back and given poor example by other students,
as well as by teachers who are not native speak-
ers. Seeing or hearing their own incorrect writ-
ten or spoken examples is immensely counter-
productive. A good language teacher will reflect
back, with appropriate degree of inflectional and
gestural approbation, what they have said in cor-
rected form. Having a close-up face as well as a
voice to emulate allows unconscious recognition
of the cultural and linguistic characteristics that
are part of language, including the way of hold-
ing the mouth that affects even the way a person
pauses or pronounces a neutral vowel sound, as
well as the whole vowel system. With languages
that have new consonants or vowels, or different
variants that are treated as allophonic in their
first language, seeing how those sounds are made
can be very important to achieving an authentic
accent. Body language, hand gesture, volume
and tone, are all parts of this that are beyond the
competence of current speech recognition and
synthesis. This ability for our ECA to control
vocal and gestural ‘accent’ is thus a primary fo-
cus of our research.
One specific application of the Language
Teaching Agent is for teaching children with a
partial or complete hearing impairment to speak
and lipread, where the face rather than the voice
is their primary cue. A related one is for teaching
corresponding speaking and signing skills to
their families. A third is for teaching literacy to
indigenous children who have reasonable verbal
competence in English (in our case) as a national
language, as well as their tribal language and
often a trade language as a first and second lan-
guage.
Preliminary trials with comprehension testing
found that appropriate facial expressions could
enhance performance by a full grade point (Re-
lated-reference, 2008). However, it also identi-
fied that inappropriate expressions could negate
this advantage – in particular it seemed that in
one case the ECA was seen as laughing at rather
than laughing with the subject matter. This has
required us to modify our emotion model to in-
clude humour with both positive and negative
affect. Moreover the emotional markup was per-
formed by hand by one of the authors. We are
currently engaged in a complex sequence of
staged trials to develop appropriate ways of elic-
iting the desired AV expressions, getting multi-
ple people to markup the texts, getting multiple
subjects to classify and evaluate both real and
head expressions, prior to undertaking a more
comprehensive range of evaluations with the
newly developed texts and markups, as well as a
human head baseline. Currently there is very
little in the way of audiovisual (as opposed to
single image only) corpora of spontaneous or
acted emotions and expressions.
</bodyText>
<page confidence="0.99635">
8
</page>
<figureCaption confidence="0.9975005">
Figure 1. Example of FaceGen morphing: female to male. Morphing is also used to provide
speech gestures/visemes, emotion gestures/expressions, as well as explicit gestures like winks.
</figureCaption>
<subsectionHeader confidence="0.994021">
2.1 Social Tutors for Children
</subsectionHeader>
<bodyText confidence="0.99980559375">
Once we started working with organizations that
provided assistance to those with various dis-
abilities and disadvantages, a major common
factor emerged: the social problems that go with
the disability or with looking different, or even
just being from a different social or cultural
background. Social skills tutoring of children
with autism, hearing impairment and other disor-
ders looks to be a promising application of our
ECA Teaching Agent, which can accurately
model facial expressions, and whose appearance
and interactions can be customized to meet
learners’ needs. Initially we have focused on
children with Autism Spectrum Disorders and
our initial trials are in this ASD community.
Individuals with autism typically lack the
skills needed to participate successfully in every-
day social interactions, particularly reading non-
verbal cues. Additionally, sufferers often feel
more comfortable learning through technology
than with other people, who may be judgmental
or unpredictable.
Two lesson sequences reflecting common dif-
ficulties for children with autism were devel-
oped, the first on basic conversation skills and
the second on managing bullying. There was a
54% average improvement from pre- to post-
testing for the managing bullying module and a
32% average improvement for the conversation
skills module, showing clearly that learning can
take place through this method (Related-
Reference, 2009).
</bodyText>
<sectionHeader confidence="0.990328" genericHeader="method">
3 Independent Living for the Ageing
</sectionHeader>
<bodyText confidence="0.999834833333333">
The Memory, Appointment and Navigation As-
sistant (MANA) system is a broad project to as-
sist elderly people, and those suffering from de-
mentia or other ailments, with independent living
in the privacy of their own home and the dignity
of an ongoing personal life style.
</bodyText>
<subsectionHeader confidence="0.994169">
3.1 MANA Calendar
</subsectionHeader>
<bodyText confidence="0.99999465">
The initial MANA Calendar application util-
izes Head X to provide a talking head companion
with an interface to Google Calendar, allowing
doctors/carers to enter appointments/events that
are provided to patients by the Head on a flexible
reminder schedule. Eventually, it will provide
localized assistance on how to get to the ap-
pointment based on public timetables, trip-
planners and previous visits, but currently this
information is supplied by carers.
The initial Calendar application of the MANA
system was developed in 2009 based on prelimi-
nary input from an Alzheimer’s Association for
deployment in the homes of Alzheimer’s suffer-
ers. A preliminary exploration of potential faces
and voices was conducted using a focus group
approach organized through the NGO. For this
preliminary stage we developed a dozen repre-
sentative face/voice/script combinations and had
representatives of the community select (indi-
vidually and anonymously) their preferred face
and voice. In associated discussion, it was appar-
ent that a major influence was how authoritative
the ECA appeared, and this was influenced by
both face and voice (as well as the accent as their
were only a couple of high quality voices avail-
able for each of the different accents). Some
comments indicated that the person was too
young or not serious enough, while positive
comments were along the lines of that’s matron,
or an orderly, or that’s someone authoritative –
I’d do what they told me. At a later stage, if we
have funds for a comprehensive study, it would
be interesting to examine this formally, but for
now we believe our “experts” and have devel-
oped our trial around the two most popular and
authoritative male and female faces and voices.
As a final stage, we dynamically combined and
altered their preferred faces to achieve those
characteristics preferred by the group.
</bodyText>
<page confidence="0.990505">
9
</page>
<figureCaption confidence="0.999599">
Figure 2. Four MANA faces selected by focus group.
</figureCaption>
<bodyText confidence="0.969810947368421">
These top four faces (Fig. 2) and the top four
voices are those from which subjects are allowed
to select the ECA for their trial. As our aim is to
show the ECA in the best possible light, we aim
to please and give the subject control over who it
is they are inviting into their home – and they do
seem to treat it as a person they are inviting.
The system comprises the following major
components (Self-Reference,2010):
Web Calendar Appointment Interface: Essen-
tially this interface works virtually identical to a
standard Google calendar, where a doctor/carer
can enter an appointment/event. The MANA
Calendar then extracts the key aspects of the
event (i.e: time, date, name, etc) and relays the
information to the Calendar Manager.
Calendar Manager and Synapse Module: The
central Calendar Manager converts the informa-
tion into a coherent human-like message to be
delivered by the Thinking Head, upon either a set
reminder time or upon a person-event. As
Synapse is used by system modules, intermodule
communications ensure concurrent productions,
e.g. the timing of voice audio and visemes (visual
phonemes), appear as human-like as possible.
Thinking Head and SAPI/Mary Integration: This
new Thinking Head was designed using Face-
GenTM software and incorporates Mary and
Nuance voices, giving greater flexibility than
using the original Stelarc face and voice.
Face Detection and Motion Analysis Module:
The system uses a camera which monitors the
space the subject moves around in (or a part of
it), and triggers upon detecting sufficient motion
energy for a human body and a human face (us-
ing the algorithm of Viola &amp; Jones (2004)). On
detecting such a “person-event”, the appointment
message is then delivered to the subject.
</bodyText>
<subsectionHeader confidence="0.597902">
Speech Recognition Trigger Module: At any time
</subsectionHeader>
<bodyText confidence="0.997945807692308">
the subject can query the MANA Calendar sys-
tem by uttering “MANA” and one of 3 key
words “appointment” (for upcoming appoint-
ments), “date” (current date) or “time” (current
time) subject to sufficiently low noise conditions.
After making a timed announcement, the system
enters a state in which the speech system is set to
recognize several acknowledgements (like “OK”).
MANA Calendar is being trialed in the homes of
people with Alzheimer’s disease during the first
half of 2010. We require that there is at least one
carer or health worker who is able to enter calen-
dar information into Google Calendar for the
primary subject. If we have a live in carer, or a
spouse or relative in the carer role, we are also
allowing them to enter their own appointments.
Currently we are using a multiuser Microsoft
Speech Recognition system that is not trained to
the specific user. For our (younger) voices tested
pre-trial these gave pretty good results, but the
system is sensitive to age and accent. We have
therefore adapted the study to provide training
opportunities (human and system) for those who
cannot initially use the speech recognition sys-
tem successfully.
In addition, we do have a back up mouse or
switch arrangement that allows such a user to use
the system, but we are not permitting use of this
option at present. MANA Calendar is designed
not to require use of either keyboard or mouse,
and this is the condition that we are insisting on
for our initial evaluation. MANA is meant to
appear as a companion, not as a computer.
Another problem that we encountered is that
the price point requested by the NGO was
$1000-$1500, and for these experiments we are
using a DELL Studio One which is really not
quite fast enough for continuous speech. Thus if
it is left on trying to follow a conversation, it
ends up filling up its buffer which gives unac-
ceptable response times. For this reason we not
only require the user to say a specific keyword or
name to get the attention of the system (by de-
fault, MANA), we also require the user to be
looking at the ECA (Viola and Jones, 2004) be-
fore we try to interpret what they say as a com-
mand. This dramatically reduces the delays, al-
though there is still a hiatus that is slightly longer
than is comfortable (about two seconds rather
than the desired one second). This problem does
not appear when run on a more powerful
machine.
</bodyText>
<page confidence="0.994909">
10
</page>
<subsectionHeader confidence="0.999632">
3.2 Mobile Living
</subsectionHeader>
<bodyText confidence="0.999985444444444">
A straightforward extension to MANA Calendar
is to implement it on a mobile phone. We are
currently exploring a couple of options for both
technologies and platforms, the latter possibili-
ties include the iPhone, Windows Mobile and
Google Android, each of which has its pro’s and
con’s.
Already MANA Calendar has options to allow
the carer/healthworker to enter directions, and
eventually a library of directions will be built up
so that commonly visited places/recurring events,
will not need reentry of directions. With the Mo-
bile extension, MANA can also popup with re-
minders, make use of GPS, and let people know
when to get off the bus, etc. This naturally com-
bines in with current directions in GPS naviga-
tion systems and aids, as well as systems for
keeping track of the elderly.
</bodyText>
<subsectionHeader confidence="0.999785">
3.3 Teaching/Training
</subsectionHeader>
<bodyText confidence="0.999952941176471">
There are also several extensions of MANA en-
visaged that make use of our Teaching ECA
technology, including teaching social skills, pro-
viding personalized family oriented reminders,
and bridges to other technologies.
We also aim to keep the client occupied and
interested in current events, interacting with fam-
ily and friends, and actively stimulated and men-
tally engaged. The selection and implementation
of these specific task-oriented activities, as well
as playing games or doing exercises, is not
unique but is beyond the scope of this paper and
will not be reviewed. Our focus here is the natu-
ralness and appropriateness of interaction, and
exemplifying the kind of task-directed interac-
tion which is not beyond the scope of current
ECA technology.
</bodyText>
<subsectionHeader confidence="0.997429">
3.4 Companion Robots
</subsectionHeader>
<bodyText confidence="0.999992288461539">
One of the first news items on our technology
described it as “Companion Robots”, picking up
very quickly on this potential, notwithstanding
the crude Eliza-like interactions. Interestingly
this comes round full circle to the kind of ethical
questions about the use of computers that were
raised in the mind of her creator by those who
wanted to put her to work immediately (Weizen-
baum, 1976). Weizenbaum argued that we
shouldn’t have computerized psychiatrists who
didn’t really understand their patients, even if
they were using the same techniques the human
experts employed. And the world agreed with
him! What has changed?
In terms of ECA vs Eliza technology, not
much – the dialogue for HeadX is based on Al-
ice, who whilst not much different in many ways
from Eliza, at least had origins that sought to
provide her with visual connection to the world.
The current versions of Alice, reflect AIML code
that is very similar in principle to Eliza code, and
don’t reflect anything of the real world except
through the medium of canned dialogue.
The issue of computer control is not limited to
dialogue and the issue of competence – computer
controlled trains and buses and planes have been
shown to be more reliable than humans under
specified conditions, but still tend to be under
direct supervision. Computer-guided missiles
are for better or worse under an even more re-
moved level of control. Our homes are full of
gadgets, and most of us spend more time inter-
acting with a computer and/or watching televi-
sion than interacting directly with a person.
So WE will leave the ethics to society to de-
termine what it wants. In an age where more
people will be retired than working within the
next twenty to forty years in most western coun-
tries, a MANA-type companion looks to be more
of a necessity than a desired outcome.
Anecdotally, from our discussions with the
NGOs and their staff, those who have had a dis-
trict nurse or social worker visiting on a regular
basis, tend to be happier with a human visitor
than some technological solution. But those who
do not have someone visiting regularly are more
apprehensive about having a stranger in the
homes telling then what to do and sapping their
independence, than they are having a technology
that purports to do the same things, or mediates
between them and a remote visitor who does not
invade the privacy of their own home.
</bodyText>
<sectionHeader confidence="0.995431" genericHeader="method">
4 Conclusion: A Competent Companion
</sectionHeader>
<bodyText confidence="0.9997614">
In summary, WE see the key issue as compe-
tence, and so will conclude by outlining our ap-
proach to building the competence of MANA as
a companion, rather than a calendar.
Emotion, Affect and Attitude: As discussed, one of
our main lines of research at present is exploring
and expanding the range of expressions and emo-
tions, developing an AV corpus of carefully elic-
ited spontaneous natural emotions, and cross-
evaluating versus acted/programmed expressions.
</bodyText>
<sectionHeader confidence="0.714456" genericHeader="conclusions">
AV Speech Recognition/Synthesis: Currently we
</sectionHeader>
<bodyText confidence="0.9876465">
can control the expression of our avatar through
markup that is based on human judgements about
what particular morphs of the face appear to
show, and which are hand tuned to someone’s
</bodyText>
<page confidence="0.997132">
11
</page>
<bodyText confidence="0.996494348837209">
idiosyncratic idea of what a particular emotion or
expression looks like – it is already reasonably
effective, but as an initial step has not been prop-
erly evaluated, although our initial evaluation re-
sults have shown that at least some of the markup
is effective, and that some is not (without sepa-
rating out at this stage the influence of the text
and the mark up). The flip side of displaying an
ECA face is recognizing human faces and ex-
pressions. Similarly there is a much neglected
auditory synthesis and recognition side that goes
beyond phoneme and word. Our motto is “one
person’s noise is another person’s signal” and our
aim is for both speech and noise to simultane-
ously analyze and account for all individual dif-
ferences, gender and age characteristics, emo-
tion/affect/attitude and related human attributes,
as well as explicit social and linguistic gestures
and expressions, including rhythmic and tonal
prosody.
Dialogue Management and Understanding: Dia-
logue management is a term WE don’t like in the
context of companiable systems – it derives from
use as a database front end for ordering pizzas or
taxis. It has a very limited concept of under-
standing related to the specific application, and
Eliza or Alice type systems are perfectly capable
of giving arbitrarily good results just by learning
a greater range of template-response patterns.
Our companionable MANA system is grounded
in the home environment and is being trained to
talk about and monitor and react to what is going
on in the home. At the moment it is focused on
body language and facial expression, and shares
with the ASD system an aim to understand and
react appropriately. The Alice substrate already
has a reasonably comprehensive dictionary built
in, but all it can do with that is define things – it
can’t actually productively use the knowledge.
The Stelarc-Alice substrate also has at least three
distinguishable personae built in – one who is
male and a performance artist, one who is female
and pretending to be human, and one who is neu-
ter and surprised that you thought it should have
that human characteristic. The latter two are an
amalgam of hundreds of different program-
mer/user enhancements, whilst the Stelarc per-
sona is the work of a single person and reflects
his wry humour so that at times it does feel like
you are talking to him. We are building in access
to a full encyclopedia, and the ability to answer a
wide variety of questions from each entry. But
this also is superficial without the ability to learn
and reason.
Learning and Reasoning: From a technological
Artificial Intelligence perspective, our primary
focus is learning. Children learn from the time
they are born (actually probably more like from
about three months before they are born) and
their learning and play are very similar to the re-
search and experimentation of a scientist.
Piagetian Psycholinguistics, and Piaget’s 20 plus
books on specific aspects of child learning, de-
velopment and reasoning, views learning and
reasoning as developing hand in hand, with the
little scientists developing new insights and
deeper reasoning models, and thus enabling
learning more about their world, society, culture
and language. Learning to speak and understand
language involves making noises and making the
connection between the vocal tract/facial articu-
lations/gestures and the heard sounds. Unsuper-
vised learning using supervised techniques is
possible using cross-modal training. Approaches
from Computational Intelligence based on simple
models from genetics, ant colonies and bee
swarms, also provide mechanisms and analogies
that help see how a system can continuously
adapt and improve. Generalization and reasoning
are part of this. Our ability to learn language is
not independent of our ability to understand the
world but an extension of it, and the constraints
and nature of language are strongly influenced by
the constraints and nature of the world. This also
includes meta-reasoning: our reasoning about the
consequences of our logic, decisions and behaviour.
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991968913043478">
Stevan Harnad (1992) The Turing test is not a trick:
Turing indistinguishability is a scientific criterion.
SIGART Bulletin 3(4) pp. 9 - 10.
David M W Powers (1998) The total Turing test and
the Loebner prize, Proceedings of the Joint Confer-
ences on New Methods in Language Processing
and Computational Natural Language Learning,
ACL, pp.279-280.
M. Schröder &amp; J. Trouvain (2003). The German Text-
to-Speech Synthesis System MARY: A Tool for
Research, Development and Teaching. Interna-
tional Journal of Speech Technology, 6, pp. 365-377.
Stuart C Shapiro (1992) The Turing test and the
economist. SIGART Bulletin 3(4) pp. 10-11.
Paul A. Viola and Michael J. Jones, 2004. Robust
real-time face detection, International Journal of
Computer Vision, vol. 57, pp. 137–154.
Joseph Weizenbaum (1966), ELIZA - a computer pro-
gram for the study of natural language communica-
tion between man and machine, CACM 9 (1): 36–45
Joseph Weizenbaum (1976), Computer Power and
Human Reason: From Judgment To Calculation,
San Francisco: W. H. Freeman
</reference>
<page confidence="0.998462">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342435">
<title confidence="0.994763">MANA for the Ageing</title>
<author confidence="0.763648">W Powers</author>
<author confidence="0.763648">Martin H Luerssen</author>
<author confidence="0.763648">Trent W Lewis</author>
<author confidence="0.763648">Richard E Marissa Milne</author>
<author confidence="0.763648">John Pashalis</author>
<author confidence="0.763648">Kenneth</author>
<affiliation confidence="0.933964">AI Lab, School of Computer Science, Engineering and Flinders University, South</affiliation>
<email confidence="0.706342">David.Powers@flinders.edu.au</email>
<abstract confidence="0.998588052631579">We present a family of Embodied Conversational Agents (ECAs) using Talking Head technology, along with a program of associated research and user trials. Whilst antecedents of our current ECAs include “chatbots” desgined to pass the Turing Test (TT) or win a Loebner Prize (LP), our current agents are task-oriented Teaching Agents and Social Companions. The current focus for our research includes the role of emotion, expression and gesture in our agents/companions, the explicit teaching of such social skills as recognizing and displaying appropriate expressions/gestures, and the integration of template/database-based dialogue managers with more conversational TT/LP systems as well as with audio-visual speech/gesture recognition/synthesis technologies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The Turing test is not a trick: Turing indistinguishability is a scientific criterion.</title>
<date>1992</date>
<journal>SIGART Bulletin</journal>
<volume>3</volume>
<issue>4</issue>
<pages>9--10</pages>
<marker>Harnad, 1992</marker>
<rawString>Stevan Harnad (1992) The Turing test is not a trick: Turing indistinguishability is a scientific criterion. SIGART Bulletin 3(4) pp. 9 - 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M W Powers</author>
</authors>
<title>The total Turing test and the Loebner prize,</title>
<date>1998</date>
<booktitle>Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning, ACL,</booktitle>
<pages>279--280</pages>
<marker>Powers, 1998</marker>
<rawString>David M W Powers (1998) The total Turing test and the Loebner prize, Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning, ACL, pp.279-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schröder</author>
<author>J Trouvain</author>
</authors>
<title>The German Textto-Speech Synthesis System MARY: A Tool for Research, Development and Teaching.</title>
<date>2003</date>
<journal>International Journal of Speech Technology,</journal>
<volume>6</volume>
<pages>365--377</pages>
<marker>Schröder, Trouvain, 2003</marker>
<rawString>M. Schröder &amp; J. Trouvain (2003). The German Textto-Speech Synthesis System MARY: A Tool for Research, Development and Teaching. International Journal of Speech Technology, 6, pp. 365-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart C Shapiro</author>
</authors>
<title>The Turing test and the economist.</title>
<date>1992</date>
<journal>SIGART Bulletin</journal>
<volume>3</volume>
<issue>4</issue>
<pages>10--11</pages>
<marker>Shapiro, 1992</marker>
<rawString>Stuart C Shapiro (1992) The Turing test and the economist. SIGART Bulletin 3(4) pp. 10-11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul A Viola</author>
<author>Michael J Jones</author>
</authors>
<title>Robust real-time face detection,</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>57</volume>
<pages>137--154</pages>
<contexts>
<context position="16333" citStr="Viola and Jones, 2004" startWordPosition="2586" endWordPosition="2589">valuation. MANA is meant to appear as a companion, not as a computer. Another problem that we encountered is that the price point requested by the NGO was $1000-$1500, and for these experiments we are using a DELL Studio One which is really not quite fast enough for continuous speech. Thus if it is left on trying to follow a conversation, it ends up filling up its buffer which gives unacceptable response times. For this reason we not only require the user to say a specific keyword or name to get the attention of the system (by default, MANA), we also require the user to be looking at the ECA (Viola and Jones, 2004) before we try to interpret what they say as a command. This dramatically reduces the delays, although there is still a hiatus that is slightly longer than is comfortable (about two seconds rather than the desired one second). This problem does not appear when run on a more powerful machine. 10 3.2 Mobile Living A straightforward extension to MANA Calendar is to implement it on a mobile phone. We are currently exploring a couple of options for both technologies and platforms, the latter possibilities include the iPhone, Windows Mobile and Google Android, each of which has its pro’s and con’s. </context>
<context position="14101" citStr="Viola &amp; Jones (2004)" startWordPosition="2197" endWordPosition="2200">ons ensure concurrent productions, e.g. the timing of voice audio and visemes (visual phonemes), appear as human-like as possible. Thinking Head and SAPI/Mary Integration: This new Thinking Head was designed using FaceGenTM software and incorporates Mary and Nuance voices, giving greater flexibility than using the original Stelarc face and voice. Face Detection and Motion Analysis Module: The system uses a camera which monitors the space the subject moves around in (or a part of it), and triggers upon detecting sufficient motion energy for a human body and a human face (using the algorithm of Viola &amp; Jones (2004)). On detecting such a “person-event”, the appointment message is then delivered to the subject. Speech Recognition Trigger Module: At any time the subject can query the MANA Calendar system by uttering “MANA” and one of 3 key words “appointment” (for upcoming appointments), “date” (current date) or “time” (current time) subject to sufficiently low noise conditions. After making a timed announcement, the system enters a state in which the speech system is set to recognize several acknowledgements (like “OK”). MANA Calendar is being trialed in the homes of people with Alzheimer’s disease during</context>
</contexts>
<marker>Viola, Jones, 2004</marker>
<rawString>Paul A. Viola and Michael J. Jones, 2004. Robust real-time face detection, International Journal of Computer Vision, vol. 57, pp. 137–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>ELIZA - a computer program for the study of natural language communication between man and machine,</title>
<date>1966</date>
<journal>CACM</journal>
<volume>9</volume>
<issue>1</issue>
<pages>36--45</pages>
<contexts>
<context position="2815" citStr="Weizenbaum, 1966" startWordPosition="434" endWordPosition="435">e agent for specific tasks where the limitations of current conversational companions, or dialog technologies, serve to match rather than conflict with the application constraints. Whereas limiting the topic was seen as a trick and a cheat in the Loebner Prize, our aim is to demonstrate and develop useful technologies and we are not interested in philosophical debates about intelligence. For these naturally constrained applications human level grammatical and syntactic understanding is not required, and the simple ELIZA-like approach of template matching is perfectly adequate as a first step (Weizenbaum, 1966). Our initial Talking Head was based around the Stelarc Prosthetic Head1 which combines multiple off-the-shelf components: keyboard input to a chatbot (AliceBot2) is linked to speech synthesis (IBM ViaVoice3) and 3D face rendering (Eyematic4). More recently we have adopted Head X5 which is capable of generating a continuous, synchronized, optionally subtitled audiovisual speech stream in many different languages, with the ability to switch and modify voices and morph different faces at the same time as interacting with the user. The system is designed to be able to use different speech and fac</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Joseph Weizenbaum (1966), ELIZA - a computer program for the study of natural language communication between man and machine, CACM 9 (1): 36–45</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>Computer Power and Human Reason: From Judgment To Calculation,</title>
<date>1976</date>
<publisher>Freeman</publisher>
<location>San Francisco:</location>
<contexts>
<context position="18624" citStr="Weizenbaum, 1976" startWordPosition="2967" endWordPosition="2969">will not be reviewed. Our focus here is the naturalness and appropriateness of interaction, and exemplifying the kind of task-directed interaction which is not beyond the scope of current ECA technology. 3.4 Companion Robots One of the first news items on our technology described it as “Companion Robots”, picking up very quickly on this potential, notwithstanding the crude Eliza-like interactions. Interestingly this comes round full circle to the kind of ethical questions about the use of computers that were raised in the mind of her creator by those who wanted to put her to work immediately (Weizenbaum, 1976). Weizenbaum argued that we shouldn’t have computerized psychiatrists who didn’t really understand their patients, even if they were using the same techniques the human experts employed. And the world agreed with him! What has changed? In terms of ECA vs Eliza technology, not much – the dialogue for HeadX is based on Alice, who whilst not much different in many ways from Eliza, at least had origins that sought to provide her with visual connection to the world. The current versions of Alice, reflect AIML code that is very similar in principle to Eliza code, and don’t reflect anything of the re</context>
</contexts>
<marker>Weizenbaum, 1976</marker>
<rawString>Joseph Weizenbaum (1976), Computer Power and Human Reason: From Judgment To Calculation, San Francisco: W. H. Freeman</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>