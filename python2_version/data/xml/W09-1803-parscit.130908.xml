<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000825">
<title confidence="0.789094">
Bounding and Comparing Methods for Correlation Clustering Beyond ILP
</title>
<author confidence="0.951567">
Micha Elsner and Warren Schudy
</author>
<affiliation confidence="0.985312">
Department of Computer Science
Brown University
</affiliation>
<address confidence="0.963106">
Providence, RI 02912
</address>
<email confidence="0.999697">
{melsner,ws}@cs.brown.edu
</email>
<sectionHeader confidence="0.996674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929">
We evaluate several heuristic solvers for corre-
lation clustering, the NP-hard problem of par-
titioning a dataset given pairwise affinities be-
tween all points. We experiment on two prac-
tical tasks, document clustering and chat dis-
entanglement, to which ILP does not scale.
On these datasets, we show that the cluster-
ing objective often, but not always, correlates
with external metrics, and that local search al-
ways improves over greedy solutions. We use
semi-definite programming (SDP) to provide a
tighter bound, showing that simple algorithms
are already close to optimality.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936340909091">
Correlation clustering is a powerful technique for
discovering structure in data. It operates on the
pairwise relationships between datapoints, partition-
ing the graph to minimize the number of unrelated
pairs that are clustered together, plus the number
of related pairs that are separated. Unfortunately,
this minimization problem is NP-hard (Ailon et al.,
2008). Practical work has adopted one of three
strategies for solving it. For a few specific tasks, one
can restrict the problem so that it is efficiently solv-
able. In most cases, however, this is impossible. In-
teger linear programming (ILP) can be used to solve
the general problem optimally, but only when the
number of data points is small. Beyond a few hun-
dred points, the only available solutions are heuristic
or approximate.
In this paper, we evaluate a variety of solu-
tions for correlation clustering on two realistic NLP
tasks, text topic clustering and chat disentangle-
ment, where typical datasets are too large for ILP
to find a solution. We show, as in previous work
on consensus clustering (Goder and Filkov, 2008),
that local search can improve the solutions found by
commonly-used methods. We investigate the rela-
tionship between the clustering objective and exter-
nal evaluation metrics such as F-score and one-to-
one overlap, showing that optimizing the objective
is usually a reasonable aim, but that other measure-
ments like number of clusters found should some-
times be used to reject pathological solutions. We
prove that the best heuristics are quite close to op-
timal, using the first implementation of the semi-
definite programming (SDP) relaxation to provide
tighter bounds.
The specific algorithms we investigate are, of
course, only a subset of the large number of pos-
sible solutions, or even of those proposed in the lit-
erature. We chose to test a few common, efficient
algorithms that are easily implemented. Our use of
a good bounding strategy means that we do not need
to perform an exhaustive comparison; we will show
that, though the methods we describe are not per-
fect, the remaining improvements possible with any
algorithm are relatively small.
</bodyText>
<sectionHeader confidence="0.997567" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999850666666667">
Correlation clustering was first introduced by Ben-
Dor et al. (1999) to cluster gene expression pat-
terns. The correlation clustering approach has sev-
eral strengths. It does not require users to specify
a parametric form for the clusters, nor to pick the
number of clusters. Unlike fully unsupervised clus-
</bodyText>
<page confidence="0.992383">
19
</page>
<note confidence="0.9921475">
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 19–27,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999846241379311">
tering methods, it can use training data to optimize
the pairwise classifier, but unlike classification, it
does not require samples from the specific clusters
found in the test data. For instance, it can use mes-
sages about cars to learn a similarity function that
can then be applied to messages about atheism.
Correlation clustering is a standard method for
coreference resolution. It was introduced to the
area by Soon et al. (2001), who describe the first-
link heuristic method for solving it. Ng and Cardie
(2002) extend this work with better features, and de-
velop the best-link heuristic, which finds better solu-
tions. McCallum and Wellner (2004) explicitly de-
scribe the problem as correlation clustering and use
an approximate technique (Bansal et al., 2004) to
enforce transitivity. Recently Finkel and Manning
(2008) show that the optimal ILP solution outper-
forms the first and best-link methods. Cohen and
Richman (2002) experiment with various heuristic
solutions for the cross-document coreference task of
grouping references to named entities.
Finally, correlation clustering has proven useful in
several discourse tasks. Barzilay and Lapata (2006)
use it for content aggregation in a generation system.
In Malioutov and Barzilay (2006), it is used for topic
segmentation—since segments must be contiguous,
the problem can be solved in polynomial time. El-
sner and Charniak (2008) address the related prob-
lem of disentanglement (which we explore in Sec-
tion 5.3), doing inference with the voting greedy al-
gorithm.
Bertolacci and Wirth (2007), Goder and Filkov
(2008) and Gionis et al. (2007) conduct experiments
on the closely related problem of consensus cluster-
ing, often solved by reduction to correlation cluster-
ing. The input to this problem is a set of clusterings;
the output is a “median” clustering which minimizes
the sum of (Rand) distance to the inputs. Although
these papers investigate some of the same algorithms
we use, they use an unrealistic lower bound, and so
cannot convincingly evaluate absolute performance.
Gionis et al. (2007) give an external evaluation on
some UCI datasets, but this is somewhat unconvinc-
ing since their metric, the impurity index, which is
essentially precision ignoring recall, gives a perfect
score to the all-singletons clustering. The other two
papers are based on objective values, not external
metrics.1
A variety of approximation algorithms for corre-
lation clustering with worst-case theoretical guar-
antees have been proposed: (Bansal et al., 2004;
Ailon et al., 2008; Demaine et al., 2006; Charikar
et al., 2005; Giotis and Guruswami, 2006). Re-
searchers including (Ben-Dor et al., 1999; Joachims
and Hopcroft, 2005; Mathieu and Schudy, 2008)
study correlation clustering theoretically when the
input is generated by randomly perturbing an un-
known ground truth clustering.
</bodyText>
<sectionHeader confidence="0.993653" genericHeader="method">
3 Algorithms
</sectionHeader>
<bodyText confidence="0.999965631578947">
We begin with some notation and a formal definition
of the problem. Our input is a complete, undirected
graph G with n nodes; each edge in the graph has
a probability pij reflecting our belief as to whether
nodes i and j come from the same cluster. Our goal
is to find a clustering, defined as a new graph G′
with edges xij E 10, 11, where if xij = 1, nodes
i and j are assigned to the same cluster. To make
this consistent, the edges must define an equivalence
relationship: xii = 1 and xij = xjk = 1 implies
xij = xik.
Our objective is to find a clustering as consistent
as possible with our beliefs—edges with high proba-
bility should not cross cluster boundaries, and edges
with low probability should. We define w+ ij as the
cost of cutting an edge whose probability is pij and
w−ij as the cost of keeping it. Mathematically, this
objective can be written (Ailon et al., 2008; Finkel
and Manning, 2008) as:
</bodyText>
<equation confidence="0.9772705">
�min xijw−ij + (1 − xij)w+ ij. (1)
ij:i&lt;j
</equation>
<bodyText confidence="0.999447555555556">
There are two plausible definitions for the costs w+
and w−, both of which have gained some support in
the literature. We can take w+ ij = pij and w−ij =
1 − pij (additive weights) as in (Ailon et al., 2008)
and others, or w+ ij = log(pij), w−ij = log(1 − pij)
(logarithmic weights) as in (Finkel and Manning,
2008). The logarithmic scheme has a tenuous math-
ematical justification, since it selects a maximum-
likelihood clustering under the assumption that the
</bodyText>
<footnote confidence="0.984161">
1Bertolacci and Wirth (2007) gave normalized mutual infor-
mation for one algorithm and data set, but almost all of their
results study objective value only.
</footnote>
<page confidence="0.997346">
20
</page>
<bodyText confidence="0.999970833333333">
pij are independent and identically distributed given
the status of the edge ij in the true clustering. If
we obtain the pij using a classifier, however, this as-
sumption is obviously untrue—some nodes will be
easy to link, while others will be hard—so we eval-
uate the different weighting schemes empirically.
</bodyText>
<subsectionHeader confidence="0.951458">
3.1 Greedy Methods
</subsectionHeader>
<bodyText confidence="0.994251388888889">
We use four greedy methods drawn from the lit-
erature; they are both fast and easy to implement.
All of them make decisions based on the net weight
w± ij= w ij− w−ij.
These algorithms step through the nodes of the
graph according to a permutation 7r. We try 100 ran-
dom permutations for each algorithm and report the
run which attains the best objective value (typically
this is slightly better than the average run; we dis-
cuss this more in the experimental sections). To sim-
plify the pseudocode we label the vertices 1, 2,... n
in the order specified by 7r. After this relabeling
7r(i) = i so 7r need not appear explicitly in the al-
gorithms.
Three of the algorithms are given in Figure 1. All
three algorithms start with the empty clustering and
add the vertices one by one. The BEST algorithm
adds each vertex i to the cluster with the strongest
w± connecting to i, or to a new singleton if none of
the w± are positive. The FIRST algorithm adds each
vertex i to the cluster containing the most recently
considered vertex j with w± ij&gt; 0. The VOTE algo-
rithm adds each vertex to the cluster that minimizes
the correlation clustering objective, i.e. to the cluster
maximizing the total net weight or to a singleton if
no total is positive.
Ailon et al. (2008) introduced the PIVOT algo-
rithm, given in Figure 2, and proved that it is a 5-
approximation if w ij+ w−ij = 1 for all i, j and
7r is chosen randomly. Unlike BEST, VOTE and
FIRST, which build clusters vertex by vertex, the
PIVOT algorithm creates each new cluster in its fi-
nal form. This algorithm repeatedly takes an unclus-
tered pivot vertex and creates a new cluster contain-
ing that vertex and all unclustered neighbors with
positive weight.
</bodyText>
<subsectionHeader confidence="0.997411">
3.2 Local Search
</subsectionHeader>
<bodyText confidence="0.999412">
We use the straightforward local search previously
used by Gionis et al. (2007) and Goder and Filkov
</bodyText>
<equation confidence="0.902763272727273">
k +— 0 // number of clusters created so far
for i = 1 ... n do
for c = 1 ... k do
if BEST then
Qualityc +— maxj∈C[c] w±ij
else if FIRST then
Qualityc +— maxj∈C[c]:w±ij&gt;0 j
else if VOTE then
±
Qualityc +— Ej∈C[c] wij
c∗ +— arg max1≤c≤k Qualityc
if Qualityc∗ &gt; 0 then
C[c∗] +— C[c∗] U {i}
else
C[k++] +— {i} // forma new cluster
Figure 1: BEST/FIRST/VOTE algorithms
k +— 0 // number of clusters created so far
for i = 1 ... n do
P +— U1≤c≤k C[c] // Vertices already placed
if i P then
C[k++] +— {i} U { i &lt; j &lt; n :
j VP and w± ij&gt; 0 }
</equation>
<figureCaption confidence="0.98936">
Figure 2: PIVOT algorithm by Ailon et al. (2008)
</figureCaption>
<bodyText confidence="0.9996708125">
(2008). The allowed one element moves consist
of removing one vertex from a cluster and either
moving it to another cluster or to a new singleton
cluster. The best one element move (BOEM) al-
gorithm repeatedly makes the most profitable best
one element move until a local optimum is reached.
Simulated Annealing (SA) makes a random single-
element move, with probability related to the dif-
ference in objective it causes and the current tem-
perature. Our annealing schedule is exponential and
designed to attempt 2000n moves for n nodes. We
initialize the local search either with all nodes clus-
tered together, or at the clustering produced by one
of our greedy algorithms (in our tables, the latter is
written, eg. PIVOT/BOEM, if the greedy algorithm
is PIVOT).
</bodyText>
<sectionHeader confidence="0.600004" genericHeader="method">
4 Bounding with SDP
</sectionHeader>
<bodyText confidence="0.9999515">
Although comparing different algorithms to one an-
other gives a good picture of relative performance, it
is natural to wonder how well they do in an absolute
sense—how they compare to the optimal solution.
</bodyText>
<page confidence="0.994681">
21
</page>
<bodyText confidence="0.992761136363636">
For very small instances, we can actually find the
optimum using ILP, but since this does not scale be-
yond a few hundred points (see Section 5.1), for re-
alistic instances we must instead bound the optimal
value. Bounds are usually obtained by solving a re-
laxation of the original problem: a simpler problem
with the same objective but fewer constraints.
The bound used in previous work (Goder and
Filkov, 2008; Gionis et al., 2007; Bertolacci and
Wirth, 2007), which we call the trivial bound, is
obtained by ignoring the transitivity constraints en-
tirely. To optimize, we link (xij = 1) all the pairs
where w+ij is larger than w−ij; since this solution is
quite far from being a clustering, the bound tends
not to be very tight.
To get a better idea of how good a real clustering
can be, we use a semi-definite programming (SDP)
relaxation to provide a better bound. Here we moti-
vate and define this relaxation.
One can picture a clustering geometrically by as-
sociating cluster c with the standard basis vector
ec = (0,0, ... , 0,1, 0, ... , 0) E Rn. If object i is
</bodyText>
<equation confidence="0.9939205">
� �� � � �� �
c−1 n−c
</equation>
<bodyText confidence="0.999946571428571">
in cluster c then it is natural to associate i with the
vector ri = ec. This gives a nice geometric picture
of a clustering, with objects i and j in the same clus-
ter if and only if ri = rj. Note that the dot product
ri • rj is 1 if i and j are in the same cluster and 0
otherwise. These ideas yield a simple reformulation
of the correlation clustering problem:
</bodyText>
<equation confidence="0.985774666666667">
E
minr i,j:i&lt;j(ri • rj)w− ij + (1 − rj • rj)w+ij
s.t. ∀i ∃c : ri = ec
</equation>
<bodyText confidence="0.991639538461539">
To get an efficiently computable lower-bound we
relax the constraints that the ris are standard basis
vectors, replacing them with two sets of constraints:
ri • ri = 1 for all i and ri • rj &gt; 0 for all i, j.
Since the ri only appear as dot products, we can
rewrite in terms of xij = ri • rj. However, we
must now constrain the xij to be the dot products
of some set of vectors in Rn. This is true if and
only if the symmetric matrix X = {xij}ij is posi-
tive semi-definite. We now have the standard semi-
definite programming (SDP) relaxation of correla-
tion clustering (e.g. (Charikar et al., 2005; Mathieu
and Schudy, 2008)):
</bodyText>
<equation confidence="0.999326">
E
minx i,j:i&lt;j xijw−ij + (1 − xij)w+ij
{ xii = 1 ∀i
xij &gt; 0 ∀i, j
X = {xij}ij PSD
</equation>
<bodyText confidence="0.999738923076923">
This SDP has been studied theoretically by a
number of authors; we mention just two here.
Charikar et al. (2005) give an approximation al-
gorithm based on rounding the SDP which is a
0.7664 approximation for the problem of maximiz-
ing agreements. Mathieu and Schudy (2008) show
that if the input is generated by corrupting the
edges of a ground truth clustering B independently,
then the SDP relaxation value is within an additive
O(n√n) of the optimum clustering. They further
show that using the PIVOT algorithm to round the
SDP yields a clustering with value at most O(n√n)
more than optimal.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.920731">
5.1 Scalability
</subsectionHeader>
<bodyText confidence="0.9708768">
Using synthetic data, we investigate the scalability
of the linear programming solver and SDP bound.
To find optimal solutions, we pass the complete ILP2
to CPLEX. This is reasonable for 100 points and
solvable for 200; beyond this point it cannot be
solved due to memory exhaustion. As noted below,
despite our inability to compute the LP bound on
large instances, we can sometimes prove that they
must be worse than SDP bounds, so we do not in-
vestigate LP-solving techniques further.
The SDP has fewer constraints than the ILP
(O(n2) vs O(n3)), but this is still more than many
SDP solvers can handle. For our experiments we
used one of the few SDP solvers that can handle such
a large number of constraints: Christoph Helmberg’s
ConicBundle library (Helmberg, 2009; Helmberg,
2000). This solver can handle several thousand data-
points. It produces loose lower-bounds (off by a few
percent) quickly but converges to optimality quite
slowly; we err on the side of inefficiency by run-
ning for up to 60 hours. Of course, the SDP solver
is only necessary to bound algorithm performance;
our solvers themselves scale much better.
2Consisting of the objective plus constraints 0 &lt; xij &lt; 1
and triangle inequality (Ailon et al., 2008).
</bodyText>
<equation confidence="0.561156">
s.t.
.
</equation>
<page confidence="0.976019">
22
</page>
<subsectionHeader confidence="0.988722">
5.2 Twenty Newsgroups
</subsectionHeader>
<bodyText confidence="0.999975170731707">
In this section, we test our approach on a typi-
cal benchmark clustering dataset, 20 Newsgroups,
which contains posts from a variety of Usenet
newsgroups such as rec.motorcycles and
alt.atheism. Since our bounding technique
does not scale to the full dataset, we restrict our at-
tention to a subsample of 100 messages3 from each
newsgroup for a total of 2000—still a realistically
large-scale problem. Our goal is to cluster messages
by their newsgroup of origin. We conduct exper-
iments by holding out four newsgroups as a train-
ing set, learning a pairwise classifier, and applying it
to the remaining 16 newsgroups to form our affinity
matrix.4
Our pairwise classifier uses three types of fea-
tures previously found useful in document cluster-
ing. First, we bucket all words5 by their log doc-
ument frequency (for an overview of TF-IDF see
(Joachims, 1997)). For a pair of messages, we create
a feature for each bucket whose value is the propor-
tion of shared words in that bucket. Secondly, we
run LSA (Deerwester et al., 1990) on the TF-IDF
matrix for the dataset, and use the cosine distance
between each message pair as a feature. Finally, we
use the same type of shared words features for terms
in message subjects. We make a training instance for
each pair of documents in the training set and learn
via logistic regression.
The classifier has an average F-score of 29% and
an accuracy of 88%—not particularly good. We
should emphasize that the clustering task for 20
newsgroups is much harder than the more com-
mon classification task—since our training set is en-
tirely disjoint with the testing set, we can only learn
weights on feature categories, not term weights. Our
aim is to create realistic-looking data on which to
test our clustering methods, not to motivate correla-
tion clustering as a solution to this specific problem.
In fact, Zhong and Ghosh (2003) report better results
using generative models.
We evaluate our clusterings using three different
</bodyText>
<footnote confidence="0.994815">
3Available as mini newsgroups.tar.gz from the UCI
machine learning repository.
4The experiments below are averaged over four disjoint
training sets.
5We omit the message header, except the subject line, and
also discard word types with fewer than 3 occurrences.
</footnote>
<table confidence="0.996848115384615">
Logarithmic Weights
Obj Rand F 1-1
SDP bound 51.1% - - -
VOTE/BOEM 55.8% 93.80 33 41
SA 56.3% 93.56 31 36
PIVOT/BOEM 56.6% 93.63 32 39
BEST/BOEM 57.6% 93.57 31 38
FIRST/BOEM 57.9% 93.65 30 36
VOTE 59.0% 93.41 29 35
BOEM 60.1% 93.51 30 35
PIVOT 100% 90.85 17 27
BEST 138% 87.11 20 29
FIRST 619% 40.97 11 8
Additive Weights
Obj Rand F 1-1
SDP bound 59.0% - - -
SA 63.5% 93.75 32 39
VOTE/BOEM 63.5% 93.75 32 39
PIVOT/BOEM 63.7% 93.70 32 39
BEST/BOEM 63.8% 93.73 31 39
FIRST/BOEM 63.9% 93.58 31 37
BOEM 64.6% 93.65 31 37
VOTE 67.3% 93.35 28 34
PIVOT 109% 90.63 17 26
BEST 165% 87.06 20 29
FIRST 761% 40.46 11 8
</table>
<tableCaption confidence="0.959409333333333">
Table 1: Score of the solution with best objective for each
solver, averaged over newsgroups training sets, sorted by
objective.
</tableCaption>
<bodyText confidence="0.9996839375">
metrics (see Meila (2007) for an overview of cluster-
ing metrics). The Rand measure counts the number
of pairs of points for which the proposed clustering
agrees with ground truth. This is the metric which
is mathematically closest to the objective. However,
since most points are in different clusters, any so-
lution with small clusters tends to get a high score.
Therefore we also report the more sensitive F-score
with respect to the minority (“same cluster”) class.
We also report the one-to-one score, which mea-
sures accuracy over single points. For this metric,
we calculate a maximum-weight matching between
proposed clusters and ground-truth clusters, then re-
port the overlap between the two.
When presenting objective values, we locate them
within the range between the trivial lower bound dis-
</bodyText>
<page confidence="0.995712">
23
</page>
<bodyText confidence="0.999978977272727">
cussed in Section 4 and the objective value of the
singletons clustering (xij = 0, i =� j). On this scale,
lower is better; 0% corresponds to the trivial bound
and 100% corresponds to the singletons clustering.
It is possible to find values greater than 100%, since
some particularly bad clusterings have objectives
worse than the singletons clustering. Plainly, how-
ever, real clusterings will not have values as low as
0%, since the trivial bound is so unrealistic.
Our results are shown in Table 1. The best re-
sults are obtained using logarithmic weights with
VOTE followed by BOEM; reasonable results are
also found using additive weights, and annealing,
VOTE or PIVOT followed by BOEM. On its own,
the best greedy scheme is VOTE, but all of them are
substantially improved by BOEM. First-link is by
far the worst. Our use of the SDP lower bound rather
than the trivial lower-bound of 0% reduces the gap
between the best clustering and the lower bound by
over a factor of ten. It is easy to show that the LP
relaxation can obtain a bound of at most 50%6—the
SDP beats the LP in both runtime and quality!
We analyze the correlation between objective val-
ues and metric values, averaging Kendall’s tau7 over
the four datasets (Table 2). Over the entire dataset,
correlations are generally good (large and negative),
showing that optimizing the objective is indeed a
useful way to find good results. We also examine
correlations for the solutions with objective values
within the top 10%. Here the correlation is much
poorer; selecting the solution with the best objective
value will not necessarily optimize the metric, al-
though the correspondence is slightly better for the
log-weights scheme. The correlations do exist, how-
ever, and so the solution with the best objective value
is typically slightly better than the median.
In Figure 3, we show the distribution of one-to-
one scores obtained (for one specific dataset) by the
best solvers. From this diagram, it is clear that log-
weights and VOTE/BOEM usually obtain the best
scores for this metric, since the median is higher
than other solvers’ upper quartile scores. All solvers
have quite high variance, with a range of about 2%
between quartiles and 4% overall. We omit the F-
</bodyText>
<footnote confidence="0.9350244">
6The solution xij = 121 `w− ij &gt; w ´ for i &lt; j is feasible
ij
in the LP.
7The standard Pearson correlation coefficient is less robust
to outliers, which causes problems for this data.
</footnote>
<figure confidence="0.992383285714286">
0.44
0.42
0.40
0.38
0.36
0.34
0.32
</figure>
<figureCaption confidence="0.8899995">
Figure 3: Box-and-whisker diagram (outliers as +) for
one-to-one scores obtained by the best few solvers on a
particular newsgroup dataset. L means using log weights.
B means improved with BOEM.
</figureCaption>
<table confidence="0.9925182">
Rand F 1-1
Log-wt -.60 -.73 -.71
Top 10 % -.14 -.22 -.24
Add-wt -.60 -.67 -.65
Top 10 % -.13 -.15 -.14
</table>
<tableCaption confidence="0.996353666666667">
Table 2: Kendall’s tau correlation between objective and
metric values, averaged over newsgroup datasets, for all
solutions and top 10% of solutions.
</tableCaption>
<bodyText confidence="0.887461">
score plot, which is similar, for space reasons.
</bodyText>
<subsectionHeader confidence="0.995729">
5.3 Chat Disentanglement
</subsectionHeader>
<bodyText confidence="0.999454">
In the disentanglement task, we examine data from a
shared discussion group where many conversations
are occurring simultaneously. The task is to partition
the utterances into a set of conversations. This task
differs from newsgroup clustering in that data points
(utterances) have an inherent linear order. Ordering
is typical in discourse tasks including topic segmen-
tation and coreference resolution.
We use the annotated dataset and pairwise classi-
</bodyText>
<page confidence="0.997012">
24
</page>
<bodyText confidence="0.995092210526316">
fier made available by Elsner and Charniak (2008);8
this study represents a competitive baseline, al-
though more recently Wang and Oard (2009) have
improved it. Since this classifier is ineffective at
linking utterances more than 129 seconds apart, we
treat all decisions for such utterances as abstentions,
p = .5. For utterance pairs on which it does make
a decision, the classifier has a reported accuracy of
75% with an F-score of 71%.
As in previous work, we run experiments on the
800-utterance test set and average metrics over 6 test
annotations. We evaluate using the three metrics re-
ported by previous work. Two node-counting met-
rics measure global accuracy: one-to-one match as
explained above, and Shen’s F (Shen et al., 2006):
F = �i ni maxj(F(i, j)). Here i is a gold con-
versation with size ni and j is a proposed conver-
sation with size nj, sharing nij utterances; F(i, j)
is the harmonic mean of precision (nij ) and recall
</bodyText>
<equation confidence="0.494660666666667">
j
(nij ). A third metric, the local agreement, counts
ni
</equation>
<bodyText confidence="0.999958923076923">
edgewise agreement for pairs of nearby utterances,
where nearby means “within three utterances.”
In this dataset, the SDP is a more moderate im-
provement over the trivial lower bound, reducing
the gap between the best clustering and best lower
bound by a factor of about 3 (Table 3).
Optimization of the objective does not correspond
to improvements in the global metrics (Table 3);
for instance, the best objectives are attained with
FIRST/BOEM, but VOTE/BOEM yields better one-
to-one and F scores. Correlation between the ob-
jective and these global metrics is extremely weak
(Table 5). The local metric is somewhat correlated.
Local search does improve metric results for each
particular greedy algorithm. For instance, when
BOEM is added to VOTE (with log weights), one-
to-one increases from 44% to 46%, local from 72%
to 73% and F from 48% to 50%. This represents a
moderate improvement on the inference scheme de-
scribed in Elsner and Charniak (2008). They use
voting with additive weights, but rather than per-
forming multiple runs over random permutations,
they process utterances in the order they occur. (We
experimented with processing in order; the results
are unclear, but there is a slight trend toward worse
performance, as in this case.) Their results (also
</bodyText>
<footnote confidence="0.588676">
8Downloaded from cs.brown.edu/∼melsner
</footnote>
<bodyText confidence="0.999981833333333">
shown in the table) are 41% one-to-one, 73% local
and .44% F-score.9 Our improvement on the global
metrics (12% relative improvement in one-to-one,
13% in F-score) is modest, but was achieved with
better inference on exactly the same input.
Since the objective function fails to distinguish
good solutions from bad ones, we examine the types
of solutions found by different methods in the hope
of explaining why some perform better than others.
In this setting, some methods (notably local search
run on its own or from a poor starting point) find far
fewer clusters than others (Table 4; log weights not
shown but similar to additive). Since the classifier
abstains for utterances more than 129 seconds apart,
the objective is unaffected if very distant utterances
are linked on the basis of little or no evidence; this
is presumably how such large clusters form. (This
raises the question of whether abstentions should
be given weaker links with p &lt; .5. We leave this
for future work.) Algorithms which find reasonable
numbers of clusters (VOTE, PIVOT, BEST and lo-
cal searches based on these) all achieve good metric
scores, although there is still no reliable way to find
the best solution among this set of methods.
</bodyText>
<sectionHeader confidence="0.995658" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999906875">
It is clear from these results that heuristic methods
can provide good correlation clustering solutions on
datasets far too large for ILP to scale. The particular
solver chosen10 has a substantial impact on the qual-
ity of results obtained, in terms of external metrics
as well as objective value.
For general problems, our recommendation is to
use log weights and run VOTE/BOEM. This algo-
rithm is fast, achieves good objective values, and
yields good metric scores on our datasets. Although
objective values are usually only weakly correlated
with metrics, our results suggest that slightly bet-
ter scores can be obtained by running the algorithm
many times and returning the solution with the best
objective. This may be worth trying even when the
datapoints are inherently ordered, as in chat.
</bodyText>
<footnote confidence="0.9796845">
9The F-score metric is not used in Elsner and Charniak
(2008); we compute it ourselves on the result produced by their
software.
10Our C++ correlation clustering software and SDP
bounding package are available for download from
cs.brown.edu/∼melsner.
</footnote>
<page confidence="0.99362">
25
</page>
<table confidence="0.995405222222222">
Log Weights
Obj 1-1 Loc3 Shen F
SDP bound 13.0% - - -
FIRST/BOEM 19.3% 41 74 44
VOTE/BOEM 20.0% 46 73 50
SA 20.3% 42 73 45
BEST/BOEM 21.3% 43 73 47
BOEM 21.5% 22 72 21
PIVOT/BOEM 22.0% 45 72 50
VOTE 26.3% 44 72 48
BEST 37.1% 40 67 44
PIVOT 44.4% 39 66 44
FIRST 58.3% 39 62 41
Additive Weights
Obj 1-1 Loc3 Shen F
SDP bound 16.2% - - -
FIRST/BOEM 21.7% 40 73 44
BOEM 22.3% 22 73 20
BEST/BOEM 22.7% 44 74 49
VOTE/BOEM 23.3% 46 73 50
SA 23.8% 41 72 46
PIVOT/BOEM 24.8% 46 73 50
VOTE 30.5% 44 71 49
EC ’08 - 41 73 44
BEST 42.1% 43 69 47
PIVOT 48.4% 38 67 44
FIRST 69.0% 40 59 41
</table>
<tableCaption confidence="0.979043666666667">
Table 3: Score of the solution with best objective found
by each solver on the chat test dataset, averaged over 6
annotations, sorted by objective.
</tableCaption>
<bodyText confidence="0.997919466666667">
Whatever algorithm is used to provide an initial
solution, we advise the use of local search as a post-
process. BOEM always improves both objective
and metric values over its starting point.
The objective value is not always sufficient to se-
lect a good solution (as in the chat dataset). If pos-
sible, experimenters should check statistics like the
number of clusters found to make sure they conform
roughly to expectations. Algorithms that find far
too many or too few clusters, regardless of objec-
tive, are unlikely to be useful. This type of problem
can be especially dangerous if the pairwise classifier
abstains for many pairs of points.
SDP provides much tighter bounds than the trivial
bound used in previous work, although how much
</bodyText>
<table confidence="0.999248266666667">
Num clusters
Max human annotator 128
PIVOT 122
VOTE 99
PIVOT/BOEM 89
VOTE/BOEM 86
Mean human annotator 81
BEST 70
FIRST 70
Elsner and Charniak (2008) 63
BEST/BOEM 62
SA 57
FIRST/BOEM 54
Min human annotator 50
BOEM 7
</table>
<tableCaption confidence="0.9956815">
Table 4: Average number of clusters found (using addi-
tive weights) for chat test data.
</tableCaption>
<table confidence="0.9996442">
1-1 Loc3 Shen F
Log-wt -.40 -.68 -.35
Top 10 % .14 -.15 .15
Add-wt -.31 -.67 -.25
Top 10 % -.07 -.22 .13
</table>
<tableCaption confidence="0.986059666666667">
Table 5: Kendall’s tau correlation between objective and
metric values for the chat test set, for all solutions and top
10% of solutions.
</tableCaption>
<bodyText confidence="0.999948466666667">
tighter varies with dataset (about 12 times smaller
for newsgroups, 3 times for chat). This bound can
be used to evaluate the absolute performance of our
solvers; the VOTE/BOEM solver whose use we rec-
ommend is within about 5% of optimality. Some of
this 5% represents the difference between the bound
and optimality; the rest is the difference between the
optimum and the solution found. If the bound were
exactly optimal, we could expect a significant im-
provement on our best results, but not a very large
one—especially since correlation between objective
and metric values grows weaker for the best solu-
tions. While it might be useful to investigate more
sophisticated local searches in an attempt to close
the gap, we do not view this as a priority.
</bodyText>
<sectionHeader confidence="0.996211" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9774675">
We thank Christoph Helmberg, Claire Mathieu and
three reviewers.
</bodyText>
<page confidence="0.994202">
26
</page>
<sectionHeader confidence="0.983062" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878689320389">
Nir Ailon, Moses Charikar, and Alantha Newman. 2008.
Aggregating inconsistent information: Ranking and
clustering. Journal of the ACM, 55(5):Article No. 23.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation clustering. Machine Learning, 56(1-
3):89–113.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
HLT-NAACL.
Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. 1999.
Clustering gene expression patterns. Journal of Com-
putational Biology, 6(3-4):281–297.
Michael Bertolacci and Anthony Wirth. 2007. Are
approximation algorithms for consensus clustering
worthwhile? In SDM ’07: Procs. 7th SIAM Inter-
national Conference on Data Mining.
Moses Charikar, Venkatesan Guruswami, and Anthony
Wirth. 2005. Clustering with qualitative information.
J. Comput. Syst. Sci., 71(3):360–383.
William W. Cohen and Jacob Richman. 2002. Learn-
ing to match and cluster large high-dimensional data
sets for data integration. In KDD ’02, pages 475–480.
ACM.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science, 41:391–
407.
Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole
Immorlica. 2006. Correlation clustering in general
weighted graphs. Theor. Comput. Sci., 361(2):172–
187.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of ACL-08: HLT,
pages 834–842, Columbus, Ohio, June. Association
for Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings ofACL-08: HLT, Short Papers, pages 45–
48, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Aristides Gionis, Heikki Mannila, and Panayiotis
Tsaparas. 2007. Clustering aggregation. ACM Trans.
on Knowledge Discovery from Data, 1(1):Article 4.
Ioannis Giotis and Venkatesan Guruswami. 2006. Corre-
lation clustering with a fixed number of clusters. The-
ory of Computing, 2(1):249–266.
Andrey Goder and Vladimir Filkov. 2008. Consensus
clustering algorithms: Comparison and refinement. In
ALENEX ’08: Procs. 10th Workshop on Algorithm En-
ginering and Experiments, pages 109–117. SIAM.
Cristoph Helmberg. 2000. Semidefinite programming
for combinatorial optimization. Technical Report ZR-
00-34, Konrad-Zuse-Zentrum f¨ur Informationstechnik
Berlin.
Cristoph Helmberg, 2009. The ConicBundle Li-
brary for Convex Optimization. Ver. 0.2i from
http://www-user.tu-chemnitz.de
/∼helmberg/ConicBundle/.
Thorsten Joachims and John Hopcroft. 2005. Error
bounds for correlation clustering. In ICML ’05, pages
385–392, New York, NY, USA. ACM.
Thorsten Joachims. 1997. A probabilistic analysis of
the Rocchio algorithm with TFIDF for text categoriza-
tion. In International Conference on Machine Learn-
ing (ICML), pages 143–151.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In ACL.
The Association for Computer Linguistics.
Claire Mathieu and Warren Schudy. 2008.
Correlation clustering with noisy input.
Unpublished manuscript available from
http://www.cs.brown.edu/∼ws/papers/clustering.pdf.
Andrew McCallum and Ben Wellner. 2004. Condi-
tional models of identity uncertainty with application
to noun coreference. In Proceedings of the 18th An-
nual Conference on Neural Information Processing
Systems (NIPS), pages 905–912. MIT Press.
Marina Meila. 2007. Comparing clusterings–an infor-
mation based distance. Journal ofMultivariate Analy-
sis, 98(5):873–895, May.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings ofthe 40th Annual Meeting ofthe Association
for Computational Linguistics, pages 104–111.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ’06, pages 35–42, New York, NY,
USA. ACM.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521–544.
Lidan Wang and Douglas W. Oard. 2009. Context-based
message expansion for disentanglement of interleaved
text conversations. In Proceedings of NAACL-09 (to
appear).
Shi Zhong and Joydeep Ghosh. 2003. Model-based clus-
tering with soft balancing. In ICDM ’03: Proceedings
of the Third IEEE International Conference on Data
Mining, page 459, Washington, DC, USA. IEEE Com-
puter Society.
</reference>
<page confidence="0.998809">
27
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915501">
<title confidence="0.999732">Bounding and Comparing Methods for Correlation Clustering Beyond ILP</title>
<author confidence="0.999539">Micha Elsner</author>
<author confidence="0.999539">Warren</author>
<affiliation confidence="0.983977">Department of Computer Brown</affiliation>
<address confidence="0.948882">Providence, RI</address>
<abstract confidence="0.999235285714286">We evaluate several heuristic solvers for correlation clustering, the NP-hard problem of partitioning a dataset given pairwise affinities between all points. We experiment on two practical tasks, document clustering and chat disentanglement, to which ILP does not scale. On these datasets, we show that the clustering objective often, but not always, correlates with external metrics, and that local search always improves over greedy solutions. We use semi-definite programming (SDP) to provide a tighter bound, showing that simple algorithms are already close to optimality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nir Ailon</author>
<author>Moses Charikar</author>
<author>Alantha Newman</author>
</authors>
<title>Aggregating inconsistent information: Ranking and clustering.</title>
<date>2008</date>
<journal>Journal of the ACM,</journal>
<volume>55</volume>
<issue>5</issue>
<contexts>
<context position="1159" citStr="Ailon et al., 2008" startWordPosition="166" endWordPosition="169">ut not always, correlates with external metrics, and that local search always improves over greedy solutions. We use semi-definite programming (SDP) to provide a tighter bound, showing that simple algorithms are already close to optimality. 1 Introduction Correlation clustering is a powerful technique for discovering structure in data. It operates on the pairwise relationships between datapoints, partitioning the graph to minimize the number of unrelated pairs that are clustered together, plus the number of related pairs that are separated. Unfortunately, this minimization problem is NP-hard (Ailon et al., 2008). Practical work has adopted one of three strategies for solving it. For a few specific tasks, one can restrict the problem so that it is efficiently solvable. In most cases, however, this is impossible. Integer linear programming (ILP) can be used to solve the general problem optimally, but only when the number of data points is small. Beyond a few hundred points, the only available solutions are heuristic or approximate. In this paper, we evaluate a variety of solutions for correlation clustering on two realistic NLP tasks, text topic clustering and chat disentanglement, where typical datase</context>
<context position="5973" citStr="Ailon et al., 2008" startWordPosition="932" endWordPosition="935">ome of the same algorithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering</context>
<context position="7452" citStr="Ailon et al., 2008" startWordPosition="1202" endWordPosition="1205">find a clustering as consistent as possible with our beliefs—edges with high probability should not cross cluster boundaries, and edges with low probability should. We define w+ ij as the cost of cutting an edge whose probability is pij and w−ij as the cost of keeping it. Mathematically, this objective can be written (Ailon et al., 2008; Finkel and Manning, 2008) as: �min xijw−ij + (1 − xij)w+ ij. (1) ij:i&lt;j There are two plausible definitions for the costs w+ and w−, both of which have gained some support in the literature. We can take w+ ij = pij and w−ij = 1 − pij (additive weights) as in (Ailon et al., 2008) and others, or w+ ij = log(pij), w−ij = log(1 − pij) (logarithmic weights) as in (Finkel and Manning, 2008). The logarithmic scheme has a tenuous mathematical justification, since it selects a maximumlikelihood clustering under the assumption that the 1Bertolacci and Wirth (2007) gave normalized mutual information for one algorithm and data set, but almost all of their results study objective value only. 20 pij are independent and identically distributed given the status of the edge ij in the true clustering. If we obtain the pij using a classifier, however, this assumption is obviously untru</context>
<context position="9448" citStr="Ailon et al. (2008)" startWordPosition="1553" endWordPosition="1556">ms. Three of the algorithms are given in Figure 1. All three algorithms start with the empty clustering and add the vertices one by one. The BEST algorithm adds each vertex i to the cluster with the strongest w± connecting to i, or to a new singleton if none of the w± are positive. The FIRST algorithm adds each vertex i to the cluster containing the most recently considered vertex j with w± ij&gt; 0. The VOTE algorithm adds each vertex to the cluster that minimizes the correlation clustering objective, i.e. to the cluster maximizing the total net weight or to a singleton if no total is positive. Ailon et al. (2008) introduced the PIVOT algorithm, given in Figure 2, and proved that it is a 5- approximation if w ij+ w−ij = 1 for all i, j and 7r is chosen randomly. Unlike BEST, VOTE and FIRST, which build clusters vertex by vertex, the PIVOT algorithm creates each new cluster in its final form. This algorithm repeatedly takes an unclustered pivot vertex and creates a new cluster containing that vertex and all unclustered neighbors with positive weight. 3.2 Local Search We use the straightforward local search previously used by Gionis et al. (2007) and Goder and Filkov k +— 0 // number of clusters created s</context>
<context position="15655" citStr="Ailon et al., 2008" startWordPosition="2699" endWordPosition="2702">ur experiments we used one of the few SDP solvers that can handle such a large number of constraints: Christoph Helmberg’s ConicBundle library (Helmberg, 2009; Helmberg, 2000). This solver can handle several thousand datapoints. It produces loose lower-bounds (off by a few percent) quickly but converges to optimality quite slowly; we err on the side of inefficiency by running for up to 60 hours. Of course, the SDP solver is only necessary to bound algorithm performance; our solvers themselves scale much better. 2Consisting of the objective plus constraints 0 &lt; xij &lt; 1 and triangle inequality (Ailon et al., 2008). s.t. . 22 5.2 Twenty Newsgroups In this section, we test our approach on a typical benchmark clustering dataset, 20 Newsgroups, which contains posts from a variety of Usenet newsgroups such as rec.motorcycles and alt.atheism. Since our bounding technique does not scale to the full dataset, we restrict our attention to a subsample of 100 messages3 from each newsgroup for a total of 2000—still a realistically large-scale problem. Our goal is to cluster messages by their newsgroup of origin. We conduct experiments by holding out four newsgroups as a training set, learning a pairwise classifier,</context>
</contexts>
<marker>Ailon, Charikar, Newman, 2008</marker>
<rawString>Nir Ailon, Moses Charikar, and Alantha Newman. 2008. Aggregating inconsistent information: Ranking and clustering. Journal of the ACM, 55(5):Article No. 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Bansal</author>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<date>2004</date>
<booktitle>Correlation clustering. Machine Learning,</booktitle>
<pages>56--1</pages>
<contexts>
<context position="4208" citStr="Bansal et al., 2004" startWordPosition="661" endWordPosition="664">ecific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time.</context>
<context position="5953" citStr="Bansal et al., 2004" startWordPosition="928" endWordPosition="931"> papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is </context>
</contexts>
<marker>Bansal, Blum, Chawla, 2004</marker>
<rawString>Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004. Correlation clustering. Machine Learning, 56(1-3):89–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Aggregation via set partitioning for natural language generation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4604" citStr="Barzilay and Lapata (2006)" startWordPosition="716" endWordPosition="719"> with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is </context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2006. Aggregation via set partitioning for natural language generation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Ben-Dor</author>
<author>Ron Shamir</author>
<author>Zohar Yakhini</author>
</authors>
<title>Clustering gene expression patterns.</title>
<date>1999</date>
<journal>Journal of Computational Biology,</journal>
<pages>6--3</pages>
<contexts>
<context position="6092" citStr="Ben-Dor et al., 1999" startWordPosition="951" endWordPosition="954">performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new graph G′ with edges xij E 10, 11, where if xij = 1, nodes i and j are assigned to the same cluster. </context>
</contexts>
<marker>Ben-Dor, Shamir, Yakhini, 1999</marker>
<rawString>Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. 1999. Clustering gene expression patterns. Journal of Computational Biology, 6(3-4):281–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bertolacci</author>
<author>Anthony Wirth</author>
</authors>
<title>Are approximation algorithms for consensus clustering worthwhile?</title>
<date>2007</date>
<booktitle>In SDM ’07: Procs. 7th SIAM International Conference on Data Mining.</booktitle>
<contexts>
<context position="4995" citStr="Bertolacci and Wirth (2007)" startWordPosition="779" endWordPosition="782">2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Rand) distance to the inputs. Although these papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since</context>
<context position="7733" citStr="Bertolacci and Wirth (2007)" startWordPosition="1247" endWordPosition="1250">Mathematically, this objective can be written (Ailon et al., 2008; Finkel and Manning, 2008) as: �min xijw−ij + (1 − xij)w+ ij. (1) ij:i&lt;j There are two plausible definitions for the costs w+ and w−, both of which have gained some support in the literature. We can take w+ ij = pij and w−ij = 1 − pij (additive weights) as in (Ailon et al., 2008) and others, or w+ ij = log(pij), w−ij = log(1 − pij) (logarithmic weights) as in (Finkel and Manning, 2008). The logarithmic scheme has a tenuous mathematical justification, since it selects a maximumlikelihood clustering under the assumption that the 1Bertolacci and Wirth (2007) gave normalized mutual information for one algorithm and data set, but almost all of their results study objective value only. 20 pij are independent and identically distributed given the status of the edge ij in the true clustering. If we obtain the pij using a classifier, however, this assumption is obviously untrue—some nodes will be easy to link, while others will be hard—so we evaluate the different weighting schemes empirically. 3.1 Greedy Methods We use four greedy methods drawn from the literature; they are both fast and easy to implement. All of them make decisions based on the net w</context>
<context position="12037" citStr="Bertolacci and Wirth, 2007" startWordPosition="2029" endWordPosition="2032">to one another gives a good picture of relative performance, it is natural to wonder how well they do in an absolute sense—how they compare to the optimal solution. 21 For very small instances, we can actually find the optimum using ILP, but since this does not scale beyond a few hundred points (see Section 5.1), for realistic instances we must instead bound the optimal value. Bounds are usually obtained by solving a relaxation of the original problem: a simpler problem with the same objective but fewer constraints. The bound used in previous work (Goder and Filkov, 2008; Gionis et al., 2007; Bertolacci and Wirth, 2007), which we call the trivial bound, is obtained by ignoring the transitivity constraints entirely. To optimize, we link (xij = 1) all the pairs where w+ij is larger than w−ij; since this solution is quite far from being a clustering, the bound tends not to be very tight. To get a better idea of how good a real clustering can be, we use a semi-definite programming (SDP) relaxation to provide a better bound. Here we motivate and define this relaxation. One can picture a clustering geometrically by associating cluster c with the standard basis vector ec = (0,0, ... , 0,1, 0, ... , 0) E Rn. If obje</context>
</contexts>
<marker>Bertolacci, Wirth, 2007</marker>
<rawString>Michael Bertolacci and Anthony Wirth. 2007. Are approximation algorithms for consensus clustering worthwhile? In SDM ’07: Procs. 7th SIAM International Conference on Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
<author>Venkatesan Guruswami</author>
<author>Anthony Wirth</author>
</authors>
<title>Clustering with qualitative information.</title>
<date>2005</date>
<journal>J. Comput. Syst. Sci.,</journal>
<volume>71</volume>
<issue>3</issue>
<contexts>
<context position="6018" citStr="Charikar et al., 2005" startWordPosition="940" endWordPosition="943">e an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new graph G′ with edges xij E </context>
<context position="13691" citStr="Charikar et al., 2005" startWordPosition="2361" endWordPosition="2364">j)w+ij s.t. ∀i ∃c : ri = ec To get an efficiently computable lower-bound we relax the constraints that the ris are standard basis vectors, replacing them with two sets of constraints: ri • ri = 1 for all i and ri • rj &gt; 0 for all i, j. Since the ri only appear as dot products, we can rewrite in terms of xij = ri • rj. However, we must now constrain the xij to be the dot products of some set of vectors in Rn. This is true if and only if the symmetric matrix X = {xij}ij is positive semi-definite. We now have the standard semidefinite programming (SDP) relaxation of correlation clustering (e.g. (Charikar et al., 2005; Mathieu and Schudy, 2008)): E minx i,j:i&lt;j xijw−ij + (1 − xij)w+ij { xii = 1 ∀i xij &gt; 0 ∀i, j X = {xij}ij PSD This SDP has been studied theoretically by a number of authors; we mention just two here. Charikar et al. (2005) give an approximation algorithm based on rounding the SDP which is a 0.7664 approximation for the problem of maximizing agreements. Mathieu and Schudy (2008) show that if the input is generated by corrupting the edges of a ground truth clustering B independently, then the SDP relaxation value is within an additive O(n√n) of the optimum clustering. They further show that us</context>
</contexts>
<marker>Charikar, Guruswami, Wirth, 2005</marker>
<rawString>Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. 2005. Clustering with qualitative information. J. Comput. Syst. Sci., 71(3):360–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Jacob Richman</author>
</authors>
<title>Learning to match and cluster large high-dimensional data sets for data integration.</title>
<date>2002</date>
<booktitle>In KDD ’02,</booktitle>
<pages>475--480</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4373" citStr="Cohen and Richman (2002)" startWordPosition="686" endWordPosition="689">heism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Berto</context>
</contexts>
<marker>Cohen, Richman, 2002</marker>
<rawString>William W. Cohen and Jacob Richman. 2002. Learning to match and cluster large high-dimensional data sets for data integration. In KDD ’02, pages 475–480. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>407</pages>
<contexts>
<context position="16712" citStr="Deerwester et al., 1990" startWordPosition="2878" endWordPosition="2881"> Our goal is to cluster messages by their newsgroup of origin. We conduct experiments by holding out four newsgroups as a training set, learning a pairwise classifier, and applying it to the remaining 16 newsgroups to form our affinity matrix.4 Our pairwise classifier uses three types of features previously found useful in document clustering. First, we bucket all words5 by their log document frequency (for an overview of TF-IDF see (Joachims, 1997)). For a pair of messages, we create a feature for each bucket whose value is the proportion of shared words in that bucket. Secondly, we run LSA (Deerwester et al., 1990) on the TF-IDF matrix for the dataset, and use the cosine distance between each message pair as a feature. Finally, we use the same type of shared words features for terms in message subjects. We make a training instance for each pair of documents in the training set and learn via logistic regression. The classifier has an average F-score of 29% and an accuracy of 88%—not particularly good. We should emphasize that the clustering task for 20 newsgroups is much harder than the more common classification task—since our training set is entirely disjoint with the testing set, we can only learn wei</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391– 407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik D Demaine</author>
<author>Dotan Emanuel</author>
<author>Amos Fiat</author>
<author>Nicole Immorlica</author>
</authors>
<title>Correlation clustering in general weighted graphs.</title>
<date>2006</date>
<journal>Theor. Comput. Sci.,</journal>
<volume>361</volume>
<issue>2</issue>
<pages>187</pages>
<contexts>
<context position="5995" citStr="Demaine et al., 2006" startWordPosition="936" endWordPosition="939">rithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new gra</context>
</contexts>
<marker>Demaine, Emanuel, Fiat, Immorlica, 2006</marker>
<rawString>Erik D. Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. 2006. Correlation clustering in general weighted graphs. Theor. Comput. Sci., 361(2):172– 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Eugene Charniak</author>
</authors>
<title>You talking to me? a corpus and algorithm for conversation disentanglement.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>834--842</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4835" citStr="Elsner and Charniak (2008)" startWordPosition="752" endWordPosition="756">to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Rand) distance to the inputs. Although these papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so c</context>
<context position="22928" citStr="Elsner and Charniak (2008)" startWordPosition="3931" endWordPosition="3934">ts, for all solutions and top 10% of solutions. score plot, which is similar, for space reasons. 5.3 Chat Disentanglement In the disentanglement task, we examine data from a shared discussion group where many conversations are occurring simultaneously. The task is to partition the utterances into a set of conversations. This task differs from newsgroup clustering in that data points (utterances) have an inherent linear order. Ordering is typical in discourse tasks including topic segmentation and coreference resolution. We use the annotated dataset and pairwise classi24 fier made available by Elsner and Charniak (2008);8 this study represents a competitive baseline, although more recently Wang and Oard (2009) have improved it. Since this classifier is ineffective at linking utterances more than 129 seconds apart, we treat all decisions for such utterances as abstentions, p = .5. For utterance pairs on which it does make a decision, the classifier has a reported accuracy of 75% with an F-score of 71%. As in previous work, we run experiments on the 800-utterance test set and average metrics over 6 test annotations. We evaluate using the three metrics reported by previous work. Two node-counting metrics measur</context>
<context position="24825" citStr="Elsner and Charniak (2008)" startWordPosition="4254" endWordPosition="4257">respond to improvements in the global metrics (Table 3); for instance, the best objectives are attained with FIRST/BOEM, but VOTE/BOEM yields better oneto-one and F scores. Correlation between the objective and these global metrics is extremely weak (Table 5). The local metric is somewhat correlated. Local search does improve metric results for each particular greedy algorithm. For instance, when BOEM is added to VOTE (with log weights), oneto-one increases from 44% to 46%, local from 72% to 73% and F from 48% to 50%. This represents a moderate improvement on the inference scheme described in Elsner and Charniak (2008). They use voting with additive weights, but rather than performing multiple runs over random permutations, they process utterances in the order they occur. (We experimented with processing in order; the results are unclear, but there is a slight trend toward worse performance, as in this case.) Their results (also 8Downloaded from cs.brown.edu/∼melsner shown in the table) are 41% one-to-one, 73% local and .44% F-score.9 Our improvement on the global metrics (12% relative improvement in one-to-one, 13% in F-score) is modest, but was achieved with better inference on exactly the same input. Sin</context>
<context position="27271" citStr="Elsner and Charniak (2008)" startWordPosition="4653" endWordPosition="4656">obtained, in terms of external metrics as well as objective value. For general problems, our recommendation is to use log weights and run VOTE/BOEM. This algorithm is fast, achieves good objective values, and yields good metric scores on our datasets. Although objective values are usually only weakly correlated with metrics, our results suggest that slightly better scores can be obtained by running the algorithm many times and returning the solution with the best objective. This may be worth trying even when the datapoints are inherently ordered, as in chat. 9The F-score metric is not used in Elsner and Charniak (2008); we compute it ourselves on the result produced by their software. 10Our C++ correlation clustering software and SDP bounding package are available for download from cs.brown.edu/∼melsner. 25 Log Weights Obj 1-1 Loc3 Shen F SDP bound 13.0% - - - FIRST/BOEM 19.3% 41 74 44 VOTE/BOEM 20.0% 46 73 50 SA 20.3% 42 73 45 BEST/BOEM 21.3% 43 73 47 BOEM 21.5% 22 72 21 PIVOT/BOEM 22.0% 45 72 50 VOTE 26.3% 44 72 48 BEST 37.1% 40 67 44 PIVOT 44.4% 39 66 44 FIRST 58.3% 39 62 41 Additive Weights Obj 1-1 Loc3 Shen F SDP bound 16.2% - - - FIRST/BOEM 21.7% 40 73 44 BOEM 22.3% 22 73 20 BEST/BOEM 22.7% 44 74 49 V</context>
<context position="29074" citStr="Elsner and Charniak (2008)" startWordPosition="4987" endWordPosition="4990">(as in the chat dataset). If possible, experimenters should check statistics like the number of clusters found to make sure they conform roughly to expectations. Algorithms that find far too many or too few clusters, regardless of objective, are unlikely to be useful. This type of problem can be especially dangerous if the pairwise classifier abstains for many pairs of points. SDP provides much tighter bounds than the trivial bound used in previous work, although how much Num clusters Max human annotator 128 PIVOT 122 VOTE 99 PIVOT/BOEM 89 VOTE/BOEM 86 Mean human annotator 81 BEST 70 FIRST 70 Elsner and Charniak (2008) 63 BEST/BOEM 62 SA 57 FIRST/BOEM 54 Min human annotator 50 BOEM 7 Table 4: Average number of clusters found (using additive weights) for chat test data. 1-1 Loc3 Shen F Log-wt -.40 -.68 -.35 Top 10 % .14 -.15 .15 Add-wt -.31 -.67 -.25 Top 10 % -.07 -.22 .13 Table 5: Kendall’s tau correlation between objective and metric values for the chat test set, for all solutions and top 10% of solutions. tighter varies with dataset (about 12 times smaller for newsgroups, 3 times for chat). This bound can be used to evaluate the absolute performance of our solvers; the VOTE/BOEM solver whose use we recomm</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>Micha Elsner and Eugene Charniak. 2008. You talking to me? a corpus and algorithm for conversation disentanglement. In Proceedings of ACL-08: HLT, pages 834–842, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4268" citStr="Finkel and Manning (2008)" startWordPosition="669" endWordPosition="672">t can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of d</context>
<context position="7198" citStr="Finkel and Manning, 2008" startWordPosition="1150" endWordPosition="1153">, defined as a new graph G′ with edges xij E 10, 11, where if xij = 1, nodes i and j are assigned to the same cluster. To make this consistent, the edges must define an equivalence relationship: xii = 1 and xij = xjk = 1 implies xij = xik. Our objective is to find a clustering as consistent as possible with our beliefs—edges with high probability should not cross cluster boundaries, and edges with low probability should. We define w+ ij as the cost of cutting an edge whose probability is pij and w−ij as the cost of keeping it. Mathematically, this objective can be written (Ailon et al., 2008; Finkel and Manning, 2008) as: �min xijw−ij + (1 − xij)w+ ij. (1) ij:i&lt;j There are two plausible definitions for the costs w+ and w−, both of which have gained some support in the literature. We can take w+ ij = pij and w−ij = 1 − pij (additive weights) as in (Ailon et al., 2008) and others, or w+ ij = log(pij), w−ij = log(1 − pij) (logarithmic weights) as in (Finkel and Manning, 2008). The logarithmic scheme has a tenuous mathematical justification, since it selects a maximumlikelihood clustering under the assumption that the 1Bertolacci and Wirth (2007) gave normalized mutual information for one algorithm and data se</context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings ofACL-08: HLT, Short Papers, pages 45– 48, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aristides Gionis</author>
<author>Heikki Mannila</author>
<author>Panayiotis Tsaparas</author>
</authors>
<title>Clustering aggregation.</title>
<date>2007</date>
<journal>ACM Trans. on Knowledge Discovery from Data,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>4</pages>
<contexts>
<context position="5045" citStr="Gionis et al. (2007)" startWordPosition="788" endWordPosition="791"> cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Rand) distance to the inputs. Although these papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essent</context>
<context position="9988" citStr="Gionis et al. (2007)" startWordPosition="1649" endWordPosition="1652"> total net weight or to a singleton if no total is positive. Ailon et al. (2008) introduced the PIVOT algorithm, given in Figure 2, and proved that it is a 5- approximation if w ij+ w−ij = 1 for all i, j and 7r is chosen randomly. Unlike BEST, VOTE and FIRST, which build clusters vertex by vertex, the PIVOT algorithm creates each new cluster in its final form. This algorithm repeatedly takes an unclustered pivot vertex and creates a new cluster containing that vertex and all unclustered neighbors with positive weight. 3.2 Local Search We use the straightforward local search previously used by Gionis et al. (2007) and Goder and Filkov k +— 0 // number of clusters created so far for i = 1 ... n do for c = 1 ... k do if BEST then Qualityc +— maxj∈C[c] w±ij else if FIRST then Qualityc +— maxj∈C[c]:w±ij&gt;0 j else if VOTE then ± Qualityc +— Ej∈C[c] wij c∗ +— arg max1≤c≤k Qualityc if Qualityc∗ &gt; 0 then C[c∗] +— C[c∗] U {i} else C[k++] +— {i} // forma new cluster Figure 1: BEST/FIRST/VOTE algorithms k +— 0 // number of clusters created so far for i = 1 ... n do P +— U1≤c≤k C[c] // Vertices already placed if i P then C[k++] +— {i} U { i &lt; j &lt; n : j VP and w± ij&gt; 0 } Figure 2: PIVOT algorithm by Ailon et al. (20</context>
<context position="12008" citStr="Gionis et al., 2007" startWordPosition="2025" endWordPosition="2028">different algorithms to one another gives a good picture of relative performance, it is natural to wonder how well they do in an absolute sense—how they compare to the optimal solution. 21 For very small instances, we can actually find the optimum using ILP, but since this does not scale beyond a few hundred points (see Section 5.1), for realistic instances we must instead bound the optimal value. Bounds are usually obtained by solving a relaxation of the original problem: a simpler problem with the same objective but fewer constraints. The bound used in previous work (Goder and Filkov, 2008; Gionis et al., 2007; Bertolacci and Wirth, 2007), which we call the trivial bound, is obtained by ignoring the transitivity constraints entirely. To optimize, we link (xij = 1) all the pairs where w+ij is larger than w−ij; since this solution is quite far from being a clustering, the bound tends not to be very tight. To get a better idea of how good a real clustering can be, we use a semi-definite programming (SDP) relaxation to provide a better bound. Here we motivate and define this relaxation. One can picture a clustering geometrically by associating cluster c with the standard basis vector ec = (0,0, ... , 0</context>
</contexts>
<marker>Gionis, Mannila, Tsaparas, 2007</marker>
<rawString>Aristides Gionis, Heikki Mannila, and Panayiotis Tsaparas. 2007. Clustering aggregation. ACM Trans. on Knowledge Discovery from Data, 1(1):Article 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Giotis</author>
<author>Venkatesan Guruswami</author>
</authors>
<title>Correlation clustering with a fixed number of clusters.</title>
<date>2006</date>
<journal>Theory of Computing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6047" citStr="Giotis and Guruswami, 2006" startWordPosition="944" endWordPosition="947">bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new graph G′ with edges xij E 10, 11, where if xij = 1, nod</context>
</contexts>
<marker>Giotis, Guruswami, 2006</marker>
<rawString>Ioannis Giotis and Venkatesan Guruswami. 2006. Correlation clustering with a fixed number of clusters. Theory of Computing, 2(1):249–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrey Goder</author>
<author>Vladimir Filkov</author>
</authors>
<title>Consensus clustering algorithms: Comparison and refinement.</title>
<date>2008</date>
<booktitle>In ALENEX ’08: Procs. 10th Workshop on Algorithm Enginering and Experiments,</booktitle>
<pages>109--117</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="1881" citStr="Goder and Filkov, 2008" startWordPosition="289" endWordPosition="292">can restrict the problem so that it is efficiently solvable. In most cases, however, this is impossible. Integer linear programming (ILP) can be used to solve the general problem optimally, but only when the number of data points is small. Beyond a few hundred points, the only available solutions are heuristic or approximate. In this paper, we evaluate a variety of solutions for correlation clustering on two realistic NLP tasks, text topic clustering and chat disentanglement, where typical datasets are too large for ILP to find a solution. We show, as in previous work on consensus clustering (Goder and Filkov, 2008), that local search can improve the solutions found by commonly-used methods. We investigate the relationship between the clustering objective and external evaluation metrics such as F-score and one-toone overlap, showing that optimizing the objective is usually a reasonable aim, but that other measurements like number of clusters found should sometimes be used to reject pathological solutions. We prove that the best heuristics are quite close to optimal, using the first implementation of the semidefinite programming (SDP) relaxation to provide tighter bounds. The specific algorithms we invest</context>
<context position="5020" citStr="Goder and Filkov (2008)" startWordPosition="783" endWordPosition="786"> heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Rand) distance to the inputs. Although these papers investigate some of the same algorithms we use, they use an unrealistic lower bound, and so cannot convincingly evaluate absolute performance. Gionis et al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impuri</context>
<context position="11987" citStr="Goder and Filkov, 2008" startWordPosition="2021" endWordPosition="2024"> SDP Although comparing different algorithms to one another gives a good picture of relative performance, it is natural to wonder how well they do in an absolute sense—how they compare to the optimal solution. 21 For very small instances, we can actually find the optimum using ILP, but since this does not scale beyond a few hundred points (see Section 5.1), for realistic instances we must instead bound the optimal value. Bounds are usually obtained by solving a relaxation of the original problem: a simpler problem with the same objective but fewer constraints. The bound used in previous work (Goder and Filkov, 2008; Gionis et al., 2007; Bertolacci and Wirth, 2007), which we call the trivial bound, is obtained by ignoring the transitivity constraints entirely. To optimize, we link (xij = 1) all the pairs where w+ij is larger than w−ij; since this solution is quite far from being a clustering, the bound tends not to be very tight. To get a better idea of how good a real clustering can be, we use a semi-definite programming (SDP) relaxation to provide a better bound. Here we motivate and define this relaxation. One can picture a clustering geometrically by associating cluster c with the standard basis vect</context>
</contexts>
<marker>Goder, Filkov, 2008</marker>
<rawString>Andrey Goder and Vladimir Filkov. 2008. Consensus clustering algorithms: Comparison and refinement. In ALENEX ’08: Procs. 10th Workshop on Algorithm Enginering and Experiments, pages 109–117. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristoph Helmberg</author>
</authors>
<title>Semidefinite programming for combinatorial optimization.</title>
<date>2000</date>
<tech>Technical Report ZR00-34,</tech>
<institution>Konrad-Zuse-Zentrum f¨ur Informationstechnik Berlin.</institution>
<contexts>
<context position="15211" citStr="Helmberg, 2000" startWordPosition="2626" endWordPosition="2627">is reasonable for 100 points and solvable for 200; beyond this point it cannot be solved due to memory exhaustion. As noted below, despite our inability to compute the LP bound on large instances, we can sometimes prove that they must be worse than SDP bounds, so we do not investigate LP-solving techniques further. The SDP has fewer constraints than the ILP (O(n2) vs O(n3)), but this is still more than many SDP solvers can handle. For our experiments we used one of the few SDP solvers that can handle such a large number of constraints: Christoph Helmberg’s ConicBundle library (Helmberg, 2009; Helmberg, 2000). This solver can handle several thousand datapoints. It produces loose lower-bounds (off by a few percent) quickly but converges to optimality quite slowly; we err on the side of inefficiency by running for up to 60 hours. Of course, the SDP solver is only necessary to bound algorithm performance; our solvers themselves scale much better. 2Consisting of the objective plus constraints 0 &lt; xij &lt; 1 and triangle inequality (Ailon et al., 2008). s.t. . 22 5.2 Twenty Newsgroups In this section, we test our approach on a typical benchmark clustering dataset, 20 Newsgroups, which contains posts from </context>
</contexts>
<marker>Helmberg, 2000</marker>
<rawString>Cristoph Helmberg. 2000. Semidefinite programming for combinatorial optimization. Technical Report ZR00-34, Konrad-Zuse-Zentrum f¨ur Informationstechnik Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristoph Helmberg</author>
</authors>
<title>The ConicBundle Library for Convex Optimization. Ver. 0.2i from http://www-user.tu-chemnitz.de /∼helmberg/ConicBundle/.</title>
<date>2009</date>
<contexts>
<context position="15194" citStr="Helmberg, 2009" startWordPosition="2624" endWordPosition="2625"> to CPLEX. This is reasonable for 100 points and solvable for 200; beyond this point it cannot be solved due to memory exhaustion. As noted below, despite our inability to compute the LP bound on large instances, we can sometimes prove that they must be worse than SDP bounds, so we do not investigate LP-solving techniques further. The SDP has fewer constraints than the ILP (O(n2) vs O(n3)), but this is still more than many SDP solvers can handle. For our experiments we used one of the few SDP solvers that can handle such a large number of constraints: Christoph Helmberg’s ConicBundle library (Helmberg, 2009; Helmberg, 2000). This solver can handle several thousand datapoints. It produces loose lower-bounds (off by a few percent) quickly but converges to optimality quite slowly; we err on the side of inefficiency by running for up to 60 hours. Of course, the SDP solver is only necessary to bound algorithm performance; our solvers themselves scale much better. 2Consisting of the objective plus constraints 0 &lt; xij &lt; 1 and triangle inequality (Ailon et al., 2008). s.t. . 22 5.2 Twenty Newsgroups In this section, we test our approach on a typical benchmark clustering dataset, 20 Newsgroups, which con</context>
</contexts>
<marker>Helmberg, 2009</marker>
<rawString>Cristoph Helmberg, 2009. The ConicBundle Library for Convex Optimization. Ver. 0.2i from http://www-user.tu-chemnitz.de /∼helmberg/ConicBundle/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>John Hopcroft</author>
</authors>
<title>Error bounds for correlation clustering.</title>
<date>2005</date>
<booktitle>In ICML ’05,</booktitle>
<pages>385--392</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6121" citStr="Joachims and Hopcroft, 2005" startWordPosition="955" endWordPosition="958"> al. (2007) give an external evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new graph G′ with edges xij E 10, 11, where if xij = 1, nodes i and j are assigned to the same cluster. To make this consistent, the </context>
</contexts>
<marker>Joachims, Hopcroft, 2005</marker>
<rawString>Thorsten Joachims and John Hopcroft. 2005. Error bounds for correlation clustering. In ICML ’05, pages 385–392, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization.</title>
<date>1997</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>143--151</pages>
<contexts>
<context position="16541" citStr="Joachims, 1997" startWordPosition="2848" endWordPosition="2849"> to the full dataset, we restrict our attention to a subsample of 100 messages3 from each newsgroup for a total of 2000—still a realistically large-scale problem. Our goal is to cluster messages by their newsgroup of origin. We conduct experiments by holding out four newsgroups as a training set, learning a pairwise classifier, and applying it to the remaining 16 newsgroups to form our affinity matrix.4 Our pairwise classifier uses three types of features previously found useful in document clustering. First, we bucket all words5 by their log document frequency (for an overview of TF-IDF see (Joachims, 1997)). For a pair of messages, we create a feature for each bucket whose value is the proportion of shared words in that bucket. Secondly, we run LSA (Deerwester et al., 1990) on the TF-IDF matrix for the dataset, and use the cosine distance between each message pair as a feature. Finally, we use the same type of shared words features for terms in message subjects. We make a training instance for each pair of documents in the training set and learn via logistic regression. The classifier has an average F-score of 29% and an accuracy of 88%—not particularly good. We should emphasize that the cluste</context>
</contexts>
<marker>Joachims, 1997</marker>
<rawString>Thorsten Joachims. 1997. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. In International Conference on Machine Learning (ICML), pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In ACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="4692" citStr="Malioutov and Barzilay (2006)" startWordPosition="730" endWordPosition="733">ons. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), it is used for topic segmentation—since segments must be contiguous, the problem can be solved in polynomial time. Elsner and Charniak (2008) address the related problem of disentanglement (which we explore in Section 5.3), doing inference with the voting greedy algorithm. Bertolacci and Wirth (2007), Goder and Filkov (2008) and Gionis et al. (2007) conduct experiments on the closely related problem of consensus clustering, often solved by reduction to correlation clustering. The input to this problem is a set of clusterings; the output is a “median” clustering which minimizes the sum of (Ra</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In ACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Mathieu</author>
<author>Warren Schudy</author>
</authors>
<title>Correlation clustering with noisy input.</title>
<date>2008</date>
<note>Unpublished manuscript available from http://www.cs.brown.edu/∼ws/papers/clustering.pdf.</note>
<contexts>
<context position="6148" citStr="Mathieu and Schudy, 2008" startWordPosition="959" endWordPosition="962">evaluation on some UCI datasets, but this is somewhat unconvincing since their metric, the impurity index, which is essentially precision ignoring recall, gives a perfect score to the all-singletons clustering. The other two papers are based on objective values, not external metrics.1 A variety of approximation algorithms for correlation clustering with worst-case theoretical guarantees have been proposed: (Bansal et al., 2004; Ailon et al., 2008; Demaine et al., 2006; Charikar et al., 2005; Giotis and Guruswami, 2006). Researchers including (Ben-Dor et al., 1999; Joachims and Hopcroft, 2005; Mathieu and Schudy, 2008) study correlation clustering theoretically when the input is generated by randomly perturbing an unknown ground truth clustering. 3 Algorithms We begin with some notation and a formal definition of the problem. Our input is a complete, undirected graph G with n nodes; each edge in the graph has a probability pij reflecting our belief as to whether nodes i and j come from the same cluster. Our goal is to find a clustering, defined as a new graph G′ with edges xij E 10, 11, where if xij = 1, nodes i and j are assigned to the same cluster. To make this consistent, the edges must define an equiva</context>
<context position="13718" citStr="Mathieu and Schudy, 2008" startWordPosition="2365" endWordPosition="2368">= ec To get an efficiently computable lower-bound we relax the constraints that the ris are standard basis vectors, replacing them with two sets of constraints: ri • ri = 1 for all i and ri • rj &gt; 0 for all i, j. Since the ri only appear as dot products, we can rewrite in terms of xij = ri • rj. However, we must now constrain the xij to be the dot products of some set of vectors in Rn. This is true if and only if the symmetric matrix X = {xij}ij is positive semi-definite. We now have the standard semidefinite programming (SDP) relaxation of correlation clustering (e.g. (Charikar et al., 2005; Mathieu and Schudy, 2008)): E minx i,j:i&lt;j xijw−ij + (1 − xij)w+ij { xii = 1 ∀i xij &gt; 0 ∀i, j X = {xij}ij PSD This SDP has been studied theoretically by a number of authors; we mention just two here. Charikar et al. (2005) give an approximation algorithm based on rounding the SDP which is a 0.7664 approximation for the problem of maximizing agreements. Mathieu and Schudy (2008) show that if the input is generated by corrupting the edges of a ground truth clustering B independently, then the SDP relaxation value is within an additive O(n√n) of the optimum clustering. They further show that using the PIVOT algorithm to </context>
</contexts>
<marker>Mathieu, Schudy, 2008</marker>
<rawString>Claire Mathieu and Warren Schudy. 2008. Correlation clustering with noisy input. Unpublished manuscript available from http://www.cs.brown.edu/∼ws/papers/clustering.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 18th Annual Conference on Neural Information Processing Systems (NIPS),</booktitle>
<pages>905--912</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4095" citStr="McCallum and Wellner (2004)" startWordPosition="644" endWordPosition="647">se training data to optimize the pairwise classifier, but unlike classification, it does not require samples from the specific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several discourse tasks. Barzilay and Lapata (2006) use it for content aggregation in a generation system. In Malioutov and Barzilay (2006), i</context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>Andrew McCallum and Ben Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In Proceedings of the 18th Annual Conference on Neural Information Processing Systems (NIPS), pages 905–912. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
</authors>
<title>Comparing clusterings–an information based distance.</title>
<date>2007</date>
<journal>Journal ofMultivariate Analysis,</journal>
<volume>98</volume>
<issue>5</issue>
<contexts>
<context position="18674" citStr="Meila (2007)" startWordPosition="3223" endWordPosition="3224">% 93.57 31 38 FIRST/BOEM 57.9% 93.65 30 36 VOTE 59.0% 93.41 29 35 BOEM 60.1% 93.51 30 35 PIVOT 100% 90.85 17 27 BEST 138% 87.11 20 29 FIRST 619% 40.97 11 8 Additive Weights Obj Rand F 1-1 SDP bound 59.0% - - - SA 63.5% 93.75 32 39 VOTE/BOEM 63.5% 93.75 32 39 PIVOT/BOEM 63.7% 93.70 32 39 BEST/BOEM 63.8% 93.73 31 39 FIRST/BOEM 63.9% 93.58 31 37 BOEM 64.6% 93.65 31 37 VOTE 67.3% 93.35 28 34 PIVOT 109% 90.63 17 26 BEST 165% 87.06 20 29 FIRST 761% 40.46 11 8 Table 1: Score of the solution with best objective for each solver, averaged over newsgroups training sets, sorted by objective. metrics (see Meila (2007) for an overview of clustering metrics). The Rand measure counts the number of pairs of points for which the proposed clustering agrees with ground truth. This is the metric which is mathematically closest to the objective. However, since most points are in different clusters, any solution with small clusters tends to get a high score. Therefore we also report the more sensitive F-score with respect to the minority (“same cluster”) class. We also report the one-to-one score, which measures accuracy over single points. For this metric, we calculate a maximum-weight matching between proposed clu</context>
</contexts>
<marker>Meila, 2007</marker>
<rawString>Marina Meila. 2007. Comparing clusterings–an information based distance. Journal ofMultivariate Analysis, 98(5):873–895, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings ofthe 40th Annual Meeting ofthe Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="3961" citStr="Ng and Cardie (2002)" startWordPosition="623" endWordPosition="626">rocessing, pages 19–27, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tering methods, it can use training data to optimize the pairwise classifier, but unlike classification, it does not require samples from the specific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping references to named entities. Finally, correlation clustering has proven useful in several </context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings ofthe 40th Annual Meeting ofthe Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Qiang Yang</author>
<author>Jian-Tao Sun</author>
<author>Zheng Chen</author>
</authors>
<title>Thread detection in dynamic text message streams.</title>
<date>2006</date>
<booktitle>In SIGIR ’06,</booktitle>
<pages>35--42</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23616" citStr="Shen et al., 2006" startWordPosition="4045" endWordPosition="4048">y Wang and Oard (2009) have improved it. Since this classifier is ineffective at linking utterances more than 129 seconds apart, we treat all decisions for such utterances as abstentions, p = .5. For utterance pairs on which it does make a decision, the classifier has a reported accuracy of 75% with an F-score of 71%. As in previous work, we run experiments on the 800-utterance test set and average metrics over 6 test annotations. We evaluate using the three metrics reported by previous work. Two node-counting metrics measure global accuracy: one-to-one match as explained above, and Shen’s F (Shen et al., 2006): F = �i ni maxj(F(i, j)). Here i is a gold conversation with size ni and j is a proposed conversation with size nj, sharing nij utterances; F(i, j) is the harmonic mean of precision (nij ) and recall j (nij ). A third metric, the local agreement, counts ni edgewise agreement for pairs of nearby utterances, where nearby means “within three utterances.” In this dataset, the SDP is a more moderate improvement over the trivial lower bound, reducing the gap between the best clustering and best lower bound by a factor of about 3 (Table 3). Optimization of the objective does not correspond to improv</context>
</contexts>
<marker>Shen, Yang, Sun, Chen, 2006</marker>
<rawString>Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen. 2006. Thread detection in dynamic text message streams. In SIGIR ’06, pages 35–42, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="3879" citStr="Soon et al. (2001)" startWordPosition="609" endWordPosition="612">s of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 19–27, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics tering methods, it can use training data to optimize the pairwise classifier, but unlike classification, it does not require samples from the specific clusters found in the test data. For instance, it can use messages about cars to learn a similarity function that can then be applied to messages about atheism. Correlation clustering is a standard method for coreference resolution. It was introduced to the area by Soon et al. (2001), who describe the firstlink heuristic method for solving it. Ng and Cardie (2002) extend this work with better features, and develop the best-link heuristic, which finds better solutions. McCallum and Wellner (2004) explicitly describe the problem as correlation clustering and use an approximate technique (Bansal et al., 2004) to enforce transitivity. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods. Cohen and Richman (2002) experiment with various heuristic solutions for the cross-document coreference task of grouping reference</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidan Wang</author>
<author>Douglas W Oard</author>
</authors>
<title>Context-based message expansion for disentanglement of interleaved text conversations.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-09</booktitle>
<note>(to appear).</note>
<contexts>
<context position="23020" citStr="Wang and Oard (2009)" startWordPosition="3945" endWordPosition="3948">.3 Chat Disentanglement In the disentanglement task, we examine data from a shared discussion group where many conversations are occurring simultaneously. The task is to partition the utterances into a set of conversations. This task differs from newsgroup clustering in that data points (utterances) have an inherent linear order. Ordering is typical in discourse tasks including topic segmentation and coreference resolution. We use the annotated dataset and pairwise classi24 fier made available by Elsner and Charniak (2008);8 this study represents a competitive baseline, although more recently Wang and Oard (2009) have improved it. Since this classifier is ineffective at linking utterances more than 129 seconds apart, we treat all decisions for such utterances as abstentions, p = .5. For utterance pairs on which it does make a decision, the classifier has a reported accuracy of 75% with an F-score of 71%. As in previous work, we run experiments on the 800-utterance test set and average metrics over 6 test annotations. We evaluate using the three metrics reported by previous work. Two node-counting metrics measure global accuracy: one-to-one match as explained above, and Shen’s F (Shen et al., 2006): F </context>
</contexts>
<marker>Wang, Oard, 2009</marker>
<rawString>Lidan Wang and Douglas W. Oard. 2009. Context-based message expansion for disentanglement of interleaved text conversations. In Proceedings of NAACL-09 (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shi Zhong</author>
<author>Joydeep Ghosh</author>
</authors>
<title>Model-based clustering with soft balancing.</title>
<date>2003</date>
<booktitle>In ICDM ’03: Proceedings of the Third IEEE International Conference on Data Mining,</booktitle>
<pages>459</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="17553" citStr="Zhong and Ghosh (2003)" startWordPosition="3021" endWordPosition="3024">e for each pair of documents in the training set and learn via logistic regression. The classifier has an average F-score of 29% and an accuracy of 88%—not particularly good. We should emphasize that the clustering task for 20 newsgroups is much harder than the more common classification task—since our training set is entirely disjoint with the testing set, we can only learn weights on feature categories, not term weights. Our aim is to create realistic-looking data on which to test our clustering methods, not to motivate correlation clustering as a solution to this specific problem. In fact, Zhong and Ghosh (2003) report better results using generative models. We evaluate our clusterings using three different 3Available as mini newsgroups.tar.gz from the UCI machine learning repository. 4The experiments below are averaged over four disjoint training sets. 5We omit the message header, except the subject line, and also discard word types with fewer than 3 occurrences. Logarithmic Weights Obj Rand F 1-1 SDP bound 51.1% - - - VOTE/BOEM 55.8% 93.80 33 41 SA 56.3% 93.56 31 36 PIVOT/BOEM 56.6% 93.63 32 39 BEST/BOEM 57.6% 93.57 31 38 FIRST/BOEM 57.9% 93.65 30 36 VOTE 59.0% 93.41 29 35 BOEM 60.1% 93.51 30 35 PI</context>
</contexts>
<marker>Zhong, Ghosh, 2003</marker>
<rawString>Shi Zhong and Joydeep Ghosh. 2003. Model-based clustering with soft balancing. In ICDM ’03: Proceedings of the Third IEEE International Conference on Data Mining, page 459, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>