<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004412">
<title confidence="0.849851">
PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation
Resource*
</title>
<author confidence="0.928868">
James Fan and David Ferrucci and David Gondek and Aditya Kalyanpur
</author>
<affiliation confidence="0.914345">
IBM Watson Research Lab
</affiliation>
<address confidence="0.9556535">
19 Skyline Dr
Hawthorne, NY 10532
</address>
<email confidence="0.981501">
{fanj, ferrucci, gondek, adityakal}@us.ibm.com
</email>
<sectionHeader confidence="0.995242" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806411764706">
One of the main bottlenecks in natural lan-
guage processing is the lack of a comprehen-
sive lexicalized relation resource that contains
fine grained knowledge on predicates. In this
paper, we present PRISMATIC, a large scale
lexicalized relation resource that is automati-
cally created over 30 gb of text. Specifically,
we describe what kind of information is col-
lected in PRISMATIC and how it compares
with existing lexical resources. Our main fo-
cus has been on building the infrastructure and
gathering the data. Although we are still in
the early stages of applying PRISMATIC to
a wide variety of applications, we believe the
resource will be of tremendous value for AI
researchers, and we discuss some of potential
applications in this paper.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896333333333">
Many natural language processing and understand-
ing applications benefit from the interpretation of
lexical relations in text (e.g. selectional preferences
for verbs and nouns). For example, if one knows that
things being annexed are typically geopolitical enti-
ties, then given the phrase Napoleon’s annexation of
Piedmont, we can infer Piedmont is a geopolitical
entity. Existing linguistic resources such as VerbNet
and FrameNet provide some argument type infor-
mation for verbs and frames. However, since they
are manually built, they tend to specify type con-
straints at a very high level (e.g, Solid, Animate),
</bodyText>
<footnote confidence="0.5288255">
&apos;Research supported in part by Air Force Contract FA8750-
09-C-0172 under the DARPA Machine Reading Program
</footnote>
<bodyText confidence="0.999393037037037">
consequently they do not suffice for cases such as
the previous example.
We would like to infer more fine grained knowl-
edge for predicates automatically from a large
amount of data. In addition, we do not want to re-
strict ourselves to only verbs, binary relations, or to
a specific type hierarchy.
In this paper, we present PRISMATIC, a large
scale lexicalized relation resource mined from over
30 gb of text. PRISMATIC is built using a suite of
NLP tools that includes a dependency parser, a rule
based named entity recognizer and a coreference
resolution component. PRISMATIC is composed
of frames which are the basic semantic representa-
tion of lexicalized relation and surrounding context.
There are approximately 1 billion frames in our cur-
rent version of PRISMATIC. To induce knowledge
from PRISMATIC, we define the notion of frame-
cuts, which basically specify a cut or slice operation
on a frame. In the case of the previous Napoleon
annexation example, we would use a noun-phrase
—* object type cut to learn the most frequent type
of things being annexed. We believe there are many
potential applications that can utilize PRISMATIC,
such as type inference, relation extraction textual en-
tailment, etc. We discuss some of these applications
in details in section 8.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999242">
2.1 Manually Created Resources
</subsectionHeader>
<bodyText confidence="0.964551666666667">
Several lexical resources have been built man-
ually, most notably WordNet (Fellbaum, 1998),
FrameNet(Baker et al., 1998) and VerbNet(Baker et
</bodyText>
<page confidence="0.962826">
122
</page>
<note confidence="0.754678">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 122–127,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999880134615384">
al., 1998). WordNet is a lexical resource that con-
tains individual word synset information, such as
definition, synonyms, antonyms, etc. However, the
amount of predicate knowledge in WordNet is lim-
ited.
FrameNet is a lexical database that describes the
frame structure of selected words. Each frame rep-
resents a predicate (e.g. eat, remove) with a list of
frame elements that constitutes the semantic argu-
ments of the predicate. Different words may map to
the same frame, and one word may map to multiple
frames based on different word senses. Frame ele-
ments are often specific to a particular frame, and
even if two frame elements with the same name,
such as “Agent”, may have subtle semantic mean-
ings in different frames.
VerbNet is a lexical database that maps verbs to
their corresponding Levin (Levin, 1993) classes, and
it includes syntactic and semantic information of the
verbs, such as the syntactic sequences of a frame
(e.g. NP V NP PP) and the selectional restriction
of a frame argument value must be ANIMATE,
Compared to these resources, in addition to being
an automatic process, PRISMATIC has three major
differences. First, unlike the descriptive knowledge
in WordNet, VerbNet or FrameNet, PRISMATIC of-
fers only numeric knowledge of the frequencies of
how different predicates and their argument values
through out a corpus. The statistical profiles are eas-
ily to produce automatically, and they allow addi-
tional knowledge, such as type restriction (see 8.1),
to be inferred from PRISMATIC easily.
Second, the frames are defined differently. The
frames in PRISMATIC are not abstract concepts
generalized over a set of words. They are defined
by the words in a sentence and the relations between
them. Two frames with different slot values are con-
sidered different even though they may be semanti-
cally similar. For example, the two sentences “John
loves Mary” and “John adores Mary” result in two
different frame even though semantically they are
very close. By choosing not to use frame concepts
generalized over words, we avoid the problem of
determining which frame a word belongs to when
processing text automatically. We believe there will
be enough redundancy in a large corpus to produce
valid values for different synonyms and variations.
Third, PRISMATIC only uses a very small set of
slots (see table 1) defined by parser and relation an-
notators to link a frame and its arguments. By using
these slots directly, we avoid the problem of map-
ping parser relations to frame elements.
</bodyText>
<subsectionHeader confidence="0.999568">
2.2 Automatically Created Resources
</subsectionHeader>
<bodyText confidence="0.999889297297297">
TextRunner (Banko et al., 2007) is an information
extraction system which automatically extracts re-
lation tuples over massive web data in an unsuper-
vised manner. TextRunner contains over 800 mil-
lion extractions (Lin et al., 2009) and has proven
to be a useful resource in a number of important
tasks in machine reading such as hypernym discov-
ery (Alan Ritter and Etzioni, 2009), and scoring in-
teresting assertions (Lin et al., 2009). TextRunner
works by automatically identifying and extracting
relationships using a conditional random field (CRF)
model over natural language text. As this is a rela-
tively inexpensive technique, it allows rapid applica-
tion to web-scale data.
DIRT (Discovering Inference Rules from Text)
(Lin and Pantel, 2001) automatically identifies in-
ference rules over dependency paths which tend to
link the same arguments. The technique consists of
applying a dependency parser over 1 gb of text, col-
lecting the paths between arguments and then cal-
culating a path similarity between paths. DIRT has
been used extensively in recognizing textual entail-
ment (RTE).
PRISMATIC is similar to TextRunner and DIRT
in that it may be applied automatically over mas-
sive corpora. At a representational level it differs
from both TextRunner and DIRT by storing full
frames from which n-ary relations may be indexed
and queried. PRISMATIC differs from TextRun-
ner as it applies a full dependency parser in order
to identify dependency relationships between terms.
In contrast to DIRT and TextRunner, PRISMATIC
also performs co-reference resolution in order to in-
crease coverage for sparsely-occurring entities and
employs a named entity recognizer (NER) and rela-
tion extractor on all of its extractions to better repre-
sent intensional information.
</bodyText>
<sectionHeader confidence="0.98325" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.92209">
The PRISMATIC pipeline consists of three phases:
</bodyText>
<listItem confidence="0.762806">
1. Corpus Processing Documents are annotated
</listItem>
<page confidence="0.991361">
123
</page>
<figure confidence="0.502625">
5 Frame Extraction
</figure>
<figureCaption confidence="0.999729">
Figure 1: System Overview
</figureCaption>
<bodyText confidence="0.964591333333333">
by a suite of components which perform depen-
dency parsing, co-reference resolution, named
entity recognition and relation detection.
</bodyText>
<listItem confidence="0.984755428571429">
2. Frame Extraction Frames are extracted based
on the dependency parses and associated anno-
tations.
3. Frame-Cut Extraction Frame-cuts of interest
(e.g. S-V-O cuts) are identified over all frames
and frequency information for each cut is tabu-
lated.
</listItem>
<sectionHeader confidence="0.982896" genericHeader="method">
4 Corpus Processing
</sectionHeader>
<bodyText confidence="0.999684444444445">
The key step in the Corpus Processing stage is the
application of a dependency parser which is used
to identify the frame slots (as listed in Table 1) for
the Frame Extraction stage. We use ESG (McCord,
1990), a slot-grammar based parser in order to fill
in the frame slots. Sentences frequently require co-
reference in order to precisely identify the participat-
ing entity, and so in order to not lose that informa-
tion, we apply a simple rule based co-reference reso-
lution component in this phase. The co-reference in-
formation helps enhance the coverage of the frame-
cuts, which is especially valuable in cases of sparse
data and for use with complex frame-cuts.
A rule based Named Entity Recognizer (NER) is
used to identify the types of arguments in all frame
slot values. This type information is then registered
in the Frame Extraction stage to construct inten-
tional frames.
</bodyText>
<table confidence="0.9982736875">
Relation Description/Example
subj subject
obj direct object
iobj indirect object
comp complement
pred predicate complement
objprep object of the preposition
mod nprep Bat Cave in Toronto is a tourist attraction.
mod vprep He made it to Broadway.
mod nobj the object of a nominalized verb
mod ndet City’s budget was passed.
mod ncomp Tweet is a word for microblogging.
mod nsubj A poem by Byron
mod aobj John is similar to Steve.
isa subsumption relation
subtypeOf subsumption relation
</table>
<tableCaption confidence="0.999975">
Table 1: Relations used in a frame and their descriptions
</tableCaption>
<bodyText confidence="0.999985566666667">
The next step of PRISMATIC is to extract a set of
frames from the parsed corpus. A frame is the basic
semantic unit representing a set of entities and their
relations in a text snippet. A frame is made of a set
of slot value pairs where the slots are dependency
relations extracted from the parse and the values are
the terms from the sentences or annotated types. Ta-
ble 2 shows the extracted frame based on the parse
tree in figure 2.
In order to capture the relationship we are inter-
ested in, frame elements are limited to those that
represent the participant information of a predicate.
Slots consist of the ones listed in table 1. Further-
more, each frame is restricted to be two levels deep
at the most, therefore, a large parse tree may re-
sult in multiple frames. Table 2 shows how two
frames are extracted from the complex parse tree
in figure 2. The depth restriction is needed for two
reasons. First, despite the best efforts from parser
researchers, no parser is perfect, and big complex
parse trees tend to have more wrong parses. By lim-
iting a frame to be only a small subset of a complex
parse tree, we reduce the chance of error parse in
each frame. Second, by isolating a subtree, each
frame focuses on the immediate participants of a
predicate.
Non-parser information may also be included in a
frame. For example, the type annotations of a word
from a named entity recognizer are included, and
such type information is useful for the various ap-
</bodyText>
<page confidence="0.998104">
124
</page>
<figureCaption confidence="0.996189">
Figure 2: The parse tree of the sentence In 1921, Einstein received the Nobel Prize for his original work on the
photoelectric effect.
</figureCaption>
<bodyText confidence="0.643481666666667">
Frame01
verb receive
subj Einstein
type PERSON / SCIENTIST
obj Nobel prize
mod vprep in
objprep 1921
type YEAR
mod vprep for
objprep Frame02
Frame02
noun work
mod ndet his / Einstein
mod nobj on
objprep effect
</bodyText>
<tableCaption confidence="0.942462">
Table 2: Frames extracted from Dependency Parse in Fig-
ure 2
</tableCaption>
<bodyText confidence="0.9979398">
plications described in section 8. We also include
a flag to indicate whether a word is proper noun.
These two kinds of information allow us to easily
separate the intensional and the extensional parts of
PRISMATIC.
</bodyText>
<sectionHeader confidence="0.989062" genericHeader="method">
6 Frame Cut
</sectionHeader>
<bodyText confidence="0.99989552">
One of the main reasons for extracting a large
amount of frame data from a corpus is to induce
interesting knowledge patterns by exploiting redun-
dancy in the data. For example, we would like to
learn that things that are annexed are typically re-
gions, i.e., a predominant object-type for the noun-
phrase “annexation of” is “Region” where “Region”
is annotated by a NER. To do this kind of knowledge
induction, we first need to abstract out specific por-
tions of the frame - in this particular case, we need
to isolate and analyze the noun-phrase —* object-
type relationship. Then, given a lot of data, and
frames containing only the above relationship, we
hope to see the frame [noun=“annexation”, prepo-
sition=“of”, object-type=“Region”] occur very fre-
quently.
To enable this induction analysis, we define
frame-cuts, which basically specify a cut or slice op-
eration on a frame. For example, we define an N-P-
OT frame cut, which when applied to a frame only
keeps the noun (N), preposition (P) and object-type
(OT) slots, and discards the rest. Similarly, we de-
fine frame-cuts such as S-V-O, S-V-O-IO, S-V-P-O
etc. (where S - subject, V - verb, O - object, IO -
indirect object) which all dissect frames along dif-
</bodyText>
<page confidence="0.997046">
125
</page>
<bodyText confidence="0.999958130434783">
ferent dimensions. Continuing with the annexation
example, we can use the V-OT frame cut to learn
that a predominant object-type for the verb “annex”
is also “Region”, by seeing lots of frames of the form
[verb=“annex”, object-type=“Region”] in our data.
To make frame-cuts more flexible, we allow them
to specify optional value constraints for slots. For
example, we can define an S-V-O frame cut, where
both the subject (S) and object (O) slot values are
constrained to be proper nouns, thereby creating
strictly extensional frames, i.e. frames containing
data about instances, e.g., [subject=“United States”
verb=“annex” object=“Texas”]. The opposite ef-
fect is achieved by constraining S and O slot val-
ues to common nouns, creating intensional frames
such as [subject=“Political-Entity” verb=“annex”
object=“Region”]. The separation of extensional
from intensional frame information is desirable,
both from a knowledge understanding and an appli-
cations perspective, e.g. the former can be used to
provide factual evidence in tasks such as question
answering, while the latter can be used to learn en-
tailment rules as seen in the annexation case.
</bodyText>
<sectionHeader confidence="0.988976" genericHeader="method">
7 Data
</sectionHeader>
<bodyText confidence="0.999960777777778">
The corpora we used to produce the initial PRIS-
MATIC are based on a selected set of sources, such
as the complete Wikipedia, New York Times archive
and web page snippets that are on the topics listed in
wikipedia. After cleaning and html detagging, there
are a total of 30 GB of text. From these sources, we
extracted approximately 1 billion frames, and from
these frames, we produce the most commonly used
cuts such as S-V-O, S-V-P-O and S-V-O-IO.
</bodyText>
<sectionHeader confidence="0.996905" genericHeader="method">
8 Potential Applications
</sectionHeader>
<subsectionHeader confidence="0.999765">
8.1 Type Inference and Its Related Uses
</subsectionHeader>
<bodyText confidence="0.999977704545455">
As noted in Section 6, we use frame-cuts to dis-
sect frames along different slot dimensions, and then
aggregate statistics for the resultant frames across
the entire dataset, in order to induce relationships
among the various frame slots, e.g., learn the pre-
dominant types for subject/object slots in verb and
noun phrases. Given a new piece of text, we can
apply this knowledge to infer types for named en-
tities. For example, since the aggregate statistics
shows the most common type for the object of
the verb “annex” is Region, we can infer from the
sentence “Napoleon annexed Piedmont in 1859”,
that “Piedmont” is most likely to be a Region.
Similarly, consider the sentence: “He ordered a
Napoleon at the restaurant”. A dictionary based
NER is very likely to label “Napoleon” as a Per-
son. However, we can learn from a large amount
of data, that in the frame: [subject type=“Person”
verb=“order” object type=[?] verb prep=“at” ob-
ject prep=“restaurant”], the object type typically
denotes a Dish, and thus correctly infer the type for
“Napoleon” in this context. Learning this kind of
fine-grained type information for a particular con-
text is not possible using traditional hand-crafted re-
sources like VerbNet or FrameNet. Unlike previ-
ous work in selectional restriction (Carroll and Mc-
Carthy, 2000; Resnik, 1993), PRISMATIC based
type inference does not dependent on a particular
taxonomy or previously annotated training data: it
works with any NER and its type system.
The automatically induced-type information can
also be used for co-reference resolution. For ex-
ample, given the sentence: “Netherlands was ruled
by the UTP party before Napolean annexed it”, we
can use the inferred type constraint on “it” (Region)
to resolve it to “Netherlands” (instead of the “UTP
Party”).
Finally, typing knowledge can be used for word
sense disambiguation. In the sentence, “Tom Cruise
is one of the biggest stars in American Cinema”, we
can infer using our frame induced type knowledge
base, that the word “stars” in this context refers to a
Person/Actor type, and not the sense of “star” as an
astronomical object.
</bodyText>
<subsectionHeader confidence="0.99056">
8.2 Factual Evidence
</subsectionHeader>
<bodyText confidence="0.99957225">
Frame data, especially extensional data involving
named entities, captured over a large corpus can be
used as factual evidence in tasks such as question
answering.
</bodyText>
<subsectionHeader confidence="0.983254">
8.3 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999952833333333">
Traditional relation extraction approach (Zelenko et
al., 2003; Bunescu and Mooney, 2005) relies on the
correct identification of the types of the argument.
For example, to identify “employs” relation between
“John Doe” and “XYZ Corporation”, a relation ex-
tractor heavily relies on “John Doe” being annotated
</bodyText>
<page confidence="0.995581">
126
</page>
<bodyText confidence="0.999989181818182">
as a “PERSON” and “XYZ Corporation” an “OR-
GANIZATION” since the “employs” relation is de-
fined between a “PERSON” and an “ORGANIZA-
TION”.
We envision PRISMATIC to be applied to rela-
tion extraction in two ways. First, as described in
section 8.1, PRISMATIC can complement a named
entity recognizer (NER) for type annotation. This
is especially useful for the cases when NER fails.
Second, since PRISMATIC has broad coverage of
named entities, it can be used as a database to
check to see if the given argument exist in related
frame. For example, in order to determine if “em-
ploys” relation exists between “Jack Welch” and
“GE” in a sentence, we can look up the SVO cut
of PRISMATIC to see if we have any frame that has
“Jack Welch” as the subject, “GE” as the object and
“work” as the verb, or frame that has “Jack Welch”
as the object, “GE” as the subject and “employs” as
the verb. This information can be passed on as an
feature along with other syntactic and semantic fea-
tures to th relation extractor.
</bodyText>
<sectionHeader confidence="0.978652" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999736">
In this paper, we presented PRISMATIC, a large
scale lexicalized relation resource that is built au-
tomatically over massive amount of text. It provides
users with knowledge about predicates and their ar-
guments. We have focused on building the infras-
tructure and gathering the data. Although we are
still in the early stages of applying PRISMATIC, we
believe it will be useful for a wide variety of AI ap-
plications as discussed in section 8, and will pursue
them in the near future.
</bodyText>
<sectionHeader confidence="0.999017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998601673469388">
Stephen Soderland Alan Ritter and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium
on Learning by Reading and Learning to Read.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86–90, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In In Inter-
national Joint Conference on Artificial Intelligence,
pages 2670–2676.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ’05: Proceedings of the conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 724–731,
Morristown, NJ, USA. Association for Computational
Linguistics.
John Carroll and Diana McCarthy. 2000. Word sense
disambiguation using automatically acquired verbal
preferences. Computers and the Humanities Senseval
Special Issue, 34.
Christiane Fellbaum, 1998. WordNet: An Electronic Lex-
ical Database.
Beth Levin, 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323–328.
Thomas Lin, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
CIKM ’09: Proceeding of the 18th ACM conference on
Information and knowledge management, pages 1787–
1790, New York, NY, USA. ACM.
Michael C. McCord. 1990. Slot grammar: A system
for simpler construction of practical natural language
grammars. In Proceedings of the International Sympo-
sium on Natural Language and Logic, pages 118–145,
London, UK. Springer-Verlag.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083–1106.
</reference>
<page confidence="0.997335">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493767">
<title confidence="0.999911">PRISMATIC: Inducing Knowledge from a Large Scale Lexicalized Relation</title>
<author confidence="0.999955">Fan Ferrucci Gondek</author>
<affiliation confidence="0.995797">IBM Watson Research</affiliation>
<address confidence="0.7807445">19 Skyline Hawthorne, NY</address>
<email confidence="0.807707">ferrucci,gondek,</email>
<abstract confidence="0.999515833333333">One of the main bottlenecks in natural language processing is the lack of a comprehensive lexicalized relation resource that contains fine grained knowledge on predicates. In this paper, we present PRISMATIC, a large scale lexicalized relation resource that is automatically created over 30 gb of text. Specifically, we describe what kind of information is collected in PRISMATIC and how it compares with existing lexical resources. Our main focus has been on building the infrastructure and gathering the data. Although we are still in the early stages of applying PRISMATIC to a wide variety of applications, we believe the resource will be of tremendous value for AI researchers, and we discuss some of potential applications in this paper.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephen Soderland Alan Ritter</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="6341" citStr="Ritter and Etzioni, 2009" startWordPosition="1010" endWordPosition="1013">ry small set of slots (see table 1) defined by parser and relation annotators to link a frame and its arguments. By using these slots directly, we avoid the problem of mapping parser relations to frame elements. 2.2 Automatically Created Resources TextRunner (Banko et al., 2007) is an information extraction system which automatically extracts relation tuples over massive web data in an unsupervised manner. TextRunner contains over 800 million extractions (Lin et al., 2009) and has proven to be a useful resource in a number of important tasks in machine reading such as hypernym discovery (Alan Ritter and Etzioni, 2009), and scoring interesting assertions (Lin et al., 2009). TextRunner works by automatically identifying and extracting relationships using a conditional random field (CRF) model over natural language text. As this is a relatively inexpensive technique, it allows rapid application to web-scale data. DIRT (Discovering Inference Rules from Text) (Lin and Pantel, 2001) automatically identifies inference rules over dependency paths which tend to link the same arguments. The technique consists of applying a dependency parser over 1 gb of text, collecting the paths between arguments and then calculati</context>
</contexts>
<marker>Ritter, Etzioni, 2009</marker>
<rawString>Stephen Soderland Alan Ritter and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3178" citStr="Baker et al., 1998" startWordPosition="499" endWordPosition="502">he notion of framecuts, which basically specify a cut or slice operation on a frame. In the case of the previous Napoleon annexation example, we would use a noun-phrase —* object type cut to learn the most frequent type of things being annexed. We believe there are many potential applications that can utilize PRISMATIC, such as type inference, relation extraction textual entailment, etc. We discuss some of these applications in details in section 8. 2 Related Work 2.1 Manually Created Resources Several lexical resources have been built manually, most notably WordNet (Fellbaum, 1998), FrameNet(Baker et al., 1998) and VerbNet(Baker et 122 Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 122–127, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics al., 1998). WordNet is a lexical resource that contains individual word synset information, such as definition, synonyms, antonyms, etc. However, the amount of predicate knowledge in WordNet is limited. FrameNet is a lexical database that describes the frame structure of selected words. Each frame represents a predicate (e.g. eat, remove) with a list of </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, pages 86–90, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderl</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web. In</title>
<date>2007</date>
<booktitle>In International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="5995" citStr="Banko et al., 2007" startWordPosition="952" endWordPosition="955">they are very close. By choosing not to use frame concepts generalized over words, we avoid the problem of determining which frame a word belongs to when processing text automatically. We believe there will be enough redundancy in a large corpus to produce valid values for different synonyms and variations. Third, PRISMATIC only uses a very small set of slots (see table 1) defined by parser and relation annotators to link a frame and its arguments. By using these slots directly, we avoid the problem of mapping parser relations to frame elements. 2.2 Automatically Created Resources TextRunner (Banko et al., 2007) is an information extraction system which automatically extracts relation tuples over massive web data in an unsupervised manner. TextRunner contains over 800 million extractions (Lin et al., 2009) and has proven to be a useful resource in a number of important tasks in machine reading such as hypernym discovery (Alan Ritter and Etzioni, 2009), and scoring interesting assertions (Lin et al., 2009). TextRunner works by automatically identifying and extracting relationships using a conditional random field (CRF) model over natural language text. As this is a relatively inexpensive technique, it</context>
</contexts>
<marker>Banko, Cafarella, Soderl, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J Cafarella, Stephen Soderl, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In In International Joint Conference on Artificial Intelligence, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="17092" citStr="Bunescu and Mooney, 2005" startWordPosition="2779" endWordPosition="2782">nally, typing knowledge can be used for word sense disambiguation. In the sentence, “Tom Cruise is one of the biggest stars in American Cinema”, we can infer using our frame induced type knowledge base, that the word “stars” in this context refers to a Person/Actor type, and not the sense of “star” as an astronomical object. 8.2 Factual Evidence Frame data, especially extensional data involving named entities, captured over a large corpus can be used as factual evidence in tasks such as question answering. 8.3 Relation Extraction Traditional relation extraction approach (Zelenko et al., 2003; Bunescu and Mooney, 2005) relies on the correct identification of the types of the argument. For example, to identify “employs” relation between “John Doe” and “XYZ Corporation”, a relation extractor heavily relies on “John Doe” being annotated 126 as a “PERSON” and “XYZ Corporation” an “ORGANIZATION” since the “employs” relation is defined between a “PERSON” and an “ORGANIZATION”. We envision PRISMATIC to be applied to relation extraction in two ways. First, as described in section 8.1, PRISMATIC can complement a named entity recognizer (NER) for type annotation. This is especially useful for the cases when NER fails</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 724–731, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation using automatically acquired verbal preferences.</title>
<date>2000</date>
<journal>Computers and the Humanities Senseval Special Issue,</journal>
<volume>34</volume>
<contexts>
<context position="15982" citStr="Carroll and McCarthy, 2000" startWordPosition="2604" endWordPosition="2608">nce: “He ordered a Napoleon at the restaurant”. A dictionary based NER is very likely to label “Napoleon” as a Person. However, we can learn from a large amount of data, that in the frame: [subject type=“Person” verb=“order” object type=[?] verb prep=“at” object prep=“restaurant”], the object type typically denotes a Dish, and thus correctly infer the type for “Napoleon” in this context. Learning this kind of fine-grained type information for a particular context is not possible using traditional hand-crafted resources like VerbNet or FrameNet. Unlike previous work in selectional restriction (Carroll and McCarthy, 2000; Resnik, 1993), PRISMATIC based type inference does not dependent on a particular taxonomy or previously annotated training data: it works with any NER and its type system. The automatically induced-type information can also be used for co-reference resolution. For example, given the sentence: “Netherlands was ruled by the UTP party before Napolean annexed it”, we can use the inferred type constraint on “it” (Region) to resolve it to “Netherlands” (instead of the “UTP Party”). Finally, typing knowledge can be used for word sense disambiguation. In the sentence, “Tom Cruise is one of the bigge</context>
</contexts>
<marker>Carroll, McCarthy, 2000</marker>
<rawString>John Carroll and Diana McCarthy. 2000. Word sense disambiguation using automatically acquired verbal preferences. Computers and the Humanities Senseval Special Issue, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<contexts>
<context position="3148" citStr="Fellbaum, 1998" startWordPosition="497" endWordPosition="498">rom PRISMATIC, we define the notion of framecuts, which basically specify a cut or slice operation on a frame. In the case of the previous Napoleon annexation example, we would use a noun-phrase —* object type cut to learn the most frequent type of things being annexed. We believe there are many potential applications that can utilize PRISMATIC, such as type inference, relation extraction textual entailment, etc. We discuss some of these applications in details in section 8. 2 Related Work 2.1 Manually Created Resources Several lexical resources have been built manually, most notably WordNet (Fellbaum, 1998), FrameNet(Baker et al., 1998) and VerbNet(Baker et 122 Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 122–127, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics al., 1998). WordNet is a lexical resource that contains individual word synset information, such as definition, synonyms, antonyms, etc. However, the amount of predicate knowledge in WordNet is limited. FrameNet is a lexical database that describes the frame structure of selected words. Each frame represents a predicate (e.g</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum, 1998. WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<contexts>
<context position="4234" citStr="Levin, 1993" startWordPosition="669" endWordPosition="670">d. FrameNet is a lexical database that describes the frame structure of selected words. Each frame represents a predicate (e.g. eat, remove) with a list of frame elements that constitutes the semantic arguments of the predicate. Different words may map to the same frame, and one word may map to multiple frames based on different word senses. Frame elements are often specific to a particular frame, and even if two frame elements with the same name, such as “Agent”, may have subtle semantic meanings in different frames. VerbNet is a lexical database that maps verbs to their corresponding Levin (Levin, 1993) classes, and it includes syntactic and semantic information of the verbs, such as the syntactic sequences of a frame (e.g. NP V NP PP) and the selectional restriction of a frame argument value must be ANIMATE, Compared to these resources, in addition to being an automatic process, PRISMATIC has three major differences. First, unlike the descriptive knowledge in WordNet, VerbNet or FrameNet, PRISMATIC offers only numeric knowledge of the frequencies of how different predicates and their argument values through out a corpus. The statistical profiles are easily to produce automatically, and they</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin, 1993. English Verb Classes and Alternations: A Preliminary Investigation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text. In</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="6707" citStr="Lin and Pantel, 2001" startWordPosition="1064" endWordPosition="1067">ive web data in an unsupervised manner. TextRunner contains over 800 million extractions (Lin et al., 2009) and has proven to be a useful resource in a number of important tasks in machine reading such as hypernym discovery (Alan Ritter and Etzioni, 2009), and scoring interesting assertions (Lin et al., 2009). TextRunner works by automatically identifying and extracting relationships using a conditional random field (CRF) model over natural language text. As this is a relatively inexpensive technique, it allows rapid application to web-scale data. DIRT (Discovering Inference Rules from Text) (Lin and Pantel, 2001) automatically identifies inference rules over dependency paths which tend to link the same arguments. The technique consists of applying a dependency parser over 1 gb of text, collecting the paths between arguments and then calculating a path similarity between paths. DIRT has been used extensively in recognizing textual entailment (RTE). PRISMATIC is similar to TextRunner and DIRT in that it may be applied automatically over massive corpora. At a representational level it differs from both TextRunner and DIRT by storing full frames from which n-ary relations may be indexed and queried. PRISM</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Oren Etzioni</author>
<author>James Fogarty</author>
</authors>
<title>Identifying interesting assertions from the web. In</title>
<date>2009</date>
<booktitle>CIKM ’09: Proceeding of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>1787--1790</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6193" citStr="Lin et al., 2009" startWordPosition="983" endWordPosition="986">e will be enough redundancy in a large corpus to produce valid values for different synonyms and variations. Third, PRISMATIC only uses a very small set of slots (see table 1) defined by parser and relation annotators to link a frame and its arguments. By using these slots directly, we avoid the problem of mapping parser relations to frame elements. 2.2 Automatically Created Resources TextRunner (Banko et al., 2007) is an information extraction system which automatically extracts relation tuples over massive web data in an unsupervised manner. TextRunner contains over 800 million extractions (Lin et al., 2009) and has proven to be a useful resource in a number of important tasks in machine reading such as hypernym discovery (Alan Ritter and Etzioni, 2009), and scoring interesting assertions (Lin et al., 2009). TextRunner works by automatically identifying and extracting relationships using a conditional random field (CRF) model over natural language text. As this is a relatively inexpensive technique, it allows rapid application to web-scale data. DIRT (Discovering Inference Rules from Text) (Lin and Pantel, 2001) automatically identifies inference rules over dependency paths which tend to link the</context>
</contexts>
<marker>Lin, Etzioni, Fogarty, 2009</marker>
<rawString>Thomas Lin, Oren Etzioni, and James Fogarty. 2009. Identifying interesting assertions from the web. In CIKM ’09: Proceeding of the 18th ACM conference on Information and knowledge management, pages 1787– 1790, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael C McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars.</title>
<date>1990</date>
<booktitle>In Proceedings of the International Symposium on Natural Language and Logic,</booktitle>
<pages>118--145</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="8491" citStr="McCord, 1990" startWordPosition="1345" endWordPosition="1346">ew by a suite of components which perform dependency parsing, co-reference resolution, named entity recognition and relation detection. 2. Frame Extraction Frames are extracted based on the dependency parses and associated annotations. 3. Frame-Cut Extraction Frame-cuts of interest (e.g. S-V-O cuts) are identified over all frames and frequency information for each cut is tabulated. 4 Corpus Processing The key step in the Corpus Processing stage is the application of a dependency parser which is used to identify the frame slots (as listed in Table 1) for the Frame Extraction stage. We use ESG (McCord, 1990), a slot-grammar based parser in order to fill in the frame slots. Sentences frequently require coreference in order to precisely identify the participating entity, and so in order to not lose that information, we apply a simple rule based co-reference resolution component in this phase. The co-reference information helps enhance the coverage of the framecuts, which is especially valuable in cases of sparse data and for use with complex frame-cuts. A rule based Named Entity Recognizer (NER) is used to identify the types of arguments in all frame slot values. This type information is then regis</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael C. McCord. 1990. Slot grammar: A system for simpler construction of practical natural language grammars. In Proceedings of the International Symposium on Natural Language and Logic, pages 118–145, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="15997" citStr="Resnik, 1993" startWordPosition="2609" endWordPosition="2610">at the restaurant”. A dictionary based NER is very likely to label “Napoleon” as a Person. However, we can learn from a large amount of data, that in the frame: [subject type=“Person” verb=“order” object type=[?] verb prep=“at” object prep=“restaurant”], the object type typically denotes a Dish, and thus correctly infer the type for “Napoleon” in this context. Learning this kind of fine-grained type information for a particular context is not possible using traditional hand-crafted resources like VerbNet or FrameNet. Unlike previous work in selectional restriction (Carroll and McCarthy, 2000; Resnik, 1993), PRISMATIC based type inference does not dependent on a particular taxonomy or previously annotated training data: it works with any NER and its type system. The automatically induced-type information can also be used for co-reference resolution. For example, given the sentence: “Netherlands was ruled by the UTP party before Napolean annexed it”, we can use the inferred type constraint on “it” (Region) to resolve it to “Netherlands” (instead of the “UTP Party”). Finally, typing knowledge can be used for word sense disambiguation. In the sentence, “Tom Cruise is one of the biggest stars in Ame</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1083</pages>
<contexts>
<context position="17065" citStr="Zelenko et al., 2003" startWordPosition="2775" endWordPosition="2778">f the “UTP Party”). Finally, typing knowledge can be used for word sense disambiguation. In the sentence, “Tom Cruise is one of the biggest stars in American Cinema”, we can infer using our frame induced type knowledge base, that the word “stars” in this context refers to a Person/Actor type, and not the sense of “star” as an astronomical object. 8.2 Factual Evidence Frame data, especially extensional data involving named entities, captured over a large corpus can be used as factual evidence in tasks such as question answering. 8.3 Relation Extraction Traditional relation extraction approach (Zelenko et al., 2003; Bunescu and Mooney, 2005) relies on the correct identification of the types of the argument. For example, to identify “employs” relation between “John Doe” and “XYZ Corporation”, a relation extractor heavily relies on “John Doe” being annotated 126 as a “PERSON” and “XYZ Corporation” an “ORGANIZATION” since the “employs” relation is defined between a “PERSON” and an “ORGANIZATION”. We envision PRISMATIC to be applied to relation extraction in two ways. First, as described in section 8.1, PRISMATIC can complement a named entity recognizer (NER) for type annotation. This is especially useful f</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. J. Mach. Learn. Res., 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>