<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005585">
<title confidence="0.9981365">
Expectation Vectors: A Semiotics Inspired Approach
to Geometric Lexical-Semantic Representation
</title>
<author confidence="0.99873">
Justin Washtell
</author>
<affiliation confidence="0.988289">
University of Leeds
</affiliation>
<note confidence="0.3235305">
Leeds, UK
washtell @ comp.leeds.ac.uk
</note>
<sectionHeader confidence="0.972679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997994117647">
We introduce a new family of geometric
models of meaning, inspired by principles
from semiotics and information theory,
based on what we call Expectation Vectors.
We present theoretical arguments in support
of these representations over traditional
context-feature vectors: primarily that they
provide a more intuitive representation of
meaning, and detach vector representation
from the specific context features thereby
allowing arbitrarily sophisticated language
models to be leveraged. We present a
preliminary evaluation of an expectation
vector based word sense disambiguation
system using the SemEval-2007 task 2
dataset, with very encouraging results,
particularly with respect to ambiguous verbs.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999828">
It is a cornerstone assumption of distributional
lexical semantics that the distribution of words in a
corpus reflects their meaning. Common
interpretations of this include the Distributional
Hypothesis (Harris, 1954) and the Contextual
Hypotheses (Miller &amp; Charles, 1991), which state
that there is a relationship between a word&apos;s
meaning, and the context(s) in which it appears. In
recent years this insight has been borne out by
correlations between human judgements and
distributional models of word similarity (Rapp,
2002), and steady advances in tasks such as word
sense disambiguation (Schütze, 1998) and
information retrieval. The workhorse of these
approaches are wordspace models: vectors built
from context features which serve as geometric
analogues of meaning. Despite many advances,
substantial problems exist with this approach to
modelling meaning. Amongst these are the
problems of data sparseness and of how to model
compositional meaning.
In this short paper, we introduce a new family
of wordspace models, based on insights gleaned
from semiotics and information theory, called
Expectation Vectors. These retain the convenient
vector-based paradigm whilst encouraging the
exploitation of advances in language modelling
from other areas of NLP. We finish by outlining
some present efforts to evaluate expectation
vectors in the area of word sense disambiguation.
</bodyText>
<sectionHeader confidence="0.862071" genericHeader="method">
2 Modelling meaning from context
</sectionHeader>
<bodyText confidence="0.99950305">
Perhaps one of the most prominent application
areas to exploit context-based wordspace models
is that of word sense induction and disambiguation
(WSI/WSD). The prevailing approach to this
problem is based on a fairly literal interpretation of
the Distributional Hypothesis: that is to cluster or
classify instances of ambiguous words according
to certain features of the context in which they
appear – invariably other words. It is not difficult
to see why this approach is limiting: as Pedersen
(2008) observes, “the unifying thread that binds
together many short context applications and
methods is the fact that similarity decisions must
be made between contexts that share few (if any)
words in common.” This is a manifestation of
what is commonly referred to at the data
sparseness problem, and it pervades all of corpus-
based NLP. This problem is exacerbated as
available examples of a word sense decrease, or
finer sense granularities are sought. For supervised
tasks this implies that a large training set is
required, which is often expensive. For
unsupervised tasks, such as WSI, it has negative
implications for cluster quality and rule learning.
Consequently, Leacock et al (1996) observe that
WSD systems which operate directly upon context
are: “plagued with the same problem, excellent
precision but low recall”.
“Backing off” to more general feature classes
through say lemmatization or part-of-speech
tagging affords one way of alleviating sparseness
(Joshi &amp; Penstein-Rosé, 2009), assuming these
features are pertinent to the task. Similar strategies
include the use of dual-context models where
immediate lexical features are backed up by more
general topical ones garnered from the wider
context of the ambiguous word (Leacock et al,
1996; Yarowsky, 1993).
Others have tackled the problem of sparseness
without recourse to generalized feature classes,
</bodyText>
<page confidence="0.992492">
45
</page>
<note confidence="0.6332115">
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 45–50,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999896977011495">
through the exploitation of higher-order
distributional information. Schütze (1998)
popularised this approach within the WSD/WSI
task. Rather than comparing contexts directly, it is
the distributional similarity of those features (in
the corpus) which are compared. Specifically,
Schütze composed context vectors by summing
the vectors for every word in a context, where
those vectors were themselves formed from the
total of word co-occurrence counts pertaining to
every instance of that word in the corpus. The
resultant context vectors are therefore
comparatively dense, and carry second-order
information which makes otherwise unlike
contexts more amenable to comparison. One
contention of this model is that it conflates co-
occurrence information from all occurrences of a
word in the corpus, regardless of their sense. The
defence is that because the actual senses of the
term instances which appear in the context of the
ambiguous word will tend to be pertinent to that
word’s own specific sense, it is that common
aspect of their respective conflated-sense vectors -
when summed - which will dominant the resultant
context vector. Purandare &amp; Pedersen (2001)
performed a comparative study of disambiguation
approaches based on first-order context, and on
second order context as per Schütze (1998). They
found that while Schütze&apos;s approach provided
gains when data was limited, when the training
corpus was large enough that sufficient examples
existed, clustering on first order context was
actually a better approach. This suggests that
while alleviating the data-sparseness problem, the
practice of expanding context vectors in this way
introduces a certain amount of noise, presumably
by inappropriately over-smoothing the data.
Another approach to the sparse data problem
which was also part of Schütze&apos;s framework is
dimensionality reduction by Singular Value
Decomposition (SVD). In SVD the set of context
features are analytically combined and reduced in
a manner that exploits their latent similarities,
whereafter traditional vector measures can be
used. Very similar techniques to both of those used
by Schütze have been used for query expansion
and document representation in information
retrieval (Qiu &amp; Frei, 1993; Xu et al, 2007).
Several variations upon Schütze’s approach to
WSD have been explored. Dagan et al (1995) and
Karov &amp; Edelman (1996) both apply what they
call “similarity-based” methods which, while
markedly different on the surface to that of
Schütze, are similar in spirit and intent. Karov &amp;
Edelman, for example, use machine-readable
dictionary glosses as opposed to corpus-derived
co-occurrences, and apply an iterative
bootstrapping approach to augment the available
data, rather than strict second-order information.
Typically, context vectors comprise a
component (dimension) for each designated
feature in a word’s context. In a simple bag-of-
words model this might equate to one vector
component for each potential word that can appear
in the context. For more sophisticated n-gram or
dependency-based models, which attempt to better
capture the structure inherent in the language, this
number of vector components must be increased.
The more sophisticated the language model
becomes therefore, the more acute the sparse data
problem. Techniques like SVD can reduce this
sparseness, but other issues remain. How does one
weight heterogeneous features when forming a
vector? How does one interpret vectors reduced
by SVD? Looking at the variety of approaches to
tackling the problem, we might be forgiven for
questioning whether representing meaning as a
vector of context features is in fact an ideal
starting point for semantic tasks such as WSD.
In the following section we describe a means of
entirely detaching context feature selection from
vector representation, such that an arbitrarily
sophisticated language model can be used to
generate dense, comparable vectors. Necessarily,
we also present a prototype distributional
language model that will serve as the basis of our
investigations into this approach.
</bodyText>
<sectionHeader confidence="0.984191" genericHeader="method">
3 System &amp; approach
</sectionHeader>
<subsectionHeader confidence="0.999759">
3.1 Lexical Expectation Vectors
</subsectionHeader>
<bodyText confidence="0.999992454545455">
Theoretical motivation. The motivation behind
the method presented herein comes both from the
fields of semiotics and information theory. It is the
notion that the “meaning” of an utterance is not in
the utterance itself, nor in its individual or typical
context; it is in the disparity between our
expectations based on that context, and the
utterance (Noth, 1990; Chandler, 2002). Meaning
in this sense can be seen as related to information
(Attneave, 1959; Shannon, 1948): an utterance
which is entirely expected under a regime where
speaker and interpreter have identical frames of
reference communicates nothing; conversely an
extremely creative utterance is laden with
information, and may have multiple non-obvious
interpretations (poetry being a case in point -
Riffaterre, 1978). This idea is also lent some
weight by psycholinguistic experiments which
have revealed correlations between a word&apos;s
disparity from its preceding context, and
processing times in human subjects. Similar
insights have been employed in some very recent
</bodyText>
<page confidence="0.996048">
46
</page>
<bodyText confidence="0.999231576923077">
attempts to model compositional word meaning
Erk &amp; Padó (2008) and Thater et al (2009). These
models augment word and context representations
with additional vectors encoding the selectional
preferences (expectations) pertaining to the
specific syntactic/semantic roles of the
participating words. So far these systems rely upon
parsed corpora and have been tested only with
very limited contexts (e.g. pairs of words having
specific dependency relations).
Lexical expectation vectors are based on a
similar and very simple premise: rather than
building a vector for a context by conflating the
features which comprise the various context words
(as per Schutze, 1998), we instead conflate all the
words which might be expected to appear within
the context (i.e. in the headword position).
Consider the following short context taken from
the SemEval-2007 task 2 dataset:
Mr. Meador takes responsibility
for &lt;?&gt; and property management .
The strongest twenty elements of its
expectation vector (as generated by the system
described below) are shown in table 1. The figures
represent some measure of confidence that a given
word will be found in the headword position &lt;?&gt;.
</bodyText>
<table confidence="0.9998075">
0.42 education 0.31 chancellor
0.38 forms 0.31 routine
0.36 housing 0.31 health
0.35 counselling 0.31 research
0.35 these 0.31 assessment
0.35 herself 0.3 detailed
0.34 database 0.3 management
0.33 injuries 0.3 many
0.32 advice 0.3 training
0.31 this 0.3 what
</table>
<tableCaption confidence="0.999962">
Table 1: An example of an expectation vector.
</tableCaption>
<bodyText confidence="0.991140136363637">
We make the supposition that when the vectors
implied by the respective likelihoods of all words
implied by two contexts are identical, the contexts
can be considered semantically equivalent.1 Note
that the actual headword appearing in the context
is not taken into consideration for the purposes of
calculating expectation. In this example it occur at
rank 62 out of ~650,000, implying that its use in
this context is not atypical.
Formal approach. For the purpose of our present
research, we adopt the following formal
framework for generating an expectation vector.
1 Equivalent with respect to the head of the context.
This is not the same as saying the passages have the same
meaning, which requires recourse to compositionality.
Given a context c, each component of the
expectation vector e arising from that context is
estimated thusly:
Where j is a given word type in the lexicon, Oj
is the set of all observed contexts of that word
type in some corpus, ojk is the kth observed context
of that word type, and sim(o,c) is some similarity
measure between two contexts.
The process of generating an expectation vector
can be thought of as a kind of transform from
syntagmatic space, into paradigmatic space. This
mapping need not be trivial: items which are close
in the syntagmatic space need not be close in the
paradigmatic space and vice-versa (although in
practice we expect some considerable correlation
by virtue of the distributional hypothesis). Note
that although our work herein assumes a popular
vector representation of context, the nature of the
contexts and the similarity measure which operates
upon them are not constrained in any way by the
framework given above. For example they may
equally well be dependency trees.
In the following section we outline a distance-
based language model comprising a context model
and a similarity metric which operates upon it.
This choice of model allows us to maintain a
purely distributional approach without suffering
the data-sparseness associated with n-gram
models.
</bodyText>
<subsectionHeader confidence="0.994918">
3.2 Language model
</subsectionHeader>
<bodyText confidence="0.999982428571429">
Theoretical motivation. The precise relationship
between syntagmatic and paradigmatic spaces
implied by the expectation transform depends
upon the language model employed. In a naive
language model which assumes independence
between features, this mapping can be fully
represented by a square matrix over word types.
Although such models are the mainstay of many
systems in NLP, adopting the toolset of an
expection transform in such a case gains us little.
Therefore the relevance of the approach to the
present task depends wholly upon having a
suitably sophisticated language model.
Building on the work of Washtell (2009) and
Terra &amp; Clarke (2004), a distance-based language
model is used in the present work. This is in
contrast to the bag-of-words, n-gram, or syntactic
dependency models more commonly described in
the NLP literature. There are two hypothesised
advantages to this approach. Firstly, this avoids the
issue of immediate context versus wider topical
</bodyText>
<page confidence="0.9985">
47
</page>
<bodyText confidence="0.999973662162162">
context. While immediate context is generally
accepted to play a dominant role in WSD, both
near and far context have been shown to be useful
- the specific balance being somewhat dependent
on the ambiguous word in question (Yarowsky,
1993; Gale et al, 1992; Leacock et al, 1996). As
Ide &amp; Veronis (1998) astutely observe, “although
a distinction is made between micro-context and
topical context in current WSD work, it is not
clear that this distinction is meaningful. It may be
more useful to regard the two as lying along a
continuum, and to consider the role and
importance of contextual information as a function
of distance from the target.” This is precisely the
assumption adopted herein. Secondly, the use of
distance-based information alleviates data
sparseness. This is simply by virtue of the fact that
all words types in a document form part of a
token&apos;s context (barring document boundaries, no
cut-off distance is imposed). Moreover, as it is
specific distance information which is being
recorded, rather than (usually low) frequency
counts, context vector components and the
similarity measurements which arise from them
exhibit good precision. Washtell (2009) showed
that these properties of distance-based metrics lead
to measurable gains in information extracted from
a corpus. In the context of modelling human
notions of association this also led to improved
predictive power (Washtell &amp; Markert, 2009).
Formal approach. We do not pre-compute any
statistical representation of the data upon which
our language model draws. With available
approaches this would either require throwing
away a large number of potentially relevant
higher-order dependencies, or would otherwise be
intractable. Our intuition is that the truest
representation of the language encoded in the
corpus is the corpus itself. We therefore use an
indexed corpus directly for all queries.
We use the following as a prototype measure of
structural similarity (see section 3.1), although
note that others are by all means possible.
Where o and c are context vectors whose j
components each specify the position in the text of
the nearest occurrence (to the head of the context)
of a given word type. O and C are the set of
indices of all non-zero (i.e. observed) components
in o and c respectively. The head of the context is
represented by an additional component in vectors
o and c, and is always treated as observed. f is a
further function of the positions of words p and q
in both contexts. It returns a similarity score in the
unit range designating how similar the distance
opHoq is to that of cpHcq.
The more consistent the relative positions of
the various symbols comprising two contexts, the
stronger their similarity. Note that the measure is
additive: symbols which occur at all in both
contexts result in positive score contributions. We
assume that a context is usually incomplete (i.e.
that that which lies outside it is unknown, rather
than non-existent). The minimum operator in the
denominator (the normalization factor) therefore
ensures that words present only in the larger of
two contexts do not constitute negative evidence.
This formulation allows for considerable
leeway in how word distances are represented and
compared. In this work we choose to treat
distances proportionately, so small variations in
word position between distant (presumably
topically related words) are tolerated better than
similar distance variations between neighbouring
(more syntactico-semantically related) words.
</bodyText>
<sectionHeader confidence="0.956917" genericHeader="method">
4 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.9997732">
A WSD system based on expectation vectors was
ineligible in the SemEval-2010 WSI/WSD task by
virtue of restrictions disallowing the use of a
corpus-based language model. Instead, this task
implicitly encouraged participants to focus on
context feature selection and clustering
approaches. It seems unlikely to us that these
stages are where the major bottlenecks for WSD
(or WSI) lie; performing WSD on short contexts
without any extra-contextual information (i.e.
general linguistic or domain experience) is
arguably not a task which even humans could be
expected to perform well. For this reason we have
chosen to focus initially on the well explored
SemEval-2007 task 2 dataset.
</bodyText>
<subsectionHeader confidence="0.998327">
4.1 Preliminary Evaluation
</subsectionHeader>
<bodyText confidence="0.999991166666667">
An expectation vector was produced for each
training and test instance in the SemEval dataset
by matching the headword’s context against that of
each word position in the British National Corpus
using an implementation of the distance based
similarity measure outlined in section 3.2. For
matters of convenience, independent forwards and
backwards expectation vectors were produced
from the context preceding the headword and that
following it, and their elements were multiplied
together to produce the final vector. No
lemmatization or part-of-speech tagging was
</bodyText>
<page confidence="0.998128">
48
</page>
<bodyText confidence="0.998977">
employed. Neither was any dimensionality
reduction, each vector therefore having ~650,000
elements: one for each word type in the corpus.
Each test sample&apos;s vector was compared against
all corresponding training sample vectors using
both cosine similarity and Euclidean distance2. In
the MAX setups (see Table 2), each test case was
assigned the sense of the single nearest training
example according to the metric being used. In the
CosOR setup, sense scores were generated by
applying a probabilistic OR operation over the
squared Cosine similarities of all relevant training
examples3. The BaseMFS setup is a popular
baseline in which the most frequent sense in the
training set for a given ambiguous word is
attributed to every test case.
</bodyText>
<figure confidence="0.790153875">
Nouns Verbs All
83.6 ▲6.1
▼22.8 70.5 ▲7.6
▼14.4 79.5 ▲6.7
▼19.5
78.9 67.0 75.1
83.5 66.1 78.0
78.8 65.5 74.5
</figure>
<tableCaption confidence="0.95868125">
Table 2: Recall on SemEval WSD task, including
relative performance gain (▲) and error reduction (▼)
over baseline for best setup (preliminary based on first
25% of test cases).
</tableCaption>
<table confidence="0.84532">
Nouns Verbs All
▲ 7.3 ▲ 0.0 ▲ 3.7
86.8 ▼30.9 76.2 ▼0.0 81.6
▼13.6
80.9 76.2 78.7
</table>
<tableCaption confidence="0.726525333333333">
Table 3: Recall of best official SemEval WSD systems
(Agirre &amp; Soroa, 2007), showing relative performance
gain and error reduction over baseline.
</tableCaption>
<bodyText confidence="0.999488909090909">
Table 2 shows the results for each test case in
terms of recall, for all words and for nouns and
verbs separately. Also shown in table 3 are the best
and baseline figures for the official entries from
the Semeval workshop. Note that figures are not
directly comparable between tables because our
preliminary results represent only the first 25% of
the SemEval dataset (hence the different
baselines). To aid some comparison, figures are
included in both tables indicating the relative
increases in recall over the baseline, and relative
</bodyText>
<footnote confidence="0.650816833333333">
2 Cosine Similarity captures the similarity between the
relative proportions of features present in each of two
vectors. By contrast, Euclidean Distance compares the
actual values of corresponding features.
3 Although encountered rarely in the literature, squared
Cosine Similarity is a pertinent quantity for tasks that go
</footnote>
<bodyText confidence="0.940985727272727">
beyond simple ranking. As with Pearson&apos;s R2, it represents
the degree or proportion of similarity (consider that the
square of an angle&apos;s cosine and that of its sine total 1).
reduction in error. Note that the system employed
here is not a word sense induction system as were
most of those participating in the official SemEval
task. The setup of the tasks however allows for
systems which perform poorly under the induction
evaluation to perform competitively as
disambiguation systems, so we are not precluded
from making meaningful comparisons here.
</bodyText>
<sectionHeader confidence="0.996383" genericHeader="evaluation">
5 Discussion and Future Direction
</sectionHeader>
<bodyText confidence="0.999805804347826">
We have presented a new type of wordspace
model based on vectors derived from the
predictions of a language model applied to a
context, rather than directly from the features of a
context itself. We have conducted a preliminary
investigation of the semantic modelling power of
such vectors in the setting of a popular WSD task.
The results are very encouraging. Although it is
too early to draw hard conclusions, preliminary
results suggest a performance at least comparable
the present state of the art on this task. What is
particularly noteworthy is that the approach taken
here seems to perform equally well at
discriminating verbs and nouns. Verbs have
traditionally proven very problematic: none of the
six SemEval systems were able to improve upon
the verb baseline. More recent studies have
focused on discriminating nouns (Brody &amp; Lapata,
2009; Klapaftis &amp; Manandhar, 2007).
Further gains might be expected by employing
a corpus which is more closely matched to the
material being disambiguated, such as the Wall
Street Journal in the present case.
It is also worth noting that the system presented
here was aided only by an untagged un-
lemmatised corpus, without the use of any
structured knowledge sources. While we expect
that judicious use of lemmatization could improve
these results, we believe the key to the quality of
expectation vectors is in the specific predictive
language model employed. We have scarcely
experimented with this, opting for a relatively
untested distance-based model throughout, and
choosing instead to experiment with the
application of different vector similarity measures.
While the nature of the language model used
enables it to capture complex interdependencies,
and long-range dependencies, it is based on direct
querying of a corpus and therefore does not scale
at all well. This makes its use in the context of
most applications or with larger corpora untenable.
Exploring alternative language models (drawing
upon the copious research in this field) is therefore
a focus for future research; the ability to do this
highlights one of the major advantages of this
approach to modelling meaning.
</bodyText>
<figure confidence="0.996079166666667">
CosMAX
EucMAX
CosOR
BaseMFS
BEST
BaseMFS
</figure>
<page confidence="0.987764">
49
</page>
<reference confidence="0.99312868627451">
References 50 associations: comparing syntagmatic and
Eneko Agirre, Airotr Soroa, 2007, SemEval-2007 paradigmatic approaches. In Proceedings of the 19th
Task 02: Evaluating Word Sense Induction and international Conference on Computational
Discrimination Systems Linguistics.
Fred Attneave, Applications of Information Hinrich Schütze, 1998, Automatic Word Sense
Theory to Psychology: A Summary of Basic Discrimination, Computational Linguistics, 24(1),
Concepts, Methods, and Results, Holt, New York pages 97-123
Samuel Brody, Mirella Lapata, 2009, Bayesian Ted Pedersen, 2008, Computational Approaches
Word Sense Induction, Proceedings of EACL 2009, to Measuring the Similarity of Short Contexts: A
pages 103-111 Review of Applications and Methods, to appear
Daniel Chandler, 2002, Semiotics: The Basics, Amruta Purandare, Ted Pederson, 2004, Word
p88, Routleadge sense discrimination by clustering contexts in vector
Ido Dagan, Shaul Marcus, Shaul Markovitch, and similarity spaces. Proceeding of the Conference
1995, Contextual Word Similarity and Estimation on Computational Natural Language Learning, pages
from Sparse Data, Proceedings of 31st ACL, pages 41-48
164-171 Yonggang Qiu, Hans-Peter Frei, 1993,
Katrin Erk, Sebastian Padó, 2008, A Structured Proceedings of the 16th annual international ACM
Vector Space Model for Word Meaning in Context, SIGIR conference on Research and development in
Proceedings on EMNLP 2008 information retrieval table of contents, 160-169
William Gale, Kenneth Church, , David Miachael Riffaterre, 1978, Semiotics Of Poetry,
Yarowsky, 1992, A Method for Disambiguating Word Methuen
Senses in a Large Corpus, Computers and the Hae Jong Seo, Peyman Milanfar, 2009, Training-
Humanities, 26, 415-429 free, Generic Object Detection using Locally
Zellig Harris, 1954, Distributional structure. Adaptive Regression Kernels, IEEE Transactions on
Word, 10(23), pages 146-162 Pattern Analysis and Machine Intelligence, 99
Nancy Ide, Jean Veronis, 1998, Word Sense Claude E Shannon, 1948, A Mathematical Theory
Disambiguation: The State of The Art, of Communication, Bell System Technical Journal,
Mahesh Joshi, Carolyn Penstein-Rosé, 2009, vol. 27, pages 379-423, 623-656,
Generalizing Dependency Features for Opinion Egidio Terra, Charles L. A. Clarke, 2004, Fast
Mining, Proceeding of the ACL-IJCNLP 2009 computation of lexical affinity models , Proceedings
Conference Short Papers, pages 313-316 of the 20th international conference on
Yael Karov, Shimon Edelman, 1996, Learning Computational Linguistics
Similarity-Based Word Sense Disambiguation from Stefan Thater, Georgiana Dinu, Manfred Pinkal,
Sparse Data 2009, Ranking Paraphrases in Context, Proceedings
Ioannis Klapaftis, Suresh Manandhar, 2008, Word of the 2009 Workshop on Applied Textual Inference,
Sense Induction using Graphs of Collocations, pages 44-47
Proceedings of ECAI 2008, pages 298-302 Justin Washtell. 2009. Co-dispersion: A
Claudia Leacock, Geoffrey Towell, Ellen M. windowless approach to lexical association. In
Voorhees, 1996, Towards Building Contextual Proceedings of EACL-2009.
Representations of Word Senses Using Statistical Justin Washtell, Katja Markert. 2009. Comparing
Models. In B. Boguraev and H.Pustejovsky (eds) windowless and window-based computational
Corpus Processing for Lexical Acquisition. MIT association measures as predictors of syntagmatic
Press, pages 97-113 human associations. In Proceedings of EMNLP-
G. A. Miller, W. G Charles, 1991. Contextual 2009, pages 628-637.
correlates of semantic similarity. Language and Xuheng Xu, Xiaodan Zhang, Xiaohua Hu, 2007,
Cognitive Processes, 6, 1-28. Using Two-Stage Concept-Based Singular Value
Winfried Noth, 1990, Handbook of Semiotics, Decomposition Technique as a Query Expansion
Indiana University Press, p 142 Strategy, AINAW&apos;07
Reinhard Rapp. 2002. The computation of word David Yarowsky, 1993, One Sense Per
Collocation, Proceedings on the Workshop on
Human Language Technology, pages 266-271
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.637888">
<title confidence="0.9980495">Expectation Vectors: A Semiotics Inspired to Geometric Lexical-Semantic Representation</title>
<author confidence="0.993788">Justin</author>
<affiliation confidence="0.8368855">University of Leeds,</affiliation>
<email confidence="0.988421">washtell@comp.leeds.ac.uk</email>
<abstract confidence="0.998294444444444">We introduce a new family of geometric models of meaning, inspired by principles from semiotics and information theory, based on what we call Expectation Vectors. We present theoretical arguments in support of these representations over traditional context-feature vectors: primarily that they provide a more intuitive representation of meaning, and detach vector representation from the specific context features thereby allowing arbitrarily sophisticated language models to be leveraged. We present a preliminary evaluation of an expectation vector based word sense disambiguation system using the SemEval-2007 task 2 dataset, with very encouraging results, particularly with respect to ambiguous verbs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>associations: comparing syntagmatic and paradigmatic approaches.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<booktitle>In Proceedings of the 19th international Conference on Computational Linguistics. Hinrich Schütze,</booktitle>
<volume>24</volume>
<issue>1</issue>
<pages>97--123</pages>
<institution>Semiotics Of Poetry, Methuen Hae Jong Seo, Peyman Milanfar,</institution>
<location>Yonggang Qiu, Hans-Peter Frei,</location>
<marker>Pedersen, 1998</marker>
<rawString>associations: comparing syntagmatic and paradigmatic approaches. In Proceedings of the 19th international Conference on Computational Linguistics. Hinrich Schütze, 1998, Automatic Word Sense Discrimination, Computational Linguistics, 24(1), pages 97-123 Ted Pedersen, 2008, Computational Approaches to Measuring the Similarity of Short Contexts: A Review of Applications and Methods, to appear Amruta Purandare, Ted Pederson, 2004, Word sense discrimination by clustering contexts in vector and similarity spaces. Proceeding of the Conference on Computational Natural Language Learning, pages 41-48 Yonggang Qiu, Hans-Peter Frei, 1993, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval table of contents, 160-169 Miachael Riffaterre, 1978, Semiotics Of Poetry, Methuen Hae Jong Seo, Peyman Milanfar, 2009, Training- free, Generic Object Detection using Locally Adaptive Regression Kernels, IEEE Transactions on Pattern Analysis and Machine Intelligence, 99 Claude E Shannon, 1948, A Mathematical Theory of Communication, Bell System Technical Journal, vol. 27, pages 379-423, 623-656, Egidio Terra, Charles L. A. Clarke, 2004, Fast computation of lexical affinity models , Proceedings of the 20th international conference on Computational Linguistics Stefan Thater, Georgiana Dinu, Manfred Pinkal, 2009, Ranking Paraphrases in Context, Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44-47 Justin Washtell. 2009. Co-dispersion: A windowless approach to lexical association. In Proceedings of EACL-2009. Justin Washtell, Katja Markert. 2009. Comparing windowless and window-based computational association measures as predictors of syntagmatic human associations. In Proceedings of EMNLP- 2009, pages 628-637. Xuheng Xu, Xiaodan Zhang, Xiaohua Hu, 2007, Using Two-Stage Concept-Based Singular Value Decomposition Technique as a Query Expansion Strategy, AINAW&apos;07 David Yarowsky, 1993, One Sense Per Collocation, Proceedings on the Workshop on Human Language Technology, pages 266-271</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>