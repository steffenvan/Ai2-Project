<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.033699">
<title confidence="0.983425">
On WordNet Semantic Classes and Dependency Parsing
</title>
<author confidence="0.9593795">
Kepa Bengoetxea†, Eneko Agirre†, Joakim Nivre‡,
Yue Zhang*, Koldo Gojenola†
</author>
<affiliation confidence="0.994252666666667">
†University of the Basque Country UPV/EHU / IXA NLP Group
‡Uppsala University / Department of Linguistics and Philology
∗ Singapore University of Technology and Design
</affiliation>
<email confidence="0.842415">
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
</email>
<sectionHeader confidence="0.993242" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998950625">
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al., 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975666666667">
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al., 2011; Fujita et al., 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al.,
2008; Suzuki et al., 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
</bodyText>
<listItem confidence="0.957174666666667">
• Does semantic information in WordNet help
dependency parsing? Agirre et al. (2011)
found improvements in dependency parsing
</listItem>
<bodyText confidence="0.991290666666667">
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
</bodyText>
<listItem confidence="0.92616105882353">
• Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
• How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
• How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
• Does parser combination benefit from seman-
</listItem>
<bodyText confidence="0.8990895">
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999782428571429">
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al. (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
</bodyText>
<page confidence="0.988986">
649
</page>
<bodyText confidence="0.990631794871795">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649–655,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al.
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al. (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al. (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al.
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T¨ackstr¨om et al. (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
</bodyText>
<sectionHeader confidence="0.998863" genericHeader="method">
3 Experimental Framework
</sectionHeader>
<bodyText confidence="0.9998076">
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
</bodyText>
<subsectionHeader confidence="0.999136">
3.1 Treebank conversions
</subsectionHeader>
<bodyText confidence="0.9891555">
Penn2Malt1 performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
</bodyText>
<footnote confidence="0.976116">
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
</footnote>
<bodyText confidence="0.999943923076923">
compare our results with related experiments (Koo
et al., 2008; Suzuki et al., 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH2 (Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
</bodyText>
<subsectionHeader confidence="0.999424">
3.2 Parsers
</subsectionHeader>
<bodyText confidence="0.999942558823529">
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al., 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST3 represents global, exhaustive graph-
based parsing (McDonald et al., 2005; McDon-
ald et al., 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar4 (Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
</bodyText>
<footnote confidence="0.999990666666667">
2http://nlp.cs.lth.se/software/treebank converter
3http://mstparser.sourceforge.net
4www.sourceforge.net/projects/zpar
</footnote>
<page confidence="0.988299">
650
</page>
<table confidence="0.999561">
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)$
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)† 91.74 (+0.22)
</table>
<tableCaption confidence="0.881244666666667">
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (†: p &lt;0.05, ‡: p
&lt;0.005). In parenthesis, difference with baseline.
</tableCaption>
<table confidence="0.9997466">
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)$ 84.99 (-0.07) 86.18 (+1.12)$
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
</table>
<tableCaption confidence="0.931099">
Table 2: LAS results with several parsing algo-
</tableCaption>
<bodyText confidence="0.9496015">
rithms in the LTH conversion (†: p &lt;0.05, ‡: p
&lt;0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al., 2008) on LTH, in order to
deal with non-projective arcs.
</bodyText>
<subsectionHeader confidence="0.997457">
3.3 Semantic information
</subsectionHeader>
<bodyText confidence="0.99981776">
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al. (2008) and
Agirre et al. (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al. (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al., 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al. (2008) describe a semi-
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999984192307692">
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser’s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al. (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2–21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel’s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
</bodyText>
<page confidence="0.989861">
651
</page>
<table confidence="0.998484">
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers: 91.93 (+0.41) 92.95
3 baselines + 3 SF extensions
Best combination of 3 parsers: 91.87 (+0.35) 92.92
3 baselines + 3 SS extensions
Best combination of 3 parsers: 91.90 (+0.38) 92.90
3 baselines + 3 cluster extensions
</table>
<tableCaption confidence="0.982894">
Table 3: Parser combinations on Penn2Malt.
</tableCaption>
<table confidence="0.9922154">
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar+ SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers: 89.56 (+0.41)‡ 92.23
3 baselines + 3 SF extensions
Best combination of 3 parsers: 89.43 (+0.28) 93.12
3 baselines + 3 SS extensions
Best combination of 3 parsers: 89.52 (+0.37)† 92.19
3 baselines + 3 cluster extensions
</table>
<tableCaption confidence="0.988343">
Table 4: Parser combinations on LTH (†: p &lt;0.05,
‡: p &lt;0.005).
</tableCaption>
<subsectionHeader confidence="0.994674">
4.1 Single Parsers
</subsectionHeader>
<bodyText confidence="0.999993129032258">
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
</bodyText>
<subsectionHeader confidence="0.979174">
4.2 Combinations
</subsectionHeader>
<bodyText confidence="0.993306466666667">
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al., 2007), combining six transition-based
parsers. We used MaltBlender5, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser’s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
</bodyText>
<listItem confidence="0.996247666666667">
• Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
• Extended parsers, adding semantic informa-
</listItem>
<bodyText confidence="0.9756232">
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
</bodyText>
<subsectionHeader confidence="0.998326">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.997387">
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
</bodyText>
<footnote confidence="0.982788">
5http://w3.msi.vxu.se/users/jni/blend/
</footnote>
<page confidence="0.989809">
652
</page>
<table confidence="0.9966076">
POS tags Parser LAS test set LAS on sentences LAS on sentences
without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
</table>
<tableCaption confidence="0.992225">
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
</tableCaption>
<bodyText confidence="0.990565125">
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar’s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net’s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters’ information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999993222222222">
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al., 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9986456">
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
</bodyText>
<page confidence="0.999109">
653
</page>
<sectionHeader confidence="0.990095" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999838915887851">
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317–325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699–703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Marie Candito and Djam´e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76–84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132–139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1–8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105–112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1–11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595–603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL’04), Main Volume, pages 279–286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings ofACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551–560, Singapore, August. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.990056">
654
</page>
<reference confidence="0.999364884615385">
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477–
487, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195–
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562–
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998874">
655
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.141848">
<title confidence="0.999984">On WordNet Semantic Classes and Dependency Parsing</title>
<author confidence="0.836509">Eneko Joakim Zhang</author>
<author confidence="0.836509">Koldo</author>
<affiliation confidence="0.993343">of the Basque Country UPV/EHU / IXA NLP Group University / Department of Linguistics and Philology University of Technology and Design</affiliation>
<abstract confidence="0.89919665">kepa.bengoetxea@ehu.es, e.agirre@ehu.es, joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg, koldo.gojenola@ehu.es Abstract This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study the effect of semantic classes in three dependency parsers, using two types of constituencyto-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011). In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Timothy Baldwin</author>
<author>David Martinez</author>
</authors>
<title>Improving parsing and PP attachment performance with sense information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1500" citStr="Agirre et al., 2008" startWordPosition="209" endWordPosition="212">ombinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing using MaltParser on gold POS tags. In this work, we will investigate the effect o</context>
<context position="3682" citStr="Agirre et al. (2008)" startWordPosition="553" endWordPosition="556">ntexts. We will run parser combination experiments with and without semantic information, to determine whether it is useful in the combined parsers. After introducing related work in section 2, section 3 describes the treebank conversions, parsers and semantic features. Section 4 presents the results and section 5 draws the main conclusions. 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora. In the first group, Agirre et al. (2008) trained two state-of-the-art constituency-based statistical parsers (Charniak, 649 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649–655, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2000; Bikel, 2004) on semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized. The results showed a signicant improvemen</context>
<context position="9936" citStr="Agirre et al. (2008)" startWordPosition="1489" endWordPosition="1492">arenthesis, difference with baseline. feature templates, associated with the top of the stack and the head of the queue, respectively. ZPar was directly trained on the Penn2Malt conversion, while we applied the pseudo-projective transformation (Nilsson et al., 2008) on LTH, in order to deal with non-projective arcs. 3.3 Semantic information Our aim was to experiment with different types of WordNet-related semantic information. For comparison with automatically acquired information, we will also experiment with bit clusters. WordNet. We will experiment with the semantic representations used in Agirre et al. (2008) and Agirre et al. (2011), based on WordNet 2.1. WordNet is organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun SFs differentiate nouns denoting acts or actions, and nouns denoting animals, among others. We experiment with both full SSs and SFs as instances of finegrained and coarse-grained semantic representation, respectively. As an example, knife in its tool sense is in the E</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and PP attachment performance with sense information. In Proceedings of ACL-08: HLT, pages 317–325, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Kepa Bengoetxea</author>
<author>Koldo Gojenola</author>
<author>Joakim Nivre</author>
</authors>
<title>Improving dependency parsing with semantic classes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>699--703</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="847" citStr="Agirre et al., 2011" startWordPosition="112" endWordPosition="115">stics and Philology ∗ Singapore University of Technology and Design kepa.bengoetxea@ehu.es, e.agirre@ehu.es, joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg, koldo.gojenola@ehu.es Abstract This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study the effect of semantic classes in three dependency parsers, using two types of constituencyto-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011). In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such </context>
<context position="4421" citStr="Agirre et al. (2011)" startWordPosition="656" endWordPosition="659">ing of the Association for Computational Linguistics (Short Papers), pages 649–655, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2000; Bikel, 2004) on semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized. The results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes. MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score by 1%, while some methods produce substantial decreases in performance. Fujita et al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et a</context>
<context position="9961" citStr="Agirre et al. (2011)" startWordPosition="1494" endWordPosition="1497">th baseline. feature templates, associated with the top of the stack and the head of the queue, respectively. ZPar was directly trained on the Penn2Malt conversion, while we applied the pseudo-projective transformation (Nilsson et al., 2008) on LTH, in order to deal with non-projective arcs. 3.3 Semantic information Our aim was to experiment with different types of WordNet-related semantic information. For comparison with automatically acquired information, we will also experiment with bit clusters. WordNet. We will experiment with the semantic representations used in Agirre et al. (2008) and Agirre et al. (2011), based on WordNet 2.1. WordNet is organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for verbs, and 26 for nouns), based on syntactic and semantic categories. For example, noun SFs differentiate nouns denoting acts or actions, and nouns denoting animals, among others. We experiment with both full SSs and SFs as instances of finegrained and coarse-grained semantic representation, respectively. As an example, knife in its tool sense is in the EDGE TOOL USED AS A CUTTIN</context>
<context position="12519" citStr="Agirre et al. (2011)" startWordPosition="1924" endWordPosition="1927">epresentation of syntactic-semantic information acquired from corpora. They use short strings of 4-6 bits to represent parts of speech and the full strings for wordforms. 4 Results In all the experiments we employed a baseline feature set using word forms and parts of speech, and an enriched feature set (WordNet or clusters). We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser’s performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in Agirre et al. (2011), we adopted the simpler approach of combining the outputs of the individual parsers by voting (Sagae and Lavie, 2006). We will use Labeled Attachment Score (LAS) as our main evaluation criteria. As in previous work, we exclude punctuation marks. For all the tests, we used a perceptron POS-tagger (Collins, 2002), trained on WSJ sections 2–21, to assign POS tags automatically to both the training (using 10-way jackknifing) and test data, obtaining a POS tagging accuracy of 97.32% on the test data. We will make use of Bikel’s randomized parsing evaluation comparator to test the statistical signi</context>
<context position="21668" citStr="Agirre et al., 2011" startWordPosition="3435" endWordPosition="3438">ar (0.91 and 1.33, repectively). This aspect deserves further investigation, as the improvements seem to be related to both the type of semantic information and the parsing algorithm.We did an initial exploration but it did not give any clear indication of the types of improvements that could be expected using each parser and semantic data. 5 Conclusions This work has tried to shed light on the contribution of semantic information to dependency parsing. The experiments were thorough, testing two treebank conversions and three parsing paradigms on automatically predicted POS tags. Compared to (Agirre et al., 2011), which used MaltParser on the LTH conversion and gold POS tags, our results can be seen as a negative outcome, as the improvements are very small and non-significant in most of the cases. For parser combination, WordNet semantic file information does give a small significant increment in the more fine-grained LTH representation. In addition we show that the improvement of automatic clusters is also weak. For the future, we think tdifferent parsers, eitherhat a more elaborate scheme is needed for word classes, requiring to explore different levels of generalization in the WordNet (or alternati</context>
</contexts>
<marker>Agirre, Bengoetxea, Gojenola, Nivre, 2011</marker>
<rawString>Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and Joakim Nivre. 2011. Improving dependency parsing with semantic classes. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 699–703, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins’ parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="3995" citStr="Bikel, 2004" startWordPosition="593" endWordPosition="594"> the main conclusions. 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora. In the first group, Agirre et al. (2008) trained two state-of-the-art constituency-based statistical parsers (Charniak, 649 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649–655, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2000; Bikel, 2004) on semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized. The results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Djam´e Seddah</author>
</authors>
<title>Parsing word clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>76--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA, USA,</location>
<contexts>
<context position="5528" citStr="Candito and Seddah (2010)" startWordPosition="820" endWordPosition="823">based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}. They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank. Suzuki et al. (2009), Sagae and Gordon (2009) and Candito and Seddah (2010) also experiment with the same cluster method. Recently, T¨ackstr¨om et al. (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. 3 Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and direct conversion from the consti</context>
</contexts>
<marker>Candito, Seddah, 2010</marker>
<rawString>Marie Candito and Djam´e Seddah. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76–84, Los Angeles, CA, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, NAACL 2000, pages 132–139, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="12832" citStr="Collins, 2002" startWordPosition="1977" endWordPosition="1978">usters). We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser’s performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in Agirre et al. (2011), we adopted the simpler approach of combining the outputs of the individual parsers by voting (Sagae and Lavie, 2006). We will use Labeled Attachment Score (LAS) as our main evaluation criteria. As in previous work, we exclude punctuation marks. For all the tests, we used a perceptron POS-tagger (Collins, 2002), trained on WSJ sections 2–21, to assign POS tags automatically to both the training (using 10-way jackknifing) and test data, obtaining a POS tagging accuracy of 97.32% on the test data. We will make use of Bikel’s randomized parsing evaluation comparator to test the statistical signicance of the results. In all of the experiments the parsers were trained on sections 2-21 of the PTB and evaluated on the development set (section 22). Finally, the best performing system was evaluated on the test set (section 23). 651 Parsers LAS UAS Best baseline (ZPar) 91.52 92.57 Best single parser (ZPar + C</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="4171" citStr="Collins, 2003" startWordPosition="617" endWordPosition="618"> as WordNet and approaches that use information automatically acquired from corpora. In the first group, Agirre et al. (2008) trained two state-of-the-art constituency-based statistical parsers (Charniak, 649 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649–655, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2000; Bikel, 2004) on semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized. The results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes. MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Exploiting semantic information for hpsg parse selection.</title>
<date>2010</date>
<journal>Research on Language and Computation,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1543" citStr="Fujita et al., 2010" startWordPosition="217" endWordPosition="220"> enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing using MaltParser on gold POS tags. In this work, we will investigate the effect of semantic information using predicted POS </context>
<context position="4864" citStr="Fujita et al. (2010)" startWordPosition="724" endWordPosition="728"> results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes. MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score by 1%, while some methods produce substantial decreases in performance. Fujita et al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}. They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank. Suzuki et a</context>
</contexts>
<marker>Fujita, Bond, Oepen, Tanaka, 2010</marker>
<rawString>Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2010. Exploiting semantic information for hpsg parse selection. Research on Language and Computation, 8(1):122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Glsen Eryigit</author>
<author>Beta Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? a study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task EMNLP-CoNLL.</booktitle>
<contexts>
<context position="16144" citStr="Hall et al., 2007" startWordPosition="2528" endWordPosition="2531">d clusters are specially effective with the MST parser in both treebank conversions, which suggests that the type of semantic information has a direct relation to the parsing algorithm. Section 4.3 will look at the details by each knowledge type. 4.2 Combinations Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5, a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algorithms. This set includes the 3 baseline algorithms, MaltParser, MST, and ZPar. • Extended parsers, adding semantic information to the baselines. We include the three base algorithms and their semantic extensions (SF, SS, and</context>
</contexts>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit, Beta Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? a study in multilingual parser optimization. In Proceedings of the CoNLL Shared Task EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="6580" citStr="Johansson and Nugues, 2007" startWordPosition="973" endWordPosition="976">ly, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and direct conversion from the constituency-based PTB to a dependency treebank. It obtains projective trees and has been used in several works, which allows us to 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html compare our results with related experiments (Koo et al., 2008; Suzuki et al., 2009; Koo and Collins, 2010). We extracted dependencies using standard head rules (Yamada and Matsumoto, 2003), and a reduced set of 12 general dependency tags. LTH2 (Johansson and Nugues, 2007) presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links to handle long-distance phenomena, giving a 6.17% of nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a sing</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6414" citStr="Koo and Collins, 2010" startWordPosition="948" endWordPosition="951">his section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and direct conversion from the constituency-based PTB to a dependency treebank. It obtains projective trees and has been used in several works, which allows us to 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html compare our results with related experiments (Koo et al., 2008; Suzuki et al., 2009; Koo and Collins, 2010). We extracted dependencies using standard head rules (Yamada and Matsumoto, 2003), and a reduced set of 12 general dependency tags. LTH2 (Johansson and Nugues, 2007) presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links to handle long-distance phenomena, giving a 6.17% of nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigm</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1642" citStr="Koo et al., 2008" startWordPosition="232" endWordPosition="235">nversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing using MaltParser on gold POS tags. In this work, we will investigate the effect of semantic information using predicted POS tags. • Is the type of semantic information related to the type of parser? We will test three diffe</context>
<context position="5030" citStr="Koo et al. (2008)" startWordPosition="748" endWordPosition="751">. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes. MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score by 1%, while some methods produce substantial decreases in performance. Fujita et al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}. They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank. Suzuki et al. (2009), Sagae and Gordon (2009) and Candito and Seddah (2010) also experiment with the same cluster method. Recently, T¨ackstr¨om et al. (2012) tested the incorpor</context>
<context position="6369" citStr="Koo et al., 2008" startWordPosition="940" endWordPosition="943">clusters. 3 Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and direct conversion from the constituency-based PTB to a dependency treebank. It obtains projective trees and has been used in several works, which allows us to 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html compare our results with related experiments (Koo et al., 2008; Suzuki et al., 2009; Koo and Collins, 2010). We extracted dependencies using standard head rules (Yamada and Matsumoto, 2003), and a reduced set of 12 general dependency tags. LTH2 (Johansson and Nugues, 2007) presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links to handle long-distance phenomena, giving a 6.17% of nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three </context>
<context position="11308" citStr="Koo et al. (2008)" startWordPosition="1726" endWordPosition="1729">mes of semantic granularity in WordNet. For each semantic representation, we need to determine the semantics of each occurrence of a target word. Agirre et al. (2011) used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word (McCarthy et al., 2004). As we will make use of the full PTB, we only have access to the first sense information. Clusters. Koo et al. (2008) describe a semisupervised approach that makes use of cluster features induced from unlabeled data, providing significant performance improvements for supervised dependency parsers on the Penn Treebank for English and the Prague Dependency Treebank for Czech. The process defines a hierarchical clustering of the words, which can be represented as a binary tree where each node is associated to a bitstring, from the more general (root of the tree) to the more specific (leaves). Using prefixes of various lengths, it can produce clusterings of different granularities. It can be seen as a representa</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT, pages 595–603, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew MacKinlay</author>
<author>Rebecca Dridan</author>
<author>Diana McCarthy</author>
<author>Timothy Baldwin</author>
</authors>
<title>The effects of semantic annotations on precision parse ranking.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>228236</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="4620" citStr="MacKinlay et al. (2012)" startWordPosition="684" endWordPosition="687">n semantically-enriched input, substituting content words with their semantic classes, trying to overcome the limitations of lexicalized approaches to parsing (Collins, 2003) where related words, like scissors and knife, cannot be generalized. The results showed a signicant improvement, giving the first results over both WordNet and the Penn Treebank (PTB) to show that semantics helps parsing. Later, Agirre et al. (2011) successfully introduced WordNet classes in a dependency parser, obtaining improvements on the full PTB using gold POS tags, trying different combinations of semantic classes. MacKinlay et al. (2012) investigate the addition of semantic annotations in the form of word sense hypernyms, in HPSG parse ranking, reducing error rate in dependency F-score by 1%, while some methods produce substantial decreases in performance. Fujita et al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters i</context>
</contexts>
<marker>MacKinlay, Dridan, McCarthy, Baldwin, 2012</marker>
<rawString>Andrew MacKinlay, Rebecca Dridan, Diana McCarthy, and Timothy Baldwin. 2012. The effects of semantic annotations on precision parse ranking. In First Joint Conference on Lexical and Computational Semantics (*SEM), page 228236, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>279--286</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11190" citStr="McCarthy et al., 2004" startWordPosition="1703" endWordPosition="1706">UMENT singleton synset, and also in the ARTIFACT SF along with thousands of words including cutter. These are the two extremes of semantic granularity in WordNet. For each semantic representation, we need to determine the semantics of each occurrence of a target word. Agirre et al. (2011) used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word (McCarthy et al., 2004). As we will make use of the full PTB, we only have access to the first sense information. Clusters. Koo et al. (2008) describe a semisupervised approach that makes use of cluster features induced from unlabeled data, providing significant performance improvements for supervised dependency parsers on the Penn Treebank for English and the Prague Dependency Treebank for Czech. The process defines a hierarchical clustering of the words, which can be represented as a binary tree where each node is associated to a bitstring, from the more general (root of the tree) to the more specific (leaves). Us</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 279–286, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7503" citStr="McDonald et al., 2005" startWordPosition="1116" endWordPosition="1119"> are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child node</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<contexts>
<context position="7527" citStr="McDonald et al., 2006" startWordPosition="1120" endWordPosition="1124">or Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zh</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
</authors>
<title>Generalizing tree transformations for inductive dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 45th Conference of the ACL.</booktitle>
<contexts>
<context position="9582" citStr="Nilsson et al., 2008" startWordPosition="1436" endWordPosition="1439">005). In parenthesis, difference with baseline. Base WordNet WordNet Clusters line SF SS Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18) MST 85.06 85.35 (+0.29)$ 84.99 (-0.07) 86.18 (+1.12)$ ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02) Table 2: LAS results with several parsing algorithms in the LTH conversion (†: p &lt;0.05, ‡: p &lt;0.005). In parenthesis, difference with baseline. feature templates, associated with the top of the stack and the head of the queue, respectively. ZPar was directly trained on the Penn2Malt conversion, while we applied the pseudo-projective transformation (Nilsson et al., 2008) on LTH, in order to deal with non-projective arcs. 3.3 Semantic information Our aim was to experiment with different types of WordNet-related semantic information. For comparison with automatically acquired information, we will also experiment with bit clusters. WordNet. We will experiment with the semantic representations used in Agirre et al. (2008) and Agirre et al. (2011), based on WordNet 2.1. WordNet is organized into sets of synonyms, called synsets (SS). Each synset in turn belongs to a unique semantic file (SF). There are a total of 45 SFs (1 for adverbs, 3 for adjectives, 15 for ver</context>
</contexts>
<marker>Nilsson, Nivre, Hall, 2008</marker>
<rawString>Jens Nilsson, Joakim Nivre, and Johan Hall. 2008. Generalizing tree transformations for inductive dependency parsing. In Proceedings of the 45th Conference of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>A Chanev</author>
<author>Glsen Eryiit</author>
<author>Sandra Kbler</author>
<author>S Marinov</author>
<author>Edwin Marsi</author>
</authors>
<title>Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering.</title>
<date>2007</date>
<contexts>
<context position="7070" citStr="Nivre et al., 2007" startWordPosition="1046" endWordPosition="1049">ndard head rules (Yamada and Matsumoto, 2003), and a reduced set of 12 general dependency tags. LTH2 (Johansson and Nugues, 2007) presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links to handle long-distance phenomena, giving a 6.17% of nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to </context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryiit, Kbler, Marinov, Marsi, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A., Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Andrew Gordon</author>
</authors>
<title>Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eleventh International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="5498" citStr="Sagae and Gordon (2009)" startWordPosition="815" endWordPosition="818">t fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}. They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank. Suzuki et al. (2009), Sagae and Gordon (2009) and Candito and Seddah (2010) also experiment with the same cluster method. Recently, T¨ackstr¨om et al. (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. 3 Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and dir</context>
</contexts>
<marker>Sagae, Gordon, 2009</marker>
<rawString>Kenji Sagae and Andrew Gordon. 2009. Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures. In Proceedings of the Eleventh International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="12637" citStr="Sagae and Lavie, 2006" startWordPosition="1943" endWordPosition="1946">nt parts of speech and the full strings for wordforms. 4 Results In all the experiments we employed a baseline feature set using word forms and parts of speech, and an enriched feature set (WordNet or clusters). We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser’s performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in Agirre et al. (2011), we adopted the simpler approach of combining the outputs of the individual parsers by voting (Sagae and Lavie, 2006). We will use Labeled Attachment Score (LAS) as our main evaluation criteria. As in previous work, we exclude punctuation marks. For all the tests, we used a perceptron POS-tagger (Collins, 2002), trained on WSJ sections 2–21, to assign POS tags automatically to both the training (using 10-way jackknifing) and test data, obtaining a POS tagging accuracy of 97.32% on the test data. We will make use of Bikel’s randomized parsing evaluation comparator to test the statistical signicance of the results. In all of the experiments the parsers were trained on sections 2-21 of the PTB and evaluated on </context>
<context position="15921" citStr="Sagae and Lavie (2006)" startWordPosition="2491" endWordPosition="2494">cantly MST, with the highest increase across the table. Overall, we see that the small improvements do not confirm the previous results on Penn2Malt, MaltParser and gold POS tags. We can also conclude that automatically acquired clusters are specially effective with the MST parser in both treebank conversions, which suggests that the type of semantic information has a direct relation to the parsing algorithm. Section 4.3 will look at the details by each knowledge type. 4.2 Combinations Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5, a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algori</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL-2010),</booktitle>
<location>Los Angeles, CA,</location>
<contexts>
<context position="16796" citStr="Surdeanu and Manning, 2010" startWordPosition="2628" endWordPosition="2632">n-based parsers. We used MaltBlender5, a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algorithms. This set includes the 3 baseline algorithms, MaltParser, MST, and ZPar. • Extended parsers, adding semantic information to the baselines. We include the three base algorithms and their semantic extensions (SF, SS, and clusters). It is known (Surdeanu and Manning, 2010) that adding more parsers to an ensemble usually improves accuracy, as long as they add to the diversity (and almost regardless of their accuracy level). So, for the comparison to be fair, we will compare ensembles of 3 parsers, taken from sets of 6 parsers (3 baselines + 3 SF, SS, and cluster extensions, respectively). In each experiment, we took the best combination of individual parsers on the development set for the final test. Tables 3 and 4 show the results. Penn2Malt. Table 3 shows that the combination of the baselines, without any semantic information, considerably improves the best ba</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL-2010), Los Angeles, CA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semisupervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>551--560</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="1664" citStr="Suzuki et al., 2009" startWordPosition="236" endWordPosition="239">uction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing using MaltParser on gold POS tags. In this work, we will investigate the effect of semantic information using predicted POS tags. • Is the type of semantic information related to the type of parser? We will test three different parsers represent</context>
<context position="5473" citStr="Suzuki et al. (2009)" startWordPosition="811" endWordPosition="814"> al. (2010) showed that fully disambiguated sense-based features smoothed using ontological information are effective for parse selection. On the second group, Koo et al. (2008) presented a semisupervised method for training dependency parsers, introducing features that incorporate word clusters automatically acquired from a large unannotated corpus. The clusters include strongly semantic associations like {apple, pear} or {Apple, IBM} and also syntactic clusters like {of, in}. They demonstrated its effectiveness in dependency parsing experiments on the PTB and the Prague Dependency Treebank. Suzuki et al. (2009), Sagae and Gordon (2009) and Candito and Seddah (2010) also experiment with the same cluster method. Recently, T¨ackstr¨om et al. (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. 3 Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 </context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semisupervised structured conditional models for dependency parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551–560, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>477--487</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477– 487, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th IWPT,</booktitle>
<pages>195--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6496" citStr="Yamada and Matsumoto, 2003" startWordPosition="959" endWordPosition="962">followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types of semantic representation that were used. 3.1 Treebank conversions Penn2Malt1 performs a simple and direct conversion from the constituency-based PTB to a dependency treebank. It obtains projective trees and has been used in several works, which allows us to 1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html compare our results with related experiments (Koo et al., 2008; Suzuki et al., 2009; Koo and Collins, 2010). We extracted dependencies using standard head rules (Yamada and Matsumoto, 2003), and a reduced set of 12 general dependency tags. LTH2 (Johansson and Nugues, 2007) presents a conversion better suited for semantic processing, with a richer structure and a more fine-grained set of dependency labels (42 different dependency labels), including links to handle long-distance phenomena, giving a 6.17% of nonprojective sentences. The results from parsing the LTH output are lower than those for Penn2Malt conversions. 3.2 Parsers We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transi</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th IWPT, pages 195– 206. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="8146" citStr="Zhang and Clark, 2008" startWordPosition="1226" endWordPosition="1229">6) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic 2http://nlp.cs.lth.se/software/treebank converter 3http://mstparser.sourceforge.net 4www.sourceforge.net/projects/zpar 650 Base WordNet WordNet Clusters line SF SS Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562– 571, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="8170" citStr="Zhang and Nivre, 2011" startWordPosition="1230" endWordPosition="1233">st scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic 2http://nlp.cs.lth.se/software/treebank converter 3http://mstparser.sourceforge.net 4www.sourceforge.net/projects/zpar 650 Base WordNet WordNet Clusters line SF SS Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13) MST 90.55 90</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>