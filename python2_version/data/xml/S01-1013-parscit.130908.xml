<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019027">
<title confidence="0.9878915">
The Japanese Translation Task: Lexical and
Structural Perspectives
</title>
<author confidence="0.994924">
Timothy Baldwin,* Atsushi Okazaki,t Takenobu Tokunagat and Hozumi Tanakat
</author>
<affiliation confidence="0.9930795">
* CSLI, Stanford University &lt;tbaldwin@csli stanf ord. edu&gt;
Tokyo Institute of Technology &lt;{okazaki , take ,tanaka}Ocl . cs . titech. ac jp&gt;
</affiliation>
<sectionHeader confidence="0.981895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819090909091">
This paper describes two distinct attempts at the
SENSEVAL-2 Japanese translation task. The first im-
plementation is based on lexical similarity and builds
on the results of Baldwin (2001b; 2001a), whereas
the second is based on structural similarity via the
medium of parse trees and includes a basic model of
conceptual similarity. Despite its simplistic nature,
the lexical method was found to perform the bet-
ter of the two, at 49.1% accuracy, as compared to
41.2% for the structural method and 36.8% for the
baseline.
</bodyText>
<sectionHeader confidence="0.997899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981921568628">
Translation retrieval is defined as the task of, for
a given source language (L1) input, retrieving the
target language (L2) string which best translates it.
Retrieval is carried out over a translation memory
made up of translation records, that is L1 strings
coupled with an L2 translation. A single transla-
tion retrieval task was offered in SENSEVAL-2, from
Japanese into English, and it is this task that we
target in this paper.
Conventionally, translation retrieval is carried out
by way of determining the Ll string in the trans-
lation memory most similar to the input, and re-
turning the L2 string paired with that string as a
translation for the input. It is important to realise
that at no point is the output compared back to the
input to determine its &amp;quot;translation adequacy&amp;quot;, a job
which is left up to the system user.
Determination of the degree of similarity between
the input and Ll component of each translation
record can take a range of factors into consideration,
including lexical (character or word) content, word
order, parse tree topology and conceptual similarity.
In this paper, we focus on a simple character-based
(lexical) method and more sophisticated parse tree
comparison (structural) method.
Both methods discussed herein are fully unsuper-
vised. The lexical method makes use of no exter-
nal resources or linguistic knowledge whatsoever. It
treats each string as a &amp;quot;bag of character bigrams&amp;quot;
and calculates similarity according to Dice&apos;s Coef-
ficient. The structural method, on the other hand,
relies on both morphological and syntactic analysis,
in the form of the publicly-available JUMAN (Kuro-
hashi and Nagao, 1998b) and KNP (Kurohashi and
Nagao, 1998a) systems, respectively, and also the
Japanese Goi-Taikei thesaurus (Ikehara et al., 1997)
to measure conceptual distance. A parse tree is
generated for the L1 component of each translation
record, and also each input, and similarity gauged
by both topological resemblance between parse trees
and conceptual similarity between nodes of the parse
tree.
Translation records used by the two systems were
taken exclusively from the translation memory pro-
vided for the task.
In the proceeding sections, we briefly review the
Japanese translation task (§ 2) and detail our par-
ticular use of the data provided for the task (§ 3).
Next, we outline the lexical method (§ 4) and struc-
tural method (§ 5), and compare and discuss the
performance of the two methods (§ 6).
</bodyText>
<sectionHeader confidence="0.856024" genericHeader="method">
2 Basic task description
</sectionHeader>
<bodyText confidence="0.977635846153846">
The Japanese translation task data was made up of
a translation memory and test set. The translation
memory was dissected into 320 disjoint segments
according to headwords, with an average of 21.6
translation records per headword (i.e. 6920 transla-
tion records overall). The purpose of the task was
to select for a given headword which (if any) of the
translation records gave a suitable translation for
that word. The task stipulated that a maximum of
one translation record could be selected for each in-
put (allowing for the possibility of an unassignable
output, indicating that no appropriate translation
could be found). Translations were selected by way
of a translation record ID, and systems were not re-
quired to actually identify what part of the L2 string
in the selected translation record was the translation
for the headword.
Translation records took the form of Japanese—
English pairings of word clusters, isolated phrases,
clauses or sentences containing the headword, at an
average of 8.0 Japanese characters&apos; and 4.0 English
words per translation record. In some instances,
multiple semantically-equivalent translations were
given for a single expression, such as &amp;quot;corporation
&apos;Ignoring punctuation but including each numeric digit as
a single character.
</bodyText>
<page confidence="0.997964">
55
</page>
<bodyText confidence="0.999935631578947">
which is in danger of bankruptcy&amp;quot; and &amp;quot;unsound cor-
poration&amp;quot; for abunai kigyd; all such occurrences were
marked by the annotator. For some other transla-
tion records, the annotator had provided a list of lex-
ical variants or a paraphrase of the L1 expression to
elucidate its meaning (not necessarily involving the
headword), or made a note as to typical arguments
taken by that expression (e.g. &amp;quot;refers to a person&amp;quot;).
In the test data, inputs took the form of para-
graphs taken from newspaper articles, within which
a single headword had been identified for transla-
tion. The average input length was 697.9 characters,
nearly 90 times the Ll component of each translation
record. In its raw form, therefore, the translation
task differs from a conventional translation retrieval
task in that translation records and inputs are not
directly comparable, in the sense that translation
records are never going to provide a full translation
approximation for the overall input.
</bodyText>
<sectionHeader confidence="0.947406" genericHeader="method">
3 Data preparation
</sectionHeader>
<bodyText confidence="0.999992844444444">
In adapting the task data to our purposes, we first
carried out limited normalisation of both the trans-
lation memory and test data by: (a) replacing all
numerical expressions with a common NUM marker,
and (b) normalising punctuation.
In order to maximise the disambiguating poten-
tial of the translation memory, we next set about
automatically deriving as many discrete translation
records as possible from the original translation
memory. Multiple lexical variants of the same basic
translation record (indexed identically) were gener-
ated in the case that: (a) a lexical alternate was
provided (in which case all variants were listed in
parallel); (b) a paraphrase was provided by the an-
notator (irrespective of whether the paraphrase in-
cluded the headword or not); (c) syntactic or seman-
tic preferences were listed for particular arguments
in the basic translation record (in which case lexical
variants took the form of strings expanded by adding
in each preference as a string). At the same time,
for each headword, any repetitions of the same L1
string were completely removed from the translation
record data. This equates to the assumption that the
translation listed first in the translation memory is
the most salient or commonplace.
This method of translation record derivation re-
sulted in a total of 152 new translation records,
whereas the removal of duplicate Ll strings for
a given headword resulted in the deletion of 670
translation records; the total number of translation
records was thus 6402, at an average of 20.0 trans-
lation records per headword.
We experimented with a number of methods for
abbreviating the inputs, so as to achieve direct com-
parability between inputs and translation records.
First, we extracted the clause containing the head-
word instance to be translated. This was achieved
through a number of ad hoc heuristics driven by the
analysis of punctuation. These clause-level instances
served as the inputs for the structural method. We
then further &amp;quot;windowed&amp;quot; the inputs for the lexical
method, by allowing a maximum of 10 characters to
either side of the headword. No attempt was made
to identify or enforce the observation of word bound-
aries in this process.
</bodyText>
<sectionHeader confidence="0.653091" genericHeader="method">
4 The lexical method
</sectionHeader>
<bodyText confidence="0.999050625">
As stated above, the lexical method is based on
character-based indexing, meaning that each string
is naively treated as a sequence of characters. Rather
than treat each individual character as a single seg-
ment, however, we chunk adjacent characters into
bigrams in order to capture local character contigu-
ity. String similarity is then determined by way of
Dice&apos;s Coefficient, calculated according to:
</bodyText>
<equation confidence="0.998818333333333">
simi =
2 x EeE/N;„TR, min (freq/N,.„ (e),.freqTR,(e))
len(IN) + len(TR)
</equation>
<bodyText confidence="0.99985875">
where IN,* is the abbreviated version of the in-
put string INn, (see above) and TR, is a transla-
tion record; each e is a character bigram occurring
in either I 1\1-7; or TR,freq I N * (e) is defined as the
</bodyText>
<page confidence="0.468214">
114
</page>
<bodyText confidence="0.999741914285714">
weighted frequency of bigram type e in IN, and
len(IN ) is the character bigram length of /N.2
Bigram frequency is weighted according to character
type: a bigram made up entirely of hiragana charac-
ters (generally used in functional words/particles) is
given a weight of 0.2 and all other bigrams a weight
of 1. Note that Dice&apos;s Coefficient ignores segment
order, and that each string is thus treated as a &amp;quot;bag
of character bigrams&amp;quot;.
Our choice of the combination of Dice&apos;s Coef-
ficient, character-based indexing and character bi-
grams (rather than any other n-gram order or mixed
n-gram model) is based on the findings of Baldwin
(2001b; 2001a), who compared character- and word-
based indexing in combination with both segment
order-sensitive and bag-of-words similarity measures
and with various n-gram models. As a result of
extensive evaluation, Baldwin found the combina-
tion of character bigram-based indexing and a bag-
of-words method (in the form of either the vector
space model or Dice&apos;s Coefficient) to be optimal.
Our choice of Dice&apos;s Coefficient over the vector space
model is due to the vector space model tending to
blithely prefer shorter strings in cases of low-level
character overlap, and the ability of Dice&apos;s Coeffi-
cient to pick up on subtle string similarities under
such high-noise conditions.
Given the limited lexical context in translation
records (8.0 Japanese characters on average), our
method is highly susceptible to the effects of data
sparseness. While we have no immediate way of rec-
onciling this shortcoming, it is possible to make use
of the rich lexical context of the full inputs (i.e. in
original paragraph form rather than clause or win-
dowed clause form). Direct comparison of the full
</bodyText>
<footnote confidence="0.538341">
2 freqTRi(e) and len(TR) are defined similarly.
</footnote>
<page confidence="0.993836">
56
</page>
<bodyText confidence="0.99996475">
inputs with translation records is undesirable as high
levels of spurious matches can be expected outside
the scope of the original translation record expres-
sion. Inter-comparison of full inputs, on the other
hand, provides a primitive model of domain similar-
ity. Assuming that high similarity correlates with a
high level of domain correspondence, we can apply
a cross-lingual corollary of the &amp;quot;one sense per dis-
course&amp;quot; observation (Gale et al., 1992) in stipulat-
ing that a given word will be translated consistently
within a given domain. By ascertaining that a given
input closely resembles a second input, we can use
the combined translation retrieval results for the two
inputs to hone in on the optimal translation for the
two. We term this procedure domain-based sim-
ilarity consolidation.
The overall retrieval process thus involves: (1)
carrying out standard translation retrieval based on
the abbreviated input, (2) using the original test set
to determine the full input string most similar to
the current input, and (3) performing translation re-
trieval independently using the abbreviated form of
the maximally similar alternate input. Numerically,
the combined similarity is calculated as:
</bodyText>
<equation confidence="0.6594125">
sim2(IN,,,TRi), 0.5 (simi(IN,TRi)
+ max simi(IN„,IN-n)
</equation>
<bodyText confidence="0.999882">
where IN, is the current input (full form), IN
is the abbreviated form of IN„, sirni is as defined
above, and IN is any input string other than the
current input. Note that the multiplication by 0.5
simply normalises the output of sim2 to the range
[0, 1]. For each input IN„, the ID for that transla-
tion record which is deemed most similar to IN, is
returned, with translation records occurring earlier
in the translation memory selected in the case of a
tie.3
</bodyText>
<sectionHeader confidence="0.851985" genericHeader="method">
5 The structural method
</sectionHeader>
<bodyText confidence="0.9505705625">
The structural method contrasts starkly with the
lexical method in that it is heavily resource-
dependent, requiring a morphological analyser,
parser and thesaurus. It operates over the same
translation memory data as the lexical method, but
uses only the abbreviated forms of the inputs (to
the clause level) and does not consider inter-input
similarity.
JUMAN (Kurohashi and Nagao, 1998b) is first
used to segment each string (translation records and
inputs), based on the output of which, the KNP
parser (Kurohashi and Nagao, 1998a) is used to de-
rive a parse tree for the string. The reason for ab-
breviating inputs only as far as the clause level for
the structural method, is to enhance parseability.
3Based on the observation that translation records are
roughly ordered according to commonality. Ties were ob-
served 7.5% of the time, with the mean number of top-scoring
translation records being 1.12.
Further pruning takes place implicitly further down-
stream as part of the parse tree matching process.
KNP returns a binary parse tree, with leaves cor-
responding to optionally case-marked phrases. Each
leaf node is simplified to the phrase head and the
(optional) case marker normalised (according to the
KNP output).
As for the lexical method, all translation records
corresponding to the current headword are matched
against the parse tree for the input, and the ID of
the closest-matching tree returned. In comparing a
given pair of parse trees T1 and T2, we proceed as
follows in direction d E {up, down}:
</bodyText>
<listItem confidence="0.987043125">
1. Set p1 to the leaf node containing the headword
in T1, and similarly initialise p2 in T2; initialise
n to 0
2. If pin, p2n„, return (n,0)
3. If pif /),,„ return (n, concept_sim(pif ,p2f))
4. Increment n by 1, set p1 and p2 to their respec-
tive adjacent leaf nodes in direction d within the
parse tree; goto step 2.
</listItem>
<bodyText confidence="0.999884621621621">
Here, pin, is the case marker associated with node pi,
pif is the filler associated with node pi, and the
operator represents lexical inequality; concept _sim
calculates the conceptual similarity of the two fillers
in question according to the Goi-Taikei thesaurus
(Ikehara et al., 1997). We do this by, for each sense
pairing of the fillers, determining the least common
hypernym and the number of edges separating each
sense node from the least common hypernym. The
conceptual distance of the given senses is then de-
termined according to the inverse of the greater of
the two edge distances to the hypernym node, and
the overall conceptual distance for the two fillers as
the minimum such sense-wise conceptual distance.
We match both up and down the tree structure
from the headword node, and evaluate the com-
bined similarity as the sum of the individual ele-
ments of the returned tuples. That is, if an up-
ward match returned (i, m) and a downward match
(j, n), the overall similarity would be (i + j,m + n).
The translation output is the ID of the translation
record producing the greatest such similarity, where
(w, x) &gt; (y, z) if w &gt; y or (w = y A x &gt; z).
As a result, conceptual similarity is essentially a tie-
breaking mechanism, and the principal determining
factor is the number of phrase levels over which the
parse trees match. In the case that there is a tie
for best translation, the translation record with the
longest Ll string is (arbitrarily) chosen, and in the
case that this doesn&apos;t resolve the stalemate, a trans-
lation record is chosen randomly. In the case that all
translation records score (0,0), we deem there to be
no suitable translation in the translation memory,
and return unassignable.
As mentioned in Section 2, crude selectional
preferences (of the form PERSON or BUILDING)
were provided on certain argument slots in trans-
</bodyText>
<page confidence="0.996997">
57
</page>
<table confidence="0.829343">
Method Accuracy
Lexical 49.1%
Structural 41.2%
Baseline 36.8%
</table>
<tableCaption confidence="0.999446">
Table 1: Results
</tableCaption>
<bodyText confidence="0.999591333333333">
lation records. These were supported by semi-
automatically mapping the preference type onto the
Goi-Taikei thesaurus structure, and modifying the
operator to non-sense subsumption of the trans-
lation record filler by the input selectional prefer-
ence, in step 3 of the parse tree match algorithm.
Selectional preferences were automatically mapped
onto nodes of the same name if they existed, and
manually linked to the thesaurus otherwise.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="evaluation">
6 Results and discussion
</sectionHeader>
<bodyText confidence="0.99999590625">
The translation retrieval accuracy for the two meth-
ods is given in Table 1, along with a baseline accu-
racy arrived at through random translation record
selection for the given headword. Note that as we
attempt to translate all inputs, the presented accu-
racy figures correspond to both recall and precision.
The most striking feature of the results is that
the lexical method has a clear advantage over the
structural method, while both methods outperform
the baseline. Obviously, it would be going too far
to discount structural methods outright based on
this limited evaluation, particularly as the lexical
method has undergone extensive testing and tuning
over other datasets, whereas the structural method
is novel to this task. It is surprising, however, that
a technique as simple as the lexical method, requir-
ing no external resources and ignoring even word
boundaries and word order, should perform so well.
The main area in which the structural method fell
short was unassignable inputs where no transla-
tion record displayed even the same case marking
on the headword. Indeed 130 or 10.8% of inputs
were tagged unassignable, despite them compris-
ing only 0.3% of the solution set. Note, however,
that even for only those inputs where the struc-
tural method was able to produce a match, the lexi-
cal method significantly outperformed the structural
method (50.2% vs. 45.4%, respectively).
Conversely for the lexical method, at present, a
translation record is selected irrespective of the mag-
nitude of the similarity value, and it would be a
trivial process to implement a similarity cutoff, be-
low which an unassignable result would be re-
turned. Preliminary analysis of the correlation be-
tween the lowest similarity values and inputs anno-
tated as unassignable indicates that this method
could be moderately successful (see Baldwin et al.
(to appear)).
The translation task was designed such that par-
ticipants didn&apos;t get access to annotated inputs until
after the submission of final results, meaning that
parameter settings and fine-tuning of techniques had
to be carried out according to intuition only. Post
hoc evaluation of methods such as domain-based
similarity consolidation suggests that it does have a
significant impact on system performance (Baldwin
et al., to appear), although even in its basic config-
uration (using clause inputs and no domain-based
similarity consolidation), the lexical method is su-
perior to the structural method as presented herein.
In conclusion, this paper has served to describe
each of a lexical and structural translation retrieval
method, as applied to the SENSEVAL-2 Japanese
translation task. The lexical method modelled
strings as a bag of character bigrams, but incor-
porated a number of novel techniques including
domain-based similarity consolidation in reaching a
final decision as to the translation record most sim-
ilar to the input. The structural method, on the
other hand, compared parse trees and had recourse
to conceptual similarity, but in a relatively rudimen-
tary form. Of the two proposed methods, the lexical
method proved to be clearly superior, although both
methods were well above the baseline performance.
</bodyText>
<sectionHeader confidence="0.998343" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99589625">
This paper was supported in part by the Research
Collaboration between the Nippon Telegraph and
Telephone Company (NTT) Communication Science
Laboratories and CSLI, Stanford University.
</bodyText>
<sectionHeader confidence="0.998629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998881806451613">
T. Baldwin, A. Okazaki, T. Tokunaga, and
II. Tanaka. to appear. The successes and failures
of lexical and structural translation retrieval. In
Transactions of the IEICE.
T. Baldwin. 2001a. Low-cost, high-performance
translation retrieval: Dumber is better. In Proc.
of the 39th Annual Meeting of the ACL and
10th Conference of the EACL (ACL-EACL 2001),
pages 18-25.
T. Baldwin. 2001b. Making Lexical Sense of
Japanese-English Machine Translation: A Dis-
ambiguation Extravaganza. Ph .D. thesis, Tokyo
Institute of Technology.
W. Gale, K. Church, and D. Yarowsky. 1992. One
sense per discourse. In Proc. of the 4th DARPA
Speech and Natural Language Workshop, pages
233-7.
S. Ikehara, M. Miyazaki, A. Yokoo, S. Shi-
rai, H. Nakaiwa, K. Ogura, Y. Ooyama, and
Y. Hayashi. 1997. Nihongo Goi Taikei - A
Japanese Lexicon. Iwanami Shoten. 5 volumes.
(In Japanese).
S. Kurohashi and M. Nagao. 1998a. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proc. of the 1st International Con-
ference on Language Resources and Evaluation
(LREC&apos;98), pages 719-24.
S. Kurohashi and M. Nagao. 1998b. Nihongo keitai-
kaiseki sisutemu JUMAN [Japanese morphologi-
cal analysis system JUMAN] version 3.5. Techni-
cal report, Kyoto University. (In Japanese).
</reference>
<page confidence="0.999257">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468084">
<title confidence="0.9987615">The Japanese Translation Task: Lexical Structural Perspectives</title>
<author confidence="0.754993">edu</author>
<affiliation confidence="0.900826">Institute of Technology , take ,tanaka}Ocl . cs . titech. ac jp&gt;</affiliation>
<abstract confidence="0.992705666666667">This paper describes two distinct attempts at the SENSEVAL-2 Japanese translation task. The first implementation is based on lexical similarity and builds on the results of Baldwin (2001b; 2001a), whereas the second is based on structural similarity via the medium of parse trees and includes a basic model of conceptual similarity. Despite its simplistic nature, the lexical method was found to perform the better of the two, at 49.1% accuracy, as compared to 41.2% for the structural method and 36.8% for the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Tanaka</author>
</authors>
<title>to appear. The successes and failures of lexical and structural translation retrieval.</title>
<booktitle>In Transactions of the IEICE.</booktitle>
<marker>Tanaka, </marker>
<rawString>T. Baldwin, A. Okazaki, T. Tokunaga, and II. Tanaka. to appear. The successes and failures of lexical and structural translation retrieval. In Transactions of the IEICE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Low-cost, high-performance translation retrieval: Dumber is better.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of the ACL and 10th Conference of the EACL (ACL-EACL</booktitle>
<pages>18--25</pages>
<contexts>
<context position="9092" citStr="Baldwin (2001" startWordPosition="1466" endWordPosition="1467">f bigram type e in IN, and len(IN ) is the character bigram length of /N.2 Bigram frequency is weighted according to character type: a bigram made up entirely of hiragana characters (generally used in functional words/particles) is given a weight of 0.2 and all other bigrams a weight of 1. Note that Dice&apos;s Coefficient ignores segment order, and that each string is thus treated as a &amp;quot;bag of character bigrams&amp;quot;. Our choice of the combination of Dice&apos;s Coefficient, character-based indexing and character bigrams (rather than any other n-gram order or mixed n-gram model) is based on the findings of Baldwin (2001b; 2001a), who compared character- and wordbased indexing in combination with both segment order-sensitive and bag-of-words similarity measures and with various n-gram models. As a result of extensive evaluation, Baldwin found the combination of character bigram-based indexing and a bagof-words method (in the form of either the vector space model or Dice&apos;s Coefficient) to be optimal. Our choice of Dice&apos;s Coefficient over the vector space model is due to the vector space model tending to blithely prefer shorter strings in cases of low-level character overlap, and the ability of Dice&apos;s Coefficie</context>
</contexts>
<marker>Baldwin, 2001</marker>
<rawString>T. Baldwin. 2001a. Low-cost, high-performance translation retrieval: Dumber is better. In Proc. of the 39th Annual Meeting of the ACL and 10th Conference of the EACL (ACL-EACL 2001), pages 18-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Making Lexical Sense of Japanese-English Machine Translation: A Disambiguation Extravaganza. Ph .D. thesis,</title>
<date>2001</date>
<institution>Tokyo Institute of Technology.</institution>
<contexts>
<context position="9092" citStr="Baldwin (2001" startWordPosition="1466" endWordPosition="1467">f bigram type e in IN, and len(IN ) is the character bigram length of /N.2 Bigram frequency is weighted according to character type: a bigram made up entirely of hiragana characters (generally used in functional words/particles) is given a weight of 0.2 and all other bigrams a weight of 1. Note that Dice&apos;s Coefficient ignores segment order, and that each string is thus treated as a &amp;quot;bag of character bigrams&amp;quot;. Our choice of the combination of Dice&apos;s Coefficient, character-based indexing and character bigrams (rather than any other n-gram order or mixed n-gram model) is based on the findings of Baldwin (2001b; 2001a), who compared character- and wordbased indexing in combination with both segment order-sensitive and bag-of-words similarity measures and with various n-gram models. As a result of extensive evaluation, Baldwin found the combination of character bigram-based indexing and a bagof-words method (in the form of either the vector space model or Dice&apos;s Coefficient) to be optimal. Our choice of Dice&apos;s Coefficient over the vector space model is due to the vector space model tending to blithely prefer shorter strings in cases of low-level character overlap, and the ability of Dice&apos;s Coefficie</context>
</contexts>
<marker>Baldwin, 2001</marker>
<rawString>T. Baldwin. 2001b. Making Lexical Sense of Japanese-English Machine Translation: A Disambiguation Extravaganza. Ph .D. thesis, Tokyo Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proc. of the 4th DARPA Speech and Natural Language Workshop,</booktitle>
<pages>233--7</pages>
<contexts>
<context position="10682" citStr="Gale et al., 1992" startWordPosition="1716" endWordPosition="1719">inputs (i.e. in original paragraph form rather than clause or windowed clause form). Direct comparison of the full 2 freqTRi(e) and len(TR) are defined similarly. 56 inputs with translation records is undesirable as high levels of spurious matches can be expected outside the scope of the original translation record expression. Inter-comparison of full inputs, on the other hand, provides a primitive model of domain similarity. Assuming that high similarity correlates with a high level of domain correspondence, we can apply a cross-lingual corollary of the &amp;quot;one sense per discourse&amp;quot; observation (Gale et al., 1992) in stipulating that a given word will be translated consistently within a given domain. By ascertaining that a given input closely resembles a second input, we can use the combined translation retrieval results for the two inputs to hone in on the optimal translation for the two. We term this procedure domain-based similarity consolidation. The overall retrieval process thus involves: (1) carrying out standard translation retrieval based on the abbreviated input, (2) using the original test set to determine the full input string most similar to the current input, and (3) performing translatio</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. One sense per discourse. In Proc. of the 4th DARPA Speech and Natural Language Workshop, pages 233-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ikehara</author>
<author>M Miyazaki</author>
<author>A Yokoo</author>
<author>S Shirai</author>
<author>H Nakaiwa</author>
<author>K Ogura</author>
<author>Y Ooyama</author>
<author>Y Hayashi</author>
</authors>
<title>Nihongo Goi Taikei - A Japanese Lexicon. Iwanami Shoten. 5 volumes.</title>
<date>1997</date>
<note>(In Japanese).</note>
<contexts>
<context position="2597" citStr="Ikehara et al., 1997" startWordPosition="407" endWordPosition="410">thod and more sophisticated parse tree comparison (structural) method. Both methods discussed herein are fully unsupervised. The lexical method makes use of no external resources or linguistic knowledge whatsoever. It treats each string as a &amp;quot;bag of character bigrams&amp;quot; and calculates similarity according to Dice&apos;s Coefficient. The structural method, on the other hand, relies on both morphological and syntactic analysis, in the form of the publicly-available JUMAN (Kurohashi and Nagao, 1998b) and KNP (Kurohashi and Nagao, 1998a) systems, respectively, and also the Japanese Goi-Taikei thesaurus (Ikehara et al., 1997) to measure conceptual distance. A parse tree is generated for the L1 component of each translation record, and also each input, and similarity gauged by both topological resemblance between parse trees and conceptual similarity between nodes of the parse tree. Translation records used by the two systems were taken exclusively from the translation memory provided for the task. In the proceeding sections, we briefly review the Japanese translation task (§ 2) and detail our particular use of the data provided for the task (§ 3). Next, we outline the lexical method (§ 4) and structural method (§ </context>
<context position="14091" citStr="Ikehara et al., 1997" startWordPosition="2279" endWordPosition="2282"> d E {up, down}: 1. Set p1 to the leaf node containing the headword in T1, and similarly initialise p2 in T2; initialise n to 0 2. If pin, p2n„, return (n,0) 3. If pif /),,„ return (n, concept_sim(pif ,p2f)) 4. Increment n by 1, set p1 and p2 to their respective adjacent leaf nodes in direction d within the parse tree; goto step 2. Here, pin, is the case marker associated with node pi, pif is the filler associated with node pi, and the operator represents lexical inequality; concept _sim calculates the conceptual similarity of the two fillers in question according to the Goi-Taikei thesaurus (Ikehara et al., 1997). We do this by, for each sense pairing of the fillers, determining the least common hypernym and the number of edges separating each sense node from the least common hypernym. The conceptual distance of the given senses is then determined according to the inverse of the greater of the two edge distances to the hypernym node, and the overall conceptual distance for the two fillers as the minimum such sense-wise conceptual distance. We match both up and down the tree structure from the headword node, and evaluate the combined similarity as the sum of the individual elements of the returned tupl</context>
</contexts>
<marker>Ikehara, Miyazaki, Yokoo, Shirai, Nakaiwa, Ogura, Ooyama, Hayashi, 1997</marker>
<rawString>S. Ikehara, M. Miyazaki, A. Yokoo, S. Shirai, H. Nakaiwa, K. Ogura, Y. Ooyama, and Y. Hayashi. 1997. Nihongo Goi Taikei - A Japanese Lexicon. Iwanami Shoten. 5 volumes. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Building a Japanese parsed corpus while improving the parsing system.</title>
<date>1998</date>
<booktitle>In Proc. of the 1st International Conference on Language Resources and Evaluation (LREC&apos;98),</booktitle>
<pages>719--24</pages>
<contexts>
<context position="2469" citStr="Kurohashi and Nagao, 1998" startWordPosition="388" endWordPosition="392">content, word order, parse tree topology and conceptual similarity. In this paper, we focus on a simple character-based (lexical) method and more sophisticated parse tree comparison (structural) method. Both methods discussed herein are fully unsupervised. The lexical method makes use of no external resources or linguistic knowledge whatsoever. It treats each string as a &amp;quot;bag of character bigrams&amp;quot; and calculates similarity according to Dice&apos;s Coefficient. The structural method, on the other hand, relies on both morphological and syntactic analysis, in the form of the publicly-available JUMAN (Kurohashi and Nagao, 1998b) and KNP (Kurohashi and Nagao, 1998a) systems, respectively, and also the Japanese Goi-Taikei thesaurus (Ikehara et al., 1997) to measure conceptual distance. A parse tree is generated for the L1 component of each translation record, and also each input, and similarity gauged by both topological resemblance between parse trees and conceptual similarity between nodes of the parse tree. Translation records used by the two systems were taken exclusively from the translation memory provided for the task. In the proceeding sections, we briefly review the Japanese translation task (§ 2) and detail</context>
<context position="12359" citStr="Kurohashi and Nagao, 1998" startWordPosition="1984" endWordPosition="1987">e [0, 1]. For each input IN„, the ID for that translation record which is deemed most similar to IN, is returned, with translation records occurring earlier in the translation memory selected in the case of a tie.3 5 The structural method The structural method contrasts starkly with the lexical method in that it is heavily resourcedependent, requiring a morphological analyser, parser and thesaurus. It operates over the same translation memory data as the lexical method, but uses only the abbreviated forms of the inputs (to the clause level) and does not consider inter-input similarity. JUMAN (Kurohashi and Nagao, 1998b) is first used to segment each string (translation records and inputs), based on the output of which, the KNP parser (Kurohashi and Nagao, 1998a) is used to derive a parse tree for the string. The reason for abbreviating inputs only as far as the clause level for the structural method, is to enhance parseability. 3Based on the observation that translation records are roughly ordered according to commonality. Ties were observed 7.5% of the time, with the mean number of top-scoring translation records being 1.12. Further pruning takes place implicitly further downstream as part of the parse tr</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>S. Kurohashi and M. Nagao. 1998a. Building a Japanese parsed corpus while improving the parsing system. In Proc. of the 1st International Conference on Language Resources and Evaluation (LREC&apos;98), pages 719-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kurohashi</author>
<author>M Nagao</author>
</authors>
<title>Nihongo keitaikaiseki sisutemu JUMAN [Japanese morphological analysis system JUMAN] version 3.5.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Kyoto University. (In Japanese).</institution>
<contexts>
<context position="2469" citStr="Kurohashi and Nagao, 1998" startWordPosition="388" endWordPosition="392">content, word order, parse tree topology and conceptual similarity. In this paper, we focus on a simple character-based (lexical) method and more sophisticated parse tree comparison (structural) method. Both methods discussed herein are fully unsupervised. The lexical method makes use of no external resources or linguistic knowledge whatsoever. It treats each string as a &amp;quot;bag of character bigrams&amp;quot; and calculates similarity according to Dice&apos;s Coefficient. The structural method, on the other hand, relies on both morphological and syntactic analysis, in the form of the publicly-available JUMAN (Kurohashi and Nagao, 1998b) and KNP (Kurohashi and Nagao, 1998a) systems, respectively, and also the Japanese Goi-Taikei thesaurus (Ikehara et al., 1997) to measure conceptual distance. A parse tree is generated for the L1 component of each translation record, and also each input, and similarity gauged by both topological resemblance between parse trees and conceptual similarity between nodes of the parse tree. Translation records used by the two systems were taken exclusively from the translation memory provided for the task. In the proceeding sections, we briefly review the Japanese translation task (§ 2) and detail</context>
<context position="12359" citStr="Kurohashi and Nagao, 1998" startWordPosition="1984" endWordPosition="1987">e [0, 1]. For each input IN„, the ID for that translation record which is deemed most similar to IN, is returned, with translation records occurring earlier in the translation memory selected in the case of a tie.3 5 The structural method The structural method contrasts starkly with the lexical method in that it is heavily resourcedependent, requiring a morphological analyser, parser and thesaurus. It operates over the same translation memory data as the lexical method, but uses only the abbreviated forms of the inputs (to the clause level) and does not consider inter-input similarity. JUMAN (Kurohashi and Nagao, 1998b) is first used to segment each string (translation records and inputs), based on the output of which, the KNP parser (Kurohashi and Nagao, 1998a) is used to derive a parse tree for the string. The reason for abbreviating inputs only as far as the clause level for the structural method, is to enhance parseability. 3Based on the observation that translation records are roughly ordered according to commonality. Ties were observed 7.5% of the time, with the mean number of top-scoring translation records being 1.12. Further pruning takes place implicitly further downstream as part of the parse tr</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>S. Kurohashi and M. Nagao. 1998b. Nihongo keitaikaiseki sisutemu JUMAN [Japanese morphological analysis system JUMAN] version 3.5. Technical report, Kyoto University. (In Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>