<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001911">
<title confidence="0.9992405">
A Framework for the Construction of
Monolingual and Cross-lingual Word Similarity Datasets
</title>
<author confidence="0.99849">
Jos´e Camacho-Collados, Mohammad Taher Pilehvar and Roberto Navigli
</author>
<affiliation confidence="0.9988915">
Department of Computer Science
Sapienza University of Rome
</affiliation>
<email confidence="0.997642">
{collados,pilehvar,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998575090909091">
Despite being one of the most popular
tasks in lexical semantics, word similar-
ity has often been limited to the English
language. Other languages, even those
that are widely spoken such as Span-
ish, do not have a reliable word similar-
ity evaluation framework. We put for-
ward robust methodologies for the ex-
tension of existing English datasets to
other languages, both at monolingual and
cross-lingual levels. We propose an au-
tomatic standardization for the construc-
tion of cross-lingual similarity datasets,
and provide an evaluation, demonstrating
its reliability and robustness. Based on
our procedure and taking the RG-65 word
similarity dataset as a reference, we re-
lease two high-quality Spanish and Farsi
(Persian) monolingual datasets, and fifteen
cross-lingual datasets for six languages:
English, Spanish, French, German, Por-
tuguese, and Farsi.
</bodyText>
<sectionHeader confidence="0.999521" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998623232142857">
Semantic similarity is a field of Natural Lan-
guage Processing which measures the extent to
which two linguistic items are similar. In par-
ticular, word similarity is one of the most pop-
ular benchmarks for the evaluation of word or
sense representations. Applications of word sim-
ilarity range from Word Sense Disambiguation
(Patwardhan et al., 2003) to Machine Translation
(Lavie and Denkowski, 2009), Information Re-
trieval (Hliaoutakis et al., 2006), Question An-
swering (Mohler et al., 2011), Text Summarization
(Mohammad and Hirst, 2012), Ontology Align-
ment (Pilehvar and Navigli, 2014), and Lexical
Substitution (McCarthy and Navigli, 2009).
However, due to the lack of standard multi-
lingual benchmarks, word similarity systems had
in the main been limited to the English lan-
guage (Mihalcea and Moldovan, 1999; Agirre and
Lopez, 2003; Agirre and de Lacalle, 2004; Strube
and Ponzetto, 2006; Gabrilovich and Markovitch,
2007; Mihalcea, 2007; Pilehvar et al., 2013; Ba-
roni et al., 2014), up until the recent creation
of datasets built by translating the English RG-
65 dataset (Rubenstein and Goodenough, 1965)
into French (Joubarne and Inkpen, 2011), Ger-
man (Gurevych, 2005), and Portuguese (Granada
et al., 2014). And what is more, cross-lingual
applications have grown in importance over the
last few years (Hassan and Mihalcea, 2009; Nav-
igli and Ponzetto, 2012; Franco-Salvador et al.,
2014; Camacho-Collados et al., 2015b). Unfor-
tunately, very few reliable datasets exist for evalu-
ating cross-lingual systems.
This paper provides two contributions: Firstly,
we construct Spanish and Farsi versions of the
standard RG-65 dataset scored by twelve annota-
tors with high inter-annotator agreements of 0.83
and 0.88, respectively, in terms of Pearson correla-
tion, and secondly, we create fifteen cross-lingual
word similarity datasets based on RG-65, cover-
ing six languages, by proposing an improved ver-
sion of the approach of Kennedy and Hirst (2012)
for the automatic construction of cross-lingual
datasets from aligned monolingual datasets.
The paper is structured as follows. We first
briefly review some of the major monolingual and
cross-lingual word similarity datasets in Section
2. We then discuss the details of our procedure
for the construction of the Spanish and Farsi word
similarity datasets in Section 3. Section 4 provides
the details of our algorithm for the automatic con-
struction of the cross-lingual datasets. We report
the results of the evaluation performed on the gen-
erated datasets in Section 5. Finally, we specify
the released resources in Section 6, followed by
concluding remarks in Section 7.
</bodyText>
<footnote confidence="0.633256">
1
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7,
</footnote>
<note confidence="0.402557">
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999512" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999952444444445">
Multiple word similarity datasets have been con-
structed for the English language: MC-30 (Miller
and Charles, 1991), WordSim-353 (Finkelstein et
al., 2002), MEN (Bruni et al., 2014), and Simlex-
999 (Hill et al., 2014). The RG-65 dataset (Ruben-
stein and Goodenough, 1965) is one of the old-
est and most popular word similarity datasets, and
has been used as a standard benchmark for mea-
suring the reliability of word and sense represen-
tations (Agirre and de Lacalle, 2004; Gabrilovich
and Markovitch, 2007; Hassan and Mihalcea,
2011; Pilehvar et al., 2013; Baroni et al., 2014;
Camacho-Collados et al., 2015a). The original
RG-65 dataset was constructed with the aim of
evaluating the degree to which contextual infor-
mation is correlated with semantic similarity for
the English language. Rubenstein and Goode-
nough (1965) reported an inter-annotator agree-
ment of 0.85 for a subset of fifteen judges (no final
inter-annotator agreement for the total fifty-one
judges was calculated). The original English RG-
65 has also been used as a base for different lan-
guages: French (Joubarne and Inkpen, 2011), Ger-
man (Gurevych, 2005), and Portuguese (Granada
et al., 2014). No inter-annotator agreement was
calculated for the French version, while the Ger-
man and Portuguese were reported to have the re-
spective inter-annotator agreements of 0.81 and
0.71 in terms of average pairwise Pearson corre-
lation. Our Spanish version of the RG-65 dataset
reports a high inter-annotator agreement of 0.83,
while the Farsi version achieves 0.88.
A few works have also focused on the con-
struction of cross-lingual resources. Hassan and
Mihalcea (2009) built two sets of cross-lingual
datasets by translating the English MC-30 (Miller
and Charles, 1991) and the WordSim-353 (Finkel-
stein et al., 2002) datasets into three languages.
However, these datasets have several issues due to
their construction procedure. The main problem
arises from keeping the original scores from the
English dataset in the translated datasets. For in-
stance, the Spanish dataset contains the identical
pair mediodia-mediodia with a similarity score of
3.42 (in the 0-4 scale). Furthermore, the datasets
contain orthographic errors such as despliege and
the previously mentioned mediodia (instead of de-
spliegue and mediodia), and nouns translated into
words with a different part of speech (e.g., imple-
ment from the English noun dataset MC-30 trans-
lated to the Spanish verb implementar). Addition-
ally, the selection of the datasets was not ideal:
MC-30 is a small subset of RG-65 and WordSim-
353 has been criticized for its annotation scheme,
which conflates similarity and relatedness (Hill et
al., 2014).
Kennedy and Hirst (2012) proposed an auto-
matic procedure for the construction of a French-
English version of RG-65. We refine their ap-
proach by also dealing with some issues that may
arise in the automatic process. Additionally, we
provide an evaluation of the automatic procedure
on different languages.
</bodyText>
<sectionHeader confidence="0.9827975" genericHeader="method">
3 Building Monolingual Word Similarity
Datasets
</sectionHeader>
<bodyText confidence="0.99997525">
In this section we explain our methodology for the
construction of the Spanish and Farsi versions of
the English RG-65 dataset (Rubenstein and Good-
enough, 1965). The methodology is divided into
two main steps: First, the original English dataset
is translated into the target language (Section 3.1)
and then, the newly translated pairs are scored by
human annotators (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.993035">
3.1 Translating from English to
Spanish/Farsi
</subsectionHeader>
<bodyText confidence="0.999915782608696">
The translation of RG-65 from English to Span-
ish and Farsi was performed by, respectively, three
English-Spanish and three English-Farsi annota-
tors who were fluent English speakers and native
speakers of the target language. The translation
procedure was as follows. First, two annotators
translated each English pair in the dataset into the
target language. Then a third annotator checked
for disagreements between the first two transla-
tors and picked the more appropriate translation
among the two options.
Finally, all three translators met and performed
a final check, with specific focus on the following
two cases: (1) duplicate pairs in the dataset, and
(2) pairs with repeated words. Our goal was to re-
duce these two cases as much as possible. A final
adjudication was performed accordingly. We note
that there remain three pairs with identical words
in both Spanish and Farsi datasets, as no suitable
translation could be found to distinguish the words
in the English pair. For instance, the two words in
the pair midday-noon translate to the same Span-
ish word mediodia.
</bodyText>
<page confidence="0.992232">
2
</page>
<figure confidence="0.8503207">
English Spanish
noon string 0.04 mediodía cuerda 0.00
cemetery woodland 0.79 cementerio bosque 1.18
mound shore 0.97 loma orilla 1.21
food rooster 1.09 comida gallo 1.54
bird woodland 1.24 pájaro bosque 1.67
glass jewel 1.78 cristal joya 1.96
bird crane 2.63 pájaro grulla 2.92
autograph signature 3.59 autógrafo firma 3.46
automobile car 3.92 automóvil coche 3.92
Farsi
0.00
0.50
1.17
1.00
1.79
1.29
2.83
4.00
3.88
</figure>
<tableCaption confidence="0.998895">
Table 1: Sample word pairs from the English and the newly created Spanish and Farsi RG-65 datasets.
</tableCaption>
<subsectionHeader confidence="0.999902">
3.2 Scoring the dataset
</subsectionHeader>
<bodyText confidence="0.99998296">
Twelve native Spanish speakers were asked to
evaluate the similarity for the Spanish translations.
In order to obtain a more global distribution of
judges, we included judges both both Spain and
Latin America. As far as the Farsi dataset was
concerned, twelve Farsi native speakers scored the
newly translated pairs. The guidelines provided
to the annotators were based on the recent Se-
mEval task on Cross-Level Semantic Similarity
(Jurgens et al., 2014), which provides clear indica-
tions in order to distinguish similarity and related-
ness. The annotators were allowed to give scores
from 0 to 4, with a step size of 0.5.
Table 1 shows example pairs with their corre-
sponding scores from the English and the newly
created Spanish and Farsi versions of the RG-
65 dataset. As we can see from the table, the
scores across languages are not necessarily iden-
tical, with small, in a few cases significant, differ-
ences between the corresponding scores. This is
due to the fact that associated senses with words
do not hold one-to-one correspondence across dif-
ferent languages. This renders the approach of
Hassan and Mihalcea (2009) insufficiently accu-
rate for handling these differences.
</bodyText>
<sectionHeader confidence="0.956209" genericHeader="method">
4 Automatic Creation of Cross-lingual
Similarity Datasets
</sectionHeader>
<bodyText confidence="0.999796">
In this section we present our automatic method
for building cross-lingual datasets. Although
being targeted at building semantic similarity
datasets, the algorithm is task-independent, so it
may also be used for any task which measures any
kind of relation between two linguistic items in a
numerical way.
Kennedy and Hirst (2012) proposed a method
which exploits two aligned monolingual word
similarity datasets for the construction of a
French-English cross-lingual dataset. We fol-
lowed their initial idea and proposed a generaliza-
tion of the approach which would be capable of
automatically constructing reliable cross-lingual
similarity datasets for any pair of languages.
Algorithm. Algorithm 1 shows our procedure
for constructing a cross-lingual dataset starting
from two monolingual datasets. Note that the
pairs in the two monolingual datasets should be
previously aligned. Specifically, we refer to each
dataset D as {PD, 5D}, where PD is the set of
pairs and 5D is a function mapping each pair
in PD to a value on a similarity scale (0-4 for
RG-65). For each two aligned pairs a-b and a’-b’
across the two datasets, if the difference in the
corresponding scores is greater than a quarter
of the similarity scale size (1.0 in RG-65), the
pairs are not considered (line 7) and therefore
discarded. Otherwise, two new pairs a-b’ and
a’-b are created with a score equal to the average
of the two original pairs’ scores (lines 8-11 and
15-18). In the case of repeated pairs, we merge
them into a single pair with a similarity equal to
their average score (lines 12-14 and lines 19-21).
By following this procedure we created fifteen
cross-lingual datasets based on the RG-65 word
similarity datasets for English, French, German,
Spanish, Portuguese, and Farsi. Table 2 shows
</bodyText>
<page confidence="0.993505">
3
</page>
<table confidence="0.787866454545455">
Algorithm 1 Automatic construction of cross-
lingual similarity datasets
Input: two aligned datasets D = {PD, SD1 and D0 =
{PD0, SD01, where PX is the set of pairs in dataset X
and SX is the mapping of these pairs to their correspond-
ing scores.
Output: a cross-lingual semantic similarity dataset C =
{PC,SC1
1: PC +— 0
2: Define Cnt, which counts how many times an output
cross-lingual pair is repeated
</table>
<tableCaption confidence="0.273411">
3: for each aligned pairs (a, b) E PD, (a0, b0) E PD0
</tableCaption>
<figure confidence="0.963217952380952">
4: score = SD(a, b)
5: score0 = SD0(a0, b0)
6: avg score = (score + score0)/2
7: if |score — score0 |&lt; size(sim scale)/4 then
8: if (a, b0) E� PC then
9: PC — PC U {(a, b0)1
10: SC(a, b0) = avg score
11: Cnt(a, b0) = 1
12: else
SC(a, b0) = (SC(a,b0)×Cnt(a,b0))+avg score
13: Cnt(a,b0)+1
14: Cnt(a, b0) + +
15: if (a0, b) E� PC then
16: PC — PC U {(a0, b)1
17: SC(a0, b) = avg score
18: Cnt(a0, b) = 1
19: else
SC(a0, b) = (SC(a0,b)×Cnt(a0,b))+avg score
Cnt(a0,b)+1
21: Cnt(a0, b) + +
22: return {PC, SC1
</figure>
<bodyText confidence="0.999415">
the number of word pairs for each cross-lingual
dataset. Note that there is not a single pair of lan-
guages whose total count reaches the maximum
number of possible word pairs, i.e., 130. This is
due, on the one hand, to language peculiarities re-
sulting in some pairs having significant score dif-
ference across languages (higher than 1 on the 0-4
scale), and, on the other hand, to the repetition of
some pairs occurring as a result of the automatic
creation process, a problem which is handled by
our algorithm.
Table 3 shows sample pairs with their cor-
responding similarity scores from four of the
cross-lingual datasets: Spanish-English, Spanish-
French, Spanish-German, and English-Farsi.
These cross-lingual datasets are constructed on the
basis of our newly-generated Spanish and Farsi
monolingual datasets (see Section 3). The quality
of these four datasets is evaluated in Section 5.2.
</bodyText>
<table confidence="0.996319833333333">
FR DE ES PT FA
EN 100 125 126 120 120
FR - 96 103 92 100
DE - - 125 118 122
ES - - - 113 122
PT - - - - 122
</table>
<tableCaption confidence="0.982034666666667">
Table 2: Number of word pairs for each cross-
lingual dataset (EN: English, FR: French, DE:
German, ES: Spanish, PT: Portuguese, FA: Farsi).
</tableCaption>
<sectionHeader confidence="0.999141" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995989">
5.1 Spanish and Farsi Monolingual Datasets
</subsectionHeader>
<bodyText confidence="0.999990210526316">
The inter-annotator agreements according to the
average pairwise Pearson correlation among the
judges for the newly created Spanish and Farsi
datasets are, respectively, 0.83 and 0.88, which
may be used as upper bounds for evaluating auto-
matic systems. Our further analysis revealed that
for both datasets no annotator obtained an aver-
age Pearson correlation with the rest of the an-
notators lower than 0.80, which attests to the re-
liability of our judges and guidelines. The Ger-
man (Gurevych, 2005) and Portuguese (Granada
et al., 2014) versions of the RG-65 dataset re-
ported a lower inter-annotator agreement of 0.81
and 0.71, respectively, whereas the original En-
glish RG-65 (Rubenstein and Goodenough, 1965)
reported an inter-annotator agreement of 0.85 for a
subset of fifteen judges. As also mentioned earlier,
the French version (Joubarne and Inkpen, 2011)
did not report any inter-annotator agreement.
</bodyText>
<subsectionHeader confidence="0.989799">
5.2 Cross-lingual Datasets
</subsectionHeader>
<bodyText confidence="0.9999285">
Along with the monolingual evaluation, we also
performed an evaluation on four of the automati-
cally created cross-lingual datasets. The evaluated
language pairs were Spanish-English, Spanish-
French, Spanish-German, and English-Farsi. In
each case a proficient speaker of both languages
was selected to carry out the evaluation. The
Pearson correlations of the human judges with
the automatically generated scores were 0.89 for
Spanish-English, 0.94 for Spanish-French, 0.91
for Spanish-German, and 0.92 for English-Farsi,
showing the reliability of our cross-lingual dataset
creation process and reinforcing the quality of the
newly created monolingual datasets.
</bodyText>
<page confidence="0.995506">
4
</page>
<table confidence="0.977258733333333">
ES EN ES
monje assylum 0.41 cuerda
bosque bird 1.46 chico
viaje car 1.74 comida
hermano monk 2.25 hermano
pollo rooster 3.36 grulla
cementerio graveyard 3.94 chaval
ES DE EN
orilla autogramm 0.02 mound
caldera werkzeug 1.04 coast
pájaro wald 1.65 journey
coche fahrt 2.34 food
cojín kissen 3.21 stove
colina berg 3.61 car
FR
</table>
<figure confidence="0.925272846153846">
midi 0.00
sage 0.54
coq 1.08
gars 1.71
oiseau 2.67
garḉon 3.88
FA
0.07
1.03
1.53
2.56
3.10
3.90
</figure>
<tableCaption confidence="0.9702975">
Table 3: Example pairs from the Spanish-English, Spanish-French, Spanish-German, and English-Farsi
cross-lingual word similarity datasets (EN: English, FR: French, DE: German, ES: Spanish, FA: Farsi).
</tableCaption>
<sectionHeader confidence="0.858672" genericHeader="method">
6 Release of the Resources
</sectionHeader>
<bodyText confidence="0.999927857142857">
All the resources obtained as a result of this
work are freely downloadable and available
to the research community at http://lcl.
uniroma1.it/similarity-datasets/.
Among these resources we include the newly
created Spanish and Farsi word similarity datasets,
together with the annotation guidelines used dur-
ing the creation of the datasets. Our algo-
rithm for the automatic creation of cross-lingual
datasets (Algorithm 1) is provided as an easy-to-
use Python script. Finally, we also release the fif-
teen cross-lingual datasets built by using this al-
gorithm, including Spanish, English, French, Ger-
man, Portuguese, and Farsi languages.
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999916923076923">
We developed two versions of the standard RG-65
dataset in Spanish and Farsi. We also proposed
and evaluated an automatic method for creating
cross-lingual semantic similarity datasets. Thanks
to this method, we release fifteen cross-lingual
datasets for pairs of languages including English,
Spanish, French, German, Portuguese, and Farsi.
All these datasets are intended for use as a stan-
dard benchmark (as RG-65 already is for the En-
glish language) for evaluating word or sense rep-
resentations and, more specifically, word similar-
ity systems, not only for languages other than En-
glish, but also across different languages.
</bodyText>
<sectionHeader confidence="0.990771" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.63487425">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We would like to thank Leyli Badiei, N´uria Bel,
Alicia Burga, Marcos Camacho, ´Angela Collados,
Jos´e Cuenca, Luis Espinosa, Dario Garigliotti,
Afsaneh Hojjat, Ignacio Iacobacci, Amin Lak,
Montserrat Marimon, Javier Mart´ınez, Ali Orang,
Ana Osorio, Lluis Padr´o, Abdolhamid, Razieh,
and Zahra Pilehvar, Mohammad Sadegh Rasooli,
Molood Sadat Safavi, Mohammad Shojafar, Hos-
sein Soleimani, and Fatemeh Torabi Asr for their
help in the construction and evaluation of the
word similarity datasets. We would also like to
thank Jim McManus for his comments on the
manuscript.
</bodyText>
<page confidence="0.994569">
5
</page>
<sectionHeader confidence="0.996156" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999602205607477">
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of LREC, pages 1123–
1126, Lisbon, Portugal.
Eneko Agirre and Oier Lopez. 2003. Clustering Word-
Net word senses. In Proceedings of Recent Ad-
vances in Natural Language Processing, pages 121–
130, Borovets, Bulgaria.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, pages 238–247, Baltimore, Maryland.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artificial Intelligence Research, 49:1–47.
Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015a. NASARI: a Novel Ap-
proach to a Semantically-Aware Representation of
Items. In Proceedings of NAACL, pages 567–577,
Denver, USA.
Jos´e Camacho-Collados, Mohammad Taher Pilehvar,
and Roberto Navigli. 2015b. A unified multilingual
semantic representation of concepts. In Proceedings
of ACL, Beijing, China.
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Rup-
pin Eytan. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Marc Franco-Salvador, Paolo Rosso, and Roberto Nav-
igli. 2014. A knowledge-based representation for
cross-language document retrieval and categoriza-
tion. In Proceedings of the 141h Conference on
European chapter of the Association for Computa-
tional Linguistics (EACL), pages 414–423, Gothen-
burg, Sweden.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
IJCAI, pages 1606–1611, Hyderabad, India.
Roger Granada, Cassia Trojahn, and Renata Vieira.
2014. Comparing semantic relatedness between
word pairs in Portuguese using Wikipedia. In Com-
putational Processing of the Portuguese Language,
pages 170–175. S˜ao Carlos/SP, Brazil.
Iryna Gurevych. 2005. Using the structure of a con-
ceptual network in computing semantic relatedness.
In Proceedings of IJCNLP, pages 767–778. Jeju Is-
land, Korea.
Samer Hassan and Rada Mihalcea. 2009. Cross-
lingual semantic relatedness using encyclopedic
knowledge. In Proceedings of EMNLP, pages 1192–
1201, Singapore.
Samer Hassan and Rada Mihalcea. 2011. Semantic
Relatedness Using Salient Semantic Analysis. In
Proceedings of AAAI, pages 884–889, San Fran-
cisco, USA.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. arXiv:1408.3456.
Angelos Hliaoutakis, Giannis Varelas, Epimenidis
Voutsakis, Euripides GM Petrakis, and Evangelos
Milios. 2006. Information retrieval by semantic
similarity. International Journal on Semantic Web
and Information Systems, 2(3):55–73.
Colette Joubarne and Diana Inkpen. 2011. Compar-
ison of semantic similarity for different languages
using the Google n-gram corpus and second-order
co-occurrence measures. In Advances in Artificial
Intelligence, pages 216–221. Perth, Australia.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 81h International Workshop on Semantic Evalu-
ation (SemEval 2014), in conjunction with COLING
2014, pages 17–26, Dublin, Ireland.
Alistair Kennedy and Graeme Hirst. 2012. Measuring
semantic relatedness across languages. In Proceed-
ings of xLiTe: Cross-Lingual Technologies Work-
shop at the Neural Information Processing Systems
Conference, pages 1–6, Lake Tahoe, USA.
Alon Lavie and Michael J. Denkowski. 2009. The
Meteor metric for automatic evaluation of Machine
Translation. Machine Translation, 23(2-3):105–
115.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Rada Mihalcea and Dan Moldovan. 1999. An auto-
matic method for generating sense tagged corpora.
In Proceedings of AAAI, pages 461–466, Orlando,
Florida, USA.
Rada Mihalcea. 2007. Using Wikipedia for automatic
Word Sense Disambiguation. In Proc. of NAACL-
HLT-07, pages 196–203, Rochester, NY.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28.
Saif Mohammad and Graeme Hirst. 2012. Distri-
butional measures of semantic distance: A survey.
CoRR, abs/1203.1858.
Michael Mohler, Razvan Bunescu, and Rada Mihal-
cea. 2011. Learning to grade short answer questions
using semantic similarity measures and dependency
graph alignments. In Proceedings of ACL, pages
752–762, Portland, Oregon.
</reference>
<page confidence="0.984226">
6
</page>
<reference confidence="0.99974364">
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of
AAAI, pages 108–114, Toronto, Canada.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for Word Sense Disambiguation. In Pro-
ceedings of CICLing, pages 241–257, Mexico City,
Mexico.
Mohammad Taher Pilehvar and Roberto Navigli. 2014.
A robust approach to aligning heterogeneous lexical
resources. In Proceedings of ACL, pages 468–478,
Baltimore, USA.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Seman-
tic Similarity. In Proceedings of ACL, pages 1341–
1351, Sofia, Bulgaria.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing semantic relatedness using
Wikipedia. In Proceedings of AAAI, pages 1419–
1424, Boston, USA.
</reference>
<page confidence="0.99952">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.176781">
<title confidence="0.997476">A Framework for the Construction Monolingual and Cross-lingual Word Similarity Datasets</title>
<author confidence="0.942437">Mohammad Taher Pilehvar Camacho-Collados</author>
<affiliation confidence="0.987377">Department of Computer Sapienza University of</affiliation>
<abstract confidence="0.992579142857143">Despite being one of the most popular tasks in lexical semantics, word similarity has often been limited to the English language. Other languages, even those that are widely spoken such as Spanish, do not have a reliable word similarity evaluation framework. We put forward robust methodologies for the extension of existing English datasets to other languages, both at monolingual and cross-lingual levels. We propose an automatic standardization for the construction of cross-lingual similarity datasets, and provide an evaluation, demonstrating its reliability and robustness. Based on our procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages:</abstract>
<keyword confidence="0.2834375">English, Spanish, French, German, Portuguese, and Farsi.</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Publicly available topic signatures for all WordNet nominal senses.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1123--1126</pages>
<location>Lisbon, Portugal.</location>
<marker>Agirre, de Lacalle, 2004</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly available topic signatures for all WordNet nominal senses. In Proceedings of LREC, pages 1123– 1126, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez</author>
</authors>
<title>Clustering WordNet word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>121--130</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="1972" citStr="Agirre and Lopez, 2003" startWordPosition="287" endWordPosition="290">uation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortun</context>
</contexts>
<marker>Agirre, Lopez, 2003</marker>
<rawString>Eneko Agirre and Oier Lopez. 2003. Clustering WordNet word senses. In Proceedings of Recent Advances in Natural Language Processing, pages 121– 130, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>238--247</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="2123" citStr="Baroni et al., 2014" startWordPosition="310" endWordPosition="314">ation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and </context>
<context position="4618" citStr="Baroni et al., 2014" startWordPosition="696" endWordPosition="699">n for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-anno</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, pages 238–247, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="4226" citStr="Bruni et al., 2014" startWordPosition="629" endWordPosition="632">rformed on the generated datasets in Section 5. Finally, we specify the released resources in Section 6, followed by concluding remarks in Section 7. 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English langu</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Camacho-Collados</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>NASARI: a Novel Approach to a Semantically-Aware Representation of Items.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>567--577</pages>
<location>Denver, USA.</location>
<contexts>
<context position="2560" citStr="Camacho-Collados et al., 2015" startWordPosition="378" endWordPosition="381">nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned mono</context>
<context position="4649" citStr="Camacho-Collados et al., 2015" startWordPosition="700" endWordPosition="703">inguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated </context>
</contexts>
<marker>Camacho-Collados, Pilehvar, Navigli, 2015</marker>
<rawString>Jos´e Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli. 2015a. NASARI: a Novel Approach to a Semantically-Aware Representation of Items. In Proceedings of NAACL, pages 567–577, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Camacho-Collados</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>A unified multilingual semantic representation of concepts.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2560" citStr="Camacho-Collados et al., 2015" startWordPosition="378" endWordPosition="381">nd Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned mono</context>
<context position="4649" citStr="Camacho-Collados et al., 2015" startWordPosition="700" endWordPosition="703">inguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated </context>
</contexts>
<marker>Camacho-Collados, Pilehvar, Navigli, 2015</marker>
<rawString>Jos´e Camacho-Collados, Mohammad Taher Pilehvar, and Roberto Navigli. 2015b. A unified multilingual semantic representation of concepts. In Proceedings of ACL, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Gabrilovich Evgenly</author>
<author>Matias Yossi</author>
<author>Rivlin Ehud</author>
<author>Solan Zach</author>
<author>Wolfman Gadi</author>
<author>Ruppin Eytan</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="4200" citStr="Finkelstein et al., 2002" startWordPosition="624" endWordPosition="627">the results of the evaluation performed on the generated datasets in Section 5. Finally, we specify the released resources in Section 6, followed by concluding remarks in Section 7. 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic simila</context>
<context position="5818" citStr="Finkelstein et al., 2002" startWordPosition="882" endWordPosition="886">al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English MC-30 (Miller and Charles, 1991) and the WordSim-353 (Finkelstein et al., 2002) datasets into three languages. However, these datasets have several issues due to their construction procedure. The main problem arises from keeping the original scores from the English dataset in the translated datasets. For instance, the Spanish dataset contains the identical pair mediodia-mediodia with a similarity score of 3.42 (in the 0-4 scale). Furthermore, the datasets contain orthographic errors such as despliege and the previously mentioned mediodia (instead of despliegue and mediodia), and nouns translated into words with a different part of speech (e.g., implement from the English</context>
</contexts>
<marker>Finkelstein, Evgenly, Yossi, Ehud, Zach, Gadi, Eytan, 2002</marker>
<rawString>Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi, Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin Eytan. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Franco-Salvador</author>
<author>Paolo Rosso</author>
<author>Roberto Navigli</author>
</authors>
<title>A knowledge-based representation for cross-language document retrieval and categorization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 141h Conference on European chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>414--423</pages>
<location>Gothenburg,</location>
<contexts>
<context position="2529" citStr="Franco-Salvador et al., 2014" startWordPosition="374" endWordPosition="377">e English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy and Hirst (2012) for the automatic construction of cross-lin</context>
</contexts>
<marker>Franco-Salvador, Rosso, Navigli, 2014</marker>
<rawString>Marc Franco-Salvador, Paolo Rosso, and Roberto Navigli. 2014. A knowledge-based representation for cross-language document retrieval and categorization. In Proceedings of the 141h Conference on European chapter of the Association for Computational Linguistics (EACL), pages 414–423, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipediabased explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2062" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="300" endWordPosition="303">rom Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper p</context>
<context position="4547" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="684" endWordPosition="687">ssing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipediabased explicit semantic analysis. In Proceedings of IJCAI, pages 1606–1611, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Granada</author>
<author>Cassia Trojahn</author>
<author>Renata Vieira</author>
</authors>
<title>Comparing semantic relatedness between word pairs in Portuguese using Wikipedia.</title>
<date>2014</date>
<booktitle>In Computational Processing of the Portuguese Language,</booktitle>
<pages>170--175</pages>
<location>S˜ao Carlos/SP, Brazil.</location>
<contexts>
<context position="2349" citStr="Granada et al., 2014" startWordPosition="346" endWordPosition="349">d Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word </context>
<context position="5203" citStr="Granada et al., 2014" startWordPosition="787" endWordPosition="790">ar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English MC-30 (Miller and Charles, 1991) and the WordSim-353 (Finkelstei</context>
<context position="14815" citStr="Granada et al., 2014" startWordPosition="2377" endWordPosition="2380"> PT: Portuguese, FA: Farsi). 5 Evaluation 5.1 Spanish and Farsi Monolingual Datasets The inter-annotator agreements according to the average pairwise Pearson correlation among the judges for the newly created Spanish and Farsi datasets are, respectively, 0.83 and 0.88, which may be used as upper bounds for evaluating automatic systems. Our further analysis revealed that for both datasets no annotator obtained an average Pearson correlation with the rest of the annotators lower than 0.80, which attests to the reliability of our judges and guidelines. The German (Gurevych, 2005) and Portuguese (Granada et al., 2014) versions of the RG-65 dataset reported a lower inter-annotator agreement of 0.81 and 0.71, respectively, whereas the original English RG-65 (Rubenstein and Goodenough, 1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges. As also mentioned earlier, the French version (Joubarne and Inkpen, 2011) did not report any inter-annotator agreement. 5.2 Cross-lingual Datasets Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs were Spanish-English, SpanishFrench, Sp</context>
</contexts>
<marker>Granada, Trojahn, Vieira, 2014</marker>
<rawString>Roger Granada, Cassia Trojahn, and Renata Vieira. 2014. Comparing semantic relatedness between word pairs in Portuguese using Wikipedia. In Computational Processing of the Portuguese Language, pages 170–175. S˜ao Carlos/SP, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
</authors>
<title>Using the structure of a conceptual network in computing semantic relatedness.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>767--778</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2310" citStr="Gurevych, 2005" startWordPosition="342" endWordPosition="343"> (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly</context>
<context position="5164" citStr="Gurevych, 2005" startWordPosition="783" endWordPosition="784">Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English MC-30 (Miller and Charles</context>
<context position="14777" citStr="Gurevych, 2005" startWordPosition="2373" endWordPosition="2374">French, DE: German, ES: Spanish, PT: Portuguese, FA: Farsi). 5 Evaluation 5.1 Spanish and Farsi Monolingual Datasets The inter-annotator agreements according to the average pairwise Pearson correlation among the judges for the newly created Spanish and Farsi datasets are, respectively, 0.83 and 0.88, which may be used as upper bounds for evaluating automatic systems. Our further analysis revealed that for both datasets no annotator obtained an average Pearson correlation with the rest of the annotators lower than 0.80, which attests to the reliability of our judges and guidelines. The German (Gurevych, 2005) and Portuguese (Granada et al., 2014) versions of the RG-65 dataset reported a lower inter-annotator agreement of 0.81 and 0.71, respectively, whereas the original English RG-65 (Rubenstein and Goodenough, 1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges. As also mentioned earlier, the French version (Joubarne and Inkpen, 2011) did not report any inter-annotator agreement. 5.2 Cross-lingual Datasets Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs w</context>
</contexts>
<marker>Gurevych, 2005</marker>
<rawString>Iryna Gurevych. 2005. Using the structure of a conceptual network in computing semantic relatedness. In Proceedings of IJCNLP, pages 767–778. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Crosslingual semantic relatedness using encyclopedic knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1192--1201</pages>
<contexts>
<context position="2471" citStr="Hassan and Mihalcea, 2009" startWordPosition="365" endWordPosition="368">d similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy an</context>
<context position="5670" citStr="Hassan and Mihalcea (2009)" startWordPosition="860" endWordPosition="863">h RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English MC-30 (Miller and Charles, 1991) and the WordSim-353 (Finkelstein et al., 2002) datasets into three languages. However, these datasets have several issues due to their construction procedure. The main problem arises from keeping the original scores from the English dataset in the translated datasets. For instance, the Spanish dataset contains the identical pair mediodia-mediodia with a similarity score of 3.42 (in the 0-4 scale). Furthermore, the datasets contain orthographic errors such as despliege and the previously mentio</context>
<context position="10226" citStr="Hassan and Mihalcea (2009)" startWordPosition="1588" endWordPosition="1591"> to distinguish similarity and relatedness. The annotators were allowed to give scores from 0 to 4, with a step size of 0.5. Table 1 shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG65 dataset. As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores. This is due to the fact that associated senses with words do not hold one-to-one correspondence across different languages. This renders the approach of Hassan and Mihalcea (2009) insufficiently accurate for handling these differences. 4 Automatic Creation of Cross-lingual Similarity Datasets In this section we present our automatic method for building cross-lingual datasets. Although being targeted at building semantic similarity datasets, the algorithm is task-independent, so it may also be used for any task which measures any kind of relation between two linguistic items in a numerical way. Kennedy and Hirst (2012) proposed a method which exploits two aligned monolingual word similarity datasets for the construction of a French-English cross-lingual dataset. We foll</context>
</contexts>
<marker>Hassan, Mihalcea, 2009</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2009. Crosslingual semantic relatedness using encyclopedic knowledge. In Proceedings of EMNLP, pages 1192– 1201, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semantic Relatedness Using Salient Semantic Analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>884--889</pages>
<location>San Francisco, USA.</location>
<contexts>
<context position="4574" citStr="Hassan and Mihalcea, 2011" startWordPosition="688" endWordPosition="691">eijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Port</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2011. Semantic Relatedness Using Salient Semantic Analysis. In Proceedings of AAAI, pages 884–889, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<pages>1408--3456</pages>
<contexts>
<context position="4261" citStr="Hill et al., 2014" startWordPosition="636" endWordPosition="639"> Section 5. Finally, we specify the released resources in Section 6, followed by concluding remarks in Section 7. 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (196</context>
<context position="6699" citStr="Hill et al., 2014" startWordPosition="1019" endWordPosition="1022">ns the identical pair mediodia-mediodia with a similarity score of 3.42 (in the 0-4 scale). Furthermore, the datasets contain orthographic errors such as despliege and the previously mentioned mediodia (instead of despliegue and mediodia), and nouns translated into words with a different part of speech (e.g., implement from the English noun dataset MC-30 translated to the Spanish verb implementar). Additionally, the selection of the datasets was not ideal: MC-30 is a small subset of RG-65 and WordSim353 has been criticized for its annotation scheme, which conflates similarity and relatedness (Hill et al., 2014). Kennedy and Hirst (2012) proposed an automatic procedure for the construction of a FrenchEnglish version of RG-65. We refine their approach by also dealing with some issues that may arise in the automatic process. Additionally, we provide an evaluation of the automatic procedure on different languages. 3 Building Monolingual Word Similarity Datasets In this section we explain our methodology for the construction of the Spanish and Farsi versions of the English RG-65 dataset (Rubenstein and Goodenough, 1965). The methodology is divided into two main steps: First, the original English dataset </context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. arXiv:1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelos Hliaoutakis</author>
</authors>
<title>Giannis Varelas, Epimenidis Voutsakis, Euripides GM Petrakis, and Evangelos Milios.</title>
<date>2006</date>
<journal>International Journal on Semantic Web and Information Systems,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Hliaoutakis, 2006</marker>
<rawString>Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis, Euripides GM Petrakis, and Evangelos Milios. 2006. Information retrieval by semantic similarity. International Journal on Semantic Web and Information Systems, 2(3):55–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colette Joubarne</author>
<author>Diana Inkpen</author>
</authors>
<title>Comparison of semantic similarity for different languages using the Google n-gram corpus and second-order co-occurrence measures.</title>
<date>2011</date>
<booktitle>In Advances in Artificial Intelligence,</booktitle>
<pages>216--221</pages>
<location>Perth, Australia.</location>
<contexts>
<context position="2285" citStr="Joubarne and Inkpen, 2011" startWordPosition="336" endWordPosition="339">and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson </context>
<context position="5139" citStr="Joubarne and Inkpen, 2011" startWordPosition="777" endWordPosition="780">; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English </context>
<context position="15141" citStr="Joubarne and Inkpen, 2011" startWordPosition="2426" endWordPosition="2429">tomatic systems. Our further analysis revealed that for both datasets no annotator obtained an average Pearson correlation with the rest of the annotators lower than 0.80, which attests to the reliability of our judges and guidelines. The German (Gurevych, 2005) and Portuguese (Granada et al., 2014) versions of the RG-65 dataset reported a lower inter-annotator agreement of 0.81 and 0.71, respectively, whereas the original English RG-65 (Rubenstein and Goodenough, 1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges. As also mentioned earlier, the French version (Joubarne and Inkpen, 2011) did not report any inter-annotator agreement. 5.2 Cross-lingual Datasets Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs were Spanish-English, SpanishFrench, Spanish-German, and English-Farsi. In each case a proficient speaker of both languages was selected to carry out the evaluation. The Pearson correlations of the human judges with the automatically generated scores were 0.89 for Spanish-English, 0.94 for Spanish-French, 0.91 for Spanish-German, and 0.92 for English-Farsi, showi</context>
</contexts>
<marker>Joubarne, Inkpen, 2011</marker>
<rawString>Colette Joubarne and Diana Inkpen. 2011. Comparison of semantic similarity for different languages using the Google n-gram corpus and second-order co-occurrence measures. In Advances in Artificial Intelligence, pages 216–221. Perth, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2014 task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 81h International Workshop on Semantic Evaluation (SemEval 2014), in conjunction with COLING 2014,</booktitle>
<pages>17--26</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="9557" citStr="Jurgens et al., 2014" startWordPosition="1474" endWordPosition="1477">00 0.50 1.17 1.00 1.79 1.29 2.83 4.00 3.88 Table 1: Sample word pairs from the English and the newly created Spanish and Farsi RG-65 datasets. 3.2 Scoring the dataset Twelve native Spanish speakers were asked to evaluate the similarity for the Spanish translations. In order to obtain a more global distribution of judges, we included judges both both Spain and Latin America. As far as the Farsi dataset was concerned, twelve Farsi native speakers scored the newly translated pairs. The guidelines provided to the annotators were based on the recent SemEval task on Cross-Level Semantic Similarity (Jurgens et al., 2014), which provides clear indications in order to distinguish similarity and relatedness. The annotators were allowed to give scores from 0 to 4, with a step size of 0.5. Table 1 shows example pairs with their corresponding scores from the English and the newly created Spanish and Farsi versions of the RG65 dataset. As we can see from the table, the scores across languages are not necessarily identical, with small, in a few cases significant, differences between the corresponding scores. This is due to the fact that associated senses with words do not hold one-to-one correspondence across differe</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. Semeval-2014 task 3: Cross-level semantic similarity. In Proceedings of the 81h International Workshop on Semantic Evaluation (SemEval 2014), in conjunction with COLING 2014, pages 17–26, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Graeme Hirst</author>
</authors>
<title>Measuring semantic relatedness across languages.</title>
<date>2012</date>
<booktitle>In Proceedings of xLiTe: Cross-Lingual Technologies Workshop at the Neural Information Processing Systems Conference,</booktitle>
<pages>1--6</pages>
<location>Lake Tahoe, USA.</location>
<contexts>
<context position="3085" citStr="Kennedy and Hirst (2012)" startWordPosition="457" endWordPosition="460">lcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy and Hirst (2012) for the automatic construction of cross-lingual datasets from aligned monolingual datasets. The paper is structured as follows. We first briefly review some of the major monolingual and cross-lingual word similarity datasets in Section 2. We then discuss the details of our procedure for the construction of the Spanish and Farsi word similarity datasets in Section 3. Section 4 provides the details of our algorithm for the automatic construction of the cross-lingual datasets. We report the results of the evaluation performed on the generated datasets in Section 5. Finally, we specify the releas</context>
<context position="6725" citStr="Kennedy and Hirst (2012)" startWordPosition="1023" endWordPosition="1026">r mediodia-mediodia with a similarity score of 3.42 (in the 0-4 scale). Furthermore, the datasets contain orthographic errors such as despliege and the previously mentioned mediodia (instead of despliegue and mediodia), and nouns translated into words with a different part of speech (e.g., implement from the English noun dataset MC-30 translated to the Spanish verb implementar). Additionally, the selection of the datasets was not ideal: MC-30 is a small subset of RG-65 and WordSim353 has been criticized for its annotation scheme, which conflates similarity and relatedness (Hill et al., 2014). Kennedy and Hirst (2012) proposed an automatic procedure for the construction of a FrenchEnglish version of RG-65. We refine their approach by also dealing with some issues that may arise in the automatic process. Additionally, we provide an evaluation of the automatic procedure on different languages. 3 Building Monolingual Word Similarity Datasets In this section we explain our methodology for the construction of the Spanish and Farsi versions of the English RG-65 dataset (Rubenstein and Goodenough, 1965). The methodology is divided into two main steps: First, the original English dataset is translated into the tar</context>
<context position="10672" citStr="Kennedy and Hirst (2012)" startWordPosition="1653" endWordPosition="1656">his is due to the fact that associated senses with words do not hold one-to-one correspondence across different languages. This renders the approach of Hassan and Mihalcea (2009) insufficiently accurate for handling these differences. 4 Automatic Creation of Cross-lingual Similarity Datasets In this section we present our automatic method for building cross-lingual datasets. Although being targeted at building semantic similarity datasets, the algorithm is task-independent, so it may also be used for any task which measures any kind of relation between two linguistic items in a numerical way. Kennedy and Hirst (2012) proposed a method which exploits two aligned monolingual word similarity datasets for the construction of a French-English cross-lingual dataset. We followed their initial idea and proposed a generalization of the approach which would be capable of automatically constructing reliable cross-lingual similarity datasets for any pair of languages. Algorithm. Algorithm 1 shows our procedure for constructing a cross-lingual dataset starting from two monolingual datasets. Note that the pairs in the two monolingual datasets should be previously aligned. Specifically, we refer to each dataset D as {PD</context>
</contexts>
<marker>Kennedy, Hirst, 2012</marker>
<rawString>Alistair Kennedy and Graeme Hirst. 2012. Measuring semantic relatedness across languages. In Proceedings of xLiTe: Cross-Lingual Technologies Workshop at the Neural Information Processing Systems Conference, pages 1–6, Lake Tahoe, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The Meteor metric for automatic evaluation of Machine Translation. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="1536" citStr="Lavie and Denkowski, 2009" startWordPosition="221" endWordPosition="224">dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi. 1 Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until th</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The Meteor metric for automatic evaluation of Machine Translation. Machine Translation, 23(2-3):105– 115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="1779" citStr="McCarthy and Navigli, 2009" startWordPosition="255" endWordPosition="258">arity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-ling</context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>An automatic method for generating sense tagged corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>461--466</pages>
<location>Orlando, Florida, USA.</location>
<contexts>
<context position="1948" citStr="Mihalcea and Moldovan, 1999" startWordPosition="283" endWordPosition="286">pular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados </context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan Moldovan. 1999. An automatic method for generating sense tagged corpora. In Proceedings of AAAI, pages 461–466, Orlando, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACLHLT-07,</booktitle>
<pages>196--203</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="2078" citStr="Mihalcea, 2007" startWordPosition="304" endWordPosition="305">wardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two cont</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Rada Mihalcea. 2007. Using Wikipedia for automatic Word Sense Disambiguation. In Proc. of NAACLHLT-07, pages 196–203, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="4160" citStr="Miller and Charles, 1991" startWordPosition="619" endWordPosition="622">f the cross-lingual datasets. We report the results of the evaluation performed on the generated datasets in Section 5. Finally, we specify the released resources in Section 6, followed by concluding remarks in Section 7. 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual inform</context>
<context position="5771" citStr="Miller and Charles, 1991" startWordPosition="875" endWordPosition="878">n (Gurevych, 2005), and Portuguese (Granada et al., 2014). No inter-annotator agreement was calculated for the French version, while the German and Portuguese were reported to have the respective inter-annotator agreements of 0.81 and 0.71 in terms of average pairwise Pearson correlation. Our Spanish version of the RG-65 dataset reports a high inter-annotator agreement of 0.83, while the Farsi version achieves 0.88. A few works have also focused on the construction of cross-lingual resources. Hassan and Mihalcea (2009) built two sets of cross-lingual datasets by translating the English MC-30 (Miller and Charles, 1991) and the WordSim-353 (Finkelstein et al., 2002) datasets into three languages. However, these datasets have several issues due to their construction procedure. The main problem arises from keeping the original scores from the English dataset in the translated datasets. For instance, the Spanish dataset contains the identical pair mediodia-mediodia with a similarity score of 3.42 (in the 0-4 scale). Furthermore, the datasets contain orthographic errors such as despliege and the previously mentioned mediodia (instead of despliegue and mediodia), and nouns translated into words with a different p</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Graeme Hirst</author>
</authors>
<title>Distributional measures of semantic distance: A survey.</title>
<date>2012</date>
<location>CoRR, abs/1203.1858.</location>
<contexts>
<context position="1675" citStr="Mohammad and Hirst, 2012" startWordPosition="241" endWordPosition="244"> six languages: English, Spanish, French, German, Portuguese, and Farsi. 1 Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Ink</context>
</contexts>
<marker>Mohammad, Hirst, 2012</marker>
<rawString>Saif Mohammad and Graeme Hirst. 2012. Distributional measures of semantic distance: A survey. CoRR, abs/1203.1858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to grade short answer questions using semantic similarity measures and dependency graph alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>752--762</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1628" citStr="Mohler et al., 2011" startWordPosition="235" endWordPosition="238">ts, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi. 1 Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and </context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011. Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In Proceedings of ACL, pages 752–762, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelRelate! a joint multilingual approach to computing semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>108--114</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="2499" citStr="Navigli and Ponzetto, 2012" startWordPosition="369" endWordPosition="373"> the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and 0.88, respectively, in terms of Pearson correlation, and secondly, we create fifteen cross-lingual word similarity datasets based on RG-65, covering six languages, by proposing an improved version of the approach of Kennedy and Hirst (2012) for the autom</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelRelate! a joint multilingual approach to computing semantic relatedness. In Proceedings of AAAI, pages 108–114, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for Word Sense Disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>241--257</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="1485" citStr="Patwardhan et al., 2003" startWordPosition="214" endWordPosition="217">r procedure and taking the RG-65 word similarity dataset as a reference, we release two high-quality Spanish and Farsi (Persian) monolingual datasets, and fifteen cross-lingual datasets for six languages: English, Spanish, French, German, Portuguese, and Farsi. 1 Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pileh</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for Word Sense Disambiguation. In Proceedings of CICLing, pages 241–257, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>A robust approach to aligning heterogeneous lexical resources.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>468--478</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="1724" citStr="Pilehvar and Navigli, 2014" startWordPosition="248" endWordPosition="251">n, Portuguese, and Farsi. 1 Introduction Semantic similarity is a field of Natural Language Processing which measures the extent to which two linguistic items are similar. In particular, word similarity is one of the most popular benchmarks for the evaluation of word or sense representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portugue</context>
</contexts>
<marker>Pilehvar, Navigli, 2014</marker>
<rawString>Mohammad Taher Pilehvar and Roberto Navigli. 2014. A robust approach to aligning heterogeneous lexical resources. In Proceedings of ACL, pages 468–478, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1341--1351</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2101" citStr="Pilehvar et al., 2013" startWordPosition="306" endWordPosition="309">2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we </context>
<context position="4597" citStr="Pilehvar et al., 2013" startWordPosition="692" endWordPosition="695">2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges (no final inter-annotator agreement for the total fifty-one judges was calculated). The original English RG65 has also been used as a base for different languages: French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al.,</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity. In Proceedings of ACL, pages 1341– 1351, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="2245" citStr="Rubenstein and Goodenough, 1965" startWordPosition="330" endWordPosition="333">r et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating cross-lingual systems. This paper provides two contributions: Firstly, we construct Spanish and Farsi versions of the standard RG-65 dataset scored by twelve annotators with high inter-annotator agreements of 0.83 and </context>
<context position="4314" citStr="Rubenstein and Goodenough, 1965" startWordPosition="643" endWordPosition="647">eased resources in Section 6, followed by concluding remarks in Section 7. 1 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 1–7, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Multiple word similarity datasets have been constructed for the English language: MC-30 (Miller and Charles, 1991), WordSim-353 (Finkelstein et al., 2002), MEN (Bruni et al., 2014), and Simlex999 (Hill et al., 2014). The RG-65 dataset (Rubenstein and Goodenough, 1965) is one of the oldest and most popular word similarity datasets, and has been used as a standard benchmark for measuring the reliability of word and sense representations (Agirre and de Lacalle, 2004; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011; Pilehvar et al., 2013; Baroni et al., 2014; Camacho-Collados et al., 2015a). The original RG-65 dataset was constructed with the aim of evaluating the degree to which contextual information is correlated with semantic similarity for the English language. Rubenstein and Goodenough (1965) reported an inter-annotator agreement of 0.85 for </context>
<context position="7213" citStr="Rubenstein and Goodenough, 1965" startWordPosition="1099" endWordPosition="1103">rdSim353 has been criticized for its annotation scheme, which conflates similarity and relatedness (Hill et al., 2014). Kennedy and Hirst (2012) proposed an automatic procedure for the construction of a FrenchEnglish version of RG-65. We refine their approach by also dealing with some issues that may arise in the automatic process. Additionally, we provide an evaluation of the automatic procedure on different languages. 3 Building Monolingual Word Similarity Datasets In this section we explain our methodology for the construction of the Spanish and Farsi versions of the English RG-65 dataset (Rubenstein and Goodenough, 1965). The methodology is divided into two main steps: First, the original English dataset is translated into the target language (Section 3.1) and then, the newly translated pairs are scored by human annotators (Section 3.2). 3.1 Translating from English to Spanish/Farsi The translation of RG-65 from English to Spanish and Farsi was performed by, respectively, three English-Spanish and three English-Farsi annotators who were fluent English speakers and native speakers of the target language. The translation procedure was as follows. First, two annotators translated each English pair in the dataset</context>
<context position="14989" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2403" endWordPosition="2406">lation among the judges for the newly created Spanish and Farsi datasets are, respectively, 0.83 and 0.88, which may be used as upper bounds for evaluating automatic systems. Our further analysis revealed that for both datasets no annotator obtained an average Pearson correlation with the rest of the annotators lower than 0.80, which attests to the reliability of our judges and guidelines. The German (Gurevych, 2005) and Portuguese (Granada et al., 2014) versions of the RG-65 dataset reported a lower inter-annotator agreement of 0.81 and 0.71, respectively, whereas the original English RG-65 (Rubenstein and Goodenough, 1965) reported an inter-annotator agreement of 0.85 for a subset of fifteen judges. As also mentioned earlier, the French version (Joubarne and Inkpen, 2011) did not report any inter-annotator agreement. 5.2 Cross-lingual Datasets Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs were Spanish-English, SpanishFrench, Spanish-German, and English-Farsi. In each case a proficient speaker of both languages was selected to carry out the evaluation. The Pearson correlations of the human judges wi</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>WikiRelate! Computing semantic relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1419--1424</pages>
<location>Boston, USA.</location>
<contexts>
<context position="2028" citStr="Strube and Ponzetto, 2006" startWordPosition="296" endWordPosition="299"> of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2003) to Machine Translation (Lavie and Denkowski, 2009), Information Retrieval (Hliaoutakis et al., 2006), Question Answering (Mohler et al., 2011), Text Summarization (Mohammad and Hirst, 2012), Ontology Alignment (Pilehvar and Navigli, 2014), and Lexical Substitution (McCarthy and Navigli, 2009). However, due to the lack of standard multilingual benchmarks, word similarity systems had in the main been limited to the English language (Mihalcea and Moldovan, 1999; Agirre and Lopez, 2003; Agirre and de Lacalle, 2004; Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Mihalcea, 2007; Pilehvar et al., 2013; Baroni et al., 2014), up until the recent creation of datasets built by translating the English RG65 dataset (Rubenstein and Goodenough, 1965) into French (Joubarne and Inkpen, 2011), German (Gurevych, 2005), and Portuguese (Granada et al., 2014). And what is more, cross-lingual applications have grown in importance over the last few years (Hassan and Mihalcea, 2009; Navigli and Ponzetto, 2012; Franco-Salvador et al., 2014; Camacho-Collados et al., 2015b). Unfortunately, very few reliable datasets exist for evaluating c</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate! Computing semantic relatedness using Wikipedia. In Proceedings of AAAI, pages 1419– 1424, Boston, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>