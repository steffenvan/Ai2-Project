<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001649">
<title confidence="0.9987285">
Using Classifier Features for Studying the Effect of Native Language on the
Choice of Written Second Language Words
</title>
<author confidence="0.998883">
Oren Tsur
</author>
<affiliation confidence="0.9879155">
Institute of Computer Science
The Hebrew University
</affiliation>
<address confidence="0.511813">
Jerusalem, Israel
</address>
<email confidence="0.99476">
oren@cs.huji.ac.il
</email>
<author confidence="0.992813">
Ari Rappoport
</author>
<affiliation confidence="0.987862">
Institute of Computer Science
The Hebrew University
</affiliation>
<address confidence="0.499585">
Jerusalem, Israel
</address>
<email confidence="0.95703">
www.cs.huji.ac.il/∼arir
</email>
<sectionHeader confidence="0.995153" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975785714286">
We apply machine learning techniques to
study language transfer, a major topic in
the theory of Second Language Acquisition
(SLA). Using an SVM for the problem of
native language classification, we show that
a careful analysis of the effects of various
features can lead to scientific insights. In
particular, we demonstrate that character bi-
grams alone allow classification levels of
about 66% for a 5-class task, even when con-
tent and function word differences are ac-
counted for. This may show that native lan-
guage has a strong effect on the word choice
of people writing in a second language.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986787641025641">
While advances in NLP achieve improved results for
NLP applications such as machine translation, ques-
tion answering and document summarization, there
are other fields of research that can benefit from the
methods used by the NLP community. Second Lan-
guage Acquisition (SLA), a major area in Applied
Linguistics and Cognitive Science, is one such field.
In this paper we demonstrate how modern machine
learning tools can contribute to SLA theory. In par-
ticular, we address the major SLA topic of language
transfer, the effect of native language on second lan-
guage learners. Using an SVM for the computa-
tional problem of native language classification, we
study in detail the effects of various SVM features.
Surprisingly, character bi-grams alone lead to a clas-
sification accuracy of about 66% in a 5-class task,
9
even when accounting for differences in content and
function words.
This result leads us to form a novel hypothesis on
the role of language transfer in SLA: that the choice
of words people make when writing in a second lan-
guage is strongly influenced by the phonology of
their native language.
As far as we know, this is the first time that such
a hypothesis has beed formulated. Moreover, this is
the first statistical learning-supported hypothesis in
language transfer. Our results should be further sub-
stantiated by additional psycholinguistic and com-
putational experiments; nonetheless, we provide a
strong starting point.
The next section provides some essential back-
ground. In Section 3 we describe our experimen-
tal setup and feature selection, and in Section 4 we
detail an array of variations of experiments for rul-
ing out some possible types of bias that might have
affected the results. In Section 5 we discuss our hy-
pothesis in the context of psycho-linguistic theory.
We conclude with directions for future research.
</bodyText>
<sectionHeader confidence="0.986623" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9998596">
Our hypothesis is tested within an algorithm ad-
dressing the practical problem of determining the
native language of an anonymous writer writing in a
foreign language. The problem is applicable to dif-
ferent fields, such as language instructing, tailored
error correction, security applications and psycho-
linguistic research.
As background, we start from the somewhat re-
lated problem of authorship attribution. The au-
thorship attribution problem was addressed by lin-
</bodyText>
<note confidence="0.674673333333333">
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9–16,
Qc
Prague, Czech Republic, June 2007 2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999792475">
guists and other literary experts trying to pinpoint
an anonymous author, such as that of The Federalist
Papers (Holmes and Forsyth, 1995). Traditionally,
authorship experts analyzed topics, stylistic idiosyn-
crasies and personal information about the possible
candidates in order to determine an author.
While authorship is usually addressed with deep
human inspection of the texts in question, it has al-
ready been shown that automatic text analysis based
on various stylistic features can identify the gender
of an anonymous author with accuracy above 80%
(Argamon et al, 2003). Various papers (Diedrich et
al, 2003; Koppel and Schler, 2003; Koppel et al,
2005a; Stamatatos et al, 2004) report relative suc-
cess in machine based authorship attribution tasks
for small sets of known candidates.
Native language detection is a harder problem
than the authorship attribution problem, since we
wish to characterize the writing style of a set of
writers rather than the unique style of a single
person. There are several works presenting non-
native speech recognition and dialect analysis sys-
tems (Bouselmi et al, 2005; Bouselmi et al, 2006;
Hansen et al, 2004). However, all those works are
based on acoustic signals, not on written texts.
Koppel et al (2005a) report an accuracy of 80% in
the task of determining a writer’s native language.
To the best of our knowledge, this is the only pub-
lished work on automated classification of an au-
thor’s native language (along with another version
of the paper by the same authors (Koppel et al,
2005b)). Koppel et al used an SVM (Sch¨olkopf and
Smola, 2002) and a combination of features in their
system (such as errors analysis and POS-error co-
occurrences, as described in section 2.2), but sur-
prisingly, it appears that a very naive set of features
achieves a relatively high accuracy. The charac-
ter bi-gram frequencies feature performs rather well,
and definitely outperforms the intuitive contribution
of frequent bigrams in this type of task.
</bodyText>
<sectionHeader confidence="0.992979" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.993777">
3.1 The Corpus
</subsectionHeader>
<bodyText confidence="0.999956088235294">
The corpus that served for all of the experiments
described in this paper is the International Corpus
of Learner English (ICLE) (Granger et al, 2002),
which was also the one used by Koppel et al (2005a;
2005b). The corpus was compiled for the purpose of
studying the English writing of non-native speakers.
All contributors to the corpus are advanced English
students and are roughly the same age. The corpus is
combined from a number of sub-corpora, each con-
taining one native language. The corpus was assem-
bled in ten years of international collaboration be-
tween a number of universities and it contains more
than 2 million words of writing by students from 19
different native language backgrounds. We followed
Koppel et al (2005a) and worked on 5 sub-corpora,
each containing 238 randomly selected essays by na-
tive speakers of the following languages: Bulgarian,
Czech, French, Russian and Spanish. Each of the
texts in the corpus was written by a different author
and is of length between 500 to 1,000 words. Each
of the sub corpora contains about 180,000 (unique)
types, for a total of 886,677 tokens.
Essays in the corpus are of two types: argumen-
tative essays and literature examination papers. De-
scriptive, narrative or technical subjects were not in-
cluded in the corpus. The literature examination es-
says were restricted to no more than 25% of each
sub-corpus. Each contributor was requested to fill a
learner profile that was used to fine-proof the corpus
as needed.
In order to verify our results we used another con-
trol corpus containing the Dutch and Italian sub-
corpora contained in the ICLE instead of the Bul-
garian and French ones.
</bodyText>
<subsectionHeader confidence="0.999605">
3.2 Document Representation
</subsectionHeader>
<bodyText confidence="0.966318">
In the original experiment by Koppel et al (2005a)
each document was represented by a numerical vec-
tor of 1,035 dimensions. Each vector entry rep-
resented the frequency (relative to the document’s
length) of a given feature. The features were of 4
types:
</bodyText>
<listItem confidence="0.9997985">
• 400 function words
• 200 most frequent letter n-grams
• 250 rare POS bi-gram
• 185 error types
</listItem>
<bodyText confidence="0.9998335">
While the first three types of attributes are relatively
straightforward, the fourth is more complex. It rep-
resents clusters of families of spelling errors as well
as co-occurrences of errors and POS tags. Document
</bodyText>
<page confidence="0.992627">
10
</page>
<bodyText confidence="0.998891">
representation is described in detail in (Koppel et al,
2005a; Koppel et al, 2005b).
A multi-class SVM (Witten and Frank, 2005) was
employed for learning and evaluating the classifica-
tion model. The experiment was run in a 10-fold
cross validation manner in order to test the effec-
tiveness of the model.
</bodyText>
<subsectionHeader confidence="0.999698">
3.3 Previous Results
</subsectionHeader>
<bodyText confidence="0.999978263157895">
Koppel et al (2005a) report that when all features
types were used in tandem, an accuracy of 80.2%
was achieved. In the discussion section they an-
alyze the frequency of a few function words, er-
ror types, the co-occurrences of POS tags and er-
rors, and the co-occurrences of POS tags and certain
function words that seem to have significance in the
support vectors learnt by the SVM.
The goal of their research was to obtain the best
classification, therefore the results obtained by us-
ing only bi-grams of characters were not particularly
noted, although, surprisingly, representing each doc-
ument by only using the relative frequency of the
top 200 characters bi-grams achieves an accuracy of
about 66%. We believe that this surprising fact ex-
poses some fundamental phenomenon of human lan-
guage behavior. In the next section we describe a set
of experiments designed to isolate the causes of this
phenomenon.
</bodyText>
<sectionHeader confidence="0.979567" genericHeader="method">
4 Experimental Variations and Results
</sectionHeader>
<bodyText confidence="0.9999322">
Intuitively, we do not expect the most frequent char-
acter n-grams to serve as good native language pre-
dictors, expecting that these will only reflect the
most frequent English words (and characters se-
quences). Accordingly, without language transfer
effects, a naive baseline classifier based on an n-
gram model is expected to achieve about 20% ac-
curacy in a 5 native languages classification task.
However, using classification based on the relative
frequency of top 200 bi-grams achieves about 66%1
in all experiments, substantially higher than the ran-
dom baseline. These results are so surprising that
they suggest that the characters bi-grams classifi-
cation masks some other bias or noise in the cor-
pus, or, conversely, that it mirrors other simple-to-
</bodyText>
<footnote confidence="0.66074">
1Koppel et al did not report these results explicitly. How-
ever, they can be roughly estimated from their graph.
</footnote>
<figureCaption confidence="0.91498">
Figure 1: Classification accuracy of the different
variations of document representation. b-g: bi-
grams, f-w: function words, c-w: content words.
</figureCaption>
<bodyText confidence="0.9998968">
explain phenomena such as shallow language trans-
fer through the use of function words, or content
bias. The following sub-sections describe different
variations of the experiment, ruling out the effect of
these different types of bias.
</bodyText>
<subsectionHeader confidence="0.998904">
4.1 Unigram Baseline
</subsectionHeader>
<bodyText confidence="0.999950538461538">
We first implemented a naive baseline classifier. We
represented each document by the normalized fre-
quencies of the (de-capitalized) letters it contains2.
These frequencies are simply a unigram model of
the sub-corpora. Using the multi-class SVM (Wit-
ten and Frank, 2005) we obtained 46.78% accu-
racy. This accuracy is more than twice the ran-
dom baseline accuracy. This result is in accordance
with our bi-grams results. Our discussion focuses on
bi-grams rather than unigrams because the former’s
results are much higher and because bi-grams are
much closer to the phonology of the language (for
alphabetic scripts, of course).
</bodyText>
<subsectionHeader confidence="0.999349">
4.2 Bi-grams Based Classification
</subsectionHeader>
<bodyText confidence="0.992752">
Choosing the 200 most frequent character bi-grams
in the corpus, we used a vector of the same dimen-
sion. Each vector entry contained the normalized
frequency of one of the bi-grams. Using a multi-
class SVM in a 10-fold cross validation manner we
2White spaces were considered a letter. However, sequences
of white spaces and tabs were collapsed to a single white space.
All the experiments that make use of character frequencies were
performed twice, including and excluding punctuation marks.
Results for both experiments are similar, therefore all the num-
bers reported in this paper are based on letters and punctuation
marks.
</bodyText>
<page confidence="0.989825">
11
</page>
<table confidence="0.998854714285714">
Bulg. Czech French Russian Spanish
dr 170 183 n/a 195 n/a
am 117 135 142 140 152
m 121 120 133 119 139
iv 104 138 144 148 148
y 161 181 196 183 166
la 122 123 122 142 105
</table>
<tableCaption confidence="0.996554">
Table 1: Some of the separating bi-grams found in
</tableCaption>
<bodyText confidence="0.989606263157895">
the feature selection process. ‘ ’ indicates a white
space. The numbers are the frequency ranking of
the bi-grams in each sub-corpus (e.g., there are 103
bi-grams more frequent than ‘iv’ in the Bulgarian
corpus). n/a indicates that this bi-gram is not one of
the 200 most frequent bi-grams of the sub-corpus.
achieved 65.60% accuracy with standard deviation
of 3.99.
The bi-grams features in the 200 dimensional vec-
tor are the 200 most frequent bi-grams in the whole
corpus, regardless of their frequency in each sub-
corpus. We note that the effect of misspelled words
on the 200 most frequent bi-grams is negligible.
A more sophisticated feature selection could re-
duce the dimension of the representation vector
without detracting from the results. Careful fea-
ture selection can also give a better intuition regard-
ing the support vectors. We performed feature se-
lection in the following manner: we chose the top
200 bi-grams of each sub-corpus, getting 245 unique
bi-grams in total. We then chose all the bi-grams
that were ranked significantly higher or significantly
lower in one language than in at least one other
language, assuming that those bi-grams have strong
separating power. With the threshold of significance
set to 20 we obtained 84 separating bi-grams. Table
1 shows some of the separating bi-grams thus found.
For example, ‘la’ is a good separator between Rus-
sian and Spanish (its rank in the Spanish corpus is
much higher than that in the Russian corpus), but
not between other pairs.
Using only those 84 bigrams we obtained clas-
sification accuracy of 61.38%, a drop of only 4%
compared to the results achieved with the 200 di-
mensional vectors. These results show that increas-
ing the dimension of the representation vector using
additional bi-grams contribute a marginal improve-
ment while it does not introduce substantial noise.
</bodyText>
<subsectionHeader confidence="0.999559">
4.3 Using Tri-gram Frequencies as Features
</subsectionHeader>
<bodyText confidence="0.999883777777778">
Repeating the same experiment with the top 200 tri-
grams, we obtained an accuracy of 59.67%, which
is 40% higher than the expected baseline and 15%
higher than the uni-grams baseline. These results
show that the texts in our corpus can be classified
by only using naive n-gram models, while the op-
timal n of the n-gram is a different question that
might be addressed in a different work (and might
be language-dependent).
</bodyText>
<subsectionHeader confidence="0.99307">
4.4 Function Words Based Classification
</subsectionHeader>
<bodyText confidence="0.999896208333333">
Function words are words that have a little lexical
meaning but instead serve to express grammatical
relations within a sentence or specify the attitude of
the speaker (function words should not be confused
with stopwords, although the lists of most frequent
function words and the stopword list share a large
subset). We used the same list of 460 function words
used by Koppel et al (2005a). A partial list includes:
{a, afterward, although, because, cannot, do, enter,
eventually, fifteenth, hither, hath, hence, lastly, oc-
casionally, presumable, said, seldom, undoubtedly,
was}.
In this variation of the experiment, we represented
each document only by the relative frequencies of
the function words it contained. Using the same
experimental setup as before, we achieved an ac-
curacy of 66.7%. These results are less surprising
than the results obtained by the character n-grams
vectors, since we do expect native speakers of a cer-
tain language to use, misuse or ignore certain func-
tion words as a result from language transfer mech-
anisms (Odlin, 1989). For example, it is well known
that native speakers of Russian tend to omit English
articles.
</bodyText>
<subsectionHeader confidence="0.984955">
4.5 Function Words Bias
</subsectionHeader>
<bodyText confidence="0.999964222222222">
The previous results suggest that the n-gram based
classification is simply the result of the different
uses of function words by speakers of different na-
tive languages. In order to rule out the effect of the
function words on the bi-gram-based classification,
we removed all function words from the corpus, re-
calculated the bi-gram frequencies and ran the ex-
periment once again, this time achieving an accuracy
of 62.92% in the 10-fold cross validation test.
</bodyText>
<page confidence="0.997194">
12
</page>
<bodyText confidence="0.99963075">
These results, obtained on the function words-free
corpus, clearly show that n-gram based classification
is not a mere artifact masking the use of function
words.
</bodyText>
<subsectionHeader confidence="0.993675">
4.6 Content Bias
</subsectionHeader>
<bodyText confidence="0.999529857142857">
Bi-gram frequencies could also reflect content bias
rather than language use. By content bias we mean
that the subject matter of the documents in the dif-
ferent sub-corpora could exhibit internal sub-corpus
uniformity and external sub-corpus disparity. In or-
der to rule this out, we employed a variation on the
Term Frequency – Inverted Document Frequency
(tf-idf) content analysis metric.
The tf-idf measure is a statistical measure that is
used in information retrieval tasks to evaluate how
important a word/term is to a document in a collec-
tion or corpus (Salton and Buckley, 1988). Given a
collection of documents D, the tf-idf weight of term
t in a document d E D is computed as follows:
</bodyText>
<equation confidence="0.9896255">
tfidft = ft,d x log |D|
ft,D
</equation>
<bodyText confidence="0.999818764705882">
where ft,d is the frequency of term t in document
d, and ft,D is the number of documents in which t
appears. Therefore, the weight of term t E d is max-
imal if it is a common term in d while the number of
documents it appears in is relatively low.
We used the tf-idf weights in the information re-
trieval sense in order to discover the dominant con-
tent words of each sub-corpus. We treated each sub-
corpus (set of documents by writers who share a
native language) as a single document and calcu-
lated the tf-idf of each word. In order to determine
whether there is a content bias or not, we set a domi-
nance threshold, and removed all words such that the
difference between their tf-idf score in two different
sub-corpora is higher than the dominance threshold.
Given a threshold t, the dominance Dw,t, of a token
w is given by:
</bodyText>
<equation confidence="0.797502">
Dw,t = maxi,�|tfidfw,i − tfidfw,9|
</equation>
<bodyText confidence="0.9994395">
where tfidfw,k is the tf-idf score of token w in
sub-corpus k. Changing the threshold in 0.0005 in-
tervals, we removed from 1 to 340 unique content
words (between 1,545 and 84,725 word tokens in to-
tal). However, the classification accuracy was essen-
tially the same (see Figure 2), with a slight drop of
</bodyText>
<table confidence="0.786524333333333">
Word Bulg. Czech Fr. Rus. Spa.
europe 0 0.3 2.7 0.2 0.2
european 0 0.3 3 0.1 0.5
imagination 4.3 2 0.8 1 0.8
television 0 3.6 1.9 3.1 0.3
women 0.4 1.7 1.2 5.5 2.6
</table>
<tableCaption confidence="0.9553655">
Table 2: The tf-idf score of some of the most domi-
nant words, multiplied by 1,000 for easier reading.
</tableCaption>
<table confidence="0.99944425">
Subcorpus content function unique
words words stems
Bulgarian 1543 94685 11325
Czech 2784 110782 12834
French 2059 67016 9474
Russian 2730 112410 12338
Spanish 2985 108052 12627
Total 12101 492945 36474
</table>
<tableCaption confidence="0.94525">
Table 3: Numbers of dominant content words (with
a threshold of 0.0025) and function words that were
</tableCaption>
<bodyText confidence="0.9660680625">
removed from each sub-corpus. The unique stems
column indicates the number of unique stems (types)
that remained after removal of c-w and f-w.
only 2% after removing 51 content words (by using
a threshold of 0.0015).
We calculated the tf-idf weights after stop-words
removal and stemming (using a Porter stemmer
(Porter, 1980)), trying to pinpoint dominant stems.
The results were similar to the word’s tf-idf and no
significantly dominant stem was found in either of
the sub-corpora.
A drop of only 3% in accuracy was noticed after
removing both dominant content words and function
words. These results show that if a content bias ex-
ists in the corpus it has only a minor effect on the
SVM classification, and that the n-grams based clas-
</bodyText>
<figureCaption confidence="0.9911665">
Figure 2: Classification accuracy as a function of the
threshold (removed content words).
</figureCaption>
<page confidence="0.982155">
13
</page>
<table confidence="0.999829625">
Thresh. 0.004 0.003 0.0025 0.0015 0.001
2 c-w 9 c-w 15 c-w 51 c-w 113 c-w
Bulg. 77 908 1543 3955 7426
Czech 306 1829 2784 5139 8588
French 665 1829 2059 3603 6205
Russian 781 1886 2730 6302 9918
Spanish 389 1418 2985 6548 10521
Total 2218 7970 12101 25547 42658
</table>
<tableCaption confidence="0.988306">
Table 4: Number of occurrences of content words
</tableCaption>
<bodyText confidence="0.966816083333333">
that were removed from each sub-corpus for some
of the thresholds. The numbers in the top row indi-
cate the threshold and the number of unique content
words that were found with this threshold.
sification is not an artifact of a content bias.
We ran the same experiment five more times, each
time on 4 sub-corpora instead of 5, removing one
(different) language each time. The results in all 5
4-class experiments were essentially the same, and
similar to those of the 5 language task (beyond the
fact that the random baseline for the former is 25%
rather than 20%).
</bodyText>
<subsectionHeader confidence="0.996035">
4.7 Suffix Bias
</subsectionHeader>
<bodyText confidence="0.999323909090909">
Bias might also be attributed to the use of suf-
fixes. There are numerous types of English suf-
fixes, which, roughly speaking, may be categorized
as derivational or inflectional. It is reasonable to ex-
pect that just like a use of function words, use or mis-
use of certain suffixes might occur due to language
transfer. Frequent use of a certain suffix or avoid-
ance of the use of a certain suffix may influence the
bi-grams statistics and thus the bi-grams classifica-
tion may be only an artifact of the suffixes usage.
Checking the use of the 50 most productive suf-
fixes taken from a standard list (e.g. ing, ed, less,
able, most, en) we have found that only a small num-
ber of suffixes are not equally used by speakers of all
5 languages. Most notable are the differences in the
use of ing between native French speakers and na-
tive Czech speakers and the differences of use of less
between Bulgarian and Spanish speakers (Table 5).
However, no real bias can be attributed to the use of
any of the suffixes because their relative aggregate
effect on the values in the support vector entries is
very small.
</bodyText>
<table confidence="0.809281">
Suffix Bulg. Czech French Russian Spanish
ing 872 719 932 903 759
less 47 36 39 45 32
</table>
<tableCaption confidence="0.871534">
Table 5: Counts of two of the suffixes whose fre-
quency of use differs the most between sub-corpora.
</tableCaption>
<subsectionHeader confidence="0.994338">
4.8 Control Corpus
</subsectionHeader>
<bodyText confidence="0.999864625">
Finally, we have also ran the experiment on a differ-
ent corpus replacing the French and the Spanish sub-
corpora by the Dutch and Italian ones, introducing a
new Roman language and a new Germanic language
to the corpus. We obtained 64.66% accuracy, essen-
tially the same as in the original 5-language setting.
The corpus was compiled from works of advanced
English students of the same level who write essays
of approximately the same length, on a set of ran-
domly and roughly equally distributed topics. We
expected that these students will use roughly the
same n-grams distribution. However, the results de-
scribed above suggest that there exists some mecha-
nism that influences the authors’ choice of words. In
the next section we present a computational psycho-
linguistic framework that might explain our results.
</bodyText>
<sectionHeader confidence="0.872153" genericHeader="method">
5 Statistical Learning and Language
Transfer in SLA
</sectionHeader>
<subsectionHeader confidence="0.999591">
5.1 Statistical Learning by Infants
</subsectionHeader>
<bodyText confidence="0.999405052631579">
Psychologists, linguists, and cognitive science re-
searchers try to understand the process of language
learning by infants. Many models for language
learning and cognitive language modeling were sug-
gested (Clark, 2003).
Infants learn their first language by a combina-
tion of speech streams, vocal cues and body ges-
tures. Infants as young as 8 months old have a
limited grasp of their native tongue as they react
to familiar words. In that age they already under-
stand the meaning of single words, they learn to spot
these words in a speech stream, and very soon they
learn to combine different words into new sentential
units. Parental speech stream analysis shows that it
is impossible to separate between words by identi-
fying sequences of silence between words (Saffran,
2001). Recent studies of infant language learning
are in favor of the statistical framework (Saffran,
2001; Saffran et al, 1996). Saffran (2002) exam-
</bodyText>
<page confidence="0.969097">
14
</page>
<bodyText confidence="0.999060434782609">
ined 8 month-old to one year-old infants who were that these are language transfer effects related to L1
stimulated by speech sequences. The infants showed sounds.
a significant discrimination between word and non- We hypothesize that there are language transfer
word stimuli. In a different experimental setup in- effects related to L1 sounds and manifested by the
fants showed a significant discrimination between words that people choose to use when writing in a
frequent syllable n-grams and non frequent sylla- second language. (We say ‘writing’ because we have
ble n-grams, heard as part of a gibberish speech se- only experimented with written texts; a more gen-
quence generated by a computer according to var- eral hypothesis covering speaking and writing can
ious statistical language models. In a third experi- be formulated as well.)
mental setup infants showed a significant discrimi- Furthermore, since the acquisition and represen-
nation in favor of English-like gibberish speech se- tation of phonology is strongly influenced by statis-
quences upon non-English-like gibberish speech se- tical considerations (Section 5.1), we speculate that
quences. These findings along with the established the general language transfer phenomenon might be
finding (Jusczyk, 1997) that infants prefer the sound related to frequency. This does not directly follow
of their native tongue suggest that humans learn ba- from our findings, of course, but is an exciting direc-
sic language units in a statistical manner and that tion to investigate, and it is in accordance with the
they store some statistical parameters pertaining to growing body of work on the effects of frequency
these units. We should note that some researchers on language learning and the emergence of syntax
doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006).
5.2 Language Transfer in SLA We note that there is one obvious and well-known
The role of the first language in second language ac- lexical transfer effect: the usage of cognates (words
quisition is under a continuous debate (Ellis, 1999). that have similar form (sound) and meaning in two
Language Transfer between L1 and L2 is the pro- different languages). However, the languages we
cess in which a language learner of L2 whose na- used in our experiments contain radically differing
tive language is L1, is influenced by L1 when using amounts of cognates of English words (just consider
L2 (actually, when building his/her inter-language). French vs. Bulgarian, for example), while the clas-
This influence might appear helpful when L2 is rel- sification results were about the same for all 5 lan-
atively close to L1, but it interferes with the learn- guages. Hence, cognates might play a role, but they
ing process due to over- and under-generalization or do not constitute a single major explaining factor for
other problems. Although there is clear evidence our findings.
that language learners use constructs of their first We note that the hypothesis put forward in the
language when learning a foreign language (James, present paper is the first that attributes a language
1980; Odlin, 1989), it is not clear that the majority transfer phenomenon to a cognitive representation
of learner errors can be attributed to the L1 transfer (phonology) whose statistical nature has been seri-
(Ellis, 1999). ously substantiated.
5.3 Sound Transfer Hypothesis 6 Conclusion
For alphabetic scripts, character bi-grams reflect ba- In this paper we have demonstrated how modern ma-
sic sounds and sound sequences of the language3. chine learning can aid other fields, here the impor-
We have shown that native language strongly corre- tant field of Second Language Acquisition (SLA).
lates with character bi-grams when people write in Our analysis of the features useful for a multi-class
English as a second language. After ruling out usage SVM in the task of native language classification has
of function words, content bias, and morphology- resulted in the formulation of a hypothesis of poten-
related influences, the most plausible explanation is tial significance in the theory of language transfer
in SLA. We hypothesize language transfer effects at
the level of basic sounds and short sound sequences,
manifested by the words that people choose when
3Note that for English, they do not directly correspond to
phonemes or syllables. Nonetheless, they do reflect English
phonology to some extent.
15
writing in a second language. In other words, we
hypothesize that use of L2 words is strongly influ-
enced by L1 sounds and sound patterns.
As noted above, further experiments (psycholog-
ical and computational) must be conducted for vali-
dating our hypothesis. In particular, construction of
a wide-scale learners’ corpus with tight control over
content bias is essential for reaching stronger con-
clusions.
Additional future work should address sound se-
quences vs. the orthographic sequences that were
used in this work. If our hypothesis is correct, then
using spoken language corpora should produce even
stronger results, since (1) writing systems rarely
show a 1-1 correspondence with how words are at
the phonological level; and (2) writing allows more
conscious thinking that speaking, thus potentially re-
duces transfer effects. Our eventual goal is creating
a unified model of statistical transfer mechanisms.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879215189873">
Argamon S., Koppel M. and Shimoni A. 2003. Gender,
Genre, and Writing Style in Formal Written Texts. Text
23(3).
Bouselmi G., Fohr D., Illina, I., and Haton J.P.
2005. Fully Automated Non-Native Speech Recog-
nition Using Confusion-Based Acoustic Model. Eu-
rospeech/Interspeech ’05.
Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006.
Fully Automated Non-Native Speech Recognition Us-
ing Confusion-Based Acoustic Model Integration and
Graphemic Constraints. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
2006.
Bybee J. 2006. Frequency of Use and the Organization
ofLanguage. Oxford University Press.
Clark, E. 2003. First Language Acquisition. Cambridge
University Press.
Diederich J., Kindermann J., Leopold E. and Paass G.
2004. Authorship Attribution with Support Vector Ma-
chines. Applied Intelligence, 109–123.
Ellis N. 2002. Frequency Effects in Language Pro-
cessing. Studies in Second Language Acquisition,
24(2):143–188.
Ellis R. 1999. Understanding Second Language Acqui-
sition. Oxford University Press.
Granger S., Dagneaux E. and Meunier F. 2002. Inter-
national Corpus ofLearner English. Presses universi-
taires de Louvain.
Hansen J. H., Yapanel U., Huang, R. and Ikeno A. 2004.
Dialect Analysis and Modeling for Automatic Classi-
fication. Interspeech-2004/ICSLP-2004: International
Conference Spoken Language Processing. Jeju Island,
South Korea.
Holmes D. and Forsyth R. 1995. The Federalist Revis-
ited: New Directions in Authorship Attribution. Liter-
ary and Linguistic Computing, pp. 111–127.
James C. E. 1980. Contrastive Analysis. New York:
Longman.
Jusczyk P. W. 1997. The Discovery of Spoken Language.
MIT Press.
Koppel M. and Schler J. 2003. Exploiting Stylistic Id-
iosyncrasies for Authorship Attribution. In Proceed-
ings of IJCAI ’03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis. Acapulco,
Mexico.
Koppel M., Schler J. and Zigdon K. 2005(a). Determin-
ing an Author’s Native Language by Mining a Textfor
Errors. Proceedings of KDD ’05. Chicago IL.
Koppel M., Schler J. and Zigdon K. 2005(b). Auto-
matically Determining an Anonymous Author’s Native
Language. In Intelligence and Security Informatics
(pp. 209–217). Berlin / Heidelberg: Springer.
Odlin T. 1989. Language Transfer: Cross-Linguistic In-
fluence in Language Learning. Cambridge University
Press.
Porter F. M. 1980. An Algorithm for Suffix Stripping.
Program, 14(3):130–137.
Saffran J. R. 2001. Words in a Sea ofSounds: The Output
of Statistical Learning. Cognition, 81, 149–169.
Saffran J. R. 2002. Constraints on Statistical Language
Learning. Journal of Memory and Language, 47, 172–
196.
Saffran J. R., Aslin R. N. and Newport E. N. 1996. Sta-
tistical Learning by 8-month Old Infants. Science, is-
sue 5294, 1926–1928.
Salton G. and Buckley C. 1988. Term Weighing Ap-
proaches in Automatic Text Retrieval. Information
Processing and Management, 24(5):513–523.
Sch¨olkopf B,. Smola A 2002. Learning with Kernels.
MIT Press.
Stamatatos E,. Fakotakis N. and Kokkinakis G. 2004.
Computer-Based Authorship Attribution Without Lex-
ical Measures. Computers and the Humanities, 193–
214.
Witten I. H. and Frank E. 2005. Data Mining: Practical
Machine Learning Tools and Techniques. San Fran-
cisco: Morgan Kaufmann.
Yang C. 2004. Universal Grammar, Statistics, or Both?.
Trends in Cognitive Science 8(10):451–456, 2004.
</reference>
<page confidence="0.998683">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.218324">
<title confidence="0.9998375">Using Classifier Features for Studying the Effect of Native Language on Choice of Written Second Language Words</title>
<author confidence="0.991063">Oren</author>
<affiliation confidence="0.8384535">Institute of Computer The Hebrew</affiliation>
<address confidence="0.818541">Jerusalem,</address>
<email confidence="0.997785">oren@cs.huji.ac.il</email>
<author confidence="0.799866">Ari</author>
<affiliation confidence="0.828192">Institute of Computer The Hebrew</affiliation>
<address confidence="0.775298">Jerusalem,</address>
<abstract confidence="0.999246466666667">We apply machine learning techniques to study language transfer, a major topic in the theory of Second Language Acquisition (SLA). Using an SVM for the problem of native language classification, we show that a careful analysis of the effects of various features can lead to scientific insights. In particular, we demonstrate that character bigrams alone allow classification levels of about 66% for a 5-class task, even when content and function word differences are accounted for. This may show that native language has a strong effect on the word choice of people writing in a second language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>M Koppel</author>
<author>A Shimoni</author>
</authors>
<title>Gender, Genre, and Writing Style in Formal Written Texts.</title>
<date>2003</date>
<journal>Text</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="4025" citStr="Argamon et al, 2003" startWordPosition="624" endWordPosition="627">Association for Computational Linguistics guists and other literary experts trying to pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995). Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author. While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those wo</context>
</contexts>
<marker>Argamon, Koppel, Shimoni, 2003</marker>
<rawString>Argamon S., Koppel M. and Shimoni A. 2003. Gender, Genre, and Writing Style in Formal Written Texts. Text 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouselmi</author>
<author>D Fohr</author>
<author>I Illina</author>
<author>J P Haton</author>
</authors>
<title>Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model.</title>
<date>2005</date>
<journal>Eurospeech/Interspeech</journal>
<volume>05</volume>
<contexts>
<context position="4559" citStr="Bouselmi et al, 2005" startWordPosition="710" endWordPosition="713">entify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel et al (2005a) report an accuracy of 80% in the task of determining a writer’s native language. To the best of our knowledge, this is the only published work on automated classification of an author’s native language (along with another version of the paper by the same authors (Koppel et al, 2005b)). Koppel et al used an SVM (Sch¨olkopf and Smola, 2002) and a combination of features in their system (such as errors analysis and POS-error cooccurrences, as described in</context>
</contexts>
<marker>Bouselmi, Fohr, Illina, Haton, 2005</marker>
<rawString>Bouselmi G., Fohr D., Illina, I., and Haton J.P. 2005. Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model. Eurospeech/Interspeech ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouselmi</author>
<author>D Fohr</author>
<author>I Illina</author>
<author>J P Haton</author>
</authors>
<title>Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model Integration and Graphemic Constraints.</title>
<date>2006</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<contexts>
<context position="4581" citStr="Bouselmi et al, 2006" startWordPosition="714" endWordPosition="717">n anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel et al (2005a) report an accuracy of 80% in the task of determining a writer’s native language. To the best of our knowledge, this is the only published work on automated classification of an author’s native language (along with another version of the paper by the same authors (Koppel et al, 2005b)). Koppel et al used an SVM (Sch¨olkopf and Smola, 2002) and a combination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but sur</context>
</contexts>
<marker>Bouselmi, Fohr, Illina, Haton, 2006</marker>
<rawString>Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006. Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model Integration and Graphemic Constraints. IEEE International Conference on Acoustics, Speech and Signal Processing, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bybee</author>
</authors>
<title>Frequency of Use and the Organization ofLanguage.</title>
<date>2006</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="25077" citStr="Bybee, 2006" startWordPosition="4149" endWordPosition="4150">ransfer phenomenon might be finding (Jusczyk, 1997) that infants prefer the sound related to frequency. This does not directly follow of their native tongue suggest that humans learn ba- from our findings, of course, but is an exciting direcsic language units in a statistical manner and that tion to investigate, and it is in accordance with the they store some statistical parameters pertaining to growing body of work on the effects of frequency these units. We should note that some researchers on language learning and the emergence of syntax doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006). 5.2 Language Transfer in SLA We note that there is one obvious and well-known The role of the first language in second language ac- lexical transfer effect: the usage of cognates (words quisition is under a continuous debate (Ellis, 1999). that have similar form (sound) and meaning in two Language Transfer between L1 and L2 is the pro- different languages). However, the languages we cess in which a language learner of L2 whose na- used in our experiments contain radically differing tive language is L1, is influenced by L1 when using amounts of cognates of English words (just consider L2 (act</context>
</contexts>
<marker>Bybee, 2006</marker>
<rawString>Bybee J. 2006. Frequency of Use and the Organization ofLanguage. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Clark</author>
</authors>
<title>First Language Acquisition.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="22533" citStr="Clark, 2003" startWordPosition="3745" endWordPosition="3746">expected that these students will use roughly the same n-grams distribution. However, the results described above suggest that there exists some mechanism that influences the authors’ choice of words. In the next section we present a computational psycholinguistic framework that might explain our results. 5 Statistical Learning and Language Transfer in SLA 5.1 Statistical Learning by Infants Psychologists, linguists, and cognitive science researchers try to understand the process of language learning by infants. Many models for language learning and cognitive language modeling were suggested (Clark, 2003). Infants learn their first language by a combination of speech streams, vocal cues and body gestures. Infants as young as 8 months old have a limited grasp of their native tongue as they react to familiar words. In that age they already understand the meaning of single words, they learn to spot these words in a speech stream, and very soon they learn to combine different words into new sentential units. Parental speech stream analysis shows that it is impossible to separate between words by identifying sequences of silence between words (Saffran, 2001). Recent studies of infant language learn</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Clark, E. 2003. First Language Acquisition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Diederich</author>
<author>J Kindermann</author>
<author>E Leopold</author>
<author>G Paass</author>
</authors>
<title>Authorship Attribution with Support Vector Machines. Applied Intelligence,</title>
<date>2004</date>
<pages>109--123</pages>
<marker>Diederich, Kindermann, Leopold, Paass, 2004</marker>
<rawString>Diederich J., Kindermann J., Leopold E. and Paass G. 2004. Authorship Attribution with Support Vector Machines. Applied Intelligence, 109–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ellis</author>
</authors>
<title>Frequency Effects</title>
<date>2002</date>
<booktitle>in Language Processing. Studies in Second Language Acquisition,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="25063" citStr="Ellis, 2002" startWordPosition="4147" endWordPosition="4148">al language transfer phenomenon might be finding (Jusczyk, 1997) that infants prefer the sound related to frequency. This does not directly follow of their native tongue suggest that humans learn ba- from our findings, of course, but is an exciting direcsic language units in a statistical manner and that tion to investigate, and it is in accordance with the they store some statistical parameters pertaining to growing body of work on the effects of frequency these units. We should note that some researchers on language learning and the emergence of syntax doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006). 5.2 Language Transfer in SLA We note that there is one obvious and well-known The role of the first language in second language ac- lexical transfer effect: the usage of cognates (words quisition is under a continuous debate (Ellis, 1999). that have similar form (sound) and meaning in two Language Transfer between L1 and L2 is the pro- different languages). However, the languages we cess in which a language learner of L2 whose na- used in our experiments contain radically differing tive language is L1, is influenced by L1 when using amounts of cognates of English words (just co</context>
</contexts>
<marker>Ellis, 2002</marker>
<rawString>Ellis N. 2002. Frequency Effects in Language Processing. Studies in Second Language Acquisition, 24(2):143–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ellis</author>
</authors>
<title>Understanding Second Language Acquisition.</title>
<date>1999</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="25317" citStr="Ellis, 1999" startWordPosition="4189" endWordPosition="4190">csic language units in a statistical manner and that tion to investigate, and it is in accordance with the they store some statistical parameters pertaining to growing body of work on the effects of frequency these units. We should note that some researchers on language learning and the emergence of syntax doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006). 5.2 Language Transfer in SLA We note that there is one obvious and well-known The role of the first language in second language ac- lexical transfer effect: the usage of cognates (words quisition is under a continuous debate (Ellis, 1999). that have similar form (sound) and meaning in two Language Transfer between L1 and L2 is the pro- different languages). However, the languages we cess in which a language learner of L2 whose na- used in our experiments contain radically differing tive language is L1, is influenced by L1 when using amounts of cognates of English words (just consider L2 (actually, when building his/her inter-language). French vs. Bulgarian, for example), while the clasThis influence might appear helpful when L2 is rel- sification results were about the same for all 5 lanatively close to L1, but it interferes w</context>
<context position="26581" citStr="Ellis, 1999" startWordPosition="4393" endWordPosition="4395">role, but they ing process due to over- and under-generalization or do not constitute a single major explaining factor for other problems. Although there is clear evidence our findings. that language learners use constructs of their first We note that the hypothesis put forward in the language when learning a foreign language (James, present paper is the first that attributes a language 1980; Odlin, 1989), it is not clear that the majority transfer phenomenon to a cognitive representation of learner errors can be attributed to the L1 transfer (phonology) whose statistical nature has been seri(Ellis, 1999). ously substantiated. 5.3 Sound Transfer Hypothesis 6 Conclusion For alphabetic scripts, character bi-grams reflect ba- In this paper we have demonstrated how modern masic sounds and sound sequences of the language3. chine learning can aid other fields, here the imporWe have shown that native language strongly corre- tant field of Second Language Acquisition (SLA). lates with character bi-grams when people write in Our analysis of the features useful for a multi-class English as a second language. After ruling out usage SVM in the task of native language classification has of function words, </context>
</contexts>
<marker>Ellis, 1999</marker>
<rawString>Ellis R. 1999. Understanding Second Language Acquisition. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<booktitle>International Corpus ofLearner English. Presses universitaires de Louvain.</booktitle>
<contexts>
<context position="5621" citStr="Granger et al, 2002" startWordPosition="888" endWordPosition="891">l used an SVM (Sch¨olkopf and Smola, 2002) and a combination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but surprisingly, it appears that a very naive set of features achieves a relatively high accuracy. The character bi-gram frequencies feature performs rather well, and definitely outperforms the intuitive contribution of frequent bigrams in this type of task. 3 Experimental Setting 3.1 The Corpus The corpus that served for all of the experiments described in this paper is the International Corpus of Learner English (ICLE) (Granger et al, 2002), which was also the one used by Koppel et al (2005a; 2005b). The corpus was compiled for the purpose of studying the English writing of non-native speakers. All contributors to the corpus are advanced English students and are roughly the same age. The corpus is combined from a number of sub-corpora, each containing one native language. The corpus was assembled in ten years of international collaboration between a number of universities and it contains more than 2 million words of writing by students from 19 different native language backgrounds. We followed Koppel et al (2005a) and worked on </context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>Granger S., Dagneaux E. and Meunier F. 2002. International Corpus ofLearner English. Presses universitaires de Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Hansen</author>
<author>U Yapanel</author>
<author>R Huang</author>
<author>A Ikeno</author>
</authors>
<title>Dialect Analysis and Modeling for Automatic Classification.</title>
<date>2004</date>
<booktitle>Interspeech-2004/ICSLP-2004: International Conference Spoken Language Processing. Jeju Island,</booktitle>
<location>South</location>
<contexts>
<context position="4602" citStr="Hansen et al, 2004" startWordPosition="718" endWordPosition="721">h accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel et al (2005a) report an accuracy of 80% in the task of determining a writer’s native language. To the best of our knowledge, this is the only published work on automated classification of an author’s native language (along with another version of the paper by the same authors (Koppel et al, 2005b)). Koppel et al used an SVM (Sch¨olkopf and Smola, 2002) and a combination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but surprisingly, it appears</context>
</contexts>
<marker>Hansen, Yapanel, Huang, Ikeno, 2004</marker>
<rawString>Hansen J. H., Yapanel U., Huang, R. and Ikeno A. 2004. Dialect Analysis and Modeling for Automatic Classification. Interspeech-2004/ICSLP-2004: International Conference Spoken Language Processing. Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Holmes</author>
<author>R Forsyth</author>
</authors>
<title>The Federalist Revisited: New Directions</title>
<date>1995</date>
<booktitle>in Authorship Attribution. Literary and Linguistic Computing,</booktitle>
<pages>111--127</pages>
<contexts>
<context position="3585" citStr="Holmes and Forsyth, 1995" startWordPosition="558" endWordPosition="561">uage. The problem is applicable to different fields, such as language instructing, tailored error correction, security applications and psycholinguistic research. As background, we start from the somewhat related problem of authorship attribution. The authorship attribution problem was addressed by linProceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9–16, Qc Prague, Czech Republic, June 2007 2007 Association for Computational Linguistics guists and other literary experts trying to pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995). Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author. While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorshi</context>
</contexts>
<marker>Holmes, Forsyth, 1995</marker>
<rawString>Holmes D. and Forsyth R. 1995. The Federalist Revisited: New Directions in Authorship Attribution. Literary and Linguistic Computing, pp. 111–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E James</author>
</authors>
<title>Contrastive Analysis.</title>
<date>1980</date>
<publisher>Longman.</publisher>
<location>New York:</location>
<marker>James, 1980</marker>
<rawString>James C. E. 1980. Contrastive Analysis. New York: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
</authors>
<title>The Discovery of Spoken Language.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24516" citStr="Jusczyk, 1997" startWordPosition="4057" endWordPosition="4058">e genquence generated by a computer according to var- eral hypothesis covering speaking and writing can ious statistical language models. In a third experi- be formulated as well.) mental setup infants showed a significant discrimi- Furthermore, since the acquisition and represennation in favor of English-like gibberish speech se- tation of phonology is strongly influenced by statisquences upon non-English-like gibberish speech se- tical considerations (Section 5.1), we speculate that quences. These findings along with the established the general language transfer phenomenon might be finding (Jusczyk, 1997) that infants prefer the sound related to frequency. This does not directly follow of their native tongue suggest that humans learn ba- from our findings, of course, but is an exciting direcsic language units in a statistical manner and that tion to investigate, and it is in accordance with the they store some statistical parameters pertaining to growing body of work on the effects of frequency these units. We should note that some researchers on language learning and the emergence of syntax doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006). 5.2 Language Transfer in SLA We note </context>
</contexts>
<marker>Jusczyk, 1997</marker>
<rawString>Jusczyk P. W. 1997. The Discovery of Spoken Language. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>J Schler</author>
</authors>
<title>Exploiting Stylistic Idiosyncrasies for Authorship Attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI ’03 Workshop on Computational Approaches to Style Analysis and Synthesis.</booktitle>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="4088" citStr="Koppel and Schler, 2003" startWordPosition="634" endWordPosition="637">iterary experts trying to pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995). Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author. While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel</context>
</contexts>
<marker>Koppel, Schler, 2003</marker>
<rawString>Koppel M. and Schler J. 2003. Exploiting Stylistic Idiosyncrasies for Authorship Attribution. In Proceedings of IJCAI ’03 Workshop on Computational Approaches to Style Analysis and Synthesis. Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>J Schler</author>
<author>K Zigdon</author>
</authors>
<title>Determining an Author’s Native Language by Mining a Textfor Errors.</title>
<date>2005</date>
<booktitle>Proceedings of KDD ’05. Chicago IL.</booktitle>
<contexts>
<context position="4108" citStr="Koppel et al, 2005" startWordPosition="638" endWordPosition="641"> pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995). Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author. While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel et al (2005a) repor</context>
<context position="5672" citStr="Koppel et al (2005" startWordPosition="899" endWordPosition="902">ination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but surprisingly, it appears that a very naive set of features achieves a relatively high accuracy. The character bi-gram frequencies feature performs rather well, and definitely outperforms the intuitive contribution of frequent bigrams in this type of task. 3 Experimental Setting 3.1 The Corpus The corpus that served for all of the experiments described in this paper is the International Corpus of Learner English (ICLE) (Granger et al, 2002), which was also the one used by Koppel et al (2005a; 2005b). The corpus was compiled for the purpose of studying the English writing of non-native speakers. All contributors to the corpus are advanced English students and are roughly the same age. The corpus is combined from a number of sub-corpora, each containing one native language. The corpus was assembled in ten years of international collaboration between a number of universities and it contains more than 2 million words of writing by students from 19 different native language backgrounds. We followed Koppel et al (2005a) and worked on 5 sub-corpora, each containing 238 randomly selecte</context>
<context position="7196" citStr="Koppel et al (2005" startWordPosition="1153" endWordPosition="1156">. Essays in the corpus are of two types: argumentative essays and literature examination papers. Descriptive, narrative or technical subjects were not included in the corpus. The literature examination essays were restricted to no more than 25% of each sub-corpus. Each contributor was requested to fill a learner profile that was used to fine-proof the corpus as needed. In order to verify our results we used another control corpus containing the Dutch and Italian subcorpora contained in the ICLE instead of the Bulgarian and French ones. 3.2 Document Representation In the original experiment by Koppel et al (2005a) each document was represented by a numerical vector of 1,035 dimensions. Each vector entry represented the frequency (relative to the document’s length) of a given feature. The features were of 4 types: • 400 function words • 200 most frequent letter n-grams • 250 rare POS bi-gram • 185 error types While the first three types of attributes are relatively straightforward, the fourth is more complex. It represents clusters of families of spelling errors as well as co-occurrences of errors and POS tags. Document 10 representation is described in detail in (Koppel et al, 2005a; Koppel et al, 20</context>
<context position="14508" citStr="Koppel et al (2005" startWordPosition="2358" endWordPosition="2361">an be classified by only using naive n-gram models, while the optimal n of the n-gram is a different question that might be addressed in a different work (and might be language-dependent). 4.4 Function Words Based Classification Function words are words that have a little lexical meaning but instead serve to express grammatical relations within a sentence or specify the attitude of the speaker (function words should not be confused with stopwords, although the lists of most frequent function words and the stopword list share a large subset). We used the same list of 460 function words used by Koppel et al (2005a). A partial list includes: {a, afterward, although, because, cannot, do, enter, eventually, fifteenth, hither, hath, hence, lastly, occasionally, presumable, said, seldom, undoubtedly, was}. In this variation of the experiment, we represented each document only by the relative frequencies of the function words it contained. Using the same experimental setup as before, we achieved an accuracy of 66.7%. These results are less surprising than the results obtained by the character n-grams vectors, since we do expect native speakers of a certain language to use, misuse or ignore certain function </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Koppel M., Schler J. and Zigdon K. 2005(a). Determining an Author’s Native Language by Mining a Textfor Errors. Proceedings of KDD ’05. Chicago IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Koppel</author>
<author>J Schler</author>
<author>K Zigdon</author>
</authors>
<title>Automatically Determining an Anonymous Author’s Native Language.</title>
<date>2005</date>
<booktitle>In Intelligence and Security Informatics</booktitle>
<pages>209--217</pages>
<publisher>Springer.</publisher>
<location>Berlin / Heidelberg:</location>
<contexts>
<context position="4108" citStr="Koppel et al, 2005" startWordPosition="638" endWordPosition="641"> pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995). Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author. While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003). Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates. Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person. There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004). However, all those works are based on acoustic signals, not on written texts. Koppel et al (2005a) repor</context>
<context position="5672" citStr="Koppel et al (2005" startWordPosition="899" endWordPosition="902">ination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but surprisingly, it appears that a very naive set of features achieves a relatively high accuracy. The character bi-gram frequencies feature performs rather well, and definitely outperforms the intuitive contribution of frequent bigrams in this type of task. 3 Experimental Setting 3.1 The Corpus The corpus that served for all of the experiments described in this paper is the International Corpus of Learner English (ICLE) (Granger et al, 2002), which was also the one used by Koppel et al (2005a; 2005b). The corpus was compiled for the purpose of studying the English writing of non-native speakers. All contributors to the corpus are advanced English students and are roughly the same age. The corpus is combined from a number of sub-corpora, each containing one native language. The corpus was assembled in ten years of international collaboration between a number of universities and it contains more than 2 million words of writing by students from 19 different native language backgrounds. We followed Koppel et al (2005a) and worked on 5 sub-corpora, each containing 238 randomly selecte</context>
<context position="7196" citStr="Koppel et al (2005" startWordPosition="1153" endWordPosition="1156">. Essays in the corpus are of two types: argumentative essays and literature examination papers. Descriptive, narrative or technical subjects were not included in the corpus. The literature examination essays were restricted to no more than 25% of each sub-corpus. Each contributor was requested to fill a learner profile that was used to fine-proof the corpus as needed. In order to verify our results we used another control corpus containing the Dutch and Italian subcorpora contained in the ICLE instead of the Bulgarian and French ones. 3.2 Document Representation In the original experiment by Koppel et al (2005a) each document was represented by a numerical vector of 1,035 dimensions. Each vector entry represented the frequency (relative to the document’s length) of a given feature. The features were of 4 types: • 400 function words • 200 most frequent letter n-grams • 250 rare POS bi-gram • 185 error types While the first three types of attributes are relatively straightforward, the fourth is more complex. It represents clusters of families of spelling errors as well as co-occurrences of errors and POS tags. Document 10 representation is described in detail in (Koppel et al, 2005a; Koppel et al, 20</context>
<context position="14508" citStr="Koppel et al (2005" startWordPosition="2358" endWordPosition="2361">an be classified by only using naive n-gram models, while the optimal n of the n-gram is a different question that might be addressed in a different work (and might be language-dependent). 4.4 Function Words Based Classification Function words are words that have a little lexical meaning but instead serve to express grammatical relations within a sentence or specify the attitude of the speaker (function words should not be confused with stopwords, although the lists of most frequent function words and the stopword list share a large subset). We used the same list of 460 function words used by Koppel et al (2005a). A partial list includes: {a, afterward, although, because, cannot, do, enter, eventually, fifteenth, hither, hath, hence, lastly, occasionally, presumable, said, seldom, undoubtedly, was}. In this variation of the experiment, we represented each document only by the relative frequencies of the function words it contained. Using the same experimental setup as before, we achieved an accuracy of 66.7%. These results are less surprising than the results obtained by the character n-grams vectors, since we do expect native speakers of a certain language to use, misuse or ignore certain function </context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Koppel M., Schler J. and Zigdon K. 2005(b). Automatically Determining an Anonymous Author’s Native Language. In Intelligence and Security Informatics (pp. 209–217). Berlin / Heidelberg: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Odlin</author>
</authors>
<title>Language Transfer: Cross-Linguistic Influence in Language Learning.</title>
<date>1989</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="15173" citStr="Odlin, 1989" startWordPosition="2463" endWordPosition="2464"> because, cannot, do, enter, eventually, fifteenth, hither, hath, hence, lastly, occasionally, presumable, said, seldom, undoubtedly, was}. In this variation of the experiment, we represented each document only by the relative frequencies of the function words it contained. Using the same experimental setup as before, we achieved an accuracy of 66.7%. These results are less surprising than the results obtained by the character n-grams vectors, since we do expect native speakers of a certain language to use, misuse or ignore certain function words as a result from language transfer mechanisms (Odlin, 1989). For example, it is well known that native speakers of Russian tend to omit English articles. 4.5 Function Words Bias The previous results suggest that the n-gram based classification is simply the result of the different uses of function words by speakers of different native languages. In order to rule out the effect of the function words on the bi-gram-based classification, we removed all function words from the corpus, recalculated the bi-gram frequencies and ran the experiment once again, this time achieving an accuracy of 62.92% in the 10-fold cross validation test. 12 These results, obt</context>
<context position="26377" citStr="Odlin, 1989" startWordPosition="4362" endWordPosition="4363">the clasThis influence might appear helpful when L2 is rel- sification results were about the same for all 5 lanatively close to L1, but it interferes with the learn- guages. Hence, cognates might play a role, but they ing process due to over- and under-generalization or do not constitute a single major explaining factor for other problems. Although there is clear evidence our findings. that language learners use constructs of their first We note that the hypothesis put forward in the language when learning a foreign language (James, present paper is the first that attributes a language 1980; Odlin, 1989), it is not clear that the majority transfer phenomenon to a cognitive representation of learner errors can be attributed to the L1 transfer (phonology) whose statistical nature has been seri(Ellis, 1999). ously substantiated. 5.3 Sound Transfer Hypothesis 6 Conclusion For alphabetic scripts, character bi-grams reflect ba- In this paper we have demonstrated how modern masic sounds and sound sequences of the language3. chine learning can aid other fields, here the imporWe have shown that native language strongly corre- tant field of Second Language Acquisition (SLA). lates with character bi-gra</context>
</contexts>
<marker>Odlin, 1989</marker>
<rawString>Odlin T. 1989. Language Transfer: Cross-Linguistic Influence in Language Learning. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="18714" citStr="Porter, 1980" startWordPosition="3080" endWordPosition="3081"> function unique words words stems Bulgarian 1543 94685 11325 Czech 2784 110782 12834 French 2059 67016 9474 Russian 2730 112410 12338 Spanish 2985 108052 12627 Total 12101 492945 36474 Table 3: Numbers of dominant content words (with a threshold of 0.0025) and function words that were removed from each sub-corpus. The unique stems column indicates the number of unique stems (types) that remained after removal of c-w and f-w. only 2% after removing 51 content words (by using a threshold of 0.0015). We calculated the tf-idf weights after stop-words removal and stemming (using a Porter stemmer (Porter, 1980)), trying to pinpoint dominant stems. The results were similar to the word’s tf-idf and no significantly dominant stem was found in either of the sub-corpora. A drop of only 3% in accuracy was noticed after removing both dominant content words and function words. These results show that if a content bias exists in the corpus it has only a minor effect on the SVM classification, and that the n-grams based clasFigure 2: Classification accuracy as a function of the threshold (removed content words). 13 Thresh. 0.004 0.003 0.0025 0.0015 0.001 2 c-w 9 c-w 15 c-w 51 c-w 113 c-w Bulg. 77 908 1543 395</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter F. M. 1980. An Algorithm for Suffix Stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
</authors>
<title>Words in a Sea ofSounds:</title>
<date>2001</date>
<journal>The Output of Statistical Learning. Cognition,</journal>
<volume>81</volume>
<pages>149--169</pages>
<contexts>
<context position="23092" citStr="Saffran, 2001" startWordPosition="3842" endWordPosition="3843">ognitive language modeling were suggested (Clark, 2003). Infants learn their first language by a combination of speech streams, vocal cues and body gestures. Infants as young as 8 months old have a limited grasp of their native tongue as they react to familiar words. In that age they already understand the meaning of single words, they learn to spot these words in a speech stream, and very soon they learn to combine different words into new sentential units. Parental speech stream analysis shows that it is impossible to separate between words by identifying sequences of silence between words (Saffran, 2001). Recent studies of infant language learning are in favor of the statistical framework (Saffran, 2001; Saffran et al, 1996). Saffran (2002) exam14 ined 8 month-old to one year-old infants who were that these are language transfer effects related to L1 stimulated by speech sequences. The infants showed sounds. a significant discrimination between word and non- We hypothesize that there are language transfer word stimuli. In a different experimental setup in- effects related to L1 sounds and manifested by the fants showed a significant discrimination between words that people choose to use when </context>
</contexts>
<marker>Saffran, 2001</marker>
<rawString>Saffran J. R. 2001. Words in a Sea ofSounds: The Output of Statistical Learning. Cognition, 81, 149–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
</authors>
<title>Constraints on Statistical Language Learning.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<volume>47</volume>
<pages>172--196</pages>
<contexts>
<context position="23231" citStr="Saffran (2002)" startWordPosition="3863" endWordPosition="3864">and body gestures. Infants as young as 8 months old have a limited grasp of their native tongue as they react to familiar words. In that age they already understand the meaning of single words, they learn to spot these words in a speech stream, and very soon they learn to combine different words into new sentential units. Parental speech stream analysis shows that it is impossible to separate between words by identifying sequences of silence between words (Saffran, 2001). Recent studies of infant language learning are in favor of the statistical framework (Saffran, 2001; Saffran et al, 1996). Saffran (2002) exam14 ined 8 month-old to one year-old infants who were that these are language transfer effects related to L1 stimulated by speech sequences. The infants showed sounds. a significant discrimination between word and non- We hypothesize that there are language transfer word stimuli. In a different experimental setup in- effects related to L1 sounds and manifested by the fants showed a significant discrimination between words that people choose to use when writing in a frequent syllable n-grams and non frequent sylla- second language. (We say ‘writing’ because we have ble n-grams, heard as par</context>
</contexts>
<marker>Saffran, 2002</marker>
<rawString>Saffran J. R. 2002. Constraints on Statistical Language Learning. Journal of Memory and Language, 47, 172– 196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E N Newport</author>
</authors>
<date>1996</date>
<journal>Statistical Learning by 8-month Old Infants. Science, issue</journal>
<volume>5294</volume>
<pages>1926--1928</pages>
<contexts>
<context position="23215" citStr="Saffran et al, 1996" startWordPosition="3859" endWordPosition="3862">h streams, vocal cues and body gestures. Infants as young as 8 months old have a limited grasp of their native tongue as they react to familiar words. In that age they already understand the meaning of single words, they learn to spot these words in a speech stream, and very soon they learn to combine different words into new sentential units. Parental speech stream analysis shows that it is impossible to separate between words by identifying sequences of silence between words (Saffran, 2001). Recent studies of infant language learning are in favor of the statistical framework (Saffran, 2001; Saffran et al, 1996). Saffran (2002) exam14 ined 8 month-old to one year-old infants who were that these are language transfer effects related to L1 stimulated by speech sequences. The infants showed sounds. a significant discrimination between word and non- We hypothesize that there are language transfer word stimuli. In a different experimental setup in- effects related to L1 sounds and manifested by the fants showed a significant discrimination between words that people choose to use when writing in a frequent syllable n-grams and non frequent sylla- second language. (We say ‘writing’ because we have ble n-gra</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Saffran J. R., Aslin R. N. and Newport E. N. 1996. Statistical Learning by 8-month Old Infants. Science, issue 5294, 1926–1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term Weighing Approaches</title>
<date>1988</date>
<booktitle>in Automatic Text Retrieval. Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="16519" citStr="Salton and Buckley, 1988" startWordPosition="2679" endWordPosition="2682"> use of function words. 4.6 Content Bias Bi-gram frequencies could also reflect content bias rather than language use. By content bias we mean that the subject matter of the documents in the different sub-corpora could exhibit internal sub-corpus uniformity and external sub-corpus disparity. In order to rule this out, we employed a variation on the Term Frequency – Inverted Document Frequency (tf-idf) content analysis metric. The tf-idf measure is a statistical measure that is used in information retrieval tasks to evaluate how important a word/term is to a document in a collection or corpus (Salton and Buckley, 1988). Given a collection of documents D, the tf-idf weight of term t in a document d E D is computed as follows: tfidft = ft,d x log |D| ft,D where ft,d is the frequency of term t in document d, and ft,D is the number of documents in which t appears. Therefore, the weight of term t E d is maximal if it is a common term in d while the number of documents it appears in is relatively low. We used the tf-idf weights in the information retrieval sense in order to discover the dominant content words of each sub-corpus. We treated each subcorpus (set of documents by writers who share a native language) a</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton G. and Buckley C. 1988. Term Weighing Approaches in Automatic Text Retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
</authors>
<title>Smola A</title>
<date>2002</date>
<publisher>MIT Press.</publisher>
<marker>Sch¨olkopf, 2002</marker>
<rawString>Sch¨olkopf B,. Smola A 2002. Learning with Kernels. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>Computer-Based Authorship Attribution Without Lexical Measures. Computers and the Humanities,</title>
<date>2004</date>
<volume>193</volume>
<pages>214</pages>
<marker>Fakotakis, Kokkinakis, 2004</marker>
<rawString>Stamatatos E,. Fakotakis N. and Kokkinakis G. 2004. Computer-Based Authorship Attribution Without Lexical Measures. Computers and the Humanities, 193– 214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<contexts>
<context position="7844" citStr="Witten and Frank, 2005" startWordPosition="1262" endWordPosition="1265">esented by a numerical vector of 1,035 dimensions. Each vector entry represented the frequency (relative to the document’s length) of a given feature. The features were of 4 types: • 400 function words • 200 most frequent letter n-grams • 250 rare POS bi-gram • 185 error types While the first three types of attributes are relatively straightforward, the fourth is more complex. It represents clusters of families of spelling errors as well as co-occurrences of errors and POS tags. Document 10 representation is described in detail in (Koppel et al, 2005a; Koppel et al, 2005b). A multi-class SVM (Witten and Frank, 2005) was employed for learning and evaluating the classification model. The experiment was run in a 10-fold cross validation manner in order to test the effectiveness of the model. 3.3 Previous Results Koppel et al (2005a) report that when all features types were used in tandem, an accuracy of 80.2% was achieved. In the discussion section they analyze the frequency of a few function words, error types, the co-occurrences of POS tags and errors, and the co-occurrences of POS tags and certain function words that seem to have significance in the support vectors learnt by the SVM. The goal of their re</context>
<context position="10525" citStr="Witten and Frank, 2005" startWordPosition="1690" endWordPosition="1694">t variations of document representation. b-g: bigrams, f-w: function words, c-w: content words. explain phenomena such as shallow language transfer through the use of function words, or content bias. The following sub-sections describe different variations of the experiment, ruling out the effect of these different types of bias. 4.1 Unigram Baseline We first implemented a naive baseline classifier. We represented each document by the normalized frequencies of the (de-capitalized) letters it contains2. These frequencies are simply a unigram model of the sub-corpora. Using the multi-class SVM (Witten and Frank, 2005) we obtained 46.78% accuracy. This accuracy is more than twice the random baseline accuracy. This result is in accordance with our bi-grams results. Our discussion focuses on bi-grams rather than unigrams because the former’s results are much higher and because bi-grams are much closer to the phonology of the language (for alphabetic scripts, of course). 4.2 Bi-grams Based Classification Choosing the 200 most frequent character bi-grams in the corpus, we used a vector of the same dimension. Each vector entry contained the normalized frequency of one of the bi-grams. Using a multiclass SVM in a</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten I. H. and Frank E. 2005. Data Mining: Practical Machine Learning Tools and Techniques. San Francisco: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>Universal Grammar, Statistics, or Both?.</title>
<date>2004</date>
<journal>Trends in Cognitive Science</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="25049" citStr="Yang, 2004" startWordPosition="4145" endWordPosition="4146">shed the general language transfer phenomenon might be finding (Jusczyk, 1997) that infants prefer the sound related to frequency. This does not directly follow of their native tongue suggest that humans learn ba- from our findings, of course, but is an exciting direcsic language units in a statistical manner and that tion to investigate, and it is in accordance with the they store some statistical parameters pertaining to growing body of work on the effects of frequency these units. We should note that some researchers on language learning and the emergence of syntax doubt these conclusions (Yang, 2004). (Ellis, 2002; Bybee, 2006). 5.2 Language Transfer in SLA We note that there is one obvious and well-known The role of the first language in second language ac- lexical transfer effect: the usage of cognates (words quisition is under a continuous debate (Ellis, 1999). that have similar form (sound) and meaning in two Language Transfer between L1 and L2 is the pro- different languages). However, the languages we cess in which a language learner of L2 whose na- used in our experiments contain radically differing tive language is L1, is influenced by L1 when using amounts of cognates of English </context>
</contexts>
<marker>Yang, 2004</marker>
<rawString>Yang C. 2004. Universal Grammar, Statistics, or Both?. Trends in Cognitive Science 8(10):451–456, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>