<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000911">
<title confidence="0.941902">
INESC-ID: A Regression Model for Large Scale Twitter Sentiment Lexicon
Induction
</title>
<author confidence="0.987884">
Silvio Amir, Ramon F. Astudillo, Wang Ling, Bruno Martins†, M´ario Silva, Isabel Trancoso
</author>
<affiliation confidence="0.963443">
Instituto de Engenharia de Sistemas e Computadores Investigac¸˜ao e Desenvolvimento
</affiliation>
<address confidence="0.910786333333333">
Rua Alves Redol 9
Lisbon, Portugal
{samir, ramon.astudillo, wlin, mjs, isabel.trancoso}@inesc-id.pt
</address>
<email confidence="0.987465">
†bruno.g.martins@tecnico.ulisboa.pt
</email>
<sectionHeader confidence="0.995323" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999584153846154">
We present the approach followed by INESC-
ID in the SemEval 2015 Twitter Sentiment
Analysis challenge, subtask E. The goal was
to determine the strength of the association of
Twitter terms with positive sentiment. Using
two labeled lexicons, we trained a regression
model to predict the sentiment polarity and in-
tensity of words and phrases. Terms were rep-
resented as word embeddings induced in an
unsupervised fashion from a corpus of tweets.
Our system attained the top ranking submis-
sion, attesting the general adequacy of the pro-
posed approach.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948615384616">
Sentiment lexicons are one of the key resources for
the automatic analysis of opinions, emotive and sub-
jective text (Liu, 2012). They compile words an-
notated with their prior polarity of sentiment, re-
gardless of the context. For instance, words such as
beautiful or amazing tend to express a positive sen-
timent, whereas words like boring or ugly are con-
sidered negative. Most sentiment analysis systems
use either word count methods, based on sentiment
lexicons, or rely on text classifiers. In the former,
the polarity of a message is estimated by computing
the ratio of (positive and negative) sentiment bear-
ing words. Despite its simplicity, this method has
been widely used (O’Connor et al., 2010; Bollen and
Mao, 2011; Mitchell et al., 2013). Even more so-
phisticated systems, based on supervised classifica-
tion, can be greatly improved with features derived
from lexicons (Kiritchenko et al., 2014). However,
manually created sentiment lexicons consist of few
carefully selected words. Consequently, they fail to
capture the use of non-conventional word spelling
and slang, commonly found in social media.
This problem motivated the creation of a task in
the SemEval 2015 Twitter Sentiment Analysis chal-
lenge. This task (subtask E), intended to evaluate au-
tomatic methods of generating Twitter specific sen-
timent lexicons. Given a set of words or phrases,
the goal was to assign a score between 0 and 1, re-
flecting the intensity and polarity of sentiment these
terms express. Participants were asked to submit a
list, with the candidate terms ranked according to
sentiment score. This list was then compared to a
ranked list obtained from human annotations and
the submissions were evaluated using the Kendall
(1938) Tau rank correlation metric.
In this paper, we describe a system developed for
this challenge, based on a novel method to create
large scale, domain-specific sentiment lexicons. The
task is addressed as a regression problem, in which
terms are represented as word embeddings, induced
from a corpus of 52 million tweets. Then, using
manually annotated lexicons, a regression model
was trained to predict the polarity and intensity of
sentiment of any word or phrase from that corpus.
We found this approach to be effective for the pro-
posed problem.
The rest of the paper proceeds as follows: we re-
view the work related to lexicon expansion in Sec-
tion 2 and describe the methods used to derive word
embeddings in Section 3. Our approach and the ex-
perimental results are presented in Sections 5 and 6,
respectively. We conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999969875">
Most of the literature on automatic lexicon expan-
sion consists of dictionary-based or corpora-based
approaches. In the former, the main idea is to use a
dictionary, such as WordNet, to extract semantic re-
lations between words. Kim and Hovy (2006) sim-
ply assign the same polarity to synonyms and the op-
posite polarity to antonyms, of known words. Oth-
ers, create a graph from the semantic relationships,
to find new sentiment words and their polarity. Us-
ing the seed words, new terms are classified using a
distance measure (Kamps et al., 2004), or propagat-
ing the labels along the edges of the graph (Rao and
Ravichandran, 2009). However, given that dictio-
naries mostly describe conventional language, these
methods are unsuited for social media.
Corpora based approaches follow the assumption
that the polarity of new words can be inferred from
co-occurrence patterns with known words. Hatzi-
vassiloglou and McKeown (1997) discovered new
polar adjectives by looking at conjunctions found in
a corpus. The adjectives connected with and got the
same polarity, whereas adjectives connected with
but were assigned opposing polarities. Turney and
Littman (2003) created two small sets of prototypi-
cal polar words, one containing positive and another
containing negative examples. The polarity of a new
term was computed using the point-wise mutual in-
formation between that word and each of the proto-
typical sets (Lin, 1998). The same method was used
by Kiritchenko et al. (2014), to create large scale
sentiment lexicons for Twitter.
A recently proposed alternative is to learn word
embeddings specific for Twitter sentiment analysis,
using distant supervision (Tang et al., 2014). The
resulting features are then used in a supervised clas-
sifier to predict the polarity of phrases. This work is
the most related to our approach, but it differs in the
sense that we use general word embeddings, learned
from unlabeled data, and predict both polarity and
intensity of sentiment.
</bodyText>
<sectionHeader confidence="0.989113" genericHeader="method">
3 Unsupervised Word Embeddings
</sectionHeader>
<bodyText confidence="0.999966333333333">
In recent years, several models have been pro-
posed, to derive word embeddings from large cor-
pora. These are essentially, dense vector repre-
sentations that implicitly capture syntactic and se-
mantic properties of words (Collobert et al., 2011;
Mikolov et al., 2013a; Pennington et al., 2014).
Moreover, a notion of semantic similarity, as well
as other linguistic regularities seem to be encoded
in the embedding space (Mikolov et al., 2013b). In
word2vec, Mikolov et al. (2013a) induce word
vectors with two simple neural network architec-
tures, CBOW and skip-gram. These models esti-
mate the optimal word embeddings by maximizing
the probability that, words within a given window
size are predicted correctly.
</bodyText>
<subsectionHeader confidence="0.976418">
Skip-gram and Structured Skip-gram
</subsectionHeader>
<bodyText confidence="0.99991825">
Central to the skip-gram is a log-linear model of
word prediction. Given the i-th word from a sen-
tence wi, the skip-gram estimates the probability of
each word at a distance p from wi as:
</bodyText>
<equation confidence="0.926169">
p(wi+pJwi; Cp, E) a exp (Cp · E · wi) (1)
</equation>
<bodyText confidence="0.996502476190476">
Here, wi E {1, 0}v×1 is a one-hot representa-
tion of the word, i.e., a sparse column vector of
the size of the vocabulary v with a 1 on the po-
sition corresponding to that word. The model is
parametrized by two matrices: E E Re×v is the
embedding matrix, transforming the one-hot sparse
representation into a compact real valued space of
size e; Cp E Rv×e is a matrix mapping the real-
valued representation to a vector with the size of
the vocabulary v. A distribution over all possible
words is then attained by exponentiating and nor-
malizing over the v possible options. In practice, due
to the large value of v, various techniques are used
to avoid having to normalize over the whole vocab-
ulary (Mikolov et al., 2013a). In the particular case
of the structured skip-gram model, the matrix Cp
depends only of the relative position between words
p (Ling et al., 2015).
After training, the low dimensional embedding E·
wi E Re×1 encapsulates the information about each
word and its surrounding contexts.
</bodyText>
<sectionHeader confidence="0.826394" genericHeader="method">
CBOW
</sectionHeader>
<bodyText confidence="0.9999255">
The CBOW model defines a different objective
function, that predicts a word at position i given the
window of context i − d, where d is the size of the
context window. The probability of the word wi is
</bodyText>
<figure confidence="0.893546">
(a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings
</figure>
<figureCaption confidence="0.999504">
Figure 1: Performance of the different embeddings and phrase representations, as function of vector size.
</figureCaption>
<equation confidence="0.695835">
defined as:
p(wi*i−d, ..., wi+di C, E) a exp(C · Si+d
i−d) (2)
</equation>
<bodyText confidence="0.9825890625">
where Si+d i−d is the point wise sum of the embed-
dings of all context words starting at E · wi−d to
E · wi+d, excluding the index wi, and once again
C E Re×v is a matrix mapping the embedding space
into the output vocabulary space v.
GloVe
The models discussed above rely on different as-
sumptions about the relations between words within
a context window. The Global Vector model, re-
ferred as GloVe (Pennington et al., 2014), combines
this approach with ideas drawn from matrix factor-
ization methods, such as LSA (Deerwester et al.,
1990). The embeddings are derived with an objec-
tive function that combines context window infor-
mation, with corpus statistics computed efficiently
from a global term co-occurrence matrix.
</bodyText>
<sectionHeader confidence="0.993898" genericHeader="method">
4 Labeled Data
</sectionHeader>
<bodyText confidence="0.999964285714286">
The evaluation of the shared task was performed
on a labeled test set, consisting of 1315 words and
phrases. To support the development of the systems,
the organizers released a trial set with 200 exam-
ples. The terms are representative of the informal
style of Twitter text, containing hashtags, slang, ab-
breviations and misspelled words. Negated expres-
sions were also included. We show a sample of the
words and phrases in Table 1. For more details on
these datasets, see (Kiritchenko et al., 2014).
Given the small size of the trial set, we used an ad-
ditional labeled lexicon: the Language Assessment
by Mechanical Turk (LabMT) lexicon (Dodds et al.,
2011). It consists of 10,000 words collected from
different sources. Words were rated on a scale of 1
(sad) to 9 (happy), by users of Amazon’s Mechan-
ical Turk service, resulting in a measure of average
happiness for each given word. Note that LabMT
contains annotations for happiness but our goal is
to label words in terms of sentiment polarity. We
rely on the fact that some emotions are correlated
with sentiment, namely, joy/happiness are associ-
ated with positivity, while sadness/disgust relate to
negativity (Liu, 2012).
This complementary dataset was used for two pur-
poses: first, as the development set to evaluate and
tune our system, and second, as additional training
data for the candidate submission.
</bodyText>
<subsectionHeader confidence="0.698018">
Type Sample words
</subsectionHeader>
<construct confidence="0.700285625">
words sweetest, giggle, sleazy, broken
slang bday, lmao, kewl, pics
negations can’t cope, don’t think, no probs
interjections weee, yays, woooo, eww
emphasized gooooood, loveeee, cuteeee, excitedddd
hashtags #gorgeous, #smelly, #fake, #classless
multiword hashtag #goodvibes, #everyonelikesitbutme
emoticons :o ): -.- :’) &lt;33
</construct>
<tableCaption confidence="0.99846">
Table 1: A sample of the different types of terms.
</tableCaption>
<sectionHeader confidence="0.939534" genericHeader="method">
5 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999837">
We addressed the task of inducing large scale sen-
timent lexicons for Twitter as a regression problem.
Each term wi was represented with an embedding
E · wi E Re×1, with e E {50, 200, 400, 600,12501}
as discussed in Section 3. Then, the manually anno-
tated lexicons were used to train a model that, given
a new term wj, predicts a score y E [0, 1] reflecting
the polarity and intensity of sentiment it conveys.
Note that the embeddings represent words, so to
deal with phrases we leveraged on the compositional
properties of word vectors (Mikolov et al., 2013b).
Given that algebraic operations in the embedding
space preserve meaning, we represented phrases as
the sum or mean of individual word vectors.
</bodyText>
<subsectionHeader confidence="0.995049">
5.1 Learning the Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999974388888889">
The first step of our approach, requires a corpus of
tweets to support the unsupervised learning of the
embedding matrix E. We resorted to the corpus of
52 million tweets used by Owoputi et al. (2013) and
the tokenizer described in the same work.
The CBOW and skip-gram embeddings were in-
duced using the word2vec2 tool, while we used
our own implementation of the structured skip-
gram. The default values in word2vec were em-
ployed for most of the parameters, but we set the
negative sampling rate to 25 words (Goldberg and
Levy, 2014). For the GloVe model, we used the
available implementation3 with the default param-
eters. In all the models, words occurring less than
100 times in the corpus were discarded, resulting in
a vocabulary of around 210,000 tokens.
Finally, embeddings of different sizes were built,
with 50, 200, 400 and 600 dimensions.
</bodyText>
<sectionHeader confidence="0.4489065" genericHeader="method">
Hyperparameter Optimization and Model
Selection
</sectionHeader>
<bodyText confidence="0.999039666666667">
Regarding the choice of learning algorithm, sev-
eral linear regression models were considered: least
squares and regularized variants, namely, the lasso,
ridge and elastic net regressors. We also experi-
mented with Support Vectors Regression (SVR) us-
ing non-linear kernels, namely, polynomial, sigmoid
</bodyText>
<footnote confidence="0.999874">
1corresponds to the concatenation of all the embeddings
2https://code.google.com/p/word2vec/
3http://nlp.stanford.edu/projects/GloVe/
</footnote>
<bodyText confidence="0.999443230769231">
and Radial Basis Function (RBF). Most of these
models have hyperparameters, thus the combination
of possible algorithms and parameters represents a
huge configuration space. A brute force approach to
find the optimal model would be cumbersome and
time consuming. Instead, for each parameter, we de-
fined meaningful distributions and ranges of values.
Then, a hyperparameter optimization algorithm was
used to find the best combination of model and pa-
rameters, by sampling from the specified configura-
tion pool. The Tree of Parzen Estimators algorithm,
as implemented in HyperOpt4, was used (Bergstra
et al., 2013).
</bodyText>
<sectionHeader confidence="0.9997" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999934277777778">
Learning word embeddings from large corpora al-
lowed us to derive representations for a considerable
number of words. Thus, we were able to find embed-
dings for 94% of the candidate terms. Using simple
normalization steps, we could find embeddings for
the remaining terms. However, we found that this
improvement in recall had almost no impact in the
performance of the system.
After mapping terms to their respective embed-
dings, we performed experiments to find the best re-
gression model and respective hyperparameters. For
this purpose, the LabMT lexicon was employed as
the development set and the trial data as a valida-
tion set, against which different configurations were
evaluated. After 1000 trials, the SVR model with
RBF kernel was selected. Finally, we performed
detailed experiments to compare word embedding
models and vectors of different dimensions.
</bodyText>
<subsectionHeader confidence="0.999327">
6.1 Submitted System
</subsectionHeader>
<bodyText confidence="0.999871625">
The evaluation on the trial data indicated that several
configurations of embedding model and size could
achieve the optimal results. Therefore, our candi-
date system was based on structured skip-gram em-
beddings with 600 dimensions, and SVR with RBF
kernel. The hyperparameters were set to C = 50,
c = 0.05 and γ = 0.01 and the system was trained
using the trial data and the LabMT lexicon.
</bodyText>
<footnote confidence="0.896093">
4http://hyperopt.github.io/hyperopt/
</footnote>
<figureCaption confidence="0.567249">
(a) Results of the top 4 ranking systems (b) Comparing word embedding models under vari-
ous training and test data regimes
Figure 2: Evaluation of the INESC-ID system.
</figureCaption>
<subsectionHeader confidence="0.931657">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999246636363636">
The experiments showed that all the word embed-
dings have comparable capabilities. In Figure 1, we
compare the results of different embeddings with the
same regression model. Regarding the representa-
tion of phrases, the skip-gram and structured skip-
gram embeddings performed better when averaged.
However, the GloVe and CBOW seemed to be more
effective when summing the individual word vec-
tors. These results were consistent across all the ex-
periments. In terms of embedding size, we observed
that smaller vectors tend to perform worse and, in
general, concatenating vectors of different dimen-
sionality improved performance. The CBOW rep-
resentations were the only exception. This suggests
that embeddings of different size capture different
aspects of words.
Our final method, attained the highest ranking re-
sult of the competition, with 0.63 rank correlation.
Figure 2a shows the results of the top 4 submissions
to SemEval. Further experiments were conducted
after the release of the test set labels. We found
that the concatenation of GloVe embeddings outper-
forms our previous choice of features on the test set.
Surprisingly, these embeddings obtained the worst
results on the trial data, but are much better than the
others in the test set, achieving a rank correlation of
0.67. At this point, it is still not clear why this is the
case.
Figure 2b shows the performance of each embed-
ding model, under different combinations of train-
ing and test data. We can see that the proposed ap-
proach is effective, and our models outperform the
other systems with as few as 200 training examples.
</bodyText>
<sectionHeader confidence="0.994666" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999965866666667">
We described the approach followed by INESC-ID
for subtask E of SemEval 2015 Twitter Sentiment
Analysis challenge. This work presents the first
steps towards a general method to extract large-scale
lexicons with fine-grained annotations from Twitter
data. Although the results are encouraging, further
investigation is required to shed light on some un-
expected outcomes (e.g., the inconsistent behavior
of the GloVe features on the trial and test sets). It
should nonetheless be noted that, given the small
size of the labeled sets, it is hard to draw defini-
tive conclusions about the soundness of any method.
Furthermore, the merit of a sentiment lexicon should
be assessed in terms of its impact on the perfor-
mance of concrete sentiment analysis applications.
</bodyText>
<sectionHeader confidence="0.996034" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.7521255">
This work was partially supported by Fundac¸˜ao
para a Ciˆencia e Tecnologia (FCT), through
contracts Pest-OE/EEI/LA0021/2013, EXCL/EEI-
ESS/0257/2012 (DataStorm), grant number
SFRH/BPD/68428/2010 and Ph.D. scholarship
SFRH/BD/89020/2012.
</footnote>
<sectionHeader confidence="0.891712" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996914921568627">
James Bergstra, Daniel Yamins, and David Cox. 2013.
Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision ar-
chitectures. In Proceedings of the 30th International
Conference on Machine Learning, pages 115–123.
Johan Bollen and Huina Mao. 2011. Twitter mood as a
stock market predictor. Computer, 44:91–94.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JAsIs,
41:391–407.
Peter Sheridan Dodds, Kameron Decker Harris, Isabel M
Kloumann, Catherine A Bliss, and Christopher M
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. PloS one, 6(12):e26752.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 8th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 174–181.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings of
4th International Conference on Language Resources
and Evaluation, Vol IV,, pages 1115–1118.
Maurice G Kendall. 1938. A new measure of rank corre-
lation. Biometrika, pages 81–93.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 200–207.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Moham-
mad. 2014. Sentiment analysis of short informal texts.
Journal ofArtificial Intelligence Research, pages 723–
762.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, volume 98, pages
296–304.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.
Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word representa-
tions in vector space. In Workshop at the International
Conference on Learning Representations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In
Proceedings of the 27th Annual Conference on Neural
Information Processing Systems.
Lewis Mitchell, Kameron Decker Harris, Morgan R
Frank, Peter Sheridan Dodds, and Christopher M Dan-
forth. 2013. The geography of happiness: connecting
twitter sentiment and expression, demographics, and
objective characteristics of place. PLoS ONE, 8(5).
Brendan O’Connor, Ramnath Balasubramanyan, Bryan R
Routledge, and Noah A Smith. 2010. From tweets to
polls: Linking text sentiment to public opinion time
series. In Proceedings of the 4th International AAAI
Conference on Weblogs and Social Media.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 380–390.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word rep-
resentation. Proceedings of the 2014 Empiricial Meth-
ods in Natural Language Processing, 12.
Delip Rao and Deepak Ravichandran. 2009. Semi-
supervised polarity lexicon induction. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
675–682.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting
Liu. 2014. Building large-scale twitter-specific senti-
ment lexicon : A representation learning approach. In
Proceedings of the 25th International Conference on
Computational Linguistics, pages 172–182.
Peter D Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21(4):315–346.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278577">
<title confidence="0.992873">INESC-ID: A Regression Model for Large Scale Twitter Sentiment Lexicon Induction</title>
<author confidence="0.960759">Ramon F Astudillo Amir</author>
<author confidence="0.960759">Wang Ling</author>
<author confidence="0.960759">Bruno M´ario Silva</author>
<author confidence="0.960759">Isabel</author>
<affiliation confidence="0.902293">de Engenharia de Sistemas e Computadores e</affiliation>
<author confidence="0.496359">Rua Alves Redol</author>
<affiliation confidence="0.745609">Lisbon,</affiliation>
<email confidence="0.979921">ramon.astudillo,wlin,mjs,</email>
<abstract confidence="0.983858571428571">We present the approach followed by INESC- ID in the SemEval 2015 Twitter Sentiment Analysis challenge, subtask E. The goal was to determine the strength of the association of Twitter terms with positive sentiment. Using two labeled lexicons, we trained a regression model to predict the sentiment polarity and intensity of words and phrases. Terms were represented as word embeddings induced in an unsupervised fashion from a corpus of tweets. Our system attained the top ranking submission, attesting the general adequacy of the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Daniel Yamins</author>
<author>David Cox</author>
</authors>
<title>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="13164" citStr="Bergstra et al., 2013" startWordPosition="2125" endWordPosition="2128">ts/GloVe/ and Radial Basis Function (RBF). Most of these models have hyperparameters, thus the combination of possible algorithms and parameters represents a huge configuration space. A brute force approach to find the optimal model would be cumbersome and time consuming. Instead, for each parameter, we defined meaningful distributions and ranges of values. Then, a hyperparameter optimization algorithm was used to find the best combination of model and parameters, by sampling from the specified configuration pool. The Tree of Parzen Estimators algorithm, as implemented in HyperOpt4, was used (Bergstra et al., 2013). 6 Experiments Learning word embeddings from large corpora allowed us to derive representations for a considerable number of words. Thus, we were able to find embeddings for 94% of the candidate terms. Using simple normalization steps, we could find embeddings for the remaining terms. However, we found that this improvement in recall had almost no impact in the performance of the system. After mapping terms to their respective embeddings, we performed experiments to find the best regression model and respective hyperparameters. For this purpose, the LabMT lexicon was employed as the developme</context>
</contexts>
<marker>Bergstra, Yamins, Cox, 2013</marker>
<rawString>James Bergstra, Daniel Yamins, and David Cox. 2013. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In Proceedings of the 30th International Conference on Machine Learning, pages 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
</authors>
<title>Twitter mood as a stock market predictor.</title>
<date>2011</date>
<journal>Computer,</journal>
<pages>44--91</pages>
<contexts>
<context position="1685" citStr="Bollen and Mao, 2011" startWordPosition="253" endWordPosition="256">bjective text (Liu, 2012). They compile words annotated with their prior polarity of sentiment, regardless of the context. For instance, words such as beautiful or amazing tend to express a positive sentiment, whereas words like boring or ugly are considered negative. Most sentiment analysis systems use either word count methods, based on sentiment lexicons, or rely on text classifiers. In the former, the polarity of a message is estimated by computing the ratio of (positive and negative) sentiment bearing words. Despite its simplicity, this method has been widely used (O’Connor et al., 2010; Bollen and Mao, 2011; Mitchell et al., 2013). Even more sophisticated systems, based on supervised classification, can be greatly improved with features derived from lexicons (Kiritchenko et al., 2014). However, manually created sentiment lexicons consist of few carefully selected words. Consequently, they fail to capture the use of non-conventional word spelling and slang, commonly found in social media. This problem motivated the creation of a task in the SemEval 2015 Twitter Sentiment Analysis challenge. This task (subtask E), intended to evaluate automatic methods of generating Twitter specific sentiment lexi</context>
</contexts>
<marker>Bollen, Mao, 2011</marker>
<rawString>Johan Bollen and Huina Mao. 2011. Twitter mood as a stock market predictor. Computer, 44:91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="5781" citStr="Collobert et al., 2011" startWordPosition="912" endWordPosition="915">nalysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sen</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<pages>41--391</pages>
<contexts>
<context position="8504" citStr="Deerwester et al., 1990" startWordPosition="1390" endWordPosition="1393">nction of vector size. defined as: p(wi*i−d, ..., wi+di C, E) a exp(C · Si+d i−d) (2) where Si+d i−d is the point wise sum of the embeddings of all context words starting at E · wi−d to E · wi+d, excluding the index wi, and once again C E Re×v is a matrix mapping the embedding space into the output vocabulary space v. GloVe The models discussed above rely on different assumptions about the relations between words within a context window. The Global Vector model, referred as GloVe (Pennington et al., 2014), combines this approach with ideas drawn from matrix factorization methods, such as LSA (Deerwester et al., 1990). The embeddings are derived with an objective function that combines context window information, with corpus statistics computed efficiently from a global term co-occurrence matrix. 4 Labeled Data The evaluation of the shared task was performed on a labeled test set, consisting of 1315 words and phrases. To support the development of the systems, the organizers released a trial set with 200 examples. The terms are representative of the informal style of Twitter text, containing hashtags, slang, abbreviations and misspelled words. Negated expressions were also included. We show a sample of the</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JAsIs, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Sheridan Dodds</author>
<author>Kameron Decker Harris</author>
<author>Isabel M Kloumann</author>
<author>Catherine A Bliss</author>
<author>Christopher M Danforth</author>
</authors>
<title>Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter.</title>
<date>2011</date>
<journal>PloS one,</journal>
<pages>6--12</pages>
<contexts>
<context position="9360" citStr="Dodds et al., 2011" startWordPosition="1530" endWordPosition="1533">erformed on a labeled test set, consisting of 1315 words and phrases. To support the development of the systems, the organizers released a trial set with 200 examples. The terms are representative of the informal style of Twitter text, containing hashtags, slang, abbreviations and misspelled words. Negated expressions were also included. We show a sample of the words and phrases in Table 1. For more details on these datasets, see (Kiritchenko et al., 2014). Given the small size of the trial set, we used an additional labeled lexicon: the Language Assessment by Mechanical Turk (LabMT) lexicon (Dodds et al., 2011). It consists of 10,000 words collected from different sources. Words were rated on a scale of 1 (sad) to 9 (happy), by users of Amazon’s Mechanical Turk service, resulting in a measure of average happiness for each given word. Note that LabMT contains annotations for happiness but our goal is to label words in terms of sentiment polarity. We rely on the fact that some emotions are correlated with sentiment, namely, joy/happiness are associated with positivity, while sadness/disgust relate to negativity (Liu, 2012). This complementary dataset was used for two purposes: first, as the developmen</context>
</contexts>
<marker>Dodds, Harris, Kloumann, Bliss, Danforth, 2011</marker>
<rawString>Peter Sheridan Dodds, Kameron Decker Harris, Isabel M Kloumann, Catherine A Bliss, and Christopher M Danforth. 2011. Temporal patterns of happiness and information in a global social network: Hedonometrics and twitter. PloS one, 6(12):e26752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="11754" citStr="Goldberg and Levy, 2014" startWordPosition="1924" endWordPosition="1927">ed phrases as the sum or mean of individual word vectors. 5.1 Learning the Word Embeddings The first step of our approach, requires a corpus of tweets to support the unsupervised learning of the embedding matrix E. We resorted to the corpus of 52 million tweets used by Owoputi et al. (2013) and the tokenizer described in the same work. The CBOW and skip-gram embeddings were induced using the word2vec2 tool, while we used our own implementation of the structured skipgram. The default values in word2vec were employed for most of the parameters, but we set the negative sampling rate to 25 words (Goldberg and Levy, 2014). For the GloVe model, we used the available implementation3 with the default parameters. In all the models, words occurring less than 100 times in the corpus were discarded, resulting in a vocabulary of around 210,000 tokens. Finally, embeddings of different sizes were built, with 50, 200, 400 and 600 dimensions. Hyperparameter Optimization and Model Selection Regarding the choice of learning algorithm, several linear regression models were considered: least squares and regularized variants, namely, the lasso, ridge and elastic net regressors. We also experimented with Support Vectors Regress</context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="4460" citStr="Hatzivassiloglou and McKeown (1997)" startWordPosition="703" endWordPosition="707">nyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Tw</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientations of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th International Conference on Language Resources and Evaluation, Vol IV,,</booktitle>
<pages>1115--1118</pages>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using wordnet to measure semantic orientations of adjectives. In Proceedings of 4th International Conference on Language Resources and Evaluation, Vol IV,, pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>81--93</pages>
<contexts>
<context position="2684" citStr="Kendall (1938)" startWordPosition="412" endWordPosition="413">ia. This problem motivated the creation of a task in the SemEval 2015 Twitter Sentiment Analysis challenge. This task (subtask E), intended to evaluate automatic methods of generating Twitter specific sentiment lexicons. Given a set of words or phrases, the goal was to assign a score between 0 and 1, reflecting the intensity and polarity of sentiment these terms express. Participants were asked to submit a list, with the candidate terms ranked according to sentiment score. This list was then compared to a ranked list obtained from human annotations and the submissions were evaluated using the Kendall (1938) Tau rank correlation metric. In this paper, we describe a system developed for this challenge, based on a novel method to create large scale, domain-specific sentiment lexicons. The task is addressed as a regression problem, in which terms are represented as word embeddings, induced from a corpus of 52 million tweets. Then, using manually annotated lexicons, a regression model was trained to predict the polarity and intensity of sentiment of any word or phrase from that corpus. We found this approach to be effective for the proposed problem. The rest of the paper proceeds as follows: we revie</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, pages 81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="3785" citStr="Kim and Hovy (2006)" startWordPosition="595" endWordPosition="598">. We found this approach to be effective for the proposed problem. The rest of the paper proceeds as follows: we review the work related to lexicon expansion in Section 2 and describe the methods used to derive word embeddings in Section 3. Our approach and the experimental results are presented in Sections 5 and 6, respectively. We conclude in Section 7. 2 Related Work Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from c</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Identifying and analyzing judgment opinions. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>723--762</pages>
<contexts>
<context position="1866" citStr="Kiritchenko et al., 2014" startWordPosition="280" endWordPosition="283"> to express a positive sentiment, whereas words like boring or ugly are considered negative. Most sentiment analysis systems use either word count methods, based on sentiment lexicons, or rely on text classifiers. In the former, the polarity of a message is estimated by computing the ratio of (positive and negative) sentiment bearing words. Despite its simplicity, this method has been widely used (O’Connor et al., 2010; Bollen and Mao, 2011; Mitchell et al., 2013). Even more sophisticated systems, based on supervised classification, can be greatly improved with features derived from lexicons (Kiritchenko et al., 2014). However, manually created sentiment lexicons consist of few carefully selected words. Consequently, they fail to capture the use of non-conventional word spelling and slang, commonly found in social media. This problem motivated the creation of a task in the SemEval 2015 Twitter Sentiment Analysis challenge. This task (subtask E), intended to evaluate automatic methods of generating Twitter specific sentiment lexicons. Given a set of words or phrases, the goal was to assign a score between 0 and 1, reflecting the intensity and polarity of sentiment these terms express. Participants were aske</context>
<context position="5011" citStr="Kiritchenko et al. (2014)" startWordPosition="791" endWordPosition="794">-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word emb</context>
<context position="9201" citStr="Kiritchenko et al., 2014" startWordPosition="1503" endWordPosition="1506">ntext window information, with corpus statistics computed efficiently from a global term co-occurrence matrix. 4 Labeled Data The evaluation of the shared task was performed on a labeled test set, consisting of 1315 words and phrases. To support the development of the systems, the organizers released a trial set with 200 examples. The terms are representative of the informal style of Twitter text, containing hashtags, slang, abbreviations and misspelled words. Negated expressions were also included. We show a sample of the words and phrases in Table 1. For more details on these datasets, see (Kiritchenko et al., 2014). Given the small size of the trial set, we used an additional labeled lexicon: the Language Assessment by Mechanical Turk (LabMT) lexicon (Dodds et al., 2011). It consists of 10,000 words collected from different sources. Words were rated on a scale of 1 (sad) to 9 (happy), by users of Amazon’s Mechanical Turk service, resulting in a measure of average happiness for each given word. Note that LabMT contains annotations for happiness but our goal is to label words in terms of sentiment polarity. We rely on the fact that some emotions are correlated with sentiment, namely, joy/happiness are ass</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mohammad. 2014. Sentiment analysis of short informal texts. Journal ofArtificial Intelligence Research, pages 723– 762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<volume>98</volume>
<pages>296--304</pages>
<contexts>
<context position="4956" citStr="Lin, 1998" startWordPosition="783" endWordPosition="784">ity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, volume 98, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="7379" citStr="Ling et al., 2015" startWordPosition="1191" endWordPosition="1194">trix, transforming the one-hot sparse representation into a compact real valued space of size e; Cp E Rv×e is a matrix mapping the realvalued representation to a vector with the size of the vocabulary v. A distribution over all possible words is then attained by exponentiating and normalizing over the v possible options. In practice, due to the large value of v, various techniques are used to avoid having to normalize over the whole vocabulary (Mikolov et al., 2013a). In the particular case of the structured skip-gram model, the matrix Cp depends only of the relative position between words p (Ling et al., 2015). After training, the low dimensional embedding E· wi E Re×1 encapsulates the information about each word and its surrounding contexts. CBOW The CBOW model defines a different objective function, that predicts a word at position i given the window of context i − d, where d is the size of the context window. The probability of the word wi is (a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings Figure 1: Performance of the different embeddings and phrase representations, as function of vector size. defined as: p(wi*i−d, ..., wi+di C, E) a exp(C · Si+d i−d) (2) where Si+d i−</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1090" citStr="Liu, 2012" startWordPosition="157" endWordPosition="158">t Analysis challenge, subtask E. The goal was to determine the strength of the association of Twitter terms with positive sentiment. Using two labeled lexicons, we trained a regression model to predict the sentiment polarity and intensity of words and phrases. Terms were represented as word embeddings induced in an unsupervised fashion from a corpus of tweets. Our system attained the top ranking submission, attesting the general adequacy of the proposed approach. 1 Introduction Sentiment lexicons are one of the key resources for the automatic analysis of opinions, emotive and subjective text (Liu, 2012). They compile words annotated with their prior polarity of sentiment, regardless of the context. For instance, words such as beautiful or amazing tend to express a positive sentiment, whereas words like boring or ugly are considered negative. Most sentiment analysis systems use either word count methods, based on sentiment lexicons, or rely on text classifiers. In the former, the polarity of a message is estimated by computing the ratio of (positive and negative) sentiment bearing words. Despite its simplicity, this method has been widely used (O’Connor et al., 2010; Bollen and Mao, 2011; Mit</context>
<context position="9880" citStr="Liu, 2012" startWordPosition="1617" endWordPosition="1618">d lexicon: the Language Assessment by Mechanical Turk (LabMT) lexicon (Dodds et al., 2011). It consists of 10,000 words collected from different sources. Words were rated on a scale of 1 (sad) to 9 (happy), by users of Amazon’s Mechanical Turk service, resulting in a measure of average happiness for each given word. Note that LabMT contains annotations for happiness but our goal is to label words in terms of sentiment polarity. We rely on the fact that some emotions are correlated with sentiment, namely, joy/happiness are associated with positivity, while sadness/disgust relate to negativity (Liu, 2012). This complementary dataset was used for two purposes: first, as the development set to evaluate and tune our system, and second, as additional training data for the candidate submission. Type Sample words words sweetest, giggle, sleazy, broken slang bday, lmao, kewl, pics negations can’t cope, don’t think, no probs interjections weee, yays, woooo, eww emphasized gooooood, loveeee, cuteeee, excitedddd hashtags #gorgeous, #smelly, #fake, #classless multiword hashtag #goodvibes, #everyonelikesitbutme emoticons :o ): -.- :’) &lt;33 Table 1: A sample of the different types of terms. 5 Proposed Appro</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Workshop at the International Conference on Learning Representations.</booktitle>
<contexts>
<context position="5803" citStr="Mikolov et al., 2013" startWordPosition="916" endWordPosition="919">upervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sentence wi, the skip-gra</context>
<context position="7230" citStr="Mikolov et al., 2013" startWordPosition="1166" endWordPosition="1169">e size of the vocabulary v with a 1 on the position corresponding to that word. The model is parametrized by two matrices: E E Re×v is the embedding matrix, transforming the one-hot sparse representation into a compact real valued space of size e; Cp E Rv×e is a matrix mapping the realvalued representation to a vector with the size of the vocabulary v. A distribution over all possible words is then attained by exponentiating and normalizing over the v possible options. In practice, due to the large value of v, various techniques are used to avoid having to normalize over the whole vocabulary (Mikolov et al., 2013a). In the particular case of the structured skip-gram model, the matrix Cp depends only of the relative position between words p (Ling et al., 2015). After training, the low dimensional embedding E· wi E Re×1 encapsulates the information about each word and its surrounding contexts. CBOW The CBOW model defines a different objective function, that predicts a word at position i given the window of context i − d, where d is the size of the context window. The probability of the word wi is (a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings Figure 1: Performance of the diff</context>
<context position="11041" citStr="Mikolov et al., 2013" startWordPosition="1803" endWordPosition="1806"> A sample of the different types of terms. 5 Proposed Approach We addressed the task of inducing large scale sentiment lexicons for Twitter as a regression problem. Each term wi was represented with an embedding E · wi E Re×1, with e E {50, 200, 400, 600,12501} as discussed in Section 3. Then, the manually annotated lexicons were used to train a model that, given a new term wj, predicts a score y E [0, 1] reflecting the polarity and intensity of sentiment it conveys. Note that the embeddings represent words, so to deal with phrases we leveraged on the compositional properties of word vectors (Mikolov et al., 2013b). Given that algebraic operations in the embedding space preserve meaning, we represented phrases as the sum or mean of individual word vectors. 5.1 Learning the Word Embeddings The first step of our approach, requires a corpus of tweets to support the unsupervised learning of the embedding matrix E. We resorted to the corpus of 52 million tweets used by Owoputi et al. (2013) and the tokenizer described in the same work. The CBOW and skip-gram embeddings were induced using the word2vec2 tool, while we used our own implementation of the structured skipgram. The default values in word2vec were</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Workshop at the International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of the 27th Annual Conference on Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5803" citStr="Mikolov et al., 2013" startWordPosition="916" endWordPosition="919">upervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sentence wi, the skip-gra</context>
<context position="7230" citStr="Mikolov et al., 2013" startWordPosition="1166" endWordPosition="1169">e size of the vocabulary v with a 1 on the position corresponding to that word. The model is parametrized by two matrices: E E Re×v is the embedding matrix, transforming the one-hot sparse representation into a compact real valued space of size e; Cp E Rv×e is a matrix mapping the realvalued representation to a vector with the size of the vocabulary v. A distribution over all possible words is then attained by exponentiating and normalizing over the v possible options. In practice, due to the large value of v, various techniques are used to avoid having to normalize over the whole vocabulary (Mikolov et al., 2013a). In the particular case of the structured skip-gram model, the matrix Cp depends only of the relative position between words p (Ling et al., 2015). After training, the low dimensional embedding E· wi E Re×1 encapsulates the information about each word and its surrounding contexts. CBOW The CBOW model defines a different objective function, that predicts a word at position i given the window of context i − d, where d is the size of the context window. The probability of the word wi is (a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings Figure 1: Performance of the diff</context>
<context position="11041" citStr="Mikolov et al., 2013" startWordPosition="1803" endWordPosition="1806"> A sample of the different types of terms. 5 Proposed Approach We addressed the task of inducing large scale sentiment lexicons for Twitter as a regression problem. Each term wi was represented with an embedding E · wi E Re×1, with e E {50, 200, 400, 600,12501} as discussed in Section 3. Then, the manually annotated lexicons were used to train a model that, given a new term wj, predicts a score y E [0, 1] reflecting the polarity and intensity of sentiment it conveys. Note that the embeddings represent words, so to deal with phrases we leveraged on the compositional properties of word vectors (Mikolov et al., 2013b). Given that algebraic operations in the embedding space preserve meaning, we represented phrases as the sum or mean of individual word vectors. 5.1 Learning the Word Embeddings The first step of our approach, requires a corpus of tweets to support the unsupervised learning of the embedding matrix E. We resorted to the corpus of 52 million tweets used by Owoputi et al. (2013) and the tokenizer described in the same work. The CBOW and skip-gram embeddings were induced using the word2vec2 tool, while we used our own implementation of the structured skipgram. The default values in word2vec were</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lewis Mitchell</author>
<author>Kameron Decker Harris</author>
<author>Morgan R Frank</author>
<author>Peter Sheridan Dodds</author>
<author>Christopher M Danforth</author>
</authors>
<title>The geography of happiness: connecting twitter sentiment and expression, demographics, and objective characteristics of place.</title>
<date>2013</date>
<journal>PLoS ONE,</journal>
<volume>8</volume>
<issue>5</issue>
<contexts>
<context position="1709" citStr="Mitchell et al., 2013" startWordPosition="257" endWordPosition="260">12). They compile words annotated with their prior polarity of sentiment, regardless of the context. For instance, words such as beautiful or amazing tend to express a positive sentiment, whereas words like boring or ugly are considered negative. Most sentiment analysis systems use either word count methods, based on sentiment lexicons, or rely on text classifiers. In the former, the polarity of a message is estimated by computing the ratio of (positive and negative) sentiment bearing words. Despite its simplicity, this method has been widely used (O’Connor et al., 2010; Bollen and Mao, 2011; Mitchell et al., 2013). Even more sophisticated systems, based on supervised classification, can be greatly improved with features derived from lexicons (Kiritchenko et al., 2014). However, manually created sentiment lexicons consist of few carefully selected words. Consequently, they fail to capture the use of non-conventional word spelling and slang, commonly found in social media. This problem motivated the creation of a task in the SemEval 2015 Twitter Sentiment Analysis challenge. This task (subtask E), intended to evaluate automatic methods of generating Twitter specific sentiment lexicons. Given a set of wor</context>
</contexts>
<marker>Mitchell, Harris, Frank, Dodds, Danforth, 2013</marker>
<rawString>Lewis Mitchell, Kameron Decker Harris, Morgan R Frank, Peter Sheridan Dodds, and Christopher M Danforth. 2013. The geography of happiness: connecting twitter sentiment and expression, demographics, and objective characteristics of place. PLoS ONE, 8(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R Routledge, and Noah A Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Empiricial Methods in Natural Language Processing,</booktitle>
<pages>12</pages>
<contexts>
<context position="5830" citStr="Pennington et al., 2014" startWordPosition="920" endWordPosition="923">, 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sentence wi, the skip-gram estimates the probability</context>
<context position="8390" citStr="Pennington et al., 2014" startWordPosition="1372" endWordPosition="1375">ases as the mean of embeddings Figure 1: Performance of the different embeddings and phrase representations, as function of vector size. defined as: p(wi*i−d, ..., wi+di C, E) a exp(C · Si+d i−d) (2) where Si+d i−d is the point wise sum of the embeddings of all context words starting at E · wi−d to E · wi+d, excluding the index wi, and once again C E Re×v is a matrix mapping the embedding space into the output vocabulary space v. GloVe The models discussed above rely on different assumptions about the relations between words within a context window. The Global Vector model, referred as GloVe (Pennington et al., 2014), combines this approach with ideas drawn from matrix factorization methods, such as LSA (Deerwester et al., 1990). The embeddings are derived with an objective function that combines context window information, with corpus statistics computed efficiently from a global term co-occurrence matrix. 4 Labeled Data The evaluation of the shared task was performed on a labeled test set, consisting of 1315 words and phrases. To support the development of the systems, the organizers released a trial set with 200 examples. The terms are representative of the informal style of Twitter text, containing ha</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the 2014 Empiricial Methods in Natural Language Processing, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Semisupervised polarity lexicon induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>675--682</pages>
<contexts>
<context position="4166" citStr="Rao and Ravichandran, 2009" startWordPosition="662" endWordPosition="665">Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing posi</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. Semisupervised polarity lexicon induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Building large-scale twitter-specific sentiment lexicon : A representation learning approach.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<pages>172--182</pages>
<contexts>
<context position="5213" citStr="Tang et al., 2014" startWordPosition="820" endWordPosition="823"> whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Penning</context>
</contexts>
<marker>Tang, Wei, Qin, Zhou, Liu, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting Liu. 2014. Building large-scale twitter-specific sentiment lexicon : A representation learning approach. In Proceedings of the 25th International Conference on Computational Linguistics, pages 172–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4694" citStr="Turney and Littman (2003)" startWordPosition="738" endWordPosition="741">t al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the </context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D Turney and Michael L Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21(4):315–346.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>