<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.9979315">
JU-Evora: A Graph Based Cross-Level Semantic Similarity Analysis
using Discourse Information
</title>
<author confidence="0.990663">
Swarnendu Ghosh
</author>
<affiliation confidence="0.5692964">
Dept. of Computer
Science and Engi-
neering
Jadavpur University,
Kolkata, India
</affiliation>
<email confidence="0.686317">
swarbir@gmail.
com
</email>
<author confidence="0.95434">
Nibaran Das
</author>
<affiliation confidence="0.958136">
Dept. of Computer
</affiliation>
<address confidence="0.4116434">
Science and Engi-
neering
Jadavpur University,
Kolkata, India
ni-
</address>
<email confidence="0.988335">
baran@ieee.org
</email>
<author confidence="0.992624">
Teresa Gonçalves
</author>
<affiliation confidence="0.998159">
Dept. of Informática
University of Évora,
</affiliation>
<address confidence="0.712026">
Évora, Portugal
</address>
<email confidence="0.998486">
tcg@evora.pt
</email>
<author confidence="0.99676">
Paulo Quaresma
</author>
<affiliation confidence="0.9981955">
Dept. of Informática
University of Évora,
</affiliation>
<address confidence="0.71204">
Évora, Portugal
</address>
<email confidence="0.998516">
pq@evora.pt
</email>
<sectionHeader confidence="0.995926" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.957447">
Text Analytics using semantic information is
the latest trend of research due to its potential
to represent better the texts content compared
with the bag-of-words approaches. On the
contrary, representation of semantics through
graphs has several advantages over the tradi-
tional representation of feature vector. There-
fore, error tolerant graph matching techniques
can be used for text comparison. Neverthe-
less, not many methodologies exist in the lit-
erature which expresses semantic representa-
tions through graphs. The present system is
designed to deal with cross level semantic
similarity analysis as proposed in the
SemEval-2014 : Semantic Evaluation, Inter-
national Workshop on Semantic Evaluation,
Dublin, Ireland.
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999330153846154">
Text Analytics has been the focus of much re-
search work in the last years. State of the art ap-
proaches typically represent documents as vec-
tors (bag-of-words) and use a machine learning
algorithm, such as k-NN or SVM, to create a
model and to compare and classify new docu-
ments. However, and in spite of being able to
obtain good results, these approaches fail to rep-
resent the semantic content of the documents,
losing much information and limiting the tasks
that can be implemented over the document rep-
resentation structures. To overcome these short-
comings some research has been done aiming to
</bodyText>
<footnote confidence="0.6393">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence
details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999635">
use and evaluate more complex knowledge rep-
resentation structures. In this paper, a new ap-
proach which integrates a deep linguistic analysis
of the documents with graph-based classification
algorithms and metrics has been proposed.
</bodyText>
<sectionHeader confidence="0.64894" genericHeader="method">
2 Overview of the Task
</sectionHeader>
<bodyText confidence="0.9788462">
This task provides an evaluation for semantic
similarity across different sizes of text, which we
refer to as lexical levels. Specifically, this task
encompasses four semantic similarity compari-
sons:
</bodyText>
<listItem confidence="0.99988225">
• paragraph to sentence(P2S),
• sentence to phrase(S2Ph),
• phrase to word(Ph2W), and
• word to sense(W2S).
</listItem>
<bodyText confidence="0.999953461538462">
Task participants were provided with pairs of
each comparison type and asked to rate the pair
according to the semantic similarity of the small-
er item to the larger item. As an example, given a
sentence and a paragraph, a system would assess
how similar is the meaning of the sentence to the
meaning of the paragraph. Ideally, a high-
similarity sentence would reflect overall meaning
of the paragraph. The participants were expected
to assign a score between [0,4] to each pairs of
sentences, where 0 shows no similarity in con-
cept while 4 shows complete similarity in con-
cept.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="method">
3 Theoretical Concepts
</sectionHeader>
<subsectionHeader confidence="0.999757">
3.1 Discourse Representation Structures
</subsectionHeader>
<bodyText confidence="0.9729945">
Extraction and representation of the information
conveyed by texts can be performed through
several approaches, starting from statistical anal-
ysis to deep linguistic techniques. In this paper
</bodyText>
<page confidence="0.988271">
375
</page>
<note confidence="0.7304435">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 375–379,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999839208333333">
we will use a deep linguistic processing se-
quence: lexical, syntactic, and semantic analysis.
One of the most prominent research work on
semantic analysis is the Discourse Representa-
tion Theory (DRT)(Kamp &amp; Reyle, 1993). In
DRT, we aim to associate sentences with expres-
sions in a logical language, which indicate their
meaning. In DRT, each sentence is viewed as an
update of an existing context, having as result a
new context.
DRT provides a very powerful platform for
the representation of semantic structures of doc-
uments including complex relations like implica-
tions, propositions and negations. It is also able
to separately analyse almost all kinds of events
and find out their agent and patient.
The main component of DRT is the Discourse
Representation Structure (DRS These expres-
sions have two main parts: a) a set of referents,
which refer to entities present in the context and
b) a set of conditions, which are the relations that
exist between the entities. An example of a DRS
representation for the sentence &amp;quot;He throws a
ball.&amp;quot; is shown below.
</bodyText>
<equation confidence="0.899346555555556">
[
x1, x2, x3:
male(x1),
ball(x2),
throw(x3),
event(x3),
agent(x3,x1),
patient(x3, x2)
]
</equation>
<subsectionHeader confidence="0.999319">
3.2 GML Structure
</subsectionHeader>
<bodyText confidence="0.999901642857143">
Graph Modelling Language (GML)(Himsolt &amp;
Passau, 1996) is a simple and efficient way to
represent weighted directed graphs. A GML file
is basically a 7-bit ASCII file, and, as such, can
be easily read, parsed, and written. Several open
source applications 1 are available that enable
viewing and editing GML files.
Graphs are represented by the keys viz. graph,
node and edge. The basic structure is modelled
with the node&apos;s id and the edge&apos;s source and tar-
get at-tributes. The id attributes assign numbers
to nodes, which are then referenced by source
and target. Weights can be represented by the
label attribute.
</bodyText>
<footnote confidence="0.9481815">
1http://en.wikipedia.org/wiki/Graph_Modelling_Lang
uage
</footnote>
<subsectionHeader confidence="0.993694">
3.3 Similarity Metrics for Graphs
</subsectionHeader>
<bodyText confidence="0.9990806">
It has already been mentioned that the objective
of the present work is to generate similarity
scores among documents of different lexical lev-
els using an approach which integrates a deep
linguistic analysis of the documents with graph-
based classification algorithms and metrics.
Here, five different distance metrics taken from
(Bunke, 2010) are utilized for this purpose. They
are popularly used in object recognition task, but
for text similarity measure they have not yet been
used.
For two graphs and , if ( ) is the
dissimilarity/similarity measure, then this meas-
ure would be a distance if has the following
properties:
</bodyText>
<table confidence="0.96003785">
( ) , iff
( ) ( )
( ) ( ) ( )
The measures used in the present work follow
the above rules and the corresponding equations
are
( )  |( )  |) |( )
(    |)
 |(
( )       ||( )|
...(2)
( )       ||( )|
...(3)
( )  |( )  ||( )|
...(4)
 |( )|
( ) ( )
 |( )|
In the equations ( ) and
( ) denote maximal common subgraph
</table>
<bodyText confidence="0.995465538461538">
and minimum common super graphs of two
graphs and . Theoretically ( ) is
the largest graph in terms of edges that is iso-
morphic to a subgraph of and . The
( ) has been formally defined in a work
of Horst Bunke (Bunke, Foggia, Guidobaldi,
Sansone, &amp; Vento, 2002). As stated earlier, it is
a NP complete problem and actually, the method
of finding the () is a brute force method
which finds all the subgraphs of both the graphs
and select the maximum graph which is common
to both. To make the program computationally
faster, the program is modified to an approxi-
</bodyText>
<page confidence="0.995658">
376
</page>
<bodyText confidence="0.999384">
mate version of mcs( Gl, G2) on the fact that the
vertices which exhibit greater similarity in their
local structures among the two graphs have a
greater probability of inclusion in the mcs() . The
two pass approach used in the present work to
form the approximate mcs( Gl, G2) is as follows:
</bodyText>
<listItem confidence="0.887512">
• All the node pairs (one from each graph)
are ranked according the number of
matching self-loops.
• The mcs is built by including each node
pair (starting with the one with the highest
number of matching self-loops) and con-
</listItem>
<bodyText confidence="0.933510931034483">
sidering it as a common node; and then in-
clude the rest of the edges (i.e. non-self-
loop edges) which occur in the same fash-
ion in both the graphs.
In this way it ensures that the approximation
version exhibits most of the properties of a mcs,
while keeping the complexity in a polynomial
time.
The minimum common supergraph
(MCS)(Angelova &amp; Weikum, 2006) is formed
using the union of two graphs, i.e.
MCS( Gl, G2) = Gl U G2.
The distance metrics of Equations 1-3 were
used directly without any modifications; the ones
of Equations 3-4 were divided by ( |Gl  |+  |G2 |)
and  |MCS( Gl, G2) + mcs( Gl, G2) |respectively
to make them normalized, keeping the value of
distance metrics within the range [ 0, 1] .
It is worthy to note that label matching that is
performed during the above mentioned step may
not necessarily be exact matching. Rather in this
case we have used the WordNet to find an ap-
proximate conceptual similarity between two
labels. For our experiment we have used the Wu
and Palmer‟s conceptual similarity (Wu &amp;
Palmer, 1994).
If L = lso( cl, c2) , where cland c2 are a pair
of concepts corresponding to two words and
lso( cl, c2) means the lowest super ordinate then,
</bodyText>
<equation confidence="0.998488">
sim ( cl,c2)
WP
2 x depth(L)
=len( cl, L) + len( c2, L) + 2 x depth(L)
</equation>
<subsectionHeader confidence="0.983144">
3.4 Tools Used
</subsectionHeader>
<bodyText confidence="0.999969555555556">
In order to process texts C&amp;C/Boxer (Bos, 2008;
Curran, Clark, &amp; Bos, 2007) a well-known open
source tool available as a plugin to Natural Lan-
guage Toolkit (NLTK) is used. The tool consists
of a combinatory categorical grammar (CCG)
(Curran et al., 2007) parser and outputs the se-
mantic representations using discourse represen-
tation structures (DRS) of Discourse Representa-
tion Theory (DRT) (Kamp &amp; Reyle, 1993).
</bodyText>
<sectionHeader confidence="0.995959" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.999916272727273">
The method described in the present work, is
mainly divided into three major components. The
first is the creation of the DRS of the semantic
interpretation of the text. The second is the con-
struction of graphs in GML from the obtained
DRS using some predefined rules. The third one
is the classification phase where the different
graph distances are assessed using a k-NN classi-
fier (Zhang, Li, Sun, &amp; Nadee, 2013).
The algorithm semantic evaluation of text con-
tent may be described as follows.
</bodyText>
<listItem confidence="0.968745">
• ILTK Module : For each pair of text, to
</listItem>
<figureCaption confidence="0.832204666666667">
Figure 1: Graphical overview of mcs and MCS: (a), (b) graph representation of sentences
meaning “Mary drinks water” and “David drinks water ” respectively, (c) maximum com-
mon subgraph, (d) minimum common supergraph.
</figureCaption>
<bodyText confidence="0.99951">
compare their similarity measure we need
to find their DRS using the C&amp;C/Boxer
toolkit. The toolkit first uses the C&amp;C Par-
ser to find the combinatorial categorical
grammar(CCG) of the text. Next the Boxer
Module uses the CCG to find the discourse
representation structures.
</bodyText>
<listItem confidence="0.963940612903226">
• Graph building module : In general Box-
er represents a sentence through some dis-
course referents and conditions based on
the semantic interpretation of the sentence.
In the graph, the referent is represented by
vertex after resolving the equity among
different referents of the DRS; and a con-
dition is represented by an edge value be-
tween two referents. The condition of a
single referent is represented as a self-loop
of the referent (source and destination ref-
erents are same). Special relationships
such as proposition, implication etc. are
treated as edge values between two refer-
ents; Agent and patient are also treated as
conditions of discourse, hence represented
by the edge values of two referents.
• Calculating Similarity Index : It has al-
ready been mentioned that the different
distance metrics (see Equations 1-5) calcu-
lated based on the mcs() and MCS(). The
values of mcs() and MCS() are represent-
ed by the number of similar edges. Thus,
ten different distances are calculated based
on Equations 1-5.
• Learning : We obtained 5 similarity
scores for each pair of texts. Our task re-
quires us to assign a score between 0-4 for
each pair of text. Hence using the gold
standard a K-NN Classifier have been
trained to find the output score for a test
</listItem>
<bodyText confidence="0.987421615384615">
sample. The value of K has been empiri-
cally adjusted using the cross validation
technique to find the optimal value.
Our method works smoothly for the first two
lexical levels. But for the last two levels i.e.
phrase to word and word to sense it is not possi-
ble to find out DRS for a single word. Hence we
have used the WordNet(Fellbaum, 1998) to ex-
tract the definition of the word in question and
calculate its DRS and proceed with the method.
When a word has multiple definitions, all the
definitions are fused to a single sentence after
conjugating them with the conjunction „or‟.
</bodyText>
<sectionHeader confidence="0.998854" genericHeader="evaluation">
5 Results and Discussions
</sectionHeader>
<bodyText confidence="0.998887375">
The JU-Evora system performed fairly in the
SemEval Competition 2014. All the correlation
scores are not as good as the Baseline(LCS)
scores, however it provides a better Pearson cor-
relation score in case of Paragraph to Sentence.
The other scores, though not higher, are in the
vicinity of the baseline. All the scores are shown
below in Table 1.
</bodyText>
<table confidence="0.974461">
PEARSON’S CORRELATION
P2S S2Ph Ph2W W2S
JU-Evora 0.536 0.442 0.090 0.091
Baseline (LCS) 0.527 0.562 0.165 0.109
SPEARMAN CORRELATION
JU-Evora 0.533 0.440 0.096 0.075
Baseline (LCS) 0.613 0.626 0.162 0.130
</table>
<tableCaption confidence="0.9473365">
Table 1: Performance of JU-Evora system with
respect to Baseline.
</tableCaption>
<sectionHeader confidence="0.997557" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999879294117647">
In this paper a new approach has been proposed
to the text comparison task which integrates a
deep linguistic analysis of the documents with a
graph-based comparison algorithm. In the lin-
guistic analysis, discourse representation struc-
tures (DRS) are used to represent text semantic
content and, afterwards, these structures are
transformed into graphs. We have evaluated ex-
istent graph distance metrics and proposed some
modifications, more adequate to calculate graph
distances between graph-drs structures. Finally,
we integrated graph-drs structures and the pro-
posed graph distance metrics into a k-NN classi-
fier for calculating the similarity between two
documents. Future works in this area would be
concentrated on the use of external knowledge
sources to make the system more robust.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999317777777778">
Angelova, Ralitsa, &amp; Weikum,Gerhard. (2006).
Graph-based Text Classification: Learn from Your
Neighbors. In Proceedings of the 29th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval (pp. 485–492). New York, NY, USA:
ACM.
Bos, Johan (2008). Wide-Coverage Semantic
Analysis with Boxer. In J. Bos &amp; R. Delmonte
</reference>
<page confidence="0.983928">
378
</page>
<reference confidence="0.999519090909091">
(Eds.), Semantics in Text Processing. STEP 2008
Conference Proceedings (pp. 277–286). College
Publications.
Bunke, Horst (2010). Graph Classification and
Clustering Based on Vector Space Embedding
(Vol. Volume 77, pp. 15–34). WORLD
SCIENTIFIC.
doi:doi:10.1142/9789814304726_0002
Bunke, Horst, Foggia, Pasquale, Guidobaldi, Corrado,
Sansone, Carlo, &amp; Vento, Mario (2002). A
Comparison of Algorithms for Maximum
Common Subgraph on Randomly Connected
Graphs. In T. Caelli, A. Amin, R. W. Duin, D.
Ridder, &amp; M. Kamel (Eds.), Structural, Syntactic,
and Statistical Pattern Recognition SE - 12 (Vol.
2396, pp. 123–132). Springer Berlin Heidelberg.
Curran, James, Clark, Stephen, &amp; Bos, Johan (2007).
Linguistically Motivated Large-Scale NLP with
C&amp;C and Boxer. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions (pp.
33–36). Prague, Czech Republic: Association for
Computational Linguistics.
Fellbaum, Christiane (1998). WordNet: An Electronic
Lexical Database. British Journal Of Hospital
Medicine London England 2005 (Vol. 71, p. 423).
Himsolt, Michael, &amp; Passau, Universität (1996).
GML : A portable Graph File Format. Syntax, 1–
11.
Kamp, Hans, &amp; Reyle, Uwe (1993). From discourse
to logic: Introduction to model theoretic semantics
of natural language, formal logic and discourse
representation theory.
Wu, Zhibiao, &amp; Palmer, Martha (1994). Verb
semantics and lexical selection. In 32nd Annual
Meeting of the Association for Computational
Linguistics,, 32, 133–138.
Zhang, Libiao, Li, Yuefeng, Sun, Chao, &amp; Nadee,
Winai (2013). Rough Set Based Approach to Text
Classification. Web Intelligence (WI) and
Intelligent Agent Technologies (IAT), 2013
IEEE/WIC/ACM International Joint Conferences
on.
</reference>
<page confidence="0.999155">
379
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.056172">
<title confidence="0.995375">JU-Evora: A Graph Based Cross-Level Semantic Similarity using Discourse Information</title>
<author confidence="0.77442">Swarnendu</author>
<affiliation confidence="0.880701">Dept. of and Jadavpur</affiliation>
<address confidence="0.986994">Kolkata, India</address>
<email confidence="0.9679425">swarbir@gmail.com</email>
<author confidence="0.653419">Nibaran</author>
<affiliation confidence="0.754075">Dept. of and Jadavpur</affiliation>
<address confidence="0.934318">Kolkata, India</address>
<email confidence="0.7626135">nibaran@ieee.org</email>
<author confidence="0.769696">Teresa</author>
<affiliation confidence="0.99875">Dept. of University of</affiliation>
<address confidence="0.977391">Évora, Portugal</address>
<email confidence="0.988924">tcg@evora.pt</email>
<author confidence="0.962965">Paulo</author>
<affiliation confidence="0.999414">Dept. of University of</affiliation>
<address confidence="0.975216">Évora, Portugal</address>
<email confidence="0.988138">pq@evora.pt</email>
<abstract confidence="0.97276">Text Analytics using semantic information is the latest trend of research due to its potential to represent better the texts content compared with the bag-of-words approaches. On the contrary, representation of semantics through graphs has several advantages over the traditional representation of feature vector. Therefore, error tolerant graph matching techniques can be used for text comparison. Nevertheless, not many methodologies exist in the literature which expresses semantic representations through graphs. The present system is designed to deal with cross level semantic similarity analysis as proposed in the SemEval-2014 : Semantic Evaluation, Inter-</abstract>
<affiliation confidence="0.911305">national Workshop on Semantic Evaluation,</affiliation>
<address confidence="0.990775">Dublin, Ireland.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ralitsa Angelova</author>
<author>Gerhard Weikum</author>
</authors>
<title>Graph-based Text Classification: Learn from Your Neighbors.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>485--492</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="7803" citStr="Angelova &amp; Weikum, 2006" startWordPosition="1273" endWordPosition="1276">m the approximate mcs( Gl, G2) is as follows: • All the node pairs (one from each graph) are ranked according the number of matching self-loops. • The mcs is built by including each node pair (starting with the one with the highest number of matching self-loops) and considering it as a common node; and then include the rest of the edges (i.e. non-selfloop edges) which occur in the same fashion in both the graphs. In this way it ensures that the approximation version exhibits most of the properties of a mcs, while keeping the complexity in a polynomial time. The minimum common supergraph (MCS)(Angelova &amp; Weikum, 2006) is formed using the union of two graphs, i.e. MCS( Gl, G2) = Gl U G2. The distance metrics of Equations 1-3 were used directly without any modifications; the ones of Equations 3-4 were divided by ( |Gl |+ |G2 |) and |MCS( Gl, G2) + mcs( Gl, G2) |respectively to make them normalized, keeping the value of distance metrics within the range [ 0, 1] . It is worthy to note that label matching that is performed during the above mentioned step may not necessarily be exact matching. Rather in this case we have used the WordNet to find an approximate conceptual similarity between two labels. For our ex</context>
</contexts>
<marker>Angelova, Weikum, 2006</marker>
<rawString>Angelova, Ralitsa, &amp; Weikum,Gerhard. (2006). Graph-based Text Classification: Learn from Your Neighbors. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 485–492). New York, NY, USA: ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-Coverage Semantic Analysis with Boxer. In</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings</booktitle>
<pages>277--286</pages>
<publisher>College Publications.</publisher>
<contexts>
<context position="8762" citStr="Bos, 2008" startWordPosition="1457" endWordPosition="1458">It is worthy to note that label matching that is performed during the above mentioned step may not necessarily be exact matching. Rather in this case we have used the WordNet to find an approximate conceptual similarity between two labels. For our experiment we have used the Wu and Palmer‟s conceptual similarity (Wu &amp; Palmer, 1994). If L = lso( cl, c2) , where cland c2 are a pair of concepts corresponding to two words and lso( cl, c2) means the lowest super ordinate then, sim ( cl,c2) WP 2 x depth(L) =len( cl, L) + len( c2, L) + 2 x depth(L) 3.4 Tools Used In order to process texts C&amp;C/Boxer (Bos, 2008; Curran, Clark, &amp; Bos, 2007) a well-known open source tool available as a plugin to Natural Language Toolkit (NLTK) is used. The tool consists of a combinatory categorical grammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Representation Theory (DRT) (Kamp &amp; Reyle, 1993). 4 System Description The method described in the present work, is mainly divided into three major components. The first is the creation of the DRS of the semantic interpretation of the text. The second is the construction of graphs </context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Bos, Johan (2008). Wide-Coverage Semantic Analysis with Boxer. In J. Bos &amp; R. Delmonte (Eds.), Semantics in Text Processing. STEP 2008 Conference Proceedings (pp. 277–286). College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horst Bunke</author>
</authors>
<title>Graph Classification and Clustering Based on Vector Space Embedding (Vol.</title>
<date>2010</date>
<volume>77</volume>
<pages>15--34</pages>
<note>WORLD SCIENTIFIC. doi:doi:10.1142/9789814304726_0002</note>
<contexts>
<context position="5791" citStr="Bunke, 2010" startWordPosition="887" endWordPosition="888">edge&apos;s source and target at-tributes. The id attributes assign numbers to nodes, which are then referenced by source and target. Weights can be represented by the label attribute. 1http://en.wikipedia.org/wiki/Graph_Modelling_Lang uage 3.3 Similarity Metrics for Graphs It has already been mentioned that the objective of the present work is to generate similarity scores among documents of different lexical levels using an approach which integrates a deep linguistic analysis of the documents with graphbased classification algorithms and metrics. Here, five different distance metrics taken from (Bunke, 2010) are utilized for this purpose. They are popularly used in object recognition task, but for text similarity measure they have not yet been used. For two graphs and , if ( ) is the dissimilarity/similarity measure, then this measure would be a distance if has the following properties: ( ) , iff ( ) ( ) ( ) ( ) ( ) The measures used in the present work follow the above rules and the corresponding equations are ( ) |( ) |) |( ) ( |) |( ( ) ||( )| ...(2) ( ) ||( )| ...(3) ( ) |( ) ||( )| ...(4) |( )| ( ) ( ) |( )| In the equations ( ) and ( ) denote maximal common subgraph and minimum common super</context>
</contexts>
<marker>Bunke, 2010</marker>
<rawString>Bunke, Horst (2010). Graph Classification and Clustering Based on Vector Space Embedding (Vol. Volume 77, pp. 15–34). WORLD SCIENTIFIC. doi:doi:10.1142/9789814304726_0002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horst Bunke</author>
<author>Pasquale Foggia</author>
<author>Corrado Guidobaldi</author>
<author>Carlo Sansone</author>
<author>Mario Vento</author>
</authors>
<title>A Comparison of Algorithms for Maximum Common Subgraph on Randomly Connected Graphs. In</title>
<date>2002</date>
<booktitle>Structural, Syntactic, and Statistical Pattern Recognition SE - 12</booktitle>
<volume>2396</volume>
<pages>123--132</pages>
<editor>Kamel (Eds.),</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6627" citStr="Bunke, Foggia, Guidobaldi, Sansone, &amp; Vento, 2002" startWordPosition="1059" endWordPosition="1065">larity/similarity measure, then this measure would be a distance if has the following properties: ( ) , iff ( ) ( ) ( ) ( ) ( ) The measures used in the present work follow the above rules and the corresponding equations are ( ) |( ) |) |( ) ( |) |( ( ) ||( )| ...(2) ( ) ||( )| ...(3) ( ) |( ) ||( )| ...(4) |( )| ( ) ( ) |( )| In the equations ( ) and ( ) denote maximal common subgraph and minimum common super graphs of two graphs and . Theoretically ( ) is the largest graph in terms of edges that is isomorphic to a subgraph of and . The ( ) has been formally defined in a work of Horst Bunke (Bunke, Foggia, Guidobaldi, Sansone, &amp; Vento, 2002). As stated earlier, it is a NP complete problem and actually, the method of finding the () is a brute force method which finds all the subgraphs of both the graphs and select the maximum graph which is common to both. To make the program computationally faster, the program is modified to an approxi376 mate version of mcs( Gl, G2) on the fact that the vertices which exhibit greater similarity in their local structures among the two graphs have a greater probability of inclusion in the mcs() . The two pass approach used in the present work to form the approximate mcs( Gl, G2) is as follows: • </context>
</contexts>
<marker>Bunke, Foggia, Guidobaldi, Sansone, Vento, 2002</marker>
<rawString>Bunke, Horst, Foggia, Pasquale, Guidobaldi, Corrado, Sansone, Carlo, &amp; Vento, Mario (2002). A Comparison of Algorithms for Maximum Common Subgraph on Randomly Connected Graphs. In T. Caelli, A. Amin, R. W. Duin, D. Ridder, &amp; M. Kamel (Eds.), Structural, Syntactic, and Statistical Pattern Recognition SE - 12 (Vol. 2396, pp. 123–132). Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</booktitle>
<pages>33--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic:</location>
<contexts>
<context position="8790" citStr="Curran, Clark, &amp; Bos, 2007" startWordPosition="1459" endWordPosition="1463">y to note that label matching that is performed during the above mentioned step may not necessarily be exact matching. Rather in this case we have used the WordNet to find an approximate conceptual similarity between two labels. For our experiment we have used the Wu and Palmer‟s conceptual similarity (Wu &amp; Palmer, 1994). If L = lso( cl, c2) , where cland c2 are a pair of concepts corresponding to two words and lso( cl, c2) means the lowest super ordinate then, sim ( cl,c2) WP 2 x depth(L) =len( cl, L) + len( c2, L) + 2 x depth(L) 3.4 Tools Used In order to process texts C&amp;C/Boxer (Bos, 2008; Curran, Clark, &amp; Bos, 2007) a well-known open source tool available as a plugin to Natural Language Toolkit (NLTK) is used. The tool consists of a combinatory categorical grammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Representation Theory (DRT) (Kamp &amp; Reyle, 1993). 4 System Description The method described in the present work, is mainly divided into three major components. The first is the creation of the DRS of the semantic interpretation of the text. The second is the construction of graphs in GML from the obtained DRS</context>
<context position="8970" citStr="Curran et al., 2007" startWordPosition="1490" endWordPosition="1493"> conceptual similarity between two labels. For our experiment we have used the Wu and Palmer‟s conceptual similarity (Wu &amp; Palmer, 1994). If L = lso( cl, c2) , where cland c2 are a pair of concepts corresponding to two words and lso( cl, c2) means the lowest super ordinate then, sim ( cl,c2) WP 2 x depth(L) =len( cl, L) + len( c2, L) + 2 x depth(L) 3.4 Tools Used In order to process texts C&amp;C/Boxer (Bos, 2008; Curran, Clark, &amp; Bos, 2007) a well-known open source tool available as a plugin to Natural Language Toolkit (NLTK) is used. The tool consists of a combinatory categorical grammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Representation Theory (DRT) (Kamp &amp; Reyle, 1993). 4 System Description The method described in the present work, is mainly divided into three major components. The first is the creation of the DRS of the semantic interpretation of the text. The second is the construction of graphs in GML from the obtained DRS using some predefined rules. The third one is the classification phase where the different graph distances are assessed using a k-NN classifier (Zhang, Li, Sun, &amp; Nadee, 2013). Th</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>Curran, James, Clark, Stephen, &amp; Bos, Johan (2007). Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions (pp. 33–36). Prague, Czech Republic: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<journal>British Journal Of Hospital Medicine London England</journal>
<volume>71</volume>
<pages>423</pages>
<contexts>
<context position="11783" citStr="Fellbaum, 1998" startWordPosition="1970" endWordPosition="1971">ed based on Equations 1-5. • Learning : We obtained 5 similarity scores for each pair of texts. Our task requires us to assign a score between 0-4 for each pair of text. Hence using the gold standard a K-NN Classifier have been trained to find the output score for a test sample. The value of K has been empirically adjusted using the cross validation technique to find the optimal value. Our method works smoothly for the first two lexical levels. But for the last two levels i.e. phrase to word and word to sense it is not possible to find out DRS for a single word. Hence we have used the WordNet(Fellbaum, 1998) to extract the definition of the word in question and calculate its DRS and proceed with the method. When a word has multiple definitions, all the definitions are fused to a single sentence after conjugating them with the conjunction „or‟. 5 Results and Discussions The JU-Evora system performed fairly in the SemEval Competition 2014. All the correlation scores are not as good as the Baseline(LCS) scores, however it provides a better Pearson correlation score in case of Paragraph to Sentence. The other scores, though not higher, are in the vicinity of the baseline. All the scores are shown bel</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane (1998). WordNet: An Electronic Lexical Database. British Journal Of Hospital Medicine London England 2005 (Vol. 71, p. 423).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Himsolt</author>
<author>Passau</author>
</authors>
<title>GML : A portable Graph File Format. Syntax,</title>
<date>1996</date>
<volume>1</volume>
<pages>11</pages>
<contexts>
<context position="4797" citStr="Himsolt &amp; Passau, 1996" startWordPosition="731" endWordPosition="734">ons. It is also able to separately analyse almost all kinds of events and find out their agent and patient. The main component of DRT is the Discourse Representation Structure (DRS These expressions have two main parts: a) a set of referents, which refer to entities present in the context and b) a set of conditions, which are the relations that exist between the entities. An example of a DRS representation for the sentence &amp;quot;He throws a ball.&amp;quot; is shown below. [ x1, x2, x3: male(x1), ball(x2), throw(x3), event(x3), agent(x3,x1), patient(x3, x2) ] 3.2 GML Structure Graph Modelling Language (GML)(Himsolt &amp; Passau, 1996) is a simple and efficient way to represent weighted directed graphs. A GML file is basically a 7-bit ASCII file, and, as such, can be easily read, parsed, and written. Several open source applications 1 are available that enable viewing and editing GML files. Graphs are represented by the keys viz. graph, node and edge. The basic structure is modelled with the node&apos;s id and the edge&apos;s source and target at-tributes. The id attributes assign numbers to nodes, which are then referenced by source and target. Weights can be represented by the label attribute. 1http://en.wikipedia.org/wiki/Graph_Mo</context>
</contexts>
<marker>Himsolt, Passau, 1996</marker>
<rawString>Himsolt, Michael, &amp; Passau, Universität (1996). GML : A portable Graph File Format. Syntax, 1– 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From discourse to logic: Introduction to model theoretic semantics of natural language, formal logic and discourse representation theory.</title>
<date>1993</date>
<contexts>
<context position="3796" citStr="Kamp &amp; Reyle, 1993" startWordPosition="567" endWordPosition="570">pt. 3 Theoretical Concepts 3.1 Discourse Representation Structures Extraction and representation of the information conveyed by texts can be performed through several approaches, starting from statistical analysis to deep linguistic techniques. In this paper 375 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 375–379, Dublin, Ireland, August 23-24, 2014. we will use a deep linguistic processing sequence: lexical, syntactic, and semantic analysis. One of the most prominent research work on semantic analysis is the Discourse Representation Theory (DRT)(Kamp &amp; Reyle, 1993). In DRT, we aim to associate sentences with expressions in a logical language, which indicate their meaning. In DRT, each sentence is viewed as an update of an existing context, having as result a new context. DRT provides a very powerful platform for the representation of semantic structures of documents including complex relations like implications, propositions and negations. It is also able to separately analyse almost all kinds of events and find out their agent and patient. The main component of DRT is the Discourse Representation Structure (DRS These expressions have two main parts: a)</context>
<context position="9128" citStr="Kamp &amp; Reyle, 1993" startWordPosition="1513" endWordPosition="1516"> where cland c2 are a pair of concepts corresponding to two words and lso( cl, c2) means the lowest super ordinate then, sim ( cl,c2) WP 2 x depth(L) =len( cl, L) + len( c2, L) + 2 x depth(L) 3.4 Tools Used In order to process texts C&amp;C/Boxer (Bos, 2008; Curran, Clark, &amp; Bos, 2007) a well-known open source tool available as a plugin to Natural Language Toolkit (NLTK) is used. The tool consists of a combinatory categorical grammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Representation Theory (DRT) (Kamp &amp; Reyle, 1993). 4 System Description The method described in the present work, is mainly divided into three major components. The first is the creation of the DRS of the semantic interpretation of the text. The second is the construction of graphs in GML from the obtained DRS using some predefined rules. The third one is the classification phase where the different graph distances are assessed using a k-NN classifier (Zhang, Li, Sun, &amp; Nadee, 2013). The algorithm semantic evaluation of text content may be described as follows. • ILTK Module : For each pair of text, to Figure 1: Graphical overview of mcs and</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Kamp, Hans, &amp; Reyle, Uwe (1993). From discourse to logic: Introduction to model theoretic semantics of natural language, formal logic and discourse representation theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting of the Association for Computational Linguistics,,</booktitle>
<volume>32</volume>
<pages>133--138</pages>
<contexts>
<context position="8486" citStr="Wu &amp; Palmer, 1994" startWordPosition="1397" endWordPosition="1400">U G2. The distance metrics of Equations 1-3 were used directly without any modifications; the ones of Equations 3-4 were divided by ( |Gl |+ |G2 |) and |MCS( Gl, G2) + mcs( Gl, G2) |respectively to make them normalized, keeping the value of distance metrics within the range [ 0, 1] . It is worthy to note that label matching that is performed during the above mentioned step may not necessarily be exact matching. Rather in this case we have used the WordNet to find an approximate conceptual similarity between two labels. For our experiment we have used the Wu and Palmer‟s conceptual similarity (Wu &amp; Palmer, 1994). If L = lso( cl, c2) , where cland c2 are a pair of concepts corresponding to two words and lso( cl, c2) means the lowest super ordinate then, sim ( cl,c2) WP 2 x depth(L) =len( cl, L) + len( c2, L) + 2 x depth(L) 3.4 Tools Used In order to process texts C&amp;C/Boxer (Bos, 2008; Curran, Clark, &amp; Bos, 2007) a well-known open source tool available as a plugin to Natural Language Toolkit (NLTK) is used. The tool consists of a combinatory categorical grammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Repres</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Wu, Zhibiao, &amp; Palmer, Martha (1994). Verb semantics and lexical selection. In 32nd Annual Meeting of the Association for Computational Linguistics,, 32, 133–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libiao Zhang</author>
<author>Yuefeng Li</author>
<author>Chao Sun</author>
<author>Nadee</author>
</authors>
<title>Rough Set Based Approach to Text Classification. Web Intelligence (WI) and Intelligent Agent Technologies (IAT),</title>
<date>2013</date>
<booktitle>IEEE/WIC/ACM International Joint Conferences on.</booktitle>
<location>Winai</location>
<contexts>
<context position="9565" citStr="Zhang, Li, Sun, &amp; Nadee, 2013" startWordPosition="1586" endWordPosition="1591">rammar (CCG) (Curran et al., 2007) parser and outputs the semantic representations using discourse representation structures (DRS) of Discourse Representation Theory (DRT) (Kamp &amp; Reyle, 1993). 4 System Description The method described in the present work, is mainly divided into three major components. The first is the creation of the DRS of the semantic interpretation of the text. The second is the construction of graphs in GML from the obtained DRS using some predefined rules. The third one is the classification phase where the different graph distances are assessed using a k-NN classifier (Zhang, Li, Sun, &amp; Nadee, 2013). The algorithm semantic evaluation of text content may be described as follows. • ILTK Module : For each pair of text, to Figure 1: Graphical overview of mcs and MCS: (a), (b) graph representation of sentences meaning “Mary drinks water” and “David drinks water ” respectively, (c) maximum common subgraph, (d) minimum common supergraph. compare their similarity measure we need to find their DRS using the C&amp;C/Boxer toolkit. The toolkit first uses the C&amp;C Parser to find the combinatorial categorical grammar(CCG) of the text. Next the Boxer Module uses the CCG to find the discourse representatio</context>
</contexts>
<marker>Zhang, Li, Sun, Nadee, 2013</marker>
<rawString>Zhang, Libiao, Li, Yuefeng, Sun, Chao, &amp; Nadee, Winai (2013). Rough Set Based Approach to Text Classification. Web Intelligence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>