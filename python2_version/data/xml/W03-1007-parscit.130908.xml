<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000095">
<title confidence="0.993978">
Maximum Entropy Models for FrameNet Classification
</title>
<author confidence="0.981905">
Michael Fleischman, Namhee Kwon and Eduard Hovy
</author>
<affiliation confidence="0.927324">
USC Information Sciences Institute
</affiliation>
<address confidence="0.7688955">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.993369">
{fleisch, nkwon, hovy }@ISI.edu
</email>
<sectionHeader confidence="0.998575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902470588235">
The development of FrameNet, a large
database of semantically annotated sen-
tences, has primed research into statistical
methods for semantic tagging. We ad-
vance previous work by adopting a
Maximum Entropy approach and by using
previous tag information to find the high-
est probability tag sequence for a given
sentence. Further we examine the use of
sentence level syntactic pattern features to
increase performance. We analyze our
strategy on both human annotated and
automatically identified frame elements,
and compare performance to previous
work on identical test data. Experiments
indicate a statistically significant im-
provement (p&lt;0.01) of over 6%.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999678789473684">
Recent work in the development of FrameNet, a
large database of semantically annotated sentences,
has laid the foundation for statistical approaches to
the task of automatic semantic classification.
The FrameNet project seeks to annotate a large
subset of the British National Corpus with seman-
tic information. Annotations are based on Frame
Semantics (Fillmore, 1976), in which frames are
defined as schematic representations of situations
involving various frame elements such as partici-
pants, props, and other conceptual roles.
In each FrameNet sentence, a single target
predicate is identified and all of its relevant frame
elements are tagged with their semantic role (e.g.,
Agent, Judge), their syntactic phrase type (e.g.,
NP, PP), and their grammatical function (e.g., ex-
ternal argument, object argument). Figure 1 shows
an example of an annotated sentence and its appro-
priate semantic frame.
</bodyText>
<figure confidence="0.858547333333333">
Frame: Body-Movement
Frame Elements:
Agent Body Part Cause
She clapped her hands in inspiration.
-NP -NP -PP
-Ext -Obj -Comp
</figure>
<figureCaption confidence="0.989714333333333">
Figure 1. Frame for lemma “clap” shown with three
core frame elements and a sentence annotated with ele-
ment type, phrase type, and grammatical function.
</figureCaption>
<bodyText confidence="0.999917230769231">
As of its first release in June 2002, FrameNet
has made available 49,000 annotated sentences.
The release contains 99,000 annotated frame ele-
ments for 1462 distinct lexical predicates (927
verbs, 339 nouns, and 175 adjectives).
While considerable in scale, the FrameNet da-
tabase does not yet approach the magnitude of re-
sources available for other NLP tasks. Each target
predicate, for example, has on average only 30 sen-
tences tagged. This data sparsity makes the task of
learning a semantic classifier formidable, and in-
creases the importance of the modeling framework
that is employed.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999956583333333">
To our knowledge, Gildea and Jurafsky (2002)
is the only work to use FrameNet to build a statis-
tically based semantic classifier. They split the
problem into two distinct sub-tasks: frame element
identification and frame element classification. In
the identification phase, syntactic information is
extracted from a parse tree to learn the boundaries
of the frame elements in a sentence. In the classi-
fication phase, similar syntactic information is
used to classify those elements into their semantic
roles.
In both phases Gildea and Jurafsky (2002)
build a model of the conditional probabilities of the
classification given a vector of syntactic features.
The full conditional probability is decomposed into
simpler conditional probabilities that are then in-
terpolated to make the classification. Their best
performance on held out test data is achieved using
a linear interpolation model:
ment into one of 120 semantic role categories. The
boundaries of each frame element are given based
on the human annotations in FrameNet. In Section
4, experiments are performed using automatically
identified frame elements.
</bodyText>
<subsectionHeader confidence="0.983992">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.9990144">
For each frame element, features are extracted
from the surface text of the sentence and from an
automatically generated syntactic parse tree
(Collins, 1997). The features used are described
below:
</bodyText>
<equation confidence="0.938668666666667">
m
p(r  |x) = E αi p(r  |xi) • Target predicate (tar): Although there may
=0 be many predicates in a sentence with associ-
</equation>
<bodyText confidence="0.999931090909091">
where r is the class to be predicted, x is the vector
of syntactic features, xi is a subset of those fea-
tures, αi is the weight given to that subset condi-
tional probability (as determined using the EM
algorithm), and m is the total number of subsets
used. Using this method, they report a test set ac-
curacy of 78.5% on classifying semantic roles and
precision/recall scores of .726/.631 on frame ele-
ment identification.
We extend Gildea and Jurafsky (2002)’s initial
effort in three ways. First, we adopt a maximum
entropy (ME) framework in order to learn a more
accurate classification model. Second, we include
features that look at previous tags and use previous
tag information to find the highest probability se-
mantic role sequence for a given sentence. Finally,
we examine sentence-level patterns that exploit
more global information in order to classify frame
elements. We compare the results of our classifier
to that of Gildea and Jurafsky (2002) on matched
test sets of both human annotated and automati-
cally identified frame elements.
</bodyText>
<sectionHeader confidence="0.981668" genericHeader="method">
3 Semantic Role Classification
</sectionHeader>
<bodyText confidence="0.99941575">
Training (36,993 sentences / 75,548 frame ele-
ments), development (4,000 sentences / 8,167
frame elements), and held out test sets (3,865 sen-
tences / 7,899 frame elements) were obtained in
order to exactly match those used in Gildea and
Jurafsky (2002)1. In the experiments presented
below, features are extracted for each frame ele-
ment in a sentence and used to classify that ele-
</bodyText>
<footnote confidence="0.8044135">
1 Data sets (including parse trees) were obtained from Dan
Gildea via personal communication.
</footnote>
<bodyText confidence="0.9989705">
ated frame elements, classification operates on
only one target predicate at a time. The target
predicate is the only feature that is not ex-
tracted from the sentence itself and must be
given by the user. Note that the frame which
the target predicate instantiates is not given,
leaving any word sense ambiguities to be han-
dled implicitly by the classifier.2
</bodyText>
<listItem confidence="0.998490545454546">
• Phrase type (pt): The syntactic phrase type of
the frame element (e.g. NP, PP) is extracted
from the parse tree of the sentence by finding
the constituent in the tree whose boundaries
match the human annotated boundaries of the
element. In cases where there exists no con-
stituent that perfectly matches the element, the
constituent is chosen which matches the largest
text span of the element and has the same left-
most boundary.
• Syntactic head (head): The syntactic heads of
the frame elements are extracted from the
frame element’s matching constituent (as de-
scribed above) using a heuristic method de-
scribed by Michael Collins. 3 This method
extracts the syntactic heads of constituents;
thus, for example, the second frame element in
Figure 1 has head “hands,” while the third
frame element has head “in.”
• Logical Function (lf): A simplification of the
grammatical function annotation (see section
1) is extracted from the parse tree. Unlike the
</listItem>
<footnote confidence="0.9973088">
2 Because of the interaction of head word features with the
target predicate, we suspect that ambiguous lexical items do
not account for much error. This question, however, will be
addressed explicitly in future work.
3 http://www.ai.mit.edu/people/mcollins/papers/heads
</footnote>
<tableCaption confidence="0.997871666666667">
Table 1. Feature sets used in ME frame element classifier. Shows individual feature sets, example feature
function from that set, and total number of feature functions in the set. Examples taken from frame element
“in inspiration,” shown in Figure 1.
</tableCaption>
<table confidence="0.992773882352941">
Number Feature Set Example function Number of Functions
in Feature Set
0 f(r, tar) f(CAUSE, ”clap”)=1 6,518
1 f(r, tar, pt) f(CAUSE, ”clap”, PP)=1 12,030
2 f(r, tar, pt, lf) f(CAUSE, ”clap”, PP, other)=1 14,615
3 f(r, pt, pos, voice) f(CAUSE, NP, ”clap”, active)=1 1,215
4 f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, ”clap”)=1 15,602
5 f(r ,head) f(CAUSE, ”in”)=1 18,504
6 f(r, head, tar) f(CAUSE, ”in”, ”clap”)=1 38,223
7 f(r, head, tar, pt) f(CAUSE, ”in”, ”clap”, PP)=1 39,740
8 f(r, order, syn) f(CAUSE, 2, 13,228
[NP-Ext,Target,NP-Obj,PP-otherJ)=1
9 f(r, tar, order, syn) f(CAUSE, ”clap”, 2, 40,580
[NP-Ext,Target,NP-Obj,PP-otherJ)=1
10 f(r,r -1) f(CAUSE, BODYPART)=1 1,158
11 f(r,r_-1,r_-2) f(CAUSE, BODYPART, AGENT)=1 2,030
Total Number of Features: 203,443
</table>
<bodyText confidence="0.999066625">
full grammatical function, the lf can have only
one of three values: external argument, object
argument, other. A node is considered an ex-
ternal argument if it is an ancestor of an S
node, an object argument if it is an ancestor of
a VP node, and other for all other cases. This
feature is only applied to frame elements
whose phrase type is NP.
</bodyText>
<listItem confidence="0.787628166666667">
• Position (pos): The position of the frame ele-
ment relative to the target (before, after) is ex-
tracted based on the surface text of the
sentence.
• Voice (voice): The voice of the sentence (ac-
tive, passive) is determined using a simple
regular expression passed over the surface text
of the sentence.
• Order (order): The position of the frame ele-
ment relative to the other frame elements in the
sentence. For example, in the sentence from
Figure 1, the element “She” has order=0, while
“in inspiration” has order=2.
• Syntactic pattern (pat): The sentence level
syntactic pattern of the sentence is generated
by looking at the phrase types and logical
functions of each frame element in the sen-
tence. For example, in the sentence: “Alexan-
dra bent her head;” “Alexandra” is an external
argument Noun Phrase, “bent” is a target
predicate, and “her head” is an object argu-
ment Noun Phrase. Thus, the syntactic pattern
associated with the sentence is [NP-ext, target,
NP-obj].
</listItem>
<bodyText confidence="0.964128">
These syntactic patterns can be highly in-
formative for classification. For example, in
the training data, a syntactic pattern of [NP-
ext, target, NP-obj] given the predicate bend
was associated 100% of the time with the
Frame Element pattern: “AGENT TARGET
BODYPART.“
• Previous role (r_n): Frame elements do not
occur in isolation, but rather, depend very
much on the other elements in a sentence.
This dependency can be exploited in classifica-
tion by using the semantic roles of previously
classified frame elements as features in the
classification of a current element. This strat-
egy takes advantage of the fact that, for exam-
ple, if a frame element is tagged as an AGENT
it is highly unlikely that the next element will
also be an AGENT.
The previous role feature indicates the
classification that the n-previous frame ele-
ment received. During training, this informa-
tion is provided by simply looking at the true
classes of the frame element occurring n posi-
tions before the target element. During testing,
hypothesized classes of the n elements are used
and Viterbi search is performed to find the
most probable tag sequence for a sentence.
</bodyText>
<subsectionHeader confidence="0.999294">
3.2 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.999986791666667">
ME models implement the intuition that the best
model will be the one that is consistent with the set
of constrains imposed by the evidence, but other-
wise is as uniform as possible (Berger et al., 1996).
We model the probability of a semantic role r
given a vector of features x according to the ME
formulation below:
Here Zx is a normalization constant, fi(r,x) is a fea-
ture function which maps each role and vector
element (or combination of elements) to a binary
value, n is the total number of feature functions,
and Ai is the weight for a given feature function.
The final classification is just the role with highest
probability given its feature vector and the model.
The feature functions that we employ can be
divided into feature sets based upon the types and
combinations of features on which they operate.
Table 1 lists the feature sets that we use, as well as
the number of individual feature functions they
contain. The feature combinations were chosen
based both on previous work and trial and error. In
future work we will examine more principled fea-
ture selection techniques.
It is important to note that the feature functions
described here are not equivalent to the subset
conditional distributions that are used in the Gildea
and Jurafsky model. ME models are log-linear
models in which feature functions map specific
instances of syntactic features and classes to binary
values (e.g., if a training element has head=”in”
and role=CAUSE, then, for that element, the feature
function f(CAUSE, ”in”) will equal 1). Thus, ME is
not here being used as another way to find weights
for an interpolated model. Rather, the ME ap-
proach provides an overarching framework in
which the full distribution of semantic roles given
syntactic features can be modeled.
We train the ME models using the GIS algo-
rithm (Darroch and Ratcliff, 1972) as implemented
in the YASMET ME package (Och, 2002). We
use the YASMET MEtagger (Bender et al., 2003)
to perform the Viterbi search. The classifier was
trained until performance on the development set
ceased to improve. Feature weights were
smoothed using Gaussian priors with mean 0
(Chen and Rosenfeld, 1999). The standard devia-
tion of this distribution was optimized on the de-
velopment set for each experiment.
</bodyText>
<subsectionHeader confidence="0.969404">
3.3 Experiments
</subsectionHeader>
<bodyText confidence="0.99978372">
We present three experiments in which different
feature sets are used to train the ME classifier. The
first experiment uses only those feature combina-
tions described in Gildea and Jurafsky (2002) (fea-
ture sets 0-7 from Table 1). The second
experiment uses a super set of the first and incor-
porates the syntactic pattern features described
above (feature sets 0-9). The final experiment uses
the previous tags and implements Viterbi search to
find the best tag sequence (feature sets 0-11).
We further investigate the effect of varying two
aspects of classifier training: the standard deviation
of the Gaussian priors used for smoothing, and the
number of sentences used for training. To examine
the effect of optimizing the standard deviation, a
range of values was chosen and a classifier was
trained using each value until performance on a
development set ceased to improve.
To examine the effect of training set size on
performance, five data sets were generated from
the original set with 36, 367, 3674, 7349, and
24496 sentences, respectively. These data sets
were created by going through the original set and
selecting every thousandth, hundredth, tenth, fifth,
and every second and third sentence, respectively.
</bodyText>
<figureCaption confidence="0.9967866">
Figure 2. Performance of models on test data using
hand annotated frame element boundaries. G&amp;J refers
to the results of Gildea and Jurafsky (2002). Exp 1 in-
corporates feature sets 0-7 from Table 1; Exp 2 feature
sets 0-9; Exp 3 features 0-11.
</figureCaption>
<sectionHeader confidence="0.820584" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.993459">
Figure 2 shows the results of our experiments
alongside those of (Gildea and Jurafsky, 2002) on
identical held out test sets. The difference in per-
formance between each classifier is statistically
significant at (p&lt;0.01) (Mitchell, 1997), with the
</bodyText>
<equation confidence="0.94904525">
n
p r x
(  |) = 1 Z exp[J Aif (r, x)]x
i=0
</equation>
<figure confidence="0.928932846153846">
Classifier Performance on Test Set
G&amp;J Exp 1 Exp 2 Exp 3
% Correct
86
84
82
80
78
76
78.5
81.7
83.6
84.7
</figure>
<bodyText confidence="0.910741">
exception of Exp 2 and Exp 3, whose difference is
statistically significant at (p&lt;0.05).
</bodyText>
<tableCaption confidence="0.9511475">
Table 2. Effect of different smoothing parameter (std.
dev.) values on classification performance.
</tableCaption>
<table confidence="0.98732575">
Std. Dev. % Correct
1 79.9
2 82.1
4 81.9
</table>
<bodyText confidence="0.653209333333333">
Table 2 shows the effect of varying the stan-
dard deviation of the Gaussian priors used for
smoothing in Experiment 1. The difference in per-
formance between the classifiers trained using
standard deviation 1 and 2 is statistically signifi-
cant at (p&lt;0.01).
</bodyText>
<figureCaption confidence="0.999089">
Figure 3. Effect of training set size on semantic role
classification.
</figureCaption>
<bodyText confidence="0.929994083333333">
Figure 3 shows the change in performance as a
function of training set size. Classifiers were
trained using the full set of features described for
Experiment 3.
Table 3 shows the confusion matrix for a subset
of semantic roles. Five roles were chosen for pres-
entation based upon their high contribution to clas-
sifier error. Confusion between these five account
for 27% of all errors made amongst the 120 possi-
ble roles. The tenth role, other, represents the sum
of the remaining 115 roles. Table 4 presents ex-
ample errors for five of the most confused roles.
</bodyText>
<subsectionHeader confidence="0.531032">
3.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999740611111111">
It is clear that the ME models improve perform-
ance on frame element classification. There are a
number of reasons for this improvement.
First, for this task the log-linear model employed
in the ME framework is better than the linear
interpolation model used by Gildea and Jurafsky.
One possible reason for this is that semantic role
classification benefits from the ME model’s bias
for more uniform probability distributions that sat-
isfy the constraints placed on the model by the
training data.
Another reason for improved performance comes
from ME’s simpler design. Instead of having to
worry about finding proper backoff strategies
amongst distributions of features subsets, ME al-
lows one to include many features in a single
model and automatically adjusts the weights of
these features appropriately.
</bodyText>
<tableCaption confidence="0.99587625">
Table 3. Confusion matrix for five roles which contrib-
ute most to overall system error. Columns refer to ac-
tual role. Rows refer to the model’s hypothesis. Other
refers to combination of all other roles.
</tableCaption>
<table confidence="0.999640625">
Area Spkr Goal Msg Path Other Prec.
Area 98 6 18 16 0.710
Spkr 373 23 41 0.853
Goal 11 431 28 50 0.828
Msg 18 1 315 33 0.858
Path 32 36 415 41 0.791
Other 15 21 26 24 33 5784 0.979
Recall 0.628 0.905 0.862 0.87 0.84 0.969
</table>
<bodyText confidence="0.999940916666667">
Also, because the ME models find weights for
many thousands of features, they have many more
degrees of freedom than the linear interpolated
models of Gildea and Jurafsky. Although many
degrees of freedom can lead to overfitting of the
training data, the smoothing procedure employed
in our experiments helps to counteract this prob-
lem. As evidenced in Table 2, by optimizing the
standard deviation used in smoothing the ME
models are able to show significant increases in
performance on held out test data.
Finally, by including in our model sentence-
level pattern features and information about previ-
ous classes, global information can be exploited for
improved classification. The accuracy gained by
including such global information confirms the
intuition that the semantic role of an element is
much related to the entire sentence of which it is a
part.
Having discussed the advantages of the models
presented here, it is interesting to look at the errors
that the system makes. It is clear from the confu-
sion matrix in Table 3 that a great deal of the sys-
tem error comes from relatively few semantic
</bodyText>
<figure confidence="0.974694666666667">
% Correct
40%
20%
90%
80%
70%
60%
50%
30%
10%
10 100 1000 10000 100000
# Sentences in Training
</figure>
<bodyText confidence="0.9975116">
roles.4 Table 4 offers some insight into why these
errors occur. For example, the confusions exem-
plified in 1 and 2 are both due to the fact that the
particular phrases employed can be used in multi-
ple roles (including the roles hypothesized by the
system). Thus, while “across the counter” may be
considered a goal when one is talking about a per-
son and their head, the same phrase would be con-
sidered a path if one were talking about a mouse
who is running.
</bodyText>
<tableCaption confidence="0.9900915">
Table 4. Example errors for five of the most often con-
fused semantic roles
</tableCaption>
<table confidence="0.962060117647059">
Actual Proposed Example Sentence
1 Goal Path The barman craned his head
across the counter.
2 Area Path Mr. Glass began hallucinating,
throwing books around the
classroom.
3 Message Speaker Debate lasted until 20 Septem-
ber, opposition being voiced
by a number of Italian and
Spanish prelates.
4 Addressee Speaker Furious staff claim they were
even called in from holiday to
be grilled by a specialist secu-
rity firm
5 Reason Evaluee We cannot but admire the
efficiency with which she
took control of her own life.
</table>
<bodyText confidence="0.999913636363637">
Examples 3 and 4, while showing phrases with
similar confusions, stand out as being errors caused
by an inability to deal with passive sentences.
Such errors are not unexpected; for, even though
the voice of the sentence is an explicit feature, the
system suffers from the paucity of passive sen-
tences in the data (approximately 5%).
Finally, example 5 shows an error that is based
on the difficult nature of the decision itself (i.e., it
is unclear whether “the efficiency” is the reason for
admiration, or what is being admired). Often
times, phrases are assigned semantic roles that are
not obvious even to human evaluators. In such
cases it is difficult to determine what information
might be useful for the system.
Having looked at the types of errors that are
common for the system, it becomes interesting to
examine what strategy may be best to overcome
such errors. Aside from new features, one solution
is obvious: more data. The curve in Figure 2
shows that there is still a great deal of performance
to be gained by training the current ME models on
</bodyText>
<page confidence="0.434462">
4 44% of all error is due to confusion between only nine roles.
</page>
<bodyText confidence="0.9998442">
more data. The slope of the curve indicates that
we are far from a plateau, and that even constant
increases in the amount of available training data
may push classifier performance above 90% accu-
racy.
Having demonstrated the effectiveness of the
ME approach on frame element classification
given hand annotated frame element boundaries,
we next examine the value of the approach given
automatically identified boundaries.
</bodyText>
<sectionHeader confidence="0.966593" genericHeader="method">
4 Frame Element Identification
</sectionHeader>
<bodyText confidence="0.999959222222222">
Gildea and Jurafsky equate the task of locating
frame element boundaries to one of identifying
frame elements amongst the parse tree constituents
of a given sentence. Because not all frame element
boundaries exactly match constituent boundaries,
this approach can perform no better than 86.9%
(i.e. the number of elements that match constitu-
ents (6864) divided by the total number of ele-
ments (7899)) on the test set.
</bodyText>
<subsectionHeader confidence="0.959983">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.998124285714286">
Frame element identification is a binary classifica-
tion problem in which each constituent in a parse
tree is described by a feature vector and, based on
that vector, tagged as either a frame element or not.
In generating feature vectors we use a subset of the
features described for role tagging as well as an
additional path feature.
</bodyText>
<figureCaption confidence="0.982199">
Figure 4. Generation of path features used in frame
element tagging. The path from the constituent “in in-
spiration” to the target predicate “clapped” is repre-
sented as the string PP↑VP↓VBD.
</figureCaption>
<bodyText confidence="0.993745666666667">
Gildea and Jurafsky introduce the path feature
in order to capture the structural relationship be-
tween a constituent and the target predicate. The
</bodyText>
<tableCaption confidence="0.907399">
Table 5. Results of frame element identification. G&amp;J represents results reported in (Gildea and Jurafsky, 2002),
ME results for the experiments reported here. The second column shows precision, recall, and F-scores for the task
of frame element identification, the third column for the combined task of identification and classification.
</tableCaption>
<table confidence="0.99383175">
Method FE ID only FE ID + FE Classification
Precision Recall F-Score Precision Recall F-Score
G&amp;J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551
ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576
</table>
<bodyText confidence="0.998318666666667">
path of a constituent is represented by the nodes
through which one passes while traveling up the
tree from the constituent and then down through
the governing category to the target. Figure 4
shows an example of this feature for a frame ele-
ment from the sentence presented in Figure 1.
</bodyText>
<subsectionHeader confidence="0.59075">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.99996275">
We use the ME formulation described in Section
3.2 to build a binary classifier. The classifier fea-
tures follow closely those used in Gildea and Juraf-
sky. We model the data using the feature sets: f(fe,
path), f(fe, path, tar), and f(fe, head, tar), where fe
represents the binary classification of the constitu-
ent. While this experiment only uses three feature
sets, the heterogeneity of the path feature is so
great that the classifier itself uses 1,119,331 unique
binary features.
With the constituents having been labeled, we
apply the ME frame element classifier described
above. Results are presented using the classifier of
Experiment 1, described in section 3.3. We then
investigate the effect of varying the number of
constituents used for training on identification per-
formance. Five data sets of approximately 100,000
10,000, 1,000, and 100 constituents were generated
from the original set by random selection and used
to train ME models as described above.
</bodyText>
<subsectionHeader confidence="0.784284">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.99988224">
Table 5 compares the results of Gildea and Juraf-
sky (2002) and the ME frame element identifier on
both the task of frame element identification alone,
and the combined task of frame element identifica-
tion and classification. In order to be counted cor-
rect on the combined task, the constituent must
have been correctly identified as a frame element,
and then must have been correctly classified into
one of the 120 semantic categories.
Recall is calculated based on the total number
of frame elements in the test set, not on the total
number of elements that have matching parse con-
stituents. Thus, the upper limit is 86.9%, not
100%. Precision is calculated as the number of
correct positive classifications divided by the num-
ber of total positive classifications.
The difference in the F-scores on the identifica-
tion task alone and on the combined task are statis-
tically significant at the (p&lt;0.01) level 5 . The
accuracy of the ME semantic classifier on the
automatically identified frame elements is 81.5%,
not a statistically significant difference from its
performance on hand labeled elements, but a statis-
tically significant difference from the classifier of
Gildea and Jurafsky (2002) (p&lt;0.01).
</bodyText>
<figureCaption confidence="0.993605">
Figure 5. Effect of training set size on frame element
boundary identification.
</figureCaption>
<bodyText confidence="0.98240775">
Figure 5 shows the results of varying the train-
ing set size on identification performance. For
each data set, thresholds were chosen to maximize
F-Score.
</bodyText>
<subsectionHeader confidence="0.899066">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99987175">
It is clear from the results above that the perform-
ance of the ME model for frame element classifica-
tion is robust to the use of automatically identified
frame element boundaries. Further, the ME
</bodyText>
<footnote confidence="0.9874515">
5 G&amp;J’s results for the combined task were generated with a
threshold applied to the FE classifier (Dan Gildea, personal
communication). This is why their precision/recall scores are
dissimilar to their accuracy scores, as reported in section 3.
Because the ME classifier does not employ a threshold, com-
parisons must be based on F-score.
</footnote>
<figure confidence="0.986714833333333">
F-Score
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
100 1000 10000 100000 1000000
# Constituents in Training
</figure>
<bodyText confidence="0.999854928571429">
framework yields better results on the frame ele-
ment identification task than the simple linear in-
terpolation model of Gildea and Jurafsky. This
result is not surprising given the discussion in Sec-
tion 3.
What is striking, however, is the drastic overall
reduction in performance on the combined
identification and classification task. The
bottleneck here is the identification of frame
element boundaries. Unlike with classification
though, Figure 5 indicates that a plateau in the
learning curve has been reached, and thus, more
data will not yield as dramatic an improvement for
the given feature set and model.
</bodyText>
<sectionHeader confidence="0.999775" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998590625">
The results reported here show that ME models
provide higher performance on frame element clas-
sification tasks, given both human and automati-
cally identified frame element boundaries, than the
linear interpolation models examined in previous
work. We attribute this increase to the benefits of
the ME framework itself, the incorporation of sen-
tence-level syntactic patterns into our feature set,
and the use of previous tag information to find the
most probable sequence of roles for a sentence.
But perhaps most striking in our results are the
effects of varying training set size on the perform-
ance of the classification and identification models.
While for classification, the learning curve appears
to be still increasing with training set size, the
learning curve for identification appears to have
already begun to plateau. This suggests that while
classification will continue to improve as the Fra-
meNet database gets larger, increased performance
on identification will rely on the development of
more sophisticated models.
In future work, we intend to apply the lessons
learned here to the problem of frame element iden-
tification. Gildea and Jurafsky have shown that
improvements in identification can be had by more
closely integrating the task with classification (they
report an F-Score of .719 using an integrated
model). We are currently exploring a ME ap-
proach which integrates these two tasks under a
tagging framework. Initial results show that sig-
nificant improvements can be had using techniques
similar to those described above.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999832">
The authors would like to thank Dan Gildea who
generously allowed us access to his data files and
Oliver Bender for making the MEtagger software
available. Finally, we thank Franz Och whose help
and expertise was invaluable.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999744264705883">
O. Bender, K. Macherey, F. J. Och, and H. Ney.
2003. Comparison of Alignment Templates and
Maximum Entropy Models for Natural Lan-
guage Processing. Proc. of EACL-2003. Buda-
pest, Hungary.
A. Berger, S. Della Pietra and V. Della Pietra,
1996. A Maximum Entropy Approach to Natu-
ral Language Processing. Computational Lin-
guistics, vol. 22, no. 1.
S. F. Chen and R. Rosenfeld. 1999. A Gaussian
prior for smoothing maximum entropy models.
Technical Report CMUCS -99-108, Carnegie
Mellon University
M. Collins. 1997. Three generative, lexicalized
models for statistical parsing. Proc. of the 35th
Annual Meeting of the ACL. pages 16-23, Ma-
drid, Spain.
J. N. Darroch and D. Ratcliff. 1972. Generalized
iterative scaling for log-linear models. Annals
of Mathematical Statistics, 43:1470-1480.
C. Fillmore 1976. Frame semantics and the nature
of language. Annals of the New York Academy
of Sciences: Conference on the Origin and De-
velopment of Language and Speech, Volume
280 (pp. 20-32).
D. Gildea and D. Jurafsky. 2002. Automatic La-
beling of Semantic Roles, Computational Lin-
guistics, 28(3) 245-288 14.
T. Mitchell. 1997. Machine Learning. McGraw-
Hill International Editions, New York, NY.
Pages 143-145.
F.J. Och. 2002. Yet another maxent toolkit:
YASMET. www-i6.informatik.rwth-
aachen.de/Colleagues/och/.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805006">
<title confidence="0.999951">Maximum Entropy Models for FrameNet Classification</title>
<author confidence="0.998436">Michael Fleischman</author>
<author confidence="0.998436">Namhee Kwon</author>
<author confidence="0.998436">Eduard</author>
<affiliation confidence="0.99894">USC Information Sciences</affiliation>
<address confidence="0.994813">4676 Admiralty Marina del Rey, CA 90292-6695</address>
<email confidence="0.994843">nkwon,hovy}@ISI.edu</email>
<abstract confidence="0.989827444444444">The development of FrameNet, a large database of semantically annotated sentences, has primed research into statistical methods for semantic tagging. We advance previous work by adopting a Maximum Entropy approach and by using previous tag information to find the highest probability tag sequence for a given sentence. Further we examine the use of sentence level syntactic pattern features to increase performance. We analyze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p&lt;0.01) of over 6%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bender</author>
<author>K Macherey</author>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<date>2003</date>
<booktitle>Comparison of Alignment Templates and Maximum Entropy Models for Natural Language Processing. Proc. of EACL-2003.</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="12718" citStr="Bender et al., 2003" startWordPosition="2050" endWordPosition="2053"> specific instances of syntactic features and classes to binary values (e.g., if a training element has head=”in” and role=CAUSE, then, for that element, the feature function f(CAUSE, ”in”) will equal 1). Thus, ME is not here being used as another way to find weights for an interpolated model. Rather, the ME approach provides an overarching framework in which the full distribution of semantic roles given syntactic features can be modeled. We train the ME models using the GIS algorithm (Darroch and Ratcliff, 1972) as implemented in the YASMET ME package (Och, 2002). We use the YASMET MEtagger (Bender et al., 2003) to perform the Viterbi search. The classifier was trained until performance on the development set ceased to improve. Feature weights were smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999). The standard deviation of this distribution was optimized on the development set for each experiment. 3.3 Experiments We present three experiments in which different feature sets are used to train the ME classifier. The first experiment uses only those feature combinations described in Gildea and Jurafsky (2002) (feature sets 0-7 from Table 1). The second experiment uses a super set of </context>
</contexts>
<marker>Bender, Macherey, Och, Ney, 2003</marker>
<rawString>O. Bender, K. Macherey, F. J. Och, and H. Ney. 2003. Comparison of Alignment Templates and Maximum Entropy Models for Natural Language Processing. Proc. of EACL-2003. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="10972" citStr="Berger et al., 1996" startWordPosition="1756" endWordPosition="1759">evious role feature indicates the classification that the n-previous frame element received. During training, this information is provided by simply looking at the true classes of the frame element occurring n positions before the target element. During testing, hypothesized classes of the n elements are used and Viterbi search is performed to find the most probable tag sequence for a sentence. 3.2 Maximum Entropy ME models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible (Berger et al., 1996). We model the probability of a semantic role r given a vector of features x according to the ME formulation below: Here Zx is a normalization constant, fi(r,x) is a feature function which maps each role and vector element (or combination of elements) to a binary value, n is the total number of feature functions, and Ai is the weight for a given feature function. The final classification is just the role with highest probability given its feature vector and the model. The feature functions that we employ can be divided into feature sets based upon the types and combinations of features on whic</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra and V. Della Pietra, 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, vol. 22, no. 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS -99-108,</tech>
<institution>Carnegie Mellon University</institution>
<contexts>
<context position="12927" citStr="Chen and Rosenfeld, 1999" startWordPosition="2082" endWordPosition="2085"> Thus, ME is not here being used as another way to find weights for an interpolated model. Rather, the ME approach provides an overarching framework in which the full distribution of semantic roles given syntactic features can be modeled. We train the ME models using the GIS algorithm (Darroch and Ratcliff, 1972) as implemented in the YASMET ME package (Och, 2002). We use the YASMET MEtagger (Bender et al., 2003) to perform the Viterbi search. The classifier was trained until performance on the development set ceased to improve. Feature weights were smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999). The standard deviation of this distribution was optimized on the development set for each experiment. 3.3 Experiments We present three experiments in which different feature sets are used to train the ME classifier. The first experiment uses only those feature combinations described in Gildea and Jurafsky (2002) (feature sets 0-7 from Table 1). The second experiment uses a super set of the first and incorporates the syntactic pattern features described above (feature sets 0-9). The final experiment uses the previous tags and implements Viterbi search to find the best tag sequence (feature se</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>S. F. Chen and R. Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS -99-108, Carnegie Mellon University</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>Proc. of the 35th Annual Meeting of the ACL.</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="3965" citStr="Collins, 1997" startWordPosition="600" endWordPosition="601">ional probability is decomposed into simpler conditional probabilities that are then interpolated to make the classification. Their best performance on held out test data is achieved using a linear interpolation model: ment into one of 120 semantic role categories. The boundaries of each frame element are given based on the human annotations in FrameNet. In Section 4, experiments are performed using automatically identified frame elements. 3.1 Features For each frame element, features are extracted from the surface text of the sentence and from an automatically generated syntactic parse tree (Collins, 1997). The features used are described below: m p(r |x) = E αi p(r |xi) • Target predicate (tar): Although there may =0 be many predicates in a sentence with associwhere r is the class to be predicted, x is the vector of syntactic features, xi is a subset of those features, αi is the weight given to that subset conditional probability (as determined using the EM algorithm), and m is the total number of subsets used. Using this method, they report a test set accuracy of 78.5% on classifying semantic roles and precision/recall scores of .726/.631 on frame element identification. We extend Gildea and </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalized models for statistical parsing. Proc. of the 35th Annual Meeting of the ACL. pages 16-23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="12616" citStr="Darroch and Ratcliff, 1972" startWordPosition="2032" endWordPosition="2035">hat are used in the Gildea and Jurafsky model. ME models are log-linear models in which feature functions map specific instances of syntactic features and classes to binary values (e.g., if a training element has head=”in” and role=CAUSE, then, for that element, the feature function f(CAUSE, ”in”) will equal 1). Thus, ME is not here being used as another way to find weights for an interpolated model. Rather, the ME approach provides an overarching framework in which the full distribution of semantic roles given syntactic features can be modeled. We train the ME models using the GIS algorithm (Darroch and Ratcliff, 1972) as implemented in the YASMET ME package (Och, 2002). We use the YASMET MEtagger (Bender et al., 2003) to perform the Viterbi search. The classifier was trained until performance on the development set ceased to improve. Feature weights were smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999). The standard deviation of this distribution was optimized on the development set for each experiment. 3.3 Experiments We present three experiments in which different feature sets are used to train the ME classifier. The first experiment uses only those feature combinations described in </context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
</authors>
<title>Frame semantics and the nature of language. Annals of the New York Academy</title>
<date>1976</date>
<booktitle>of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<pages>(pp.</pages>
<contexts>
<context position="1263" citStr="Fillmore, 1976" startWordPosition="182" endWordPosition="183">ze our strategy on both human annotated and automatically identified frame elements, and compare performance to previous work on identical test data. Experiments indicate a statistically significant improvement (p&lt;0.01) of over 6%. 1 Introduction Recent work in the development of FrameNet, a large database of semantically annotated sentences, has laid the foundation for statistical approaches to the task of automatic semantic classification. The FrameNet project seeks to annotate a large subset of the British National Corpus with semantic information. Annotations are based on Frame Semantics (Fillmore, 1976), in which frames are defined as schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles. In each FrameNet sentence, a single target predicate is identified and all of its relevant frame elements are tagged with their semantic role (e.g., Agent, Judge), their syntactic phrase type (e.g., NP, PP), and their grammatical function (e.g., external argument, object argument). Figure 1 shows an example of an annotated sentence and its appropriate semantic frame. Frame: Body-Movement Frame Elements: Agent Body Part Cause She clap</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>C. Fillmore 1976. Frame semantics and the nature of language. Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, Volume 280 (pp. 20-32).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles,</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<contexts>
<context position="2722" citStr="Gildea and Jurafsky (2002)" startWordPosition="410" endWordPosition="413">e in June 2002, FrameNet has made available 49,000 annotated sentences. The release contains 99,000 annotated frame elements for 1462 distinct lexical predicates (927 verbs, 339 nouns, and 175 adjectives). While considerable in scale, the FrameNet database does not yet approach the magnitude of resources available for other NLP tasks. Each target predicate, for example, has on average only 30 sentences tagged. This data sparsity makes the task of learning a semantic classifier formidable, and increases the importance of the modeling framework that is employed. 2 Related Work To our knowledge, Gildea and Jurafsky (2002) is the only work to use FrameNet to build a statistically based semantic classifier. They split the problem into two distinct sub-tasks: frame element identification and frame element classification. In the identification phase, syntactic information is extracted from a parse tree to learn the boundaries of the frame elements in a sentence. In the classification phase, similar syntactic information is used to classify those elements into their semantic roles. In both phases Gildea and Jurafsky (2002) build a model of the conditional probabilities of the classification given a vector of syntac</context>
<context position="4580" citStr="Gildea and Jurafsky (2002)" startWordPosition="709" endWordPosition="712">lins, 1997). The features used are described below: m p(r |x) = E αi p(r |xi) • Target predicate (tar): Although there may =0 be many predicates in a sentence with associwhere r is the class to be predicted, x is the vector of syntactic features, xi is a subset of those features, αi is the weight given to that subset conditional probability (as determined using the EM algorithm), and m is the total number of subsets used. Using this method, they report a test set accuracy of 78.5% on classifying semantic roles and precision/recall scores of .726/.631 on frame element identification. We extend Gildea and Jurafsky (2002)’s initial effort in three ways. First, we adopt a maximum entropy (ME) framework in order to learn a more accurate classification model. Second, we include features that look at previous tags and use previous tag information to find the highest probability semantic role sequence for a given sentence. Finally, we examine sentence-level patterns that exploit more global information in order to classify frame elements. We compare the results of our classifier to that of Gildea and Jurafsky (2002) on matched test sets of both human annotated and automatically identified frame elements. 3 Semantic</context>
<context position="13242" citStr="Gildea and Jurafsky (2002)" startWordPosition="2132" endWordPosition="2135"> as implemented in the YASMET ME package (Och, 2002). We use the YASMET MEtagger (Bender et al., 2003) to perform the Viterbi search. The classifier was trained until performance on the development set ceased to improve. Feature weights were smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999). The standard deviation of this distribution was optimized on the development set for each experiment. 3.3 Experiments We present three experiments in which different feature sets are used to train the ME classifier. The first experiment uses only those feature combinations described in Gildea and Jurafsky (2002) (feature sets 0-7 from Table 1). The second experiment uses a super set of the first and incorporates the syntactic pattern features described above (feature sets 0-9). The final experiment uses the previous tags and implements Viterbi search to find the best tag sequence (feature sets 0-11). We further investigate the effect of varying two aspects of classifier training: the standard deviation of the Gaussian priors used for smoothing, and the number of sentences used for training. To examine the effect of optimizing the standard deviation, a range of values was chosen and a classifier was t</context>
<context position="14615" citStr="Gildea and Jurafsky, 2002" startWordPosition="2356" endWordPosition="2359">ata sets were generated from the original set with 36, 367, 3674, 7349, and 24496 sentences, respectively. These data sets were created by going through the original set and selecting every thousandth, hundredth, tenth, fifth, and every second and third sentence, respectively. Figure 2. Performance of models on test data using hand annotated frame element boundaries. G&amp;J refers to the results of Gildea and Jurafsky (2002). Exp 1 incorporates feature sets 0-7 from Table 1; Exp 2 feature sets 0-9; Exp 3 features 0-11. 3.4 Results Figure 2 shows the results of our experiments alongside those of (Gildea and Jurafsky, 2002) on identical held out test sets. The difference in performance between each classifier is statistically significant at (p&lt;0.01) (Mitchell, 1997), with the n p r x ( |) = 1 Z exp[J Aif (r, x)]x i=0 Classifier Performance on Test Set G&amp;J Exp 1 Exp 2 Exp 3 % Correct 86 84 82 80 78 76 78.5 81.7 83.6 84.7 exception of Exp 2 and Exp 3, whose difference is statistically significant at (p&lt;0.05). Table 2. Effect of different smoothing parameter (std. dev.) values on classification performance. Std. Dev. % Correct 1 79.9 2 82.1 4 81.9 Table 2 shows the effect of varying the standard deviation of the Ga</context>
<context position="22321" citStr="Gildea and Jurafsky, 2002" startWordPosition="3672" endWordPosition="3675">vector, tagged as either a frame element or not. In generating feature vectors we use a subset of the features described for role tagging as well as an additional path feature. Figure 4. Generation of path features used in frame element tagging. The path from the constituent “in inspiration” to the target predicate “clapped” is represented as the string PP↑VP↓VBD. Gildea and Jurafsky introduce the path feature in order to capture the structural relationship between a constituent and the target predicate. The Table 5. Results of frame element identification. G&amp;J represents results reported in (Gildea and Jurafsky, 2002), ME results for the experiments reported here. The second column shows precision, recall, and F-scores for the task of frame element identification, the third column for the combined task of identification and classification. Method FE ID only FE ID + FE Classification Precision Recall F-Score Precision Recall F-Score G&amp;J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551 ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576 path of a constituent is represented by the nodes through which one passes while traveling up the tree from the constituent and then down through the</context>
<context position="24115" citStr="Gildea and Jurafsky (2002)" startWordPosition="3966" endWordPosition="3970">eat that the classifier itself uses 1,119,331 unique binary features. With the constituents having been labeled, we apply the ME frame element classifier described above. Results are presented using the classifier of Experiment 1, described in section 3.3. We then investigate the effect of varying the number of constituents used for training on identification performance. Five data sets of approximately 100,000 10,000, 1,000, and 100 constituents were generated from the original set by random selection and used to train ME models as described above. 4.3 Results Table 5 compares the results of Gildea and Jurafsky (2002) and the ME frame element identifier on both the task of frame element identification alone, and the combined task of frame element identification and classification. In order to be counted correct on the combined task, the constituent must have been correctly identified as a frame element, and then must have been correctly classified into one of the 120 semantic categories. Recall is calculated based on the total number of frame elements in the test set, not on the total number of elements that have matching parse constituents. Thus, the upper limit is 86.9%, not 100%. Precision is calculated</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic Labeling of Semantic Roles, Computational Linguistics, 28(3) 245-288 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<title>Machine Learning.</title>
<date>1997</date>
<pages>143--145</pages>
<publisher>McGrawHill International Editions,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="14760" citStr="Mitchell, 1997" startWordPosition="2379" endWordPosition="2380"> original set and selecting every thousandth, hundredth, tenth, fifth, and every second and third sentence, respectively. Figure 2. Performance of models on test data using hand annotated frame element boundaries. G&amp;J refers to the results of Gildea and Jurafsky (2002). Exp 1 incorporates feature sets 0-7 from Table 1; Exp 2 feature sets 0-9; Exp 3 features 0-11. 3.4 Results Figure 2 shows the results of our experiments alongside those of (Gildea and Jurafsky, 2002) on identical held out test sets. The difference in performance between each classifier is statistically significant at (p&lt;0.01) (Mitchell, 1997), with the n p r x ( |) = 1 Z exp[J Aif (r, x)]x i=0 Classifier Performance on Test Set G&amp;J Exp 1 Exp 2 Exp 3 % Correct 86 84 82 80 78 76 78.5 81.7 83.6 84.7 exception of Exp 2 and Exp 3, whose difference is statistically significant at (p&lt;0.05). Table 2. Effect of different smoothing parameter (std. dev.) values on classification performance. Std. Dev. % Correct 1 79.9 2 82.1 4 81.9 Table 2 shows the effect of varying the standard deviation of the Gaussian priors used for smoothing in Experiment 1. The difference in performance between the classifiers trained using standard deviation 1 and 2 </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>T. Mitchell. 1997. Machine Learning. McGrawHill International Editions, New York, NY. Pages 143-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<date>2002</date>
<note>Yet another maxent toolkit: YASMET. www-i6.informatik.rwthaachen.de/Colleagues/och/.</note>
<contexts>
<context position="12668" citStr="Och, 2002" startWordPosition="2043" endWordPosition="2044">ar models in which feature functions map specific instances of syntactic features and classes to binary values (e.g., if a training element has head=”in” and role=CAUSE, then, for that element, the feature function f(CAUSE, ”in”) will equal 1). Thus, ME is not here being used as another way to find weights for an interpolated model. Rather, the ME approach provides an overarching framework in which the full distribution of semantic roles given syntactic features can be modeled. We train the ME models using the GIS algorithm (Darroch and Ratcliff, 1972) as implemented in the YASMET ME package (Och, 2002). We use the YASMET MEtagger (Bender et al., 2003) to perform the Viterbi search. The classifier was trained until performance on the development set ceased to improve. Feature weights were smoothed using Gaussian priors with mean 0 (Chen and Rosenfeld, 1999). The standard deviation of this distribution was optimized on the development set for each experiment. 3.3 Experiments We present three experiments in which different feature sets are used to train the ME classifier. The first experiment uses only those feature combinations described in Gildea and Jurafsky (2002) (feature sets 0-7 from Ta</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>F.J. Och. 2002. Yet another maxent toolkit: YASMET. www-i6.informatik.rwthaachen.de/Colleagues/och/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>