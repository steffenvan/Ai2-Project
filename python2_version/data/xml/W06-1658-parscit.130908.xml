<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8976314">
Entity Annotation based on Inverse Index Operations
Ganesh Ramakrishnan, Sreeram Balakrishnan, Sachindra Joshi
IBM India Research Labs
IIT Delhi, Hauz Khas,
New Delhi, India
</note>
<email confidence="0.985549">
{ganramkr, sreevb, jsachind}@in.ibm.com
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991605">
Entity annotation involves attaching a la-
bel such as ‘name’ or ‘organization’ to a
sequence of tokens in a document. All the
current rule-based and machine learning-
based approaches for this task operate at
the document level. We present a new
and generic approach to entity annotation
which uses the inverse index typically cre-
ated for rapid key-word based searching
of a document collection. We define a set
of operations on the inverse index that al-
lows us to create annotations defined by
cascading regular expressions. The entity
annotations for an entire document cor-
pus can be created purely of the index
with no need to access the original docu-
ments. Experiments on two publicly avail-
able data sets show very significant perfor-
mance improvements over the document-
based annotators.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972827586207">
Entity Annotation associates a well-defined label
such as ‘person name’, ‘organization’, ‘place’,
etc., with a sequence of tokens in unstructured
text. The dominant paradigm for annotating a
document collection is to annotate each document
separately. The computational complexity of an-
notating the collection in this paradigm, depends
linearly on the number of documents and the cost
of annotating each document. More precisely, it
depends on the total number of tokens in the doc-
ument collection. It is not uncommon to have mil-
lions of documents in a collection. Using this par-
adigm, it can take hours or days to annotate such
big collections even with highly parallel server
farms. Another drawback of this paradigm is that
the entire document collection needs to be re-
processed whenever new annotations are required.
In this paper, we propose an alternative para-
digm for entity annotation. We build an index for
the tokens in the document collection first. Us-
ing a set of operators on the index, we can gener-
ate new index entries for sequences of tokens that
match any given regular expression. Since a large
class of annotators (e.g., GATE (Cunningham et
al., 2002)) can be built using cascading regular ex-
pressions, this approach allows us to support anno-
tation of the document collection purely from the
index.
We show both theoretically and experimentally
that this approach can lead to substantial reduc-
tions in computational complexity, since the order
of computation is dependent on the size of the in-
dexes and not the number of tokens in the doc-
ument collection. In most cases, the index sizes
used for computing the annotations will be a small
fraction of the total number of tokens.
In (Cho and Rajagopalan, 2002) the authors de-
velop a method for speeding up the evaluation of
a regular expression ‘R’ on a large text corpus by
use of an optimally constructed multi-gram index
to filter documents that will match ‘R’. Unfortu-
nately, their method requires access to the docu-
ment collection for the final match of ‘R’ to the
filtered document set, which can be very time con-
suming. The other bodies of related prior work
concern indexing annotated data (Cooper et al.,
2001; Li and Moon, 2001) and methods for doc-
ument level annotation (Agichtein and Gravano,
2000; McCallum et al., 2000). The work on index-
ing annotated data is not directly relevant, since
our method creates the index to the annotations di-
rectly as part of the algorithm for computing the
annotation. (Eikvil, 1999) has a good survey of
existing document level IE methods. The rele-
vance to our work is that only a certain class of
annotators can be implemented using our method:
namely anything that can be implemented using
cascading weighted regular expressions. Fortu-
</bodyText>
<page confidence="0.975525">
492
</page>
<note confidence="0.8563695">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 492–500,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998308583333333">
nately, this is still powerful enough to enable a
large class of highly effective entity annotators.
The rest of the paper is organized as follows. In
Section 2, we present an overview of the proposed
approach for entity annotation. In Section 3, we
construct an algorithm for implementing a deter-
ministic finite automaton (DFA) using an inverse
index of a document collection. We also compare
the complexity of this approach against the direct
approach of running the DFA over the document
collection, and show that under typical conditions,
the index-based approach will be an order of mag-
nitude faster. In Section 4, we develop an alter-
native algorithm which is based on translating the
original regular expression directly into an ordered
AND/OR graph with an associated set of index
level operators. This has the advantage of oper-
ating directly on the much more compact regular
expressions instead of the equivalent DFA (which
can become very large as a result of the NFA to
DFA conversion and epsilon removal steps). We
provide details of our experiments on two publicly
available data sets in Section 5. Finally we present
our conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.990592" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999552984375">
Figure 1 shows the process for entity annotation
presented in the paper. A given document collec-
tion D is tokenized and segmented into sentences.
The tokens are stored in an inverse index I. The
inverse index I has an ordered list U of the unique
tokens u1, u2, ..uW that occur in the collection,
where W is the number of tokens in I. Addition-
ally, for each unique token uz, I has a postings
list L(uz) _&lt; l1, l2, ... lmt(ui) &gt; of locations in
D at which uz occurs. cnt(uz) is the length of
L(uz). Each entry lk, in the postings list L(uz),
has three fields: (1) a sentence identifier, lk.sid,
(2) the begin position of the particular occurrence
of uz, lk.first and (3) the end position of the same
occurrence of uz, lk.last.
We require the input grammar to be the same
as that used for named entity annotations in GATE
(Cunningham et al., 2002). The GATE architec-
ture for text engineering uses the Java Annota-
tions Pattern Engine (JAPE) (Cunningham, 1999)
for its information extraction task. JAPE is a pat-
tern matching language. We support two classes
of properties for tokens that are required by gram-
mars such as JAPE: (1) orthographic properties
such as an uppercase character followed by lower
case characters, and (2) gazetteer (dictionary) con-
tainment properties of tokens and token sequences
such as ‘location’ and ‘person name’. The set of
tokens along with entity types specified by either
of these two properties are referred to as Basic
Entities. The instances of basic entities specified
by orthographic properties must be single tokens.
However, instances of basic entities specified us-
ing gazetteer containment properties can be token
sequences.
The module (1) of our system shown in Fig-
ure 1, identifies postings lists for each basic en-
tity type. These postings lists are entered as index
entries in I for the corresponding types. For ex-
ample, if the input rules require tokens/token se-
quences that satisfy Capsword or Location Dic-
tionary properties, a postings list is created for
each of these basic types. Constructing the post-
ings list for a basic entity type with some ortho-
graphic property is a fairly straightforward task;
the postings lists of tokens satisfying the ortho-
graphic properties are merged (while retaining the
sorted order of each postings list). The mecha-
nism for generating the postings list of basic en-
tities with gazetteer properties will be developed
in the following sections. A rule for NE an-
notation may require a token to satisfy multiple
properties such as Location Dictionary as well as
Capsword. The posting list for tokens that satisfy
multiple properties are determined by perform-
ing an operation parallelint(L, L&apos;) over the post-
ing lists of the corresponding basic entities. The
parallelint(L, L&apos;) operation returns a posting list
such that each entry in the returned list occurs in
both L as well as L&apos;. The module (2) of our sys-
tem shown in Figure 1 identifies instances of each
annotation type, by performing index-based oper-
ations on the postings lists of basic entity types and
other tokens.
</bodyText>
<sectionHeader confidence="0.852172" genericHeader="method">
3 Annotation using Cascading Regular
Expressions
</sectionHeader>
<bodyText confidence="0.992838">
Regular expressions over basic entities have been
extensively used for NE annotations. The Com-
mon Pattern Specification Language (CSPL)1
specifies a standard for describing Annotators that
can be implemented by a series of cascading regu-
lar expression matches.
Consider a regular expression R over an al-
phabet E of basic entities, and a token sequence
</bodyText>
<footnote confidence="0.990629">
1http://www.ai.sri.com/—appelt/TextPro
</footnote>
<page confidence="0.998486">
493
</page>
<figureCaption confidence="0.999706">
Figure 1: Overview of the entity annotation process described in this paper
</figureCaption>
<equation confidence="0.952971">
T = {t1, ... , tW}. The annotation problem aims
</equation>
<bodyText confidence="0.860809625">
at determining all matches of regular expression
R in the token sequence T. Additionally, NE an-
notations do not span multiple sentences. We will
therefore assume that the length of any annotated
token sequence is bounded by A, where A can
be the maximum sentence length in the document
collection of interest. In practice, A can be even
smaller.
</bodyText>
<subsectionHeader confidence="0.999628">
3.1 Computing Annotations using a DFA
</subsectionHeader>
<bodyText confidence="0.999845653846154">
Given a regular expression R, we can convert it
into a deterministic finite automate (DFA) DR. A
DFA is a finite state machine, where for each pair
of state and input symbol, there is one and only
one transition to a next state. DR starts process-
ing of an input sequence from a start state sR, and
for each input symbol, it makes a transition to a
state given by a transition function 4bR. Whenever
DR lands in an accept state, the symbol sequence
till that point is accepted by DR. For simplicity of
the document and index algorithms, we will ignore
document and sentence boundaries in the follow-
ing analysis.
Let @ti,i+A,1 &lt; i &lt; W − A be a subsequence
of T of length A. On a given input @ti,i+A, DR
will determine all token sequences originating at ti
that are accepted by the regular expression gram-
mar specified through DR. Figure 2 outlines the
algorithm findAnnotations that locates all token
sequences in T that are accepted by DR.
Let DR have {S1, ... , SN} states. We assume
that the states have been topologically ordered so
that S1 is the start state. Let α be the time taken
to consume a single token and advance the DFA
to the next state (this is typically implemented as
a table or hash look-up). The time taken by the al-
</bodyText>
<equation confidence="0.809222428571429">
findAnnotations(T, DR)
Let T = {t1, ... , try}
for i = 1 to W − A do
let @ti,i+Δ be a subsequence of length A starting
from ti in T
use DR to annotate @ti,i+Δ
end for
</equation>
<figureCaption confidence="0.988834">
Figure 2: The algorithm for finding all the occur-
rences of R in a token sequence T.
</figureCaption>
<bodyText confidence="0.9997705">
gorithm findAnnotations can be obtained by sum-
ming up the number of times each state is vis-
ited as the input tokens are consumed. Clearly,
the state S1 is visited W times, W being the total
number of symbols in the token sequence T. Let
cnt(Si) give the total number of times the state Si
has been visited. The complexity of this method
is:
</bodyText>
<equation confidence="0.988339">
cnt(Si) = α I W + E cnt(Si)J (1)
</equation>
<subsectionHeader confidence="0.995981">
3.2 Computing Regular Expression Matches
using Index
</subsectionHeader>
<bodyText confidence="0.999944833333333">
In this section, we present a new approach for find-
ing all matches of a regular expression R in a to-
ken sequence T, based on the inverse index I of T.
The structure of the inverse index was presented in
Section 2. We define two operations on postings
lists which find use in our annotation algorithm.
</bodyText>
<listItem confidence="0.99374">
1. merge(L, L&apos;): Returns a postings list such
that each entry in the returned list occurs either in
Lor L&apos; or both. This operation takes O(|L|+|L&apos;|)
time.
2. consint(L, L&apos;): Returns a postings list such
that each entry in the returned list points to a to-
ken sequence which consists of two consecutive
</listItem>
<equation confidence="0.99314">
i=N
CD = α
i=1
i=N
i=2
</equation>
<page confidence="0.98731">
494
</page>
<bodyText confidence="0.9998348">
subsequences @sa and @sb within the same sen-
tence, such that, L has an entry for @sa and L0
has an entry for @sb. There are several meth-
ods for computing this depending on the relative
size of L and L0. If they are roughly equal in
size, a simple linear pass through L and L0, anal-
ogous to a merge, can be performed. If there is
a significant difference in sizes, a more efficient
modified binary search algorithm can be imple-
mented. The details are shown in Figure 3. The
</bodyText>
<equation confidence="0.647202">
consint(L, L0)
</equation>
<bodyText confidence="0.9949435">
Let M elements of L be l1 · · · lM
Let N elements of L’ be l01 · · · lN
</bodyText>
<figure confidence="0.575629214285714">
if M &lt; N then
setj=1
for i = 1 to M do
set k = 1, keep doubling k until
l0j.first &lt; li.last &lt; l0j+k.first
binary search the L0 in the interval j · · · k
to determine the value of p such that
l0 p.first &lt; li.last &lt; l0 p+1.first
if l0p.first = li.last a match exists, copy to output
set j = p + 1
end for
else
Same as above except l and l0 are reversed
end if
</figure>
<figureCaption confidence="0.86199">
Figure 3: The modified binary search algorithm
for consint
</figureCaption>
<bodyText confidence="0.998926095238095">
complexity of this algorithm is determined by the
size qi of the interval required to satisfy l0j.first &lt;
li.last &lt; l0j+qi.first (assuming |L |&lt; |L0|). It
will take an average of 1o92(qi) operations to de-
termine the size of interval and 1o92(qi) opera-
tions to perform the binary search, giving a to-
tal of 21o92(qi). Let q1 · · · qM be the sequence
of intervals. Since the intervals will be at most
two times larger than the actual interval between
the nearest matches in L0 to L, we can see that
|L0 |&lt; EMi=1 qi &lt; 2 * |L0|. Hence the worst case
will be reached when qi = 2|L0|/|L |with a time
complexity given by 2|L |(1o92(|L0|/|L|) + 1), as-
suming |L |&lt; |L0|.
To support annotation of a token sequence that
matches a regular expression only in the con-
text of some regular expression match on its left
and/or right, we implement simple extensions to
the consint(L1, L2) operator. Details of the ex-
tensions are left out from this paper owing to space
constraints.
</bodyText>
<subsectionHeader confidence="0.9367275">
3.3 Implementing a DFA using the Inverse
Index
</subsectionHeader>
<bodyText confidence="0.999109684210526">
In this section, we present a method that takes a
DFA DR and an inverse index I of a token se-
quence T, to compute a postings list of subse-
quences of length at most A, that match the regu-
lar expression R.
Let the set S = {S1, ... , SN} denote the set
of states in DR, and let the states be topologi-
cally ordered with S1 as the start state. We as-
sociate an object lists,k with each state s E S and
b1 &lt; k &lt; A. The object lists,k is a posting list
of all token sequences of length exactly k that end
in state s. The lists,k is initialized to be empty
for all states and lengths. We iteratively compute
lists,k for all the states using the algorithm given
in Figure 4. The function dest(Si) returns a set
of states, such that for each s E dest(Si), there
is an arc from state Si to state s. The function
label(Si, Sj) returns the token associated with the
edge (Si, Sj).
</bodyText>
<equation confidence="0.968564555555555">
for k = 1 to A do
for i = 1 to N do
for s E dest(Si) do
if i == 1 then
t = L(label(Si, s))
else
t = consint(listS,,k−1, L(label(Si, s)))
end if
lists,k = merge(lists,k, t)
</equation>
<tableCaption confidence="0.969240666666667">
end for
end for
end for
</tableCaption>
<figureCaption confidence="0.993498">
Figure 4: The algorithm for building the index to
all token sequences in T that match R.
</figureCaption>
<bodyText confidence="0.98577925">
At the end of the algorithm, all token sequences
corresponding to postings lists lists,i, s E S,1 &lt;
i &lt; A are sequences that are matched by the reg-
ular expression R.
</bodyText>
<subsectionHeader confidence="0.397982">
3.4 Complexity Analysis for the Index-based
Approach
</subsectionHeader>
<bodyText confidence="0.80929">
The complexity analysis of the algorithm given
in Figure 4 is based on the observation that,
Ek=Δ
</bodyText>
<equation confidence="0.774849">
k=1 |listSi,k |= cnt(Si). This holds, since
</equation>
<bodyText confidence="0.998444857142857">
listSi,k contains an entry for all sequences that
visit the state Si and are of length exactly k. Sum-
ming the length of these lists for a particular state
Si across all the values of k will yield the total
number of sequences of length at most A that visit
the state Si.
For the algorithm in Figure 3, the time taken by
</bodyText>
<page confidence="0.996609">
495
</page>
<bodyText confidence="0.991274909090909">
one consint operation is given by 2Q(|listSi,k |*
(log(pijk) + 1)) where Q is a constant that varies
with the lower level implementation. pijk =
|L(label(Si,Sj)) |is the ratio of the postings list size
|listSi,k |
of the label associated with the arc from Si to
Sj to the list size of Si at step k. Note that
pijk &gt; 1. Let prev(Si) be the list of pre-
decessor states to Si. The time taken by all
the merge operations for a state Si at step k
is given by -y(log(|prev(Si)|)|listSi,k|) Assum-
ing all the merges are performed simultaneously,
-y(log(|prev(Si)|) is the time taken to create each
entry in the final merged list, where -y is a con-
stant that varies with the lower level implementa-
tion. Note this scales as the log of the number of
lists that are being merged.
The total time taken by the algorithm given in
Figure 4 can be computed using the time spent on
merge and consint operations for all states and
all lengths. Setting ¯�is = maxk pisk, the total time
CI can be given as:
</bodyText>
<listItem confidence="0.662900333333333">
Note that in deriving Equation 2, we have ig-
nored the cost of merging list(Sa, k) for k =
1 · · · Δ for the accept states.
</listItem>
<subsectionHeader confidence="0.99439">
3.5 Comparison of Complexities
</subsectionHeader>
<bodyText confidence="0.996235833333333">
To simplify further analysis, we can replace
cnt(Si) with fcnt(Si) where fcnt(Si) =
cnt(Si)/W. If we assume that the token distribu-
tion statistics of the document collection remain
constant as the number of documents increases,
we can also assume that fcnt(Si) is invariant to
W. Since pijk is given by a ratio of list sizes, we
can also consider it to be invariant to W. We now
assume α Pt� Q pt� -y since these are implementa-
tion specific times for similar low level compute
operations. With this assumptions from Equations
1 and 2, the ratio CD/CI can be approximated by:
</bodyText>
<equation confidence="0.99763975">
1 + ENi=2 fcnt(Si)
rN [ ) g(lp ( )1)] f ( )
L-�i=2 L�sEdest(St) 21og(Pis + 10 rC&apos;U Si CTZt Si
(3)
</equation>
<bodyText confidence="0.999369117647059">
The overall ratio of CD to CI is invariant to W
and depends on two key factors fcnt(Si) and
Es∈dest(Si) log( ¯�is). If fcnt(Si) « 1, the ratio
will be large and the index-based approach will be
much faster. However, if either fcnt(Si) starts ap-
proaching 1 or Es∈dest(Si) log( ¯�is) starts getting
very large (caused by a large fan out from Si), the
direct match using the DFA may be more efficient.
Intuitively, this makes sense since the main ben-
efit of the index is to eliminate unnecessary hash
lookups for tokens do not match the arcs of the
DFA. As fcnt(Si) approaches 1, this assumption
breaks down and hence the inherent efficiency of
the direct DFA approach, where only a single hash
lookup is required per state regardless of the num-
ber of destination states, becomes the dominant
factor.
</bodyText>
<subsectionHeader confidence="0.8432855">
3.6 Comparison of Complexities for Simple
Dictionary DFA
</subsectionHeader>
<bodyText confidence="0.99984925">
To illustrate the potential gains from the index-
based annotation, consider a simple DFA DR with
two states S1 and S2. Let the set of unique to-
kens A be {a, b, c · · · z}. Let E be the dictionary
{a, e, i, o, u}. Let DR have five arcs from S1 to S2
one for each element in E. The DFA DR is a sim-
ple acceptor for the dictionary E, and if run over
a token sequence T drawn from A, it will match
) any single token that is in E. For this simple case
fcnt(S2) is just the fraction of tokens that occur
in E and hence by definition fcnt(S2) &lt; 1. Sub-
stituting into 3 we get
</bodyText>
<equation confidence="0.99699">
1 + fcnt(S2) (4)
2 log(5)fcnt(S2)
</equation>
<bodyText confidence="0.9999055">
As long as fcnt(S2) &lt; 0.27, this ratio will always
be greater than 1.
</bodyText>
<sectionHeader confidence="0.984019" genericHeader="method">
4 Inverse Index-based Annotation using
Regular Expressions
</sectionHeader>
<bodyText confidence="0.999929133333333">
A DFA corresponding to a given regular expres-
sion can be used for annotation, using the inverse
index approach as described in Section 3.3. How-
ever, the NFA to DFA conversion step may result
in a DFA with a very large number of states. We
develop an alternative algorithm that translates the
original regular expression directly into an ordered
AND/OR graph. Associated with each node in the
graph is a regular expression and a postings list
that points to all the matches for the node’s regu-
lar expression in the document collection. There
are two node types: AND nodes where the output
list is computed from the consint of the postings
lists of two children nodes and OR nodes where
the output list is formed by merging the posting
</bodyText>
<equation confidence="0.9583354">
� �
�
�γ log(|prev(Si)|) + 2β log( ¯ρis) �cnt(Si
sEdest(Si)
(2)
i=NE
i=2
CI =
CD
CI =
</equation>
<page confidence="0.993975">
496
</page>
<bodyText confidence="0.99975">
lists of all the children nodes. Additionally, each
node has two binary properties: isOpt and self-
Loop. The first property is set if the regular ex-
pression being matched is of the form ‘R?’, where
‘?’ denotes that the regular expression R is op-
tional. The second property is set if the regular
expression is of the form ‘R+’, where ‘+’ is the
Kleen operator denoting one or more occurrences.
For the case of ‘R*’, both properties are set.
The AND/OR graph is recursively built by scan-
ning the regular expression from left to right and
identifying every sub-regular expression for which
a sub-graph can be built. We use capital letters
R, X to denote regular expressions and small let-
ters a, b, c, etc., to denote terminal symbols in
the symbol set E. Figure 5 details the algorithm
used to build the AND/OR graph. Effectively, the
AND/OR graph decomposes the computation of
the postings list for R into a ordered set of merge
and consint operations, such that the output L(v)
for node v become the input to its parents. The
graph specifies the ordering, and by evaluating all
the nodes in dependency order, the root node will
end up with a postings list that corresponds to the
desired regular expression.
</bodyText>
<equation confidence="0.9338789">
if R is empty then
Return NULL
else if R is a symbol a E E then
Return createNode(name = a)
else
Decompose R such that R --+ R&apos; &lt;regexp&gt;
if &lt;regexp&gt; is empty then
if R&apos; == (X) or X+ or X* or X? then
node = createGraph(X)
if R&apos; == X+ or X* then
node.selfLoop = 1
end if
if R&apos; == X? or X* then
node.isOpt = 1
end if
else if R&apos; == (X1JX2J..JXk) then
node = createNode(name = R)
node.nodetype = OR
fori=1tokdo
node.children[i] = createGraph(Xi)
end for
end if
else
node = createNode(name = R)
node.nodetype = AND
node.children[1] = createGraph(R&apos;)
node.children[2] = createGraph(&lt;regexp&gt;)
end if
Return node
end if
</equation>
<figureCaption confidence="0.994052666666667">
Figure 5: createGraph(R)
Figure 6: An example regular expression and cor-
responding AND/OR graph
</figureCaption>
<subsectionHeader confidence="0.999613">
4.1 Handling ‘?’ and Meen Operators
</subsectionHeader>
<bodyText confidence="0.999866775">
The isOpt and selfLoop properties of a node are
set if the corresponding regular expression is of
the form R?, R+ or R*. To handle the R? case
we associate a new property isOpt with the output
list L(v) from node v, such that L(v).isOpt = 1
if the v.isOpt = 1. We also define two operations
consint, in Figure 7 and merge, which account
for the isOpt property of their argument lists. For
consint,, the generated list has its isOpt set to
1 if and only if both the argument lists have their
isOpt property set to 1. The merge, operation re-
mains the same as merge, except that the resultant
list has isOpt set to 1 if any of its argument lists
has isOpt set to 1. The worst case time taken by
consint, is bounded by 1 consint and 2 merge
operations.
To handle the R+ case, we define a new oper-
ator consint,(L, +) which returns a postings list
L&apos;, such that each entry in the returned list points
to a token sequence consisting of all k E [1, A]
consecutive subsequences @s1, @s2 ... @sk, each
@si,1 &lt; i &lt; k being an entry in L. A sim-
ple linear pass through L is sufficient to obtain
consint(L, +). The time complexity of this op-
eration is linear in the size of L&apos;. The isOpt prop-
erty of the result list L&apos; is set to the same value as
its argument list L.
Figure 6 shows an example regular expres-
sion and its corresponding AND/OR graph; AND
nodes are shown as circles whereas OR nodes are
shown as square boxes. Nodes having isOpt and
selfLoop properties are labeled with +, * or ?.
Any AND/OR graph thus constructed is acyclic.
The edges in the graph represent dependency be-
tween computing nodes. The main regular expres-
sion is at the root node of the graph. The leaf
nodes correspond to symbols in E. Figure 8 out-
lines the algorithm for computing the postings list
of a regular expression by operating bottom-up on
the AND/OR graph.
</bodyText>
<page confidence="0.958682">
497
</page>
<equation confidence="0.946204928571429">
consint,(L, L&apos;)
if ((L.isOpt == 0) and (L’.isOpt == 0)) then
Return consint(L, L&apos;)
end if
if ((L.isOpt == 0) and (L’.isOpt == 1)) then
Return merge(L, consint(L, L&apos;))
end if
if ((L.isOpt == 1) and (L’.isOpt == 0)) then
Return merge(consint(L, L&apos;), L&apos;)
end if
if ((L.isOpt == 1) and (L’.isOpt == 1)) then
t = merge(consint(L, L&apos;), L&apos;)
Return merge(t, L)
end if
</equation>
<figureCaption confidence="0.973479">
Figure 7: consint,
</figureCaption>
<bodyText confidence="0.68409075">
for Each node v in the reverse topological sorting of GR
do
if v.nodetype == AND then
Let v1 and v2 be the children of v
</bodyText>
<equation confidence="0.900242454545454">
L(v) = consint,(L(v1), L(v2))
else if v.type == OR then
L(v) = merge,(L(v.child1), � � � ,L(v.childn))
end if
if v.selfLoop == 1 then
L(v) = consint,(L(v), +)
end if
if v.isOpt == 1 then
L(v).isOpt = 1
end if
end for
</equation>
<figureCaption confidence="0.970889333333333">
Figure 8: The algorithm for computing postings
list of a regular expression R using the inverse in-
dex I and the corresponding AND/OR graph GR
</figureCaption>
<sectionHeader confidence="0.984165" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999892466666667">
In this section, we present empirical compari-
son of performance of the index-based annotation
technique (Section 4) against annotation based on
the ‘document paradigm’ using GATE. The exper-
iments were performed on two data sets, viz., (i)
the enron email data set2 and (ii) a combination of
Reuters-21578 data set3 and the 20 Newsgroups
data set4. After cleaning, the former data set was
2.3 GB while the latter was 93 MB in size. Our
code is entirely in Java. The experiments were
performed on a dual 3.2GHz Xeon server with 4
GB RAM. The code for creation of the index was
custom-built in Java. Prior to indexing, the sen-
tence segmentation and tokenization of each data
set was performed using in-house Java versions of
</bodyText>
<footnote confidence="0.9931596">
2http://www.cs.cmu.edu/—enron/
3http://www.daviddlewis.com/resources/
testcollections/reuters21578/
4http://people.csail.mit.edu/jrennie/
20Newsgroups/
</footnote>
<note confidence="0.415768">
standard tools5.
</note>
<subsectionHeader confidence="0.991435">
5.1 Rule Specification using JAPE
</subsectionHeader>
<bodyText confidence="0.999271333333333">
JAPE is a version of CPSL6 (Common Pattern
Specification Language). JAPE provides finite
state transduction over annotations based on reg-
ular expressions. The JAPE grammar requires in-
formation from two main resources: (i) a tokenizer
and (ii) a gazetteer.
</bodyText>
<listItem confidence="0.9567145">
(1) Tokenizer: The tokenizer splits the text into
very simple tokens such as numbers, punctuation
</listItem>
<bodyText confidence="0.931018888888889">
and words of different types. For example, one
might distinguish between words in uppercase and
lowercase, and between certain types of punctua-
tion. Although the tokenizer is ca pable of much
deeper analysis than this, the aim is to limit its
work to maximise efficiency, and enable greater
flexibility by placing the burden on the grammar
rules, which are more adaptable. A rule has a
left hand side (LHS) and a right hand side (RHS).
The LHS is a regular expression which has to be
matched on the input; the RHS describes the an-
notations to be added to the Annotation Set. The
LHS is separated from the RHS by ’&gt;’. The fol-
lowing four operators can be used on the LHS: ’|’,
’?’, ’�’ and ’+’. The RHS uses ’;’ as a separa-
tor between statements that set the values of the
different attributes. The following tokenizer rule
identifies each character sequence that begins with
a letter in upper case and is followed by 0 or more
letters in lower case:
&amp;quot;UPPERCASELETTER&amp;quot; &amp;quot;LOWERCASELETTER&amp;quot;*
&gt;&gt;&gt; Token; orth=upperInitial; kind=word;
Each such character sequence will be annotated as
type “Token”. The attribute “orth” (orthography)
has the value “upperInitial”; the attribute “kind”
has the value “word”.
(2) Gazetteer: The gazetteer lists used are plain
text files, with one entry per line. Each list rep-
resents a set of names, such as names of cities,
organizations, days of the week, etc. An index file
is used to access these lists; for each list, a ma-
jor type is specified and, optionally, a minor type.
These lists are compiled into finite state machines.
Any text tokens that are matched by these ma-
chines will be annotated with features specifying
the major and minor types. JAPE grammar rules
</bodyText>
<footnote confidence="0.9957326">
5http://l2r.cs.uiuc.edu/—cogcomp/
tools.php
6A good description of the original version of this lan-
guage is in Doug Appelt’s TextPro manual: http://www.
ai.sri.com/—appelt/TextPro.
</footnote>
<page confidence="0.996397">
498
</page>
<bodyText confidence="0.999293285714286">
then specify the types to be identified in particular
circumstances.
The JAPE Rule: Each JAPE rule has two parts,
separated by “–&gt;”. The LHS consists of an an-
notation pattern to be matched; the RHS describes
the annotation to be assigned. A basic rule is given
as:
</bodyText>
<figure confidence="0.987650666666667">
Rule::=
&lt;rule&gt; &lt;ident&gt; ( &lt;priority&gt; &lt;integer&gt; )?
LeftHandSide &amp;quot;&gt;&gt;&gt;&amp;quot; RightHandSide
</figure>
<figureCaption confidence="0.872848333333333">
(1) Left hand side: On the LHS, the pattern is
described in terms of the annotations already as-
signed by the tokenizer and gazetteer. The annota-
tion pattern may contain regular expression opera-
tors (e.g. *, ?, +). There are 3 main ways in which
the pattern can be specified:
</figureCaption>
<listItem confidence="0.949282">
1. value: specify a string of text, e.g.
{Token.string == “of”}
2. attribute: specify the attributes (and values)
of a token (or any other annotation), e.g.
{Token.kind == number}
3. annotation: specify an annotation type from
the gazetteer, e.g. {Lookup.minorType ==
month}
(2) Right hand side: The RHS consists of de-
</listItem>
<bodyText confidence="0.968265428571428">
tails of the annotations and optional features to be
created. Annotations matched on the LHS of a rule
may be referred to on the RHS by means of labels
that are attached to pattern elements. Finally, at-
tributes and their corresponding values are added
to the annotation. An example of a complete rule
is:
</bodyText>
<figure confidence="0.72101">
Rule: NumbersAndUnit
(({Token.kind==&amp;quot;number&amp;quot;})+:numbers
{Token.kind==&amp;quot;unit&amp;quot;})
&gt;&gt;&gt;
:numbers.Name={rule=&amp;quot;NumbersAndUnit&amp;quot;}
</figure>
<bodyText confidence="0.96297125">
This says ‘match sequences of numbers followed
by a unit; create a Name annotation across the span
of the numbers, and attribute rule with value Num-
bersAndUnit’.
Use of context: Context can be dealt with in the
grammar rules in the following way. The pattern to
be annotated is always enclosed by a set of round
brackets. If preceding context is to be included in
the rule, this is placed before this set of brackets.
This context is described in exactly the same way
as the pattern to be matched. If context follow-
ing the pattern needs to be included, it is placed
Figure 9: An example JAPE rule used in the ex-
periments
after the label given to the annotation. Context is
used where a pattern should only be recognised if
it occurs in a certain situation, but the context itself
does not form part of the pattern to be annotated.
For example, the following rule for ‘email-id’s
(assuming an appropriate regular expression for
“EMAIL-ADD”) would mean that an email ad-
dress would only be recognized if it occurred in-
side angled brackets (which would not themselves
form part of the entity):
</bodyText>
<figure confidence="0.9869981">
Rule: Emailaddress1
({Token.string==&amp;quot;&lt;&amp;quot;})
(
{Token.kind==EMAIL-ADD}
)
:email
({Token.string==&amp;quot;&gt;&amp;quot;})
&gt;&gt;&gt;
:email.Address={kind=&amp;quot;email&amp;quot;,
rule=&amp;quot;Emailaddress1&amp;quot;}
</figure>
<subsectionHeader confidence="0.630509">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.99904725">
In our first experiment, we performed annotation
of the two corpora for 4 annotation types using 2
JAPE rules for each type. The 4 annotation types
were ‘Person name’, ‘Organization’, ‘Location’
and ‘Date’. A sample JAPE rule for identifying
person names is shown in Figure 9. This rule iden-
tifies a sequence of words as a person name when
each word in the sequence starts with an alpha-
bet in upper-case and when the sequence is imme-
diately preceded by a word from a dictionary of
‘INITIAL’s. Example words in the ‘INITIAL’ dic-
tionary are: ‘Mr.’, ‘Dr.’, ’Lt.’, etc.
</bodyText>
<page confidence="0.997217">
499
</page>
<bodyText confidence="0.9979702">
Table 1 compares the time taken by the index-
based annotator against that taken by GATE for the
8 JAPE rules. The index-based annotator performs
8-13 times faster than GATE. Table 2 splits the
time mentioned for the index-based annotator in
Table 1 into the time taken for the task of comput-
ing postings lists for basic entities and derived en-
tities (c.f. Section 2) for each of the data sets. We
can also observe that a greater speedup is achieved
for the larger corpus.
</bodyText>
<table confidence="0.991614666666667">
Data set GATE Index-based
Enron 4974343 374926
Reuters 752287 92238
</table>
<tableCaption confidence="0.9931435">
Table 1: Time (in milliseconds) for computing an-
notations using the two techniques
</tableCaption>
<table confidence="0.9939335">
Data set Orthographic Gazetteer Derived
entity types entity types entity types
Enron 38285 105870 230771
Reuters 28493 21531 42214
</table>
<tableCaption confidence="0.9684605">
Table 2: Time (in milliseconds) for computing
postings lists of entity types
</tableCaption>
<bodyText confidence="0.9997834375">
An important advantage of performing annota-
tions over the inverse index is that index entries
for basic entity types can be preserved and reused
for annotation types as additional rules for anno-
tation are specified by users. For instance, the in-
dex entry for ‘Capsword’ might find reuse in sev-
eral annotation rules. As against this, a document-
based annotator has to process each document
from scratch for every newly introduced annota-
tion rule. To verify this, we introduced 1 addi-
tional rule for each of the 4 named entity types.
In Table 3, we compare the time required by
the index-based annotator against that required by
GATE for annotating the two corpora using the 4
additional rules. We achieve a greater speedup fac-
tor of 23-37 for incremental annotation.
</bodyText>
<table confidence="0.917153666666667">
Data set GATE Index-based
Enron 1479954 62227
Reuters 661157 17929
</table>
<tableCaption confidence="0.95643">
Table 3: Time (in milliseconds) for computing an-
</tableCaption>
<bodyText confidence="0.8275065">
notations using the two techniques for the addi-
tional 4 rules
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999876666666667">
In this paper we demonstrated that a suitably con-
structed inverse index contains all the necessary
information to implement entity annotators that
use cascading regular expressions. The approach
has the key advantage of not requiring access to
the original unstructured data to compute the an-
notations. The method uses a basic set of opera-
tors on the inverse index to construct indexes to all
matches for a regular expression in the tokenized
data set. We showed theoretically, that for a DFA
implementation, the index approach can be much
faster if the index sizes corresponding to the labels
on the DFA are a small fraction of the total num-
ber of tokens in the data set. We also provided
a more efficient index-based implementation that
is directly computed from the regular expressions
without the need of a DFA conversion and experi-
mentally demonstrated the gains.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996769">
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Interna-
tional Conference on Digital Libraries.
Junghoo Cho and Sridhar Rajagopalan. 2002. A fast
regular expression indexing engine. In Proceedings
of the 18th International Conference on Data Engi-
neering.
Brian Cooper, Neal Sample, Michael J. Franklin,
Gisli R. Hjaltason, and Moshe Shadmon. 2001. A
fast index for semistructured data. In The VLDB
Conference, pages 341–350.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graph-
ical development environment for robust NLP tools
and applications.
H. Cunningham. 1999. Jape – a java annotation pat-
terns engine.
Line Eikvil. 1999. Information extraction from world
wide web - a survey. Technical Report 945, Nor-
weigan Computing Center.
Quanzhong Li and Bongki Moon. 2001. Indexing and
querying XML data for regular path expressions. In
The VLDB Journal, pages 361–370.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proc. 17th International Conf. on Machine Learn-
ing, pages 591–598. Morgan Kaufmann, San Fran-
cisco, CA.
</reference>
<page confidence="0.9956">
500
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621515">
<title confidence="0.999538">Entity Annotation based on Inverse Index Operations</title>
<author confidence="0.994987">Ganesh Ramakrishnan</author>
<author confidence="0.994987">Sreeram Balakrishnan</author>
<author confidence="0.994987">Sachindra</author>
<affiliation confidence="0.999538">IBM India Research</affiliation>
<address confidence="0.773858">IIT Delhi, Hauz New Delhi,</address>
<email confidence="0.991777">sreevb,</email>
<abstract confidence="0.997552952380952">Entity annotation involves attaching a label such as ‘name’ or ‘organization’ to a sequence of tokens in a document. All the current rule-based and machine learningbased approaches for this task operate at the document level. We present a new and generic approach to entity annotation which uses the inverse index typically created for rapid key-word based searching of a document collection. We define a set of operations on the inverse index that allows us to create annotations defined by cascading regular expressions. The entity annotations for an entire document corpus can be created purely of the index with no need to access the original documents. Experiments on two publicly available data sets show very significant performance improvements over the documentbased annotators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth ACM International Conference on Digital Libraries.</booktitle>
<contexts>
<context position="3302" citStr="Agichtein and Gravano, 2000" startWordPosition="537" endWordPosition="540">a small fraction of the total number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding up the evaluation of a regular expression ‘R’ on a large text corpus by use of an optimally constructed multi-gram index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing annotated data is not directly relevant, since our method creates the index to the annotations directly as part of the algorithm for computing the annotation. (Eikvil, 1999) has a good survey of existing document level IE methods. The relevance to our work is that only a certain class of annotators can be implemented using our method: namely anything that can be implemented using cascading weighted regular expressions. Fortu492 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 492–500, Sydne</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junghoo Cho</author>
<author>Sridhar Rajagopalan</author>
</authors>
<title>A fast regular expression indexing engine.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th International Conference on Data Engineering.</booktitle>
<contexts>
<context position="2753" citStr="Cho and Rajagopalan, 2002" startWordPosition="442" endWordPosition="445">nce a large class of annotators (e.g., GATE (Cunningham et al., 2002)) can be built using cascading regular expressions, this approach allows us to support annotation of the document collection purely from the index. We show both theoretically and experimentally that this approach can lead to substantial reductions in computational complexity, since the order of computation is dependent on the size of the indexes and not the number of tokens in the document collection. In most cases, the index sizes used for computing the annotations will be a small fraction of the total number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding up the evaluation of a regular expression ‘R’ on a large text corpus by use of an optimally constructed multi-gram index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing anno</context>
</contexts>
<marker>Cho, Rajagopalan, 2002</marker>
<rawString>Junghoo Cho and Sridhar Rajagopalan. 2002. A fast regular expression indexing engine. In Proceedings of the 18th International Conference on Data Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Cooper</author>
<author>Neal Sample</author>
<author>Michael J Franklin</author>
<author>Gisli R Hjaltason</author>
<author>Moshe Shadmon</author>
</authors>
<title>A fast index for semistructured data.</title>
<date>2001</date>
<booktitle>In The VLDB Conference,</booktitle>
<pages>341--350</pages>
<contexts>
<context position="3211" citStr="Cooper et al., 2001" startWordPosition="522" endWordPosition="525">lection. In most cases, the index sizes used for computing the annotations will be a small fraction of the total number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding up the evaluation of a regular expression ‘R’ on a large text corpus by use of an optimally constructed multi-gram index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing annotated data is not directly relevant, since our method creates the index to the annotations directly as part of the algorithm for computing the annotation. (Eikvil, 1999) has a good survey of existing document level IE methods. The relevance to our work is that only a certain class of annotators can be implemented using our method: namely anything that can be implemented using cascading weighted regular expressions. Fortu492 Proceedings of the 2006 Confer</context>
</contexts>
<marker>Cooper, Sample, Franklin, Hjaltason, Shadmon, 2001</marker>
<rawString>Brian Cooper, Neal Sample, Michael J. Franklin, Gisli R. Hjaltason, and Moshe Shadmon. 2001. A fast index for semistructured data. In The VLDB Conference, pages 341–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<contexts>
<context position="2196" citStr="Cunningham et al., 2002" startWordPosition="348" endWordPosition="351">s in a collection. Using this paradigm, it can take hours or days to annotate such big collections even with highly parallel server farms. Another drawback of this paradigm is that the entire document collection needs to be reprocessed whenever new annotations are required. In this paper, we propose an alternative paradigm for entity annotation. We build an index for the tokens in the document collection first. Using a set of operators on the index, we can generate new index entries for sequences of tokens that match any given regular expression. Since a large class of annotators (e.g., GATE (Cunningham et al., 2002)) can be built using cascading regular expressions, this approach allows us to support annotation of the document collection purely from the index. We show both theoretically and experimentally that this approach can lead to substantial reductions in computational complexity, since the order of computation is dependent on the size of the indexes and not the number of tokens in the document collection. In most cases, the index sizes used for computing the annotations will be a small fraction of the total number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding </context>
<context position="5981" citStr="Cunningham et al., 2002" startWordPosition="992" endWordPosition="995">ered list U of the unique tokens u1, u2, ..uW that occur in the collection, where W is the number of tokens in I. Additionally, for each unique token uz, I has a postings list L(uz) _&lt; l1, l2, ... lmt(ui) &gt; of locations in D at which uz occurs. cnt(uz) is the length of L(uz). Each entry lk, in the postings list L(uz), has three fields: (1) a sentence identifier, lk.sid, (2) the begin position of the particular occurrence of uz, lk.first and (3) the end position of the same occurrence of uz, lk.last. We require the input grammar to be the same as that used for named entity annotations in GATE (Cunningham et al., 2002). The GATE architecture for text engineering uses the Java Annotations Pattern Engine (JAPE) (Cunningham, 1999) for its information extraction task. JAPE is a pattern matching language. We support two classes of properties for tokens that are required by grammars such as JAPE: (1) orthographic properties such as an uppercase character followed by lower case characters, and (2) gazetteer (dictionary) containment properties of tokens and token sequences such as ‘location’ and ‘person name’. The set of tokens along with entity types specified by either of these two properties are referred to as B</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
</authors>
<title>Jape – a java annotation patterns engine.</title>
<date>1999</date>
<contexts>
<context position="6092" citStr="Cunningham, 1999" startWordPosition="1011" endWordPosition="1012">tionally, for each unique token uz, I has a postings list L(uz) _&lt; l1, l2, ... lmt(ui) &gt; of locations in D at which uz occurs. cnt(uz) is the length of L(uz). Each entry lk, in the postings list L(uz), has three fields: (1) a sentence identifier, lk.sid, (2) the begin position of the particular occurrence of uz, lk.first and (3) the end position of the same occurrence of uz, lk.last. We require the input grammar to be the same as that used for named entity annotations in GATE (Cunningham et al., 2002). The GATE architecture for text engineering uses the Java Annotations Pattern Engine (JAPE) (Cunningham, 1999) for its information extraction task. JAPE is a pattern matching language. We support two classes of properties for tokens that are required by grammars such as JAPE: (1) orthographic properties such as an uppercase character followed by lower case characters, and (2) gazetteer (dictionary) containment properties of tokens and token sequences such as ‘location’ and ‘person name’. The set of tokens along with entity types specified by either of these two properties are referred to as Basic Entities. The instances of basic entities specified by orthographic properties must be single tokens. Howe</context>
</contexts>
<marker>Cunningham, 1999</marker>
<rawString>H. Cunningham. 1999. Jape – a java annotation patterns engine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Line Eikvil</author>
</authors>
<title>Information extraction from world wide web - a survey.</title>
<date>1999</date>
<tech>Technical Report 945,</tech>
<institution>Norweigan Computing Center.</institution>
<contexts>
<context position="3522" citStr="Eikvil, 1999" startWordPosition="576" endWordPosition="577">m index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing annotated data is not directly relevant, since our method creates the index to the annotations directly as part of the algorithm for computing the annotation. (Eikvil, 1999) has a good survey of existing document level IE methods. The relevance to our work is that only a certain class of annotators can be implemented using our method: namely anything that can be implemented using cascading weighted regular expressions. Fortu492 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 492–500, Sydney, July 2006. c�2006 Association for Computational Linguistics nately, this is still powerful enough to enable a large class of highly effective entity annotators. The rest of the paper is organized as follows. In Sectio</context>
</contexts>
<marker>Eikvil, 1999</marker>
<rawString>Line Eikvil. 1999. Information extraction from world wide web - a survey. Technical Report 945, Norweigan Computing Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quanzhong Li</author>
<author>Bongki Moon</author>
</authors>
<title>Indexing and querying XML data for regular path expressions.</title>
<date>2001</date>
<booktitle>In The VLDB Journal,</booktitle>
<pages>361--370</pages>
<contexts>
<context position="3231" citStr="Li and Moon, 2001" startWordPosition="526" endWordPosition="529">s, the index sizes used for computing the annotations will be a small fraction of the total number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding up the evaluation of a regular expression ‘R’ on a large text corpus by use of an optimally constructed multi-gram index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing annotated data is not directly relevant, since our method creates the index to the annotations directly as part of the algorithm for computing the annotation. (Eikvil, 1999) has a good survey of existing document level IE methods. The relevance to our work is that only a certain class of annotators can be implemented using our method: namely anything that can be implemented using cascading weighted regular expressions. Fortu492 Proceedings of the 2006 Conference on Empirical Me</context>
</contexts>
<marker>Li, Moon, 2001</marker>
<rawString>Quanzhong Li and Bongki Moon. 2001. Indexing and querying XML data for regular path expressions. In The VLDB Journal, pages 361–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Machine Learning,</booktitle>
<pages>591--598</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="3326" citStr="McCallum et al., 2000" startWordPosition="541" endWordPosition="544"> number of tokens. In (Cho and Rajagopalan, 2002) the authors develop a method for speeding up the evaluation of a regular expression ‘R’ on a large text corpus by use of an optimally constructed multi-gram index to filter documents that will match ‘R’. Unfortunately, their method requires access to the document collection for the final match of ‘R’ to the filtered document set, which can be very time consuming. The other bodies of related prior work concern indexing annotated data (Cooper et al., 2001; Li and Moon, 2001) and methods for document level annotation (Agichtein and Gravano, 2000; McCallum et al., 2000). The work on indexing annotated data is not directly relevant, since our method creates the index to the annotations directly as part of the algorithm for computing the annotation. (Eikvil, 1999) has a good survey of existing document level IE methods. The relevance to our work is that only a certain class of annotators can be implemented using our method: namely anything that can be implemented using cascading weighted regular expressions. Fortu492 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 492–500, Sydney, July 2006. c�2006 Ass</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proc. 17th International Conf. on Machine Learning, pages 591–598. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>