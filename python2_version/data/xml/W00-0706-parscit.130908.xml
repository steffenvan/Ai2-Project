<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000324">
<note confidence="0.816266">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 31-36, Lisbon, Portugal, 2000.
</note>
<title confidence="0.997018">
A Comparison between Supervised Learning Algorithms for Word
Sense Disambiguation*
</title>
<author confidence="0.944405">
Gerard Escudero and Lluis Marquez and German Rigau
</author>
<affiliation confidence="0.711804">
TALP Research Center. LSI Department. Universitat Politecnica de Catalunya (UPC)
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia
</affiliation>
<email confidence="0.56937">
{escudero, lluism, g.rigau}Insi.upc.es
</email>
<sectionHeader confidence="0.949798" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525692307692">
This paper describes a set of comparative exper-
iments, including cross–corpus evaluation, be-
tween five alternative algorithms for supervised
Word Sense Disambiguation (WSD), namely
Naive Bayes, Exemplar-based learning, SNoW,
Decision Lists, and Boosting. Two main conclu-
sions can be drawn: 1) The LazyBoosting algo-
rithm outperforms the other four state-of-the-
art algorithms in terms of accuracy and ability
to tune to new domains; 2) The domain depen-
dence of WSD systems seems very strong and
suggests that some kind of adaptation or tun-
ing is required for cross–corpus application.
</bodyText>
<sectionHeader confidence="0.994767" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996876611111111">
Word Sense Disambiguation (WSD) is the prob-
lem of assigning the appropriate meaning (or
sense) to a given word in a text or discourse.
Resolving the ambiguity of words is a central
problem for large scale language understanding
applications and their associate tasks (Ide and
Veronis, 1998). Besides, WSD is one of the most
important open problems in NLP. Despite the
wide range of approaches investigated (Kilgar-
riff and Rosenzweig, 2000) and the large effort
devoted to tackling this problem, to date, no
large-scale broad-coverage and highly accurate
WSD system has been built.
One of the most successful current lines of
research is the corpus-based approach, in which
statistical or Machine Learning (ML) algorithms
have been applied to learn statistical models
or classifiers from corpora in order to per-
</bodyText>
<footnote confidence="0.9877115">
* This research has been partially funded by the Spanish
Research Department (CICYT&apos;s project TIC98-0423—
C06), by the EU Commission (NAMIC IST-1999-12392),
and by the Catalan Research Department (CIRIT&apos;s
consolidated research group 1999SGR-150 and CIRIT&apos;s
grant 1999FI 00773).
</footnote>
<bodyText confidence="0.997815028571428">
form WSD. Generally, supervised approaches
(those that learn from previously semantically
annotated corpora) have obtained better results
than unsupervised methods on small sets of se-
lected ambiguous words, or artificial pseudo-
words. Many standard ML algorithms for su-
pervised learning have been applied, such as:
Decision Lists (Yarowsky, 1994; Agirre and
Martinez, 2000), Neural Networks (Towell and
Voorhees, 1998), Bayesian learning (Bruce and
Wiebe, 1999), Exemplar-based learning (Ng,
1997), Boosting (Escudero et al., 2000a), etc.
Further, in (Mooney, 1996) some of the previ-
ous methods are compared jointly with Decision
Trees and Rule Induction algorithms, on a very
restricted domain.
*Although some published works include the
comparison between some alternative algo-
rithms (Mooney, 1996; Ng, 1997; Escudero et
al., 2000a; Escudero et al., 2000b), none of
them addresses the issue of the portability of
supervised ML algorithms for WSD, i.e., testing
whether the accuracy of a system trained on
a certain corpus can be extrapolated to other
corpora or not. We think that the study of the
domain dependence of WSD —in the style of
other studies devoted to parsing (Sekine, 1997;
Ratnaparkhi, 1999)— is needed to assure the
validity of the supervised approach, and to de-
termine to which extent a tuning pre–process is
necessary to make real WSD systems portable.
In this direction, this work compares five differ-
ent ML algorithms and explores their portability
and tuning ability by training and testing them
on different corpora.
</bodyText>
<sectionHeader confidence="0.891947" genericHeader="method">
2 Learning Algorithms Tested
</sectionHeader>
<bodyText confidence="0.887492333333333">
Naive-Bayes (NB). Naive Bayes is intended
as a simple representative of statistical learning
methods. It has been used in its most classi-
</bodyText>
<page confidence="0.999798">
31
</page>
<bodyText confidence="0.986165603960396">
cal setting (Duda and Hart, 1973). That is,
assuming the independence of features, it clas-
sifies a new example by assigning the class that
maximizes the conditional probability of the
class given the observed sequence of features
of that example. Model probabilities are esti-
mated during the training process using relative
frequencies. To avoid the effect of zero counts, a
very simple smoothing technique has been used,
which was proposed in (Ng, 1997).
Despite its simplicity, Naive Bayes is claimed
to obtain state–of–the–art accuracy on super-
vised WSD in many papers (Mooney, 1996; Ng,
1997; Leacock et al., 1998).
Exemplar-based Classifier (EB). In exem-
plar, instance, or memory–based learning (Aha
et al., 1991) no generalization of training ex-
amples is performed. Instead, the examples are
simply stored in memory and the classification
of new examples is based on the most similar
stored exemplars. In our implementation, all
examples are kept in memory and the classifica-
tion is based on a k–NN (Nearest–Neighbours)
algorithm using Hamming distance to measure
closeness. For k&apos;s greater than 1, the resulting
sense is the weighted majority sense of the k
nearest neighbours —where each example votes
its sense with a strength proportional to its
closeness to the test example.
Exemplar–based learning is said to be the
best option for WSD (Ng, 1997). Other au-
thors (Daelemans et al., 1999) point out that
exemplar–based methods tend to be superior in
language learning problems because they do not
forget exceptions.
The SNoW Architecture (SN). SNoW is a
Sparse Network of linear separators which uti-
lizes the Winnow learning algorithml. In the
SNoW architecture there is a winnow node for
each class, which learns to separate that class
from all the rest. During training, which is per-
formed in an on–line fashion, each example is
considered a positive example for the winnow
node associated to its class and a negative ex-
ample for all the others. A key point that allows
a fast learning is that the winnow nodes are not
connected to all features but only to those that
1The Winnow algorithm (Littlestone, 1988) consists
of a linear threshold algorithm with multiplicative weight
updating for 2-class problems.
are &amp;quot;relevant&amp;quot; for their class. When classify-
ing a new example, SNoW is similar to a neural
network which takes the input features and out-
puts the class with the highest activation. Our
implementation of SNoW for WSD is explained
in (Escudero et al., 2000c).
SNoW is proven to perform very well in
high dimensional NLP problems, where both the
training examples and the target function reside
very sparsely in the feature space (Roth, 1998),
e.g: context–sensitive spelling correction, POS
tagging, PP–attachment disambiguation, etc.
Decision Lists (DL). In this setting, a Deci-
sion List is a list of features extracted from the
training examples and sorted by a log–likelihood
measure. This measure estimates how strong a
particular feature is as an indicator of a specific
sense (Yarowsky, 1994). When testing, the deci-
sion list is checked in order and the feature with
the highest weight that matches the test exam-
ple is used to select the winning word sense.
Thus, only the single most reliable piece of ev-
idence is used to perform disambiguation. Re-
garding the details of implementation (smooth-
ing, pruning of the decision list, etc.) we have
followed (Agirre and Martinez, 2000).
Decision Lists were one of the most success-
ful systems on the 1st Senseval competition for
WSD (Kilgarriff and Rosenzweig, 2000).
LazyBoosting (LB). The main idea of boost-
ing algorithms is to combine many simple and
moderately accurate hypotheses (weak classi-
fiers) into a single, highly accurate classifier.
The weak classifiers are trained sequentially
and, conceptually, each of them is trained on the
examples which were most difficult to classify by
the preceding weak classifiers. These weak hy-
potheses are then linearly combined into a single
rule called the combined hypothesis.
Schapire and Singer&apos;s real AdaBoost.MH al-
gorithm for multiclass multi–label classifica-
tion (Schapire and Singer, 1999) has been used.
It constructs a combination of very simple
weak hypotheses that test the value of a single
boolean predicate and make a real–valued pre-
diction based on that value. LazyBoosting (Es-
cudero et al., 2000a) is a simple modification
of the AdaBoost.MH algorithm, which consists
in reducing the feature space that is explored
when learning each weak classifier. This mod-
ification significantly increases the efficiency of
</bodyText>
<page confidence="0.993041">
32
</page>
<bodyText confidence="0.493184">
the learning process with no loss in accuracy.
</bodyText>
<sectionHeader confidence="0.375875" genericHeader="method">
3 Setting
</sectionHeader>
<bodyText confidence="0.995339552631579">
A number of comparative experiments has been
carried out on a subset of 21 highly ambiguous
words of the DSO corpus, which is a semanti-
cally annotated English corpus collected by Ng
and colleagues (Ng and Lee, 1996). Each word
is treated as a different classification problem.
The 21 words comprise 13 nouns (age, art, body,
car, child, cost, head, interest, line, point, state,
thing, work) and 8 verbs (become, fall, grow, lose,
set, speak, strike, tell), which frequently appear
in the WSD literature. The average number of
senses per word is close to 10 and the number
of training examples is around 1,000.
The DSO corpus contains sentences from two
different corpora, namely Wall Street Journal
(WSJ) and Brown Corpus (BC). Therefore, it is
easy to perform experiments about the porta-
bility of systems by training them on the WSJ
part (A part, hereinafter) and testing them on
the BC part (B part, hereinafter), or vice-versa.
Two kinds of information are used to train
classifiers: local and topical context. Let
&amp;quot;... w_3 w_2 w_1 w w+1 w+2 w+3 ...&amp;quot; be
the context of consecutive words around the
word w to be disambiguated, and p±i (-3 &lt;
i &lt; 3) be the part-of-speech tag of word
w±i. Attributes referring to local context
are the following 15: P-3/ P-2/ P-1/ P-I-1/
P+2/ P+3/ w-11 w+1/ (w-2/ w-1)/ (w-1/ IBA/
(W+1, W+2), (w-3/ w-2/ w-1)/ (w-2/ w-1/7141)/
(w-1/ w+1, w+2), and (w+i, w+2, w+3), where
the last seven correspond to collocations of two
and three consecutive words. The topical con-
text is formed by cl, ,C, which stand for the
unordered set of open class words appearing in
the sentence2. Details about how the different
algorithms translate this information into fea-
tures can be found in (Escudero et al., 2000c).
</bodyText>
<sectionHeader confidence="0.971549" genericHeader="method">
4 Comparing the five approaches
</sectionHeader>
<tableCaption confidence="0.32164275">
The five algorithms, jointly with a naive Most-
Frequent-sense Classifier (MFC), have been
tested, by 10-fold cross validation, on 7 different
combinations of training-test sets3. Accuracy
</tableCaption>
<footnote confidence="0.8229636">
2 This set of attributes corresponds to that used in (Ng
and Lee, 1996), with the exception of the morphology of
the target word and the verb-object syntactic relation.
3The combinations of training-test sets are called:
A+B-A-1-B, A±B-A, A-A, B-B, A-B, and B-A,
</footnote>
<bodyText confidence="0.999038744186046">
figures, micro-averaged over the 21 words and
over the ten folds, are reported in table 1. The
comparison leads to the following conclusions:
As expected, the five algorithms significantly
outperform the baseline MFC classifier. Among
them, three groups can be observed: NB, DL,
and SN perform similarly; LB outperforms all
the other algorithms in all experiments; and EB
is somewhere in between. The difference be-
tween LB and the rest is statistically significant
in all cases except when comparing LB to the EB
approach in the case marked with an asterisk4.
Extremely poor results are observed when
testing the portability of the systems. Restrict-
ing to LB results, it can be observed that the
accuracy obtained in A-B is 47.1%, while the
accuracy in B-B (which can be considered an
upper bound for LB in B corpus) is 59.0%, that
is, that there is a difference of 12 points. Fur-
thermore, 47.1% is only slightly better than the
most frequent sense in corpus B, 45.5%.
Apart from accuracy figures, the comparison
between the predictions made by the five meth-
ods on the test sets provides interesting infor-
mation about the relative behaviour of the algo-
rithms. Table 2 shows the agreement rates and
the Kappa statistics5 between all pairs of meth-
ods in the A+B-A+B experiment. Note that
&apos;DSO&apos; stands for the annotation of DSO corpus,
which is taken as the correct one.
It can be observed that NB obtains the most
similar results with regard to MFC in agreement
and Kappa values. The agreement ratio is 74%,
that is, almost 3 out of 4 times it predicts the
most frequent sense. On the other extreme, LB
obtains the most similar results with regard to
DSO in agreement and Kappa values, and it has
the least similar with regard to MFC, suggesting
respectively. In this notation, the training set is placed
on the left hand side of symbol &amp;quot;-&amp;quot;, while the test set
is on the right hand side. For instance, A-B means that
the training set is corpus A and the test set is corpus B.
The symbol &amp;quot;+&amp;quot; stands for set union.
</bodyText>
<footnote confidence="0.477971">
4Statistical tests of significance applied: McNemar&apos;s
test and 10-fold cross-validation paired Student&apos;s t-test
at a confidence value of 95% (Dietterich, 1998).
5The Kappa statistic (Cohen, 1960) is a better mea-
sure of inter-annotator agreement which reduces the ef-
fect of chance agreement. It has been used for measur-
ing inter-annotator agreement during the construction
of semantic annotated corpora (Veronis, 1998; Ng et al.,
1999). A Kappa value of 1 indicates perfect agreement,
while 0.8 is considered as indicating good agreement.
</footnote>
<page confidence="0.976237">
33
</page>
<table confidence="0.999789875">
Accu racy (%)
A+B-A+B A+B-A A+B-B A-A B-B A-B B-A
MFC 46.55+0.71 53.90+2.01 39.21+1.90 55.94±1.10 45.52+1.27 36.40 38.71
Naive Bayes 61.55+1.04 67.25+1.07 55.85+1.81 65.86±1.11 56.80+1.12 41.38 47.66
Decision Lists 61.58+0.98 67.64+0.94 55.53+1.88 67.57+1.44 56.56+1.89 43.01 48.83
SNoW 60.92+1.09 65.57+1.33 56.28±1.10 67.12+1.16 56.13+1.23 44.07 49.76
Exemplar-based 63.01+0.93 69.08+1.66 56.97+1.22 68.98+1.06 57.36+1.68 45.32 51.13
LazyBoosting 66.32±1.34 71.79±1.51 60.85±1.81 71.26±1.15 58.96+1.86 47.10 51.99*
</table>
<tableCaption confidence="0.991595">
Table 1: Accuracy results (± standard deviation) of the methods on all training-test combinations
</tableCaption>
<table confidence="0.999943222222222">
A+B-A+B
DSO MFC NB EB SN DL LB
DSO - 46.6 61.6 63.0 60.9 61.6 66.3
MFC -0.19 - 73.9 60.0 55.9 64.9 54.9
NB 0.24 -0.09 - 76.3 74.5 76.8 71.4
EB 0.36 -0.15 0.44 - 69.6 70.7 72.5
SN 0.36 -0.17 0.44 0.44 - 67.5 69.0
DL 0.32 -0.13 0.40 0.41 0.38 - 69.9
LB 0.44 -0.17 0.37 0.50 0.46 0.42 -
</table>
<tableCaption confidence="0.985314">
Table 2: Kappa statistic (below diagonal) and
</tableCaption>
<bodyText confidence="0.98297">
% of agreement (above diagonal) between all
methods in the A+B-A+B experiment
that LB is the algorithm that better learns the
behaviour of the DSO examples.
In absolute terms, the Kappa values are very
low. But, as it is suggested in (Veronis, 1998),
evaluation measures should be computed rela-
tive to the agreement between the human an-
notators of the corpus and not to a theoreti-
cal 100%. It seems pointless to expect more
agreement between the system and the refer-
ence corpus than between the annotators them-
selves. Contrary to the intuition that the agree-
ment between human annotators should be very
high in the WSD task, some papers report sur-
prisingly low figures. For instance, (Ng et al.,
1999) reports an accuracy rate of 56.7% and a
Kappa value of 0.317 when comparing the anno-
tation of a subset of the DSO corpus performed
by two independent research groups. From this
perspective, the Kappa value of 0.44 achieved
by LB in A+B-A+B could be considered an ex-
cellent result. Unfortunately, the subset of the
DSO corpus studied by (Ng et al., 1999) and
that used in this report are not the same and,
thus, a direct comparison is not possible.
</bodyText>
<subsectionHeader confidence="0.997452">
4.1 About the tuning to new domains
</subsectionHeader>
<bodyText confidence="0.999951846153846">
This experiment explores the effect of a sim-
ple tuning process consisting in adding to the
original training set A a relatively small sample
of manually sense-tagged examples of the new
domain B. The size of this supervised portion
varies from 10% to 50% of the available corpus
in steps of 10% (the remaining 50% is kept for
testing)6. This experiment will be referred to
as A+%B-B7. In order to determine to which
extent the original training set contributes to
accurately disambiguating in the new domain,
we also calculate the results for %B-B, that is,
using only the tuning corpus for training.
</bodyText>
<figureCaption confidence="0.5283142">
Figure 1 graphically presents the results ob-
tained by all methods. Each plot contains the
A+%B-B and %B-B curves, and the straight
lines corresponding to the lower bound MFC,
and to the upper bounds B-B and A+B-B.
</figureCaption>
<bodyText confidence="0.999974625">
As expected, the accuracy of all methods
grows (towards the upper bound) as more tun-
ing corpus is added to the training set. How-
ever, the relation between A+%B-B and %B-B
reveals some interesting facts. In plots (c) and
(d), the contribution of the original training cor-
pus is null, while in plots (a) and (b), a degrada-
tion on the accuracy is observed. Summarizing,
these results suggest that for NB, DL, SN, and
EB methods it is not worth keeping the original
training examples. Instead, a better (but dis-
appointing) strategy would be simply using the
tuning corpus. However, this is not the situa-
tion of LB -plot (d)- for which a moderate,
but consistent, improvement of accuracy is ob-
served when retaining the original training set.
</bodyText>
<footnote confidence="0.870434777777778">
6Tuning examples can be weighted more highly than
the training examples to force the learning algorithm to
adapt more quickly to the new corpus. Some experi-
ments in this direction revealed that slightly better re-
sults can be obtained, though the improvement was not
statistically significant.
7The converse experiment B-I-%A-A is not reported
in this paper due to space limitations. Results can be
found in (Escudero et al., 2000c).
</footnote>
<page confidence="0.997183">
34
</page>
<figure confidence="0.98307503125">
(a) Naive Bayes
-
(b) Decision Lists
(c) Exemplar Based
MFS
B-B
A+B-B a
A+%B-B -
58
56
54
t&amp;quot; 52
0 50
3
48
46
58
56
54
52
113
8. 50
48
46
44
A+%B-B
•
42
0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50
44
0 5 10 15 20 25 30 35 40 45 50
58
56
54
F 52
50
Ea: 48
g46
44
42
40
(d) SNoW
(e) LazyBoosting
A44343 a—
A+%B-B -
58
56
54
g 52
50
48
46
44
62
60
58
oe&amp;quot;.. 56
fi 54
I. 52
50
48
46
44
0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50
</figure>
<figureCaption confidence="0.999995">
Figure 1: Results of the tuning experiment
</figureCaption>
<bodyText confidence="0.999937944444444">
We observed that part of the poor results
obtained is explained by: 1) corpus A and
B have a very different distribution of senses,
and, therefore, different a—priori biases; further-
more, 2) examples of corpus A and B contain
different information and, therefore, the learn-
ing algorithms acquire different (and non inter-
changeable) classification clues from both cor-
pora. The study of the rules acquired by Lazy-
Boosting from WSJ and BC helped understand-
ing the differences between corpora. On the one
hand, the type of features used in the rules was
significantly different between corpora and, ad-
ditionally, there were very few rules that applied
to both sets. On the other hand, the sign of the
prediction of many of these common rules was
somewhat contradictory between corpora. See
(Escudero et al., 2000c) for details.
</bodyText>
<subsectionHeader confidence="0.992463">
4.2 About the training data quality
</subsectionHeader>
<bodyText confidence="0.999923652173913">
The observation of the rules acquired by Lazy-
Boosting could also help improving data quality
in a semi—supervised fashion. It is known that
mislabelled examples resulting from annotation
errors tend to be hard examples to classify cor-
rectly and, therefore, tend to have large weights
in the final distribution. This observation al-
lows both to identify the noisy examples and
use LazyBoosting as a way to improve the train-
ing corpus.
A preliminary experiment has been carried
out in this direction by studying the rules ac-
quired by LazyBoosting from the training ex-
amples of the word state. The manual revi-
sion, by four different people, of the 50 high-
est scored rules, allowed us to identify 28 noisy
training examples. 11 of them were clear tag-
ging errors, and the remaining 17 were not co-
herently tagged and very difficult to judge, since
the four annotators achieved systematic dis-
agreement (probably due to the extremely fine
grained sense definitions involved in these ex-
amples).
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999923444444444">
This work reports a comparative study of five
ML algorithms for WSD, and provides some re-
sults on cross corpora evaluation and domain
re-tuning.
Regarding portability, it seems that the per-
formance of supervised sense taggers is not
guaranteed when moving from one domain to
another (e.g. from a balanced corpus, such
as BC, to an economic domain, such as WSJ).
</bodyText>
<page confidence="0.997454">
35
</page>
<bodyText confidence="0.999929277777778">
These results imply that some kind of adap-
tation is required for cross-corpus application.
Consequently, it is our belief that a number of
issues regarding portability, tuning, knowledge
acquisition, etc., should be thoroughly studied
before stating that the supervised ML paradigm
is able to resolve a realistic WSD problem.
Regarding the ML algorithms tested, Lazy-
Boosting emerges as the best option, since
it outperforms the other four state-of-the-art
methods in all experiments. Furthermore, this
algorithm shows better properties when tuned
to new domains. Future work is planned for
an extensive evaluation of LazyBoosting on the
WSD task. This would include taking into ac-
count additional/alternative attributes, learn-
ing curves, testing the algorithm on other cor-
pora, etc.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999294263736264">
E. Agirre and D. Martinez. 2000. Decision Lists and
Automatic Word Sense Disambiguation. In Pro-
ceedings of the COLING Workshop on Semantic
Annotation and Intelligent Content.
D. Aha, D. Kibler, and M. Albert. 1991. Instance-
based Learning Algorithms. Machine Learning,
7:37-66.
R. F. Bruce and J. M. Wiebe. 1999. Decomposable
Modeling in Natural Language Processing. Com-
putational Linguistics, 25(2):195-207.
J. Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Journal of Educational and Psy-
chological Measurement, 20:37-46.
W. Daelemans, A. van den Bosch, and J. Zavrel.
1999. Forgetting Exceptions is Harmful in Lan-
guage Learning. Machine Learning, 34:11-41.
T. G. Dietterich. 1998. Approximate Statistical
Tests for Comparing Supervised Classification
Learning Algorithms. Neural Computation, 10(7).
R. 0. Duda and P. E. Hart. 1973. Pattern Classifi-
cation and Scene Analysis. Wiley &amp; Sons.
G. Escudero, L. Marquez, and G. Rigau. 2000a.
Boosting Applied to Word Sense Disambiguation.
In Proceedings of the 12th European Conference
on Machine Learning, ECML.
G. Escudero, L. Marquez, and G. Rigau. 2000b.
Naive Bayes and Exemplar-Based Approaches to
Word Sense Disambiguation Revisited. In Pro-
ceedings of the 14th European Conference on Ar-
tificial Intelligence, ECAL
G. Escudero, L. Marquez, and G. Rigau. 2000c. On
the Portability and Tuning of Supervised Word
Sense Disambiguation Systems. Research Report
LSI-00-30-R, Software Department (LSI). Techni-
cal University of Catalonia (UPC).
N. Ide and J. Veronis. 1998. Introduction to the
Special Issue on Word Sense Disambiguation:
The State of the Art. Computational Linguistics,
24(1):1-40.
A. Kilgarriff and J. Rosenzweig. 2000. English SEN-
SEVAL: Report and Results. In Proceedings of the
2nd International Conference on Language Re-
sources and Evaluation, LREC.
C. Leacock, M. Chodorow, and G. A. Miller. 1998.
Using Corpus Statistics and WordNet Relations
for Sense Identification. Computational Linguis-
tics, 24(1):147-166.
N. Littlestone. 1988. Learning Quickly when Ir-
relevant Attributes Abound. Machine Learning,
2:285-318.
R. J. Mooney. 1996. Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proceed-
ings of the 1st Conference on Empirical Methods
in Natural Language Processing, EMNLP.
H. T. Ng and H. B. Lee. 1996. Integrating Multiple
Knowledge Sources to Disambiguate Word Senses:
An Exemplar-based Approach. In Proceedings of
the 34th Annual Meeting of the ACL.
H. T. Ng, C. Lim, and S. Foo. 1999. A Case Study
on Inter-Annotator Agreement for Word Sense
Disambiguation. In Procs. of the ACL SIGLEX
Workshop: Standardizing Lexical Resources.
H. T. Ng. 1997. Exemplar-Base Word Sense Disam-
biguation: Some Recent Improvements. In Procs.
of the 2nd Conference on Empirical Methods in
Natural Language Processing, EMNLP.
A. Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Ma-
chine Learning, 34:151-175.
D. Roth. 1998. Learning to Resolve Natural Lan-
guage Ambiguities: A Unified Approach. In Pro-
ceedings of the National Conference on Artificial
Intelligence, AAAI &apos;98.
R. E. Schapire and Y. Singer. 1999. Improved
Boosting Algorithms Using Confidence-rated Pre-
dictions. Machine Learning, 37(3):297-336.
S. Sekine. 1997. The Domain Dependence of Pars-
ing. In Proceedings of the 5th Conference on Ap-
plied Natural Language Processing, ANLP.
G. Towell and E. M. Voorhees. 1998. Disambiguat-
ing Highly Ambiguous Words. Computational
Linguistics, 24(1):125-146.
J. Veronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In Programme
and advanced papers of the Senseval workshop,
Herstmonceux Castle, England.
D. Yarowsky. 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to Accent Restora-
tion in Spanish and French. In Proceedings of the
32nd Annual Meeting of the ACL.
</reference>
<page confidence="0.998894">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420905">
<note confidence="0.89116">of CoNLL-2000 and LLL-2000, 31-36, Lisbon, Portugal, 2000.</note>
<title confidence="0.994982">A Comparison between Supervised Learning Algorithms for Word Sense Disambiguation*</title>
<author confidence="0.996091">Escudero Marquez</author>
<affiliation confidence="0.89901">Research Center. Universitat Politecnica de Catalunya</affiliation>
<address confidence="0.516543">Jordi Girona Salgado 1-3. E-08034 Barcelona.</address>
<email confidence="0.984714">escuderoInsi.upc.es</email>
<email confidence="0.984714">lluismInsi.upc.es</email>
<email confidence="0.984714">g.rigauInsi.upc.es</email>
<abstract confidence="0.996664785714286">This paper describes a set of comparative experiments, including cross–corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Bayes, Exemplar-based learning, Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-theart algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross–corpus application.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Decision Lists and Automatic Word Sense Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING Workshop on Semantic Annotation and Intelligent Content.</booktitle>
<contexts>
<context position="2457" citStr="Agirre and Martinez, 2000" startWordPosition="357" endWordPosition="360"> partially funded by the Spanish Research Department (CICYT&apos;s project TIC98-0423— C06), by the EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Department (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether th</context>
<context position="7200" citStr="Agirre and Martinez, 2000" startWordPosition="1124" endWordPosition="1127">this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disambiguation. Regarding the details of implementation (smoothing, pruning of the decision list, etc.) we have followed (Agirre and Martinez, 2000). Decision Lists were one of the most successful systems on the 1st Senseval competition for WSD (Kilgarriff and Rosenzweig, 2000). LazyBoosting (LB). The main idea of boosting algorithms is to combine many simple and moderately accurate hypotheses (weak classifiers) into a single, highly accurate classifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire an</context>
</contexts>
<marker>Agirre, Martinez, 2000</marker>
<rawString>E. Agirre and D. Martinez. 2000. Decision Lists and Automatic Word Sense Disambiguation. In Proceedings of the COLING Workshop on Semantic Annotation and Intelligent Content.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instancebased Learning Algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>7--37</pages>
<contexts>
<context position="4512" citStr="Aha et al., 1991" startWordPosition="682" endWordPosition="685">w example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state–of–the–art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). Exemplar-based Classifier (EB). In exemplar, instance, or memory–based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k–NN (Nearest–Neighbours) algorithm using Hamming distance to measure closeness. For k&apos;s greater than 1, the resulting sense is the weighted majority sense of the k nearest neighbours —where each example votes its sense with a strength proportional to its closeness to the test example. Exemplar–based learning is said</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. Aha, D. Kibler, and M. Albert. 1991. Instancebased Learning Algorithms. Machine Learning, 7:37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Bruce</author>
<author>J M Wiebe</author>
</authors>
<date>1999</date>
<booktitle>Decomposable Modeling in Natural Language Processing. Computational Linguistics,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2545" citStr="Bruce and Wiebe, 1999" startWordPosition="369" endWordPosition="372">he EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Department (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora </context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>R. F. Bruce and J. M. Wiebe. 1999. Decomposable Modeling in Natural Language Processing. Computational Linguistics, 25(2):195-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales.</title>
<date>1960</date>
<journal>Journal of Educational and Psychological Measurement,</journal>
<pages>20--37</pages>
<contexts>
<context position="12818" citStr="Cohen, 1960" startWordPosition="2064" endWordPosition="2065">me, LB obtains the most similar results with regard to DSO in agreement and Kappa values, and it has the least similar with regard to MFC, suggesting respectively. In this notation, the training set is placed on the left hand side of symbol &amp;quot;-&amp;quot;, while the test set is on the right hand side. For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol &amp;quot;+&amp;quot; stands for set union. 4Statistical tests of significance applied: McNemar&apos;s test and 10-fold cross-validation paired Student&apos;s t-test at a confidence value of 95% (Dietterich, 1998). 5The Kappa statistic (Cohen, 1960) is a better measure of inter-annotator agreement which reduces the effect of chance agreement. It has been used for measuring inter-annotator agreement during the construction of semantic annotated corpora (Veronis, 1998; Ng et al., 1999). A Kappa value of 1 indicates perfect agreement, while 0.8 is considered as indicating good agreement. 33 Accu racy (%) A+B-A+B A+B-A A+B-B A-A B-B A-B B-A MFC 46.55+0.71 53.90+2.01 39.21+1.90 55.94±1.10 45.52+1.27 36.40 38.71 Naive Bayes 61.55+1.04 67.25+1.07 55.85+1.81 65.86±1.11 56.80+1.12 41.38 47.66 Decision Lists 61.58+0.98 67.64+0.94 55.53+1.88 67.57+</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Journal of Educational and Psychological Measurement, 20:37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<date>1999</date>
<booktitle>Forgetting Exceptions is Harmful in Language Learning. Machine Learning,</booktitle>
<pages>34--11</pages>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting Exceptions is Harmful in Language Learning. Machine Learning, 34:11-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.</title>
<date>1998</date>
<journal>Neural Computation,</journal>
<volume>10</volume>
<issue>7</issue>
<contexts>
<context position="12782" citStr="Dietterich, 1998" startWordPosition="2059" endWordPosition="2060">e most frequent sense. On the other extreme, LB obtains the most similar results with regard to DSO in agreement and Kappa values, and it has the least similar with regard to MFC, suggesting respectively. In this notation, the training set is placed on the left hand side of symbol &amp;quot;-&amp;quot;, while the test set is on the right hand side. For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol &amp;quot;+&amp;quot; stands for set union. 4Statistical tests of significance applied: McNemar&apos;s test and 10-fold cross-validation paired Student&apos;s t-test at a confidence value of 95% (Dietterich, 1998). 5The Kappa statistic (Cohen, 1960) is a better measure of inter-annotator agreement which reduces the effect of chance agreement. It has been used for measuring inter-annotator agreement during the construction of semantic annotated corpora (Veronis, 1998; Ng et al., 1999). A Kappa value of 1 indicates perfect agreement, while 0.8 is considered as indicating good agreement. 33 Accu racy (%) A+B-A+B A+B-A A+B-B A-A B-B A-B B-A MFC 46.55+0.71 53.90+2.01 39.21+1.90 55.94±1.10 45.52+1.27 36.40 38.71 Naive Bayes 61.55+1.04 67.25+1.07 55.85+1.81 65.86±1.11 56.80+1.12 41.38 47.66 Decision Lists 61.</context>
</contexts>
<marker>Dietterich, 1998</marker>
<rawString>T. G. Dietterich. 1998. Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms. Neural Computation, 10(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>P E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley &amp; Sons.</publisher>
<contexts>
<context position="3827" citStr="Duda and Hart, 1973" startWordPosition="575" endWordPosition="578">D —in the style of other studies devoted to parsing (Sekine, 1997; Ratnaparkhi, 1999)— is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre–process is necessary to make real WSD systems portable. In this direction, this work compares five different ML algorithms and explores their portability and tuning ability by training and testing them on different corpora. 2 Learning Algorithms Tested Naive-Bayes (NB). Naive Bayes is intended as a simple representative of statistical learning methods. It has been used in its most classi31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state–of–the–art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). Exemplar-base</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. 0. Duda and P. E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Rigau</author>
</authors>
<title>Boosting Applied to Word Sense Disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning, ECML.</booktitle>
<contexts>
<context position="2614" citStr="Escudero et al., 2000" startWordPosition="378" endWordPosition="381">epartment (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in t</context>
<context position="6271" citStr="Escudero et al., 2000" startWordPosition="974" endWordPosition="977">ositive example for the winnow node associated to its class and a negative example for all the others. A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm (Littlestone, 1988) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems. are &amp;quot;relevant&amp;quot; for their class. When classifying a new example, SNoW is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, PP–attachment disambiguation, etc. Decision Lists (DL). In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and </context>
<context position="8122" citStr="Escudero et al., 2000" startWordPosition="1267" endWordPosition="1271">assifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire and Singer&apos;s real AdaBoost.MH algorithm for multiclass multi–label classification (Schapire and Singer, 1999) has been used. It constructs a combination of very simple weak hypotheses that test the value of a single boolean predicate and make a real–valued prediction based on that value. LazyBoosting (Escudero et al., 2000a) is a simple modification of the AdaBoost.MH algorithm, which consists in reducing the feature space that is explored when learning each weak classifier. This modification significantly increases the efficiency of 32 the learning process with no loss in accuracy. 3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO corpus, which is a semantically annotated English corpus collected by Ng and colleagues (Ng and Lee, 1996). Each word is treated as a different classification problem. The 21 words comprise 13 nouns (age, art, body,</context>
<context position="10126" citStr="Escudero et al., 2000" startWordPosition="1608" endWordPosition="1611">w to be disambiguated, and p±i (-3 &lt; i &lt; 3) be the part-of-speech tag of word w±i. Attributes referring to local context are the following 15: P-3/ P-2/ P-1/ P-I-1/ P+2/ P+3/ w-11 w+1/ (w-2/ w-1)/ (w-1/ IBA/ (W+1, W+2), (w-3/ w-2/ w-1)/ (w-2/ w-1/7141)/ (w-1/ w+1, w+2), and (w+i, w+2, w+3), where the last seven correspond to collocations of two and three consecutive words. The topical context is formed by cl, ,C, which stand for the unordered set of open class words appearing in the sentence2. Details about how the different algorithms translate this information into features can be found in (Escudero et al., 2000c). 4 Comparing the five approaches The five algorithms, jointly with a naive MostFrequent-sense Classifier (MFC), have been tested, by 10-fold cross validation, on 7 different combinations of training-test sets3. Accuracy 2 This set of attributes corresponds to that used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation. 3The combinations of training-test sets are called: A+B-A-1-B, A±B-A, A-A, B-B, A-B, and B-A, figures, micro-averaged over the 21 words and over the ten folds, are reported in table 1. The comparison leads to</context>
<context position="17276" citStr="Escudero et al., 2000" startWordPosition="2807" endWordPosition="2810">sing the tuning corpus. However, this is not the situation of LB -plot (d)- for which a moderate, but consistent, improvement of accuracy is observed when retaining the original training set. 6Tuning examples can be weighted more highly than the training examples to force the learning algorithm to adapt more quickly to the new corpus. Some experiments in this direction revealed that slightly better results can be obtained, though the improvement was not statistically significant. 7The converse experiment B-I-%A-A is not reported in this paper due to space limitations. Results can be found in (Escudero et al., 2000c). 34 (a) Naive Bayes - (b) Decision Lists (c) Exemplar Based MFS B-B A+B-B a A+%B-B - 58 56 54 t&amp;quot; 52 0 50 3 48 46 58 56 54 52 113 8. 50 48 46 44 A+%B-B • 42 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 44 0 5 10 15 20 25 30 35 40 45 50 58 56 54 F 52 50 Ea: 48 g46 44 42 40 (d) SNoW (e) LazyBoosting A44343 a— A+%B-B - 58 56 54 g 52 50 48 46 44 62 60 58 oe&amp;quot;.. 56 fi 54 I. 52 50 48 46 44 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 Figure 1: Results of the tuning experiment We observed that part of the poor results obtained is explained by: 1) corpus A and B have</context>
<context position="18594" citStr="Escudero et al., 2000" startWordPosition="3086" endWordPosition="3089"> 2) examples of corpus A and B contain different information and, therefore, the learning algorithms acquire different (and non interchangeable) classification clues from both corpora. The study of the rules acquired by LazyBoosting from WSJ and BC helped understanding the differences between corpora. On the one hand, the type of features used in the rules was significantly different between corpora and, additionally, there were very few rules that applied to both sets. On the other hand, the sign of the prediction of many of these common rules was somewhat contradictory between corpora. See (Escudero et al., 2000c) for details. 4.2 About the training data quality The observation of the rules acquired by LazyBoosting could also help improving data quality in a semi—supervised fashion. It is known that mislabelled examples resulting from annotation errors tend to be hard examples to classify correctly and, therefore, tend to have large weights in the final distribution. This observation allows both to identify the noisy examples and use LazyBoosting as a way to improve the training corpus. A preliminary experiment has been carried out in this direction by studying the rules acquired by LazyBoosting from</context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. Marquez, and G. Rigau. 2000a. Boosting Applied to Word Sense Disambiguation. In Proceedings of the 12th European Conference on Machine Learning, ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Rigau</author>
</authors>
<title>Naive Bayes and Exemplar-Based Approaches to Word Sense Disambiguation Revisited.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th European Conference on Artificial Intelligence, ECAL</booktitle>
<contexts>
<context position="2614" citStr="Escudero et al., 2000" startWordPosition="378" endWordPosition="381">epartment (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in t</context>
<context position="6271" citStr="Escudero et al., 2000" startWordPosition="974" endWordPosition="977">ositive example for the winnow node associated to its class and a negative example for all the others. A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm (Littlestone, 1988) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems. are &amp;quot;relevant&amp;quot; for their class. When classifying a new example, SNoW is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, PP–attachment disambiguation, etc. Decision Lists (DL). In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and </context>
<context position="8122" citStr="Escudero et al., 2000" startWordPosition="1267" endWordPosition="1271">assifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire and Singer&apos;s real AdaBoost.MH algorithm for multiclass multi–label classification (Schapire and Singer, 1999) has been used. It constructs a combination of very simple weak hypotheses that test the value of a single boolean predicate and make a real–valued prediction based on that value. LazyBoosting (Escudero et al., 2000a) is a simple modification of the AdaBoost.MH algorithm, which consists in reducing the feature space that is explored when learning each weak classifier. This modification significantly increases the efficiency of 32 the learning process with no loss in accuracy. 3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO corpus, which is a semantically annotated English corpus collected by Ng and colleagues (Ng and Lee, 1996). Each word is treated as a different classification problem. The 21 words comprise 13 nouns (age, art, body,</context>
<context position="10126" citStr="Escudero et al., 2000" startWordPosition="1608" endWordPosition="1611">w to be disambiguated, and p±i (-3 &lt; i &lt; 3) be the part-of-speech tag of word w±i. Attributes referring to local context are the following 15: P-3/ P-2/ P-1/ P-I-1/ P+2/ P+3/ w-11 w+1/ (w-2/ w-1)/ (w-1/ IBA/ (W+1, W+2), (w-3/ w-2/ w-1)/ (w-2/ w-1/7141)/ (w-1/ w+1, w+2), and (w+i, w+2, w+3), where the last seven correspond to collocations of two and three consecutive words. The topical context is formed by cl, ,C, which stand for the unordered set of open class words appearing in the sentence2. Details about how the different algorithms translate this information into features can be found in (Escudero et al., 2000c). 4 Comparing the five approaches The five algorithms, jointly with a naive MostFrequent-sense Classifier (MFC), have been tested, by 10-fold cross validation, on 7 different combinations of training-test sets3. Accuracy 2 This set of attributes corresponds to that used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation. 3The combinations of training-test sets are called: A+B-A-1-B, A±B-A, A-A, B-B, A-B, and B-A, figures, micro-averaged over the 21 words and over the ten folds, are reported in table 1. The comparison leads to</context>
<context position="17276" citStr="Escudero et al., 2000" startWordPosition="2807" endWordPosition="2810">sing the tuning corpus. However, this is not the situation of LB -plot (d)- for which a moderate, but consistent, improvement of accuracy is observed when retaining the original training set. 6Tuning examples can be weighted more highly than the training examples to force the learning algorithm to adapt more quickly to the new corpus. Some experiments in this direction revealed that slightly better results can be obtained, though the improvement was not statistically significant. 7The converse experiment B-I-%A-A is not reported in this paper due to space limitations. Results can be found in (Escudero et al., 2000c). 34 (a) Naive Bayes - (b) Decision Lists (c) Exemplar Based MFS B-B A+B-B a A+%B-B - 58 56 54 t&amp;quot; 52 0 50 3 48 46 58 56 54 52 113 8. 50 48 46 44 A+%B-B • 42 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 44 0 5 10 15 20 25 30 35 40 45 50 58 56 54 F 52 50 Ea: 48 g46 44 42 40 (d) SNoW (e) LazyBoosting A44343 a— A+%B-B - 58 56 54 g 52 50 48 46 44 62 60 58 oe&amp;quot;.. 56 fi 54 I. 52 50 48 46 44 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 Figure 1: Results of the tuning experiment We observed that part of the poor results obtained is explained by: 1) corpus A and B have</context>
<context position="18594" citStr="Escudero et al., 2000" startWordPosition="3086" endWordPosition="3089"> 2) examples of corpus A and B contain different information and, therefore, the learning algorithms acquire different (and non interchangeable) classification clues from both corpora. The study of the rules acquired by LazyBoosting from WSJ and BC helped understanding the differences between corpora. On the one hand, the type of features used in the rules was significantly different between corpora and, additionally, there were very few rules that applied to both sets. On the other hand, the sign of the prediction of many of these common rules was somewhat contradictory between corpora. See (Escudero et al., 2000c) for details. 4.2 About the training data quality The observation of the rules acquired by LazyBoosting could also help improving data quality in a semi—supervised fashion. It is known that mislabelled examples resulting from annotation errors tend to be hard examples to classify correctly and, therefore, tend to have large weights in the final distribution. This observation allows both to identify the noisy examples and use LazyBoosting as a way to improve the training corpus. A preliminary experiment has been carried out in this direction by studying the rules acquired by LazyBoosting from</context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. Marquez, and G. Rigau. 2000b. Naive Bayes and Exemplar-Based Approaches to Word Sense Disambiguation Revisited. In Proceedings of the 14th European Conference on Artificial Intelligence, ECAL</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Escudero</author>
<author>L Marquez</author>
<author>G Rigau</author>
</authors>
<title>On the Portability and Tuning of Supervised Word Sense Disambiguation Systems.</title>
<date>2000</date>
<institution>Software Department (LSI). Technical University of Catalonia (UPC).</institution>
<note>Research Report LSI-00-30-R,</note>
<contexts>
<context position="2614" citStr="Escudero et al., 2000" startWordPosition="378" endWordPosition="381">epartment (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in t</context>
<context position="6271" citStr="Escudero et al., 2000" startWordPosition="974" endWordPosition="977">ositive example for the winnow node associated to its class and a negative example for all the others. A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm (Littlestone, 1988) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems. are &amp;quot;relevant&amp;quot; for their class. When classifying a new example, SNoW is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, PP–attachment disambiguation, etc. Decision Lists (DL). In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and </context>
<context position="8122" citStr="Escudero et al., 2000" startWordPosition="1267" endWordPosition="1271">assifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire and Singer&apos;s real AdaBoost.MH algorithm for multiclass multi–label classification (Schapire and Singer, 1999) has been used. It constructs a combination of very simple weak hypotheses that test the value of a single boolean predicate and make a real–valued prediction based on that value. LazyBoosting (Escudero et al., 2000a) is a simple modification of the AdaBoost.MH algorithm, which consists in reducing the feature space that is explored when learning each weak classifier. This modification significantly increases the efficiency of 32 the learning process with no loss in accuracy. 3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO corpus, which is a semantically annotated English corpus collected by Ng and colleagues (Ng and Lee, 1996). Each word is treated as a different classification problem. The 21 words comprise 13 nouns (age, art, body,</context>
<context position="10126" citStr="Escudero et al., 2000" startWordPosition="1608" endWordPosition="1611">w to be disambiguated, and p±i (-3 &lt; i &lt; 3) be the part-of-speech tag of word w±i. Attributes referring to local context are the following 15: P-3/ P-2/ P-1/ P-I-1/ P+2/ P+3/ w-11 w+1/ (w-2/ w-1)/ (w-1/ IBA/ (W+1, W+2), (w-3/ w-2/ w-1)/ (w-2/ w-1/7141)/ (w-1/ w+1, w+2), and (w+i, w+2, w+3), where the last seven correspond to collocations of two and three consecutive words. The topical context is formed by cl, ,C, which stand for the unordered set of open class words appearing in the sentence2. Details about how the different algorithms translate this information into features can be found in (Escudero et al., 2000c). 4 Comparing the five approaches The five algorithms, jointly with a naive MostFrequent-sense Classifier (MFC), have been tested, by 10-fold cross validation, on 7 different combinations of training-test sets3. Accuracy 2 This set of attributes corresponds to that used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation. 3The combinations of training-test sets are called: A+B-A-1-B, A±B-A, A-A, B-B, A-B, and B-A, figures, micro-averaged over the 21 words and over the ten folds, are reported in table 1. The comparison leads to</context>
<context position="17276" citStr="Escudero et al., 2000" startWordPosition="2807" endWordPosition="2810">sing the tuning corpus. However, this is not the situation of LB -plot (d)- for which a moderate, but consistent, improvement of accuracy is observed when retaining the original training set. 6Tuning examples can be weighted more highly than the training examples to force the learning algorithm to adapt more quickly to the new corpus. Some experiments in this direction revealed that slightly better results can be obtained, though the improvement was not statistically significant. 7The converse experiment B-I-%A-A is not reported in this paper due to space limitations. Results can be found in (Escudero et al., 2000c). 34 (a) Naive Bayes - (b) Decision Lists (c) Exemplar Based MFS B-B A+B-B a A+%B-B - 58 56 54 t&amp;quot; 52 0 50 3 48 46 58 56 54 52 113 8. 50 48 46 44 A+%B-B • 42 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 44 0 5 10 15 20 25 30 35 40 45 50 58 56 54 F 52 50 Ea: 48 g46 44 42 40 (d) SNoW (e) LazyBoosting A44343 a— A+%B-B - 58 56 54 g 52 50 48 46 44 62 60 58 oe&amp;quot;.. 56 fi 54 I. 52 50 48 46 44 0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50 Figure 1: Results of the tuning experiment We observed that part of the poor results obtained is explained by: 1) corpus A and B have</context>
<context position="18594" citStr="Escudero et al., 2000" startWordPosition="3086" endWordPosition="3089"> 2) examples of corpus A and B contain different information and, therefore, the learning algorithms acquire different (and non interchangeable) classification clues from both corpora. The study of the rules acquired by LazyBoosting from WSJ and BC helped understanding the differences between corpora. On the one hand, the type of features used in the rules was significantly different between corpora and, additionally, there were very few rules that applied to both sets. On the other hand, the sign of the prediction of many of these common rules was somewhat contradictory between corpora. See (Escudero et al., 2000c) for details. 4.2 About the training data quality The observation of the rules acquired by LazyBoosting could also help improving data quality in a semi—supervised fashion. It is known that mislabelled examples resulting from annotation errors tend to be hard examples to classify correctly and, therefore, tend to have large weights in the final distribution. This observation allows both to identify the noisy examples and use LazyBoosting as a way to improve the training corpus. A preliminary experiment has been carried out in this direction by studying the rules acquired by LazyBoosting from</context>
</contexts>
<marker>Escudero, Marquez, Rigau, 2000</marker>
<rawString>G. Escudero, L. Marquez, and G. Rigau. 2000c. On the Portability and Tuning of Supervised Word Sense Disambiguation Systems. Research Report LSI-00-30-R, Software Department (LSI). Technical University of Catalonia (UPC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="1287" citStr="Ide and Veronis, 1998" startWordPosition="184" endWordPosition="187">nclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-theart algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross–corpus application. 1 Introduction Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (or sense) to a given word in a text or discourse. Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks (Ide and Veronis, 1998). Besides, WSD is one of the most important open problems in NLP. Despite the wide range of approaches investigated (Kilgarriff and Rosenzweig, 2000) and the large effort devoted to tackling this problem, to date, no large-scale broad-coverage and highly accurate WSD system has been built. One of the most successful current lines of research is the corpus-based approach, in which statistical or Machine Learning (ML) algorithms have been applied to learn statistical models or classifiers from corpora in order to per* This research has been partially funded by the Spanish Research Department (CI</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>N. Ide and J. Veronis. 1998. Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art. Computational Linguistics, 24(1):1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>English SENSEVAL: Report and Results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation, LREC.</booktitle>
<contexts>
<context position="1436" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="207" endWordPosition="211">y to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross–corpus application. 1 Introduction Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (or sense) to a given word in a text or discourse. Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks (Ide and Veronis, 1998). Besides, WSD is one of the most important open problems in NLP. Despite the wide range of approaches investigated (Kilgarriff and Rosenzweig, 2000) and the large effort devoted to tackling this problem, to date, no large-scale broad-coverage and highly accurate WSD system has been built. One of the most successful current lines of research is the corpus-based approach, in which statistical or Machine Learning (ML) algorithms have been applied to learn statistical models or classifiers from corpora in order to per* This research has been partially funded by the Spanish Research Department (CICYT&apos;s project TIC98-0423— C06), by the EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Department (CIRIT&apos;s consolidated research gr</context>
<context position="7330" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="1145" endWordPosition="1148">ure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disambiguation. Regarding the details of implementation (smoothing, pruning of the decision list, etc.) we have followed (Agirre and Martinez, 2000). Decision Lists were one of the most successful systems on the 1st Senseval competition for WSD (Kilgarriff and Rosenzweig, 2000). LazyBoosting (LB). The main idea of boosting algorithms is to combine many simple and moderately accurate hypotheses (weak classifiers) into a single, highly accurate classifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire and Singer&apos;s real AdaBoost.MH algorithm for multiclass multi–label classification (Schapire and Singer, 1999) has been used. It cons</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig. 2000. English SENSEVAL: Report and Results. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G A Miller</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="4412" citStr="Leacock et al., 1998" startWordPosition="668" endWordPosition="671">31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state–of–the–art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). Exemplar-based Classifier (EB). In exemplar, instance, or memory–based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k–NN (Nearest–Neighbours) algorithm using Hamming distance to measure closeness. For k&apos;s greater than 1, the resulting sense is the weighted majority sense of the k nearest neighbours —where each example votes its sens</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>C. Leacock, M. Chodorow, and G. A. Miller. 1998. Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24(1):147-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Littlestone</author>
</authors>
<title>Learning Quickly when Irrelevant Attributes Abound.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--285</pages>
<contexts>
<context position="5916" citStr="Littlestone, 1988" startWordPosition="918" endWordPosition="919">not forget exceptions. The SNoW Architecture (SN). SNoW is a Sparse Network of linear separators which utilizes the Winnow learning algorithml. In the SNoW architecture there is a winnow node for each class, which learns to separate that class from all the rest. During training, which is performed in an on–line fashion, each example is considered a positive example for the winnow node associated to its class and a negative example for all the others. A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm (Littlestone, 1988) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems. are &amp;quot;relevant&amp;quot; for their class. When classifying a new example, SNoW is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, P</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>N. Littlestone. 1988. Learning Quickly when Irrelevant Attributes Abound. Machine Learning, 2:285-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1st Conference on Empirical Methods in Natural Language Processing, EMNLP.</booktitle>
<contexts>
<context position="2649" citStr="Mooney, 1996" startWordPosition="385" endWordPosition="386">oup 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in the style of other studies devoted t</context>
<context position="4379" citStr="Mooney, 1996" startWordPosition="664" endWordPosition="665"> used in its most classi31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state–of–the–art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). Exemplar-based Classifier (EB). In exemplar, instance, or memory–based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k–NN (Nearest–Neighbours) algorithm using Hamming distance to measure closeness. For k&apos;s greater than 1, the resulting sense is the weighted majority sense of the k nearest neighbours —</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. J. Mooney. 1996. Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning. In Proceedings of the 1st Conference on Empirical Methods in Natural Language Processing, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Senses: An Exemplar-based Approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="8613" citStr="Ng and Lee, 1996" startWordPosition="1348" endWordPosition="1351">value of a single boolean predicate and make a real–valued prediction based on that value. LazyBoosting (Escudero et al., 2000a) is a simple modification of the AdaBoost.MH algorithm, which consists in reducing the feature space that is explored when learning each weak classifier. This modification significantly increases the efficiency of 32 the learning process with no loss in accuracy. 3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO corpus, which is a semantically annotated English corpus collected by Ng and colleagues (Ng and Lee, 1996). Each word is treated as a different classification problem. The 21 words comprise 13 nouns (age, art, body, car, child, cost, head, interest, line, point, state, thing, work) and 8 verbs (become, fall, grow, lose, set, speak, strike, tell), which frequently appear in the WSD literature. The average number of senses per word is close to 10 and the number of training examples is around 1,000. The DSO corpus contains sentences from two different corpora, namely Wall Street Journal (WSJ) and Brown Corpus (BC). Therefore, it is easy to perform experiments about the portability of systems by train</context>
<context position="10420" citStr="Ng and Lee, 1996" startWordPosition="1653" endWordPosition="1656">the last seven correspond to collocations of two and three consecutive words. The topical context is formed by cl, ,C, which stand for the unordered set of open class words appearing in the sentence2. Details about how the different algorithms translate this information into features can be found in (Escudero et al., 2000c). 4 Comparing the five approaches The five algorithms, jointly with a naive MostFrequent-sense Classifier (MFC), have been tested, by 10-fold cross validation, on 7 different combinations of training-test sets3. Accuracy 2 This set of attributes corresponds to that used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation. 3The combinations of training-test sets are called: A+B-A-1-B, A±B-A, A-A, B-B, A-B, and B-A, figures, micro-averaged over the 21 words and over the ten folds, are reported in table 1. The comparison leads to the following conclusions: As expected, the five algorithms significantly outperform the baseline MFC classifier. Among them, three groups can be observed: NB, DL, and SN perform similarly; LB outperforms all the other algorithms in all experiments; and EB is somewhere in between. The differe</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H. T. Ng and H. B. Lee. 1996. Integrating Multiple Knowledge Sources to Disambiguate Word Senses: An Exemplar-based Approach. In Proceedings of the 34th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>C Lim</author>
<author>S Foo</author>
</authors>
<title>A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation.</title>
<date>1999</date>
<booktitle>In Procs. of the ACL SIGLEX Workshop: Standardizing Lexical Resources.</booktitle>
<contexts>
<context position="13057" citStr="Ng et al., 1999" startWordPosition="2100" endWordPosition="2103">ymbol &amp;quot;-&amp;quot;, while the test set is on the right hand side. For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol &amp;quot;+&amp;quot; stands for set union. 4Statistical tests of significance applied: McNemar&apos;s test and 10-fold cross-validation paired Student&apos;s t-test at a confidence value of 95% (Dietterich, 1998). 5The Kappa statistic (Cohen, 1960) is a better measure of inter-annotator agreement which reduces the effect of chance agreement. It has been used for measuring inter-annotator agreement during the construction of semantic annotated corpora (Veronis, 1998; Ng et al., 1999). A Kappa value of 1 indicates perfect agreement, while 0.8 is considered as indicating good agreement. 33 Accu racy (%) A+B-A+B A+B-A A+B-B A-A B-B A-B B-A MFC 46.55+0.71 53.90+2.01 39.21+1.90 55.94±1.10 45.52+1.27 36.40 38.71 Naive Bayes 61.55+1.04 67.25+1.07 55.85+1.81 65.86±1.11 56.80+1.12 41.38 47.66 Decision Lists 61.58+0.98 67.64+0.94 55.53+1.88 67.57+1.44 56.56+1.89 43.01 48.83 SNoW 60.92+1.09 65.57+1.33 56.28±1.10 67.12+1.16 56.13+1.23 44.07 49.76 Exemplar-based 63.01+0.93 69.08+1.66 56.97+1.22 68.98+1.06 57.36+1.68 45.32 51.13 LazyBoosting 66.32±1.34 71.79±1.51 60.85±1.81 71.26±1.15 </context>
<context position="14810" citStr="Ng et al., 1999" startWordPosition="2383" endWordPosition="2386"> is the algorithm that better learns the behaviour of the DSO examples. In absolute terms, the Kappa values are very low. But, as it is suggested in (Veronis, 1998), evaluation measures should be computed relative to the agreement between the human annotators of the corpus and not to a theoretical 100%. It seems pointless to expect more agreement between the system and the reference corpus than between the annotators themselves. Contrary to the intuition that the agreement between human annotators should be very high in the WSD task, some papers report surprisingly low figures. For instance, (Ng et al., 1999) reports an accuracy rate of 56.7% and a Kappa value of 0.317 when comparing the annotation of a subset of the DSO corpus performed by two independent research groups. From this perspective, the Kappa value of 0.44 achieved by LB in A+B-A+B could be considered an excellent result. Unfortunately, the subset of the DSO corpus studied by (Ng et al., 1999) and that used in this report are not the same and, thus, a direct comparison is not possible. 4.1 About the tuning to new domains This experiment explores the effect of a simple tuning process consisting in adding to the original training set A </context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>H. T. Ng, C. Lim, and S. Foo. 1999. A Case Study on Inter-Annotator Agreement for Word Sense Disambiguation. In Procs. of the ACL SIGLEX Workshop: Standardizing Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
</authors>
<title>Exemplar-Base Word Sense Disambiguation: Some Recent Improvements.</title>
<date>1997</date>
<booktitle>In Procs. of the 2nd Conference on Empirical Methods in Natural Language Processing, EMNLP.</booktitle>
<contexts>
<context position="2581" citStr="Ng, 1997" startWordPosition="375" endWordPosition="376">he Catalan Research Department (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of t</context>
<context position="4248" citStr="Ng, 1997" startWordPosition="644" endWordPosition="645">orithms Tested Naive-Bayes (NB). Naive Bayes is intended as a simple representative of statistical learning methods. It has been used in its most classi31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a new example by assigning the class that maximizes the conditional probability of the class given the observed sequence of features of that example. Model probabilities are estimated during the training process using relative frequencies. To avoid the effect of zero counts, a very simple smoothing technique has been used, which was proposed in (Ng, 1997). Despite its simplicity, Naive Bayes is claimed to obtain state–of–the–art accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997; Leacock et al., 1998). Exemplar-based Classifier (EB). In exemplar, instance, or memory–based learning (Aha et al., 1991) no generalization of training examples is performed. Instead, the examples are simply stored in memory and the classification of new examples is based on the most similar stored exemplars. In our implementation, all examples are kept in memory and the classification is based on a k–NN (Nearest–Neighbours) algorithm using Hamming dist</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>H. T. Ng. 1997. Exemplar-Base Word Sense Disambiguation: Some Recent Improvements. In Procs. of the 2nd Conference on Empirical Methods in Natural Language Processing, EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to Parse Natural Language with Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151</pages>
<contexts>
<context position="3292" citStr="Ratnaparkhi, 1999" startWordPosition="490" endWordPosition="491">hods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in the style of other studies devoted to parsing (Sekine, 1997; Ratnaparkhi, 1999)— is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre–process is necessary to make real WSD systems portable. In this direction, this work compares five different ML algorithms and explores their portability and tuning ability by training and testing them on different corpora. 2 Learning Algorithms Tested Naive-Bayes (NB). Naive Bayes is intended as a simple representative of statistical learning methods. It has been used in its most classi31 cal setting (Duda and Hart, 1973). That is, assuming the independence of features, it classifies a</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to Parse Natural Language with Maximum Entropy Models. Machine Learning, 34:151-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to Resolve Natural Language Ambiguities: A Unified Approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence, AAAI &apos;98.</booktitle>
<contexts>
<context position="6456" citStr="Roth, 1998" startWordPosition="1006" endWordPosition="1007">atures but only to those that 1The Winnow algorithm (Littlestone, 1988) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems. are &amp;quot;relevant&amp;quot; for their class. When classifying a new example, SNoW is similar to a neural network which takes the input features and outputs the class with the highest activation. Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, PP–attachment disambiguation, etc. Decision Lists (DL). In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disa</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to Resolve Natural Language Ambiguities: A Unified Approach. In Proceedings of the National Conference on Artificial Intelligence, AAAI &apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved Boosting Algorithms Using Confidence-rated Predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="7907" citStr="Schapire and Singer, 1999" startWordPosition="1231" endWordPosition="1234">etition for WSD (Kilgarriff and Rosenzweig, 2000). LazyBoosting (LB). The main idea of boosting algorithms is to combine many simple and moderately accurate hypotheses (weak classifiers) into a single, highly accurate classifier. The weak classifiers are trained sequentially and, conceptually, each of them is trained on the examples which were most difficult to classify by the preceding weak classifiers. These weak hypotheses are then linearly combined into a single rule called the combined hypothesis. Schapire and Singer&apos;s real AdaBoost.MH algorithm for multiclass multi–label classification (Schapire and Singer, 1999) has been used. It constructs a combination of very simple weak hypotheses that test the value of a single boolean predicate and make a real–valued prediction based on that value. LazyBoosting (Escudero et al., 2000a) is a simple modification of the AdaBoost.MH algorithm, which consists in reducing the feature space that is explored when learning each weak classifier. This modification significantly increases the efficiency of 32 the learning process with no loss in accuracy. 3 Setting A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO </context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R. E. Schapire and Y. Singer. 1999. Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning, 37(3):297-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>The Domain Dependence of Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing, ANLP.</booktitle>
<contexts>
<context position="3272" citStr="Sekine, 1997" startWordPosition="488" endWordPosition="489">e previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain corpus can be extrapolated to other corpora or not. We think that the study of the domain dependence of WSD —in the style of other studies devoted to parsing (Sekine, 1997; Ratnaparkhi, 1999)— is needed to assure the validity of the supervised approach, and to determine to which extent a tuning pre–process is necessary to make real WSD systems portable. In this direction, this work compares five different ML algorithms and explores their portability and tuning ability by training and testing them on different corpora. 2 Learning Algorithms Tested Naive-Bayes (NB). Naive Bayes is intended as a simple representative of statistical learning methods. It has been used in its most classi31 cal setting (Duda and Hart, 1973). That is, assuming the independence of featu</context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>S. Sekine. 1997. The Domain Dependence of Parsing. In Proceedings of the 5th Conference on Applied Natural Language Processing, ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Towell</author>
<author>E M Voorhees</author>
</authors>
<date>1998</date>
<booktitle>Disambiguating Highly Ambiguous Words. Computational Linguistics,</booktitle>
<pages>24--1</pages>
<contexts>
<context position="2502" citStr="Towell and Voorhees, 1998" startWordPosition="363" endWordPosition="366">artment (CICYT&apos;s project TIC98-0423— C06), by the EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Department (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for WSD, i.e., testing whether the accuracy of a system trained on a certain c</context>
</contexts>
<marker>Towell, Voorhees, 1998</marker>
<rawString>G. Towell and E. M. Voorhees. 1998. Disambiguating Highly Ambiguous Words. Computational Linguistics, 24(1):125-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
</authors>
<title>A study of polysemy judgements and inter-annotator agreement.</title>
<date>1998</date>
<booktitle>In Programme and advanced papers of the Senseval workshop,</booktitle>
<location>Herstmonceux Castle, England.</location>
<contexts>
<context position="1287" citStr="Veronis, 1998" startWordPosition="186" endWordPosition="187">s can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-theart algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross–corpus application. 1 Introduction Word Sense Disambiguation (WSD) is the problem of assigning the appropriate meaning (or sense) to a given word in a text or discourse. Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks (Ide and Veronis, 1998). Besides, WSD is one of the most important open problems in NLP. Despite the wide range of approaches investigated (Kilgarriff and Rosenzweig, 2000) and the large effort devoted to tackling this problem, to date, no large-scale broad-coverage and highly accurate WSD system has been built. One of the most successful current lines of research is the corpus-based approach, in which statistical or Machine Learning (ML) algorithms have been applied to learn statistical models or classifiers from corpora in order to per* This research has been partially funded by the Spanish Research Department (CI</context>
<context position="13039" citStr="Veronis, 1998" startWordPosition="2098" endWordPosition="2099"> hand side of symbol &amp;quot;-&amp;quot;, while the test set is on the right hand side. For instance, A-B means that the training set is corpus A and the test set is corpus B. The symbol &amp;quot;+&amp;quot; stands for set union. 4Statistical tests of significance applied: McNemar&apos;s test and 10-fold cross-validation paired Student&apos;s t-test at a confidence value of 95% (Dietterich, 1998). 5The Kappa statistic (Cohen, 1960) is a better measure of inter-annotator agreement which reduces the effect of chance agreement. It has been used for measuring inter-annotator agreement during the construction of semantic annotated corpora (Veronis, 1998; Ng et al., 1999). A Kappa value of 1 indicates perfect agreement, while 0.8 is considered as indicating good agreement. 33 Accu racy (%) A+B-A+B A+B-A A+B-B A-A B-B A-B B-A MFC 46.55+0.71 53.90+2.01 39.21+1.90 55.94±1.10 45.52+1.27 36.40 38.71 Naive Bayes 61.55+1.04 67.25+1.07 55.85+1.81 65.86±1.11 56.80+1.12 41.38 47.66 Decision Lists 61.58+0.98 67.64+0.94 55.53+1.88 67.57+1.44 56.56+1.89 43.01 48.83 SNoW 60.92+1.09 65.57+1.33 56.28±1.10 67.12+1.16 56.13+1.23 44.07 49.76 Exemplar-based 63.01+0.93 69.08+1.66 56.97+1.22 68.98+1.06 57.36+1.68 45.32 51.13 LazyBoosting 66.32±1.34 71.79±1.51 60.8</context>
<context position="14358" citStr="Veronis, 1998" startWordPosition="2307" endWordPosition="2308"> on all training-test combinations A+B-A+B DSO MFC NB EB SN DL LB DSO - 46.6 61.6 63.0 60.9 61.6 66.3 MFC -0.19 - 73.9 60.0 55.9 64.9 54.9 NB 0.24 -0.09 - 76.3 74.5 76.8 71.4 EB 0.36 -0.15 0.44 - 69.6 70.7 72.5 SN 0.36 -0.17 0.44 0.44 - 67.5 69.0 DL 0.32 -0.13 0.40 0.41 0.38 - 69.9 LB 0.44 -0.17 0.37 0.50 0.46 0.42 - Table 2: Kappa statistic (below diagonal) and % of agreement (above diagonal) between all methods in the A+B-A+B experiment that LB is the algorithm that better learns the behaviour of the DSO examples. In absolute terms, the Kappa values are very low. But, as it is suggested in (Veronis, 1998), evaluation measures should be computed relative to the agreement between the human annotators of the corpus and not to a theoretical 100%. It seems pointless to expect more agreement between the system and the reference corpus than between the annotators themselves. Contrary to the intuition that the agreement between human annotators should be very high in the WSD task, some papers report surprisingly low figures. For instance, (Ng et al., 1999) reports an accuracy rate of 56.7% and a Kappa value of 0.317 when comparing the annotation of a subset of the DSO corpus performed by two independe</context>
</contexts>
<marker>Veronis, 1998</marker>
<rawString>J. Veronis. 1998. A study of polysemy judgements and inter-annotator agreement. In Programme and advanced papers of the Senseval workshop, Herstmonceux Castle, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2429" citStr="Yarowsky, 1994" startWordPosition="355" endWordPosition="356">esearch has been partially funded by the Spanish Research Department (CICYT&apos;s project TIC98-0423— C06), by the EU Commission (NAMIC IST-1999-12392), and by the Catalan Research Department (CIRIT&apos;s consolidated research group 1999SGR-150 and CIRIT&apos;s grant 1999FI 00773). form WSD. Generally, supervised approaches (those that learn from previously semantically annotated corpora) have obtained better results than unsupervised methods on small sets of selected ambiguous words, or artificial pseudowords. Many standard ML algorithms for supervised learning have been applied, such as: Decision Lists (Yarowsky, 1994; Agirre and Martinez, 2000), Neural Networks (Towell and Voorhees, 1998), Bayesian learning (Bruce and Wiebe, 1999), Exemplar-based learning (Ng, 1997), Boosting (Escudero et al., 2000a), etc. Further, in (Mooney, 1996) some of the previous methods are compared jointly with Decision Trees and Rule Induction algorithms, on a very restricted domain. *Although some published works include the comparison between some alternative algorithms (Mooney, 1996; Ng, 1997; Escudero et al., 2000a; Escudero et al., 2000b), none of them addresses the issue of the portability of supervised ML algorithms for W</context>
<context position="6813" citStr="Yarowsky, 1994" startWordPosition="1059" endWordPosition="1060">Our implementation of SNoW for WSD is explained in (Escudero et al., 2000c). SNoW is proven to perform very well in high dimensional NLP problems, where both the training examples and the target function reside very sparsely in the feature space (Roth, 1998), e.g: context–sensitive spelling correction, POS tagging, PP–attachment disambiguation, etc. Decision Lists (DL). In this setting, a Decision List is a list of features extracted from the training examples and sorted by a log–likelihood measure. This measure estimates how strong a particular feature is as an indicator of a specific sense (Yarowsky, 1994). When testing, the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense. Thus, only the single most reliable piece of evidence is used to perform disambiguation. Regarding the details of implementation (smoothing, pruning of the decision list, etc.) we have followed (Agirre and Martinez, 2000). Decision Lists were one of the most successful systems on the 1st Senseval competition for WSD (Kilgarriff and Rosenzweig, 2000). LazyBoosting (LB). The main idea of boosting algorithms is to combine many simple</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>