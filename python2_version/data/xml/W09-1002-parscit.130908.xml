<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.146390">
<title confidence="0.971552">
On bootstrapping of linguistic features for bootstrapping grammars
</title>
<author confidence="0.99014">
Damir ´Cavar
</author>
<affiliation confidence="0.746819">
University of Zadar
Zadar, Croatia
</affiliation>
<email confidence="0.991042">
dcavar@unizd.hr
</email>
<sectionHeader confidence="0.993714" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999839733333333">
We discuss a cue-based grammar induc-
tion approach based on a parallel theory of
grammar. Our model is based on the hy-
potheses of interdependency between lin-
guistic levels (of representation) and in-
ductability of specific structural properties
at a particular level, with consequences
for the induction of structural properties at
other linguistic levels. We present the re-
sults of three different cue-learning exper-
iments and settings, covering the induc-
tion of phonological, morphological, and
syntactic properties, and discuss potential
consequences for our general grammar in-
duction model.1
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="acknowledgments">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99846425">
We assume that individual linguistic levels of nat-
ural languages differ with respect to their for-
mal complexity. In particular, the assumption is
that structural properties of linguistic levels like
phonology or morphology can be characterized
fully by Regular grammars, and if not, at least a
large subset can. Structural properties of natural
language syntax on the other hand might be char-
acterized by Mildly context-free grammars (Joshi
et al., 1991), where at least a large subset could be
characterized by Regular and Context-free gram-
mars.2
</bodyText>
<footnote confidence="0.9654332">
1This article is builds on joint work and articles with K.
Elghamri, J. Herring, T. Ikuta, P. Rodrigues, G. Schrementi
and colleagues at the Institute of Croatian Language and Lin-
guistics and the University of Zadar. The research activities
were partially funded by several grants over a couple of years,
at Indiana University and from the Croatian Ministry of Sci-
ence, Education and Sports of the Republic of Croatia.
2We are abstracting away from concrete linguistic models
and theories, and their particular complexity, as discussed e.g.
in (Ristad, 1990) or (Tesar and Smolensky, 2000).
</footnote>
<bodyText confidence="0.999905902439025">
Ignoring for the time being extra-linguistic con-
ditions and cues for linguistic properties, and in-
dependent of the complexity of specific linguis-
tic levels for particular languages, we assume
that specific properties at one particular linguistic
level correlate with properties at another level. In
natural languages certain phonological processes
might be triggered at morphological boundaries
only, e.g. (Chomsky and Halle, 1968), or prosodic
properties correlate with syntactic phrase bound-
aries and semantic properties, e.g. (Inkelas and
Zec, 1990). Similarly, lexical properties, as for
example stress patterns and morphological struc-
ture tend to be specific to certain word types (e.g.
substantives, but not function words). i.e. corre-
late with the lexical morpho-syntactic properties
used in grammars of syntax. Other more informal
correlations that are discussed in linguistics, that
rather lack a formal model or explanation, are for
example the relation between morphological rich-
ness and the freedom of word order in syntax.
Thus, it seems that specific regularities and
grammatical properties at one linguistic level
might provide cues for structural properties at an-
other level. We expect such correlations to be lan-
guage specific, given that languages qualitatively
significantly differ at least at the phonetic, phono-
logical and morphological level, and at least quan-
titatively also at the syntactic level.
Thus in our model of grammar induction, we
favor the view expressed e.g. in (Frank, 2000)
that complex grammars are bootstrapped (or grow)
from less complex grammars. On the other hand,
the intuition that structural or inherent proper-
ties at different linguistic levels correlate, i.e. they
seem to be used as cues in processing and acquisi-
tion, might require a parallel model of language
learning or grammar induction, as for example
suggested in (Jackendoff, 1996) or the Competi-
tion Model (MacWhinney and Bates, 1989).
In general, we start with the observation that
</bodyText>
<note confidence="0.670046">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 5–6,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.984614">
5
</page>
<bodyText confidence="0.999991485714286">
natural languages are learnable. In principle, the
study of how this might be modeled, and what the
minimal assumptions about the grammar proper-
ties and the induction algorithm could be, could
start top-down, by assuming maximal knowledge
of the target grammar, and subsequently eliminat-
ing elements that are obviously learnable in an un-
supervised way, or fall out as side-effects. Alter-
natively, a bottom-up approach could start with the
question about how much supervision has to be
added to an unsupervised model in order to con-
verge to a concise grammar.
Here we favor the bottom-up approach, and ask
how simple properties of grammar can be learned
in an unsupervised way, and how cues could be
identified that allow for the induction of higher
level properties of the target grammar, or other lin-
guistic levels, by for example favoring some struc-
tural hypotheses over others.
In this article we will discuss in detail sev-
eral experiments of morphological cue induction
for lexical classification (´Cavar et al., 2004a) and
(´Cavar et al., 2004b) using Vector Space Models
for category induction and subsequent rule for-
mation. Furthermore, we discuss structural cohe-
sion measured via Entropy-based statistics on the
basis of distributional properties for unsupervised
syntactic structure induction (´Cavar et al., 2004c)
from raw text, and compare the results with syn-
tactic corpora like the Penn Treebank. We ex-
pand these results with recent experiments in the
domain of unsupervised induction of phonotactic
regularities and phonological structure (´Cavar and
´Cavar, 2009), providing cues for morphological
structure induction and syntactic phrasing.
</bodyText>
<sectionHeader confidence="0.920823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962032735849057">
Damir ´Cavar and Małgorzata E. ´Cavar. 2009. On the
induction of linguistic categories and learning gram-
mars. Paper presented at the 10th Szklarska Poreba
Workshop, March.
Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul
Rodrigues, and Giancarlo Schrementi. 2004a.
Alignment based induction of morphology grammar
and its role for bootstrapping. In Gerhard J¨ager,
Paola Monachesi, Gerald Penn, and Shuly Wint-
ner, editors, Proceedings of Formal Grammar 2004,
pages 47–62, Nancy.
Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul
Rodrigues, and Giancarlo Schrementi. 2004b. On
statistical bootstrapping. In William G. Sakas, ed-
itor, Proceedings of the First Workshop on Psycho-
computational Models of Human Language Acqui-
sition, pages 9–16.
Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul
Rodrigues, and Giancarlo Schrementi. 2004c. Syn-
tactic parsing using mutual information and relative
entropy. Midwest Computational Linguistics Collo-
quium (MCLC), June.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper &amp; Row, New York.
Robert Frank. 2000. From regular to context free to
mildly context sensitive tree rewriting systems: The
path of child language acquisition. In A. Abeill´e
and O. Rambow, editors, Tree Adjoining Gram-
mars: Formalisms, Linguistic Analysis and Process-
ing, pages 101–120. CSLI Publications.
Sharon Inkelas and Draga Zec. 1990. The Phonology-
Syntax Connection. University Of Chicago Press,
Chicago.
Ray Jackendoff. 1996. The Architecture of the Lan-
guage Faculty. Number 28 in Linguistic Inquiry
Monographs. MIT Press, Cambridge, MA.
Aravind Joshi, K. Vijay-Shanker, and David Weird.
1991. The convergence of mildly context-sensitive
grammar formalisms. In Peter Sells, Stuart Shieber,
and Thomas Wasow, editors, Foundational Issues in
Natural Language Processing, pages 31–81. MIT
Press, Cambridge, MA.
Brian MacWhinney and Elizabeth Bates. 1989. The
Crosslinguistic Study of Sentence Processing. Cam-
bridge University Press, New York.
Eric S. Ristad. 1990. Computational structure of gen-
erative phonology and its relation to language com-
prehension. In Proceedings of the 28th annual meet-
ing on Association for Computational Linguistics,
pages 235–242. Association for Computational Lin-
guistics.
Bruce Tesar and Paul Smolensky. 2000. Learnability
in Optimality Theory. MIT Press, Cambridge, MA.
</reference>
<page confidence="0.998781">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.692899">
<title confidence="0.999873">On bootstrapping of linguistic features for bootstrapping grammars</title>
<author confidence="0.967471">Damir</author>
<affiliation confidence="0.999715">University of</affiliation>
<address confidence="0.761927">Zadar,</address>
<email confidence="0.997007">dcavar@unizd.hr</email>
<abstract confidence="0.9960486">We discuss a cue-based grammar induction approach based on a parallel theory of grammar. Our model is based on the hypotheses of interdependency between linguistic levels (of representation) and inductability of specific structural properties at a particular level, with consequences for the induction of structural properties at other linguistic levels. We present the results of three different cue-learning experiments and settings, covering the induction of phonological, morphological, and syntactic properties, and discuss potential consequences for our general grammar in-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Damir ´Cavar</author>
<author>Małgorzata E ´Cavar</author>
</authors>
<title>On the induction of linguistic categories and learning grammars. Paper presented at the 10th Szklarska Poreba Workshop,</title>
<date>2009</date>
<marker>´Cavar, ´Cavar, 2009</marker>
<rawString>Damir ´Cavar and Małgorzata E. ´Cavar. 2009. On the induction of linguistic categories and learning grammars. Paper presented at the 10th Szklarska Poreba Workshop, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damir ´Cavar</author>
<author>Joshua Herring</author>
<author>Toshikazu Ikuta</author>
<author>Paul Rodrigues</author>
<author>Giancarlo Schrementi</author>
</authors>
<title>Alignment based induction of morphology grammar and its role for bootstrapping.</title>
<date>2004</date>
<booktitle>Proceedings of Formal Grammar 2004,</booktitle>
<pages>47--62</pages>
<editor>In Gerhard J¨ager, Paola Monachesi, Gerald Penn, and Shuly Wintner, editors,</editor>
<location>Nancy.</location>
<marker>´Cavar, Herring, Ikuta, Rodrigues, Schrementi, 2004</marker>
<rawString>Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul Rodrigues, and Giancarlo Schrementi. 2004a. Alignment based induction of morphology grammar and its role for bootstrapping. In Gerhard J¨ager, Paola Monachesi, Gerald Penn, and Shuly Wintner, editors, Proceedings of Formal Grammar 2004, pages 47–62, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damir ´Cavar</author>
<author>Joshua Herring</author>
<author>Toshikazu Ikuta</author>
<author>Paul Rodrigues</author>
<author>Giancarlo Schrementi</author>
</authors>
<title>On statistical bootstrapping. In</title>
<date>2004</date>
<booktitle>Proceedings of the First Workshop on Psychocomputational Models of Human Language Acquisition,</booktitle>
<pages>9--16</pages>
<editor>William G. Sakas, editor,</editor>
<marker>´Cavar, Herring, Ikuta, Rodrigues, Schrementi, 2004</marker>
<rawString>Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul Rodrigues, and Giancarlo Schrementi. 2004b. On statistical bootstrapping. In William G. Sakas, editor, Proceedings of the First Workshop on Psychocomputational Models of Human Language Acquisition, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damir ´Cavar</author>
<author>Joshua Herring</author>
<author>Toshikazu Ikuta</author>
<author>Paul Rodrigues</author>
<author>Giancarlo Schrementi</author>
</authors>
<title>Syntactic parsing using mutual information and relative entropy.</title>
<date>2004</date>
<booktitle>Midwest Computational Linguistics Colloquium (MCLC),</booktitle>
<marker>´Cavar, Herring, Ikuta, Rodrigues, Schrementi, 2004</marker>
<rawString>Damir ´Cavar, Joshua Herring, Toshikazu Ikuta, Paul Rodrigues, and Giancarlo Schrementi. 2004c. Syntactic parsing using mutual information and relative entropy. Midwest Computational Linguistics Colloquium (MCLC), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English.</title>
<date>1968</date>
<publisher>Harper &amp; Row,</publisher>
<location>New York.</location>
<contexts>
<context position="2319" citStr="Chomsky and Halle, 1968" startWordPosition="344" endWordPosition="347">Republic of Croatia. 2We are abstracting away from concrete linguistic models and theories, and their particular complexity, as discussed e.g. in (Ristad, 1990) or (Tesar and Smolensky, 2000). Ignoring for the time being extra-linguistic conditions and cues for linguistic properties, and independent of the complexity of specific linguistic levels for particular languages, we assume that specific properties at one particular linguistic level correlate with properties at another level. In natural languages certain phonological processes might be triggered at morphological boundaries only, e.g. (Chomsky and Halle, 1968), or prosodic properties correlate with syntactic phrase boundaries and semantic properties, e.g. (Inkelas and Zec, 1990). Similarly, lexical properties, as for example stress patterns and morphological structure tend to be specific to certain word types (e.g. substantives, but not function words). i.e. correlate with the lexical morpho-syntactic properties used in grammars of syntax. Other more informal correlations that are discussed in linguistics, that rather lack a formal model or explanation, are for example the relation between morphological richness and the freedom of word order in syn</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. Harper &amp; Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frank</author>
</authors>
<title>From regular to context free to mildly context sensitive tree rewriting systems: The path of child language acquisition.</title>
<date>2000</date>
<booktitle>Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing,</booktitle>
<pages>101--120</pages>
<editor>In A. Abeill´e and O. Rambow, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="3399" citStr="Frank, 2000" startWordPosition="510" endWordPosition="511">lack a formal model or explanation, are for example the relation between morphological richness and the freedom of word order in syntax. Thus, it seems that specific regularities and grammatical properties at one linguistic level might provide cues for structural properties at another level. We expect such correlations to be language specific, given that languages qualitatively significantly differ at least at the phonetic, phonological and morphological level, and at least quantitatively also at the syntactic level. Thus in our model of grammar induction, we favor the view expressed e.g. in (Frank, 2000) that complex grammars are bootstrapped (or grow) from less complex grammars. On the other hand, the intuition that structural or inherent properties at different linguistic levels correlate, i.e. they seem to be used as cues in processing and acquisition, might require a parallel model of language learning or grammar induction, as for example suggested in (Jackendoff, 1996) or the Competition Model (MacWhinney and Bates, 1989). In general, we start with the observation that Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 5–6, Athens, G</context>
</contexts>
<marker>Frank, 2000</marker>
<rawString>Robert Frank. 2000. From regular to context free to mildly context sensitive tree rewriting systems: The path of child language acquisition. In A. Abeill´e and O. Rambow, editors, Tree Adjoining Grammars: Formalisms, Linguistic Analysis and Processing, pages 101–120. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Inkelas</author>
<author>Draga Zec</author>
</authors>
<title>The PhonologySyntax Connection.</title>
<date>1990</date>
<publisher>University Of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="2440" citStr="Inkelas and Zec, 1990" startWordPosition="361" endWordPosition="364">, as discussed e.g. in (Ristad, 1990) or (Tesar and Smolensky, 2000). Ignoring for the time being extra-linguistic conditions and cues for linguistic properties, and independent of the complexity of specific linguistic levels for particular languages, we assume that specific properties at one particular linguistic level correlate with properties at another level. In natural languages certain phonological processes might be triggered at morphological boundaries only, e.g. (Chomsky and Halle, 1968), or prosodic properties correlate with syntactic phrase boundaries and semantic properties, e.g. (Inkelas and Zec, 1990). Similarly, lexical properties, as for example stress patterns and morphological structure tend to be specific to certain word types (e.g. substantives, but not function words). i.e. correlate with the lexical morpho-syntactic properties used in grammars of syntax. Other more informal correlations that are discussed in linguistics, that rather lack a formal model or explanation, are for example the relation between morphological richness and the freedom of word order in syntax. Thus, it seems that specific regularities and grammatical properties at one linguistic level might provide cues for </context>
</contexts>
<marker>Inkelas, Zec, 1990</marker>
<rawString>Sharon Inkelas and Draga Zec. 1990. The PhonologySyntax Connection. University Of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>The Architecture of the Language Faculty. Number 28 in Linguistic Inquiry Monographs.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3776" citStr="Jackendoff, 1996" startWordPosition="569" endWordPosition="570">tatively significantly differ at least at the phonetic, phonological and morphological level, and at least quantitatively also at the syntactic level. Thus in our model of grammar induction, we favor the view expressed e.g. in (Frank, 2000) that complex grammars are bootstrapped (or grow) from less complex grammars. On the other hand, the intuition that structural or inherent properties at different linguistic levels correlate, i.e. they seem to be used as cues in processing and acquisition, might require a parallel model of language learning or grammar induction, as for example suggested in (Jackendoff, 1996) or the Competition Model (MacWhinney and Bates, 1989). In general, we start with the observation that Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 5–6, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 5 natural languages are learnable. In principle, the study of how this might be modeled, and what the minimal assumptions about the grammar properties and the induction algorithm could be, could start top-down, by assuming maximal knowledge of the target grammar, and subsequently eliminating elements that</context>
</contexts>
<marker>Jackendoff, 1996</marker>
<rawString>Ray Jackendoff. 1996. The Architecture of the Language Faculty. Number 28 in Linguistic Inquiry Monographs. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>K Vijay-Shanker</author>
<author>David Weird</author>
</authors>
<title>The convergence of mildly context-sensitive grammar formalisms.</title>
<date>1991</date>
<booktitle>Foundational Issues in Natural Language Processing,</booktitle>
<pages>31--81</pages>
<editor>In Peter Sells, Stuart Shieber, and Thomas Wasow, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1203" citStr="Joshi et al., 1991" startWordPosition="174" endWordPosition="177"> the induction of phonological, morphological, and syntactic properties, and discuss potential consequences for our general grammar induction model.1 1 Introduction We assume that individual linguistic levels of natural languages differ with respect to their formal complexity. In particular, the assumption is that structural properties of linguistic levels like phonology or morphology can be characterized fully by Regular grammars, and if not, at least a large subset can. Structural properties of natural language syntax on the other hand might be characterized by Mildly context-free grammars (Joshi et al., 1991), where at least a large subset could be characterized by Regular and Context-free grammars.2 1This article is builds on joint work and articles with K. Elghamri, J. Herring, T. Ikuta, P. Rodrigues, G. Schrementi and colleagues at the Institute of Croatian Language and Linguistics and the University of Zadar. The research activities were partially funded by several grants over a couple of years, at Indiana University and from the Croatian Ministry of Science, Education and Sports of the Republic of Croatia. 2We are abstracting away from concrete linguistic models and theories, and their partic</context>
</contexts>
<marker>Joshi, Vijay-Shanker, Weird, 1991</marker>
<rawString>Aravind Joshi, K. Vijay-Shanker, and David Weird. 1991. The convergence of mildly context-sensitive grammar formalisms. In Peter Sells, Stuart Shieber, and Thomas Wasow, editors, Foundational Issues in Natural Language Processing, pages 31–81. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Elizabeth Bates</author>
</authors>
<title>The Crosslinguistic Study of Sentence Processing.</title>
<date>1989</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="3830" citStr="MacWhinney and Bates, 1989" startWordPosition="576" endWordPosition="579">e phonetic, phonological and morphological level, and at least quantitatively also at the syntactic level. Thus in our model of grammar induction, we favor the view expressed e.g. in (Frank, 2000) that complex grammars are bootstrapped (or grow) from less complex grammars. On the other hand, the intuition that structural or inherent properties at different linguistic levels correlate, i.e. they seem to be used as cues in processing and acquisition, might require a parallel model of language learning or grammar induction, as for example suggested in (Jackendoff, 1996) or the Competition Model (MacWhinney and Bates, 1989). In general, we start with the observation that Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 5–6, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 5 natural languages are learnable. In principle, the study of how this might be modeled, and what the minimal assumptions about the grammar properties and the induction algorithm could be, could start top-down, by assuming maximal knowledge of the target grammar, and subsequently eliminating elements that are obviously learnable in an unsupervised way, or fa</context>
</contexts>
<marker>MacWhinney, Bates, 1989</marker>
<rawString>Brian MacWhinney and Elizabeth Bates. 1989. The Crosslinguistic Study of Sentence Processing. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>Computational structure of generative phonology and its relation to language comprehension.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1855" citStr="Ristad, 1990" startWordPosition="280" endWordPosition="281">e characterized by Regular and Context-free grammars.2 1This article is builds on joint work and articles with K. Elghamri, J. Herring, T. Ikuta, P. Rodrigues, G. Schrementi and colleagues at the Institute of Croatian Language and Linguistics and the University of Zadar. The research activities were partially funded by several grants over a couple of years, at Indiana University and from the Croatian Ministry of Science, Education and Sports of the Republic of Croatia. 2We are abstracting away from concrete linguistic models and theories, and their particular complexity, as discussed e.g. in (Ristad, 1990) or (Tesar and Smolensky, 2000). Ignoring for the time being extra-linguistic conditions and cues for linguistic properties, and independent of the complexity of specific linguistic levels for particular languages, we assume that specific properties at one particular linguistic level correlate with properties at another level. In natural languages certain phonological processes might be triggered at morphological boundaries only, e.g. (Chomsky and Halle, 1968), or prosodic properties correlate with syntactic phrase boundaries and semantic properties, e.g. (Inkelas and Zec, 1990). Similarly, le</context>
</contexts>
<marker>Ristad, 1990</marker>
<rawString>Eric S. Ristad. 1990. Computational structure of generative phonology and its relation to language comprehension. In Proceedings of the 28th annual meeting on Association for Computational Linguistics, pages 235–242. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Tesar</author>
<author>Paul Smolensky</author>
</authors>
<title>Learnability in Optimality Theory.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1886" citStr="Tesar and Smolensky, 2000" startWordPosition="283" endWordPosition="286"> Regular and Context-free grammars.2 1This article is builds on joint work and articles with K. Elghamri, J. Herring, T. Ikuta, P. Rodrigues, G. Schrementi and colleagues at the Institute of Croatian Language and Linguistics and the University of Zadar. The research activities were partially funded by several grants over a couple of years, at Indiana University and from the Croatian Ministry of Science, Education and Sports of the Republic of Croatia. 2We are abstracting away from concrete linguistic models and theories, and their particular complexity, as discussed e.g. in (Ristad, 1990) or (Tesar and Smolensky, 2000). Ignoring for the time being extra-linguistic conditions and cues for linguistic properties, and independent of the complexity of specific linguistic levels for particular languages, we assume that specific properties at one particular linguistic level correlate with properties at another level. In natural languages certain phonological processes might be triggered at morphological boundaries only, e.g. (Chomsky and Halle, 1968), or prosodic properties correlate with syntactic phrase boundaries and semantic properties, e.g. (Inkelas and Zec, 1990). Similarly, lexical properties, as for exampl</context>
</contexts>
<marker>Tesar, Smolensky, 2000</marker>
<rawString>Bruce Tesar and Paul Smolensky. 2000. Learnability in Optimality Theory. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>