<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062930">
<title confidence="0.992183">
Robust Logistic Regression using Shift Parameters
</title>
<author confidence="0.994748">
Julie Tibshirani and Christopher D. Manning
</author>
<affiliation confidence="0.993209">
Stanford University
</affiliation>
<address confidence="0.682024">
Stanford, CA 94305, USA
</address>
<email confidence="0.996384">
{jtibs, manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.997357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922">
Annotation errors can significantly hurt
classifier performance, yet datasets are
only growing noisier with the increased
use of Amazon Mechanical Turk and tech-
niques like distant supervision that auto-
matically generate labels. In this paper,
we present a robust extension of logistic
regression that incorporates the possibil-
ity of mislabelling directly into the objec-
tive. This model can be trained through
nearly the same means as logistic regres-
sion, and retains its efficiency on high-
dimensional datasets. We conduct exper-
iments on named entity recognition data
and find that our approach can provide a
significant improvement over the standard
model when annotation errors are present.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987712244898">
Almost any large dataset has annotation errors,
especially those complex, nuanced datasets com-
monly used in natural language processing. Low-
quality annotations have become even more com-
mon in recent years with the rise of Amazon Me-
chanical Turk, as well as methods like distant su-
pervision and co-training that involve automati-
cally generating training data.
Although small amounts of noise may not be
detrimental, in some applications the level can
be high: upon manually inspecting a relation ex-
traction corpus commonly used in distant super-
vision, Riedel et al. (2010) report a 31% false
positive rate. In cases like these, annotation er-
rors have frequently been observed to hurt perfor-
mance. Dingare et al. (2005), for example, con-
duct error analysis on a system to extract relations
from biomedical text, and observe that over half
of the system’s errors could be attributed to incon-
sistencies in how the data was annotated. Simi-
larly, in a case study on co-training for natural lan-
guage tasks, Pierce and Cardie (2001) find that
the degradation in data quality from automatic la-
belling prevents these systems from performing
comparably to their fully-supervised counterparts.
In this work we argue that incorrect exam-
ples should be explicitly modelled during train-
ing, and present a simple extension of logistic re-
gression that incorporates the possibility of mis-
labelling directly into the objective. Following a
technique from robust statistics, our model intro-
duces sparse ‘shift parameters’ to allow datapoints
to slide along the sigmoid, changing class if ap-
propriate. It has a convex objective, is well-suited
to high-dimensional data, and can be efficiently
trained with minimal changes to the logistic re-
gression pipeline.
In experiments on a large, noisy NER dataset,
we find that this method can provide an improve-
ment over standard logistic regression when anno-
tation errors are present. The model also provides
a means to identify which examples were misla-
belled: through experiments on biological data,
we demonstrate how our method can be used to
accurately identify annotation errors. This robust
extension of logistic regression shows particular
promise for NLP applications: it helps account
for incorrect labels, while remaining efficient on
large, high-dimensional datasets.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999825363636364">
Much of the previous work on dealing with anno-
tation errors centers around filtering the data be-
fore training. Brodley and Friedl (1999) introduce
what is perhaps the simplest form of supervised
filtering: they train various classifiers, then record
their predictions on a different part of the train set
and eliminate contentious examples. Sculley and
Cormack (2008) apply this approach to spam fil-
tering with noisy user feedback.
One obvious issue with these methods is that the
noise-detecting classifiers are themselves trained
</bodyText>
<page confidence="0.977911">
124
</page>
<bodyText confidence="0.992336415584416">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 124–129,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
on noisy labels. Unsupervised filtering tries to
avoid this problem by clustering training instances
based solely on their features, then using the clus-
ters to detect labelling anomalies (Rebbapragada
et al., 2009). Recently, Intxaurrondo et al. (2013)
applied this approach to distantly-supervised rela-
tion extraction, using heuristics such as the num-
ber of mentions per tuple to eliminate suspicious
examples.
Unsupervised filtering, however, relies on the
perhaps unwarranted assumption that examples
with the same label lie close together in feature
space. Moreover filtering techniques in general
may not be well-justified: if a training example
does not fit closely with the current model, it is
not necessarily mislabelled. It may represent an
important exception that would improve the over-
all fit, or appear unusual simply because we have
made poor modelling assumptions.
Perhaps the most promising approaches are
those that directly model annotation errors, han-
dling mislabelled examples as they train. This
way, there is an active trade-off between fitting the
model and identifying suspected errors. Bootkra-
jang and Kaban (2012) present an extension of
logistic regression that models annotation errors
through flipping probabilities. While intuitive, this
approach has shortcomings of its own: the objec-
tive function is nonconvex and the authors note
that local optima are an issue, and the model can
be difficult to fit when there are many more fea-
tures than training examples.
There is a growing body of literature on learn-
ing from several annotators, each of whom may be
inaccurate (Bachrach et al., 2012; Raykar et al.,
2009). It is important to note that we are consid-
ering a separate, and perhaps more general, prob-
lem: we have only one source of noisy labels, and
the errors need not come from the human annota-
tors, but could be introduced through contamina-
tion or automatic labelling.
The field of ‘robust statistics’ seeks to develop
estimators that are not unduly affected by devi-
ations from the model assumptions (Huber and
Ronchetti, 2009). Since mislabelled points are
one type of outlier, this goal is naturally related
to our interest in dealing with noisy data, and it
seems many of the existing techniques would be
relevant. A common strategy is to use a modi-
fied loss function that gives less influence to points
far from the boundary, and several models along
Figure 1: Fit resulting from a standard vs. robust
model, where data is generated from the dashed
sigmoid and negative labels flipped with probabil-
ity 0.2.
these lines have been proposed (Ding and Vish-
wanathan., 2010; Masnadi-Shirazi et al., 2010).
Unfortunately these approaches require optimiz-
ing nonstandard, often nonconvex objectives, and
fail to give insight into which datapoints are mis-
labelled.
In a recent advance, She and Owen (2011)
demonstrate that introducing a regularized ‘shift
parameter’ per datapoint can help increase the ro-
bustness of linear regression. Candes et al. (2009)
propose a similar approach for principal compo-
nent analysis, while Wright and Ma (2009) ex-
plore its effectiveness in sparse signal recovery. In
this work we adapt the technique to logistic re-
gression. To the best of our knowledge, we are
the first to experiment with adding ‘shift param-
eters’ to logistic regression and demonstrate that
the model is especially well-suited to the type of
high-dimensional, noisy datasets commonly used
in NLP.
</bodyText>
<sectionHeader confidence="0.992555" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.994605666666667">
Recall that in binary logistic regression, the prob-
ability of an example xi being positive is modeled
as
</bodyText>
<equation confidence="0.9920585">
1
g(0T xi) =
</equation>
<bodyText confidence="0.998506833333333">
For simplicity, we assume the intercept term has
been folded into the weight vector 0, so 0 ∈ Rm+1
where m is the number of features.
Following She and Owen (2011), we propose
the following robust extension: for each datapoint
i = 1, ... , n, we introduce a real-valued shift pa-
</bodyText>
<figure confidence="0.994576">
4 -2 0 2 4
0.0 0.2 0.4 0.6 0.8 1.0
original sigmoid
standard LR
robust LR
1 + e−BT xi .
</figure>
<page confidence="0.983775">
125
</page>
<bodyText confidence="0.986149166666667">
rameter γi so that the sigmoid becomes
Since we believe that most examples are correctly
labelled, we L1-regularize the shift parameters to
encourage sparsity. Letting yi E {0, 11 be the la-
bel for datapoint i and fixing λ &gt; 0, our objective
is now given by
</bodyText>
<equation confidence="0.991472">
n
l(θ, γ) _ lyi log g(θTxi + γi) (1)
i=1
n
+ (1 − yi) log (1 − g(θTxi + γi))i − λ |γi|.
i=1
</equation>
<bodyText confidence="0.999938588235294">
These parameters γi let certain datapoints shift
along the sigmoid, perhaps switching from one
class to the other. If a datapoint i is correctly an-
notated, then we would expect its corresponding
γi to be zero. If it actually belongs to the posi-
tive class but is labelled negative, then γi might be
positive, and analogously for the other direction.
One way to interpret the model is that it al-
lows the log-odds of select datapoints to be
shifted. Compared to models based on label-
flipping, where there is a global set of flipping
probabilities, our method has the advantage of tar-
geting each example individually.
It is worth noting that there is no difficulty in
regularizing the θ parameters as well. For exam-
ple, if we choose to use an L1 penalty then our
objective becomes
</bodyText>
<equation confidence="0.975842333333333">
n
l(θ, γ) _ lyi log g(θTxi + γi) (2)
i=1
</equation>
<bodyText confidence="0.9996598">
Finally, it may seem concerning that we have
introduced a new parameter for each datapoint.
But in many applications the number of features
already exceeds n, so with proper regularization,
this increase is actually quite reasonable.
</bodyText>
<subsectionHeader confidence="0.991125">
3.1 Training
</subsectionHeader>
<bodyText confidence="0.999829875">
Notice that adding these shift parameters is equiv-
alent to introducing n features, where the ith new
feature is 1 for datapoint i and 0 otherwise. With
this observation, we can simply modify the fea-
ture matrix and parameter vector and train the lo-
gistic model as usual. Specifically, we let θ0 _
(θ0, . . . , θm,γ1, . . . ,γn) and X0 _ [X|In] so that
the objective (1) simplifies to
</bodyText>
<equation confidence="0.990788666666667">
n
l(θ0) _
i=1 lyi log g(θ0Tx0i)
m+n
+ (1 − yi) log (1 − g(θ0T x0i))i − λ � |θ0(j)|.
j=m+1
</equation>
<bodyText confidence="0.999741333333333">
Upon writing the objective in this way, we imme-
diately see that it is convex, just as standard L1-
penalized logistic regression is convex.
</bodyText>
<subsectionHeader confidence="0.997814">
3.2 Testing
</subsectionHeader>
<bodyText confidence="0.93435925">
To obtain our final logistic model, we keep only
the θ parameters. Predictions are then made as
usual:
I{g(ˆθTx) &gt; 0.51.
</bodyText>
<subsectionHeader confidence="0.999618">
3.3 Selecting Regularization Parameters
</subsectionHeader>
<bodyText confidence="0.990966791666667">
The parameter λ from equation (1) would nor-
mally be chosen through cross-validation, but our
set-up is unusual in that the training set may con-
tain errors, and even if we have a designated devel-
opment set it is unlikely to be error-free. We found
in simulations that the errors largely do not inter-
fere in selecting λ, so in the experiments below we
cross-validate as normal.
Notice that λ has a direct effect on the number
of nonzero shifts γ and hence the suspected num-
ber of errors in the training set. So if we have in-
formation about the noise level, we can directly
incorporate it into the selection procedure. For ex-
ample, we may believe the training set has no more
than 15% noise, and so would restrict the choice
of λ during cross-validation to only those values
where 15% or fewer of the estimated shift param-
eters are nonzero.
We now consider situations in which the θ pa-
rameters are regularized as well. Assume, for ex-
ample, that we use L1-regularization as in equa-
tion (2), so that we now need to optimize over both
κ and λ. We perform the following simple proce-
dure:
</bodyText>
<listItem confidence="0.99378775">
1. Cross-validate using standard logistic regres-
sion to select κ.
2. Fix this value for κ, and cross-validate using
the robust model to find the best choice of λ.
</listItem>
<equation confidence="0.996683833333333">
1
g(θT xi + γi) _
1 + e−θT xi−γi .
+ (1 − yi) log (1 − g(θTxi + γi))i
− κ �m |θj |− λ n |γi|.
j=1 i=1
</equation>
<page confidence="0.986957">
126
</page>
<table confidence="0.983486571428571">
method suspects identified false positives
Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36
Furey et al. (2000) • • • • • •
Kadota et al. (2003) • • • • • T6, N2
Malossini et al. (2006) • • • • • • • T8, N2, N28, N29
Bootkrajang et al. (2012) • • • • • • •
Robust LR • • • • • • •
</table>
<tableCaption confidence="0.9079425">
Table 1: Results of various error-identification methods on the colon cancer dataset. The first row lists
the samples that are biologically confirmed to be suspicious, and each other row gives the output from
an automatic detection method. Bootkrajang et al. report confidences, so we threshold at 0.5 to obtain
these results.
</tableCaption>
<sectionHeader confidence="0.99901" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999685">
We conduct two sets of experiments to assess the
effectiveness of the approach, in terms of both
identifying mislabelled examples and producing
accurate predictions.
</bodyText>
<subsectionHeader confidence="0.991561">
4.1 Contaminated Data
</subsectionHeader>
<bodyText confidence="0.999955966666667">
Our first experiment is centered around a biologi-
cal dataset with suspected labelling errors. Called
the colon cancer dataset, it contains the expres-
sion levels of 2000 genes from 40 tumor and 22
normal tissues (Alon et al., 1999). There is evi-
dence in the literature that certain tissue samples
may have been cross-contaminated. In particular,
5 tumor and 4 normal samples should have their
labels flipped.
In this experiment, we examine the model’s
ability to identify mislabelled training examples.
Because there are many more features than data-
points and it is likely that not all genes are relevant,
we choose to place an Li penalty on θ.
Using glmnet, an R package for training reg-
ularized models (Friedman et al., 2009), we se-
lect n and A using cross-validation. Looking at
the resulting values for γ, we find that only 7 of
the shift parameters are nonzero and that each one
corresponds to a suspicious datapoint. As further
confirmation, the signs of the gammas correctly
match the direction of the mislabelling. Compared
to previous attempts to automatically detect errors
in this dataset, our approach identifies at least as
many suspicious examples but with no false posi-
tives. A detailed comparison is given in Table 1.
Although Bootkrajang and Kaban (2012) are quite
accurate, it is worth noting that due to its noncon-
vexity, their model needed to be trained 20 times
to achieve these results.
</bodyText>
<subsectionHeader confidence="0.998585">
4.2 Manually Annotated Data
</subsectionHeader>
<bodyText confidence="0.999775351351352">
We now consider the problem of named entity
recognition (NER) to evaluate how our model per-
forms in a large-scale prediction task. In tradi-
tional NER, the goal is to determine whether each
word is a person, organization, location, or not a
named entity (‘other’). Since our model is binary,
we concentrate on the task of deciding whether a
word is a person or not. (This task does not triv-
ially reduce to finding the capitalized words, as the
model must distinguish between people and other
named entities like organizations).
For training, we use a large, noisy NER dataset
collected by Jenny Finkel. The data was created
by taking various Wikipedia articles and giving
them to five Amazon Mechanical Turkers to anno-
tate. Few to no quality controls were put in place,
so that certain annotators produced very noisy la-
bels. To construct the train set we chose a Turker
who was about average in how much he disagreed
with the majority vote, and used only his annota-
tions. Negative examples are subsampled to bring
the class ratio to a reasonable level, for a total of
200,000 negative and 24,002 positive examples.
We find that in 0.4% of examples, the majority
agreed they were negative but the chosen annota-
tor marked them positive, and 7.5% were labelled
positive by the majority but negative by the an-
notator. Note that we still include examples for
which there was no majority consensus, so these
noise estimates are quite conservative.
We evaluate on the English development test set
from the CoNLL shared task (Tjong Kim Sang and
Meulder, 2003). This data consists of news arti-
cles from the Reuters corpus, hand-annotated by
researchers at the University of Antwerp.
We extract a set of features using Stanford’s
NER pipeline (Finkel et al., 2005). This set was
</bodyText>
<page confidence="0.993677">
127
</page>
<table confidence="0.9952574">
Precision
model precision recall F1
standard 76.99 85.87 81.19
flipping 76.62 86.28 81.17
robust 77.04 90.47 83.22
</table>
<tableCaption confidence="0.986889">
Table 2: Performance of standard vs. robust logis-
</tableCaption>
<bodyText confidence="0.999334925">
tic regression in the Wikipedia NER experiment.
The flipping model refers to the approach from
Bootkrajang and Kaban (2012).
chosen for simplicity and is not highly engineered
– it largely consists of lexical features such as the
current word, the previous and next words in the
sentence, as well as character n-grams and vari-
ous word shape features. With a total of 393,633
features in the train set, we choose to use L2-
regularization, so that our penalty now becomes
This choice is natural as L2 is the most common
form of regularization in NLP, and we wish to ver-
ify that our approach works for penalties besides
L1.
The robust model is fit using Orthant-Wise
Limited-Memory Quasi Newton (OWL-QN), a
technique for optimizing an L1-penalized objec-
tive (Andrew and Gao, 2007). We tune both
models through 5-fold cross-validation to obtain
σ2 = 1.0 and A = 0.1. Note that from the way
we cross-validate (first tuning σ using standard lo-
gistic regression, fixing this choice, then tuning A)
our procedure may give an unfair advantage to the
baseline.
We also compare against the algorithm pro-
posed in Bootkrajang and Kaban (2012), an exten-
sion of logistic regression mentioned in the section
on prior work. This approach assumes that each
example’s true label is flipped with a certain prob-
ability before being observed, and fits the resulting
latent-variable model using EM.
The results of these experiments are shown in
Table 2 as well as Figure 2. Robust logistic re-
gression offers a noticeable improvement over the
baseline, and this improvement holds at essentially
all levels of precision and recall. Interestingly, be-
cause of the large dimension, the flipping model
consistently learns that no labels have been flipped
and thus does not show a substantial difference
with standard logistic regression.
</bodyText>
<figure confidence="0.523568">
Recall
</figure>
<figureCaption confidence="0.940122">
Figure 2: Precision-recall curve obtained from
training on noisy Wikipedia data and testing on
CoNLL. The flipping model refers to the approach
from Bootkrajang and Kaban (2012).
</figureCaption>
<sectionHeader confidence="0.998668" genericHeader="discussions">
5 Future Work
</sectionHeader>
<bodyText confidence="0.999981357142857">
A natural direction for future work is to extend the
model to a multi-class setting. One option is to
introduce a y for every class except the negative
one, so that there are n(c − 1) shift parameters in
all. We could then apply a group lasso, with each
group consisting of they for a particular datapoint
(Meier et al., 2008). This way all of a datapoint’s
shift parameters drop out together, which corre-
sponds to the example being correctly labelled.
CRFs and other sequence models could also
benefit from the addition of shift parameters.
Since the extra variables can be neatly folded into
the linear term, convexity is preserved and the
model could essentially be trained as usual.
</bodyText>
<sectionHeader confidence="0.99814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999744416666667">
Stanford University gratefully acknowledges the
support of the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under Air
Force Research Laboratory (AFRL) contract no.
FA8750-13-2-0040. Any opinions, findings, and
conclusion or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the view of the DARPA, AFRL, or
the US government. We are especially grateful to
Rob Tibshirani and Stefan Wager for their invalu-
able advice and encouragement.
</bodyText>
<figure confidence="0.997646076923077">
0.5 0.6 0.7 0.8 0.9 1.0
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
normal LR
flipping model
robust LR
1
|yz|.
n
z=1
|θj|2 + A
2σ2
�m
j=0
</figure>
<page confidence="0.980969">
128
</page>
<sectionHeader confidence="0.995164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999557051546392">
U. Alon, N. Barkai, D. A. Notterman, K. Gish,
S. Ybarra, D. Mack, A. J. Levine. 1999. Broad
patterns of gene expression revealed by clustering
analysis of tumor and normal colon tissues probed
by oligonucleotide arrays. National Academy of Sci-
ences of the USA.
Galen Andrew and Jianfeng Gao. 2007. Scal-
able Training of Ll-Regularized Log-Linear Mod-
els. ICML.
Yoram Bachrach, Thore Graepel, Tom Minka, and
John Guiver. 2012. How To Grade a Test With-
out Knowing the Answers: A Bayesian Graphical
Model for Adaptive Crowdsourcing and Aptitude
Testing. arXiv preprint arXiv:1206.6386 (2012).
Jakramate Bootkrajang and Ata Kaban. 2012. Label-
noise Robust Logistic Regression and Its Applica-
tions. ECML PKDD.
Carla E. Brodley and Mark A. Friedl. 1999. Identify-
ing mislabeled Training Data. JAIR, 11, 131-167.
Emmanuel J. Candes, Xiaodong Li, Yi Ma, John
Wright. 2009. Robust Principal Component Analy-
sis? arXiv preprint arXiv:0912.3599, 2009.
Nan Ding and S. V. N. Vishwanathan. 2010. t-Logistic
regression. NIPS.
Shipra Dingare, Malvina Nissim, Jenny Finkel,
Christopher Manning, and Claire Grover. 2005. A
system for identifying named entities in biomedical
text: How results from two evaluations reflect on
both the system and the evaluations. Comparative
and Functional Genomics. 6(1–2), 77-85.
Jenny Rose Finkel, Trond Grenager, Christopher Man-
ning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. ACL.
Jerome Friedman, Trevor Hastie, Rob Tibshirani 2009.
Regularization Paths for Generalized Linear Models
via Coordinate Descent. Journal of statistical soft-
ware, 33(1), 1.
Terrence S. Furey, Nello Cristianini, Nigel Duffy,
David W. Bednarski, Michel Schummer, David
Haussler. 2000. Support vector machine classifi-
cation and validation of cancer tissue samples using
microarray expression data. Bioinformatics, 16(10),
906-914.
Peter J. Huber and Elvezio M. Ronchetti. 2000. Robust
Statistics. John Wiley &amp; Sons, Inc., Hoboken, NJ.
Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de
Lacalle, and Eneko Agirre. 2013. Removing
Noisy Mentions for Distant Supervision. Congreso
de la Sociedad Espaola para el Procesamiento del
Lenguaje Natural.
Koji Kadota, Daisuke Tominaga, Yutaka Akiyama,
Katsutoshi Takahashi. 2003. Detecting outlying
samples in microarray data: A critical assessment
of the effect of outliers on sample. ChemBio Infor-
matics Journal, 3(1), 30-45.
Andrea Malossini, Enrico Blanzieri, Raymond T. Ng.
2006. Detecting potential labeling errors in microar-
rays by data perturbation. Bioinformatics, 22(17),
2114-2121.
Hamed Masnadi-Shirazi, Vijay Mahadevan, and Nuno
Vasconcelos. 2010. On the design of robust classi-
fiers for computer vision. IEEE International Con-
ference Computer Vision and Pattern Recognition.
Lukas Meier, Sara van de Geer, Peter Buhlmann. 2008.
The group lasso for logistic regression. Journal of
the Royal Statistical Society, 70(1), 53-71.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. EMNLP.
Vikas Raykar, Shipeng Yu, Linda H. Zhao, Anna Jere-
bko, Charles Florin, Gerardo Hermosillo Valadez,
Luca Bogoni, and Linda Moy. 2009. Supervised
learning from multiple experts: whom to trust when
everyone lies a bit. ICML.
Umaa Rebbapragada, Lukas Mandrake, Kiri L.
Wagstaff, Damhnait Gleeson, Rebecca Castano,
Steve Chien, Carla E. Brodley 2009. Improv-
ing Onboard Analysis of Hyperion Images by Fil-
tering mislabelled Training Data Examples. IEEE
Aerospace Conference.
Sebastian Riedel, Limin Yao, Andrew McCallum.
2010. Modeling Relations and Their Mentions with-
out Labelled Text. ECML PKDD.
D. Sculley and Gordon V. Cormack 2008. Filtering
Email Spam in the Presence of Noisy User Feed-
back. CEAS.
Yiyuan She and Art Owen. 2011. Outlier Detection
Using Nonconvex Penalized Regression. Journal of
the American Statistical Association, 106(494).
Erik F. Tjong Kim Sang, Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
CoNLL.
John Wright and Yi Ma. 2009. Dense Error Correction
via l1-Minimization IEEE Transactions on Informa-
tion Theory.
</reference>
<page confidence="0.998538">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852064">
<title confidence="0.999125">Robust Logistic Regression using Shift Parameters</title>
<author confidence="0.917975">D Tibshirani</author>
<affiliation confidence="0.903877">Stanford</affiliation>
<address confidence="0.99552">Stanford, CA 94305,</address>
<abstract confidence="0.999604777777778">Annotation errors can significantly hurt classifier performance, yet datasets are only growing noisier with the increased use of Amazon Mechanical Turk and techniques like distant supervision that automatically generate labels. In this paper, we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. This model can be trained through nearly the same means as logistic regression, and retains its efficiency on highdimensional datasets. We conduct experiments on named entity recognition data and find that our approach can provide a significant improvement over the standard model when annotation errors are present.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>U Alon</author>
<author>N Barkai</author>
<author>D A Notterman</author>
<author>K Gish</author>
<author>S Ybarra</author>
<author>D Mack</author>
<author>A J Levine</author>
</authors>
<title>Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays.</title>
<date>1999</date>
<booktitle>National Academy of Sciences of the USA.</booktitle>
<contexts>
<context position="11487" citStr="Alon et al. (1999)" startWordPosition="1916" endWordPosition="1919"> the estimated shift parameters are nonzero. We now consider situations in which the θ parameters are regularized as well. Assume, for example, that we use L1-regularization as in equation (2), so that we now need to optimize over both κ and λ. We perform the following simple procedure: 1. Cross-validate using standard logistic regression to select κ. 2. Fix this value for κ, and cross-validate using the robust model to find the best choice of λ. 1 g(θT xi + γi) _ 1 + e−θT xi−γi . + (1 − yi) log (1 − g(θTxi + γi))i − κ �m |θj |− λ n |γi|. j=1 i=1 126 method suspects identified false positives Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36 Furey et al. (2000) • • • • • • Kadota et al. (2003) • • • • • T6, N2 Malossini et al. (2006) • • • • • • • T8, N2, N28, N29 Bootkrajang et al. (2012) • • • • • • • Robust LR • • • • • • • Table 1: Results of various error-identification methods on the colon cancer dataset. The first row lists the samples that are biologically confirmed to be suspicious, and each other row gives the output from an automatic detection method. Bootkrajang et al. report confidences, so we threshold at 0.5 to obtain these results. 4 Experiments We conduct two sets of experiments </context>
</contexts>
<marker>Alon, Barkai, Notterman, Gish, Ybarra, Mack, Levine, 1999</marker>
<rawString>U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, A. J. Levine. 1999. Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. National Academy of Sciences of the USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable Training of Ll-Regularized Log-Linear Models.</title>
<date>2007</date>
<publisher>ICML.</publisher>
<contexts>
<context position="16390" citStr="Andrew and Gao, 2007" startWordPosition="2761" endWordPosition="2764"> highly engineered – it largely consists of lexical features such as the current word, the previous and next words in the sentence, as well as character n-grams and various word shape features. With a total of 393,633 features in the train set, we choose to use L2- regularization, so that our penalty now becomes This choice is natural as L2 is the most common form of regularization in NLP, and we wish to verify that our approach works for penalties besides L1. The robust model is fit using Orthant-Wise Limited-Memory Quasi Newton (OWL-QN), a technique for optimizing an L1-penalized objective (Andrew and Gao, 2007). We tune both models through 5-fold cross-validation to obtain σ2 = 1.0 and A = 0.1. Note that from the way we cross-validate (first tuning σ using standard logistic regression, fixing this choice, then tuning A) our procedure may give an unfair advantage to the baseline. We also compare against the algorithm proposed in Bootkrajang and Kaban (2012), an extension of logistic regression mentioned in the section on prior work. This approach assumes that each example’s true label is flipped with a certain probability before being observed, and fits the resulting latent-variable model using EM. T</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable Training of Ll-Regularized Log-Linear Models. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoram Bachrach</author>
<author>Thore Graepel</author>
<author>Tom Minka</author>
<author>John Guiver</author>
</authors>
<title>How To Grade a Test Without Knowing the Answers: A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing. arXiv preprint arXiv:1206.6386</title>
<date>2012</date>
<contexts>
<context position="5569" citStr="Bachrach et al., 2012" startWordPosition="846" endWordPosition="849">s they train. This way, there is an active trade-off between fitting the model and identifying suspected errors. Bootkrajang and Kaban (2012) present an extension of logistic regression that models annotation errors through flipping probabilities. While intuitive, this approach has shortcomings of its own: the objective function is nonconvex and the authors note that local optima are an issue, and the model can be difficult to fit when there are many more features than training examples. There is a growing body of literature on learning from several annotators, each of whom may be inaccurate (Bachrach et al., 2012; Raykar et al., 2009). It is important to note that we are considering a separate, and perhaps more general, problem: we have only one source of noisy labels, and the errors need not come from the human annotators, but could be introduced through contamination or automatic labelling. The field of ‘robust statistics’ seeks to develop estimators that are not unduly affected by deviations from the model assumptions (Huber and Ronchetti, 2009). Since mislabelled points are one type of outlier, this goal is naturally related to our interest in dealing with noisy data, and it seems many of the exis</context>
</contexts>
<marker>Bachrach, Graepel, Minka, Guiver, 2012</marker>
<rawString>Yoram Bachrach, Thore Graepel, Tom Minka, and John Guiver. 2012. How To Grade a Test Without Knowing the Answers: A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing. arXiv preprint arXiv:1206.6386 (2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakramate Bootkrajang</author>
<author>Ata Kaban</author>
</authors>
<title>Labelnoise Robust Logistic Regression and Its Applications.</title>
<date>2012</date>
<tech>ECML PKDD.</tech>
<contexts>
<context position="5089" citStr="Bootkrajang and Kaban (2012)" startWordPosition="766" endWordPosition="770"> same label lie close together in feature space. Moreover filtering techniques in general may not be well-justified: if a training example does not fit closely with the current model, it is not necessarily mislabelled. It may represent an important exception that would improve the overall fit, or appear unusual simply because we have made poor modelling assumptions. Perhaps the most promising approaches are those that directly model annotation errors, handling mislabelled examples as they train. This way, there is an active trade-off between fitting the model and identifying suspected errors. Bootkrajang and Kaban (2012) present an extension of logistic regression that models annotation errors through flipping probabilities. While intuitive, this approach has shortcomings of its own: the objective function is nonconvex and the authors note that local optima are an issue, and the model can be difficult to fit when there are many more features than training examples. There is a growing body of literature on learning from several annotators, each of whom may be inaccurate (Bachrach et al., 2012; Raykar et al., 2009). It is important to note that we are considering a separate, and perhaps more general, problem: w</context>
<context position="13510" citStr="Bootkrajang and Kaban (2012)" startWordPosition="2272" endWordPosition="2275">. Using glmnet, an R package for training regularized models (Friedman et al., 2009), we select n and A using cross-validation. Looking at the resulting values for γ, we find that only 7 of the shift parameters are nonzero and that each one corresponds to a suspicious datapoint. As further confirmation, the signs of the gammas correctly match the direction of the mislabelling. Compared to previous attempts to automatically detect errors in this dataset, our approach identifies at least as many suspicious examples but with no false positives. A detailed comparison is given in Table 1. Although Bootkrajang and Kaban (2012) are quite accurate, it is worth noting that due to its nonconvexity, their model needed to be trained 20 times to achieve these results. 4.2 Manually Annotated Data We now consider the problem of named entity recognition (NER) to evaluate how our model performs in a large-scale prediction task. In traditional NER, the goal is to determine whether each word is a person, organization, location, or not a named entity (‘other’). Since our model is binary, we concentrate on the task of deciding whether a word is a person or not. (This task does not trivially reduce to finding the capitalized words</context>
<context position="15735" citStr="Bootkrajang and Kaban (2012)" startWordPosition="2648" endWordPosition="2651">ervative. We evaluate on the English development test set from the CoNLL shared task (Tjong Kim Sang and Meulder, 2003). This data consists of news articles from the Reuters corpus, hand-annotated by researchers at the University of Antwerp. We extract a set of features using Stanford’s NER pipeline (Finkel et al., 2005). This set was 127 Precision model precision recall F1 standard 76.99 85.87 81.19 flipping 76.62 86.28 81.17 robust 77.04 90.47 83.22 Table 2: Performance of standard vs. robust logistic regression in the Wikipedia NER experiment. The flipping model refers to the approach from Bootkrajang and Kaban (2012). chosen for simplicity and is not highly engineered – it largely consists of lexical features such as the current word, the previous and next words in the sentence, as well as character n-grams and various word shape features. With a total of 393,633 features in the train set, we choose to use L2- regularization, so that our penalty now becomes This choice is natural as L2 is the most common form of regularization in NLP, and we wish to verify that our approach works for penalties besides L1. The robust model is fit using Orthant-Wise Limited-Memory Quasi Newton (OWL-QN), a technique for opti</context>
<context position="17606" citStr="Bootkrajang and Kaban (2012)" startWordPosition="2956" endWordPosition="2959"> using EM. The results of these experiments are shown in Table 2 as well as Figure 2. Robust logistic regression offers a noticeable improvement over the baseline, and this improvement holds at essentially all levels of precision and recall. Interestingly, because of the large dimension, the flipping model consistently learns that no labels have been flipped and thus does not show a substantial difference with standard logistic regression. Recall Figure 2: Precision-recall curve obtained from training on noisy Wikipedia data and testing on CoNLL. The flipping model refers to the approach from Bootkrajang and Kaban (2012). 5 Future Work A natural direction for future work is to extend the model to a multi-class setting. One option is to introduce a y for every class except the negative one, so that there are n(c − 1) shift parameters in all. We could then apply a group lasso, with each group consisting of they for a particular datapoint (Meier et al., 2008). This way all of a datapoint’s shift parameters drop out together, which corresponds to the example being correctly labelled. CRFs and other sequence models could also benefit from the addition of shift parameters. Since the extra variables can be neatly fo</context>
</contexts>
<marker>Bootkrajang, Kaban, 2012</marker>
<rawString>Jakramate Bootkrajang and Ata Kaban. 2012. Labelnoise Robust Logistic Regression and Its Applications. ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carla E Brodley</author>
<author>Mark A Friedl</author>
</authors>
<title>Identifying mislabeled Training Data.</title>
<date>1999</date>
<journal>JAIR,</journal>
<volume>11</volume>
<pages>131--167</pages>
<contexts>
<context position="3338" citStr="Brodley and Friedl (1999)" startWordPosition="510" endWordPosition="513">rovement over standard logistic regression when annotation errors are present. The model also provides a means to identify which examples were mislabelled: through experiments on biological data, we demonstrate how our method can be used to accurately identify annotation errors. This robust extension of logistic regression shows particular promise for NLP applications: it helps account for incorrect labels, while remaining efficient on large, high-dimensional datasets. 2 Related Work Much of the previous work on dealing with annotation errors centers around filtering the data before training. Brodley and Friedl (1999) introduce what is perhaps the simplest form of supervised filtering: they train various classifiers, then record their predictions on a different part of the train set and eliminate contentious examples. Sculley and Cormack (2008) apply this approach to spam filtering with noisy user feedback. One obvious issue with these methods is that the noise-detecting classifiers are themselves trained 124 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 124–129, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational L</context>
</contexts>
<marker>Brodley, Friedl, 1999</marker>
<rawString>Carla E. Brodley and Mark A. Friedl. 1999. Identifying mislabeled Training Data. JAIR, 11, 131-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel J Candes</author>
<author>Xiaodong Li</author>
<author>Yi Ma</author>
<author>John Wright</author>
</authors>
<title>Robust Principal Component Analysis? arXiv preprint arXiv:0912.3599,</title>
<date>2009</date>
<contexts>
<context position="6936" citStr="Candes et al. (2009)" startWordPosition="1070" endWordPosition="1073">and several models along Figure 1: Fit resulting from a standard vs. robust model, where data is generated from the dashed sigmoid and negative labels flipped with probability 0.2. these lines have been proposed (Ding and Vishwanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimizing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mislabelled. In a recent advance, She and Owen (2011) demonstrate that introducing a regularized ‘shift parameter’ per datapoint can help increase the robustness of linear regression. Candes et al. (2009) propose a similar approach for principal component analysis, while Wright and Ma (2009) explore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic regression. To the best of our knowledge, we are the first to experiment with adding ‘shift parameters’ to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP. 3 Model Recall that in binary logistic regression, the probability of an example xi being positive is modeled as 1 g(0T xi) = For simplicity, we assume the </context>
</contexts>
<marker>Candes, Li, Ma, Wright, 2009</marker>
<rawString>Emmanuel J. Candes, Xiaodong Li, Yi Ma, John Wright. 2009. Robust Principal Component Analysis? arXiv preprint arXiv:0912.3599, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Ding</author>
<author>S V N Vishwanathan</author>
</authors>
<title>t-Logistic regression.</title>
<date>2010</date>
<publisher>NIPS.</publisher>
<marker>Ding, Vishwanathan, 2010</marker>
<rawString>Nan Ding and S. V. N. Vishwanathan. 2010. t-Logistic regression. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shipra Dingare</author>
<author>Malvina Nissim</author>
<author>Jenny Finkel</author>
<author>Christopher Manning</author>
<author>Claire Grover</author>
</authors>
<title>A system for identifying named entities in biomedical text: How results from two evaluations reflect on both the system and the evaluations. Comparative and Functional Genomics.</title>
<date>2005</date>
<volume>6</volume>
<issue>1</issue>
<pages>77--85</pages>
<contexts>
<context position="1604" citStr="Dingare et al. (2005)" startWordPosition="240" endWordPosition="243">nly used in natural language processing. Lowquality annotations have become even more common in recent years with the rise of Amazon Mechanical Turk, as well as methods like distant supervision and co-training that involve automatically generating training data. Although small amounts of noise may not be detrimental, in some applications the level can be high: upon manually inspecting a relation extraction corpus commonly used in distant supervision, Riedel et al. (2010) report a 31% false positive rate. In cases like these, annotation errors have frequently been observed to hurt performance. Dingare et al. (2005), for example, conduct error analysis on a system to extract relations from biomedical text, and observe that over half of the system’s errors could be attributed to inconsistencies in how the data was annotated. Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. In this work we argue that incorrect examples should be explicitly modelled during training, and present a simple extension of logistic</context>
</contexts>
<marker>Dingare, Nissim, Finkel, Manning, Grover, 2005</marker>
<rawString>Shipra Dingare, Malvina Nissim, Jenny Finkel, Christopher Manning, and Claire Grover. 2005. A system for identifying named entities in biomedical text: How results from two evaluations reflect on both the system and the evaluations. Comparative and Functional Genomics. 6(1–2), 77-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<publisher>ACL.</publisher>
<contexts>
<context position="15429" citStr="Finkel et al., 2005" startWordPosition="2600" endWordPosition="2603">% of examples, the majority agreed they were negative but the chosen annotator marked them positive, and 7.5% were labelled positive by the majority but negative by the annotator. Note that we still include examples for which there was no majority consensus, so these noise estimates are quite conservative. We evaluate on the English development test set from the CoNLL shared task (Tjong Kim Sang and Meulder, 2003). This data consists of news articles from the Reuters corpus, hand-annotated by researchers at the University of Antwerp. We extract a set of features using Stanford’s NER pipeline (Finkel et al., 2005). This set was 127 Precision model precision recall F1 standard 76.99 85.87 81.19 flipping 76.62 86.28 81.17 robust 77.04 90.47 83.22 Table 2: Performance of standard vs. robust logistic regression in the Wikipedia NER experiment. The flipping model refers to the approach from Bootkrajang and Kaban (2012). chosen for simplicity and is not highly engineered – it largely consists of lexical features such as the current word, the previous and next words in the sentence, as well as character n-grams and various word shape features. With a total of 393,633 features in the train set, we choose to us</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jerome Friedman</author>
<author>Trevor Hastie</author>
</authors>
<title>Rob Tibshirani 2009. Regularization Paths for Generalized Linear Models via Coordinate Descent.</title>
<journal>Journal of statistical software,</journal>
<volume>33</volume>
<issue>1</issue>
<pages>1</pages>
<marker>Friedman, Hastie, </marker>
<rawString>Jerome Friedman, Trevor Hastie, Rob Tibshirani 2009. Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of statistical software, 33(1), 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrence S Furey</author>
<author>Nello Cristianini</author>
<author>Nigel Duffy</author>
<author>David W Bednarski</author>
<author>Michel Schummer</author>
<author>David Haussler</author>
</authors>
<title>Support vector machine classification and validation of cancer tissue samples using microarray expression data.</title>
<date>2000</date>
<journal>Bioinformatics,</journal>
<volume>16</volume>
<issue>10</issue>
<pages>906--914</pages>
<contexts>
<context position="11541" citStr="Furey et al. (2000)" startWordPosition="1929" endWordPosition="1932">onsider situations in which the θ parameters are regularized as well. Assume, for example, that we use L1-regularization as in equation (2), so that we now need to optimize over both κ and λ. We perform the following simple procedure: 1. Cross-validate using standard logistic regression to select κ. 2. Fix this value for κ, and cross-validate using the robust model to find the best choice of λ. 1 g(θT xi + γi) _ 1 + e−θT xi−γi . + (1 − yi) log (1 − g(θTxi + γi))i − κ �m |θj |− λ n |γi|. j=1 i=1 126 method suspects identified false positives Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36 Furey et al. (2000) • • • • • • Kadota et al. (2003) • • • • • T6, N2 Malossini et al. (2006) • • • • • • • T8, N2, N28, N29 Bootkrajang et al. (2012) • • • • • • • Robust LR • • • • • • • Table 1: Results of various error-identification methods on the colon cancer dataset. The first row lists the samples that are biologically confirmed to be suspicious, and each other row gives the output from an automatic detection method. Bootkrajang et al. report confidences, so we threshold at 0.5 to obtain these results. 4 Experiments We conduct two sets of experiments to assess the effectiveness of the approach, in terms </context>
</contexts>
<marker>Furey, Cristianini, Duffy, Bednarski, Schummer, Haussler, 2000</marker>
<rawString>Terrence S. Furey, Nello Cristianini, Nigel Duffy, David W. Bednarski, Michel Schummer, David Haussler. 2000. Support vector machine classification and validation of cancer tissue samples using microarray expression data. Bioinformatics, 16(10), 906-914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Huber</author>
<author>Elvezio M Ronchetti</author>
</authors>
<title>Robust Statistics.</title>
<date>2000</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>Hoboken, NJ.</location>
<marker>Huber, Ronchetti, 2000</marker>
<rawString>Peter J. Huber and Elvezio M. Ronchetti. 2000. Robust Statistics. John Wiley &amp; Sons, Inc., Hoboken, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ander Intxaurrondo</author>
</authors>
<title>Mihai Surdeanu, Oier Lopez de Lacalle, and Eneko Agirre.</title>
<date>2013</date>
<marker>Intxaurrondo, 2013</marker>
<rawString>Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez de Lacalle, and Eneko Agirre. 2013. Removing Noisy Mentions for Distant Supervision. Congreso de la Sociedad Espaola para el Procesamiento del Lenguaje Natural.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Kadota</author>
</authors>
<title>Daisuke Tominaga, Yutaka Akiyama, Katsutoshi Takahashi.</title>
<date>2003</date>
<journal>ChemBio Informatics Journal,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>30--45</pages>
<marker>Kadota, 2003</marker>
<rawString>Koji Kadota, Daisuke Tominaga, Yutaka Akiyama, Katsutoshi Takahashi. 2003. Detecting outlying samples in microarray data: A critical assessment of the effect of outliers on sample. ChemBio Informatics Journal, 3(1), 30-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Malossini</author>
<author>Enrico Blanzieri</author>
<author>Raymond T Ng</author>
</authors>
<title>Detecting potential labeling errors in microarrays by data perturbation.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>22</volume>
<issue>17</issue>
<pages>2114--2121</pages>
<contexts>
<context position="11615" citStr="Malossini et al. (2006)" startWordPosition="1950" endWordPosition="1953">Assume, for example, that we use L1-regularization as in equation (2), so that we now need to optimize over both κ and λ. We perform the following simple procedure: 1. Cross-validate using standard logistic regression to select κ. 2. Fix this value for κ, and cross-validate using the robust model to find the best choice of λ. 1 g(θT xi + γi) _ 1 + e−θT xi−γi . + (1 − yi) log (1 − g(θTxi + γi))i − κ �m |θj |− λ n |γi|. j=1 i=1 126 method suspects identified false positives Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36 Furey et al. (2000) • • • • • • Kadota et al. (2003) • • • • • T6, N2 Malossini et al. (2006) • • • • • • • T8, N2, N28, N29 Bootkrajang et al. (2012) • • • • • • • Robust LR • • • • • • • Table 1: Results of various error-identification methods on the colon cancer dataset. The first row lists the samples that are biologically confirmed to be suspicious, and each other row gives the output from an automatic detection method. Bootkrajang et al. report confidences, so we threshold at 0.5 to obtain these results. 4 Experiments We conduct two sets of experiments to assess the effectiveness of the approach, in terms of both identifying mislabelled examples and producing accurate prediction</context>
</contexts>
<marker>Malossini, Blanzieri, Ng, 2006</marker>
<rawString>Andrea Malossini, Enrico Blanzieri, Raymond T. Ng. 2006. Detecting potential labeling errors in microarrays by data perturbation. Bioinformatics, 22(17), 2114-2121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamed Masnadi-Shirazi</author>
<author>Vijay Mahadevan</author>
<author>Nuno Vasconcelos</author>
</authors>
<title>On the design of robust classifiers for computer vision.</title>
<date>2010</date>
<booktitle>IEEE International Conference Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="6588" citStr="Masnadi-Shirazi et al., 2010" startWordPosition="1019" endWordPosition="1022">from the model assumptions (Huber and Ronchetti, 2009). Since mislabelled points are one type of outlier, this goal is naturally related to our interest in dealing with noisy data, and it seems many of the existing techniques would be relevant. A common strategy is to use a modified loss function that gives less influence to points far from the boundary, and several models along Figure 1: Fit resulting from a standard vs. robust model, where data is generated from the dashed sigmoid and negative labels flipped with probability 0.2. these lines have been proposed (Ding and Vishwanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimizing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mislabelled. In a recent advance, She and Owen (2011) demonstrate that introducing a regularized ‘shift parameter’ per datapoint can help increase the robustness of linear regression. Candes et al. (2009) propose a similar approach for principal component analysis, while Wright and Ma (2009) explore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic regression. To the best of our knowledge, we are the first to </context>
</contexts>
<marker>Masnadi-Shirazi, Mahadevan, Vasconcelos, 2010</marker>
<rawString>Hamed Masnadi-Shirazi, Vijay Mahadevan, and Nuno Vasconcelos. 2010. On the design of robust classifiers for computer vision. IEEE International Conference Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lukas Meier</author>
<author>Sara van de Geer</author>
<author>Peter Buhlmann</author>
</authors>
<title>The group lasso for logistic regression.</title>
<date>2008</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>70</volume>
<issue>1</issue>
<pages>53--71</pages>
<marker>Meier, van de Geer, Buhlmann, 2008</marker>
<rawString>Lukas Meier, Sara van de Geer, Peter Buhlmann. 2008. The group lasso for logistic regression. Journal of the Royal Statistical Society, 70(1), 53-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pierce</author>
<author>Claire Cardie</author>
</authors>
<title>Limitations of co-training for natural language learning from large datasets.</title>
<date>2001</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1911" citStr="Pierce and Cardie (2001)" startWordPosition="293" endWordPosition="296">e detrimental, in some applications the level can be high: upon manually inspecting a relation extraction corpus commonly used in distant supervision, Riedel et al. (2010) report a 31% false positive rate. In cases like these, annotation errors have frequently been observed to hurt performance. Dingare et al. (2005), for example, conduct error analysis on a system to extract relations from biomedical text, and observe that over half of the system’s errors could be attributed to inconsistencies in how the data was annotated. Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. In this work we argue that incorrect examples should be explicitly modelled during training, and present a simple extension of logistic regression that incorporates the possibility of mislabelling directly into the objective. Following a technique from robust statistics, our model introduces sparse ‘shift parameters’ to allow datapoints to slide along the sigmoid, changing class if appropriate. It has a convex objective, is well-suited to</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>David Pierce and Claire Cardie. 2001. Limitations of co-training for natural language learning from large datasets. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Anna Jerebko</author>
<author>Charles Florin</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Supervised learning from multiple experts: whom to trust when everyone lies a bit.</title>
<date>2009</date>
<publisher>ICML.</publisher>
<contexts>
<context position="5591" citStr="Raykar et al., 2009" startWordPosition="850" endWordPosition="853"> there is an active trade-off between fitting the model and identifying suspected errors. Bootkrajang and Kaban (2012) present an extension of logistic regression that models annotation errors through flipping probabilities. While intuitive, this approach has shortcomings of its own: the objective function is nonconvex and the authors note that local optima are an issue, and the model can be difficult to fit when there are many more features than training examples. There is a growing body of literature on learning from several annotators, each of whom may be inaccurate (Bachrach et al., 2012; Raykar et al., 2009). It is important to note that we are considering a separate, and perhaps more general, problem: we have only one source of noisy labels, and the errors need not come from the human annotators, but could be introduced through contamination or automatic labelling. The field of ‘robust statistics’ seeks to develop estimators that are not unduly affected by deviations from the model assumptions (Huber and Ronchetti, 2009). Since mislabelled points are one type of outlier, this goal is naturally related to our interest in dealing with noisy data, and it seems many of the existing techniques would </context>
</contexts>
<marker>Raykar, Yu, Zhao, Jerebko, Florin, Valadez, Bogoni, Moy, 2009</marker>
<rawString>Vikas Raykar, Shipeng Yu, Linda H. Zhao, Anna Jerebko, Charles Florin, Gerardo Hermosillo Valadez, Luca Bogoni, and Linda Moy. 2009. Supervised learning from multiple experts: whom to trust when everyone lies a bit. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umaa Rebbapragada</author>
<author>Lukas Mandrake</author>
<author>Kiri L Wagstaff</author>
<author>Damhnait Gleeson</author>
<author>Rebecca Castano</author>
<author>Steve Chien</author>
<author>Carla E Brodley</author>
</authors>
<title>Improving Onboard Analysis of Hyperion Images by Filtering mislabelled Training Data Examples.</title>
<date>2009</date>
<journal>IEEE Aerospace Conference.</journal>
<contexts>
<context position="4163" citStr="Rebbapragada et al., 2009" startWordPosition="629" endWordPosition="632">ples. Sculley and Cormack (2008) apply this approach to spam filtering with noisy user feedback. One obvious issue with these methods is that the noise-detecting classifiers are themselves trained 124 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 124–129, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on noisy labels. Unsupervised filtering tries to avoid this problem by clustering training instances based solely on their features, then using the clusters to detect labelling anomalies (Rebbapragada et al., 2009). Recently, Intxaurrondo et al. (2013) applied this approach to distantly-supervised relation extraction, using heuristics such as the number of mentions per tuple to eliminate suspicious examples. Unsupervised filtering, however, relies on the perhaps unwarranted assumption that examples with the same label lie close together in feature space. Moreover filtering techniques in general may not be well-justified: if a training example does not fit closely with the current model, it is not necessarily mislabelled. It may represent an important exception that would improve the overall fit, or appe</context>
</contexts>
<marker>Rebbapragada, Mandrake, Wagstaff, Gleeson, Castano, Chien, Brodley, 2009</marker>
<rawString>Umaa Rebbapragada, Lukas Mandrake, Kiri L. Wagstaff, Damhnait Gleeson, Rebecca Castano, Steve Chien, Carla E. Brodley 2009. Improving Onboard Analysis of Hyperion Images by Filtering mislabelled Training Data Examples. IEEE Aerospace Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling Relations and Their Mentions without Labelled Text.</title>
<date>2010</date>
<tech>ECML PKDD.</tech>
<contexts>
<context position="1458" citStr="Riedel et al. (2010)" startWordPosition="215" endWordPosition="218">en annotation errors are present. 1 Introduction Almost any large dataset has annotation errors, especially those complex, nuanced datasets commonly used in natural language processing. Lowquality annotations have become even more common in recent years with the rise of Amazon Mechanical Turk, as well as methods like distant supervision and co-training that involve automatically generating training data. Although small amounts of noise may not be detrimental, in some applications the level can be high: upon manually inspecting a relation extraction corpus commonly used in distant supervision, Riedel et al. (2010) report a 31% false positive rate. In cases like these, annotation errors have frequently been observed to hurt performance. Dingare et al. (2005), for example, conduct error analysis on a system to extract relations from biomedical text, and observe that over half of the system’s errors could be attributed to inconsistencies in how the data was annotated. Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised cou</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum. 2010. Modeling Relations and Their Mentions without Labelled Text. ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
<author>Gordon V Cormack</author>
</authors>
<title>Filtering Email Spam in the Presence of Noisy User Feedback.</title>
<date>2008</date>
<publisher>CEAS.</publisher>
<contexts>
<context position="3569" citStr="Sculley and Cormack (2008)" startWordPosition="544" endWordPosition="547">be used to accurately identify annotation errors. This robust extension of logistic regression shows particular promise for NLP applications: it helps account for incorrect labels, while remaining efficient on large, high-dimensional datasets. 2 Related Work Much of the previous work on dealing with annotation errors centers around filtering the data before training. Brodley and Friedl (1999) introduce what is perhaps the simplest form of supervised filtering: they train various classifiers, then record their predictions on a different part of the train set and eliminate contentious examples. Sculley and Cormack (2008) apply this approach to spam filtering with noisy user feedback. One obvious issue with these methods is that the noise-detecting classifiers are themselves trained 124 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 124–129, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on noisy labels. Unsupervised filtering tries to avoid this problem by clustering training instances based solely on their features, then using the clusters to detect labelling anomalies (Rebbapragada et al., 2009). Rece</context>
</contexts>
<marker>Sculley, Cormack, 2008</marker>
<rawString>D. Sculley and Gordon V. Cormack 2008. Filtering Email Spam in the Presence of Noisy User Feedback. CEAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiyuan She</author>
<author>Art Owen</author>
</authors>
<title>Outlier Detection Using Nonconvex Penalized Regression.</title>
<date>2011</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>106</volume>
<issue>494</issue>
<contexts>
<context position="6785" citStr="She and Owen (2011)" startWordPosition="1048" endWordPosition="1051">xisting techniques would be relevant. A common strategy is to use a modified loss function that gives less influence to points far from the boundary, and several models along Figure 1: Fit resulting from a standard vs. robust model, where data is generated from the dashed sigmoid and negative labels flipped with probability 0.2. these lines have been proposed (Ding and Vishwanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimizing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mislabelled. In a recent advance, She and Owen (2011) demonstrate that introducing a regularized ‘shift parameter’ per datapoint can help increase the robustness of linear regression. Candes et al. (2009) propose a similar approach for principal component analysis, while Wright and Ma (2009) explore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic regression. To the best of our knowledge, we are the first to experiment with adding ‘shift parameters’ to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP. 3 Mod</context>
</contexts>
<marker>She, Owen, 2011</marker>
<rawString>Yiyuan She and Art Owen. 2011. Outlier Detection Using Nonconvex Penalized Regression. Journal of the American Statistical Association, 106(494).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang, Fien De Meulder.</title>
<date>2003</date>
<publisher>CoNLL.</publisher>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang, Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wright</author>
<author>Yi Ma</author>
</authors>
<title>Dense Error Correction via l1-Minimization</title>
<date>2009</date>
<journal>IEEE Transactions on Information Theory.</journal>
<contexts>
<context position="7024" citStr="Wright and Ma (2009)" startWordPosition="1084" endWordPosition="1087"> data is generated from the dashed sigmoid and negative labels flipped with probability 0.2. these lines have been proposed (Ding and Vishwanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimizing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mislabelled. In a recent advance, She and Owen (2011) demonstrate that introducing a regularized ‘shift parameter’ per datapoint can help increase the robustness of linear regression. Candes et al. (2009) propose a similar approach for principal component analysis, while Wright and Ma (2009) explore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic regression. To the best of our knowledge, we are the first to experiment with adding ‘shift parameters’ to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP. 3 Model Recall that in binary logistic regression, the probability of an example xi being positive is modeled as 1 g(0T xi) = For simplicity, we assume the intercept term has been folded into the weight vector 0, so 0 ∈ Rm+1 where m is the numb</context>
</contexts>
<marker>Wright, Ma, 2009</marker>
<rawString>John Wright and Yi Ma. 2009. Dense Error Correction via l1-Minimization IEEE Transactions on Information Theory.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>