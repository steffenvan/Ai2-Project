<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.080524">
<title confidence="0.987073">
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
</title>
<author confidence="0.877664">
Zhifei Li, Chris Callison-Burch, Chris Dyer,† Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,* Wren N. G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
</author>
<affiliation confidence="0.563654">
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
† Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
* Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
</affiliation>
<sectionHeader confidence="0.951949" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999587">
We describe the progress we have made in
the past year on Joshua (Li et al., 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998748380952381">
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al., 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
</bodyText>
<listItem confidence="0.998950466666666">
• Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
• Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al., 2010)
• Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al., 2008)
• A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
• Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al., 2009b)
• Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
• A parallelization of MERT’s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
• Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
• A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
</listItem>
<bodyText confidence="0.997807">
The sections below give short descriptions for
each of these new functions.
</bodyText>
<page confidence="0.987859">
133
</page>
<note confidence="0.6802905">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.973954" genericHeader="introduction">
2 Support for Syntax-based Translation
</sectionHeader>
<bodyText confidence="0.996398806451613">
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)’s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X] ⇒ [X,1] sans [X,2]  |[X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP] ⇒ [VBN] sans [NP]  |[VBN] without [NP]
[NP] ⇒ [NP] sans [NP]  |[NP] without [NP]
Unlike GHKM (Galley et al., 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al., 2009).
</bodyText>
<sectionHeader confidence="0.954024" genericHeader="method">
3 Specifying Constraints on Translation
</sectionHeader>
<bodyText confidence="0.99994580952381">
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al., 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua’s chart-based decoder to support these
constraints.
</bodyText>
<sectionHeader confidence="0.904678" genericHeader="method">
4 Semiring Parsing
</sectionHeader>
<bodyText confidence="0.999964153846154">
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
</bodyText>
<sectionHeader confidence="0.980532" genericHeader="method">
5 Word Lattice Input
</sectionHeader>
<bodyText confidence="0.999968066666667">
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation’s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al., 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
</bodyText>
<page confidence="0.997601">
134
</page>
<sectionHeader confidence="0.995592" genericHeader="method">
6 Variational Decoding
</sectionHeader>
<bodyText confidence="0.999976724137931">
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al.,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions 2 in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
</bodyText>
<sectionHeader confidence="0.968052" genericHeader="method">
7 Hypergraph-based Discriminative
Training
</sectionHeader>
<bodyText confidence="0.999961344827586">
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al.,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al. (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
</bodyText>
<sectionHeader confidence="0.961827" genericHeader="method">
8 Minimum Error Rate Training
</sectionHeader>
<bodyText confidence="0.998770517241379">
Joshua’s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al., 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module’s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
</bodyText>
<footnote confidence="0.995770625">
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE’s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
˜ozaidan/zmert.)
</footnote>
<page confidence="0.997817">
135
</page>
<sectionHeader confidence="0.965791" genericHeader="method">
9 Visualization
</sectionHeader>
<bodyText confidence="0.999988952380952">
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
</bodyText>
<sectionHeader confidence="0.95822" genericHeader="method">
10 Pipeline for Running MT
Experiments
</sectionHeader>
<bodyText confidence="0.9998528">
Reproducing other researchers’ machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run – from downloading data and
software through scoring the final translated re-
sults – by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
</bodyText>
<sectionHeader confidence="0.990274" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999937666666667">
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors’ alone.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999382906976744">
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218–226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012–
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107–116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10–18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
</reference>
<page confidence="0.986347">
136
</page>
<reference confidence="0.999711882352942">
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on “MT
From Text”.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503–528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586–591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157–166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127–136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
</reference>
<page confidence="0.997829">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.044868">
<title confidence="0.4611665">Joshua 2.0: A Toolkit for Parsing-Based Machine with Syntax, Semirings, Discriminative Training and Other Goodies</title>
<degree confidence="0.516916">Li, Chris Callison-Burch, Chris Irvine, Sanjeev Khudanpur, Lane N. G. Wang, Jonathan Weese F.</degree>
<affiliation confidence="0.8305975">Center for Language and Speech Processing, Johns Hopkins University, Baltimore, Linguistics and Information Processing Lab, University of Maryland, College Park,</affiliation>
<address confidence="0.764511">Natural Language Processing Lab, University of Minnesota, Minneapolis, MN</address>
<abstract confidence="0.998063904761905">We describe the progress we have made in past year on et al., 2009a), an open source toolkit for parsing based machine translation. The new functionality includes: support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, documentlevel and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Kathy Baker</author>
<author>Steven Bethard</author>
<author>Michael Bloodgood</author>
<author>Ralf Brown</author>
<author>Chris Callison-Burch</author>
<author>Glen Coppersmith</author>
<author>Bonnie Dorr</author>
<author>Wes Filardo</author>
<author>Kendall Giles</author>
</authors>
<title>Semantically informed machine translation (SIMT). SCALE summer workshop final report,</title>
<date>2009</date>
<institution>Human Language Technology Center Of Excellence.</institution>
<location>Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane</location>
<contexts>
<context position="5220" citStr="Baker et al., 2009" startWordPosition="785" endWordPosition="788">what types of phrases the decoder can use when producing a translation. [VP] ⇒ [VBN] sans [NP] |[VBN] without [NP] [NP] ⇒ [NP] sans [NP] |[NP] without [NP] Unlike GHKM (Galley et al., 2004), SAMT has the same coverage as Hiero, because it allows non-constituent phrases to get syntactic labels using CCG-style slash notation. Experimentally, we have found that the derivations created using syntactically motivated grammars exhibit more coherent syntactic structure than Hiero and typically result in better reordering, especially for languages with word orders that diverge from English, like Urdu (Baker et al., 2009). 3 Specifying Constraints on Translation Integrating output from specialized modules (like transliterators, morphological analyzers, and modality translators) into the MT pipeline can improve translation performance, particularly for low-resource languages. We have implemented an XML interface that allows external modules to propose alternate translation rules (constraints) for a particular word span to the decoder (Irvine et al., 2010). Processing that is separate from the MT engine can suggest translations for some set of source side words and phrases. The XML format allows for both hard co</context>
</contexts>
<marker>Baker, Bethard, Bloodgood, Brown, Callison-Burch, Coppersmith, Dorr, Filardo, Giles, 2009</marker>
<rawString>Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine, Mike Kayser, Lori Levin, Justin Martineau, Jim Mayfield, Scott Miller, Aaron Phillips, Andrew Philpot, Christine Piatko, Lane Schwartz, and David Zajic. 2009. Semantically informed machine translation (SIMT). SCALE summer workshop final report, Human Language Technology Center Of Excellence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>218--226</pages>
<contexts>
<context position="10824" citStr="Chiang et al. (2009)" startWordPosition="1642" endWordPosition="1645">so implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.e., the translation in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al. (2009)) can also be easily supported with minimum coding. We plan to implement a large number of feature functions in Joshua so that exhaustive feature engineering is possible for MT. 8 Minimum Error Rate Training Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu (Och, 2003). We have parallelized our MERT module in two ways: parallelizing the computation of metric scores, and parallelizing the search over parameters. The computation of metric scores is a computational concern when tuning to</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL, pages 218–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1598" citStr="Chiang, 2007" startWordPosition="233" endWordPosition="234">tainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT module, documentlevel and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments. 1 Introduction Joshua is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1012--1020</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2814" citStr="Dyer et al., 2008" startWordPosition="423" endWordPosition="426">y the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A con</context>
<context position="7801" citStr="Dyer et al., 2008" startWordPosition="1170" endWordPosition="1173">ities over hypergraphs. These quantities include expected translation length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on. 5 Word Lattice Input We generalized the bottom-up parsing algorithm that generates the translation hypergraph so that it supports translation of word lattices instead of just sentences. Our implementation’s runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al., 2008). Accepting lattice-based input allows the decoder to explore a distribution over input sentences, allowing it to select the best translation from among all of them. This is especially useful when Joshua is used to translate the output of statistical preprocessing components, such as speech recognizers or Chinese word segmenters, which can encode their alternative analyses as confusion networks or lattices. 134 6 Variational Decoding Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct derivations (e.g.,</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL-08: HLT, pages 1012– 1020, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Parameter estimation for probabilistic finite-state transducers.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6971" citStr="Eisner, 2002" startWordPosition="1047" endWordPosition="1048"> derivation trees generated by the decoder for an input sentence. Given a hypergraph, we may perform many atomic inference operations, such as finding one-best or k-best translations, or computing expectations over the hypergraph. For each such operation, we could implement a dedicated dynamic programming algorithm. However, a more general framework to specify these algorithms is semiring-weighted parsing (Goodman, 1999). We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009). All of these use our newly implemented semiring framework. The first- and second-order expectation semirings can also be used to compute many interesting quantities over hypergraphs. These quantities include expected translation length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on. 5 Word Lattice Input We generalized the bottom-up parsing algorithm that generates the translation hypergraph so th</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4790" citStr="Galley et al., 2004" startWordPosition="720" endWordPosition="723"> SAMT grammar extraction is identical to Hiero grammar extraction, except that one side of the parallel corpus is parsed, and syntactic labels replace the X nonterminals in Hiero-style rules. Instead of extracting this Hiero rule from the bitext [X] ⇒ [X,1] sans [X,2] |[X,1] without [X,2] the nonterminals can be labeled according to which constituents cover the nonterminal span on the parsed side of the bitext. This constrains what types of phrases the decoder can use when producing a translation. [VP] ⇒ [VBN] sans [NP] |[VBN] without [NP] [NP] ⇒ [NP] sans [NP] |[NP] without [NP] Unlike GHKM (Galley et al., 2004), SAMT has the same coverage as Hiero, because it allows non-constituent phrases to get syntactic labels using CCG-style slash notation. Experimentally, we have found that the derivations created using syntactically motivated grammars exhibit more coherent syntactic structure than Hiero and typically result in better reordering, especially for languages with word orders that diverge from English, like Urdu (Baker et al., 2009). 3 Specifying Constraints on Translation Integrating output from specialized modules (like transliterators, morphological analyzers, and modality translators) into the M</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="6782" citStr="Goodman, 1999" startWordPosition="1017" endWordPosition="1019"> NP. We modified Joshua’s chart-based decoder to support these constraints. 4 Semiring Parsing In Joshua, we use a hypergraph (or packed forest) to compactly represent the exponentially many derivation trees generated by the decoder for an input sentence. Given a hypergraph, we may perform many atomic inference operations, such as finding one-best or k-best translations, or computing expectations over the hypergraph. For each such operation, we could implement a dedicated dynamic programming algorithm. However, a more general framework to specify these algorithms is semiring-weighted parsing (Goodman, 1999). We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009). All of these use our newly implemented semiring framework. The first- and second-order expectation semirings can also be used to compute many interesting quantities over hypergraphs. These quantities include expected translation length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence, Bayes risk, variance of hypothesis length, grad</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Mike Kayser</author>
<author>Zhifei Li</author>
<author>Wren Thornton</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Integrating output from specialized modules in machine translation: Transliteration in joshua.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--107</pages>
<contexts>
<context position="2662" citStr="Irvine et al., 2010" startWordPosition="399" endWordPosition="402">ur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting do</context>
<context position="5661" citStr="Irvine et al., 2010" startWordPosition="842" endWordPosition="845">erent syntactic structure than Hiero and typically result in better reordering, especially for languages with word orders that diverge from English, like Urdu (Baker et al., 2009). 3 Specifying Constraints on Translation Integrating output from specialized modules (like transliterators, morphological analyzers, and modality translators) into the MT pipeline can improve translation performance, particularly for low-resource languages. We have implemented an XML interface that allows external modules to propose alternate translation rules (constraints) for a particular word span to the decoder (Irvine et al., 2010). Processing that is separate from the MT engine can suggest translations for some set of source side words and phrases. The XML format allows for both hard constraints, which must be used, and soft constraints, which compete with standard extracted translation rules, as well as specifying associated feature weights. In addition to specifying translations, the XML format allows constraints on the lefthand side of SCFG rules, which allows constraints like forcing a particular span to be translated as an NP. We modified Joshua’s chart-based decoder to support these constraints. 4 Semiring Parsin</context>
</contexts>
<marker>Irvine, Kayser, Li, Thornton, Callison-Burch, 2010</marker>
<rawString>Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton, and Chris Callison-Burch. 2010. Integrating output from specialized modules in machine translation: Transliteration in joshua. The Prague Bulletin of Mathematical Linguistics, 93:107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and secondorder expectation semirings with applications to minimum-risk training on translation forests.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<contexts>
<context position="2956" citStr="Li and Eisner, 2009" startWordPosition="444" endWordPosition="447">nt new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give shor</context>
<context position="6913" citStr="Li and Eisner (2009)" startWordPosition="1036" endWordPosition="1040"> (or packed forest) to compactly represent the exponentially many derivation trees generated by the decoder for an input sentence. Given a hypergraph, we may perform many atomic inference operations, such as finding one-best or k-best translations, or computing expectations over the hypergraph. For each such operation, we could implement a dedicated dynamic programming algorithm. However, a more general framework to specify these algorithms is semiring-weighted parsing (Goodman, 1999). We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009). All of these use our newly implemented semiring framework. The first- and second-order expectation semirings can also be used to compute many interesting quantities over hypergraphs. These quantities include expected translation length, feature expectation, entropy, cross-entropy, Kullback-Leibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covariance and Hessian matrix, and so on. 5 Word Lattice Input We generalized the bottom-up parsing</context>
<context position="9868" citStr="Li and Eisner, 2009" startWordPosition="1493" endWordPosition="1496"> which inference is tractable. Third, the best string according to q (instead of p) is found. In our implementation, the q distribution is parameterized by an n-gram model, under which the second and third steps can be performed efficiently and exactly via dynamic programming. In this way, variational decoding considers all derivations in the hypergraph but still allows tractable decoding. 7 Hypergraph-based Discriminative Training Discriminative training with a large number of features has potential to improve the MT performance. We have implemented the hypergraphbased minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objective can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al., 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In EMNLP, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>A scalable decoder for parsing-based machine translation with equivalent language model state maintenance.</title>
<date>2008</date>
<booktitle>In ACL SSST,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="2051" citStr="Li and Khudanpur, 2008" startWordPosition="298" endWordPosition="301">for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et</context>
</contexts>
<marker>Li, Khudanpur, 2008</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2008. A scalable decoder for parsing-based machine translation with equivalent language model state maintenance. In ACL SSST, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3198" citStr="Li and Khudanpur, 2009" startWordPosition="478" endWordPosition="482">slations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 </context>
<context position="10296" citStr="Li and Khudanpur, 2009" startWordPosition="1558" endWordPosition="1562"> Training Discriminative training with a large number of features has potential to improve the MT performance. We have implemented the hypergraphbased minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objective can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al., 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.e., the translation in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al. (2009)) can also be easily supported with minimum coding. We plan to implement</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient extraction of oracle-best translations from hypergraphs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Forest reranking for machine translation with the perceptron algorithm.</title>
<date>2009</date>
<booktitle>In GALE book chapter on “MT From Text”.</booktitle>
<contexts>
<context position="3198" citStr="Li and Khudanpur, 2009" startWordPosition="478" endWordPosition="482">slations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 </context>
<context position="10296" citStr="Li and Khudanpur, 2009" startWordPosition="1558" endWordPosition="1562"> Training Discriminative training with a large number of features has potential to improve the MT performance. We have implemented the hypergraphbased minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objective can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al., 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.e., the translation in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al. (2009)) can also be easily supported with minimum coding. We plan to implement</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009b. Forest reranking for machine translation with the perceptron algorithm. In GALE book chapter on “MT From Text”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsingbased machine translation.</title>
<date>2009</date>
<booktitle>In WMT09.</booktitle>
<contexts>
<context position="1937" citStr="Zaidan, 2009" startWordPosition="284" endWordPosition="285">ation trees, and a cleaner pipeline for MT experiments. 1 Introduction Joshua is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Sup</context>
<context position="12390" citStr="Zaidan (2009)" startWordPosition="1894" endWordPosition="1895"> Another feature is the module’s awareness of document information, and the capability to perform optimization of document-based variants of the automatic metric (Zaidan, 2010). For example, in document-based Bleu, a Bleu score is calculated for each document, and the tuned score is the average of those document scores. The MERT module can furthermore be instructed to target a specific subset of those documents, namely the tail subset, where only the subset of documents with the lowest document Bleu scores are considered.2 More details on the MERT method and the implementation can be found in Zaidan (2009).3 1Based on sample code by Kenneth Heafield. 2This feature is of interest to GALE teams, for instance, since GALE’s evaluation criteria place a lot of focus on translation quality of tail documents. 3The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 135 9 Visualization We created tools for visualizing two of the main data structures used in Joshua (Weese and Callison-Burch, 2010). The first visualizer displays hypergraphs. The user can choose from a set of input sentence</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar. Zaidan. 2009a. Joshua: An open source toolkit for parsingbased machine translation. In WMT09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Variational decoding for statistical machine translation.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="656" citStr="Li et al., 2009" startWordPosition="90" endWordPosition="93">achine Translation with Syntax, Semirings, Discriminative Training and Other Goodies Zhifei Li, Chris Callison-Burch, Chris Dyer,† Juri Ganitkevitch, Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,* Wren N. G. Thornton, Ziyuan Wang, Jonathan Weese and Omar F. Zaidan Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD † Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD * Natural Language Processing Lab, University of Minnesota, Minneapolis, MN Abstract We describe the progress we have made in the past year on Joshua (Li et al., 2009a), an open source toolkit for parsing based machine translation. The new functionality includes: support for translation grammars with a rich set of syntactic nonterminals, the ability for external modules to posit constraints on how spans in the input sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a unified way of doing various dynamic programming calculations, variational decoding for approximating the intractable MAP decoding, hypergraph-based discriminative training for better feature engineering, a parallelized MERT m</context>
<context position="3098" citStr="Li et al., 2009" startWordPosition="466" endWordPosition="469">n (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, </context>
<context position="8922" citStr="Li et al., 2009" startWordPosition="1344" endWordPosition="1347">. That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations) that have the same yield. In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string during decoding is then NP-hard. The first version of Joshua implemented the Viterbi approximation, which measures the goodness of a translation using only its most probable derivation. The Viterbi approximation is efficient, but it ignores most of the derivations in the hypergraph. We implemented variational decoding (Li et al., 2009b), which works as follows. First, given a foreign string (or lattice), the MT system produces a hypergraph, which encodes a probability distribution p over possible output strings and their derivations. Second, a distribution q is selected that approximates p as well as possible but comes from a family of distributions 2 in which inference is tractable. Third, the best string according to q (instead of p) is found. In our implementation, the q distribution is parameterized by an n-gram model, under which the second and third steps can be performed efficiently and exactly via dynamic programmi</context>
</contexts>
<marker>Li, Eisner, Khudanpur, 2009</marker>
<rawString>Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b. Variational decoding for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="10153" citStr="Liu et al., 1989" startWordPosition="1537" endWordPosition="1540">y, variational decoding considers all derivations in the hypergraph but still allows tractable decoding. 7 Hypergraph-based Discriminative Training Discriminative training with a large number of features has potential to improve the MT performance. We have implemented the hypergraphbased minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objective can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al., 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.e., the translation in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g.,</context>
</contexts>
<marker>Liu, Nocedal, Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1844" citStr="Lopez, 2007" startWordPosition="270" endWordPosition="271">, a parallelized MERT module, documentlevel and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments. 1 Introduction Joshua is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) incl</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1922" citStr="Och, 2003" startWordPosition="282" endWordPosition="283">f the derivation trees, and a cleaner pipeline for MT experiments. 1 Introduction Joshua is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugop</context>
<context position="11204" citStr="Och, 2003" startWordPosition="1707" endWordPosition="1708">emented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al. (2009)) can also be easily supported with minimum coding. We plan to implement a large number of feature functions in Joshua so that exhaustive feature engineering is possible for MT. 8 Minimum Error Rate Training Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu (Och, 2003). We have parallelized our MERT module in two ways: parallelizing the computation of metric scores, and parallelizing the search over parameters. The computation of metric scores is a computational concern when tuning to a metric that is slow to compute, such as translation edit rate (Snover et al., 2006). Since scoring a candidate is independent from scoring any other candidate, we parallelize this computation using a multi-threaded solution1. Similarly, we parallelize the optimization of the intermediate initial weight vectors, also using a multi-threaded solution. Another feature is the mod</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Riedmiller</author>
<author>Heinrich Braun</author>
</authors>
<title>A direct adaptive method for faster backpropagation learning: The rprop algorithm.</title>
<date>1993</date>
<booktitle>In IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS,</booktitle>
<pages>586--591</pages>
<contexts>
<context position="10192" citStr="Riedmiller and Braun, 1993" startWordPosition="1543" endWordPosition="1546">ders all derivations in the hypergraph but still allows tractable decoding. 7 Hypergraph-based Discriminative Training Discriminative training with a large number of features has potential to improve the MT performance. We have implemented the hypergraphbased minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objective can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al., 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Perceptron algorithm and forest-reranking (Li and Khudanpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or inherent defficiency of the translation grammar, we need to use an oracle translation (i.e., the translation in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA</context>
</contexts>
<marker>Riedmiller, Braun, 1993</marker>
<rawString>Martin Riedmiller and Heinrich Braun. 1993. A direct adaptive method for faster backpropagation learning: The rprop algorithm. In IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, pages 586–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Hierarchical phrase-based grammar extraction in joshua.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--157</pages>
<marker>Schwartz, Callison-Burch, 2010</marker>
<rawString>Lane Schwartz and Chris Callison-Burch. 2010. Hierarchical phrase-based grammar extraction in joshua. The Prague Bulletin of Mathematical Linguistics, 93:157–166.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lane Schwartz</author>
</authors>
<title>under review. Reproducible results in parsing-based machine translation: The JHU shared task submission.</title>
<booktitle>In WMT10.</booktitle>
<marker>Schwartz, </marker>
<rawString>Lane Schwartz. under review. Reproducible results in parsing-based machine translation: The JHU shared task submission. In WMT10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="11510" citStr="Snover et al., 2006" startWordPosition="1757" endWordPosition="1760">lement a large number of feature functions in Joshua so that exhaustive feature engineering is possible for MT. 8 Minimum Error Rate Training Joshua’s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu (Och, 2003). We have parallelized our MERT module in two ways: parallelizing the computation of metric scores, and parallelizing the search over parameters. The computation of metric scores is a computational concern when tuning to a metric that is slow to compute, such as translation edit rate (Snover et al., 2006). Since scoring a candidate is independent from scoring any other candidate, we parallelize this computation using a multi-threaded solution1. Similarly, we parallelize the optimization of the intermediate initial weight vectors, also using a multi-threaded solution. Another feature is the module’s awareness of document information, and the capability to perform optimization of document-based variants of the automatic metric (Zaidan, 2010). For example, in document-based Bleu, a Bleu score is calculated for each document, and the tuned score is the average of those document scores. The MERT mo</context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2006</marker>
<rawString>Matthew Snover, Bonnie J. Dorr, and Richard Schwartz. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Visualizing data structures in parsing-based machine translation.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--127</pages>
<contexts>
<context position="3406" citStr="Weese and Callison-Burch, 2010" startWordPosition="506" endWordPosition="509">ese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Support for Syntax-based Translation The initial release of Joshua supported only Hiero-style SCFGs, which use a single nonterminal symbol X. This release includes support for arbitrary SCFGs, including ones </context>
<context position="12897" citStr="Weese and Callison-Burch, 2010" startWordPosition="1971" endWordPosition="1974">t document Bleu scores are considered.2 More details on the MERT method and the implementation can be found in Zaidan (2009).3 1Based on sample code by Kenneth Heafield. 2This feature is of interest to GALE teams, for instance, since GALE’s evaluation criteria place a lot of focus on translation quality of tail documents. 3The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 135 9 Visualization We created tools for visualizing two of the main data structures used in Joshua (Weese and Callison-Burch, 2010). The first visualizer displays hypergraphs. The user can choose from a set of input sentences, then call the decoder to build the hypergraph. The second visualizer displays derivation trees. Setting a flag in the configuration file causes the decoder to output parse trees instead of strings, where each nonterminal is annotated with its source-side span. The visualizer can read in multiple n-best lists in this format, then display the resulting derivation trees side-byside. We have found that visually inspecting these derivation trees is useful for debugging grammars. We would like to add visu</context>
</contexts>
<marker>Weese, Callison-Burch, 2010</marker>
<rawString>Jonathan Weese and Chris Callison-Burch. 2010. Visualizing data structures in parsing-based machine translation. The Prague Bulletin of Mathematical Linguistics, 93:127–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="1937" citStr="Zaidan, 2009" startWordPosition="284" endWordPosition="285">ation trees, and a cleaner pipeline for MT experiments. 1 Introduction Joshua is an open-source toolkit for parsing-based machine translation that is written in Java. The initial release of Joshua (Li et al., 2009a) was a re-implementation of the Hiero system (Chiang, 2007) and all its associated algorithms, including: chart parsing, n-gram language model integration, beam and cube pruning, and k-best extraction. The Joshua 1.0 release also included re-implementations of suffix array grammar extraction (Lopez, 2007; Schwartz and CallisonBurch, 2010) and minimum error rate training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Sup</context>
<context position="12390" citStr="Zaidan (2009)" startWordPosition="1894" endWordPosition="1895"> Another feature is the module’s awareness of document information, and the capability to perform optimization of document-based variants of the automatic metric (Zaidan, 2010). For example, in document-based Bleu, a Bleu score is calculated for each document, and the tuned score is the average of those document scores. The MERT module can furthermore be instructed to target a specific subset of those documents, namely the tail subset, where only the subset of documents with the lowest document Bleu scores are considered.2 More details on the MERT method and the implementation can be found in Zaidan (2009).3 1Based on sample code by Kenneth Heafield. 2This feature is of interest to GALE teams, for instance, since GALE’s evaluation criteria place a lot of focus on translation quality of tail documents. 3The module is also available as a standalone application, Z-MERT, that can be used with other MT systems. (Software and documentation at: http://cs.jhu.edu/ ˜ozaidan/zmert.) 135 9 Visualization We created tools for visualizing two of the main data structures used in Joshua (Weese and Callison-Burch, 2010). The first visualizer displays hypergraphs. The user can choose from a set of input sentence</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Document- and tail-based minimum error rate training of machine translation systems.</title>
<date>2010</date>
<booktitle>In preparation.</booktitle>
<contexts>
<context position="3317" citStr="Zaidan, 2010" startWordPosition="495" endWordPosition="496">uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminative training for better feature engineering (Li and Khudanpur, 2009b) • A parallelization of MERT’s computations, and supporting document-level and tail-based optimization (Zaidan, 2010) • Visualization of the derivation trees and hypergraphs (Weese and Callison-Burch, 2010) • A convenient framework for designing and running reproducible machine translation experiments (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Support for Syntax-based Translation The initial release of Joshua supported only Hiero-style SCFGs, which use a single</context>
<context position="11953" citStr="Zaidan, 2010" startWordPosition="1819" endWordPosition="1820">rameters. The computation of metric scores is a computational concern when tuning to a metric that is slow to compute, such as translation edit rate (Snover et al., 2006). Since scoring a candidate is independent from scoring any other candidate, we parallelize this computation using a multi-threaded solution1. Similarly, we parallelize the optimization of the intermediate initial weight vectors, also using a multi-threaded solution. Another feature is the module’s awareness of document information, and the capability to perform optimization of document-based variants of the automatic metric (Zaidan, 2010). For example, in document-based Bleu, a Bleu score is calculated for each document, and the tuned score is the average of those document scores. The MERT module can furthermore be instructed to target a specific subset of those documents, namely the tail subset, where only the subset of documents with the lowest document Bleu scores are considered.2 More details on the MERT method and the implementation can be found in Zaidan (2009).3 1Based on sample code by Kenneth Heafield. 2This feature is of interest to GALE teams, for instance, since GALE’s evaluation criteria place a lot of focus on tr</context>
</contexts>
<marker>Zaidan, 2010</marker>
<rawString>Omar F. Zaidan. 2010. Document- and tail-based minimum error rate training of machine translation systems. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT-06),</booktitle>
<location>New York, New York.</location>
<contexts>
<context position="2531" citStr="Zollmann and Venugopal, 2006" startWordPosition="376" endWordPosition="379"> training (Och, 2003; Zaidan, 2009). Additionally, it included parallel and distributed computing techniques for scalability (Li and Khudanpur, 2008). This paper describes the additions to the toolkit over the past year, which together form the 2.0 release. The software has been heavily used by the authors and several other groups in their daily research, and has been substantially refined since the first release. The most important new functions in the toolkit are: • Support for any style of synchronous context free grammar (SCFG) including syntax augment machine translation (SAMT) grammars (Zollmann and Venugopal, 2006) • Support for external modules to posit translations for spans in the input sentence that constrain decoding (Irvine et al., 2010) • Lattice parsing for dealing with input uncertainty, including ambiguous output from speech recognizers or Chinese word segmenters (Dyer et al., 2008) • A semiring architecture over hypergraphs that allows many inference operations to be implemented easily and elegantly (Li and Eisner, 2009) • Improvements to decoding through variational decoding and other approximate methods that overcome intractable MAP decoding (Li et al., 2009b) • Hypergraph-based discriminat</context>
<context position="4130" citStr="Zollmann and Venugopal (2006)" startWordPosition="611" endWordPosition="615">s (Schwartz, under review) The sections below give short descriptions for each of these new functions. 133 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133–137, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 2 Support for Syntax-based Translation The initial release of Joshua supported only Hiero-style SCFGs, which use a single nonterminal symbol X. This release includes support for arbitrary SCFGs, including ones that use a rich set of linguistic nonterminal symbols. In particular we have added support for Zollmann and Venugopal (2006)’s syntax-augmented machine translation. SAMT grammar extraction is identical to Hiero grammar extraction, except that one side of the parallel corpus is parsed, and syntactic labels replace the X nonterminals in Hiero-style rules. Instead of extracting this Hiero rule from the bitext [X] ⇒ [X,1] sans [X,2] |[X,1] without [X,2] the nonterminals can be labeled according to which constituents cover the nonterminal span on the parsed side of the bitext. This constrains what types of phrases the decoder can use when producing a translation. [VP] ⇒ [VBN] sans [NP] |[VBN] without [NP] [NP] ⇒ [NP] sa</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the NAACL-2006 Workshop on Statistical Machine Translation (WMT-06), New York, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>