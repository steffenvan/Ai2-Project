<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002783">
<title confidence="0.986311">
Using CCG categories to improve Hindi dependency parsing
</title>
<author confidence="0.985439">
Bharat Ram Ambati Tejaswini Deoskar Mark Steedman
</author>
<affiliation confidence="0.998559">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<email confidence="0.993379">
bharat.ambati@ed.ac.uk, {tdeoskar,steedman}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993803" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999425625">
We show that informative lexical cate-
gories from a strongly lexicalised for-
malism such as Combinatory Categorial
Grammar (CCG) can improve dependency
parsing of Hindi, a free word order lan-
guage. We first describe a novel way to
obtain a CCG lexicon and treebank from
an existing dependency treebank, using a
CCG parser. We use the output of a su-
pertagger trained on the CCGbank as a
feature for a state-of-the-art Hindi depen-
dency parser (Malt). Our results show that
using CCG categories improves the accu-
racy of Malt on long distance dependen-
cies, for which it is known to have weak
rates of recovery.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906076923077">
As compared to English, many Indian languages
including Hindi have a freer word order and are
also morphologically richer. These characteristics
pose challenges to statistical parsers. Today, the
best dependency parsing accuracies for Hindi are
obtained by the shift-reduce parser of Nivre et
al. (2007) (Malt). It has been observed that Malt
is relatively accurate at recovering short distance
dependencies, like arguments of a verb, but is less
accurate at recovering long distance dependencies
like co-ordination, root of the sentence, etc
(Mcdonald and Nivre, 2007; Ambati et al., 2010).
In this work, we show that using CCG lexical
categories (Steedman, 2000), which contain sub-
categorization information and capture long dis-
tance dependencies elegantly, can help Malt with
those dependencies. Section 2 first shows how we
extract a CCG lexicon from an existing Hindi de-
pendency treebank (Bhatt et al., 2009) and then
use it to create a Hindi CCGbank. In section 3, we
develop a supertagger using the CCGbank and ex-
plore different ways of providing CCG categories
from the supertagger as features to Malt. Our re-
sults show that using CCG categories can help
Malt by improving the recovery of long distance
relations.
</bodyText>
<sectionHeader confidence="0.989408" genericHeader="method">
2 A CCG Treebank from a Dependency
Treebank
</sectionHeader>
<bodyText confidence="0.999843411764706">
There have been some efforts at automatically ex-
tracting treebanks of CCG derivations from phrase
structure treebanks (Hockenmaier and Steedman,
2007; Hockenmaier, 2006; Tse and Curran, 2010),
and CCG lexicons from dependency treebanks
(C¸akıcı, 2005). Bos et al. (2009) created a
CCGbank from an Italian dependency treebank by
converting dependency trees into phrase structure
trees and then applying an algorithm similar
to Hockenmaier and Steedman (2007). In this
work, following C¸akıcı (2005), we first extract a
Hindi CCG lexicon from a dependency treebank.
We then use a CKY parser based on the CCG
formalism to automatically obtain a treebank
of CCG derivations from this lexicon, a novel
methodology that may be applicable to obtaining
CCG treebanks in other languages as well.
</bodyText>
<subsectionHeader confidence="0.981545">
2.1 Hindi Dependency Treebank
</subsectionHeader>
<bodyText confidence="0.999975">
In this paper, we work with a subset of the Hindi
Dependency Treebank (HDT ver-0.5) released
as part of Coling 2012 Shared Task on parsing
(Bharati et al., 2012). HDT is a multi-layered
dependency treebank (Bhatt et al., 2009) an-
notated with morpho-syntactic (morphological,
part-of-speech and chunk information) and
syntactico-semantic (dependency) information
(Bharati et al., 2006; Bharati et al., 2009).
Dependency labels are fine-grained, and mark de-
pendencies that are syntactico-semantic in nature,
such as agent (usually corresponding to subject),
patient (object), and time and place expressions.
There are special labels to mark long distance
relations like relative clauses, co-ordination etc
</bodyText>
<page confidence="0.983667">
604
</page>
<bodyText confidence="0.856966818181818">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
(Bharati et al., 1995; Bharati et al., 2009).
The treebank contains 12,041 training, 1,233
development and 1,828 testing sentences with an
average of 22 words per sentence. We used the
CoNLL format1 for our purposes, which contains
word, lemma, pos-tag, and coarse pos-tag in the
WORD, LEMMA, POS, and CPOS fields respectively
and morphological features and chunk information
in the FEATS column.
</bodyText>
<subsectionHeader confidence="0.997467">
2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.99978282051282">
We first made a list of argument and adjunct
dependency labels in the treebank. For e.g.,
dependencies with the label k1 and k2 (corre-
sponding to subject and object respectively) are
considered to be arguments, while labels like
k7p and k7t (corresponding to place and time
expressions) are considered to be adjuncts. For
readability reasons, we will henceforth refer to
dependency labels with their English equivalents
(e.g., SUBJ, OBJ, PURPOSE, CASE for k1, k2,
rt, lwg psp respectively).
Starting from the root of the dependency tree,
we traverse each node. The category of a node de-
pends on both its parent and children. If the node
is an argument of its parent, we assign the chunk
tag of the node (e.g., NP, PP) as its CCG category.
Otherwise, we assign it a category of X|X, where
X is the parent’s result category and  |is direction-
ality (\ or /), which depends on the position of
the node w.r.t. its parent. The result category of
a node is the category obtained once its arguments
are resolved. For example, S, is the result category
for (S\NP)\NP. Once we get the partial category
of a node based on the node’s parent information,
we traverse through the children of the node. If
a child is an argument, we add that child’s chunk
tag, with appropriate directionality, to the node’s
category. The algorithm is sketched in Figure 1
and an example of a CCG derivation for a simple
sentence (marked with chunk tags; NP and VGF
are the chunk tags for noun and finite verb chunks
respectively.) is shown in Figure 2. Details of
some special cases are described in the following
subsections.
We created two types of lexicon. In Type 1,
we keep morphological information in noun cate-
gories and in Type 2, we don’t. For example, con-
sider a noun chunk ‘raam ne (Ram ERG)’. In Type
1, CCG categories for ‘raam’ and ‘ne’ are NP and
</bodyText>
<footnote confidence="0.781199">
1http://nextens.uvt.nl/depparse-wiki/DataFormat
</footnote>
<equation confidence="0.9744054">
ModifyTree(DependencyTree tree);
for (each node in tree):
handlePostPositionMarkers(node);
handleCoordination(node);
handleRelativeClauses(node);
if (node is an argument of parent):
cat = node.chunkTag;
else:
prescat = parent.resultCategory;
cat = prescat + getDir(node, parent) + prescat;
</equation>
<bodyText confidence="0.4433425">
for(each child of node):
if (child is an argument of node):
</bodyText>
<equation confidence="0.63508">
cat = cat + getDir(child, node) + child.chunkTag;
</equation>
<figureCaption confidence="0.9938185">
Figure 1: Algorithm for extracting CCG lexicon
from a dependency tree.
</figureCaption>
<figure confidence="0.995783222222222">
ROOT
ROOT mohan ne raam ke lie kitaab khariidii
Mohan Erg Ram for book buy-past-fem
[NP mohan ne] [NP raam ke lie] [NP kitaab] [VGF khariidii]
NP NP\NP NP (VGF/VGF)\NP NP (VGF\NP)\NP
&lt; &lt; &lt;
NP VGF/VGF VGF\NP
VGF
‘Mohan bought a book for Ram.’
</figure>
<figureCaption confidence="0.849366">
Figure 2: An example dependency tree with its
CCG derivation (Erg = Ergative case).
</figureCaption>
<bodyText confidence="0.999147166666667">
NP[ne]\NP respectively. In Type 2, respective
CCG categories for ‘raam’ and ‘ne’ are NP and
NP\NP. Morphological information such as case
(e.g., Ergative case - ‘ne’) in noun categories is ex-
pected to help with determining their dependency
labels, but makes the lexicon more sparse.
</bodyText>
<subsectionHeader confidence="0.999184">
2.3 Morphological Markers
</subsectionHeader>
<bodyText confidence="0.999978857142857">
In Hindi, morphological information is encoded in
the form of post-positional markers on nouns, and
tense, aspect and modality markers on verbs. A
post-positional marker following a noun plays the
role of a case-marker (e.g., ‘raam ne (Ram ERG)’,
here ‘ne’ is the ergative case marker) and can also
have a role similar to English prepositions (e.g.,
‘mej par (table on)’). Post-positional markers on
nouns can be simple one word expressions like
‘ne’ or ‘par’ or can be multiple words as in ‘raam
ke lie (Ram for)’. Complex post position markers
as a whole give information about how the head
noun or verb behaves. We merged complex post
position markers into single words like ‘ke lie’ so
</bodyText>
<figure confidence="0.998266125">
CASE
CASE
SUBJ
PURPOSE
OBJ
&lt; B×
VGF\NP
&lt;
</figure>
<page confidence="0.99201">
605
</page>
<bodyText confidence="0.9998775">
that the entire marker gets a single CCG category.
For an adjunct like ‘raam ke lie (for Ram)’
in Figure 2, ‘raam’ can have a CCG category
VGF/VGF as it is the head of the chunk and
‘ke lie’ a category of (VGF/VGF)\(VGF/VGF).
Alternatively, if we pass the adjunct information
to the post-position marker (‘ke lie’), and use the
chunk tag ‘NP’ as the category for the head word
(‘raam’), then categories of ‘raam’ and ‘ke lie’ are
NP and (VGF/VGF)\NP respectively. Though
both these analysis give the same semantics, we
chose the latter as it leads to a less sparse lexi-
con. Also, adjuncts that modify adjacent adjuncts
are assigned identical categories X/X making use
of CCG’s composition rule and following C¸ ak~c~
(2005).
</bodyText>
<subsectionHeader confidence="0.99824">
2.4 Co-ordination
</subsectionHeader>
<bodyText confidence="0.999789125">
The CCG category of a conjunction is (X\X)/X,
where a conjunction looks for a child to its right
and then a child to its left. To handle conjunc-
tion with multiple children, we modified the de-
pendency tree, as follows.
For the example given below, in the original de-
pendency tree, the conjunction ora ‘and’ has three
children ‘Ram’ , ‘Shyam’ and ‘Sita’. We modified
the original dependency tree and treat the comma
‘,
’ as a conjunction. As a result, ‘,’ will have ‘Ram’
and ‘Shyam’ as children and ‘and’ will have ‘,’ and
‘Sita’ as children. It is straightforward to convert
this tree into the original dependency tree for the
purpose of evaluation/comparison with other de-
pendency parsers.
</bodyText>
<figure confidence="0.841252">
ROOT
ROOT raam , shyam ora siitaa skoola gaye
raam ,shyam ora siitaa skoola gaye
Ram ,Shyam and Sita school went
NP
&lt;
&lt;
</figure>
<subsectionHeader confidence="0.920039">
2.5 Relative Clauses
</subsectionHeader>
<bodyText confidence="0.997016666666667">
In English, relative clauses have the category type
NP\NP, where they combine with a noun phrase
on the left to give a resulting noun phrase. Hindi,
due to its freer word order, has relative clauses of
the type NP\NP or NP/NP based on the position of
the relative clause with respect to the head noun.
Similar to English, the relative pronoun has a CCG
category of (NP|NP)|X where directionality de-
pends on the position of the relative pronoun in the
clause and the category X depends on the gram-
matical role of the relative pronoun. In the follow-
ing example, X is VGF\NP
</bodyText>
<figure confidence="0.992662181818182">
ROOT
ROOT vaha ladakaa jo baithaa hai raam hai
vaha ladakaa jo baithaa hai raam hai
that boy who sitting be-1P-pres Ram be-1P-pres
NP/NP NP (NP\NP)/X VGF\NP VGF\VGF NP (VGF\NP)\NP
&gt; &gt; B× &lt;
NP VGF\NP VGF\NP
&gt;
NP
&lt;
‘The boy who is sitting is Ram’
</figure>
<subsectionHeader confidence="0.750398">
2.6 CCG Lexicon to Treebank conversion
</subsectionHeader>
<bodyText confidence="0.999811866666667">
We use a CCG parser to convert the CCG lexicon
to a CCG treebank as conversion to CCG trees
directly from dependency trees is not straight-
forward. Using the above algorithm, we get one
CCG category for every word in a sentence. We
then run a non-statistical CKY chart parser based
on the CCG formalism2, which gives CCG deriva-
tions based on the lexical categories. This gives
multiple derivations for some sentences. We rank
these derivations using two criteria. The first cri-
terion is correct recovery of the gold dependency
tree. Derivations which lead to gold dependencies
are given higher weight. In the second criterion,
we prefer derivations which yield intra-chunk de-
pendencies (e.g., verb and auxiliary) prior to inter-
chunk (e.g., verb and its arguments). For exam-
ple, morphological markers (which lead to intra-
chunk dependencies) play a crucial role in identi-
fying correct dependencies . Resolving these de-
pendencies first helps parsers in better identifica-
tion of inter-chunk dependencies such as argument
structure of the verb (Ambati, 2011). We thus ex-
tract the best derivation for each sentence and cre-
ate a CCGbank for Hindi. Coverage, i.e., number
of sentences for which we got at least one com-
plete derivation, using this lexicon is 96%. The
remaining 4% are either cases of wrong annota-
tions in the original treebank, or rare constructions
which are currently not handled by our conversion
algorithm.
</bodyText>
<footnote confidence="0.675514">
2http://openccg.sourceforge.net/
</footnote>
<figure confidence="0.990442714285714">
COORD
COORD
COORD COORD
SUBJ
DEST
SUBJ
DEM
RELC
AUX
SUBJ
OBJ
NP (NP\NP)/NP NP (NP\NP)/NP NP NP (VGF\NP)\NP
&gt; &gt; &lt;
NP\NP NP\NP VGF\NP
&lt;
NP
VGF
‘Ram , Shyam and Sita went to school.’
NP\NP
&gt;
VGF
</figure>
<page confidence="0.991796">
606
</page>
<sectionHeader confidence="0.998786" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999986090909091">
In this section, we first describe the method of de-
veloping a supertagger using the CCGbank. We
then describe different ways of providing CCG
categories from the supertagger as features to a
state-of-the-art Hindi Dependency parser (Malt).
We did all our experiments using both gold fea-
tures (pos, chunk and morphological information)
provided in the treebank and automatic features
extracted using a Hindi shallow parser3. We re-
port results with automatic features but we also
obtained similar improvements with gold features.
</bodyText>
<subsectionHeader confidence="0.99884">
3.1 Category Set
</subsectionHeader>
<bodyText confidence="0.999991466666667">
For supertagging, we first obtained a category set
from the CCGbank training data. There are 2,177
and 718 category types in Type 1 (with morph. in-
formation) and Type 2 (without morph. informa-
tion) data respectively. Clark and Curran (2004)
showed that using a frequency cutoff can signif-
icantly reduce the size of the category set with
only a small loss in coverage. We explored dif-
ferent cut-off values and finally used a cutoff of
10 for building the tagger. This reduced the cat-
egory types to 376 and 202 for Type 1 and Type
2 respectively. The percent of category tokens in
development data that don’t appear in the category
set entrailed by this cut-off are 1.39 &amp; 0.47 for
Type 1 and Type 2 respectively.
</bodyText>
<subsectionHeader confidence="0.992964">
3.2 Supertagger
</subsectionHeader>
<bodyText confidence="0.997068388888889">
Following Clark and Curran (2004), we used
a Maximum Entropy approach to build our su-
pertagger. We explored different features in the
context of a 5-word window surrounding the tar-
get word. We used features based on WORD (w),
LEMMA (l), POS (p), CPOS (c) and the FEATS (f)
columns of the CoNLL format. Table 1 shows the
impact of different features on supertagger perfor-
mance. Experiments 1, 2, 3 have current word (wi)
features while Experiments 4, 5, 6 show the im-
pact of contextual and complex bi-gram features.
Accuracy of the supertagger after Experiment 6
is 82.92% and 84.40% for Type 1 and Type 2 data
respectively. As the number of category types in
Type 1 data (376) are much higher than in Type 2
(202), it is not surprising that the performance of
the supertagger is better for Type 2 as compared to
Type 1.
</bodyText>
<footnote confidence="0.927602">
3http://ltrc.iiit.ac.in/analyzer/hindi/
</footnote>
<table confidence="0.999040181818182">
Experiments: Features Accuracy
Type 1 Type 2
Exp 1: wi, pi 75.14 78.47
Exp 2: Exp 1 + li, ci 77.58 80.17
Exp 3: Exp 2 + fi 80.43 81.88
Exp 4: Exp 3 +wi−1,wi−2, pi−1,pi−2, 82.72 84.15
wi+1, wi+2, pi+1, pi+2
Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29
Exp 6: Exp 5 + wi−2wi−1, wi−1wi, 82.92 84.40
wiwi+1, wi+1wi+2, pi−2pi−1,
pi−1pi, pipi+1, pi+1pi+2
</table>
<tableCaption confidence="0.9945005">
Table 1: Impact of different features on the su-
pertagger performance for development data.
</tableCaption>
<subsectionHeader confidence="0.992614">
3.3 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9999571">
There has been a significant amount of work on
Hindi dependency parsing in the recent past (Hu-
sain, 2009; Husain et al., 2010; Bharati et al.,
2012). Out of all these efforts, state-of-the-art ac-
curacy is achieved using the Malt parser. We first
run Malt with previous best settings (Bharati et
al., 2012) which use the arc-standard parsing al-
gorithm with a liblinear learner, and treat this as
our baseline. We compare and analyze results af-
ter adding supertags as features with this baseline.
</bodyText>
<subsectionHeader confidence="0.997806">
3.4 Using Supertags as Features to Malt
</subsectionHeader>
<bodyText confidence="0.999799619047619">
C¸akıcı (2009) showed that using gold CCG cate-
gories extracted from dependency trees as features
to MST parser (McDonald et al., 2006) boosted
the performance for Turkish. But using automatic
categories from a supertagger radically decreased
performance in their case as supertagger accuracy
was very low. We have explored different ways
of incorporating both gold CCG categories and
supertagger-provided CCG categories into depen-
dency parsing. Following C¸akıcı (2009), instead
of using supertags for all words, we used supertags
which occurred at least K times in the training
data, and backed off to coarse POS-tags otherwise.
We experimented with different values of K and
found that K=15 gave the best results.
We first provided gold CCG categories as fea-
tures to the Malt parser and then provided the out-
put of the supertagger described in section 3.2. We
did all these experiments with both Type 1 and
Type 2 data. Unlabelled Attachment Scores (UAS)
and Labelled Attachment Scores (LAS) for Malt
</bodyText>
<page confidence="0.996071">
607
</page>
<bodyText confidence="0.999787166666667">
are shown in Table 2. As expected, gold CCG
categories boosted UAS and LAS by around 6%
and 7% respectively, for both Type 1 and Type 2
data. This clearly shows that the rich subcatego-
rization information provided by CCG categories
can help a shift-reduce parser. With automatic cat-
egories from a supertagger, we also got improve-
ments over the baseline, for both Type 1 and Type
2 data. All the improvements are statistically sig-
nificant (McNemar’s test, p &lt; 0.01).
With gold CCG categories, Type 1 data gave
slightly better improvements over Type 2 as Type
1 data has richer morphological information. But,
in the case of supertagger output, Type 2 data
gave more improvements over the baseline Malt
as compared to Type 1. This is because the perfor-
mance of the supertagger on Type 2 data is slightly
better than that of Type 1 data (see Table 1).
</bodyText>
<table confidence="0.996763857142857">
Experiment Development Testing
UAS LAS UAS LAS
Malt: Baseline 89.09 83.46 88.67 83.04
Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22*
Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18*
Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23*
Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35*
</table>
<tableCaption confidence="0.875583333333333">
Table 2: Supertagger impact on Hindi dependency
parsing (ST=Supertags). McNemar’s test, * = p &lt;
0.01.
</tableCaption>
<bodyText confidence="0.999483">
It is interesting to notice the impact of using
automatic CCG categories from a supertagger on
long distance dependencies. It is known that Malt
is weak at long-distance relations (Mcdonald and
Nivre, 2007; Ambati et al., 2010). Providing
CCG categories as features improved handling of
long-distance dependencies for Malt. Figure 3
shows the F-score of the impact of CCG categories
on three dependency labels, which take the ma-
jor share of long distance dependencies, namely,
ROOT, COORD, and RELC, the labels for sentence
root, co-ordination, and relative clause respec-
tively. For these relations, providing CCG cate-
gories gave an increment of 1.2%, 1.4% and 1.6%
respectively over the baseline.
We also found that the impact of CCG cate-
gories is higher when the span of the dependency
is longer. Figure 4 shows the F-score of the impact
of CCG categories on dependencies based on the
distance between words. Using CCG categories
</bodyText>
<figureCaption confidence="0.982359166666667">
Figure 3: Label-wise impact of supertag features.
does not have much impact on short distance de-
pendencies (1−5), which Malt is already good at.
For longer range distances, 6−10, and &gt;10, there
is an improvement of 1.8% and 1.4% respectively.
Figure 4: Impact of supertags on distance ranges.
</figureCaption>
<sectionHeader confidence="0.859388" genericHeader="conclusions">
4 Conclusion and Future Direction
</sectionHeader>
<bodyText confidence="0.999971111111111">
We have presented an approach for automatically
extracting a CCG lexicon from a dependency tree-
bank for Hindi. We have also presented a novel
way of creating a CCGbank from a dependency
treebank using a CCG parser and the CCG lex-
icon. Unlike previous work, we obtained im-
provements in dependency recovery using auto-
matic supertags, as well as gold information. We
have shown that informative CCG categories im-
prove the performance of a shift-reduce depen-
dency parser (Malt) in recovering some long dis-
tance relations. In future work, we would like to
directly train a CCG shift-reduce parser (such as
Zhang and Clark (2011)’s English parser) on the
Hindi CCGbank. We would also like to see the
impact of generalisation of our lexicon using the
free-word order formalism for CCG categories of
Baldridge (2002).
</bodyText>
<sectionHeader confidence="0.996512" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9982335">
We would like to thank three anonymous review-
ers for their useful suggestions. This work was
supported by ERC Advanced Fellowship 249520
GRAMPLUS.
</bodyText>
<figure confidence="0.997287285714286">
F-score
100
75
50
25
73.12
ROOT COORD RELC
Dependency Labels
74.35
80.9
82.28
Malt Malt + Type 2 ST
29.89
31.46
F-score
100
90
80
70
97.3
1 - 5 6 - 10 &gt; 10
Distance ranges
97.5
79.0
80.8
Malt Malt + Type 2 ST
76.4
77.8
</figure>
<page confidence="0.985499">
608
</page>
<sectionHeader confidence="0.988196" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999451268518519">
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010.
Two Methods to Incorporate ’Local Morphosyntac-
tic’ Features in Hindi Dependency Parsing. In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages, pages 22–30, Los Angeles, CA, USA, June.
Bharat Ram Ambati. 2011. Hindi Dependency Parsing
and Treebank Validation. Master’s Thesis, Interna-
tional Institute of Information Technology - Hyder-
abad, India.
Jason M. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, UK.
Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal.
1995. Natural Language Processing: A Paninian
Perspective. Prentice-Hall of India, pages 65–106.
Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma,
and Lakshmi Bai. 2006. AnnCorra: Annotating
Corpora Guidelines for POS and Chunk Annotation
for Indian Languages. In Technical Report (TR-
LTRC-31), LTRC, IIIT-Hyderabad.
Akshar Bharati, Dipti Misra Sharma, Samar
Husain, Lakshmi Bai, Rafiya Begum, and
Rajeev Sangal. 2009. AnnCorra: Tree-
Banks for Indian Languages, Guidelines for
Annotating Hindi TreeBank (version 2.0).
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf.
Akshar Bharati, Prashanth Mannem, and Dipti Misra
Sharma. 2012. Hindi Parsing Shared Task. In Pro-
ceedings of Coling Workshop on Machine Transla-
tion and Parsing in Indian Languages, Kharagpur,
India.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. A multi-representational and multi-layered
treebank for Hindi/Urdu. In Proceedings of the
Third Linguistic Annotation Workshop at 47th ACL
and 4th IJCNLP, pages 186–189, Suntec, Singapore.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a Dependency Treebank to a Cat-
egorial Grammar Treebank for Italian. In M. Pas-
sarotti, Adam Przepi´orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27–38, Milan, Italy.
Ruken C¸akıcı. 2005. Automatic induction of a CCG
grammar for Turkish. In Proceedings of Student Re-
search Workshop, 43rd Annual Meeting of the ACL,
pages 73–78.
Ruket C¸akıcı. 2009. Parser Models for a Highly In-
flected Language. Ph.D. thesis, University of Edin-
burgh, UK.
Stephen Clark and James R. Curran. 2004. The impor-
tance of supertagging for wide-coverage CCG pars-
ing. In Proceedings of COLING-04, pages 282–288.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355–396, Septem-
ber.
Julia Hockenmaier. 2006. Creating a CCGbank and
a wide-coverage CCG lexicon for German. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 505–512, Sydney, Aus-
tralia.
Samar Husain, Prashanth Mannem, Bharat Ram Am-
bati, and Phani Gadde. 2010. The ICON-
2010 Tools Contest on Indian Language Depen-
dency Parsing. In Proceedings of ICON-2010 Tools
Contest on Indian Language Dependency Parsing,
Kharagpur, India.
Samar Husain. 2009. Dependency Parsers for Indian
Languages. In Proceedings of the ICON09 NLP
Tools Contest: Indian Language Dependency Pars-
ing, India.
Ryan Mcdonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
and Natural Language Learning.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216–220, New
York City, New York.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: extracting CCG derivations from the Penn
Chinese Treebank. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ’10, pages 1083–1091, Beijing, China.
Yue Zhang and Stephen Clark. 2011. Shift-Reduce
CCG Parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
683–692, Portland, Oregon, USA, June.
</reference>
<page confidence="0.998831">
609
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984099">
<title confidence="0.999952">Using CCG categories to improve Hindi dependency parsing</title>
<author confidence="0.99996">Bharat Ram Ambati Tejaswini Deoskar Mark Steedman</author>
<affiliation confidence="0.998317">Institute for Language, Cognition and School of Informatics, University of Edinburgh</affiliation>
<abstract confidence="0.999076647058824">We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bharat Ram Ambati</author>
<author>Samar Husain</author>
<author>Sambhav Jain</author>
<author>Dipti Misra Sharma</author>
<author>Rajeev Sangal</author>
</authors>
<title>Two Methods to Incorporate ’Local Morphosyntactic’ Features in Hindi Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>22--30</pages>
<location>Los Angeles, CA, USA,</location>
<contexts>
<context position="1480" citStr="Ambati et al., 2010" startWordPosition="222" endWordPosition="225">s of recovery. 1 Introduction As compared to English, many Indian languages including Hindi have a freer word order and are also morphologically richer. These characteristics pose challenges to statistical parsers. Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recov</context>
<context position="17681" citStr="Ambati et al., 2010" startWordPosition="2928" endWordPosition="2931"> data (see Table 1). Experiment Development Testing UAS LAS UAS LAS Malt: Baseline 89.09 83.46 88.67 83.04 Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22* Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18* Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23* Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35* Table 2: Supertagger impact on Hindi dependency parsing (ST=Supertags). McNemar’s test, * = p &lt; 0.01. It is interesting to notice the impact of using automatic CCG categories from a supertagger on long distance dependencies. It is known that Malt is weak at long-distance relations (Mcdonald and Nivre, 2007; Ambati et al., 2010). Providing CCG categories as features improved handling of long-distance dependencies for Malt. Figure 3 shows the F-score of the impact of CCG categories on three dependency labels, which take the major share of long distance dependencies, namely, ROOT, COORD, and RELC, the labels for sentence root, co-ordination, and relative clause respectively. For these relations, providing CCG categories gave an increment of 1.2%, 1.4% and 1.6% respectively over the baseline. We also found that the impact of CCG categories is higher when the span of the dependency is longer. Figure 4 shows the F-score o</context>
</contexts>
<marker>Ambati, Husain, Jain, Sharma, Sangal, 2010</marker>
<rawString>Bharat Ram Ambati, Samar Husain, Sambhav Jain, Dipti Misra Sharma, and Rajeev Sangal. 2010. Two Methods to Incorporate ’Local Morphosyntactic’ Features in Hindi Dependency Parsing. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22–30, Los Angeles, CA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharat Ram Ambati</author>
</authors>
<title>Hindi Dependency Parsing and Treebank Validation. Master’s Thesis,</title>
<date>2011</date>
<institution>International Institute of Information Technology - Hyderabad, India.</institution>
<contexts>
<context position="11481" citStr="Ambati, 2011" startWordPosition="1873" endWordPosition="1874">ons using two criteria. The first criterion is correct recovery of the gold dependency tree. Derivations which lead to gold dependencies are given higher weight. In the second criterion, we prefer derivations which yield intra-chunk dependencies (e.g., verb and auxiliary) prior to interchunk (e.g., verb and its arguments). For example, morphological markers (which lead to intrachunk dependencies) play a crucial role in identifying correct dependencies . Resolving these dependencies first helps parsers in better identification of inter-chunk dependencies such as argument structure of the verb (Ambati, 2011). We thus extract the best derivation for each sentence and create a CCGbank for Hindi. Coverage, i.e., number of sentences for which we got at least one complete derivation, using this lexicon is 96%. The remaining 4% are either cases of wrong annotations in the original treebank, or rare constructions which are currently not handled by our conversion algorithm. 2http://openccg.sourceforge.net/ COORD COORD COORD COORD SUBJ DEST SUBJ DEM RELC AUX SUBJ OBJ NP (NP\NP)/NP NP (NP\NP)/NP NP NP (VGF\NP)\NP &gt; &gt; &lt; NP\NP NP\NP VGF\NP &lt; NP VGF ‘Ram , Shyam and Sita went to school.’ NP\NP &gt; VGF 606 3 Exp</context>
</contexts>
<marker>Ambati, 2011</marker>
<rawString>Bharat Ram Ambati. 2011. Hindi Dependency Parsing and Treebank Validation. Master’s Thesis, International Institute of Information Technology - Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Baldridge</author>
</authors>
<title>Lexically Specified Derivational Control in Combinatory Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh, UK.</institution>
<marker>Baldridge, 2002</marker>
<rawString>Jason M. Baldridge. 2002. Lexically Specified Derivational Control in Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Vineet Chaitanya</author>
<author>Rajeev Sangal</author>
</authors>
<title>Natural Language Processing: A Paninian Perspective. Prentice-Hall of India,</title>
<date>1995</date>
<pages>65--106</pages>
<contexts>
<context position="3889" citStr="Bharati et al., 1995" startWordPosition="590" endWordPosition="593">unk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. There are special labels to mark long distance relations like relative clauses, co-ordination etc 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Bharati et al., 1995; Bharati et al., 2009). The treebank contains 12,041 training, 1,233 development and 1,828 testing sentences with an average of 22 words per sentence. We used the CoNLL format1 for our purposes, which contains word, lemma, pos-tag, and coarse pos-tag in the WORD, LEMMA, POS, and CPOS fields respectively and morphological features and chunk information in the FEATS column. 2.2 Algorithm We first made a list of argument and adjunct dependency labels in the treebank. For e.g., dependencies with the label k1 and k2 (corresponding to subject and object respectively) are considered to be arguments,</context>
</contexts>
<marker>Bharati, Chaitanya, Sangal, 1995</marker>
<rawString>Akshar Bharati, Vineet Chaitanya, and Rajeev Sangal. 1995. Natural Language Processing: A Paninian Perspective. Prentice-Hall of India, pages 65–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Rajeev Sangal</author>
<author>Dipti Misra Sharma</author>
<author>Lakshmi Bai</author>
</authors>
<title>AnnCorra: Annotating Corpora Guidelines for POS and Chunk Annotation for Indian Languages. In</title>
<date>2006</date>
<tech>Technical Report (TRLTRC-31), LTRC, IIIT-Hyderabad.</tech>
<contexts>
<context position="3356" citStr="Bharati et al., 2006" startWordPosition="516" endWordPosition="519">Y parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank (HDT ver-0.5) released as part of Coling 2012 Shared Task on parsing (Bharati et al., 2012). HDT is a multi-layered dependency treebank (Bhatt et al., 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. There are special labels to mark long distance relations like relative clauses, co-ordination etc 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Bharati et al., 1995; Bharati et al., 2009). The treebank contains 12,041 training, 1,2</context>
</contexts>
<marker>Bharati, Sangal, Sharma, Bai, 2006</marker>
<rawString>Akshar Bharati, Rajeev Sangal, Dipti Misra Sharma, and Lakshmi Bai. 2006. AnnCorra: Annotating Corpora Guidelines for POS and Chunk Annotation for Indian Languages. In Technical Report (TRLTRC-31), LTRC, IIIT-Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Dipti Misra Sharma</author>
<author>Samar Husain</author>
<author>Lakshmi Bai</author>
<author>Rafiya Begum</author>
<author>Rajeev Sangal</author>
</authors>
<title>AnnCorra: TreeBanks for Indian Languages, Guidelines for Annotating Hindi TreeBank (version</title>
<date>2009</date>
<volume>2</volume>
<pages>2--28</pages>
<contexts>
<context position="3379" citStr="Bharati et al., 2009" startWordPosition="520" endWordPosition="523">CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank (HDT ver-0.5) released as part of Coling 2012 Shared Task on parsing (Bharati et al., 2012). HDT is a multi-layered dependency treebank (Bhatt et al., 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. There are special labels to mark long distance relations like relative clauses, co-ordination etc 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Bharati et al., 1995; Bharati et al., 2009). The treebank contains 12,041 training, 1,233 development and 1,82</context>
</contexts>
<marker>Bharati, Sharma, Husain, Bai, Begum, Sangal, 2009</marker>
<rawString>Akshar Bharati, Dipti Misra Sharma, Samar Husain, Lakshmi Bai, Rafiya Begum, and Rajeev Sangal. 2009. AnnCorra: TreeBanks for Indian Languages, Guidelines for Annotating Hindi TreeBank (version 2.0). http://ltrc.iiit.ac.in/MachineTrans/research/tb/DSguidelines/DS-guidelines-ver2-28-05-09.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Prashanth Mannem</author>
<author>Dipti Misra Sharma</author>
</authors>
<title>Hindi Parsing Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings of Coling Workshop on Machine Translation and Parsing in Indian Languages,</booktitle>
<location>Kharagpur,</location>
<contexts>
<context position="3134" citStr="Bharati et al., 2012" startWordPosition="489" endWordPosition="492">nto phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank (HDT ver-0.5) released as part of Coling 2012 Shared Task on parsing (Bharati et al., 2012). HDT is a multi-layered dependency treebank (Bhatt et al., 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. There are special labels to mark long distance relations like relative clauses, co-ordination etc 604 Proceedings of the 51st Annual Meeting of the Associa</context>
<context position="14839" citStr="Bharati et al., 2012" startWordPosition="2451" endWordPosition="2454">ndi/ Experiments: Features Accuracy Type 1 Type 2 Exp 1: wi, pi 75.14 78.47 Exp 2: Exp 1 + li, ci 77.58 80.17 Exp 3: Exp 2 + fi 80.43 81.88 Exp 4: Exp 3 +wi−1,wi−2, pi−1,pi−2, 82.72 84.15 wi+1, wi+2, pi+1, pi+2 Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29 Exp 6: Exp 5 + wi−2wi−1, wi−1wi, 82.92 84.40 wiwi+1, wi+1wi+2, pi−2pi−1, pi−1pi, pipi+1, pi+1pi+2 Table 1: Impact of different features on the supertagger performance for development data. 3.3 Dependency Parsing There has been a significant amount of work on Hindi dependency parsing in the recent past (Husain, 2009; Husain et al., 2010; Bharati et al., 2012). Out of all these efforts, state-of-the-art accuracy is achieved using the Malt parser. We first run Malt with previous best settings (Bharati et al., 2012) which use the arc-standard parsing algorithm with a liblinear learner, and treat this as our baseline. We compare and analyze results after adding supertags as features with this baseline. 3.4 Using Supertags as Features to Malt C¸akıcı (2009) showed that using gold CCG categories extracted from dependency trees as features to MST parser (McDonald et al., 2006) boosted the performance for Turkish. But using automatic categories from a sup</context>
</contexts>
<marker>Bharati, Mannem, Sharma, 2012</marker>
<rawString>Akshar Bharati, Prashanth Mannem, and Dipti Misra Sharma. 2012. Hindi Parsing Shared Task. In Proceedings of Coling Workshop on Machine Translation and Parsing in Indian Languages, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajesh Bhatt</author>
<author>Bhuvana Narasimhan</author>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Dipti Misra Sharma</author>
<author>Fei Xia</author>
</authors>
<title>A multi-representational and multi-layered treebank for Hindi/Urdu.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop at 47th ACL and 4th IJCNLP,</booktitle>
<pages>186--189</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="1803" citStr="Bhatt et al., 2009" startWordPosition="273" endWordPosition="276">l. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2</context>
<context position="3199" citStr="Bhatt et al., 2009" startWordPosition="499" endWordPosition="502">o Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dependency Treebank (HDT ver-0.5) released as part of Coling 2012 Shared Task on parsing (Bharati et al., 2012). HDT is a multi-layered dependency treebank (Bhatt et al., 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al., 2006; Bharati et al., 2009). Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. There are special labels to mark long distance relations like relative clauses, co-ordination etc 604 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604–609, Sofia, Bulgari</context>
</contexts>
<marker>Bhatt, Narasimhan, Palmer, Rambow, Sharma, Xia, 2009</marker>
<rawString>Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009. A multi-representational and multi-layered treebank for Hindi/Urdu. In Proceedings of the Third Linguistic Annotation Workshop at 47th ACL and 4th IJCNLP, pages 186–189, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Cristina Bosco</author>
<author>Alessandro Mazzei</author>
</authors>
<title>Converting a Dependency Treebank to a Categorial Grammar Treebank for Italian.</title>
<date>2009</date>
<booktitle>Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8),</booktitle>
<pages>27--38</pages>
<editor>In M. Passarotti, Adam Przepi´orkowski, S. Raynaud, and Frank Van Eynde, editors,</editor>
<location>Milan, Italy.</location>
<contexts>
<context position="2426" citStr="Bos et al. (2009)" startWordPosition="374" endWordPosition="377">hen use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 Hindi Dependency Treebank In this paper, we work with a subset of the Hindi Dep</context>
</contexts>
<marker>Bos, Bosco, Mazzei, 2009</marker>
<rawString>Johan Bos, Cristina Bosco, and Alessandro Mazzei. 2009. Converting a Dependency Treebank to a Categorial Grammar Treebank for Italian. In M. Passarotti, Adam Przepi´orkowski, S. Raynaud, and Frank Van Eynde, editors, Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8), pages 27–38, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruken C¸akıcı</author>
</authors>
<title>Automatic induction of a CCG grammar for Turkish.</title>
<date>2005</date>
<booktitle>In Proceedings of Student Research Workshop, 43rd Annual Meeting of the ACL,</booktitle>
<pages>73--78</pages>
<marker>C¸akıcı, 2005</marker>
<rawString>Ruken C¸akıcı. 2005. Automatic induction of a CCG grammar for Turkish. In Proceedings of Student Research Workshop, 43rd Annual Meeting of the ACL, pages 73–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruket C¸akıcı</author>
</authors>
<title>Parser Models for a Highly Inflected Language.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh, UK.</institution>
<marker>C¸akıcı, 2009</marker>
<rawString>Ruket C¸akıcı. 2009. Parser Models for a Highly Inflected Language. Ph.D. thesis, University of Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<contexts>
<context position="12875" citStr="Clark and Curran (2004)" startWordPosition="2101" endWordPosition="2104">supertagger as features to a state-of-the-art Hindi Dependency parser (Malt). We did all our experiments using both gold features (pos, chunk and morphological information) provided in the treebank and automatic features extracted using a Hindi shallow parser3. We report results with automatic features but we also obtained similar improvements with gold features. 3.1 Category Set For supertagging, we first obtained a category set from the CCGbank training data. There are 2,177 and 718 category types in Type 1 (with morph. information) and Type 2 (without morph. information) data respectively. Clark and Curran (2004) showed that using a frequency cutoff can significantly reduce the size of the category set with only a small loss in coverage. We explored different cut-off values and finally used a cutoff of 10 for building the tagger. This reduced the category types to 376 and 202 for Type 1 and Type 2 respectively. The percent of category tokens in development data that don’t appear in the category set entrailed by this cut-off are 1.39 &amp; 0.47 for Type 1 and Type 2 respectively. 3.2 Supertagger Following Clark and Curran (2004), we used a Maximum Entropy approach to build our supertagger. We explored diff</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="2305" citStr="Hockenmaier and Steedman, 2007" startWordPosition="356" endWordPosition="359">dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG </context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Creating a CCGbank and a wide-coverage CCG lexicon for German.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>505--512</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2324" citStr="Hockenmaier, 2006" startWordPosition="360" endWordPosition="361">ows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other </context>
</contexts>
<marker>Hockenmaier, 2006</marker>
<rawString>Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 505–512, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samar Husain</author>
<author>Prashanth Mannem</author>
<author>Bharat Ram Ambati</author>
<author>Phani Gadde</author>
</authors>
<title>The ICON2010 Tools Contest on Indian Language Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ICON-2010 Tools Contest on Indian Language Dependency Parsing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="14816" citStr="Husain et al., 2010" startWordPosition="2447" endWordPosition="2450">iit.ac.in/analyzer/hindi/ Experiments: Features Accuracy Type 1 Type 2 Exp 1: wi, pi 75.14 78.47 Exp 2: Exp 1 + li, ci 77.58 80.17 Exp 3: Exp 2 + fi 80.43 81.88 Exp 4: Exp 3 +wi−1,wi−2, pi−1,pi−2, 82.72 84.15 wi+1, wi+2, pi+1, pi+2 Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29 Exp 6: Exp 5 + wi−2wi−1, wi−1wi, 82.92 84.40 wiwi+1, wi+1wi+2, pi−2pi−1, pi−1pi, pipi+1, pi+1pi+2 Table 1: Impact of different features on the supertagger performance for development data. 3.3 Dependency Parsing There has been a significant amount of work on Hindi dependency parsing in the recent past (Husain, 2009; Husain et al., 2010; Bharati et al., 2012). Out of all these efforts, state-of-the-art accuracy is achieved using the Malt parser. We first run Malt with previous best settings (Bharati et al., 2012) which use the arc-standard parsing algorithm with a liblinear learner, and treat this as our baseline. We compare and analyze results after adding supertags as features with this baseline. 3.4 Using Supertags as Features to Malt C¸akıcı (2009) showed that using gold CCG categories extracted from dependency trees as features to MST parser (McDonald et al., 2006) boosted the performance for Turkish. But using automati</context>
</contexts>
<marker>Husain, Mannem, Ambati, Gadde, 2010</marker>
<rawString>Samar Husain, Prashanth Mannem, Bharat Ram Ambati, and Phani Gadde. 2010. The ICON2010 Tools Contest on Indian Language Dependency Parsing. In Proceedings of ICON-2010 Tools Contest on Indian Language Dependency Parsing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samar Husain</author>
</authors>
<title>Dependency Parsers for Indian Languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing,</booktitle>
<contexts>
<context position="14795" citStr="Husain, 2009" startWordPosition="2444" endWordPosition="2446">3http://ltrc.iiit.ac.in/analyzer/hindi/ Experiments: Features Accuracy Type 1 Type 2 Exp 1: wi, pi 75.14 78.47 Exp 2: Exp 1 + li, ci 77.58 80.17 Exp 3: Exp 2 + fi 80.43 81.88 Exp 4: Exp 3 +wi−1,wi−2, pi−1,pi−2, 82.72 84.15 wi+1, wi+2, pi+1, pi+2 Exp 5: Exp 4 + wipi, wici, wifi, pifi 82.81 84.29 Exp 6: Exp 5 + wi−2wi−1, wi−1wi, 82.92 84.40 wiwi+1, wi+1wi+2, pi−2pi−1, pi−1pi, pipi+1, pi+1pi+2 Table 1: Impact of different features on the supertagger performance for development data. 3.3 Dependency Parsing There has been a significant amount of work on Hindi dependency parsing in the recent past (Husain, 2009; Husain et al., 2010; Bharati et al., 2012). Out of all these efforts, state-of-the-art accuracy is achieved using the Malt parser. We first run Malt with previous best settings (Bharati et al., 2012) which use the arc-standard parsing algorithm with a liblinear learner, and treat this as our baseline. We compare and analyze results after adding supertags as features with this baseline. 3.4 Using Supertags as Features to Malt C¸akıcı (2009) showed that using gold CCG categories extracted from dependency trees as features to MST parser (McDonald et al., 2006) boosted the performance for Turkis</context>
</contexts>
<marker>Husain, 2009</marker>
<rawString>Samar Husain. 2009. Dependency Parsers for Indian Languages. In Proceedings of the ICON09 NLP Tools Contest: Indian Language Dependency Parsing, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Mcdonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning.</booktitle>
<contexts>
<context position="1458" citStr="Mcdonald and Nivre, 2007" startWordPosition="218" endWordPosition="221">is known to have weak rates of recovery. 1 Introduction As compared to English, many Indian languages including Hindi have a freer word order and are also morphologically richer. These characteristics pose challenges to statistical parsers. Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt </context>
<context position="17659" citStr="Mcdonald and Nivre, 2007" startWordPosition="2924" endWordPosition="2927">better than that of Type 1 data (see Table 1). Experiment Development Testing UAS LAS UAS LAS Malt: Baseline 89.09 83.46 88.67 83.04 Malt + Type 1 Gold 95.87* 90.79* 95.27* 90.22* Malt + Type 2 Gold 95.73* 90.70* 95.26* 90.18* Malt + Type 1 ST 89.54* 83.68* 88.93* 83.23* Malt + Type 2 ST 89.90* 83.96* 89.04* 83.35* Table 2: Supertagger impact on Hindi dependency parsing (ST=Supertags). McNemar’s test, * = p &lt; 0.01. It is interesting to notice the impact of using automatic CCG categories from a supertagger on long distance dependencies. It is known that Malt is weak at long-distance relations (Mcdonald and Nivre, 2007; Ambati et al., 2010). Providing CCG categories as features improved handling of long-distance dependencies for Malt. Figure 3 shows the F-score of the impact of CCG categories on three dependency labels, which take the major share of long distance dependencies, namely, ROOT, COORD, and RELC, the labels for sentence root, co-ordination, and relative clause respectively. For these relations, providing CCG categories gave an increment of 1.2%, 1.4% and 1.6% respectively over the baseline. We also found that the impact of CCG categories is higher when the span of the dependency is longer. Figure</context>
</contexts>
<marker>Mcdonald, Nivre, 2007</marker>
<rawString>Ryan Mcdonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>216--220</pages>
<location>New York City, New York.</location>
<contexts>
<context position="15360" citStr="McDonald et al., 2006" startWordPosition="2537" endWordPosition="2540">Hindi dependency parsing in the recent past (Husain, 2009; Husain et al., 2010; Bharati et al., 2012). Out of all these efforts, state-of-the-art accuracy is achieved using the Malt parser. We first run Malt with previous best settings (Bharati et al., 2012) which use the arc-standard parsing algorithm with a liblinear learner, and treat this as our baseline. We compare and analyze results after adding supertags as features with this baseline. 3.4 Using Supertags as Features to Malt C¸akıcı (2009) showed that using gold CCG categories extracted from dependency trees as features to MST parser (McDonald et al., 2006) boosted the performance for Turkish. But using automatic categories from a supertagger radically decreased performance in their case as supertagger accuracy was very low. We have explored different ways of incorporating both gold CCG categories and supertagger-provided CCG categories into dependency parsing. Following C¸akıcı (2009), instead of using supertags for all words, we used supertags which occurred at least K times in the training data, and backed off to coarse POS-tags otherwise. We experimented with different values of K and found that K=15 gave the best results. We first provided </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 216–220, New York City, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1193" citStr="Nivre et al. (2007)" startWordPosition="178" endWordPosition="181">ng a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. 1 Introduction As compared to English, many Indian languages including Hindi have a freer word order and are also morphologically richer. These characteristics pose challenges to statistical parsers. Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1554" citStr="Steedman, 2000" startWordPosition="236" endWordPosition="237">luding Hindi have a freer word order and are also morphologically richer. These characteristics pose challenges to statistical parsers. Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. (2007) (Malt). It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al., 2010). In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treeban</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<title>Chinese CCGbank: extracting CCG derivations from the Penn Chinese Treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1083--1091</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2347" citStr="Tse and Curran, 2010" startWordPosition="362" endWordPosition="365">a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al., 2009) and then use it to create a Hindi CCGbank. In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. Our results show that using CCG categories can help Malt by improving the recovery of long distance relations. 2 A CCG Treebank from a Dependency Treebank There have been some efforts at automatically extracting treebanks of CCG derivations from phrase structure treebanks (Hockenmaier and Steedman, 2007; Hockenmaier, 2006; Tse and Curran, 2010), and CCG lexicons from dependency treebanks (C¸akıcı, 2005). Bos et al. (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). In this work, following C¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. 2.1 </context>
</contexts>
<marker>Tse, Curran, 2010</marker>
<rawString>Daniel Tse and James R. Curran. 2010. Chinese CCGbank: extracting CCG derivations from the Penn Chinese Treebank. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1083–1091, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Shift-Reduce CCG Parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>683--692</pages>
<location>Portland, Oregon, USA,</location>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Shift-Reduce CCG Parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 683–692, Portland, Oregon, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>