<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001290">
<title confidence="0.9662195">
A Character-Based Joint Model
for CIPS-SIGHAN Word Segmentation Bakeoff 2010
</title>
<author confidence="0.995395">
Kun Wang and Chengqing Zong Keh-Yih Su
</author>
<affiliation confidence="0.998094">
National Laboratory of Pattern Recognition Behavior Design Corporation
Institute of Automation, Chinese Academy of Science
</affiliation>
<email confidence="0.994422">
{kunwang,cqzong}@nlpr.ia.ac.cn kysu@bdc.com.tw
</email>
<sectionHeader confidence="0.996611" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863875">
This paper presents a Chinese Word
Segmentation system for the closed track
of CIPS-SIGHAN Word Segmentation
Bakeoff 2010. This system adopts a
character-based joint approach, which
combines a character-based generative
model and a character-based discrimina-
tive model. To further improve the cross-
domain performance, we use an addi-
tional semi-supervised learning proce-
dure to incorporate the unlabeled corpus.
The final performance on the closed
track for the simplified-character text
shows that our system achieves compa-
rable results with other state-of-the-art
systems.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973838709677">
The character-based tagging approach (Xue,
2003) has become the dominant technique for
Chinese word segmentation (CWS) as it can tol-
erate out-of-vocabulary (OOV) words. In the last
few years, this method has been widely adopted
and further improved in many previous works
(Tseng et al., 2005; Zhang et al., 2006; Jiang et
al., 2008). Among various character-based tag-
ging approaches, the character-based joint model
(Wang et al., 2010) achieves a good balance be-
tween in-vocabulary (IV) words recognition and
OOV words identification.
In this work, we adopt the character-based
joint model as our basic system, which combines
a character-based discriminative model and a
character-based generative model. The genera-
tive module holds a robust performance on IV
words, while the discriminative module can
handle the extra features easily and enhance the
OOV words segmentation. However, the per-
formance of out-of-domain text is still not satis-
factory as that of in-domain text, while few pre-
vious works have paid attention to this problem.
To further improve the performance of the ba-
sic system in out-of-domain text, we use a semi-
supervised learning procedure to incorporate the
unlabeled corpora of Literature (Unlabeled-A)
and Computer (Unlabeled-B). The final results
show that our system performs well on all four
testing-sets and achieves comparable segmenta-
tion results with other participants.
</bodyText>
<sectionHeader confidence="0.983329" genericHeader="method">
2 Our system
</sectionHeader>
<subsectionHeader confidence="0.996165">
2.1 Character-Based Joint Model
</subsectionHeader>
<bodyText confidence="0.988161">
The character-based joint model in our system
contains two basic components:
➢ The character-based discriminative model.
➢ The character-based generative model.
The character-based discriminative model
(Xue, 2003) is based on a Maximum Entropy
(ME) framework (Ratnaparkhi, 1998) and can be
formulated as follows:
</bodyText>
<equation confidence="0.99293425">
n
2
P(t1 c1) - fl P(tk tk-1,ck±2) (1)
k=1
</equation>
<bodyText confidence="0.9980364">
Where tk is a member of {Begin, Middle, End,
Single} (abbreviated as B, M, E and S from now
on) to indicate the corresponding position of
character ck in its associated word. For example,
the word “北京市 (Beijing City)” will be as-
signed with the corresponding tags as: “北/B
(North) 京/M (Capital) 市/E (City)”.
This discriminative module can flexibly in-
corporate extra features and it is implemented
with the ME package1 given by Zhang Le. All
training experiments are done with Gaussian
prior 1.0 and 200 iterations.
The character-based generative module is a
character-tag-pair-based trigram model (Wang et
al., 2009) and can be expressed as below:
</bodyText>
<equation confidence="0.997117375">
n
n 1
P c t P c t c t −
([ , ] ) ([ , ] [ , ] ).
i
≈∏
1 i i − 2
1
</equation>
<bodyText confidence="0.999866285714286">
In our experiments, SRI Language Modeling
Toolkit2 (Stolcke, 2002) is used to train the gen-
erative trigram model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998).
The character-based joint model combines the
above discriminative module and the generative
module with log-linear interpolation as follows:
</bodyText>
<equation confidence="0.9984132">
k + 2
+ − ×
(1 ) log( ( , ))
α P t t c
k k− 1 k − 2
</equation>
<bodyText confidence="0.995625166666667">
Where the parameter α (0.0≤ α ≤1.0) is the
weight for the generative model. Score(tk) will
be directly used during searching the best se-
quence. We set an empirical value (α = 0.3) to
this model as there is no development-set for
various domains.
</bodyText>
<subsectionHeader confidence="0.962456">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999918333333333">
In this work, the feature templates adopted in the
character-based discriminative model are very
simple and are listed below:
</bodyText>
<equation confidence="0.985969714285714">
(a) Cn(n = −2,−1,0,12);
( b) C C n = − −
n n+1( 2, 1,0,1);
( ) ;
c C C
− 1 1
(d) T(C−2)T(C−1)T(C0)T(C1)T(C2)
</equation>
<bodyText confidence="0.999341571428571">
In the above templates, Cn represents a char-
acter and the index n indicates the position. For
example, when we consider the third character
“奥” in the sequence “北京奥运会”, template (a)
results in the features as following: C-2=北, C-1=
京, C0=奥, C1=运, C2=会, and template (b) gen-
erates the features as: C-2C-1=北京, C-1C0=京奥,
</bodyText>
<footnote confidence="0.991484">
1 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html
2 http://www.speech.sri.com/projects/srilm/
</footnote>
<bodyText confidence="0.998030347826087">
C0C1=奥运, C1C2=运会, and template (c) gives
the feature C-1C1=京运.
Template (d) is the feature of character type.
Five types classes are defined: dates (“年”, “月”,
“日”, the Chinese character for “year”, “month”
and “day” respectively) represents class 0; for-
eign alphabets represent class 1; Arabic and
Chinese numbers represent class 2; punctuation
represents class 3 and other characters represent
class 4. For example, when we consider the
character “,” in the sequence “八月,阿Q”,
the feature T C
( −2)T(C−1)T(C0)T(C1)T(C2) will
be set to “20341”.
When training the character-based discrimina-
tive module, we convert all the binary features
into real-value features, and set the real-value of
C0 to be 2.0, the value of C-1C0 and C0C1 to be
3.0, and the values of all other features to be 1.0.
This method sounds a little strange because it is
equal to duplicate some features for the maxi-
mum entropy training. However, it effectively
improves the performance in our previous works.
</bodyText>
<subsectionHeader confidence="0.997146">
2.3 Restrictions in constructing lattice
</subsectionHeader>
<bodyText confidence="0.999814125">
As the closed track allows the participants to use
the character type information, we add some re-
strictions to our system when constructing the
character-tag lattice. When we consider a char-
acter in the sequence, the type information of
both the previous and the next character would
be taken into account. The restrictions are list as
follows:
</bodyText>
<listItem confidence="0.9869271">
• If the previous, the current and the next
characters are all English or numbers, we
would fix the current tag to be “M”;
• If the previous and the next characters are
both English or numbers, while the current
character is a connective symbol such as “-”,
“/”, “_”, “\” etc., we would also fix the cur-
rent tag to be “M”;
• Otherwise, all four tags {B, E, M, S} would
be given to the current character.
</listItem>
<bodyText confidence="0.983816">
It is shown that in the Computer domain these
simple restrictions not only greatly reduce the
number of words segmented, but also speed up
the system.
</bodyText>
<equation confidence="0.9434969">
Score
( ) log( ([ , ] [ , ] ))
k − 1
t = ×
α P c t c t
k k k − 2
(2)
i
=
(3)
</equation>
<table confidence="0.9977854">
Domain Mark OOV Rate R P F1 ROOV RIV
Literature A 0.069 0.937 0.937 0.937 0.652 0.958
Computer B 0.152 0.941 0.940 0.940 0.757 0.974
Medicine C 0.110 0.930 0.917 0.923 0.674 0.961
Finance D 0.087 0.957 0.956 0.957 0.813 0.971
</table>
<tableCaption confidence="0.996109">
Table 1: Official segmentation results of our system.
</tableCaption>
<table confidence="0.1763965">
Algorithm 1: Semi-Supervised Learning Ui _1 and Ui: treat Ui_1 as the benchmark, Ui as a
Given:
</table>
<listItem confidence="0.96935825">
• Labeled training corpus: L
• Unlabeled training corpus: U
1: Use L to train a segmenter S0;
2: Use S to segment the unlabeled corpus U
</listItem>
<figure confidence="0.703406333333333">
0
and then get labeled corpus U ;
0
</figure>
<listItem confidence="0.99071">
3: for i= 1 to K do
4: Add Ui_1 to L and get a new corpus Li;
5: Use Li to train a new segmenter Si;
6: Use Si to segment the unlabeled corpus
U and then get labeled corpus Ui;
7: if convergence criterion meets
8: break
8: end for
Output: the last segmenter SK
</listItem>
<subsectionHeader confidence="0.982882">
2.4 Semi-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.999534148148148">
In the last decade, Chinese word segmentation
has been improved significantly and gets a high
precision rate in performance. However, the per-
formance for out-of-domain text is still unsatis-
factory at the present. Also, few works have paid
attention to the cross-domain problem in Chi-
nese word segmentation task so far.
Self-training and Co-training are two simple
semi-supervised learning methods to incorporate
unlabeled corpus (Zhu, 2006). In this work, we
use an iterative self-training method to incorpo-
rate the unlabeled data. A segmenter is first
trained with the labeled corpus. Then this seg-
menter is used to segment the unlabeled data.
Then the predicted data is added to the original
training corpus as a new training-set. The seg-
menter will be re-trained and the procedure re-
peated. To simplify the task, we fix the weight
α = 0.3 for the generative module of our joint
model in the training iterations. The procedure is
shown in Algorithm 1. The iterations will not be
ended until the similarity of two segmentation
results Ui_1 and Ui reach a certain level. Here we
used F-score to measure the similarity between
testing-set. From our observation, this method
converges quickly in only 3 or 4 iterations for
both Literature and Computer corpora.
</bodyText>
<sectionHeader confidence="0.995367" genericHeader="evaluation">
3 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.684299">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.999983">
In this CIPS-SIGHAN bakeoff, we only partici-
pate the closed track for simplified-character text.
There are two kinds of training corpora:
</bodyText>
<listItem confidence="0.998841">
• Labeled corpus from News Domain
• Unlabeled corpora from Literature Do-
main (Unlabeled-A) and Computer Do-
main (Unlabeled-B).
</listItem>
<bodyText confidence="0.930118166666667">
Also, the testing corpus covers four domains:
Literature (Testing-A), Computer (Testing-B),
Medicine (Testing-C) and Finance (Testing-D).
As there are only two unlabeled corpora for
Domain A and B, we thus adopt different strate-
gies for each testing-set:
</bodyText>
<listItem confidence="0.93832325">
• Testing-A: Character-Based Joint Model
with semi-supervised learning, training
on Labeled corpus and Unlabeled-A;
• Testing-B: Character-Based Joint Model
with semi-supervised learning, training
on Labeled corpus and Unlabeled-B;
• Testing-C and D: Character-Based Joint
Model, training on Labeled corpus;
</listItem>
<bodyText confidence="0.985449">
Table 1 shows that our system achieves F-
scores for various testing-sets: 0.937 (A), 0.940
(B), 0.923 (C) and 0.957 (D), which are compa-
rable with other systems. Among those four test-
ing domains, our system performs unsatisfactor-
ily on Testing-C (Medicine) even the OOV rate
of this domain is not the highest. There are pos-
sible reasons for this result: (1) Semi-supervised
learning is not conducted for this domain; (2) the
statistical property between News and Medicine
are significantly different.
</bodyText>
<table confidence="0.999539615384615">
Domain Model F1 ROOV
A J + R + S 0.937 0.652
J + S 0.937 0.646
J + R 0.936 0.646
J 0.936 0.642
B J + R + S 0.940 0.757
J + S 0.931 0.721
J + R 0.938 0.744
J 0.927 0.699
C J + R 0.923 0.674
J 0.923 0.674
D J + R 0.957 0.813
J 0.954 0.786
</table>
<tableCaption confidence="0.937413">
Table 2: Performance of various approaches
J: Baseline, the character-based joint model
</tableCaption>
<reference confidence="0.222146">
R: Adding restrictions in constructing lattice
S: Conduct Semi-Supervised Learning
</reference>
<subsectionHeader confidence="0.908689">
3.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999993125">
The aim of restrictions in constructing lattice is
to improve the performance of English and nu-
merical expressions, both of which appear fre-
quently in Computer and Finance domain.
Therefore, the improvements gained from these
restrictions are significantly in these two do-
mains (as shown in Table 2).
Besides, the adopted semi-supervised learning
procedure improves the performance in Domain
A and B., but the improvement is not significant.
Semi-supervised learning aims to incorporate
large amounts of unlabeled data. However, the
size of unlabeled corpora provided here is too
small. The semi-supervised learning procedure is
expected to be more effective if a large amount
of unlabeled data is available.
</bodyText>
<sectionHeader confidence="0.999154" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.997266777777778">
Our system is based on a character-based joint
model, which combines a generative module and
a discriminative module. In addition, we applied
a semi-supervised learning method to the base-
line approach to incorporate the unlabeled cor-
pus. Our system achieves comparable perform-
ance with other participants. However, cross-
domain performance is still not satisfactory and
further study is needed.
</bodyText>
<sectionHeader confidence="0.97561" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.998689125">
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053, 90820303 and 60736014,
the National Key Technology R&amp;D Program
under Grant No. 2006BAH03B02, and also the
Hi-Tech Research and Development Program
(“863” Program) of China under Grant No.
2006AA010108-4 as well.
</bodyText>
<sectionHeader confidence="0.998837" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975">
Stanley F. Chen and Joshua Goodman, 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University Center for Research in Computing
Technology.
Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu,
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging.
In Proceedings of ACL, pages 897-904.
Adwait Ratnaparkhi, 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Uni-
versity of Pennsylvania.
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Proc-
essing, pages 311-318.
Huihsin Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky and Christopher Manning, 2005. A
Conditional Random Field Word Segmenter for
Sighan Bakeoff 2005. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 168-171.
Kun Wang, Chengqing Zong and Keh-Yih Su, 2009.
Which is more suitable for Chinese word segmen-
tation, the generative model or the discriminative
one? In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC23), pages 827-834.
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. To appear in COLING 2010.
Nianwen Xue, 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, 8 (1). pages 29-48.
Ruiqiang Zhang, Genichiro Kikui and Eiichiro
Sumita, 2006. Subword-based Tagging for Confi-
dence-dependent Chinese Word Segmentation. In
Proceedings of the COLING/ACL, pages 961-968.
Xiaojin Zhu, 2006. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.428068">
<title confidence="0.935253">A Character-Based Joint for CIPS-SIGHAN Word Segmentation Bakeoff 2010</title>
<author confidence="0.998263">Wang Zong Keh-Yih Su</author>
<affiliation confidence="0.998615">National Laboratory of Pattern Recognition Behavior Design Institute of Automation, Chinese Academy of Science</affiliation>
<email confidence="0.506794">kunwang@nlpr.ia.ac.cnkysu@bdc.com.tw</email>
<email confidence="0.506794">cqzong@nlpr.ia.ac.cnkysu@bdc.com.tw</email>
<abstract confidence="0.996801529411765">This paper presents a Chinese Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>R</author>
</authors>
<title>Adding restrictions in constructing lattice S: Conduct Semi-Supervised Learning</title>
<marker>R, </marker>
<rawString>R: Adding restrictions in constructing lattice S: Conduct Semi-Supervised Learning</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="3569" citStr="Chen and Goodman, 1998" startWordPosition="551" endWordPosition="554"> as: “北/B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le. All training experiments are done with Gaussian prior 1.0 and 200 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: n n 1 P c t P c t c t − ([ , ] ) ([ , ] [ , ] ). i ≈∏ 1 i i − 2 1 In our experiments, SRI Language Modeling Toolkit2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: k + 2 + − × (1 ) log( ( , )) α P t t c k k− 1 k − 2 Where the parameter α (0.0≤ α ≤1.0) is the weight for the generative model. Score(tk) will be directly used during searching the best sequence. We set an empirical value (α = 0.3) to this model as there is no development-set for various domains. 2.2 Features In this work, the feature templates adopted in the character-based discriminative model are very simple and are listed below: (a) Cn(n = −2,−1,0,1</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman, 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="1216" citStr="Jiang et al., 2008" startWordPosition="168" endWordPosition="171">in performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and enhance the OOV words segmentation. However, the performance of out-of-domain text is still </context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu, 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL, pages 897-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2607" citStr="Ratnaparkhi, 1998" startWordPosition="374" endWordPosition="375">main text, we use a semisupervised learning procedure to incorporate the unlabeled corpora of Literature (Unlabeled-A) and Computer (Unlabeled-B). The final results show that our system performs well on all four testing-sets and achieves comparable segmentation results with other participants. 2 Our system 2.1 Character-Based Joint Model The character-based joint model in our system contains two basic components: ➢ The character-based discriminative model. ➢ The character-based generative model. The character-based discriminative model (Xue, 2003) is based on a Maximum Entropy (ME) framework (Ratnaparkhi, 1998) and can be formulated as follows: n 2 P(t1 c1) - fl P(tk tk-1,ck±2) (1) k=1 Where tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word. For example, the word “北京市 (Beijing City)” will be assigned with the corresponding tags as: “北/B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le. All training experiments are done with Gaussian prior 1.0 and 200 iterations. The chara</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi, 1998. Maximum entropy models for natural language ambiguity resolution. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="3463" citStr="Stolcke, 2002" startWordPosition="536" endWordPosition="537">ted word. For example, the word “北京市 (Beijing City)” will be assigned with the corresponding tags as: “北/B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le. All training experiments are done with Gaussian prior 1.0 and 200 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: n n 1 P c t P c t c t − ([ , ] ) ([ , ] [ , ] ). i ≈∏ 1 i i − 2 1 In our experiments, SRI Language Modeling Toolkit2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: k + 2 + − × (1 ) log( ( , )) α P t t c k k− 1 k − 2 Where the parameter α (0.0≤ α ≤1.0) is the weight for the generative model. Score(tk) will be directly used during searching the best sequence. We set an empirical value (α = 0.3) to this model as there is no development-set for various domains. 2.2 Features In this work, the feature templates adop</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke, 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A Conditional Random Field Word Segmenter for Sighan Bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="1175" citStr="Tseng et al., 2005" startWordPosition="160" endWordPosition="163"> model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and enhance the OOV words segmentation. However, the pe</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning, 2005. A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>Which is more suitable for Chinese word segmentation, the generative model or the discriminative one?</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC23),</booktitle>
<pages>827--834</pages>
<contexts>
<context position="3299" citStr="Wang et al., 2009" startWordPosition="488" endWordPosition="491">=1 Where tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word. For example, the word “北京市 (Beijing City)” will be assigned with the corresponding tags as: “北/B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le. All training experiments are done with Gaussian prior 1.0 and 200 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: n n 1 P c t P c t c t − ([ , ] ) ([ , ] [ , ] ). i ≈∏ 1 i i − 2 1 In our experiments, SRI Language Modeling Toolkit2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: k + 2 + − × (1 ) log( ( , )) α P t t c k k− 1 k − 2 Where the parameter α (0.0≤ α ≤1.0) is the weight for the generative model. Score(tk) will be directly used during searching the best s</context>
</contexts>
<marker>Wang, Zong, Su, 2009</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2009. Which is more suitable for Chinese word segmentation, the generative model or the discriminative one? In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC23), pages 827-834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
</authors>
<title>Chengqing Zong and Keh-Yih Su,</title>
<date>2010</date>
<note>To appear in COLING</note>
<marker>Wang, 2010</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. A Character-Based Joint Model for Chinese Word Segmentation. To appear in COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<contexts>
<context position="932" citStr="Xue, 2003" startWordPosition="122" endWordPosition="123">se Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based disc</context>
<context position="2542" citStr="Xue, 2003" startWordPosition="364" endWordPosition="365"> improve the performance of the basic system in out-of-domain text, we use a semisupervised learning procedure to incorporate the unlabeled corpora of Literature (Unlabeled-A) and Computer (Unlabeled-B). The final results show that our system performs well on all four testing-sets and achieves comparable segmentation results with other participants. 2 Our system 2.1 Character-Based Joint Model The character-based joint model in our system contains two basic components: ➢ The character-based discriminative model. ➢ The character-based generative model. The character-based discriminative model (Xue, 2003) is based on a Maximum Entropy (ME) framework (Ratnaparkhi, 1998) and can be formulated as follows: n 2 P(t1 c1) - fl P(tk tk-1,ck±2) (1) k=1 Where tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word. For example, the word “北京市 (Beijing City)” will be assigned with the corresponding tags as: “北/B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package1 given by Zhang Le. All training experimen</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue, 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, 8 (1). pages 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
</authors>
<title>Genichiro Kikui and Eiichiro Sumita,</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL,</booktitle>
<pages>961--968</pages>
<marker>Zhang, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita, 2006. Subword-based Tagging for Confidence-dependent Chinese Word Segmentation. In Proceedings of the COLING/ACL, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2006</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<contexts>
<context position="7956" citStr="Zhu, 2006" startWordPosition="1328" endWordPosition="1329">t the unlabeled corpus U and then get labeled corpus Ui; 7: if convergence criterion meets 8: break 8: end for Output: the last segmenter SK 2.4 Semi-Supervised Learning In the last decade, Chinese word segmentation has been improved significantly and gets a high precision rate in performance. However, the performance for out-of-domain text is still unsatisfactory at the present. Also, few works have paid attention to the cross-domain problem in Chinese word segmentation task so far. Self-training and Co-training are two simple semi-supervised learning methods to incorporate unlabeled corpus (Zhu, 2006). In this work, we use an iterative self-training method to incorporate the unlabeled data. A segmenter is first trained with the labeled corpus. Then this segmenter is used to segment the unlabeled data. Then the predicted data is added to the original training corpus as a new training-set. The segmenter will be re-trained and the procedure repeated. To simplify the task, we fix the weight α = 0.3 for the generative module of our joint model in the training iterations. The procedure is shown in Algorithm 1. The iterations will not be ended until the similarity of two segmentation results Ui_1</context>
</contexts>
<marker>Zhu, 2006</marker>
<rawString>Xiaojin Zhu, 2006. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>