<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.70635">
WSD and Closed Semantic Constraint
</title>
<author confidence="0.99097">
Jiangsheng Yu∗
</author>
<affiliation confidence="0.9924795">
Institute of Computational Linguistics
Peking University, Beijing, China, 100871
</affiliation>
<bodyText confidence="0.944307181818182">
Abstract The application-driven construction
of lexicon has been emphasized as a methodology
of Computational Lexicology recently. We focus on
the closed semantic constraint of the argument(s)
of any verb concept by the noun concepts in a
WordNet-like lexicon, which theoretically is related
to Word Sense Disambiguation (WSD) at differ-
ent levels. From the viewpoint of Dynamic Lexi-
con, WSD provides a way of automatic construc-
tion for the closed semantic constraints and also
benefits from the semantic descriptions.
</bodyText>
<keyword confidence="0.7412475">
Keywords dynamic lexicon, evolution, WSD,
WordNet-like lexicon, closed semantic constraint
</keyword>
<sectionHeader confidence="0.996249" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932703703704">
As the underlying resource of semantic analysis,
the most important descriptions in a semantic lexi-
con are the relationships between verbs and nouns,
which usually comes down to the closed semantic
constraint of each argument of any verb. Once the
structure of the semantic lexicon is determined,
the closed semantic constraint becomes a well-
defined problem.
Example 1.1 The verb dˇa has many mean-
ings in Chinese, which differ in dˇa h´aizi (pun-
ish the child), dˇa m´aoy¯i (weaver the sweater),
dˇa ji`angy´ou (buy the soy), etc. Actually, the se-
mantics of dˇa is distinguished by the semantics of
its arguments.
Different from the traditional lexicon, we advo-
cate the conception of dynamic lexicon ([15]) and
its evolution oriented to some particular applica-
tion, which will be mentioned in Section 2. The
WordNet-like lexicon is treated as a dynamic one,
which means that the structures representing se-
mantic knowledge could be changed according to
some empirical standards. In the next section,
we’ll define the WSD based on the WordNet-like
lexicon, and then discuss the training of concept
TagSet and the statistical model of WSD. By the
statistical WSD, in Section 4, we introduce an ap-
proach to the automatic construction of the closed
</bodyText>
<note confidence="0.507468333333333">
∗This paper is supported by National Foundation of Nat-
ural Science (Research on Chinese Information Extraction),
No. 69483003 and Project 985 in Peking University.
</note>
<bodyText confidence="0.980817">
semantic constraints in a WordNet-like lexicon.
The last section is the conclusion.
</bodyText>
<sectionHeader confidence="0.83123" genericHeader="method">
2 Dynamic Lexicon and Its
Structural Evolution
</sectionHeader>
<construct confidence="0.6817265">
Definition 2.1 A dynamic lexicon is a triple
Lex = (S, R, T) in which
</construct>
<listItem confidence="0.97878375">
1. S is a well-structured set with a types t,
2. R is the set of deductive rules on S, and
3. T is the set of all structural transformations
of S, keeping the type t.
</listItem>
<bodyText confidence="0.888609666666667">
Definition 2.2 Lexicon (S&apos;,R,T) is called the
evolution result of the lexicon (S, R, T) if Elt E T*
such that S t� S&apos; (or briefly S S&apos;). The pro-
cess of a dynamic lexicon to its evolution result is
called an evolution. Obviously, T is a group with
the operation of composition.
Definition 2.3 (S, R, T) is called simple struc-
tured if T is a commutative group, otherwise com-
plex structured.
The more complex is the structure of S, the more
difficult are the applications of R and T. Since
some part of semantic knowledge is represented
by the structure, the complexity balance between
the structure and R (or T) is one of the serious
problems in Computational Lexicology.
</bodyText>
<construct confidence="0.6427265">
Definition 2.4 Let Q(S) denote the least number
of operations constructing S, and Q(S S&apos;) the
least number of operations from S to S&apos;. It’s easy
to verify that
Theorem 2.1 Q(·) is a distance, i.e., it satisfies
that VS, S&apos;, S&apos;&apos;,
</construct>
<listItem confidence="0.99802925">
1. Q(S S&apos;) &gt; 0
2. Q(S S&apos;) = 0 S S = S&apos;
3. Q(S S&apos;) = Q(S&apos; S)
4. Q(S S&apos;&apos;) &lt; Q(S S&apos;) + Q(S&apos; S&apos;&apos;)
</listItem>
<equation confidence="0.525204">
&apos;For instance, labeled tree or complete lattice.
Corollary 2.1 Q(S&apos;) ≤ Q(S) + Q(S S&apos;) 2. some accessorial relations between the noun
(or verb) concepts, and
</equation>
<bodyText confidence="0.6283835">
Definition 2.5 The degree of structural destruc-
tion from S to S&apos;, is defined by
</bodyText>
<equation confidence="0.98478925">
&apos; Q(S&apos;)
ρ(S S) = 1 − Q(S) + Q(S S&apos;) (1)
Property 2.1 0 ≤ ρ(S S&apos;) ≤ 1
Definition 2.6 Let S S1 · · · Sn · · ·
</equation>
<bodyText confidence="0.981071533333333">
be a sequence of evolution, the sequence is called
convergent if there exists a constant A s.t. 0 ≤
A ≤ 1 and lim ρ(S Sn) = A.
n—co
It’s easy to see that a local evolution of the lex-
icon may not be an optimization even for a spe-
cific application. The index ρ indicates the con-
vergence of lexical structure, guaranteeing a stable
machine learning of the dynamic lexicon. Actually,
the structure of the so-called common knowledge
is nothing but a statistical distribution, which is
effected by the cultures and personal experiences.
Oriented to a particular application, such as IE,
IR, MT, etc, the appropriate semantic descriptions
in a WordNet-like lexicon seem necessary.
</bodyText>
<equation confidence="0.670415">
Example 2.1 C = {earthquake, quake, tem-
</equation>
<bodyText confidence="0.824963666666667">
blor, seism} is not only a kind of C&apos; =
{geological phenomenon}, but also a kind of C&apos;&apos; =
{natural disaster}.
</bodyText>
<sectionHeader confidence="0.7018265" genericHeader="method">
3 WSD based on WordNet-
like Lexicon
</sectionHeader>
<bodyText confidence="0.979358711538461">
What does it mean that a machine could under-
stand a given sentence S or a text T? As we
know, Turing Test of NLU includes at least the
meaning of any word w in S or T. Thus, the pre-
requisite WSD is to tag the semantic information
of w automatically. WordNet2 in Princeton Uni-
versity, in despite of its disputed quality, provides
an approach to the formalization of concepts in
natural language, in which a concept is defined by
a synonym set (SynSet). A more important work
in WordNet is the construction of a well-structured
concept network based on the hypernymy relation
(the main framework) and other accessorial rela-
tions, such as, the opposite relation, the holonymy
relation, entailment, cause, etc.
Definition 3.1 A WordNet-like lexicon is a dy-
namic lexicon with the type of WordNet:
1. restricted to each category, S is a labeled tree
from the viewpoint of the hypernymy relation
for both noun concepts and verb concepts,
&apos;The specification of WordNet could be found in [3], [4],
[5], [9], [10], [11], etc.
3. closed semantic constraint of the argument(s)
of each verb concept from the noun concepts.
The WordNet-like lexicon is complex structured, it
may not have the same ontology of WordNet, nei-
ther the semantic knowledge representations. But
the description method seems a general format for
all languages from the fact of EuroWordNet (see
[12]), Chinese Concept Dictionary (CCD, see [7],
[13], [14] and [15]), Korean WordNet, Tamil Word-
Net, etc.
Definition 3.2 Let Σ be the set of all words,
then Γ, the set of all concepts (or SynSets) in a
WordNet-like lexicon, is a subset of 2Σ. The set
of all SynSets containing w is denoted by Δ(w), in
which each element is called a sense of w.
Definition 3.3 Given a well-defined sentence
S = w1w2 · · · wn, WSD is the computable pro-
cessing which tags wi a unique sense si =
{wi, wi1, · · · , wil,} such that each derived combi-
natorial path is a well-defined sentence with the
semantics of S. The Principle of Substitution pro-
vides a corpus-based empirical approach to test
a SynSet well-defined or not. The SynSet is the
smallest unit in a WordNet-like lexicon, which is
the underlying of the structural descriptions be-
tween the concepts.
The training of concept TagSet and the statistical
model of WSD are interactional, which is the main
idea of our approach to WSD based on a WordNet-
like lexicon.
</bodyText>
<subsectionHeader confidence="0.999004">
3.1 The Training of TagSet
</subsectionHeader>
<bodyText confidence="0.997755875">
The traditional semantic tags are from some on-
tology, the apriority of which is often criticized
by computational linguists. For us, the empiri-
cal method must impenetrate each step of WSD
because of the complexity of language knowledge.
The statistical approach to WSD needs a well
concept-tagged corpus as the training set for the
concept TagSet and the statistical data in the Hid-
den Markov Model (HMM). To avoid the sparse
data problem, only a few real subsets of Γ could
act as the TagSet in the statistical model (see [15]
and [16]). The first step leads to a set of structured
TagSets {T1, T2, · · · , Tm}, then the second step is
to choose the most efficient one which makes the
best accuracy of the statistical concept tagging.
Different from those unframed tags, the deductive
rule along the hypernymy trees works out the sense
of w by the following property:
Property 3.1 Suppose that the TagSet is T =
{C1, C2, · · · , Ck}, and the word w in a given sen-
tence is tagged by Ci, then the sense of w here
is the SynSet C which satisfies that Ci � C and
w ∈ C, where � is the partial ordering of the nodes
in the hypernymy tree.
</bodyText>
<subsectionHeader confidence="0.993505">
3.2 Statistical Model of WSD
</subsectionHeader>
<bodyText confidence="0.9999852">
In some sense, WSD is the kernel problem of both
NLU and NLP ([1], [6], [8]). POS and concept tag
are two random variables in the HMM of WSD.
Sometimes POS of w determines its sense, some-
times not. But in most cases, a sense of w implies
a unique POS. The distribution of w’s senses with
the POS, P, is important in the (POS, concept)-
tagging. A Hidden Markov Model with two pa-
rameters will be adopted as the main statistical
model for WSD, and the Statistical Decision The-
ory and Bayesian Analysis, which are good at an-
alyzing the small samples, conducted as a com-
parison. The training corpus, T, is done by hand,
where the cursor sensitive display of the senses pro-
vides the help information.
</bodyText>
<equation confidence="0.941233166666667">
Definition 3.4 Consider the well-defined sen-
tence S = w1w2 · · · wn. By the lexicon, let
S = w1/P (i)
1 w2/P(i)
2 · · · wn/P (i)
n be a possible POS
tagged result, where i ∈ I. Define
P(C(i,j)
1 ··· C(i,j) n|P(i)
1 ··· P(i)
n )
P(C(i,j)
1 ··· C(i,j) n ,P (i)
1 ··· P(i)
n )
P(C(i,j)
1 ··· C(i,j)
n )
</equation>
<bodyText confidence="0.998508">
The HMM of concept can simulate the HMM with
two parameters of (POS, concept). f(i) in (2) is
predigested to
</bodyText>
<equation confidence="0.950077">
P(Cki,j)|Cki,1 ) (3)
</equation>
<bodyText confidence="0.9493325">
Property 3.2 There exists a unique map g from
the set of {P (i)
</bodyText>
<equation confidence="0.919304615384615">
1 P(i)
2 · · · P(i)
n |i ∈ I} to the set of
{C(i,j)
1 C(i,j) 2· · · C(i,j)
n |(i, j) ∈ I×J}, which satisfies
that
g(P(i)
1 ··· P(i)
n ) = C(i,f(i)) ··· C(i,f(i)) (4)
1 n
where ∀i, k, ∃C ∈ Δ(wk) s.t. C(i,f(i)) � C. If
k
</equation>
<bodyText confidence="0.893993">
there is C&apos; =6 C satisfying C&apos; ∈ Δ(wk) and
Cki,f(i)) �_ C&apos;, then the one with more distribu-
tion is the selected sense of wk.
Property 3.3 Let s = w1w2 · · · wn be any pos-
sible segmented sequence of S, corresponding
a set of probabilities of POS sequences As =
</bodyText>
<equation confidence="0.928356538461539">
{P(P (i)
1 P(i)
2 ···P(i)
n )|i ∈ I}. Each P(i)
1 P(i)
2 · · · P(i)
n
corresponds a set of probabilities of concept se-
quences B(i)
s = {P(C(i,j)
1 C(i,j)
2 · · ·C(i,j)
n )|j ∈ J},
</equation>
<bodyText confidence="0.935645">
where C(i,j) khas the POS of P(i)
</bodyText>
<equation confidence="0.90756025">
k , then
(As) + b · max(B(i)
s )) (5)
i,s
</equation>
<bodyText confidence="0.8284685">
is the choice of segmentation, where a &gt; 0, b &gt; 0
and a + b = 1. More precisely, (5) is rewritten by
</bodyText>
<equation confidence="0.957979888888889">
{max{a · P(P(i)
s ) + b · P(g(P(i)
s ))}} (6)
i
where P(i)
s = P(i)
1 P(i)
2 ··· P(i)
n.
</equation>
<sectionHeader confidence="0.6950365" genericHeader="method">
4 WSD driven Closed Seman-
tic Constraint
</sectionHeader>
<bodyText confidence="0.99849175">
From the corpus and the statistical WSD, we can
make an induction of the arguments along the hy-
pernymy tree, which leads to the closed semantic
constraints automatically. At the same time, the
closed semantic constraints also provide a possible
approach to the empirical optimization of ΓN and
ΓV . While the total optimization of a WordNet-
like lexicon is still an open problem.
</bodyText>
<subsectionHeader confidence="0.967989">
4.1 Similarity between Concepts
</subsectionHeader>
<bodyText confidence="0.590462">
Definition 4.1 A labeled tree is a 5-tuple T =
hN, Q, D, P, Li satisfying that:
</bodyText>
<listItem confidence="0.997701333333333">
1. N is a finite set of nodes
2. Q is a finite set of labels
3. D is a partial ordering on N, called domi-
nance relation
4. P is a strict partial ordering on N, called
precedence relation
5. (∃x ∈ N)(∀y ∈ N)[(x, y) ∈ D]
6. (∀x, y ∈ N)[[(x, y) ∈ P ∨ (y, x) ∈ P] ↔
[(x, y) ∈/ D ∧ (y, x) ∈/ D]]
7. (∀x, y, z, w ∈ N)[[(w, x) ∈ P ∧ (w, y) ∈ D ∧
(x, z) ∈ D] → (y, z) ∈ P]
8. L : N → Q is a label map
</listItem>
<bodyText confidence="0.991155">
Definition 4.2 A hypernymy tree is a labeled
tree, in which the label map is one-to-one. Al-
ways, we presume that the hypernymy tree is not
degenerative.
In a hypernymy tree of a WordNet-like lexi-
con, a node is a code and a label is a SynSet.
Since the label map is injective, without gen-
erality, a SynSet is usually denoted by a node.
We assume that the precedence relation between
</bodyText>
<equation confidence="0.946139928571429">
f(i) =argmax
j∈J
= argmax
j∈J
= argmax
j∈J
(2)
f(i) = argmax P(C(i,j) n
jEJ 1 ) H
k=2
argmax (a · max
s s
argmax
s
</equation>
<bodyText confidence="0.715042333333333">
the brother nodes always implies an ordering of
time, usage, frequency, mood, etc. For instance,
{spring, springtime} ≺ {summer, summertime} ≺
{fall, autumn} ≺ {winter, wintertime} as the hy-
ponyms of {season, time of year}.
Definition 4.3 Let f, b and B denote father, the
nearest younger-brother and the nearest elder-
brother respectively, satisfying that f = fb, f =
fB and Bb = bB = 1.
Definition 4.4 ∀x, y ∈ N, let z ∈ N be their
nearest ancestor satisfying z = fm(x) and z =
fn(y), D(x, y) def = m + n. k ∈ N is called the
offset of x from its eldest brother if ∃Bk(x) and
�Bk+1(x). Let the offset of y is l, the similarity
between x and y is:
</bodyText>
<listItem confidence="0.6884122">
• If mn = 1, S(x, y) def = h0, |k − l|i
• If mn =6 1, S(x, y) def = hm + n, 0i
Definition 4.5 Suppose that S(x1, y1) = ha1, b1i
and S(x2, y2) = ha2, b2i, the comparison of simi-
larities is defined as follows:
1. a1 = a2
• S(x1, y1) _ S(x2, y2) H b1 &lt; b2
2. a1 =� a2
• If a1 &lt; a2, then S(x1, y1) S(x2, y2)
• If a1 &gt; a2, then S(x2, y2) S(x1, y1)
</listItem>
<bodyText confidence="0.9891485">
Theorem 4.1 h{S(x, y)|x, y ∈ N}, --&lt;i is a totally
ordered set.
The elementary structural transformations in a
WordNet-like lexicon include:
</bodyText>
<listItem confidence="0.9988444">
1. insert a non-root brother-node;
2. collapse a non-root node to its father-node;
3. root is adding a new root;
4. add a link between two labeled trees;
5. delete a link between two labeled trees.
</listItem>
<subsectionHeader confidence="0.996028">
4.2 Induction of Constraints
</subsectionHeader>
<bodyText confidence="0.956011823529412">
PN (or PV ) denotes the set of noun (or verb) con-
cepts. Let C ∈ PV be a verb concept with one
argument. Suppose that we have gotten the ini-
tial closed semantic constraint of its argument,
C&apos; ∈ PN, from a concept-tagged sentence. A
link from C&apos; to C is added between PN and PV .
If C&apos;&apos; from another sentence is also a close se-
mantic constraint of C’s argument, then the in-
fimum of C&apos; and C&apos;&apos;, inf(C&apos;, C&apos;&apos;), is the new C&apos;.
∀x ∈ P, C&apos; --&lt; x, if the substitution from C to x still
induces well-formed sentences, then the induction
succeeds. Otherwise, the disjointed union C&apos; ⊕ C&apos;&apos;
is the closed semantic constraint.
Definition 4.6 The induction of the closed
semantic constraints of C, D ∈ P is defined by
inf(C, D) if ∀x[inf(C, D) --&lt; x]
succeeds in the substitution
</bodyText>
<equation confidence="0.7609744">
C ⊕ D otherwise
Definition 4.7 By Theorem 4.1, the induction
between C ⊕ D and E ∈ P is defined by
(C ® D) n E = J (C n E) ® D if S(C, E) -&lt; S(D, E)
l C ® (D n E) otherwise
</equation>
<bodyText confidence="0.999127375">
Theoretically, if C1 ⊕C2 ⊕· · ·⊕Cn is the closed se-
mantic constraint of the argument of C ∈ PV , then
∀i, ∀x[Ci --&lt; x] succeeds in the substitution. Thus,
in the WordNet-like lexicon, there are n links from
PN to PV for C, where n is called the length of the
constraint. The approach to the closed semantic
constraints of the verb concepts with two argu-
ments is similar.
</bodyText>
<subsectionHeader confidence="0.99981">
4.3 Clustering of Constraints
</subsectionHeader>
<bodyText confidence="0.960312">
Definition 4.8 Suppose that there are N argu-
ments for all verb concepts and the length of the
</bodyText>
<equation confidence="0.932717333333333">
N
i-th constraint is li, then l¯ = li/N is called the
i=1
</equation>
<bodyText confidence="0.876200545454546">
average length of the constraints.
l indicates the rationality of the concept classifi-
cation in a WordNet-like lexicon, which also acts
as an index of the evolution. Our presupposition
is that the optimization of the lexicon must have
the least average length of the constraints. The
clustering of noun concepts constrained by the ar-
guments of the verb concepts should be a standard
of the classification of PN.
Definition 4.9 S ­� S1 ­� · · · ­� Sn ­� · · · is
Cauchy sequence of evolution iff ∀e &gt; 0, ∃N ∈
</bodyText>
<equation confidence="0.777765">
N, ∀i, j &gt; N,ρ(Si Sj) &lt; e.
</equation>
<bodyText confidence="0.9075464">
Theorem 4.2 The Cauchy sequence of evolution
is convergent. And ∀e &gt; 0, ∃i, j ∈ N s.t. |¯l(Si) −
¯l(Sj) |&lt; e.
PN is structured by not only the hypernymy rela-
tion but also the closed semantic constraints. Of
course, the hypernymy relation in PN is princi-
pal, but not necessarily unique. As described in
Example 2.1, the distinct angles of view provide
enough space for the evolution. By the hypernymy
relation in PV , we have
</bodyText>
<reference confidence="0.8925476">
Property 4.1 ∀C, C&apos; ∈ PV , C --&lt; C&apos;, if the closed
semantic constraint of C&apos; is C1 ⊕ C2 ⊕ · · · ⊕ Cn,
then ∃Cn+1, · · · , Cm ∈ PN such that (((C1 ⊕ C2 ⊕
· · ·⊕Cn)uCn+1)u· · ·uCm) is the closed semantic
constraint of C.
</reference>
<figure confidence="0.85997025">
⎧
⎨
⎩
CuD =
</figure>
<bodyText confidence="0.999936416666667">
This property provides an approach to the empiri-
cal testing of the concept classification of PV if PN
is fixed. Separately, PN (or PV ) can be evaluated
by some indexes and evolves to a satisfiable result.
A little more complicated, the closed semantic con-
straints destroy the independent evolution of PN
and PV . If PV is fixed, then the optimization of
PN may be implemented (but not completely re-
liable) and vice versa. While it is still an open
problem to define a numerical measure that could
formalize the optimization of the total structures
in a WordNet-like lexicon, especially PN and PV .
</bodyText>
<sectionHeader confidence="0.997298" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999650625">
A scheme of the closed semantic constraint in a
WordNet-like lexicon based on WSD has been de-
scribed as an application driven construction of
a dynamic lexicon. At the same time, the fur-
ther topic leads to how the rule-based concept
tagging benefits from the descriptions of seman-
tic constraint. The empirical method is much em-
phasized in the WSD and the development of the
dynamic lexicon, such as the TagSet training, the
SynSet testing and the evolution of a WordNet-like
lexicon (see [15], [16] and [17]). The author be-
lieves that the computable part in Computational
Lexicology is nothing but the evolution of the dy-
namic lexicon oriented to a particular application,
which is actually the optimization of the language
knowledge base.
</bodyText>
<sectionHeader confidence="0.96893" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999954777777778">
I appreciate all my colleagues participating in the
CCD project, the blithesome collaboration with
them is always memorable for me. Many thanks to
my friends in the Second and the Third Workshop
on Chinese Lexical Semantics for their kindly dis-
cussion with the author. Lastly, the most thankful
words are given to my wife for her longtime toler-
ance to my weaselling from the housework under
the false pretense of research.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922684210526">
[1] ALPAC 1966 Language and Machine: Com-
puters in Translation and Linguistics, Na-
tional Research Council Automatic Language
Processing Advisory Committee, Washing-
ton, D.C.
[2] Aristotle 1941 Categoriae, in The Basic
Works of Aristotle, R. McKeon (ed). Random
House, New York.
[3] Beckwith R. 1998 Design and Implementation
of the WordNet Lexical Database and Search-
ing Software, in [5], pp105-127.
[4] Fellbaum C. 1998 A Semantic Net of English
Verbs, in [5], pp69-104.
[5] Fellbaum C. (ed) 1999 WordNet: An Elec-
tronic Lexical Database, The MIT Press.
[6] Ide N. and V´eronis J. 1998 Introduction to
Special Issue on Word Sense Disambiguation:
The State of Art, Computational Linguistics,
Vol. 24, No. 1, pp1-40.
[7] Liu Y, Yu S.W. and Yu J.S. Building a Bilin-
gual WordNet: New Approaches and algo-
rithms, accepted by COLING2002.
[8] Manning C.D. and Sch¨utze H. 1999, Founda-
tions of Statistical Natural Language Process-
ing, The MIT Press.
[9] Miller G.A. et al 1993 Introduction to Word-
Net: An On-line Lexical Database, in the at-
tached specification of WordNet 1.6.
[10] Miller G.A. 1998 Nouns in WordNet, in [5],
pp23-46.
[11] Priss U. 1999 The Formalization of WordNet
by Methods of Relational Concept Analysis, in
[5], pp179-196.
[12] Vossen P. (ed.) 1998 EuroWordNet: A Multi-
linugual Database with Lexical Semantic Net-
works. Dordrecht: Kluwer.
[13] Yu J.S. and Yu S.W. et al 2001 Introduc-
tion to Chinese Concept Dictionary, in Inter-
national Conference on Chinese Computing
(ICCC2001), pp361-367.
[14] Yu J.S. 2001 The Structure of Chinese Con-
cept Dictionary, accepted by Journal of Chi-
nese Information Processing, 2001.
[15] Yu J.S. 2001 Evolution of WordNet-like Lexi-
con, in The First International Conference of
Global WordNet, Mysore, India, 2002.
[16] Yu J.S. and Yu S.W. 2002 Word Sense Dis-
ambiguation based on Integrated Language
Knowledge Base, in The 2nd International
Conference on East-Asian Language Pro-
cessing and Internet Information Technology
(EALPIIT’2002).
[17] Yu J.S. 2002 Statistical Methods in Word
Sense Disambiguation, draft (can be down-
loaded from http://icl.pku.edu.cn/yujs/) of
seminar at the Institute of Computational
Linguistics, Peking Univ..
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154485">
<title confidence="0.785484">WSD and Closed Semantic Constraint</title>
<affiliation confidence="0.840291">Institute of Computational</affiliation>
<address confidence="0.658757">Peking University, Beijing, China, 100871</address>
<abstract confidence="0.998087818181818">application-driven construction of lexicon has been emphasized as a methodology of Computational Lexicology recently. We focus on the closed semantic constraint of the argument(s) of any verb concept by the noun concepts in a WordNet-like lexicon, which theoretically is related to Word Sense Disambiguation (WSD) at different levels. From the viewpoint of Dynamic Lexicon, WSD provides a way of automatic construction for the closed semantic constraints and also benefits from the semantic descriptions.</abstract>
<keyword confidence="0.4196095">lexicon, evolution, WSD, WordNet-like lexicon, closed semantic constraint</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>PV</author>
</authors>
<title>if the closed semantic constraint of C&apos;</title>
<booktitle>is C1 ⊕ C2 ⊕ · · · ⊕ Cn, then ∃Cn+1, · · · , Cm ∈ PN such that (((C1 ⊕ C2 ⊕ · · ·⊕Cn)uCn+1)u· · ·uCm) is</booktitle>
<marker>PV, </marker>
<rawString> Property 4.1 ∀C, C&apos; ∈ PV , C --&lt; C&apos;, if the closed semantic constraint of C&apos; is C1 ⊕ C2 ⊕ · · · ⊕ Cn, then ∃Cn+1, · · · , Cm ∈ PN such that (((C1 ⊕ C2 ⊕ · · ·⊕Cn)uCn+1)u· · ·uCm) is the closed semantic constraint of C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ALPAC</author>
</authors>
<title>Language and Machine: Computers in Translation and Linguistics, National Research Council Automatic Language Processing Advisory Committee,</title>
<date>1966</date>
<location>Washington, D.C.</location>
<contexts>
<context position="8339" citStr="[1]" startWordPosition="1470" endWordPosition="1470">nd step is to choose the most efficient one which makes the best accuracy of the statistical concept tagging. Different from those unframed tags, the deductive rule along the hypernymy trees works out the sense of w by the following property: Property 3.1 Suppose that the TagSet is T = {C1, C2, · · · , Ck}, and the word w in a given sentence is tagged by Ci, then the sense of w here is the SynSet C which satisfies that Ci � C and w ∈ C, where � is the partial ordering of the nodes in the hypernymy tree. 3.2 Statistical Model of WSD In some sense, WSD is the kernel problem of both NLU and NLP ([1], [6], [8]). POS and concept tag are two random variables in the HMM of WSD. Sometimes POS of w determines its sense, sometimes not. But in most cases, a sense of w implies a unique POS. The distribution of w’s senses with the POS, P, is important in the (POS, concept)- tagging. A Hidden Markov Model with two parameters will be adopted as the main statistical model for WSD, and the Statistical Decision Theory and Bayesian Analysis, which are good at analyzing the small samples, conducted as a comparison. The training corpus, T, is done by hand, where the cursor sensitive display of the senses </context>
</contexts>
<marker>[1]</marker>
<rawString>ALPAC 1966 Language and Machine: Computers in Translation and Linguistics, National Research Council Automatic Language Processing Advisory Committee, Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aristotle</author>
</authors>
<title>Categoriae, in The Basic Works of</title>
<date>1941</date>
<location>New York.</location>
<marker>[2]</marker>
<rawString>Aristotle 1941 Categoriae, in The Basic Works of Aristotle, R. McKeon (ed). Random House, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Beckwith</author>
</authors>
<date>1998</date>
<booktitle>Design and Implementation of the WordNet Lexical Database and Searching Software, in [5],</booktitle>
<pages>105--127</pages>
<contexts>
<context position="5708" citStr="[3]" startWordPosition="988" endWordPosition="988">tural language, in which a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is </context>
</contexts>
<marker>[3]</marker>
<rawString>Beckwith R. 1998 Design and Implementation of the WordNet Lexical Database and Searching Software, in [5], pp105-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>A Semantic Net of English Verbs,</title>
<date>1998</date>
<journal>in</journal>
<volume>5</volume>
<pages>69--104</pages>
<contexts>
<context position="5713" citStr="[4]" startWordPosition="989" endWordPosition="989"> language, in which a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a sub</context>
</contexts>
<marker>[4]</marker>
<rawString>Fellbaum C. 1998 A Semantic Net of English Verbs, in [5], pp69-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database,</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5718" citStr="[5]" startWordPosition="990" endWordPosition="990">uage, in which a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset o</context>
</contexts>
<marker>[5]</marker>
<rawString>Fellbaum C. (ed) 1999 WordNet: An Electronic Lexical Database, The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J V´eronis</author>
</authors>
<title>Introduction to Special Issue on Word Sense Disambiguation: The State of Art,</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>1--40</pages>
<contexts>
<context position="8344" citStr="[6]" startWordPosition="1471" endWordPosition="1471">ep is to choose the most efficient one which makes the best accuracy of the statistical concept tagging. Different from those unframed tags, the deductive rule along the hypernymy trees works out the sense of w by the following property: Property 3.1 Suppose that the TagSet is T = {C1, C2, · · · , Ck}, and the word w in a given sentence is tagged by Ci, then the sense of w here is the SynSet C which satisfies that Ci � C and w ∈ C, where � is the partial ordering of the nodes in the hypernymy tree. 3.2 Statistical Model of WSD In some sense, WSD is the kernel problem of both NLU and NLP ([1], [6], [8]). POS and concept tag are two random variables in the HMM of WSD. Sometimes POS of w determines its sense, sometimes not. But in most cases, a sense of w implies a unique POS. The distribution of w’s senses with the POS, P, is important in the (POS, concept)- tagging. A Hidden Markov Model with two parameters will be adopted as the main statistical model for WSD, and the Statistical Decision Theory and Bayesian Analysis, which are good at analyzing the small samples, conducted as a comparison. The training corpus, T, is done by hand, where the cursor sensitive display of the senses provi</context>
</contexts>
<marker>[6]</marker>
<rawString>Ide N. and V´eronis J. 1998 Introduction to Special Issue on Word Sense Disambiguation: The State of Art, Computational Linguistics, Vol. 24, No. 1, pp1-40.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Liu</author>
<author>S W Yu</author>
<author>J S Yu</author>
</authors>
<title>Building a Bilingual WordNet: New Approaches and algorithms, accepted by COLING2002.</title>
<contexts>
<context position="6127" citStr="[7]" startWordPosition="1054" endWordPosition="1054">tricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of all SynSets containing w is denoted by Δ(w), in which each element is called a sense of w. Definition 3.3 Given a well-defined sentence S = w1w2 · · · wn, WSD is the computable processing which tags wi a unique sense si = {wi, wi1, · · · , wil,} such that each derived combinatorial path is a well-defined sentence with the semantics of S. The Principle of Substitution provides a corpus-base</context>
</contexts>
<marker>[7]</marker>
<rawString>Liu Y, Yu S.W. and Yu J.S. Building a Bilingual WordNet: New Approaches and algorithms, accepted by COLING2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8349" citStr="[8]" startWordPosition="1472" endWordPosition="1472"> to choose the most efficient one which makes the best accuracy of the statistical concept tagging. Different from those unframed tags, the deductive rule along the hypernymy trees works out the sense of w by the following property: Property 3.1 Suppose that the TagSet is T = {C1, C2, · · · , Ck}, and the word w in a given sentence is tagged by Ci, then the sense of w here is the SynSet C which satisfies that Ci � C and w ∈ C, where � is the partial ordering of the nodes in the hypernymy tree. 3.2 Statistical Model of WSD In some sense, WSD is the kernel problem of both NLU and NLP ([1], [6], [8]). POS and concept tag are two random variables in the HMM of WSD. Sometimes POS of w determines its sense, sometimes not. But in most cases, a sense of w implies a unique POS. The distribution of w’s senses with the POS, P, is important in the (POS, concept)- tagging. A Hidden Markov Model with two parameters will be adopted as the main statistical model for WSD, and the Statistical Decision Theory and Bayesian Analysis, which are good at analyzing the small samples, conducted as a comparison. The training corpus, T, is done by hand, where the cursor sensitive display of the senses provides t</context>
</contexts>
<marker>[8]</marker>
<rawString>Manning C.D. and Sch¨utze H. 1999, Foundations of Statistical Natural Language Processing, The MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G A Miller</author>
</authors>
<title>et al 1993 Introduction to WordNet: An On-line Lexical Database, in the attached specification of WordNet 1.6.</title>
<contexts>
<context position="5723" citStr="[9]" startWordPosition="991" endWordPosition="991"> in which a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ.</context>
</contexts>
<marker>[9]</marker>
<rawString>Miller G.A. et al 1993 Introduction to WordNet: An On-line Lexical Database, in the attached specification of WordNet 1.6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<date>1998</date>
<pages>23--46</pages>
<note>Nouns in WordNet, in [5],</note>
<contexts>
<context position="5729" citStr="[10]" startWordPosition="992" endWordPosition="992">hich a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The s</context>
</contexts>
<marker>[10]</marker>
<rawString>Miller G.A. 1998 Nouns in WordNet, in [5], pp23-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Priss</author>
</authors>
<title>The Formalization of WordNet by Methods of Relational Concept Analysis,</title>
<date>1999</date>
<note>in</note>
<contexts>
<context position="5735" citStr="[11]" startWordPosition="993" endWordPosition="993"> concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of </context>
</contexts>
<marker>[11]</marker>
<rawString>Priss U. 1999 The Formalization of WordNet by Methods of Relational Concept Analysis, in</rawString>
</citation>
<citation valid="false">
<pages>179--196</pages>
<contexts>
<context position="5718" citStr="[5]" startWordPosition="990" endWordPosition="990">uage, in which a concept is defined by a synonym set (SynSet). A more important work in WordNet is the construction of a well-structured concept network based on the hypernymy relation (the main framework) and other accessorial relations, such as, the opposite relation, the holonymy relation, entailment, cause, etc. Definition 3.1 A WordNet-like lexicon is a dynamic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset o</context>
</contexts>
<marker>[5]</marker>
<rawString>, pp179-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Vossen</author>
</authors>
<date>1998</date>
<booktitle>EuroWordNet: A Multilinugual Database with Lexical Semantic Networks.</booktitle>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="6084" citStr="[12]" startWordPosition="1048" endWordPosition="1048">mic lexicon with the type of WordNet: 1. restricted to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of all SynSets containing w is denoted by Δ(w), in which each element is called a sense of w. Definition 3.3 Given a well-defined sentence S = w1w2 · · · wn, WSD is the computable processing which tags wi a unique sense si = {wi, wi1, · · · , wil,} such that each derived combinatorial path is a well-defined sentence with the semantics of S. The Princ</context>
</contexts>
<marker>[12]</marker>
<rawString>Vossen P. (ed.) 1998 EuroWordNet: A Multilinugual Database with Lexical Semantic Networks. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yu</author>
<author>S W Yu</author>
</authors>
<title>Introduction to Chinese Concept Dictionary,</title>
<date>2001</date>
<booktitle>in International Conference on Chinese Computing (ICCC2001),</booktitle>
<pages>361--367</pages>
<contexts>
<context position="6133" citStr="[13]" startWordPosition="1055" endWordPosition="1055">ed to each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of all SynSets containing w is denoted by Δ(w), in which each element is called a sense of w. Definition 3.3 Given a well-defined sentence S = w1w2 · · · wn, WSD is the computable processing which tags wi a unique sense si = {wi, wi1, · · · , wil,} such that each derived combinatorial path is a well-defined sentence with the semantics of S. The Principle of Substitution provides a corpus-based empi</context>
</contexts>
<marker>[13]</marker>
<rawString>Yu J.S. and Yu S.W. et al 2001 Introduction to Chinese Concept Dictionary, in International Conference on Chinese Computing (ICCC2001), pp361-367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yu</author>
</authors>
<title>The Structure of Chinese Concept Dictionary, accepted by</title>
<date>2001</date>
<booktitle>Journal of Chinese Information Processing,</booktitle>
<contexts>
<context position="6139" citStr="[14]" startWordPosition="1056" endWordPosition="1056">each category, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of all SynSets containing w is denoted by Δ(w), in which each element is called a sense of w. Definition 3.3 Given a well-defined sentence S = w1w2 · · · wn, WSD is the computable processing which tags wi a unique sense si = {wi, wi1, · · · , wil,} such that each derived combinatorial path is a well-defined sentence with the semantics of S. The Principle of Substitution provides a corpus-based empirical </context>
</contexts>
<marker>[14]</marker>
<rawString>Yu J.S. 2001 The Structure of Chinese Concept Dictionary, accepted by Journal of Chinese Information Processing, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yu</author>
</authors>
<title>Evolution of WordNet-like Lexicon,</title>
<date>2001</date>
<booktitle>in The First International Conference of Global WordNet,</booktitle>
<location>Mysore, India,</location>
<contexts>
<context position="1459" citStr="[15]" startWordPosition="220" endWordPosition="220">on are the relationships between verbs and nouns, which usually comes down to the closed semantic constraint of each argument of any verb. Once the structure of the semantic lexicon is determined, the closed semantic constraint becomes a welldefined problem. Example 1.1 The verb dˇa has many meanings in Chinese, which differ in dˇa h´aizi (punish the child), dˇa m´aoy¯i (weaver the sweater), dˇa ji`angy´ou (buy the soy), etc. Actually, the semantics of dˇa is distinguished by the semantics of its arguments. Different from the traditional lexicon, we advocate the conception of dynamic lexicon ([15]) and its evolution oriented to some particular application, which will be mentioned in Section 2. The WordNet-like lexicon is treated as a dynamic one, which means that the structures representing semantic knowledge could be changed according to some empirical standards. In the next section, we’ll define the WSD based on the WordNet-like lexicon, and then discuss the training of concept TagSet and the statistical model of WSD. By the statistical WSD, in Section 4, we introduce an approach to the automatic construction of the closed ∗This paper is supported by National Foundation of Natural Sc</context>
<context position="6148" citStr="[15]" startWordPosition="1058" endWordPosition="1058">gory, S is a labeled tree from the viewpoint of the hypernymy relation for both noun concepts and verb concepts, &apos;The specification of WordNet could be found in [3], [4], [5], [9], [10], [11], etc. 3. closed semantic constraint of the argument(s) of each verb concept from the noun concepts. The WordNet-like lexicon is complex structured, it may not have the same ontology of WordNet, neither the semantic knowledge representations. But the description method seems a general format for all languages from the fact of EuroWordNet (see [12]), Chinese Concept Dictionary (CCD, see [7], [13], [14] and [15]), Korean WordNet, Tamil WordNet, etc. Definition 3.2 Let Σ be the set of all words, then Γ, the set of all concepts (or SynSets) in a WordNet-like lexicon, is a subset of 2Σ. The set of all SynSets containing w is denoted by Δ(w), in which each element is called a sense of w. Definition 3.3 Given a well-defined sentence S = w1w2 · · · wn, WSD is the computable processing which tags wi a unique sense si = {wi, wi1, · · · , wil,} such that each derived combinatorial path is a well-defined sentence with the semantics of S. The Principle of Substitution provides a corpus-based empirical approach </context>
<context position="7637" citStr="[15]" startWordPosition="1327" endWordPosition="1327">r approach to WSD based on a WordNetlike lexicon. 3.1 The Training of TagSet The traditional semantic tags are from some ontology, the apriority of which is often criticized by computational linguists. For us, the empirical method must impenetrate each step of WSD because of the complexity of language knowledge. The statistical approach to WSD needs a well concept-tagged corpus as the training set for the concept TagSet and the statistical data in the Hidden Markov Model (HMM). To avoid the sparse data problem, only a few real subsets of Γ could act as the TagSet in the statistical model (see [15] and [16]). The first step leads to a set of structured TagSets {T1, T2, · · · , Tm}, then the second step is to choose the most efficient one which makes the best accuracy of the statistical concept tagging. Different from those unframed tags, the deductive rule along the hypernymy trees works out the sense of w by the following property: Property 3.1 Suppose that the TagSet is T = {C1, C2, · · · , Ck}, and the word w in a given sentence is tagged by Ci, then the sense of w here is the SynSet C which satisfies that Ci � C and w ∈ C, where � is the partial ordering of the nodes in the hypernym</context>
</contexts>
<marker>[15]</marker>
<rawString>Yu J.S. 2001 Evolution of WordNet-like Lexicon, in The First International Conference of Global WordNet, Mysore, India, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yu</author>
<author>S W Yu</author>
</authors>
<title>Word Sense Disambiguation based on Integrated Language Knowledge Base,</title>
<date>2002</date>
<booktitle>in The 2nd International Conference on East-Asian Language Processing and Internet Information Technology (EALPIIT’2002).</booktitle>
<contexts>
<context position="7646" citStr="[16]" startWordPosition="1329" endWordPosition="1329">h to WSD based on a WordNetlike lexicon. 3.1 The Training of TagSet The traditional semantic tags are from some ontology, the apriority of which is often criticized by computational linguists. For us, the empirical method must impenetrate each step of WSD because of the complexity of language knowledge. The statistical approach to WSD needs a well concept-tagged corpus as the training set for the concept TagSet and the statistical data in the Hidden Markov Model (HMM). To avoid the sparse data problem, only a few real subsets of Γ could act as the TagSet in the statistical model (see [15] and [16]). The first step leads to a set of structured TagSets {T1, T2, · · · , Tm}, then the second step is to choose the most efficient one which makes the best accuracy of the statistical concept tagging. Different from those unframed tags, the deductive rule along the hypernymy trees works out the sense of w by the following property: Property 3.1 Suppose that the TagSet is T = {C1, C2, · · · , Ck}, and the word w in a given sentence is tagged by Ci, then the sense of w here is the SynSet C which satisfies that Ci � C and w ∈ C, where � is the partial ordering of the nodes in the hypernymy tree. 3</context>
</contexts>
<marker>[16]</marker>
<rawString>Yu J.S. and Yu S.W. 2002 Word Sense Disambiguation based on Integrated Language Knowledge Base, in The 2nd International Conference on East-Asian Language Processing and Internet Information Technology (EALPIIT’2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yu</author>
</authors>
<title>Statistical Methods in Word Sense Disambiguation, draft (can be downloaded from http://icl.pku.edu.cn/yujs/) of seminar at the Institute of Computational Linguistics,</title>
<date>2002</date>
<location>Peking Univ..</location>
<marker>[17]</marker>
<rawString>Yu J.S. 2002 Statistical Methods in Word Sense Disambiguation, draft (can be downloaded from http://icl.pku.edu.cn/yujs/) of seminar at the Institute of Computational Linguistics, Peking Univ..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>