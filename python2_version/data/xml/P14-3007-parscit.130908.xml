<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.804837">
Learning Grammar with Explicit Annotations for Subordinating
Conjunctions
</title>
<note confidence="0.69511275">
Dongchen Li, Xiantao Zhang and Xihong Wu
Key Laboratory of Machine Perception and Intelligence
Speech and Hearing Research Center
Peking University, Beijing, China
</note>
<email confidence="0.941769">
{lidc,zhangxt,wxh}@cis.pku.edu.cn
</email>
<sectionHeader confidence="0.982253" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853107142857">
Data-driven approach for parsing may suf-
fer from data sparsity when entirely un-
supervised. External knowledge has been
shown to be an effective way to alleviate
this problem. Subordinating conjunctions
impose important constraints on Chinese
syntactic structures. This paper proposes a
method to develop a grammar with hierar-
chical category knowledge of subordinat-
ing conjunctions as explicit annotations.
Firstly, each part-of-speech tag of the sub-
ordinating conjunctions is annotated with
the most general category in the hierar-
chical knowledge. Those categories are
human-defined to represent distinct syn-
tactic constraints, and provide an appropri-
ate starting point for splitting. Secondly,
based on the data-driven state-split ap-
proach, we establish a mapping from each
automatic refined subcategory to the one
in the hierarchical knowledge. Then the
data-driven splitting of these categories is
restricted by the knowledge to avoid over
refinement. Experiments demonstrate that
constraining the grammar learning by the
hierarchical knowledge improves parsing
performance significantly over the base-
line.
</bodyText>
<sectionHeader confidence="0.787686" genericHeader="introduction">
1 Introduction
</sectionHeader>
<note confidence="0.723913444444445">
Probabilistic context-free grammars (PCFGs) un-
derlie most of the high-performance parsers
(Collins, 1999; Charniak, 2000; Charniak and
Johnson, 2005; Zhang and Clark, 2009; Chen and
Kit, 2012; Zhang et al., 2013). However, a naive
PCFG which simply takes the empirical rules and
probabilities off of a Treebank does not perform
well (Klein and Manning, 2003; Levy and Man-
ning, 2003; Bansal and Klein, 2012), because
</note>
<bodyText confidence="0.999800512195122">
its context-freedom assumptions are too strong in
some cases (e.g. it assumes that subject and ob-
ject NPs share the same distribution). Therefore,
a variety of techniques have been developed to en-
rich PCFG (Klein and Manning, 2005; Matsuzaki
et al., 2005; Zhang and Clark, 2011; Shindo et al.,
2012).
Hierarchical state-split approach (Petrov et al.,
2006; Petrov and Klein, 2007; Petrov and Klein,
2008a; Petrov and Klein, 2008b; Petrov, 2009)
refines and generalizes the original grammars in
a data-driven manner, and achieves state-of-the-
art performance. Starting from a completely
markovized X-Bar grammar, each category is split
into two subcategories. EM is initialized with this
starting point and used to climb the highly non-
convex objective function of computing the joint
likelihood of the observed parse trees. Then a
merging step applies a likelihood ratio test to re-
verse the least useful half part of the splits. Learn-
ing proceeds by iterating between those two steps
for six rounds. Spectral learning of latent-variable
PCFGs (Cohen et al., 2012; Bailly et al., ; Co-
hen et al., 2013b; Cohen et al., 2013a) is an-
other effective manner of state-split approach that
provides accurate and consistent parameter esti-
mates. However, this two complete data-driven
approaches are likely to be hindered by the over-
fitting issue.
Incorporating knowledge (Zhang et al., 2013;
Wu et al., 2011) to refine the categories in train-
ing a parser has been proved to remedy the
weaknesses of probabilistic context-free grammar
(PCFG). The knowledge contains content words
semantic resources base (Fujita et al., 2010; Agirre
et al., 2008; Lin et al., 2009), named entity cues
(Li et al., 2013) and so on. However, they are
limited in that they do not take into account the
knowledge about subordinating conjunctions.
Subordinating conjunctions are important in-
dications for different syntactic structure, espe-
</bodyText>
<page confidence="0.681656">
48
</page>
<note confidence="0.52251">
Proceedings of the ACL 2014 Student Research Workshop, pages 48–55,
</note>
<bodyText confidence="0.97297021875">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
cially for Chinese. For example, the subordinating
conjunction “�i`” (no matter what) is typically
ahead of a sentence with pros and cons of the sit-
uation; on the contrary, a sufficient condition of-
ten occurs after the subordinating conjunction “�
F,” (if). Those two cases are of distinct syntac-
tic structure. Figure 1 demonstrates that although
the sequences of the part-of-speech of the input
words are similar, these two subordinating con-
junctions exert quite different syntactic constraints
to the following clauses.
IP
nating conjunctions. In order to constrain the auto-
matic subcategory refinement, we firstly establish
the mapping between the automatic clustered sub-
categories and the predefined subcategories. Then
we employ a knowledge criterion to supervise the
hierarchical splitting of these subordinating con-
junction subcategories by the automatic state-split
approach, which can alleviate over-fitting. The ex-
periments are carried out on Penn Chinese Tree-
bank and Tsinghua Treebank, which verify that
the refined grammars with refined subordinating
conjunction categories can improve parsing per-
formance significantly.
The rest of this paper is organized as follows.
We first describe our hierarchical subcategories of
subordinating conjunction. Section 3 illustrates
the constrained grammar learning process in de-
tails. Section 4 presents the experimental evalua-
tion and the comparison with other approaches.
</bodyText>
<figure confidence="0.996372666666666">
ADVP
CS
VP CC VP
VA il,A ADVP VP
or
)AVj AD VA
succeed
T, )AVj
not succeed
T,iE�
No
matter
(a) “)GiiR” (no matter what) is typically ahead of a sentence
with pros and cons of the situation.
IP
</figure>
<figureCaption confidence="0.9870785">
Figure 1: Different types of subordinating con-
junctions indicate distinct syntactic structure.
</figureCaption>
<bodyText confidence="0.999453">
Based on the hierarchical state-split approach,
this paper proposes a data-oriented model super-
vised by our hierarchical subcategories of subordi-
</bodyText>
<sectionHeader confidence="0.877352" genericHeader="method">
2 Hierarchical Subcategories of
</sectionHeader>
<subsectionHeader confidence="0.910967">
Subordinating Conjunction
</subsectionHeader>
<bodyText confidence="0.999983517241379">
The only tag “CS” for all the various subordinat-
ing conjunctions is too coarse to indicate the in-
tricate subordinating relationship. The words in-
dicating different grammatical features share the
same tag “CS”, such as transition relationship,
progression relationship, preference relationship,
purpose relationship and condition relationship. In
each case, the context is different, and the subor-
dinating conjunction is an obvious indication for
the parse disambiguation for the context. The ex-
isting resources for computational linguistic, like
HowNet (Dong and Dong, 2003) and Cilin (Mei
et al., 1983), have classified all subordinating con-
junctions as one category, which is too coarse to
capture the syntactic implication.
To make use of the indication, we subdivide the
subordinating conjunctions according to its gram-
matical features in our scheme. Subordinating
conjunctions indicating each relationship is further
subdivided into two subcategories: one is used be-
fore the principal clause, the other is before the
subordinate clause. For example, the conjunc-
tions representing cause and effect contains “be-
cause” and “so”, where “because” should mod-
ify the cause, and “so” should modify the effect.
In addition, we found that there are several cases
in the conditional clause. Accordingly, we sub-
divide the conditional subordinating conjunctions
into seven types: assumption, universalization,
</bodyText>
<figure confidence="0.996424166666667">
ADVP
CS
SpA PN ADVP VP
if
*1 AD ADVP VP
you
il,A AD VA
still
T, )Azj
don’t succeed
(b) “AnF,” (if) often precedes a sufficient condition.
NP
VP
IP
49
5ubordinatingConjunction
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎩
Transition
LatterOf“Transition&amp;quot; %
Formerof“Transition&amp;quot; 14,
�
FormerOf“Progression�� TT9
Progression
LatterOf“Progression&amp;quot; 1Y_
�
LatterOf“Preference�� TX
Preference
FormerOf“Preference&amp;quot; 4,
LogicCoordination ⎧ LatterOfTheCoordination q
⎨ Logic“And&amp;quot; #FA.
⎩
Assumption XT
Universalization T1�L&apos;
UnnecessaryCondition LOE,
InsufficientCondition RP&apos;B�
5ufficientCondition PI!
NecessaryCondition P,k
Equality W,4
�
LatterOf“Purpose�� �E
Purpose
FormerOf“Purpose&amp;quot; �4*
�
Cause du
CauseAndEffect
Effect A
Condition ⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
</figure>
<figureCaption confidence="0.998711">
Figure 2: Hierarchical subcategories of subordinating conjunctions with examples.
</figureCaption>
<bodyText confidence="0.999836">
equality, sufficient condition, necessary condition,
sufficient but unnecessary condition and necessary
but insufficient condition (concession). The de-
tailed hierarchical subcategories of subordinating
conjunctions are displayed in Figure 2.
</bodyText>
<sectionHeader confidence="0.631015" genericHeader="method">
3 Parsing with Hierarchical Categories
</sectionHeader>
<bodyText confidence="0.999985757575758">
The automatic state-split approach is designed to
refine all symbols together through a data-driven
manner, which takes the over-fitting risk. Instead
of splitting and merging all symbols together auto-
matically, we employ a knowledge-based criterion
with hierarchical refinement knowledge to con-
straint the splitting of these new refined tags for
subordinating conjunctions.
At the beginning, we produce a good starting
annotation with the top subcategories in the hi-
erarchical subcategories, which is of great use to
constraining the automatic splitting process. As
demonstrated in Figure 4, our parser is trained on
the good initialization with the automatic hierar-
chical state-split process, and gets improvements
compared with the original training data. For ex-
ample, as shown in Figure 2, the category for
%(but) and “Cause” for du(because) is anno-
tated as the top category “Transition” and “Cause
And Effect” respectively.
However, during this process, only the most
general hypernyms are used as the semantic rep-
resentation of words, and the lower subcategory
knowledge in the hierarchy is not explored. Thus,
we further constraint the split of the subordinating
conjunctions subcategories to be consistent with
the hierarchical subcategories to alleviate the over-
fitting issue. The top class is only used as the start-
ing annotations of POS tags to reduce the search
space for EM in our method. It is followed by the
hierarchical state-split process to further refine the
starting annotations based on the hierarchical sub-
categories.
</bodyText>
<subsectionHeader confidence="0.973546">
3.1 Mapping from Automatic Subcategories
to Predefined Subcategories
</subsectionHeader>
<bodyText confidence="0.999515">
With the initialization proposed above, the auto-
matically split-merge approach produces a series
of refined categories for each tag. We restrict each
automatically refined subcategory of subordinat-
ing conjunctions to correspond to a special node
</bodyText>
<page confidence="0.455243">
50
</page>
<figureCaption confidence="0.989262">
Figure 3: A schematic figure for the hierarchical state-split process of the tag “CS”. Each subcategory
</figureCaption>
<bodyText confidence="0.996881625">
of this tag has its own word set, and corresponds to one layer at the appropriate level in the hierarchical
subcategories.
in the hierarchical subcategories, as a hyponym
of “CS”. The hierarchical subcategories are em-
ployed in the hierarchical state-split process to im-
pose restrictions on the subcategory refinement.
First of all, it is necessary to establish the map-
ping from each subcategory in the data-driven hi-
erarchical subcategories to the subcategory in the
predefined hierarchical subcategories. We trans-
fer the method for semantic-related labels (Lin et
al., 2009) to our case here. The mapping is imple-
mented with the word set related to each automati-
cally refined granularity of clustered subordinating
conjunctions and the node at the special level in
the subcategory knowledge. The schematic in Fig-
ure 3 demonstrates this supervised splitting pro-
cess for CS. The left part of this figure is the word
sets of automatic clustered subcategories of the
CS, which is split hierarchically. As expressed
by the lines, each subcategory corresponds to one
node in the right part of this figure, which is our hi-
erarchical subcategory knowledge of subordinat-
ing conjunctions.
As it is shown in Figure 3, the original tag “CS”
treats all the words it produces as its word set.
Upon splitting each coarse category into two more
specific subcategories, its word set is also cut into
two subsets accordingly, through forcedly divid-
ing each word in the word set into one subcategory
which is most probable for this word in the lex-
ical grammar. And each automatic refinement is
mapped to the most specific subcategory (that is to
say, the lowest node) that contains the entirely cor-
responding word set in the human-defined knowl-
edge. On this basis, the new knowledge-based cri-
terion is introduced to enrich and generalize these
subcategories, with the purpose of fitting the re-
finement to the subcategory knowledge rather than
the training data.
</bodyText>
<subsectionHeader confidence="0.999245">
3.2 Knowledge-based Criterion for
Subordinating Conjunctions Refinement
</subsectionHeader>
<bodyText confidence="0.99997152631579">
With the mapping between the automatic refined
subcategories and the human-defined hierarchical
subcategory knowledge, we could supervise the
automatic state refinement by the knowledge.
Instead of being merged by likelihood, a
knowledge-based criterion is employed, to decide
whether or not to go back to the upper layer in
the hierarchical subcategories and thus remove the
new subcategories of these tags. The criterion is
that, we assume that the bottom layer in the hi-
erarchical subcategories is special enough to ex-
press the distinction of the subordinating conjunc-
tions. If the subcategories of the subordinating
conjunctions has gone beyond the bottom layer,
then the new split subcategories are deemed to be
unnecessary and should be merged back. That is
to say, once the parent layer of this new subcate-
gory is mapped onto the most special subcategory,
it should be removed immediately. As illustrated
</bodyText>
<page confidence="0.836503">
51
</page>
<table confidence="0.994583666666667">
Treebank Train Dataset Develop Dataset Test Dataset
CTB5 Articles 1-270 Articles 400-1151, 301-325 Articles 271-300
TCT 16000 sentences 800 sentences 758 sentences
</table>
<tableCaption confidence="0.999759">
Table 1: Data allocation of our experiment.
</tableCaption>
<bodyText confidence="0.999791769230769">
in Figure 3, if the node has no hyponym, this sub-
category has been specialized enough according to
the knowledge, and thus the corresponding subcat-
egory will stop splitting.
By introducing a knowledge-based criterion,
the issue is settled whether or not to further split
subcategories from the perspective of predefined
knowledge. To investigate the effectiveness of the
presented approach, several experiments are con-
ducted on both Penn Chinese Treebank and Ts-
inghua Treebank. They reveal that the subcategory
knowledge of subordinating conjunctions is effec-
tive for parsing.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997064">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999995454545455">
We present experimental results on both Chinese
Treebank (CTB) 5.0 (Xue et al., 2002) (All traces
and functional tags were stripped.) and Tsinghua
Treebank (TCT) (Zhou, 2004). All the experi-
ments were carried out after six cycles of split-
merge.
The data set allocation is described in Table 1.
We use the EVALB parseval reference imple-
mentation (Sekine, 1997) for scoring. Statistical
significance was checked by Bikel’s randomized
parsing evaluation comparator (Bikel, 2000).
</bodyText>
<subsectionHeader confidence="0.9893235">
4.2 Parsing Performance with Hierarchical
Subcategories
</subsectionHeader>
<bodyText confidence="0.99973">
We presented a flexible approach which refines
the subordinating conjunctions in a hierarchy fash-
ion where the hierarchical layers provide different
granularity of specificity. To facilitate the compar-
isons, we set up 6 experiments on CTB5.0 with
different strategies of choosing the subcategory
layers in the hierarchical subcategory knowledge:
</bodyText>
<listItem confidence="0.996895384615385">
• baseline: Training without hierarchical sub-
category knowledge
• top: Choosing the top layer in hierarchi-
cal subcategories (using “Transition”, “Con-
dition” , “Purpose” and so on)
• bottom: Choosing the bottom layer in hierar-
chical subcategories (the most specified sub-
categories)
• word: Substituting POS tag with the word it-
self
• knowledge criterion: Automatically choos-
ing the appropriate layer through the knowl-
edge criterion
</listItem>
<figureCaption confidence="0.9286395">
Figure 4: Comparison of parsing performance for
each model in the split-merge cycles.
</figureCaption>
<bodyText confidence="0.98832525">
Figure 4 shows the F1 scores of the last 4 cy-
cles in the 6 split-merge cycles. The results are
just as expectation, through which we can tell that
the “top” model performs slightly better than the
baseline owing to a better start point of the state-
splitting. This result confirms the value of our
initial explicit annotations. While the “bottom”
model doesn’t improve the performance due to
excessive refinement and causes over-fitting, the
“word” model behaves even worse for the same
reason. In the 5th split-merge cycle, the “knowl-
edge criterion” model picks the appropriate layer
</bodyText>
<page confidence="0.797666">
52
</page>
<bodyText confidence="0.998730857142857">
in hierarchical subcategories and achieves the best
result.
We also test our method on TCT. Table 2 com-
pares the accuracies of the baseline, initialization
with top subcategories and the “knowledge cri-
terion” model, and confirms that the subcategory
knowledge helps parse disambiguation.
</bodyText>
<table confidence="0.99927">
Parser P R F1
baseline 74.40 74.28 74.34
top 75.12 75.17 75.14
knowledge criterion 76.18 76.27 76.22
</table>
<tableCaption confidence="0.9950915">
Table 2: Our parsing performance with different
criterions on TCT.
</tableCaption>
<subsectionHeader confidence="0.999202">
4.3 Final Results
</subsectionHeader>
<bodyText confidence="0.998495888888889">
Our final results are achieved using the “knowl-
edge criterion” model. As we can see from the
table 3, our final parsing performance is higher
than the unlexicalized parser (Levy and Manning,
2003; Petrov, 2009) and the parsing system in
Qian and Liu (2012), but falls short of the systems
using semantic knowledge of Lin et al. (2009) and
exhaustive word formation knowledge of Zhang et
al. (2013).
</bodyText>
<table confidence="0.999816714285714">
Parser P R F1
Levy(2003) 78.40 79.20 78.80
Petrov(2009) 84.82 81.93 83.33
Qian(2012) 84.57 83.68 84.13
Zhang(2013) 84.42 84.43 84.43
Lin(2009) 86.00 83.10 84.50
This paper 85.93 82.87 84.32
</table>
<tableCaption confidence="0.993233">
Table 3: Our final parsing performance compared
</tableCaption>
<bodyText confidence="0.959119555555556">
with the best previous works on CTB5.0.
The improvement on the hierarchical state-split
approach verifies the effectiveness of the subcat-
egory knowledge of subordinating conjunctions
for alleviating over-fitting. And the subcategory
knowledge could be integrated with the knowl-
edge base employed in Lin et al. (2009) and Zhang
et al. (2013) to contribute more on parsing accu-
racy improvement.
</bodyText>
<sectionHeader confidence="0.988078" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99992275">
In this paper, we present an approach to constrain
the data-driven state-split method by hierarchi-
cal subcategories of subordinating conjunctions,
which appear as explicit annotations in the gram-
mar. The parsing accuracy is improved by this
method owing to two reasons. Firstly, the most
general hypernym of subordinating conjunctions
exerts an initial restrict to the following splitting
step. Secondly, the splitting process is confined
by a knowledge-based criterion with the human-
defined hierarchical subcategories to avoid over
refinement.
</bodyText>
<sectionHeader confidence="0.974385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999944818181818">
We thank Baidu for travel and conference sup-
port for this paper. We thank Meng Zhang and
Dingsheng Luo for their valuable advice. This
work was supported in part by the National Ba-
sic Research Program of China (973 Program) un-
der grant 2013CB329304, the Research Special
Fund for Public Welfare Industry of Health under
grant 201202001, the Key National Social Science
Foundation of China under grant 12&amp;ZD119, the
National Natural Science Foundation of China un-
der grant 91120001.
</bodyText>
<sectionHeader confidence="0.93146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998582090909091">
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and pp attachment perfor-
mance with sense information. Proceedings ofACL-
08: HLT, pages 317–325.
Rapha¨el Bailly, Xavier Carreras P´erez, Franco M
Luque, and Ariadna Julieta Quattoni. Unsupervised
spectral learning of wcfg as low-rank matrix com-
pletion. Association for Computational Linguistics.
Mohit Bansal and Daniel Klein. 2012. An all-
fragments grammar for simple and accurate parsing.
Technical report, DTIC Document.
Bikel. 2000. Dan bikel’s random-
ized parsing evaluation comparator. In
http://www.cis.upenn.edu/dbikel/software.html.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd annual meet-
ing on Association for Computational Linguistics,
pages 173–180. Association for Computational Lin-
guistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
</reference>
<page confidence="0.811916">
53
</page>
<reference confidence="0.997593971698113">
American chapter of the association for computa-
tional Linguistics conference, pages 132–139. Asso-
ciation for Computational Linguistics.
Xiao Chen and Chunyu Kit. 2012. Higher-order con-
stituent parsing and parser combination. In Pro-
ceedings of the 50th annual meeting of the Associ-
ation for Computational Linguistics: Short papers-
Volume 2, pages 1–5. Association for Computational
Linguistics.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th an-
nual meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 223–231.
Association for Computational Linguistics.
Shay B Cohen, Giorgio Satta, and Michael Collins.
2013a. Approximate pcfg parsing using tensor de-
composition. In Proceedings of NAACL-HLT, pages
487–496.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2013b. Experiments with
spectral learning of latent-variable pcfgs. In Pro-
ceedings of NAACL-HLT, pages 148–157.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Zhendong Dong and Qiang Dong. 2003. Hownet-a hy-
brid language and knowledge resource. In Proceed-
ings of the international conference on natural lan-
guage processing and knowledge engineering, pages
820–824. IEEE.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on lan-
guage and computation, 8(1):1–22.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st annual meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Dan Klein and Christopher D Manning. 2005. Parsing
and hypergraphs. In New developments in parsing
technology, pages 351–372. Springer.
Roger Levy and Christopher D Manning. 2003. Is
it harder to parse chinese, or the chinese treebank?
In Proceedings of the 41st annual meeting on As-
sociation for Computational Linguistics-Volume 1,
pages 439–446. Association for Computational Lin-
guistics.
Dongchen Li, Xiantao Zhang, and Xihong Wu. 2013.
Improved chinese parsing using named entity cue.
In Proceeding of the 13th international conference
on parsing technology, pages 45–53.
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu,
and Huisheng Chi. 2009. Refining grammars for
parsing with hierarchical semantic knowledge. In
Proceedings of the 2009 conference on empirical
methods in natural language processing: Volume 3-
Volume 3, pages 1298–1307. Association for Com-
putational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd annual meeting on Associ-
ation for Computational Linguistics, pages 75–82.
Association for Computational Linguistics.
Jia-Ju Mei, YM Li, YQ Gao, et al. 1983. Chinese
thesaurus (tong-yi-ci-ci-lin).
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human language tech-
nologies 2007: the conference of the North Amer-
ican chapter of the Association for Computational
Linguistics, pages 404–411.
Slav Petrov and Dan Klein. 2008a. Discriminative
log-linear grammars with latent variables. Advances
in neural information processing systems, 20:1153–
1160.
Slav Petrov and Dan Klein. 2008b. Sparse multi-scale
grammars for discriminative latent variable parsing.
In Proceedings of the conference on empirical meth-
ods in natural language processing, pages 867–876.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
international conference on computational linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433–440. As-
sociation for Computational Linguistics.
Slav Orlinov Petrov. 2009. Coarse-to-Fine natural
language processing. Ph.D. thesis, University of
California.
Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 501–
511. Association for Computational Linguistics.
Collins Sekine. 1997. Evalb bracket scoring program.
In http://nlp.cs.nyu.edu/evalb/.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th annual meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 440–448. Association for
Computational Linguistics.
</reference>
<page confidence="0.523476">
54
</page>
<reference confidence="0.999367814814815">
Xihong Wu, Meng Zhang, and Xiaojun Lin. 2011.
Parsing-based chinese word segmentation integrat-
ing morphological and syntactic information. In
Proceedings of 7th international conference on nat-
ural language processing and knowledge engineer-
ing (NLP-KE), pages 114–121. IEEE.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of the 19th international confer-
ence on computational linguistics-Volume 1, pages
1–8. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proceedings of the 11th Inter-
national Conference on Parsing Technologies, pages
162–171. Association for Computational Linguis-
tics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational linguistics, 37(1):105–151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
51st annual meeting of the Association for Compu-
tational Linguistics.
Qiang Zhou. 2004. Annotation scheme for chinese
treebank. Journal of Chinese information process-
ing, 18(4):1–8.
</reference>
<page confidence="0.940559">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515029">
<title confidence="0.999703">Learning Grammar with Explicit Annotations for Conjunctions</title>
<author confidence="0.72807">Dongchen Li</author>
<author confidence="0.72807">Xiantao Zhang</author>
<author confidence="0.72807">Xihong</author>
<affiliation confidence="0.851214666666667">Key Laboratory of Machine Perception and Speech and Hearing Research Peking University, Beijing,</affiliation>
<abstract confidence="0.994656551724138">Data-driven approach for parsing may suffer from data sparsity when entirely unsupervised. External knowledge has been shown to be an effective way to alleviate this problem. Subordinating conjunctions impose important constraints on Chinese syntactic structures. This paper proposes a method to develop a grammar with hierarchical category knowledge of subordinating conjunctions as explicit annotations. Firstly, each part-of-speech tag of the subordinating conjunctions is annotated with the most general category in the hierarchical knowledge. Those categories are human-defined to represent distinct syntactic constraints, and provide an appropriate starting point for splitting. Secondly, based on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Timothy Baldwin</author>
<author>David Martinez</author>
</authors>
<title>Improving parsing and pp attachment performance with sense information.</title>
<date>2008</date>
<booktitle>Proceedings ofACL08: HLT,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="3453" citStr="Agirre et al., 2008" startWordPosition="513" endWordPosition="516">ing of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics cially for Chinese. For example, the subordinating conjunction “�i`” (no matter what) is typically ahead of a sentence with pros and cons of the situation; on the con</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and pp attachment performance with sense information. Proceedings ofACL08: HLT, pages 317–325.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xavier Carreras P´erez Rapha¨el Bailly</author>
<author>Franco M Luque</author>
<author>Ariadna Julieta Quattoni</author>
</authors>
<title>Unsupervised spectral learning of wcfg as low-rank matrix completion. Association for Computational Linguistics.</title>
<marker>Rapha¨el Bailly, Luque, Quattoni, </marker>
<rawString>Rapha¨el Bailly, Xavier Carreras P´erez, Franco M Luque, and Ariadna Julieta Quattoni. Unsupervised spectral learning of wcfg as low-rank matrix completion. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Daniel Klein</author>
</authors>
<title>An allfragments grammar for simple and accurate parsing.</title>
<date>2012</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="1812" citStr="Bansal and Klein, 2012" startWordPosition="252" endWordPosition="255">ted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely marko</context>
</contexts>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Daniel Klein. 2012. An allfragments grammar for simple and accurate parsing. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bikel</author>
</authors>
<title>Dan bikel’s randomized parsing evaluation comparator.</title>
<date>2000</date>
<note>In http://www.cis.upenn.edu/dbikel/software.html.</note>
<contexts>
<context position="14572" citStr="Bikel, 2000" startWordPosition="2165" endWordPosition="2166"> They reveal that the subcategory knowledge of subordinating conjunctions is effective for parsing. 4 Experiments 4.1 Experimental Setup We present experimental results on both Chinese Treebank (CTB) 5.0 (Xue et al., 2002) (All traces and functional tags were stripped.) and Tsinghua Treebank (TCT) (Zhou, 2004). All the experiments were carried out after six cycles of splitmerge. The data set allocation is described in Table 1. We use the EVALB parseval reference implementation (Sekine, 1997) for scoring. Statistical significance was checked by Bikel’s randomized parsing evaluation comparator (Bikel, 2000). 4.2 Parsing Performance with Hierarchical Subcategories We presented a flexible approach which refines the subordinating conjunctions in a hierarchy fashion where the hierarchical layers provide different granularity of specificity. To facilitate the comparisons, we set up 6 experiments on CTB5.0 with different strategies of choosing the subcategory layers in the hierarchical subcategory knowledge: • baseline: Training without hierarchical subcategory knowledge • top: Choosing the top layer in hierarchical subcategories (using “Transition”, “Condition” , “Purpose” and so on) • bottom: Choosi</context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>Bikel. 2000. Dan bikel’s randomized parsing evaluation comparator. In http://www.cis.upenn.edu/dbikel/software.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1554" citStr="Charniak and Johnson, 2005" startWordPosition="207" endWordPosition="210">opriate starting point for splitting. Secondly, based on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approa</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the association for computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1526" citStr="Charniak, 2000" startWordPosition="205" endWordPosition="206"> provide an appropriate starting point for splitting. Secondly, based on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hie</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the association for computational Linguistics conference, pages 132–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Chen</author>
<author>Chunyu Kit</author>
</authors>
<title>Higher-order constituent parsing and parser combination.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Short</booktitle>
<volume>2</volume>
<pages>1--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1597" citStr="Chen and Kit, 2012" startWordPosition="215" endWordPosition="218">ed on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, </context>
</contexts>
<marker>Chen, Kit, 2012</marker>
<rawString>Xiao Chen and Chunyu Kit. 2012. Higher-order constituent parsing and parser combination. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Short papersVolume 2, pages 1–5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable pcfgs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>223--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2882" citStr="Cohen et al., 2012" startWordPosition="421" endWordPosition="424">9) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named en</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2012. Spectral learning of latent-variable pcfgs. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 223–231. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Giorgio Satta</author>
<author>Michael Collins</author>
</authors>
<title>Approximate pcfg parsing using tensor decomposition.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>487--496</pages>
<contexts>
<context position="2919" citStr="Cohen et al., 2013" startWordPosition="429" endWordPosition="433">al grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on</context>
</contexts>
<marker>Cohen, Satta, Collins, 2013</marker>
<rawString>Shay B Cohen, Giorgio Satta, and Michael Collins. 2013a. Approximate pcfg parsing using tensor decomposition. In Proceedings of NAACL-HLT, pages 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable pcfgs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>148--157</pages>
<contexts>
<context position="2919" citStr="Cohen et al., 2013" startWordPosition="429" endWordPosition="433">al grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2013b. Experiments with spectral learning of latent-variable pcfgs. In Proceedings of NAACL-HLT, pages 148–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1510" citStr="Collins, 1999" startWordPosition="203" endWordPosition="204">onstraints, and provide an appropriate starting point for splitting. Secondly, based on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>Hownet-a hybrid language and knowledge resource.</title>
<date>2003</date>
<booktitle>In Proceedings of the international conference on natural language processing and knowledge engineering,</booktitle>
<pages>820--824</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6377" citStr="Dong and Dong, 2003" startWordPosition="944" endWordPosition="947">al Subcategories of Subordinating Conjunction The only tag “CS” for all the various subordinating conjunctions is too coarse to indicate the intricate subordinating relationship. The words indicating different grammatical features share the same tag “CS”, such as transition relationship, progression relationship, preference relationship, purpose relationship and condition relationship. In each case, the context is different, and the subordinating conjunction is an obvious indication for the parse disambiguation for the context. The existing resources for computational linguistic, like HowNet (Dong and Dong, 2003) and Cilin (Mei et al., 1983), have classified all subordinating conjunctions as one category, which is too coarse to capture the syntactic implication. To make use of the indication, we subdivide the subordinating conjunctions according to its grammatical features in our scheme. Subordinating conjunctions indicating each relationship is further subdivided into two subcategories: one is used before the principal clause, the other is before the subordinate clause. For example, the conjunctions representing cause and effect contains “because” and “so”, where “because” should modify the cause, an</context>
</contexts>
<marker>Dong, Dong, 2003</marker>
<rawString>Zhendong Dong and Qiang Dong. 2003. Hownet-a hybrid language and knowledge resource. In Proceedings of the international conference on natural language processing and knowledge engineering, pages 820–824. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Exploiting semantic information for hpsg parse selection. Research on language and computation,</title>
<date>2010</date>
<pages>8--1</pages>
<contexts>
<context position="3432" citStr="Fujita et al., 2010" startWordPosition="509" endWordPosition="512">ounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics cially for Chinese. For example, the subordinating conjunction “�i`” (no matter what) is typically ahead of a sentence with pros and cons of the </context>
</contexts>
<marker>Fujita, Bond, Oepen, Tanaka, 2010</marker>
<rawString>Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2010. Exploiting semantic information for hpsg parse selection. Research on language and computation, 8(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1763" citStr="Klein and Manning, 2003" startWordPosition="243" endWordPosition="246">a-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-the</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs. In New developments in parsing technology,</title>
<date>2005</date>
<pages>351--372</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2052" citStr="Klein and Manning, 2005" startWordPosition="291" endWordPosition="294">ree grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a m</context>
</contexts>
<marker>Klein, Manning, 2005</marker>
<rawString>Dan Klein and Christopher D Manning. 2005. Parsing and hypergraphs. In New developments in parsing technology, pages 351–372. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>Is it harder to parse chinese, or the chinese treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>439--446</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1787" citStr="Levy and Manning, 2003" startWordPosition="247" endWordPosition="251">se categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Startin</context>
<context position="16749" citStr="Levy and Manning, 2003" startWordPosition="2501" endWordPosition="2504">best result. We also test our method on TCT. Table 2 compares the accuracies of the baseline, initialization with top subcategories and the “knowledge criterion” model, and confirms that the subcategory knowledge helps parse disambiguation. Parser P R F1 baseline 74.40 74.28 74.34 top 75.12 75.17 75.14 knowledge criterion 76.18 76.27 76.22 Table 2: Our parsing performance with different criterions on TCT. 4.3 Final Results Our final results are achieved using the “knowledge criterion” model. As we can see from the table 3, our final parsing performance is higher than the unlexicalized parser (Levy and Manning, 2003; Petrov, 2009) and the parsing system in Qian and Liu (2012), but falls short of the systems using semantic knowledge of Lin et al. (2009) and exhaustive word formation knowledge of Zhang et al. (2013). Parser P R F1 Levy(2003) 78.40 79.20 78.80 Petrov(2009) 84.82 81.93 83.33 Qian(2012) 84.57 83.68 84.13 Zhang(2013) 84.42 84.43 84.43 Lin(2009) 86.00 83.10 84.50 This paper 85.93 82.87 84.32 Table 3: Our final parsing performance compared with the best previous works on CTB5.0. The improvement on the hierarchical state-split approach verifies the effectiveness of the subcategory knowledge of su</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher D Manning. 2003. Is it harder to parse chinese, or the chinese treebank? In Proceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 439–446. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongchen Li</author>
<author>Xiantao Zhang</author>
<author>Xihong Wu</author>
</authors>
<title>Improved chinese parsing using named entity cue.</title>
<date>2013</date>
<booktitle>In Proceeding of the 13th international conference on parsing technology,</booktitle>
<pages>45--53</pages>
<contexts>
<context position="3509" citStr="Li et al., 2013" startWordPosition="524" endWordPosition="527"> al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics cially for Chinese. For example, the subordinating conjunction “�i`” (no matter what) is typically ahead of a sentence with pros and cons of the situation; on the contrary, a sufficient condition often occurs after the sub</context>
</contexts>
<marker>Li, Zhang, Wu, 2013</marker>
<rawString>Dongchen Li, Xiantao Zhang, and Xihong Wu. 2013. Improved chinese parsing using named entity cue. In Proceeding of the 13th international conference on parsing technology, pages 45–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Lin</author>
<author>Yang Fan</author>
<author>Meng Zhang</author>
<author>Xihong Wu</author>
<author>Huisheng Chi</author>
</authors>
<title>Refining grammars for parsing with hierarchical semantic knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 conference on empirical methods in natural language processing: Volume</booktitle>
<volume>3</volume>
<pages>1298--1307</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3472" citStr="Lin et al., 2009" startWordPosition="517" endWordPosition="520">e PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics cially for Chinese. For example, the subordinating conjunction “�i`” (no matter what) is typically ahead of a sentence with pros and cons of the situation; on the contrary, a sufficient</context>
<context position="10919" citStr="Lin et al., 2009" startWordPosition="1589" endWordPosition="1592">process of the tag “CS”. Each subcategory of this tag has its own word set, and corresponds to one layer at the appropriate level in the hierarchical subcategories. in the hierarchical subcategories, as a hyponym of “CS”. The hierarchical subcategories are employed in the hierarchical state-split process to impose restrictions on the subcategory refinement. First of all, it is necessary to establish the mapping from each subcategory in the data-driven hierarchical subcategories to the subcategory in the predefined hierarchical subcategories. We transfer the method for semantic-related labels (Lin et al., 2009) to our case here. The mapping is implemented with the word set related to each automatically refined granularity of clustered subordinating conjunctions and the node at the special level in the subcategory knowledge. The schematic in Figure 3 demonstrates this supervised splitting process for CS. The left part of this figure is the word sets of automatic clustered subcategories of the CS, which is split hierarchically. As expressed by the lines, each subcategory corresponds to one node in the right part of this figure, which is our hierarchical subcategory knowledge of subordinating conjuncti</context>
<context position="16888" citStr="Lin et al. (2009)" startWordPosition="2526" endWordPosition="2529">wledge criterion” model, and confirms that the subcategory knowledge helps parse disambiguation. Parser P R F1 baseline 74.40 74.28 74.34 top 75.12 75.17 75.14 knowledge criterion 76.18 76.27 76.22 Table 2: Our parsing performance with different criterions on TCT. 4.3 Final Results Our final results are achieved using the “knowledge criterion” model. As we can see from the table 3, our final parsing performance is higher than the unlexicalized parser (Levy and Manning, 2003; Petrov, 2009) and the parsing system in Qian and Liu (2012), but falls short of the systems using semantic knowledge of Lin et al. (2009) and exhaustive word formation knowledge of Zhang et al. (2013). Parser P R F1 Levy(2003) 78.40 79.20 78.80 Petrov(2009) 84.82 81.93 83.33 Qian(2012) 84.57 83.68 84.13 Zhang(2013) 84.42 84.43 84.43 Lin(2009) 86.00 83.10 84.50 This paper 85.93 82.87 84.32 Table 3: Our final parsing performance compared with the best previous works on CTB5.0. The improvement on the hierarchical state-split approach verifies the effectiveness of the subcategory knowledge of subordinating conjunctions for alleviating over-fitting. And the subcategory knowledge could be integrated with the knowledge base employed i</context>
</contexts>
<marker>Lin, Fan, Zhang, Wu, Chi, 2009</marker>
<rawString>Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu, and Huisheng Chi. 2009. Refining grammars for parsing with hierarchical semantic knowledge. In Proceedings of the 2009 conference on empirical methods in natural language processing: Volume 3-Volume 3, pages 1298–1307. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic cfg with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>75--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2076" citStr="Matsuzaki et al., 2005" startWordPosition="295" endWordPosition="298">rlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a li</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic cfg with latent annotations. In Proceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 75–82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia-Ju Mei</author>
<author>YM Li</author>
<author>YQ Gao</author>
</authors>
<date>1983</date>
<note>Chinese thesaurus (tong-yi-ci-ci-lin).</note>
<contexts>
<context position="6406" citStr="Mei et al., 1983" startWordPosition="950" endWordPosition="953">g Conjunction The only tag “CS” for all the various subordinating conjunctions is too coarse to indicate the intricate subordinating relationship. The words indicating different grammatical features share the same tag “CS”, such as transition relationship, progression relationship, preference relationship, purpose relationship and condition relationship. In each case, the context is different, and the subordinating conjunction is an obvious indication for the parse disambiguation for the context. The existing resources for computational linguistic, like HowNet (Dong and Dong, 2003) and Cilin (Mei et al., 1983), have classified all subordinating conjunctions as one category, which is too coarse to capture the syntactic implication. To make use of the indication, we subdivide the subordinating conjunctions according to its grammatical features in our scheme. Subordinating conjunctions indicating each relationship is further subdivided into two subcategories: one is used before the principal clause, the other is before the subordinate clause. For example, the conjunctions representing cause and effect contains “because” and “so”, where “because” should modify the cause, and “so” should modify the effe</context>
</contexts>
<marker>Mei, Li, Gao, 1983</marker>
<rawString>Jia-Ju Mei, YM Li, YQ Gao, et al. 1983. Chinese thesaurus (tong-yi-ci-ci-lin).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="2201" citStr="Petrov and Klein, 2007" startWordPosition="314" endWordPosition="317">Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two ste</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human language technologies 2007: the conference of the North American chapter of the Association for Computational Linguistics, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Discriminative log-linear grammars with latent variables. Advances in neural information processing systems,</title>
<date>2008</date>
<pages>20--1153</pages>
<contexts>
<context position="2225" citStr="Petrov and Klein, 2008" startWordPosition="318" endWordPosition="321">g et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spect</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008a. Discriminative log-linear grammars with latent variables. Advances in neural information processing systems, 20:1153– 1160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Sparse multi-scale grammars for discriminative latent variable parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<pages>867--876</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2225" citStr="Petrov and Klein, 2008" startWordPosition="318" endWordPosition="321">g et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spect</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008b. Sparse multi-scale grammars for discriminative latent variable parsing. In Proceedings of the conference on empirical methods in natural language processing, pages 867–876. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2177" citStr="Petrov et al., 2006" startWordPosition="310" endWordPosition="313">ang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterati</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 433–440. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Orlinov Petrov</author>
</authors>
<title>Coarse-to-Fine natural language processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California.</institution>
<contexts>
<context position="2266" citStr="Petrov, 2009" startWordPosition="326" endWordPosition="327">y takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Co</context>
<context position="16764" citStr="Petrov, 2009" startWordPosition="2505" endWordPosition="2506">t our method on TCT. Table 2 compares the accuracies of the baseline, initialization with top subcategories and the “knowledge criterion” model, and confirms that the subcategory knowledge helps parse disambiguation. Parser P R F1 baseline 74.40 74.28 74.34 top 75.12 75.17 75.14 knowledge criterion 76.18 76.27 76.22 Table 2: Our parsing performance with different criterions on TCT. 4.3 Final Results Our final results are achieved using the “knowledge criterion” model. As we can see from the table 3, our final parsing performance is higher than the unlexicalized parser (Levy and Manning, 2003; Petrov, 2009) and the parsing system in Qian and Liu (2012), but falls short of the systems using semantic knowledge of Lin et al. (2009) and exhaustive word formation knowledge of Zhang et al. (2013). Parser P R F1 Levy(2003) 78.40 79.20 78.80 Petrov(2009) 84.82 81.93 83.33 Qian(2012) 84.57 83.68 84.13 Zhang(2013) 84.42 84.43 84.43 Lin(2009) 86.00 83.10 84.50 This paper 85.93 82.87 84.32 Table 3: Our final parsing performance compared with the best previous works on CTB5.0. The improvement on the hierarchical state-split approach verifies the effectiveness of the subcategory knowledge of subordinating con</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>Slav Orlinov Petrov. 2009. Coarse-to-Fine natural language processing. Ph.D. thesis, University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Joint chinese word segmentation, pos tagging and parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>501--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16810" citStr="Qian and Liu (2012)" startWordPosition="2512" endWordPosition="2515">e accuracies of the baseline, initialization with top subcategories and the “knowledge criterion” model, and confirms that the subcategory knowledge helps parse disambiguation. Parser P R F1 baseline 74.40 74.28 74.34 top 75.12 75.17 75.14 knowledge criterion 76.18 76.27 76.22 Table 2: Our parsing performance with different criterions on TCT. 4.3 Final Results Our final results are achieved using the “knowledge criterion” model. As we can see from the table 3, our final parsing performance is higher than the unlexicalized parser (Levy and Manning, 2003; Petrov, 2009) and the parsing system in Qian and Liu (2012), but falls short of the systems using semantic knowledge of Lin et al. (2009) and exhaustive word formation knowledge of Zhang et al. (2013). Parser P R F1 Levy(2003) 78.40 79.20 78.80 Petrov(2009) 84.82 81.93 83.33 Qian(2012) 84.57 83.68 84.13 Zhang(2013) 84.42 84.43 84.43 Lin(2009) 86.00 83.10 84.50 This paper 85.93 82.87 84.32 Table 3: Our final parsing performance compared with the best previous works on CTB5.0. The improvement on the hierarchical state-split approach verifies the effectiveness of the subcategory knowledge of subordinating conjunctions for alleviating over-fitting. And th</context>
</contexts>
<marker>Qian, Liu, 2012</marker>
<rawString>Xian Qian and Yang Liu. 2012. Joint chinese word segmentation, pos tagging and parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501– 511. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collins Sekine</author>
</authors>
<title>Evalb bracket scoring program.</title>
<date>1997</date>
<note>In http://nlp.cs.nyu.edu/evalb/.</note>
<contexts>
<context position="14456" citStr="Sekine, 1997" startWordPosition="2151" endWordPosition="2152">ess of the presented approach, several experiments are conducted on both Penn Chinese Treebank and Tsinghua Treebank. They reveal that the subcategory knowledge of subordinating conjunctions is effective for parsing. 4 Experiments 4.1 Experimental Setup We present experimental results on both Chinese Treebank (CTB) 5.0 (Xue et al., 2002) (All traces and functional tags were stripped.) and Tsinghua Treebank (TCT) (Zhou, 2004). All the experiments were carried out after six cycles of splitmerge. The data set allocation is described in Table 1. We use the EVALB parseval reference implementation (Sekine, 1997) for scoring. Statistical significance was checked by Bikel’s randomized parsing evaluation comparator (Bikel, 2000). 4.2 Parsing Performance with Hierarchical Subcategories We presented a flexible approach which refines the subordinating conjunctions in a hierarchy fashion where the hierarchical layers provide different granularity of specificity. To facilitate the comparisons, we set up 6 experiments on CTB5.0 with different strategies of choosing the subcategory layers in the hierarchical subcategory knowledge: • baseline: Training without hierarchical subcategory knowledge • top: Choosing </context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Collins Sekine. 1997. Evalb bracket scoring program. In http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian symbol-refined tree substitution grammars for syntactic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>440--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2121" citStr="Shindo et al., 2012" startWordPosition="303" endWordPosition="306">ins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least usef</context>
</contexts>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th annual meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 440–448. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xihong Wu</author>
<author>Meng Zhang</author>
<author>Xiaojun Lin</author>
</authors>
<title>Parsing-based chinese word segmentation integrating morphological and syntactic information.</title>
<date>2011</date>
<booktitle>In Proceedings of 7th international conference on natural language processing and knowledge engineering (NLP-KE),</booktitle>
<pages>114--121</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3217" citStr="Wu et al., 2011" startWordPosition="476" endWordPosition="479">nt likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Baltimore, Maryland U</context>
</contexts>
<marker>Wu, Zhang, Lin, 2011</marker>
<rawString>Xihong Wu, Meng Zhang, and Xiaojun Lin. 2011. Parsing-based chinese word segmentation integrating morphological and syntactic information. In Proceedings of 7th international conference on natural language processing and knowledge engineering (NLP-KE), pages 114–121. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on computational linguistics-Volume 1,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14182" citStr="Xue et al., 2002" startWordPosition="2104" endWordPosition="2107">ugh according to the knowledge, and thus the corresponding subcategory will stop splitting. By introducing a knowledge-based criterion, the issue is settled whether or not to further split subcategories from the perspective of predefined knowledge. To investigate the effectiveness of the presented approach, several experiments are conducted on both Penn Chinese Treebank and Tsinghua Treebank. They reveal that the subcategory knowledge of subordinating conjunctions is effective for parsing. 4 Experiments 4.1 Experimental Setup We present experimental results on both Chinese Treebank (CTB) 5.0 (Xue et al., 2002) (All traces and functional tags were stripped.) and Tsinghua Treebank (TCT) (Zhou, 2004). All the experiments were carried out after six cycles of splitmerge. The data set allocation is described in Table 1. We use the EVALB parseval reference implementation (Sekine, 1997) for scoring. Statistical significance was checked by Bikel’s randomized parsing evaluation comparator (Bikel, 2000). 4.2 Parsing Performance with Hierarchical Subcategories We presented a flexible approach which refines the subordinating conjunctions in a hierarchy fashion where the hierarchical layers provide different gra</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>Nianwen Xue, Fu-Dong Chiou, and Martha Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of the 19th international conference on computational linguistics-Volume 1, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1577" citStr="Zhang and Clark, 2009" startWordPosition="211" endWordPosition="214">plitting. Secondly, based on the data-driven state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the chinese treebank using a global discriminative model. In Proceedings of the 11th International Conference on Parsing Technologies, pages 162–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2099" citStr="Zhang and Clark, 2011" startWordPosition="299" endWordPosition="302">rformance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klein, 2008a; Petrov and Klein, 2008b; Petrov, 2009) refines and generalizes the original grammars in a data-driven manner, and achieves state-of-theart performance. Starting from a completely markovized X-Bar grammar, each category is split into two subcategories. EM is initialized with this starting point and used to climb the highly nonconvex objective function of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to </context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Chinese parsing exploiting characters. 51st annual meeting of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="1618" citStr="Zhang et al., 2013" startWordPosition="219" endWordPosition="222">n state-split approach, we establish a mapping from each automatic refined subcategory to the one in the hierarchical knowledge. Then the data-driven splitting of these categories is restricted by the knowledge to avoid over refinement. Experiments demonstrate that constraining the grammar learning by the hierarchical knowledge improves parsing performance significantly over the baseline. 1 Introduction Probabilistic context-free grammars (PCFGs) underlie most of the high-performance parsers (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Zhang and Clark, 2009; Chen and Kit, 2012; Zhang et al., 2013). However, a naive PCFG which simply takes the empirical rules and probabilities off of a Treebank does not perform well (Klein and Manning, 2003; Levy and Manning, 2003; Bansal and Klein, 2012), because its context-freedom assumptions are too strong in some cases (e.g. it assumes that subject and object NPs share the same distribution). Therefore, a variety of techniques have been developed to enrich PCFG (Klein and Manning, 2005; Matsuzaki et al., 2005; Zhang and Clark, 2011; Shindo et al., 2012). Hierarchical state-split approach (Petrov et al., 2006; Petrov and Klein, 2007; Petrov and Klei</context>
<context position="3199" citStr="Zhang et al., 2013" startWordPosition="472" endWordPosition="475">of computing the joint likelihood of the observed parse trees. Then a merging step applies a likelihood ratio test to reverse the least useful half part of the splits. Learning proceeds by iterating between those two steps for six rounds. Spectral learning of latent-variable PCFGs (Cohen et al., 2012; Bailly et al., ; Cohen et al., 2013b; Cohen et al., 2013a) is another effective manner of state-split approach that provides accurate and consistent parameter estimates. However, this two complete data-driven approaches are likely to be hindered by the overfitting issue. Incorporating knowledge (Zhang et al., 2013; Wu et al., 2011) to refine the categories in training a parser has been proved to remedy the weaknesses of probabilistic context-free grammar (PCFG). The knowledge contains content words semantic resources base (Fujita et al., 2010; Agirre et al., 2008; Lin et al., 2009), named entity cues (Li et al., 2013) and so on. However, they are limited in that they do not take into account the knowledge about subordinating conjunctions. Subordinating conjunctions are important indications for different syntactic structure, espe48 Proceedings of the ACL 2014 Student Research Workshop, pages 48–55, Bal</context>
<context position="16951" citStr="Zhang et al. (2013)" startWordPosition="2536" endWordPosition="2539">owledge helps parse disambiguation. Parser P R F1 baseline 74.40 74.28 74.34 top 75.12 75.17 75.14 knowledge criterion 76.18 76.27 76.22 Table 2: Our parsing performance with different criterions on TCT. 4.3 Final Results Our final results are achieved using the “knowledge criterion” model. As we can see from the table 3, our final parsing performance is higher than the unlexicalized parser (Levy and Manning, 2003; Petrov, 2009) and the parsing system in Qian and Liu (2012), but falls short of the systems using semantic knowledge of Lin et al. (2009) and exhaustive word formation knowledge of Zhang et al. (2013). Parser P R F1 Levy(2003) 78.40 79.20 78.80 Petrov(2009) 84.82 81.93 83.33 Qian(2012) 84.57 83.68 84.13 Zhang(2013) 84.42 84.43 84.43 Lin(2009) 86.00 83.10 84.50 This paper 85.93 82.87 84.32 Table 3: Our final parsing performance compared with the best previous works on CTB5.0. The improvement on the hierarchical state-split approach verifies the effectiveness of the subcategory knowledge of subordinating conjunctions for alleviating over-fitting. And the subcategory knowledge could be integrated with the knowledge base employed in Lin et al. (2009) and Zhang et al. (2013) to contribute more </context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. 51st annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Zhou</author>
</authors>
<title>Annotation scheme for chinese treebank.</title>
<date>2004</date>
<journal>Journal of Chinese information processing,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="14271" citStr="Zhou, 2004" startWordPosition="2119" endWordPosition="2120">introducing a knowledge-based criterion, the issue is settled whether or not to further split subcategories from the perspective of predefined knowledge. To investigate the effectiveness of the presented approach, several experiments are conducted on both Penn Chinese Treebank and Tsinghua Treebank. They reveal that the subcategory knowledge of subordinating conjunctions is effective for parsing. 4 Experiments 4.1 Experimental Setup We present experimental results on both Chinese Treebank (CTB) 5.0 (Xue et al., 2002) (All traces and functional tags were stripped.) and Tsinghua Treebank (TCT) (Zhou, 2004). All the experiments were carried out after six cycles of splitmerge. The data set allocation is described in Table 1. We use the EVALB parseval reference implementation (Sekine, 1997) for scoring. Statistical significance was checked by Bikel’s randomized parsing evaluation comparator (Bikel, 2000). 4.2 Parsing Performance with Hierarchical Subcategories We presented a flexible approach which refines the subordinating conjunctions in a hierarchy fashion where the hierarchical layers provide different granularity of specificity. To facilitate the comparisons, we set up 6 experiments on CTB5.0</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Qiang Zhou. 2004. Annotation scheme for chinese treebank. Journal of Chinese information processing, 18(4):1–8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>