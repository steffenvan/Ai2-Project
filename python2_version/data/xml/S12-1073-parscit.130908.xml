<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055280">
<title confidence="0.970903">
ICT:A System Combination for Chinese Semantic Dependency Parsing
</title>
<author confidence="0.842095">
Hao Xiong and Qun Liu
</author>
<affiliation confidence="0.900420666666667">
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.752596">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.998068">
{xionghao, liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999341111111111">
The goal of semantic dependency parsing is to
build dependency structure and label seman-
tic relation between a head and its modifier.
To attain this goal, we concentrate on obtain-
ing better dependency structure to predict bet-
ter semantic relations, and propose a method
to combine the results of three state-of-the-art
dependency parsers. Unfortunately, we made
a mistake when we generate the final output
that results in a lower score of 56.31% in term
of Labeled Attachment Score (LAS), reported
by organizers. After giving golden testing set,
we fix the bug and rerun the evaluation script,
this time we obtain the score of 62.8% which
is consistent with the results on developing set.
We will report detailed experimental results
with correct program as a comparison stan-
dard for further research.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999671692307692">
In this year’s Semantic Evaluation Task, the organiz-
ers hold a task for Chinese Semantic Dependency
Parsing. The semantic dependency parsing (SDP)
is a kind of dependency parsing. It builds a depen-
dency structure for a sentence and labels the seman-
tic relation between a head and its modifier. The
semantic relations are different from syntactic rela-
tions. They are position independent, e.g., the pa-
tient can be before or behind a predicate. On the
other hand, their grains are finer than syntactic re-
lations, e.g., the syntactic subject can be agent or
experiencer. Readers can refer to (Wanxiang Che,
2012) for detailed introduction.
</bodyText>
<figureCaption confidence="0.987664333333333">
Figure 1: The pipeline of our system, where we com-
bine the results of three dependency parsers and use max-
entropy classifier to predict the semantic relations.
</figureCaption>
<bodyText confidence="0.9971615">
Different from most methods proposed in
CoNLL-2008 1 and 2009 2, in which some re-
searchers build a joint model to simultaneously gen-
erate dependency structure and its syntactic relations
(Surdeanu et al., 2008; Hajiˇc et al., 2009), here,
we first employ several parsers to generate depen-
dency structure and then propose a method to com-
bine their outputs. After that, we label relation be-
tween each head and its modifier via the traversal
of this refined parse tree. The reason why we use
a pipeline model while not a joint model is that
the number of semantic relations annotated by or-
ganizers is more than 120 types, while in the for-
mer task is only 21 types. Compared to the former
task, the large number of types will obviously drop
the performance of classifier. On the other hand, the
performance of syntactic dependency parsing is ap-
proaching to perfect, intuitively, that better depen-
dency structure does help to semantic parsing, thus
we can concentrate on improving the accuracy of de-
pendency structure construction.
The overall framework of our system is illustrated
</bodyText>
<footnote confidence="0.99998">
1http://www.yr-bcn.es/conll2008/
2http://ufal.mff.cuni.cz/conll2009-st/
</footnote>
<page confidence="0.895609">
514
</page>
<note confidence="0.5430765">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 514–518,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999746583333333">
in figure 1, where three dependency parsers are em-
ployed to generate the dependency structure, and a
maximum entropy classifier is used to predict rela-
tion for head and its modifier over combined parse
tree. Final experimental results show that our sys-
tem achieves 80.45% in term of unlabeled attach-
ment score (UAS), and 62.8 % in term of LAS. Both
of them are higher than the baseline without using
system combinational techniques.
In the following of this paper, we will demonstrate
the detailed information of our system, and report
several experimental results.
</bodyText>
<sectionHeader confidence="0.972121" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999893571428571">
As mentioned, we employ three single dependency
parsers to generate respect dependency structure. To
further improve the accuracy of dependency struc-
ture construction, we blend the syntactic outputs and
find a better dependency structure. In the followings,
we will first introduce the details of our strategy for
dependency structure construction.
</bodyText>
<subsectionHeader confidence="0.98327">
2.1 Parsers
</subsectionHeader>
<bodyText confidence="0.9969788">
We implement three transition-based dependency
parsers with three different parsing algorithms:
Nivre’s arc standard, Nivre’s arc eager (see Nivre
(2004) for a comparison between the two Nivre al-
gorithms), and Liang’s dynamic algorithm(Huang
and Sagae, 2010). We use these algorithms for
several reasons: first, they are easy to implement
and their reported performance are approaching to
state-of-the-art. Second, their outputs are projective,
which is consistent with given corpus.
</bodyText>
<subsectionHeader confidence="0.997725">
2.2 Parser Combination
</subsectionHeader>
<bodyText confidence="0.999881266666667">
We use the similar method presented in Hall et al.
(2011) to advance the accuracy of parses. The parses
of each sentence are combined into a weighted di-
rected graph. The left procedure is similar to tradi-
tional graph-based dependency parsing except that
the number of edges in our system is smaller since
we reserve best edges predicted by three single
parsers. We use the popular Chu-Liu-Edmonds al-
gorithm (Chu and Liu, 1965; Edmonds et al., 1968)
to find the maximum spanning tree (MST) of the
new constructed graph, which is considered as the
final parse of the sentence. Specifically, we use the
parsing accuracy on developing set to represent the
weight of graph edge. Formally, the weight of graph
edge is computed as follows,
</bodyText>
<equation confidence="0.9913705">
�w� = Accuracy(p) · I(e, p) (1)
PEP
</equation>
<bodyText confidence="0.999966777777778">
where the Accuracy(p) is the parsing score of
parse tree p whose value is the score of parsing accu-
racy on developing set, and I(e, p) is an indicator, if
there is such dependency in parse tree p, it returns 1,
otherwise returns 0. Since the value of Accuracy(p)
ranges from 0 to 1, we doesn’t need to normalize its
value.
Thus, the detailed procedure for dependency
structure construction is,
</bodyText>
<listItem confidence="0.981737875">
• Parsing each sentence using Nivre’s arc stan-
dard, Nivre’s arc eager and Liang’s dynamic al-
gorithm, respectively.
• Combining parses outputted by three parsers
into weighted directed graph, and representing
its weight using equation 1.
• Using Chu-Liu-Edmonds algorithm to search
final parse for each sentence.
</listItem>
<subsectionHeader confidence="0.868246">
2.3 Features for Labeling
</subsectionHeader>
<bodyText confidence="0.999963266666667">
After given dependency structure, for each relation
between head and its modifier, we extract 31 types
of features, which are typically exploited in syntac-
tic dependency parsing, as our basic features. Based
on these basic features, we also add a additional dis-
tance metric for each features and obtain 31 types of
distance incorporated features. Besides that, we use
greedy hill climbing approach to select additional 29
features to obtain better performance. Table 1 shows
the basic features used in our system,
And the table 2 gives the additional features. It
is worth mentioning, that the distance is calculated
as the difference between the head and its modifier,
which is different from the calculation reported by
most literatures.
</bodyText>
<subsectionHeader confidence="0.889299">
2.4 Classifier
</subsectionHeader>
<bodyText confidence="0.9982695">
We use the classifier from Le Zhang’s Maximum
Entropy Modeling Toolkit3 and use the L-BFGS
</bodyText>
<footnote confidence="0.9967985">
3http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit
.html
</footnote>
<page confidence="0.970587">
515
</page>
<table confidence="0.999865178571429">
Features
mw:modifier’s word
mp:modifier’s POS tag
Basic hw:head’s word
hp:head’s POS tag
hw|hp,mw|mp,hw|mw
hp|mp,hw|mp,hp|mw
hw|hp|mw
hw|hp|mp
hw|mw|mp
hp|mw|mp
hp|mp|mp-1
hp|mp|mp+1
hp|hp-1|mp
hp|hp+1|mp
hp|hp-1|mp-1
Combination hp|hp-1|mp+1
hp|hp+1|mp-1
hp|hp+1|mp+1
hp-1|mp|mp-1
hp-1|mp|mp+1
hp+1|mp|mp-1
hp+1|mp|mp+1
hw|hp|mw|mp
hp|hp-1|mp|mp-1
hp|hp+1|mp|mp+1
hp|hp+1|mp|mp-1
hp|hp-1|mp|mp+1
</table>
<tableCaption confidence="0.9853685">
Table 1: The basic features used in our system. -1 and
+1 indicate the one on the left and right of given word.
</tableCaption>
<bodyText confidence="0.99993175">
parameter estimation algorithm with gaussian prior
smoothing(Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 1000 iter-
ations according to the previous experience.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999922875">
The given corpus consists of 8301 sentences
for training(TR), and 569 sentences for develop-
ing(DE). For tuning parameters, we just use TR por-
tion, while for testing, we combine two parts and
retrain the parser to obtain better results. Surely, we
also give results of testing set trained on TR portion
for comparison. In the following of this section, we
will report the detailed experimental results both on
</bodyText>
<table confidence="0.999425684210526">
Features
Distance dist:basic features with distance
Additional lmw:leftmost word of modifier
rnw:rightnearest word of modifier
gfw:grandfather of modifier
lmp,rnp,gfp
lmw|lmp,rnw|rnp,lmw|rnw
lmp|rnp,lmw|mw,lmp|mp
rnw|mw,rnp|mp,gfw|mw
gfp|mp,gfw|hw,gfp|hp
gfw|mw|gfp|mp
lmw|lmp|mw|mp
rnw|rnp|mw|mp
lmw|rnw|mw,lmp|rnp|mp
gfw|hw|gfp|hp
gfw|mw|hw,gfp|mp|hp
gfw|mw|hw|gfp|mp|hp
lmw|rnw|lmp|rnp|mw|mp
lmw|rnw|lmp|rnp
</table>
<tableCaption confidence="0.8636025">
Table 2: The additional features used in our system.
developing and testing set.
</tableCaption>
<subsectionHeader confidence="0.954548">
3.1 Results on Developing Set
</subsectionHeader>
<bodyText confidence="0.999245333333333">
We first report the accuracy of dependency construc-
tion on developing set using different parsing al-
gorithms in table 3. Note that, the features used
in our system are similar to that used in their pub-
lished papers(Nivre, 2003; Nivre, 2004; Huang and
Sagae, 2010). From table 3 we find that although
</bodyText>
<table confidence="0.999606">
Precision (%)
Nivre’s arc standard 78.86
Nivre’s arc eager 79.11
Liang’s dynamic 79.78
System Combination 80.85
</table>
<tableCaption confidence="0.992955">
Table 3: Syntactic precision of different parsers on devel-
oping set.
</tableCaption>
<bodyText confidence="0.999542166666667">
using simple method for combination over three sin-
gle parsers, the system combination technique still
achieves 1.1 points improvement over the highest
single system. Since the Liang’s algorithm is a dy-
namic algorithm, which enlarges the searching space
in decoding, while the former two Nivre’s arc al-
</bodyText>
<page confidence="0.995459">
516
</page>
<bodyText confidence="0.999299444444444">
gorithms actually still are simple beam search al-
gorithm, thus the Liang’s algorithm achieves better
performance than Nivre’s two algorithm, which is
consistent with the experiments in Liang’s paper.
To acknowledge that the better dependency struc-
ture does help to semantic relation labeling, we fur-
ther predict semantic relations on different depen-
dency structures. For comparison, we also report the
performance on golden structure. Since our combi-
</bodyText>
<table confidence="0.999896666666667">
Precision (%)
Nivre’s arc standard 60.84
Nivre’s arc eager 60.76
Liang’s dynamic 61.43
System Combination 62.92
Golden Tree 76.63
</table>
<tableCaption confidence="0.9877385">
Table 4: LAS of semantic relations over different parses
on developing set.
</tableCaption>
<bodyText confidence="0.9999481875">
national algorithm requires weight for each edges,
we use the developing parsing accuracy 0.7886,
0.7911, and 0.7978 as corresponding weights for
each single system. Table 4 shows, that the pre-
diction of semantic relation could benefit from the
improvement of dependency structure. We also no-
tice that even given the golden parse tree, the per-
formance of relation labeling is still far from per-
fect. Two reasons could be explained for that: first
is the small size of supplied corpus, second is that
the relation between head and its modifier is too
fine-grained to distinguish for a classifier. More-
over, here we use golden segmentation for parsing,
imagining that an automatic segmenter would fur-
ther drop the accuracy both on syntactic and seman-
tic parsing.
</bodyText>
<subsectionHeader confidence="0.994236">
3.2 Results on Testing Set
</subsectionHeader>
<bodyText confidence="0.999789142857143">
Since there is a bug4 in our final results submitted
to organizers, here, in order to confirm the improve-
ment of our method and supply comparison standard
for further research, we reevaluate the correct output
and report its performance on different training set.
Table 5 and table 6 give the results trained on dif-
ferent corpus. We can see that when increasing the
</bodyText>
<footnote confidence="0.8611725">
4The bug is come from that when we converting the CoNLL-
styled outputs generated by our combination system into plain
text. While in developing stage, we directly used CoNLL-styled
outputs as our input, thus we didn’t realize this mistake.
</footnote>
<bodyText confidence="0.99955225">
training size, the performance is slightly improved.
Also, we find the results on testing set is consistent
with that on developing set, where best dependency
structure achieves the best performance.
</bodyText>
<table confidence="0.9995265">
LAS (%) UAS(%)
Nivre’s arc standard 60.38 78.19
Nivre’s arc eager 60.78 78.62
Liang’s dynamic 60.85 79.09
System Combination 62.76 80.23
Submitted Error Results 55.26 71.85
</table>
<tableCaption confidence="0.985242">
Table 5: LAS and UAS on testing set trained on TR.
</tableCaption>
<table confidence="0.999831333333333">
LAS (%) UAS(%)
Nivre’s arc standard 60.49 78.25
Nivre’s arc eager 60.99 78.78
Liang’s dynamic 61.29 79.59
System Combination 62.80 80.45
Submitted Error Results 56.31 73.20
</table>
<tableCaption confidence="0.9139165">
Table 6: LAS and UAS on testing set trained on TR and
DE.
</tableCaption>
<sectionHeader confidence="0.997369" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999938777777778">
In this paper, we demonstrate our system framework
for Chinese Semantic Dependency Parsing, and re-
port the experiments with different configurations.
We propose to use system combination to better the
dependency structure construction, and then label
semantic relations over refined parse tree. Final ex-
periments show that better syntactic parsing do help
to improve the accuracy of semantic relation predic-
tion.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999915666666667">
The authors were supported by National Science
Foundation of China, Contracts 90920004, and
High-Technology R&amp;D Program (863) Project No
2011AA01A207 and 2012BAH39B03. We thank
Heng Yu for generating parse tree using Liang’s al-
gorithm. We thank organizers for their generous
supplied resources and arduous preparation. We also
thank anonymous reviewers for their thoughtful sug-
gestions.
</bodyText>
<page confidence="0.993845">
517
</page>
<sectionHeader confidence="0.995853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99938688372093">
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14(1396-
1400):270.
J. Edmonds, J. Edmonds, and J. Edmonds. 1968. Opti-
mum branchings. National Bureau of standards.
J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o,
J. ˇStˇep´anek, et al. 2009. The conll-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1–18. Association for Computa-
tional Linguistics.
J. Hall, J. Nilsson, and J. Nivre. 2011. Single malt or
blended? a study in multilingual parser optimization.
Trends in Parsing Technology, pages 19–33.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1077–1086. Association
for Computational Linguistics.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT. Cite-
seer.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proceedings of the Workshop on In-
cremental Parsing: Bringing Engineering and Cogni-
tion Together, pages 50–57. Association for Computa-
tional Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The conll-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159–177.
Association for Computational Linguistics.
Ting Liu Wanxiang Che. 2012. Semeval-2012 Task 5:
Chinese Semantic Dependency Parsing. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval2012).
</reference>
<page confidence="0.993851">
518
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.185619">
<title confidence="0.999551">ICT:A System Combination for Chinese Semantic Dependency Parsing</title>
<author confidence="0.603981">Hao Xiong</author>
<author confidence="0.603981">Qun</author>
<keyword confidence="0.470021">Key Lab. of Intelligent Information</keyword>
<affiliation confidence="0.974317">Institute of Computing</affiliation>
<address confidence="0.923166">Chinese Academy of P.O. Box 2704, Beijing 100190,</address>
<abstract confidence="0.988434368421053">The goal of semantic dependency parsing is to build dependency structure and label semantic relation between a head and its modifier. To attain this goal, we concentrate on obtaining better dependency structure to predict better semantic relations, and propose a method to combine the results of three state-of-the-art dependency parsers. Unfortunately, we made a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers. After giving golden testing set, we fix the bug and rerun the evaluation script, this time we obtain the score of 62.8% which is consistent with the results on developing set. We will report detailed experimental results with correct program as a comparison standard for further research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical report,</tech>
<pages>99--108</pages>
<contexts>
<context position="7670" citStr="Chen and Rosenfeld, 1999" startWordPosition="1182" endWordPosition="1185">0736/maxent toolkit .html 515 Features mw:modifier’s word mp:modifier’s POS tag Basic hw:head’s word hp:head’s POS tag hw|hp,mw|mp,hw|mw hp|mp,hw|mp,hp|mw hw|hp|mw hw|hp|mp hw|mw|mp hp|mw|mp hp|mp|mp-1 hp|mp|mp+1 hp|hp-1|mp hp|hp+1|mp hp|hp-1|mp-1 Combination hp|hp-1|mp+1 hp|hp+1|mp-1 hp|hp+1|mp+1 hp-1|mp|mp-1 hp-1|mp|mp+1 hp+1|mp|mp-1 hp+1|mp|mp+1 hw|hp|mw|mp hp|hp-1|mp|mp-1 hp|hp+1|mp|mp+1 hp|hp+1|mp|mp-1 hp|hp-1|mp|mp+1 Table 1: The basic features used in our system. -1 and +1 indicate the one on the left and right of given word. parameter estimation algorithm with gaussian prior smoothing(Chen and Rosenfeld, 1999). We set the gaussian prior to 2 and train the model in 1000 iterations according to the previous experience. 3 Experiments The given corpus consists of 8301 sentences for training(TR), and 569 sentences for developing(DE). For tuning parameters, we just use TR portion, while for testing, we combine two parts and retrain the parser to obtain better results. Surely, we also give results of testing set trained on TR portion for comparison. In the following of this section, we will report the detailed experimental results both on Features Distance dist:basic features with distance Additional lmw:</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian prior for smoothing maximum entropy models. Technical report, CMU-CS-99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<pages>14--1396</pages>
<contexts>
<context position="5088" citStr="Chu and Liu, 1965" startWordPosition="793" endWordPosition="796">y are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing except that the number of edges in our system is smaller since we reserve best edges predicted by three single parsers. We use the popular Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds et al., 1968) to find the maximum spanning tree (MST) of the new constructed graph, which is considered as the final parse of the sentence. Specifically, we use the parsing accuracy on developing set to represent the weight of graph edge. Formally, the weight of graph edge is computed as follows, �w� = Accuracy(p) · I(e, p) (1) PEP where the Accuracy(p) is the parsing score of parse tree p whose value is the score of parsing accuracy on developing set, and I(e, p) is an indicator, if there is such dependency in parse tree p, it returns 1, otherwise returns 0. Since the value of Accur</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14(1396-1400):270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
<author>J Edmonds</author>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings. National Bureau of standards.</title>
<date>1968</date>
<contexts>
<context position="5111" citStr="Edmonds et al., 1968" startWordPosition="797" endWordPosition="800">ment and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing except that the number of edges in our system is smaller since we reserve best edges predicted by three single parsers. We use the popular Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds et al., 1968) to find the maximum spanning tree (MST) of the new constructed graph, which is considered as the final parse of the sentence. Specifically, we use the parsing accuracy on developing set to represent the weight of graph edge. Formally, the weight of graph edge is computed as follows, �w� = Accuracy(p) · I(e, p) (1) PEP where the Accuracy(p) is the parsing score of parse tree p whose value is the score of parsing accuracy on developing set, and I(e, p) is an indicator, if there is such dependency in parse tree p, it returns 1, otherwise returns 0. Since the value of Accuracy(p) ranges from 0 to</context>
</contexts>
<marker>Edmonds, Edmonds, Edmonds, 1968</marker>
<rawString>J. Edmonds, J. Edmonds, and J. Edmonds. 1968. Optimum branchings. National Bureau of standards.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Marti</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J ˇStˇep´anek</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, 2009</marker>
<rawString>J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, et al. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
</authors>
<title>Single malt or blended? a study in multilingual parser optimization. Trends in Parsing Technology,</title>
<date>2011</date>
<pages>pages</pages>
<contexts>
<context position="4720" citStr="Hall et al. (2011)" startWordPosition="731" endWordPosition="734">gy for dependency structure construction. 2.1 Parsers We implement three transition-based dependency parsers with three different parsing algorithms: Nivre’s arc standard, Nivre’s arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang’s dynamic algorithm(Huang and Sagae, 2010). We use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing except that the number of edges in our system is smaller since we reserve best edges predicted by three single parsers. We use the popular Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds et al., 1968) to find the maximum spanning tree (MST) of the new constructed graph, which is considered as the final parse of the sentence. Specifically, we use the parsing accuracy on developing set to represent the weigh</context>
</contexts>
<marker>Hall, Nilsson, Nivre, 2011</marker>
<rawString>J. Hall, J. Nilsson, and J. Nivre. 2011. Single malt or blended? a study in multilingual parser optimization. Trends in Parsing Technology, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4414" citStr="Huang and Sagae, 2010" startWordPosition="685" endWordPosition="688">on As mentioned, we employ three single dependency parsers to generate respect dependency structure. To further improve the accuracy of dependency structure construction, we blend the syntactic outputs and find a better dependency structure. In the followings, we will first introduce the details of our strategy for dependency structure construction. 2.1 Parsers We implement three transition-based dependency parsers with three different parsing algorithms: Nivre’s arc standard, Nivre’s arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang’s dynamic algorithm(Huang and Sagae, 2010). We use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing except that the number of edges in our system is smaller since we reserve best edges predicted by three singl</context>
<context position="8988" citStr="Huang and Sagae, 2010" startWordPosition="1367" endWordPosition="1370">gfp lmw|lmp,rnw|rnp,lmw|rnw lmp|rnp,lmw|mw,lmp|mp rnw|mw,rnp|mp,gfw|mw gfp|mp,gfw|hw,gfp|hp gfw|mw|gfp|mp lmw|lmp|mw|mp rnw|rnp|mw|mp lmw|rnw|mw,lmp|rnp|mp gfw|hw|gfp|hp gfw|mw|hw,gfp|mp|hp gfw|mw|hw|gfp|mp|hp lmw|rnw|lmp|rnp|mw|mp lmw|rnw|lmp|rnp Table 2: The additional features used in our system. developing and testing set. 3.1 Results on Developing Set We first report the accuracy of dependency construction on developing set using different parsing algorithms in table 3. Note that, the features used in our system are similar to that used in their published papers(Nivre, 2003; Nivre, 2004; Huang and Sagae, 2010). From table 3 we find that although Precision (%) Nivre’s arc standard 78.86 Nivre’s arc eager 79.11 Liang’s dynamic 79.78 System Combination 80.85 Table 3: Syntactic precision of different parsers on developing set. using simple method for combination over three single parsers, the system combination technique still achieves 1.1 points improvement over the highest single system. Since the Liang’s algorithm is a dynamic algorithm, which enlarges the searching space in decoding, while the former two Nivre’s arc al516 gorithms actually still are simple beam search algorithm, thus the Liang’s al</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT.</booktitle>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8951" citStr="Nivre, 2003" startWordPosition="1363" endWordPosition="1364">ather of modifier lmp,rnp,gfp lmw|lmp,rnw|rnp,lmw|rnw lmp|rnp,lmw|mw,lmp|mp rnw|mw,rnp|mp,gfw|mw gfp|mp,gfw|hw,gfp|hp gfw|mw|gfp|mp lmw|lmp|mw|mp rnw|rnp|mw|mp lmw|rnw|mw,lmp|rnp|mp gfw|hw|gfp|hp gfw|mw|hw,gfp|mp|hp gfw|mw|hw|gfp|mp|hp lmw|rnw|lmp|rnp|mw|mp lmw|rnw|lmp|rnp Table 2: The additional features used in our system. developing and testing set. 3.1 Results on Developing Set We first report the accuracy of dependency construction on developing set using different parsing algorithms in table 3. Note that, the features used in our system are similar to that used in their published papers(Nivre, 2003; Nivre, 2004; Huang and Sagae, 2010). From table 3 we find that although Precision (%) Nivre’s arc standard 78.86 Nivre’s arc eager 79.11 Liang’s dynamic 79.78 System Combination 80.85 Table 3: Syntactic precision of different parsers on developing set. using simple method for combination over three single parsers, the system combination technique still achieves 1.1 points improvement over the highest single system. Since the Liang’s algorithm is a dynamic algorithm, which enlarges the searching space in decoding, while the former two Nivre’s arc al516 gorithms actually still are simple beam </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>J. Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4309" citStr="Nivre (2004)" startWordPosition="671" endWordPosition="672">detailed information of our system, and report several experimental results. 2 System Description As mentioned, we employ three single dependency parsers to generate respect dependency structure. To further improve the accuracy of dependency structure construction, we blend the syntactic outputs and find a better dependency structure. In the followings, we will first introduce the details of our strategy for dependency structure construction. 2.1 Parsers We implement three transition-based dependency parsers with three different parsing algorithms: Nivre’s arc standard, Nivre’s arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang’s dynamic algorithm(Huang and Sagae, 2010). We use these algorithms for several reasons: first, they are easy to implement and their reported performance are approaching to state-of-the-art. Second, their outputs are projective, which is consistent with given corpus. 2.2 Parser Combination We use the similar method presented in Hall et al. (2011) to advance the accuracy of parses. The parses of each sentence are combined into a weighted directed graph. The left procedure is similar to traditional graph-based dependency parsing exce</context>
<context position="8964" citStr="Nivre, 2004" startWordPosition="1365" endWordPosition="1366">fier lmp,rnp,gfp lmw|lmp,rnw|rnp,lmw|rnw lmp|rnp,lmw|mw,lmp|mp rnw|mw,rnp|mp,gfw|mw gfp|mp,gfw|hw,gfp|hp gfw|mw|gfp|mp lmw|lmp|mw|mp rnw|rnp|mw|mp lmw|rnw|mw,lmp|rnp|mp gfw|hw|gfp|hp gfw|mw|hw,gfp|mp|hp gfw|mw|hw|gfp|mp|hp lmw|rnw|lmp|rnp|mw|mp lmw|rnw|lmp|rnp Table 2: The additional features used in our system. developing and testing set. 3.1 Results on Developing Set We first report the accuracy of dependency construction on developing set using different parsing algorithms in table 3. Note that, the features used in our system are similar to that used in their published papers(Nivre, 2003; Nivre, 2004; Huang and Sagae, 2010). From table 3 we find that although Precision (%) Nivre’s arc standard 78.86 Nivre’s arc eager 79.11 Liang’s dynamic 79.78 System Combination 80.85 Table 3: Syntactic precision of different parsers on developing set. using simple method for combination over three single parsers, the system combination technique still achieves 1.1 points improvement over the highest single system. Since the Liang’s algorithm is a dynamic algorithm, which enlarges the searching space in decoding, while the former two Nivre’s arc al516 gorithms actually still are simple beam search algori</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>J. Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The conll-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu Wanxiang Che</author>
</authors>
<title>Semeval-2012 Task 5: Chinese Semantic Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval2012).</booktitle>
<contexts>
<context position="1697" citStr="Che, 2012" startWordPosition="267" endWordPosition="268"> In this year’s Semantic Evaluation Task, the organizers hold a task for Chinese Semantic Dependency Parsing. The semantic dependency parsing (SDP) is a kind of dependency parsing. It builds a dependency structure for a sentence and labels the semantic relation between a head and its modifier. The semantic relations are different from syntactic relations. They are position independent, e.g., the patient can be before or behind a predicate. On the other hand, their grains are finer than syntactic relations, e.g., the syntactic subject can be agent or experiencer. Readers can refer to (Wanxiang Che, 2012) for detailed introduction. Figure 1: The pipeline of our system, where we combine the results of three dependency parsers and use maxentropy classifier to predict the semantic relations. Different from most methods proposed in CoNLL-2008 1 and 2009 2, in which some researchers build a joint model to simultaneously generate dependency structure and its syntactic relations (Surdeanu et al., 2008; Hajiˇc et al., 2009), here, we first employ several parsers to generate dependency structure and then propose a method to combine their outputs. After that, we label relation between each head and its </context>
</contexts>
<marker>Che, 2012</marker>
<rawString>Ting Liu Wanxiang Che. 2012. Semeval-2012 Task 5: Chinese Semantic Dependency Parsing. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval2012).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>