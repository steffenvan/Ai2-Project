<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003640">
<note confidence="0.68434275">
LIMSI: Learning Semantic Similarity by Selecting Random Word Subsets
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay, France
</note>
<email confidence="0.991767">
artem.sokolov@limsi.fr
</email>
<sectionHeader confidence="0.995517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9992045">
We propose a semantic similarity learning
method based on Random Indexing (RI) and
ranking with boosting. Unlike classical RI, we
use only those context vector features that are
informative for the semantics modeled. De-
spite ignoring text preprocessing and dispens-
ing with semantic resources, the approach was
ranked as high as 22nd among 89 participants
in the SemEval-2012 Task6: Semantic Textual
Similarity.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907545454546">
One of the popular and flexible tools of semantics
modeling are vector distributional representations of
texts (also known as vector space models, seman-
tic word spaces or distributed representations). The
principle idea behind vector space models is to use
word usage statistics in different contexts to gen-
erate a high-dimensional vector representations for
each word. Words are represented by context vec-
tors whose closeness in the vector space is postu-
lated to reflect semantic similarity (Sahlgren, 2005).
The approach rests upon the distributional hypothe-
sis: words with similar meanings or functions tend
to appear in similar contexts. The prominent ex-
amples of vector space models are Latent Seman-
tic Analysis (or Indexing) (Landauer and Dutnais,
1997) and Random Indexing (Kanerva et al., 2000).
Because of the heuristic nature of distributional
methods, they are often designed with a specific
semantic relation in mind (synonymy, paraphrases,
contradiction, etc.). This complicates their adaption
to other application domains and tasks, requiring
manual trial-and-error feature redesigns and tailored
preprocessing steps to remove morphology/syntax
variations that are not supposed to contribute to the
semantics facet in question (e.g., stemming, stop-
words). Further, assessing closeness of semantic
vectors is usually based on a fixed simple similarity
function between distributed representations (often,
the cosine function). The cosine function implicitly
assigns equal weights to each component of the se-
mantic vectors regardless of its importance for the
particular semantic relation and task. Finally, dur-
ing production of training and evaluation sets, the
continuum of possible grades of semantic similar-
ity is usually substituted with several integer values,
although often only the relative grade order matters
and not their absolute values. Trying to reproduce
the same values or the same gaps between grades
when designing a semantic representation scheme
may introduce an unnecessary bias.
In this paper we address all of the above draw-
backs and present a semantic similarity learning
method based on Random Indexing. It does not re-
quire manual feature design, and is automatically
adapted to the specific semantic relations by select-
ing needed important features and/or learning neces-
sary feature transformations before calculating sim-
ilarity. In the proof-of-concept experiments on the
SemEval-2012 data we deliberately ignored all rou-
tine preprocessing steps, that are often considered
obligatory in semantic text processing, we did not
use any of the semantic resources (like WordNet)
nor trained different models for different data do-
mains/types. Despite such over-constrained setting,
the method showed very positive performance and
</bodyText>
<page confidence="0.980935">
543
</page>
<note confidence="0.537073">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 543–546,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.961384">
was ranked as high as 22nd among 89 participants.
</bodyText>
<sectionHeader confidence="0.943764" genericHeader="method">
2 Random Indexing
</sectionHeader>
<bodyText confidence="0.999404548387097">
Random Indexing (RI) is an alternative to LSA-
like models with large co-occurrence matrices and
separate matrix decomposition phase to reduce di-
mension. RI constructs context vectors on-the-fly
based on the occurrence of words in contexts. First,
each word is assigned a unique and randomly gener-
ated high-dimensional sparse ternary vector. Vec-
tors contain a small number (between 0.1-1%) of
randomly distributed +1s and -1s, with the rest of
the elements set to 0. Next, the final context vectors
for words are produced by scanning through the text
with a sliding window of fixed size, and each time
the word occurs in the text, the generated vectors of
all its neighbors in the sliding context window are
added to the context vector of this word1. Finally,
the obtained context vectors are normalized by the
occurrence count of the word.
RI is a practical variant of the well-known
dimension reduction technique of the Johnson-
Lindenstrauss (JL) lemma (Dasgupta and Gupta,
2003). An Euclidean space can be projected with a
random Gaussian matrix R onto smaller dimension
Euclidean space, such that with high probability the
distance between any pair of points in the new space
is within a distortion factor of 1 ± e of their origi-
nal distance. Same or similar guarantees also hold
for a uniform {−1, +1}-valued or ternary (from a
certain distribution) random R (Achlioptas, 2003) or
for even sparser matrices (Dasgupta et al., 2010)
Restating the JL-lemma in the RI-terminology,
one can think of the initial space of characteristic
vectors of word sets of all contexts (each compo-
nent counts corresponding words seen in the context
window over the corpus) embedded into a smaller
dimension space, and approximately preserving dis-
tances between characteristic vectors. Because
of the ternary generation scheme, each resulting
feature-vector dimension either rewards, penalizes
or “switches off” certain words for which the cor-
responding row of R contained, resp., +1, −1 or 0.
So far, RI has been a naive approach to feature
1Although decreasing discounts dampening contribution of
far-located context words may by beneficial, we do not use it
putting our method in more difficult conditions.
learning – although it produces low-dimensional
feature representations, it is unconscious of the
learning task behind. There is no guarantee that the
Euclidean distance (or cosine similarity) will cor-
rectly reflect the necessary semantic relation: for a
pair of vectors, not all word subsets are characteris-
tic of a particular semantic relation or specific to it,
as presence or absence of certain words may play no
role in assessing given similarity type. Implications
of RI in the context of learning textual similarity
are coming from the feature selection (equivalently,
word subset selection) method, based on boosting,
that selects only those features that are informative
for the semantic relation being learned (Section 4).
Thus, the supervision information on sentence simi-
larity guides the choose of word subsets (among all
randomly generated by the projection matrix) that
happen to be relevant to the semantic annotations.
</bodyText>
<sectionHeader confidence="0.990574" genericHeader="method">
3 Semantic Textual Similarity Task
</sectionHeader>
<bodyText confidence="0.9999815">
Let {(si�, si�)} be the training set of N pairs of sen-
tences, provided along with similarity labels yi. The
higher the value of yi the more semantically similar
is the pair (si�, si�). Usually absolute values of yi are
chosen arbitrary; only their relative order matters.
We would learn semantic similarity between
(si�, si�) as a function H(xi), where xi is a sin-
gle vector combining sentence context vectors v(si�)
and v(si�). Context representation v(s) for a sen-
tence s is defined as an average of the word context
vectors v(w) contained in it, found using a large text
corpus with the RI approach, described in the pre-
vious section: v(s) = EwE. v(w)/ |s|. Possible
transformations into xi include a concatenation of
v(si�) and v(si�), concatenation of the sum and dif-
ference vectors or a vector composed of component-
wise symmetric functions (e.g., a product of cor-
responding components). In order to learn a sym-
metric H, one can either use each pair twice during
training, or symmetrize the construction of x.
</bodyText>
<sectionHeader confidence="0.994559" genericHeader="method">
4 Feature Selection with Boosting
</sectionHeader>
<bodyText confidence="0.9993326">
We propose to exploit natural ordering of (si�, si�)
according to yi to learn a parameterized similarity
function H(xi). In this way we do not try learn-
ing the absolute values of similarity provided in the
training. Also, by using boosting approach we allow
</bodyText>
<page confidence="0.990702">
544
</page>
<bodyText confidence="0.99989095">
for gradual inclusion of features into similarity func-
tion H, implementing in this way feature selection.
For a given number of training steps T, a boost-
ing ranking algorithm learns a scoring function H,
which is a linear combination of T simple, non-
linear functions ht called weak learners: H(x) =
ETt=1 αtht(x), where each αt is the weight assigned
to ht at step t of the learning process.
Usually the weak learner is defined on only few
components of x. Having build H at step t, the next
in turn (t + 1)’s leaner is selected, optimized and
weighted with the corresponding coefficient αt+1.
In this way the learning process selects only those
features in x� (or, if viewed from the RI perspective,
random word subsets) that contribute most to learn-
ing the desired type input similarity.
As the first ranking method we applied the pair-
wise ranking algorithm RankBoost (Freund et al.,
2003), that learns H by minimizing a convex ap-
proximation to a weighted pair-wise loss:
</bodyText>
<equation confidence="0.9780855">
� P(i, j)QH(V) &gt; H(tj)].
(si1,si2),(sj1,sj2):yi&lt;yj
</equation>
<bodyText confidence="0.992462166666667">
Operator QA] = 1 if the A = true and 0 other-
wise. Positive values of P weight pairs of xi and Xj
– the higher is P(i, j), the more important it is to
preserve the relative ordering of xi and xj. We used
the simplest decision stumps that depend on one fea-
ture as weak learners: h(x; 0, k) = Qxk &gt; 0], where
k is a feature index and 0 is a learned threshold.
The second ranking method we used was a point-
wise ranking algorithm, based on gradient boosting
regression for ranking (Zheng et al., 2007), called
RtRank and implemented by Mohan et al. (2011)2.
The loss optimized by RtRank is slightly different:
</bodyText>
<equation confidence="0.985513">
� (max{0, H(xi) − H(xj)})2.
(si1,si2),(sj1,sj2):yi&lt;yj
</equation>
<bodyText confidence="0.9998636">
Another difference is in the method for selecting
weak learner at each boosting step, that relies on re-
gression loss and not scalar product as RankBoost.
Weak learners for RtRank were regression trees of
fixed depth (4 in our experiments).
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99484">
We learned context vectors on the GigaWord En-
glish corpus. The only preprocessing of the cor-
</bodyText>
<footnote confidence="0.914385">
2http://sites.google.com/site/rtranking
</footnote>
<table confidence="0.998383333333334">
learner transform correl. Q
pure RI, cos - 0.264 0.005
logistic reg. - 0.508 0.041
logistic reg. concat 0.537 0.052
RankBoost sumdiff 0.685 0.027
product 0.663 0.018
crossprod 0.648 0.028
crossdiff 0.643 0.023
concat 0.625 0.025
absdiff 0.602 0.021
RtRank sumdiff 0.730 0.020
product 0.721 0.023
</table>
<tableCaption confidence="0.9983375">
Table 1: Mean performance of the transformation and
boosting methods for N = 100 on train data.
</tableCaption>
<bodyText confidence="0.99971255">
pus was stripping all tag data, removing punctuation
and lowercasing. Stop-words were not removed.
Context vectors were built with the JavaSDM pack-
age (Hassel, 2004)3 of dimensionality N = 100 and
N = 105, resp., for preliminary and final experi-
ments, with random degree 10 (five +1s and -1s in
each initial vector), right and left context window
size of 4 words4 and constant weighting scheme.
Training and test data provided in the SemEval-
2012 Task 6 contained 5 training and 5 testing text
sets each of different domains or types of sentences
(short video descriptions, pairs of outputs of a ma-
chine translation system, etc.). Although the 5 sets
had very different characteristics, we concatenated
all training files and trained a single model. The
principal evaluation metrics was Pearson correlation
coefficient, that we report here. Two related other
measures were also used (Agirre et al., 2012).
Obtained sentence vectors v(8) for were trans-
formed into vectors x� with several methods:
</bodyText>
<listItem confidence="0.9023775">
• ‘sumdiff’: x� = (�v(81) + v(82),sgn(v1(81) −
v1(82))(v(81) − v(82)))
• ‘concat’: x� = (v(81), v(82)), and V =
(v(82), v(81))
• ‘product’: xi = vi(81) · vi(82)
• ‘crossprod’: xij = vi(81) · vj(82)
• ‘crossdiff’: xij = vi(81) − vj(82)
• ‘absdiff’: xi = |vi(81) − vi(82)|.
Methods ‘concat’ and ‘sumdiff’ were proposed
by Hertz et al. (2004) for distance learning for clus-
</listItem>
<footnote confidence="0.9993575">
3http://www.csc.kth.se/—xmartin/java
4Little sensitivity was found to the window sizes from 3 to 6.
</footnote>
<figure confidence="0.660065">
boosting
baseline
</figure>
<page confidence="0.942777">
545
</page>
<bodyText confidence="0.5032708">
learner transform train±o, test rank MSRpar MSRvid SMTeur OnWN SMTnews
RankBoost product 0.748±0.017 0.6392 32 0.3948 0.6597 0.0143 0.4157 0.2889
sumdiff 0.735±0.016 0.6196 45 0.4295 0.5724 0.2842 0.3989 0.2575
product 0.784±0.017 0.6789 22 0.4848 0.6636 0.0934 0.3706 0.2455
sumdiff 0.763±0.014
</bodyText>
<tableCaption confidence="0.993689">
Table 2: Mean performance of the best-performing two transformation and two boosting methods for N = 105.
</tableCaption>
<bodyText confidence="0.973032913043478">
RtRank
tering. Comparison of mean performance of differ-
ent transformation and learning methods on the 5-
fold splitting of the training set is given in Table 1
for short context vectors (N = 100). The correlation
is given for the optimal algorithms’ parameters (T
for RankBoost and, additionally, tree depth and ran-
dom ratio for RtRank), found with cross-validation
on 5 folds. With these results for small N, two trans-
formation methods were preselected (‘sumdiff’ and
‘product’) for testing and submission with N = 105
(Table 2), as increasing N usually increased perfor-
mance. Yet, only about 103 features were actually
selected by RankBoost, meaning that a relatively
few random word subsets were informative for ap-
proximating semantic textual similarity.
In result, RtRank showed better performance,
most likely because of more powerful learners, that
depend on several features (word subsets) simulta-
neously. Performance on machine translation test
sets was the lowest that can be explained by very
poor quality of the training data5: models for these
subsets should have been trained separately.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999749">
We presented a semantic similarity learning ap-
proach that learns a similarity function specific to
the semantic relation modeled and that selects only
those word subsets in RI, presence of which in the
compared sentences is indicative of their similarity,
by using only relative order of the labels and not
their absolute values. In spite of paying no atten-
tion to preprocessing, nor using semantic corpora,
and with no domain adaptation the method showed
promising results.
</bodyText>
<sectionHeader confidence="0.998234" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8825545">
This work has been funded by OSEO under the Quaero
program.
</bodyText>
<footnote confidence="0.9098365">
5A reviewer suggested another reason: more varied or even
incorrect lexical choice that is sometimes found in MT output.
</footnote>
<sectionHeader confidence="0.996759" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998379475">
Dimitris Achlioptas. 2003. Database-friendly random
projections: Johnson-Lindenstrauss with binary coins.
Comput. Syst. Sci., 66:671–687.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez. 2012. Semeval-2012 task 6: A pilot on semantic
textual similarity. In Proc. of the Int. Workshop on Se-
mantic Evaluation (SemEval 2012) // Joint Conf. on
Lexical &amp; Computational Semantics (*SEM 2012).
Sanjoy Dasgupta and Anupam Gupta. 2003. An elemen-
tary proof of a theorem of Johnson and Lindenstrauss.
Random Struct. Algorithms, 22(1):60–65.
Anirban Dasgupta, Ravi Kumar, and Tam´as Sarlos. 2010.
A sparse Johnson-Lindenstrauss transform. In Proc. of
the ACM Symp. on Theory of Comput., pages 341–350.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. Mach.Learn.Res., 4:933–969.
Martin Hassel. 2004. JavaSDM - a Java package for
working with Random Indexing and Granska.
Tomer Hertz, Aharon Bar-hillel, and Daphna Weinshall.
2004. Boosting margin based distance functions for
clustering. In ICML, pages 393–400.
Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent se-
mantic analysis. In Proc. of the Conf. of the Cogn.
Science Society.
Thomas K. Landauer and Susan T. Dutnais. 1997. A so-
lution to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychol. Rev., pages 211–240.
Ananth Mohan, Zheng Chen, and Kilian Q. Weinberger.
2011. Web-search ranking with initialized gradient
boosted regression trees. Mach.Learn.Res., 14:77–89.
Magnus Sahlgren. 2005. An introduction to random in-
dexing. In Workshop on Methods &amp; Applic. of Sem.
Indexing // Int. Conf. on Terminol. &amp; Knowl. Eng.
Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier
Chapelle, Keke Chen, and Gordon Sun. 2007. A
general boosting method and its application to learn-
ing ranking functions for web search. In NIPS.
</reference>
<page confidence="0.998477">
546
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.143714">
<title confidence="0.9198745">LIMSI: Learning Semantic Similarity by Selecting Random Word Subsets Artem</title>
<address confidence="0.460815">B.P. 133, 91403 Orsay,</address>
<email confidence="0.993077">artem.sokolov@limsi.fr</email>
<abstract confidence="0.907445181818182">We propose a semantic similarity learning method based on Random Indexing (RI) and ranking with boosting. Unlike classical RI, we use only those context vector features that are informative for the semantics modeled. Despite ignoring text preprocessing and dispensing with semantic resources, the approach was ranked as high as 22nd among 89 participants in the SemEval-2012 Task6: Semantic Textual Similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dimitris Achlioptas</author>
</authors>
<title>Database-friendly random projections: Johnson-Lindenstrauss with binary coins.</title>
<date>2003</date>
<journal>Comput. Syst. Sci.,</journal>
<pages>66--671</pages>
<contexts>
<context position="4965" citStr="Achlioptas, 2003" startWordPosition="748" endWordPosition="749">obtained context vectors are normalized by the occurrence count of the word. RI is a practical variant of the well-known dimension reduction technique of the JohnsonLindenstrauss (JL) lemma (Dasgupta and Gupta, 2003). An Euclidean space can be projected with a random Gaussian matrix R onto smaller dimension Euclidean space, such that with high probability the distance between any pair of points in the new space is within a distortion factor of 1 ± e of their original distance. Same or similar guarantees also hold for a uniform {−1, +1}-valued or ternary (from a certain distribution) random R (Achlioptas, 2003) or for even sparser matrices (Dasgupta et al., 2010) Restating the JL-lemma in the RI-terminology, one can think of the initial space of characteristic vectors of word sets of all contexts (each component counts corresponding words seen in the context window over the corpus) embedded into a smaller dimension space, and approximately preserving distances between characteristic vectors. Because of the ternary generation scheme, each resulting feature-vector dimension either rewards, penalizes or “switches off” certain words for which the corresponding row of R contained, resp., +1, −1 or 0. So </context>
</contexts>
<marker>Achlioptas, 2003</marker>
<rawString>Dimitris Achlioptas. 2003. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Comput. Syst. Sci., 66:671–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proc. of the Int. Workshop on Semantic Evaluation (SemEval 2012) // Joint Conf. on Lexical &amp; Computational Semantics (*SEM</booktitle>
<contexts>
<context position="11425" citStr="Agirre et al., 2012" startWordPosition="1810" endWordPosition="1813"> and -1s in each initial vector), right and left context window size of 4 words4 and constant weighting scheme. Training and test data provided in the SemEval2012 Task 6 contained 5 training and 5 testing text sets each of different domains or types of sentences (short video descriptions, pairs of outputs of a machine translation system, etc.). Although the 5 sets had very different characteristics, we concatenated all training files and trained a single model. The principal evaluation metrics was Pearson correlation coefficient, that we report here. Two related other measures were also used (Agirre et al., 2012). Obtained sentence vectors v(8) for were transformed into vectors x� with several methods: • ‘sumdiff’: x� = (�v(81) + v(82),sgn(v1(81) − v1(82))(v(81) − v(82))) • ‘concat’: x� = (v(81), v(82)), and V = (v(82), v(81)) • ‘product’: xi = vi(81) · vi(82) • ‘crossprod’: xij = vi(81) · vj(82) • ‘crossdiff’: xij = vi(81) − vj(82) • ‘absdiff’: xi = |vi(81) − vi(82)|. Methods ‘concat’ and ‘sumdiff’ were proposed by Hertz et al. (2004) for distance learning for clus3http://www.csc.kth.se/—xmartin/java 4Little sensitivity was found to the window sizes from 3 to 6. boosting baseline 545 learner transfor</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proc. of the Int. Workshop on Semantic Evaluation (SemEval 2012) // Joint Conf. on Lexical &amp; Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Anupam Gupta</author>
</authors>
<title>An elementary proof of a theorem of Johnson and Lindenstrauss. Random Struct.</title>
<date>2003</date>
<journal>Algorithms,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4564" citStr="Dasgupta and Gupta, 2003" startWordPosition="678" endWordPosition="681">in a small number (between 0.1-1%) of randomly distributed +1s and -1s, with the rest of the elements set to 0. Next, the final context vectors for words are produced by scanning through the text with a sliding window of fixed size, and each time the word occurs in the text, the generated vectors of all its neighbors in the sliding context window are added to the context vector of this word1. Finally, the obtained context vectors are normalized by the occurrence count of the word. RI is a practical variant of the well-known dimension reduction technique of the JohnsonLindenstrauss (JL) lemma (Dasgupta and Gupta, 2003). An Euclidean space can be projected with a random Gaussian matrix R onto smaller dimension Euclidean space, such that with high probability the distance between any pair of points in the new space is within a distortion factor of 1 ± e of their original distance. Same or similar guarantees also hold for a uniform {−1, +1}-valued or ternary (from a certain distribution) random R (Achlioptas, 2003) or for even sparser matrices (Dasgupta et al., 2010) Restating the JL-lemma in the RI-terminology, one can think of the initial space of characteristic vectors of word sets of all contexts (each com</context>
</contexts>
<marker>Dasgupta, Gupta, 2003</marker>
<rawString>Sanjoy Dasgupta and Anupam Gupta. 2003. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Struct. Algorithms, 22(1):60–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anirban Dasgupta</author>
<author>Ravi Kumar</author>
<author>Tam´as Sarlos</author>
</authors>
<title>A sparse Johnson-Lindenstrauss transform.</title>
<date>2010</date>
<booktitle>In Proc. of the ACM Symp. on Theory of Comput.,</booktitle>
<pages>341--350</pages>
<contexts>
<context position="5018" citStr="Dasgupta et al., 2010" startWordPosition="755" endWordPosition="758">ccurrence count of the word. RI is a practical variant of the well-known dimension reduction technique of the JohnsonLindenstrauss (JL) lemma (Dasgupta and Gupta, 2003). An Euclidean space can be projected with a random Gaussian matrix R onto smaller dimension Euclidean space, such that with high probability the distance between any pair of points in the new space is within a distortion factor of 1 ± e of their original distance. Same or similar guarantees also hold for a uniform {−1, +1}-valued or ternary (from a certain distribution) random R (Achlioptas, 2003) or for even sparser matrices (Dasgupta et al., 2010) Restating the JL-lemma in the RI-terminology, one can think of the initial space of characteristic vectors of word sets of all contexts (each component counts corresponding words seen in the context window over the corpus) embedded into a smaller dimension space, and approximately preserving distances between characteristic vectors. Because of the ternary generation scheme, each resulting feature-vector dimension either rewards, penalizes or “switches off” certain words for which the corresponding row of R contained, resp., +1, −1 or 0. So far, RI has been a naive approach to feature 1Althoug</context>
</contexts>
<marker>Dasgupta, Kumar, Sarlos, 2010</marker>
<rawString>Anirban Dasgupta, Ravi Kumar, and Tam´as Sarlos. 2010. A sparse Johnson-Lindenstrauss transform. In Proc. of the ACM Symp. on Theory of Comput., pages 341–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>Mach.Learn.Res.,</journal>
<pages>4--933</pages>
<contexts>
<context position="8951" citStr="Freund et al., 2003" startWordPosition="1398" endWordPosition="1401">alled weak learners: H(x) = ETt=1 αtht(x), where each αt is the weight assigned to ht at step t of the learning process. Usually the weak learner is defined on only few components of x. Having build H at step t, the next in turn (t + 1)’s leaner is selected, optimized and weighted with the corresponding coefficient αt+1. In this way the learning process selects only those features in x� (or, if viewed from the RI perspective, random word subsets) that contribute most to learning the desired type input similarity. As the first ranking method we applied the pairwise ranking algorithm RankBoost (Freund et al., 2003), that learns H by minimizing a convex approximation to a weighted pair-wise loss: � P(i, j)QH(V) &gt; H(tj)]. (si1,si2),(sj1,sj2):yi&lt;yj Operator QA] = 1 if the A = true and 0 otherwise. Positive values of P weight pairs of xi and Xj – the higher is P(i, j), the more important it is to preserve the relative ordering of xi and xj. We used the simplest decision stumps that depend on one feature as weak learners: h(x; 0, k) = Qxk &gt; 0], where k is a feature index and 0 is a learned threshold. The second ranking method we used was a pointwise ranking algorithm, based on gradient boosting regression fo</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Mach.Learn.Res., 4:933–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Hassel</author>
</authors>
<title>JavaSDM - a Java package for working with Random Indexing and Granska.</title>
<date>2004</date>
<contexts>
<context position="10687" citStr="Hassel, 2004" startWordPosition="1690" endWordPosition="1691">ssing of the cor2http://sites.google.com/site/rtranking learner transform correl. Q pure RI, cos - 0.264 0.005 logistic reg. - 0.508 0.041 logistic reg. concat 0.537 0.052 RankBoost sumdiff 0.685 0.027 product 0.663 0.018 crossprod 0.648 0.028 crossdiff 0.643 0.023 concat 0.625 0.025 absdiff 0.602 0.021 RtRank sumdiff 0.730 0.020 product 0.721 0.023 Table 1: Mean performance of the transformation and boosting methods for N = 100 on train data. pus was stripping all tag data, removing punctuation and lowercasing. Stop-words were not removed. Context vectors were built with the JavaSDM package (Hassel, 2004)3 of dimensionality N = 100 and N = 105, resp., for preliminary and final experiments, with random degree 10 (five +1s and -1s in each initial vector), right and left context window size of 4 words4 and constant weighting scheme. Training and test data provided in the SemEval2012 Task 6 contained 5 training and 5 testing text sets each of different domains or types of sentences (short video descriptions, pairs of outputs of a machine translation system, etc.). Although the 5 sets had very different characteristics, we concatenated all training files and trained a single model. The principal ev</context>
</contexts>
<marker>Hassel, 2004</marker>
<rawString>Martin Hassel. 2004. JavaSDM - a Java package for working with Random Indexing and Granska.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomer Hertz</author>
<author>Aharon Bar-hillel</author>
<author>Daphna Weinshall</author>
</authors>
<title>Boosting margin based distance functions for clustering.</title>
<date>2004</date>
<booktitle>In ICML,</booktitle>
<pages>393--400</pages>
<contexts>
<context position="11856" citStr="Hertz et al. (2004)" startWordPosition="1885" endWordPosition="1888">files and trained a single model. The principal evaluation metrics was Pearson correlation coefficient, that we report here. Two related other measures were also used (Agirre et al., 2012). Obtained sentence vectors v(8) for were transformed into vectors x� with several methods: • ‘sumdiff’: x� = (�v(81) + v(82),sgn(v1(81) − v1(82))(v(81) − v(82))) • ‘concat’: x� = (v(81), v(82)), and V = (v(82), v(81)) • ‘product’: xi = vi(81) · vi(82) • ‘crossprod’: xij = vi(81) · vj(82) • ‘crossdiff’: xij = vi(81) − vj(82) • ‘absdiff’: xi = |vi(81) − vi(82)|. Methods ‘concat’ and ‘sumdiff’ were proposed by Hertz et al. (2004) for distance learning for clus3http://www.csc.kth.se/—xmartin/java 4Little sensitivity was found to the window sizes from 3 to 6. boosting baseline 545 learner transform train±o, test rank MSRpar MSRvid SMTeur OnWN SMTnews RankBoost product 0.748±0.017 0.6392 32 0.3948 0.6597 0.0143 0.4157 0.2889 sumdiff 0.735±0.016 0.6196 45 0.4295 0.5724 0.2842 0.3989 0.2575 product 0.784±0.017 0.6789 22 0.4848 0.6636 0.0934 0.3706 0.2455 sumdiff 0.763±0.014 Table 2: Mean performance of the best-performing two transformation and two boosting methods for N = 105. RtRank tering. Comparison of mean performance</context>
</contexts>
<marker>Hertz, Bar-hillel, Weinshall, 2004</marker>
<rawString>Tomer Hertz, Aharon Bar-hillel, and Daphna Weinshall. 2004. Boosting margin based distance functions for clustering. In ICML, pages 393–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
<author>Jan Kristoferson</author>
<author>Anders Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proc. of the Conf. of the Cogn.</booktitle>
<publisher>Science Society.</publisher>
<contexts>
<context position="1384" citStr="Kanerva et al., 2000" startWordPosition="202" endWordPosition="205">d representations). The principle idea behind vector space models is to use word usage statistics in different contexts to generate a high-dimensional vector representations for each word. Words are represented by context vectors whose closeness in the vector space is postulated to reflect semantic similarity (Sahlgren, 2005). The approach rests upon the distributional hypothesis: words with similar meanings or functions tend to appear in similar contexts. The prominent examples of vector space models are Latent Semantic Analysis (or Indexing) (Landauer and Dutnais, 1997) and Random Indexing (Kanerva et al., 2000). Because of the heuristic nature of distributional methods, they are often designed with a specific semantic relation in mind (synonymy, paraphrases, contradiction, etc.). This complicates their adaption to other application domains and tasks, requiring manual trial-and-error feature redesigns and tailored preprocessing steps to remove morphology/syntax variations that are not supposed to contribute to the semantics facet in question (e.g., stemming, stopwords). Further, assessing closeness of semantic vectors is usually based on a fixed simple similarity function between distributed represen</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Pentti Kanerva, Jan Kristoferson, and Anders Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proc. of the Conf. of the Cogn. Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dutnais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychol. Rev.,</journal>
<pages>211--240</pages>
<contexts>
<context position="1341" citStr="Landauer and Dutnais, 1997" startWordPosition="195" endWordPosition="198"> space models, semantic word spaces or distributed representations). The principle idea behind vector space models is to use word usage statistics in different contexts to generate a high-dimensional vector representations for each word. Words are represented by context vectors whose closeness in the vector space is postulated to reflect semantic similarity (Sahlgren, 2005). The approach rests upon the distributional hypothesis: words with similar meanings or functions tend to appear in similar contexts. The prominent examples of vector space models are Latent Semantic Analysis (or Indexing) (Landauer and Dutnais, 1997) and Random Indexing (Kanerva et al., 2000). Because of the heuristic nature of distributional methods, they are often designed with a specific semantic relation in mind (synonymy, paraphrases, contradiction, etc.). This complicates their adaption to other application domains and tasks, requiring manual trial-and-error feature redesigns and tailored preprocessing steps to remove morphology/syntax variations that are not supposed to contribute to the semantics facet in question (e.g., stemming, stopwords). Further, assessing closeness of semantic vectors is usually based on a fixed simple simil</context>
</contexts>
<marker>Landauer, Dutnais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychol. Rev., pages 211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananth Mohan</author>
<author>Zheng Chen</author>
<author>Kilian Q Weinberger</author>
</authors>
<title>Web-search ranking with initialized gradient boosted regression trees.</title>
<date>2011</date>
<journal>Mach.Learn.Res.,</journal>
<pages>14--77</pages>
<contexts>
<context position="9635" citStr="Mohan et al. (2011)" startWordPosition="1527" endWordPosition="1530">d pair-wise loss: � P(i, j)QH(V) &gt; H(tj)]. (si1,si2),(sj1,sj2):yi&lt;yj Operator QA] = 1 if the A = true and 0 otherwise. Positive values of P weight pairs of xi and Xj – the higher is P(i, j), the more important it is to preserve the relative ordering of xi and xj. We used the simplest decision stumps that depend on one feature as weak learners: h(x; 0, k) = Qxk &gt; 0], where k is a feature index and 0 is a learned threshold. The second ranking method we used was a pointwise ranking algorithm, based on gradient boosting regression for ranking (Zheng et al., 2007), called RtRank and implemented by Mohan et al. (2011)2. The loss optimized by RtRank is slightly different: � (max{0, H(xi) − H(xj)})2. (si1,si2),(sj1,sj2):yi&lt;yj Another difference is in the method for selecting weak learner at each boosting step, that relies on regression loss and not scalar product as RankBoost. Weak learners for RtRank were regression trees of fixed depth (4 in our experiments). 5 Experiments We learned context vectors on the GigaWord English corpus. The only preprocessing of the cor2http://sites.google.com/site/rtranking learner transform correl. Q pure RI, cos - 0.264 0.005 logistic reg. - 0.508 0.041 logistic reg. concat 0</context>
</contexts>
<marker>Mohan, Chen, Weinberger, 2011</marker>
<rawString>Ananth Mohan, Zheng Chen, and Kilian Q. Weinberger. 2011. Web-search ranking with initialized gradient boosted regression trees. Mach.Learn.Res., 14:77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Workshop on Methods &amp; Applic. of Sem. Indexing // Int. Conf. on Terminol. &amp; Knowl. Eng.</booktitle>
<contexts>
<context position="1090" citStr="Sahlgren, 2005" startWordPosition="158" endWordPosition="159"> high as 22nd among 89 participants in the SemEval-2012 Task6: Semantic Textual Similarity. 1 Introduction One of the popular and flexible tools of semantics modeling are vector distributional representations of texts (also known as vector space models, semantic word spaces or distributed representations). The principle idea behind vector space models is to use word usage statistics in different contexts to generate a high-dimensional vector representations for each word. Words are represented by context vectors whose closeness in the vector space is postulated to reflect semantic similarity (Sahlgren, 2005). The approach rests upon the distributional hypothesis: words with similar meanings or functions tend to appear in similar contexts. The prominent examples of vector space models are Latent Semantic Analysis (or Indexing) (Landauer and Dutnais, 1997) and Random Indexing (Kanerva et al., 2000). Because of the heuristic nature of distributional methods, they are often designed with a specific semantic relation in mind (synonymy, paraphrases, contradiction, etc.). This complicates their adaption to other application domains and tasks, requiring manual trial-and-error feature redesigns and tailor</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Workshop on Methods &amp; Applic. of Sem. Indexing // Int. Conf. on Terminol. &amp; Knowl. Eng.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Zheng</author>
<author>Hongyuan Zha</author>
<author>Tong Zhang</author>
<author>Olivier Chapelle</author>
<author>Keke Chen</author>
<author>Gordon Sun</author>
</authors>
<title>A general boosting method and its application to learning ranking functions for web search.</title>
<date>2007</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9581" citStr="Zheng et al., 2007" startWordPosition="1518" endWordPosition="1521">ns H by minimizing a convex approximation to a weighted pair-wise loss: � P(i, j)QH(V) &gt; H(tj)]. (si1,si2),(sj1,sj2):yi&lt;yj Operator QA] = 1 if the A = true and 0 otherwise. Positive values of P weight pairs of xi and Xj – the higher is P(i, j), the more important it is to preserve the relative ordering of xi and xj. We used the simplest decision stumps that depend on one feature as weak learners: h(x; 0, k) = Qxk &gt; 0], where k is a feature index and 0 is a learned threshold. The second ranking method we used was a pointwise ranking algorithm, based on gradient boosting regression for ranking (Zheng et al., 2007), called RtRank and implemented by Mohan et al. (2011)2. The loss optimized by RtRank is slightly different: � (max{0, H(xi) − H(xj)})2. (si1,si2),(sj1,sj2):yi&lt;yj Another difference is in the method for selecting weak learner at each boosting step, that relies on regression loss and not scalar product as RankBoost. Weak learners for RtRank were regression trees of fixed depth (4 in our experiments). 5 Experiments We learned context vectors on the GigaWord English corpus. The only preprocessing of the cor2http://sites.google.com/site/rtranking learner transform correl. Q pure RI, cos - 0.264 0.</context>
</contexts>
<marker>Zheng, Zha, Zhang, Chapelle, Chen, Sun, 2007</marker>
<rawString>Zhaohui Zheng, Hongyuan Zha, Tong Zhang, Olivier Chapelle, Keke Chen, and Gordon Sun. 2007. A general boosting method and its application to learning ranking functions for web search. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>