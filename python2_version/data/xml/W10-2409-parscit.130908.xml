<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001787">
<title confidence="0.980097">
Reranking with Multiple Features for Better Transliteration
</title>
<author confidence="0.987472">
Yan Song† Chunyu Kit† Hai Zhao‡††Department of Chinese, Translation and Linguistics
</author>
<affiliation confidence="0.997449333333333">
City University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong
‡Department of Computer Science and Engineering
Shanghai Jiao Tong University, #800, Dongchuan Rd, Shanghai, China
</affiliation>
<email confidence="0.998234">
{yansong,ctckit}@cityu.edu.hk, zhaohai@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.993889" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998191875">
Effective transliteration of proper names
via grapheme conversion needs to find
transliteration patterns in training data,
and then generate optimized candidates
for testing samples accordingly. However,
the top-1 accuracy for the generated candi-
dates cannot be good if the right one is not
ranked at the top. To tackle this issue, we
propose to rerank the output candidates for
a better order using the averaged percep-
tron with multiple features. This paper de-
scribes our recent work in this direction for
our participation in NEWS2010 transliter-
ation evaluation. The official results con-
firm its effectiveness in English-Chinese
bidirectional transliteration.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951404761905">
Since transliteration can be considered a direct or-
thographic mapping process, one may adopt gen-
eral statistical machine translation (SMT) proce-
dures for its implementation. Aimed at finding
phonetic equivalence in another language for a
given named entity, however, different translitera-
tion options with different syllabification may gen-
erate multiple choices with the symphonic form
for the same source text. Consequently, even the
overall results by SMT output are acceptable, it
is still unreliable to rank the candidates simply by
their statistical translation scores for the purpose
of selecting the best one. In order to make a proper
choice, the direct orthographic mapping requires a
precise alignment and a better transliteration op-
tion selection. Thus, powerful algorithms for ef-
fective use of the parallel data is indispensable, es-
pecially when the available data is limited in vol-
ume.
Interestingly, although an SMT based approach
could not achieve a precise top-1 transliteration re-
sult, it is found in (Song et al., 2009) that, in con-
trast to the ordinary top-1 accuracy (ACC) score,
its recall rate, which is defined in terms of whether
the correct answer is generated in the n-best output
list, is rather high. This observation suggests that
if we could rearrange those outputs into a better
order, especially, push the correct one to the top,
the overall performance could be enhanced signif-
icantly, without any further refinement of the orig-
inal generation process. This reranking strategy is
proved to be efficient in transliteration generation
with a multi-engine approach (Oh et al., 2009).
In this paper, we present our recent work on
reranking the transliteration candidates via an on-
line discriminative learning framework, namely,
the averaged perceptron. Multiple features are in-
corporated into it for performance enhancement.
The following sections will give the technical de-
tails of our method and present its results for
NEWS2010 shared task for named entity translit-
eration.
</bodyText>
<sectionHeader confidence="0.988997" genericHeader="introduction">
2 Generation
</sectionHeader>
<bodyText confidence="0.99714375">
For the generation of transliteration candidates,
we follow the work (Song et al., 2009), using a
phrase-based SMT procedure with the log-linear
model
</bodyText>
<equation confidence="0.993694666666667">
exp[�n i�1 Aihi(s, t)]
P(t|s) = (1)
Et exp[EZ 1 Aihi(s, t)]
</equation>
<bodyText confidence="0.999899625">
for decoding. Originally we use two directional
phrase1 tables, which are learned for both direc-
tions of source-to-target and target-to-source, con-
taining different entries of transliteration options.
In order to facilitate the decoding by exploiting all
possible choices in a better way, we combine the
forward and backward directed phrase tables to-
gether, and recalculate the probability for each en-
</bodyText>
<footnote confidence="0.9937805">
1It herein refers to a character sequence as described in
(Song et al., 2009).
</footnote>
<page confidence="0.982007">
62
</page>
<note confidence="0.6361825">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 62–65,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999972692307692">
try in it. After that, we use a phoneme resource2 to
refine the phrase table by filtering out the wrongly
extracted phrases and cleaning up the noise in it.
In the decoding process, a dynamic pruning is per-
formed when generating the hypothesis in each
step, in which the threshold is variable according
to the current searching space, for we need to ob-
tain a good candidate list as precise as possible
for the next stage. The parameter for each fea-
ture function in log-linear model is optimized by
MERT training (Och, 2003). Finally, a maximum
number of 50 candidates are generated for each
source name.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="method">
3 Reranking
</sectionHeader>
<subsectionHeader confidence="0.999815">
3.1 Learning Framework
</subsectionHeader>
<bodyText confidence="0.999979">
For reranking training and prediction, we adopt
the averaged perceptron (Collins, 2002) as our
learning framework, which has a more stable per-
formance than the non-averaged version. It is pre-
sented in Algorithm 1. Where ω~ is the vector of
parameters we want to optimize, x, y are the cor-
responding source (with different syllabification)
and target graphemes in the candidate list, and Φ
represents the feature vector in the pair of x and
y. In this algorithm, reference y∗i is the most ap-
propriate output in the candidate list according to
the true target named entity in the training data.
We use the Mean-F score to identify which candi-
date can be the reference, by locating the one with
the maximum Mean-F score value. This process
updates the parameters of the feature vector and
also relocate all of the candidates according to the
ranking scores, which are calculated in terms of
the resulted parameters in each round of training
as well as in the testing process. The number of
iteration for the final model is determined by the
development data.
</bodyText>
<subsectionHeader confidence="0.999724">
3.2 Multiple Features
</subsectionHeader>
<bodyText confidence="0.986022">
The following features are used in our reranking
process:
Transliteration correspondence feature, f(si, ti);
This feature describes the mapping between
source and target graphemes, similar to the
transliteration options in the phrase table in
our previous generation process, where s and
</bodyText>
<footnote confidence="0.92519">
2In this work, we use Pinyin as the phonetic representa-
tion for Chinese.
</footnote>
<figure confidence="0.5507852">
Algorithm 1 Averaged perceptron training
Input: Candidate list with reference
j, n y�&apos;
{LISTxy,�) ( 3=Y y8∗} i=1
Output: Averaged parameters
</figure>
<listItem confidence="0.941735923076923">
1: ω~← 0, ~ωa ← 0, c ← 1
2: fort = 1 to T do
3: for i = 1 to N do
4: ˆyi ← argmaxy∈LIST(x„y;)~ω · Φ(xi,yi)
5: if ˆyi =6 y∗i then
6: ω~ ← ω~ + Φ(x∗i, y∗i ) − Φ(ˆxi, ˆyi)
7: ~ωa ← ~ωa +c·{Φ(x∗i,y∗i )−Φ(ˆxi, ˆyi)}
8: end if
9: c ← c + 1
10: end for
11: end for
12: return ω~ − ~ωa/c
t refer to the source and target language re-
</listItem>
<bodyText confidence="0.9951656">
spectively, and i to the current position.
Source grapheme chain feature, f(sii−1);
It measures the syllabification for a given
source text. There are two types of units
in different levels. One is on syllable level,
e.g., “aa/bye”, “aa/gaar/d”, reflecting the
segmentation of the source text, and the other
on character level, such as “a/b”, “a/g”,
“r/d”, showing the combination power of
several characters. These features on differ-
ent source grapheme levels can help the sys-
tem to achieve a more reliable syllabification
result from the candidates. We only consider
bi-grams when using this feature.
Target grapheme chain feature, f(tii−2);
This feature measures the appropriateness of
the generated target graphemes on both char-
acter and syllables level. It performs in a
similar way as the language model for SMT
decoding. We use tri-gram syllables in this
learning framework.
Paired source-to-target transition feature, f(&lt;
s, t &gt;i i−1);
This type of feature is firstly proposed in
(Li et al., 2004), aiming at generating source
and target graphemes simultaneously under
a suitable constraint. We use this feature
to restrict the synchronous transition of both
source and target graphemes, measuring how
well are those transitions, such as for “st”,
</bodyText>
<page confidence="0.997521">
63
</page>
<bodyText confidence="0.990839671428572">
whether “s” transliterated by “�” is followed
by “t” transliterated by “�”. In order to deal
with the data sparseness, only bi-gram transi-
tion relations are considered in this feature.
Hidden Markov model (HMM) style features;
There are a group of features with HMM
style constraint for evaluating the candi-
dates generated in previous SMT process,
including, previous syllable HMM features,
f(sii−n+1, ti), posterior syllable HMM fea-
tures, f(si+n−1, ti), and posterior character
i
HMM features, f(si,l, ti), where l denotes
the character following the previous syllable
in the source language. For the last feature,
it is effective to use both the current sylla-
ble and the first letter of the next syllable
to bound the current target grapheme. The
reason for applying this feature in our learn-
ing framework is that, empirically, the letters
following many syllables strongly affect the
transliteration for them, e.g., Aves —* fkA
, “a” followed by “v” is always translated
into “fk” rather than “�”.
Target grapheme position feature, f(ti, p);
This feature is an improved version of that
proposed in (Song et al., 2009), where p
refers to the position of ti. We have a mea-
sure for the target graphemes according to
their source graphemes and the current posi-
tion of their correspondent target characters.
There are three categories of such position,
namely, start (S), mediate (M) and end (E). S
refers to the first character in a target name, E
to the final, and the others belong to M. This
feature is used to exploit the observation that
some characters are more likely to appear at
certain positions in the target name. Some are
always found at the beginning of a named en-
tity while others only at the middle or the end.
For example, “re” associated to first charac-
ter in a target name is always transliterated as
“-r-or-”, such as Redd —* -r-or-Pt. When “re” ap-
pears at the end of a source name, however,
its transliteration will be “i1” in most cases,
just like Gore —* .
Target tone feature;
This feature is only applied to the translit-
eration task with Chinese as the target lan-
guage. It can be seen as a combination
of a target grapheme chain with some posi-
tion features, using tone instead of the target
grapheme itself for evaluation. There are 5
tones (0,1,2,3,4) for Chinese characters. It is
easy to conduct a comprehensive analysis for
the use of a higher ordered transition chain as
a better constraint. Many fixed tone patterns
can be identified in the Chinese translitera-
tion training data. The tone information can
also be extracted from the Pinyin resource we
used in the previous stage.
Besides the above string features, we also have
some numeric features, as listed below.
Transliteration score;
This score is the joint probabilities of all
transliteration options, included in the output
candidates generated by our decoder.
Target language model score;
This score is calculated from the probabilistic
tri-gram language model.
</bodyText>
<subsubsectionHeader confidence="0.423873">
Source/target Pinyin feature;
</subsubsectionHeader>
<bodyText confidence="0.96681748">
This feature uses Pinyin representation for a
source or target name, depending on what
side the Chinese language is used. It mea-
sures how good the output candidates can be
in terms of the comparison between English
text and Pinyin representation. The resulted
score is updated according to the Levenshtein
distance for the two input letter strings of En-
glish and Pinyin.
For a task with English as the target language,
we add the following two additional features into
the learning framework.
Vowel feature;
It is noticed that when English is the target
language, vowels can sometimes be missing
in the generated candidates. This feature is
thus used to punish those outputs unqualified
to be a valid English word for carrying no
vowel.
Syllable consistent feature;
This feature measures whether an English tar-
get name generated in the previous step has
the same number of syllables as the source
name. In Chinese-to-English transliteration,
Chinese characters are single-syllabled, thus
</bodyText>
<page confidence="0.999569">
64
</page>
<tableCaption confidence="0.999799">
Table 1: Evaluation results for our NEWS2010 task.
</tableCaption>
<table confidence="0.996521666666667">
Task Source Target ACC Mean F MRR Map ref Recall ACCSMT
EnCh English Chinese 0.477 0.740 0.506 0.455 0.561 0.381
ChEn Chinese English 0.227 0.749 0.269 0.226 0.371 0.152
</table>
<bodyText confidence="0.981713">
we can easily identify their number. For syl-
labification, we have an independent segmen-
tation process for calculating the syllables.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999994028571429">
For NEWS2010, we participated in all two
Chinese related transliteration tasks, namely,
EnCh (English-to-Chinese) and ChEn (Chinese-
to-English back transliteration). The official eval-
uation scores for our submissions are presented
in Table 1 with recall rate, and the ACC score
(ACCSMT) for original SMT outputs. It is easy
to see the performance gain for the reranking, and
also from the recall rate that there is still some
room for improvement, in spite of the high ratio of
ACC/Recall3 calculated from Table 1. However, it
is also worth noting that, some of the source texts
cannot be correctly transliterated, due to many
multiple-word name entities with semantic com-
ponents in the test data, e.g., “MANCHESTER
BRIDGE”, “BRIGHAM CITY” etc. These seman-
tic parts are beyond our transliteration system’s ca-
pability to tackle, especially when the training data
is limited and the only focus of the system is on the
phonetic equivalent correspondence.
Compared to the EnCh transliteration, we get a
rather low ACC score for the ChEn back translit-
eration, suggesting that ChEn task is somewhat
harder than the EnCh (in which Chinese char-
acters are always limited). The ChEn task is a
one-to-many translation, involving a lot of pos-
sible choices and combinations of English sylla-
bles. This certainly makes it a more challenge-
able task than EnCh. However, looking into the
details of the outputs, we find that, in the ChEn
back transliteration, some characters in the test
corpus are unseen in the training and the devel-
opment data, resulting in incorrect transliterations
for many graphemes. This is another factor affect-
ing our final results for the ChEn task.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.8404245">
In this paper, we have presented our work on
multiple feature based reranking for transliteration
</bodyText>
<footnote confidence="0.669986">
3Compared to the results from (Song et al., 2009)
</footnote>
<bodyText confidence="0.999956846153846">
generation. It NEWS2010 results show that this
approach is effective and promising, in the sense
that it ranks the best in EnCh and ChEn tasks. The
reranking used in this work can also be consid-
ered a regeneration process based on an existing
set, as part of our features are always used directly
to generate the initial transliteration output in other
researches. Though, those features are strongly
dependent on the nature of English and Chinese
languages, it is thus not an easy task to transplant
this model for other language pairs. It is an inter-
esting job to turn it into a language independent
model that can be applied to other languages.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999879875">
The research described in this paper was par-
tially supported by City University of Hong Kong
through the Strategic Research Grants (SRG)
7002267 and 7008003. Dr. Hai Zhao was sup-
ported by the Natural Science Foundation of China
(NSFC) through the grant 60903119. We also
thank Mr. Wenbin Jiang for his helpful sugges-
tions on averaged perceptron learning.
</bodyText>
<sectionHeader confidence="0.996499" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.956080285714286">
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-2002, pages 1–8, July.
Haizhou Li, Min Zhang, and Jian Su. 2004. A
joint source-channel model for machine transliter-
ation. In Proceedings of ACL-04, pages 159–166,
Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL-03, pages 160–167, Sapporo, Japan, July.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration using target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proceedings of NEWS
2009, pages 36–39, Suntec, Singapore, August.
Yan Song, Chunyu Kit, and Xiao Chen. 2009. Translit-
eration of name entity via improved statistical trans-
lation on character sequences. In Proceedings of
NEWS 2009, pages 57–60, Suntec, Singapore, Au-
gust.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.462564">
<title confidence="0.822158">Reranking with Multiple Features for Better Transliteration of Chinese, Translation and</title>
<author confidence="0.805894">City University of Hong Kong</author>
<author confidence="0.805894">Tat Chee Ave</author>
<author confidence="0.805894">Hong Kowloon</author>
<affiliation confidence="0.968908">of Computer Science and</affiliation>
<address confidence="0.795293">Shanghai Jiao Tong University, #800, Dongchuan Rd, Shanghai,</address>
<abstract confidence="0.996877823529412">Effective transliteration of proper names via grapheme conversion needs to find transliteration patterns in training data, and then generate optimized candidates for testing samples accordingly. However, the top-1 accuracy for the generated candidates cannot be good if the right one is not ranked at the top. To tackle this issue, we propose to rerank the output candidates for a better order using the averaged perceptron with multiple features. This paper describes our recent work in this direction for our participation in NEWS2010 transliteration evaluation. The official results confirm its effectiveness in English-Chinese bidirectional transliteration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="4657" citStr="Collins, 2002" startWordPosition="716" endWordPosition="717">ted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for each feature function in log-linear model is optimized by MERT training (Och, 2003). Finally, a maximum number of 50 candidates are generated for each source name. 3 Reranking 3.1 Learning Framework For reranking training and prediction, we adopt the averaged perceptron (Collins, 2002) as our learning framework, which has a more stable performance than the non-averaged version. It is presented in Algorithm 1. Where ω~ is the vector of parameters we want to optimize, x, y are the corresponding source (with different syllabification) and target graphemes in the candidate list, and Φ represents the feature vector in the pair of x and y. In this algorithm, reference y∗i is the most appropriate output in the candidate list according to the true target named entity in the training data. We use the Mean-F score to identify which candidate can be the reference, by locating the one </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP-2002, pages 1–8, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04,</booktitle>
<pages>159--166</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7479" citStr="Li et al., 2004" startWordPosition="1202" endWordPosition="1205"> several characters. These features on different source grapheme levels can help the system to achieve a more reliable syllabification result from the candidates. We only consider bi-grams when using this feature. Target grapheme chain feature, f(tii−2); This feature measures the appropriateness of the generated target graphemes on both character and syllables level. It performs in a similar way as the language model for SMT decoding. We use tri-gram syllables in this learning framework. Paired source-to-target transition feature, f(&lt; s, t &gt;i i−1); This type of feature is firstly proposed in (Li et al., 2004), aiming at generating source and target graphemes simultaneously under a suitable constraint. We use this feature to restrict the synchronous transition of both source and target graphemes, measuring how well are those transitions, such as for “st”, 63 whether “s” transliterated by “�” is followed by “t” transliterated by “�”. In order to deal with the data sparseness, only bi-gram transition relations are considered in this feature. Hidden Markov model (HMM) style features; There are a group of features with HMM style constraint for evaluating the candidates generated in previous SMT process</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proceedings of ACL-04, pages 159–166, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="4454" citStr="Och, 2003" startWordPosition="686" endWordPosition="687">62–65, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics try in it. After that, we use a phoneme resource2 to refine the phrase table by filtering out the wrongly extracted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for each feature function in log-linear model is optimized by MERT training (Och, 2003). Finally, a maximum number of 50 candidates are generated for each source name. 3 Reranking 3.1 Learning Framework For reranking training and prediction, we adopt the averaged perceptron (Collins, 2002) as our learning framework, which has a more stable performance than the non-averaged version. It is presented in Algorithm 1. Where ω~ is the vector of parameters we want to optimize, x, y are the corresponding source (with different syllabification) and target graphemes in the candidate list, and Φ represents the feature vector in the pair of x and y. In this algorithm, reference y∗i is the m</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL-03, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Machine transliteration using targetlanguage grapheme and phoneme: Multi-engine transliteration approach.</title>
<date>2009</date>
<booktitle>In Proceedings of NEWS 2009,</booktitle>
<pages>36--39</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2678" citStr="Oh et al., 2009" startWordPosition="399" endWordPosition="402">esult, it is found in (Song et al., 2009) that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high. This observation suggests that if we could rearrange those outputs into a better order, especially, push the correct one to the top, the overall performance could be enhanced significantly, without any further refinement of the original generation process. This reranking strategy is proved to be efficient in transliteration generation with a multi-engine approach (Oh et al., 2009). In this paper, we present our recent work on reranking the transliteration candidates via an online discriminative learning framework, namely, the averaged perceptron. Multiple features are incorporated into it for performance enhancement. The following sections will give the technical details of our method and present its results for NEWS2010 shared task for named entity transliteration. 2 Generation For the generation of transliteration candidates, we follow the work (Song et al., 2009), using a phrase-based SMT procedure with the log-linear model exp[�n i�1 Aihi(s, t)] P(t|s) = (1) Et exp</context>
</contexts>
<marker>Oh, Uchimoto, Torisawa, 2009</marker>
<rawString>Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Machine transliteration using targetlanguage grapheme and phoneme: Multi-engine transliteration approach. In Proceedings of NEWS 2009, pages 36–39, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Xiao Chen</author>
</authors>
<title>Transliteration of name entity via improved statistical translation on character sequences.</title>
<date>2009</date>
<booktitle>In Proceedings of NEWS 2009,</booktitle>
<pages>57--60</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2103" citStr="Song et al., 2009" startWordPosition="306" endWordPosition="309">ently, even the overall results by SMT output are acceptable, it is still unreliable to rank the candidates simply by their statistical translation scores for the purpose of selecting the best one. In order to make a proper choice, the direct orthographic mapping requires a precise alignment and a better transliteration option selection. Thus, powerful algorithms for effective use of the parallel data is indispensable, especially when the available data is limited in volume. Interestingly, although an SMT based approach could not achieve a precise top-1 transliteration result, it is found in (Song et al., 2009) that, in contrast to the ordinary top-1 accuracy (ACC) score, its recall rate, which is defined in terms of whether the correct answer is generated in the n-best output list, is rather high. This observation suggests that if we could rearrange those outputs into a better order, especially, push the correct one to the top, the overall performance could be enhanced significantly, without any further refinement of the original generation process. This reranking strategy is proved to be efficient in transliteration generation with a multi-engine approach (Oh et al., 2009). In this paper, we prese</context>
<context position="3774" citStr="Song et al., 2009" startWordPosition="569" endWordPosition="572">ong et al., 2009), using a phrase-based SMT procedure with the log-linear model exp[�n i�1 Aihi(s, t)] P(t|s) = (1) Et exp[EZ 1 Aihi(s, t)] for decoding. Originally we use two directional phrase1 tables, which are learned for both directions of source-to-target and target-to-source, containing different entries of transliteration options. In order to facilitate the decoding by exploiting all possible choices in a better way, we combine the forward and backward directed phrase tables together, and recalculate the probability for each en1It herein refers to a character sequence as described in (Song et al., 2009). 62 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 62–65, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics try in it. After that, we use a phoneme resource2 to refine the phrase table by filtering out the wrongly extracted phrases and cleaning up the noise in it. In the decoding process, a dynamic pruning is performed when generating the hypothesis in each step, in which the threshold is variable according to the current searching space, for we need to obtain a good candidate list as precise as possible for the next stage. The parameter for eac</context>
<context position="8853" citStr="Song et al., 2009" startWordPosition="1421" endWordPosition="1424"> ti), where l denotes the character following the previous syllable in the source language. For the last feature, it is effective to use both the current syllable and the first letter of the next syllable to bound the current target grapheme. The reason for applying this feature in our learning framework is that, empirically, the letters following many syllables strongly affect the transliteration for them, e.g., Aves —* fkA , “a” followed by “v” is always translated into “fk” rather than “�”. Target grapheme position feature, f(ti, p); This feature is an improved version of that proposed in (Song et al., 2009), where p refers to the position of ti. We have a measure for the target graphemes according to their source graphemes and the current position of their correspondent target characters. There are three categories of such position, namely, start (S), mediate (M) and end (E). S refers to the first character in a target name, E to the final, and the others belong to M. This feature is used to exploit the observation that some characters are more likely to appear at certain positions in the target name. Some are always found at the beginning of a named entity while others only at the middle or the</context>
<context position="13889" citStr="Song et al., 2009" startWordPosition="2252" endWordPosition="2255">tion, involving a lot of possible choices and combinations of English syllables. This certainly makes it a more challengeable task than EnCh. However, looking into the details of the outputs, we find that, in the ChEn back transliteration, some characters in the test corpus are unseen in the training and the development data, resulting in incorrect transliterations for many graphemes. This is another factor affecting our final results for the ChEn task. 5 Conclusion In this paper, we have presented our work on multiple feature based reranking for transliteration 3Compared to the results from (Song et al., 2009) generation. It NEWS2010 results show that this approach is effective and promising, in the sense that it ranks the best in EnCh and ChEn tasks. The reranking used in this work can also be considered a regeneration process based on an existing set, as part of our features are always used directly to generate the initial transliteration output in other researches. Though, those features are strongly dependent on the nature of English and Chinese languages, it is thus not an easy task to transplant this model for other language pairs. It is an interesting job to turn it into a language independe</context>
</contexts>
<marker>Song, Kit, Chen, 2009</marker>
<rawString>Yan Song, Chunyu Kit, and Xiao Chen. 2009. Transliteration of name entity via improved statistical translation on character sequences. In Proceedings of NEWS 2009, pages 57–60, Suntec, Singapore, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>