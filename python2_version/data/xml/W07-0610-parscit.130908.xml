<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006443">
<title confidence="0.966771">
The Topology of Synonymy and Homonymy Networks
</title>
<author confidence="0.997266">
James Gorman and James R. Curran
</author>
<affiliation confidence="0.9988435">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.667564">
NSW 2006, Australia
</address>
<email confidence="0.998849">
{jgorman2,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941285714286">
Semantic networks have been used suc-
cessfully to explain access to the men-
tal lexicon. Topological analyses of these
networks have focused on acquisition and
generation. We extend this work to look
at models that distinguish semantic rela-
tions. We find the scale-free properties
of association networks are not found in
synonymy-homonymy networks, and that
this is consistent with studies of childhood
acquisition of these relationships. We fur-
ther find that distributional models of lan-
guage acquisition display similar topolog-
ical properties to these networks.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99216144">
Semantic networks have played an important role in
the modelling of the organisation of lexical knowl-
edge. In these networks, words are connected by
graph edges based on their semantic relations. In re-
cent years, researchers have found that many seman-
tic networks are small-world, scale-free networks,
having a high degree of structure and a short distance
between nodes (Steyvers and Tenenbaum, 2005).
Early models were taxonomic and explained some
aspects of human reasoning (Collins and Quillian,
1969) (and are still used in artificial reasoning sys-
tems), but were replaced by models that focused on
general graph structures (e.g. Collins and Loftus,
1975). These better modelled many observed phe-
nomena but explained only the searching of seman-
73
tic space, not its generation or properties that exist
at a whole-network level.
Topological analyses, looking at the statistical
regularities of whole semantic networks, can be
used to model phenomena not easily explained from
the smaller scale data found in human experiments.
These networks are typically formed from corpora,
expert compiled lexical resources, or human word-
association data.
Existing work has focused language acquisition
(Steyvers and Tenenbaum, 2005) and generation
(Cancho and Sol´e, 2001). These models use the gen-
eral notion of semantic association which subsumes
all specific semantic relations, e.g. synonymy.
There is evidence that there are distinct cogni-
tive processes for different semantic relations (e.g.
Casenhiser, 2005). We perform a graph analysis
of synonymy, nearness of meaning, and homonymy,
shared lexicalisation.
We find that synonymy and homonymy produce
graphs that are topologically distinct from those pro-
duced using association. They still produce small-
world networks with short path lengths but lack
scale-free properties. Adding edges of different se-
mantic relations, in particular hyponymy, produces
graphs more similar to the association networks. We
argue our analyses consistent with other semantic
network models where nodes of a common type
share edges of different types (e.g. Collins and Lof-
tus, 1975).
We further analyse the distributional model of lan-
guage acquisition. We find that it does not well
explain whole-language acquisition, but provides a
model for synonym and homonym acquisition.
</bodyText>
<note confidence="0.872983">
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73–80,
Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.945935" genericHeader="method">
2 Graph Theory
</sectionHeader>
<bodyText confidence="0.999714857142857">
Our overview of graph theory follows Watts (1999).
A graph consists of a set of n vertices (nodes) and
a set of edges, or arcs, which join pairs of ver-
tices. Edges are undirected and arcs are directed.
Edges and arcs can be weighted or unweighted,
with weights indicating the relative strength or im-
portance of the edges. We will only consider un-
weighted, undirected networks. Although there is
evidence that semantic relations are both directed
(Tversky, 1977) and weighted (Collins and Loftus,
1975), we do not have access to this information in
a consistent and meaningful format for all our re-
sources.
Two vertices connected by an edge are called
neighbours. The degree k of a vertex is the count
of it neighbours. From this we measure the average
degree (k) for the graph and the degree distribution
P(k) for all values of k. The degree distribution is
the probability of a vertex having a degree k.
The neighbourhood Γv of a vertex v is the set of all
neighbours of v not including v. The neighbourhood
ΓS of a subgraph S is the set of all neighbours of S,
not including the members of S.
The distance between any two vertices is the
shortest path length, or the minimum number of
edges that must be traversed, to reach the first from
the second. The characteristic path length L is the
average distance between vertices.1 The diameter
D of a graph is the maximum shortest path length
between any two vertices. At most D steps are re-
quired to reach any vertex from any other vertex but,
on average, only L are required.
For very large graphs, calculating the values for L
and D is computationally difficult. We instead sam-
ple n&apos; &lt;&lt; n nodes and find the mean values of L and
D across the samples. The diameter produced will
always be less than or equal to the true diameter. We
found n&apos; = 100 to be most efficient.
It is not a requirement that every vertex be reach-
able from every other vertex and in these cases both
L and D will be infinite. In these cases we analyse
the largest connected subgraph.
</bodyText>
<footnote confidence="0.9934365">
1Here we follow Steyvers and Tenenbaum (2005) as it is
more commonly used in the cognitive science literature. Watts
(1999) defines the characteristic path length as the median of
the means of shortest path lengths for each vertex.
</footnote>
<subsectionHeader confidence="0.958228">
2.1 Small-world Networks
</subsectionHeader>
<bodyText confidence="0.999969454545454">
Traditional network models assume that networks
are either completely random or completely regu-
lar. Many natural networks are somewhere between
these two extremes. These small-world networks a
have the high degree of clustering of a regular lattice
and the short average path length of a random net-
work (Watts and Strogatz, 1998). The clustering is
indicative of organisation, and the short paths make
for easier navigation.
The clustering coefficient Cv is used to measure
the degree of clustering around a vertex v:
</bodyText>
<equation confidence="0.9892675">
JE(Γv)J
Cv =
</equation>
<bodyText confidence="0.99899925">
where JE(Γv)J is the number of edges in the neigh-
bourhood Γv and (2) is the total number of possible
edges in Γv. The clustering coefficient C of a graph
is the average over the coefficients of all the vertices.
</bodyText>
<subsectionHeader confidence="0.998996">
2.2 The Scale of Networks
</subsectionHeader>
<bodyText confidence="0.999942576923077">
Amaral et al. (2000) describe three classes of small
world networks based on their degree distributions:
Scale-free networks are characterised by their
degree distribution decaying as a power law, having
a small number of vertices with many links (hubs)
and many vertices with few links. Networks in this
class include the internet (Faloutsos et al., 1999)
and semantic networks (Steyvers and Tenenbaum,
2005).
Broad-scale networks are characterised by their
degree distribution decaying as a power law fol-
lowed by a sharp cut-off. This class includes col-
laborative networks (Watts and Strogatz, 1998).
Single-scale networks are characterised by fast
decaying degree distribution, such exponential or
Gaussian, in which hubs are scarce or nonexistent.
This class includes power grids (Watts and Strogatz,
1998) and airport traffic (Amaral et al., 2000).
Amaral et al. (2000) model these differences us-
ing a constrained preferential attachment model,
where new nodes prefer to attach to highly con-
nected nodes. Scale-free networks result when there
are no constraints. Broad-scale networks are pro-
duced when ageing and cost-to-add-link constraints
are added, making it more difficult to produce very
high degree hubs. Single-scale networks occur when
</bodyText>
<equation confidence="0.4368555">
(kv )
2
</equation>
<page confidence="0.994068">
74
</page>
<bodyText confidence="0.997613">
these constraints are strengthened. This is one of
several models for scale-free network generation,
and different models will result in different internal
structures and properties (Keller, 2005).
</bodyText>
<sectionHeader confidence="0.97873" genericHeader="method">
3 Semantics Networks
</sectionHeader>
<bodyText confidence="0.999990378787879">
Semantic networks represent the structure of hu-
man knowledge through the connections of words.
Collins and Quillian (1969) proposed a taxonomic
representation of knowledge, where words are con-
nected by hyponym relations, like in the WordNet
noun hierarchy (Fellbaum, 1998). While this struc-
ture predicted human reaction times for verifying
facts it allows only a limited portion of knowledge
to be expressed. Later models represented knowl-
edge as semi-structured networks, and focused on
explaining performance in memory retrieval tasks.
One such model is spreading-activation, in which
the degree to which a concept is able to be recalled is
related to its similarity both to other concepts in gen-
eral and to some particular prime or primes (Collins
and Loftus, 1975). In this way, if one is asked to
name a red vehicle, fire truck is more likely re-
sponse than car: while both are strongly associated
with vehicle, fire truck is more strongly associated
with red than is car.
More recently, graph theoretic approaches have
examined the topologies of various semantic net-
works. Cancho and Sol´e (2001) examine graphs of
English modelled from the British National Corpus.
Since co-occurrence is non-trivial — words in a sen-
tence must share some semantic content for the sen-
tence to be coherent — edges were formed between
adjacent words, with punctuation skipped. Two
graphs were formed: one from all co-occurrences
and the other from only those co-occurrences with
a frequency greater than chance. Both models pro-
duced scale-free networks. They find this model
compelling for word choice during speech, not-
ing function words are the most highly connected.
These give structure without conveying significant
meaning, so can be omitted without rendering a
sentence incoherent, but when unavailable render
speech non-fluent. This is consistent with work by
Albert et al. (2000) showing that scale-free networks
are tolerant to random deletion but sensitive to tar-
geted removal of highly connected vertices.
Sigman and Cecchi (2002) investigate the struc-
ture of WordNet to study the effects of nounal pol-
ysemy on graph navigation. Beginning with synsets
and the hyponym tree, they find adding polysemy
both reduces the characteristic path length and in-
creases the clustering coefficient, producing a small-
world network. They propose, citing word priming
experiments as evidence, that these changes in struc-
ture give polysemy a role in metaphoric thinking and
generalisation by increasing the navigability of se-
mantic networks.
Steyvers and Tenenbaum (2005) examine the
growth of semantic networks using graphs formed
from several resources: the free association index
collected by Nelson et al. (1998), Wordnet and
the 1911 Roget’s thesaurus. All these produced
scale-free networks, and, using an age of acquisi-
tion and frequency weighted preferential attache-
ment model, show that this corresponds to age-of-
acquisition norms for a small set of words. This is
compared to networks produced by Latent Semantic
Analysis (LSA, Landauer and Dumais, 1997), and
conclude that LSA is an inadequate model for lan-
guage growth as it does not produce the same scale-
free networks as their association models.
</bodyText>
<subsectionHeader confidence="0.998246">
3.1 Synonymy and Homonymy
</subsectionHeader>
<bodyText confidence="0.999926952380952">
While there have been many studies using human
subjects on the acquisition of particular semantic re-
lations, there have been no topological studies differ-
entiating these from the general notion of semantic
association. This is interesting as psycholinguistic
studies have shown that semantic relationships are
distinguishable (e.g. Casenhiser, 2005). Here we
consider synonymy and homonymy.
There are very few cases of true synonymy, where
two words are substitutable in all contexts. Near-
synonymy, where two words share some close com-
mon meaning, is more common. Sets of synonyms
can be grouped together into synsets, representing a
common idea.
Homonymy occurs when a word has multiple
meanings. Formally, homonymy is occurs when
words do not share an etymological root (in lin-
guistics) or when the distinction between meanings
is coarse (in cognitive science). When the words
share a root or meanings are close, the relationship
is called polysemy. This distinction is significant
</bodyText>
<page confidence="0.993251">
75
</page>
<bodyText confidence="0.842362782608696">
in language acquisition, but as yet little research 4.1 Lexical Semantic Resources
has been performed on the learning of polysemes
(Casenhiser, 2005). It is also significant for Natural
Language Processing. The effect of disambiguating
homonyms is markedly different from polysemes in
Information Retrieval (Stokoe, 2005).
We do not have access to these distinctions, as
they are not available in most resources, nor are
there techniques to automatically acquire these dis-
tinctions (Kilgarriff and Yallop, 2000). For simplic-
ity, will conflate the categories under homonymy.
There have been several studies into synonymy
and homonymy acquisition in children, and these
have shown that it lags behind vocabulary growth
(Doherty and Perner, 1998; Garnham et al., 2000).
A child will associate both rabbit and bunny with
the same concept, but before the age of four, most
children have difficulty in choosing the word bunny
if they have already been presented with the word
rabbit. Similarly, a young child asked to point to two
pictures that have the same name but mean different
things will have difficulty, despite knowing each of
the things independently.
Despite this improvement with age, there are
tendencies for language to avoid synonyms and
homonyms as a more general principle of economy
(Casenhiser, 2005). This is balanced by the utility of
ambiguous relations for mental navigation (Sigman
and Cecchi, 2002) which goes some way to explain-
ing why they still play such a large role in language.
4 The Topology of Synonymy and
Homonymy Relations
For each of our resources we form a graph based on
the relations between lexical items. This differs to
the earlier work of Sigman and Cecchi (2002), who
use synsets as vertices, and Steyvers and Tenenbaum
(2005) who use both lexical items and synsets..
This is motivated largely by our automatic ac-
quisition techniques, and also by human studies, in
which we can only directly access relationships be-
tween words. This also allows us to directly com-
pare resources where we have information about
synsets to those without. We distinguish parts of
speech as disambiguation across them is relatively
easy psychologically (Casenhiser, 2005) and com-
putationally (e.g. Ratnaparkhi, 1996).
76
A typical resource for providing this information
are manually constructed lexical semantic resources.
We will consider three: Roget’s, WordNet and Moby
Roget’s thesaurus is a common language the-
saurus providing a hierarchy of synsets. Synsets
with the same general or overlapping meaning and
part of speech are collected into paragraphs. The
parts of speech covered are nouns, verbs, adjectives,
adverbs, prepositions, phrases, pronouns, interjec-
tions, conjunctions, and interrogatives. Paragraphs
with similar meaning are collated by part of speech
into labeled categories. Categories are then collected
into classes using a three-tiered hierarchy, with the
most general concept at the top. Where a word has
several senses, it will appear in several synsets. Sev-
eral editions of Roget’s have been released repre-
senting the change in language since the first edi-
tion in 1852. The last freely available edition is the
1911, which uses outdated vocabulary, but the global
topology has not changed with more recent editions
(Old, 2003). As our analysis is not concerned with
the specifics of the vocabulary, this is the edition we
will use. It consists of a vocabulary of 29,460 nouns,
15,173 verbs, 13,052 adjectives and 3,005 adverbs.
WordNet (Fellbaum, 1998) is an electronic lex-
ical database. Like Roget’s, it main unit of or-
ganisation is the synset, and a word with several
senses will appear in several synsets. These are di-
vided into four parts of speech: nouns, verbs, ad-
jectives and adverbs. Synsets are connected by se-
mantic relationships, e.g antonymy, hyponymy and
meronym. WordNet 2.1 provides a vocabulary of
117,097 nouns, 11,488 verbs, 22,141 adjectives and
4,601 adverbs.
The Moby thesaurus provides synonymy lists
for over 30,000 words, with a total vocabulary of
322,263 words. These lists are not distinguished by
part of speech. A separate file is supplied containing
part of speech mappings for words in the vocabu-
lary. We extracted separate synonym lists for nouns,
verbs, adjectives and adverbs using this list com-
bined with WordNet part of speech information.2
This produces a vocabulary of 42,821 nouns, 11,957
verbs, 16,825 adjectives and 3,572 adverbs.
Table 1 presents the statistics for the largest con-
</bodyText>
<table confidence="0.994034272727273">
2http://aspell.sourceforge.net/wl/
Noun Roget’s Adv Noun WordNet Adv Noun Moby Adv
Verb Adj Verb Adj Verb Adj
n 15,517 8,060 6,327 626 11,746 6,506 4,786 62 42,819 11,934 16,784 3501
(k) 8.97 8.46 7.40 7.17 4.58 6.34 5.16 4.97 34.65 51.98 39.26 16.07
L 6.5 6.0 6.4 10.5 9.8 6.0 9.5 5.6 3.7 3.1 3.4 3.7
D 21.4 17 17 31 27 15.3 26.4 14 9.6 9.8 9.3 9.8
C 0.74 0.68 0.69 0.77 0.63 0.62 0.66 0.57 0.60 0.49 0.57 0.55
Lr 4.7 4.5 4.6 3.5 6.3 5.0 5.9 3.3 3.4 2.8 2.9 3.2
Dr 8.5 8.4 9.0 7 13.3 10.1 11.8 8 5.5 5 5 6
Cr 0.00051 0.0011 0.0012 0.0090 0.00036 0.00099 0.00094 0.028 0.00081 0.0043 0.0023 0.0047
</table>
<tableCaption confidence="0.999866">
Table 1: Topological statistics for nouns, verbs, adjectives and adverbs for our three gold standard resources
</tableCaption>
<figure confidence="0.9175145">
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.999703">
Figure 1: Degree distributions for nouns
</figureCaption>
<bodyText confidence="0.999659826086957">
nected subgraph for the four parts of speech con-
sidered, along with statistics for random graphs of
equivalent size and average degree (subscript r). In
all cases the clustering coefficient is significantly
higher than that for the random graph. While the
characteristic path length and diameter are larger
than for the random graphs, they are still short in
comparison to an equivalent latice. This, combined
with the high clustering coefficient, indicates that
they are producing small-world networks. The di-
ameter is larger still than for the random graphs. To-
gether these indicate a more lattice like structure,
which is consistent with the intuition that dissimi-
lar words are unlikely to share similar words. This
is independent of part of speech.
Figure 1 shows the degree distributions for nouns,
and for a random graph plotted on log-log axes.
Other parts of speech produce equivalent graphs.
These clearly show that we have not produced scale-
free networks as we are not seeing straight line
power law distributions. Instead we are seeing what
is closer to single- or broad-scale distributions.
The differences in the graphs is explained by the
</bodyText>
<table confidence="0.999655">
WordNet Synset Roget’s Cat
Hyp Para
n 11,746 118,264 15,517 27,989 29,431
(k) 4.58 6.61 8.97 26.84 140.36
L 9.8 6.3 6.5 4.3 2.9
D 27 16.4 21.4 12.6 7
C 0.63 0.74 0.74 0.85 0.86
</table>
<tableCaption confidence="0.999636">
Table 2: Effect of adding hyponym relations
</tableCaption>
<bodyText confidence="0.9998424">
granularity of the synonymy relations presented, as
indicated by the characteristic path length. WordNet
has fine grained synsets and the smallest characteris-
tic path length, while Moby has coarse grained syn-
onyms and the largest characteristic path length.
</bodyText>
<subsectionHeader confidence="0.966368">
4.2 Synonymy-Like Relations
</subsectionHeader>
<bodyText confidence="0.963164260869565">
Having seen that synonymy and homonymy alone
do not produce scale-free networks, we investigate
the synonymy-like relations of hyponymy and topic
relatedness. Hyponymy is the IS-A class subsump-
tion relationship and occurs between noun synsets in
WordNet. Topic relatedness occurs in the grouping
of synsets in Roget’s in paragraphs and categories.
Table 2 compares adding hyponym edges to the
graph of WordNet nouns and increasing the gran-
ularity of Roget’s synsets using edges between all
words in a paragraph or category. Adding hyponymy
relations increases the connectivity of the graph sig-
nificantly and there are no longer any disconnected
subgraphs. At the same time the diameter is nearly
halved and characteristic path length reduce one
third, but average degree only increases by one third.
To achieving the same reduction in path length and
diameter by the granularity of Roget’s requires the
average degree to increase by nearly three times.
Figure 2 shows the degree distributions when hy-
ponyms are added to WordNet nouns and the granu-
larity of Roget’s is increased. Roget’s category level
graph is omitted for clarity. We can see that the orig-
</bodyText>
<figure confidence="0.969114076923077">
P(k)
0.001
1e-04
0.01
0.1
1
Roget’s
WordNet
Moby
Random
77
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.9972615">
Figure 2: Degree distributions adding hyponym re-
lations to nouns
</figureCaption>
<bodyText confidence="0.999169111111111">
inally broad-scale structure of the Roget’s distribu-
tion is tending to have a more gaussian distribution.
The addition of hyponyms produces a power law dis-
tribution for k &gt; 10 of P(k) 7z� k−1 7.
Additional constraints on attachment reduce the
ability of networks to be scale-free (Amaral et
al., 2000). The difference between synonymy-
homonymy networks and association networks can
be explained by this. Steyvers and Tenenbaum
(2005) propose a plausible attachment model for
their association networks which has no additional
constraint function. If we use the tendency for lan-
guages to avoid lexical ambiguity from synonymy
and homonymy as a constraint to the production of
edges we will produce broad-scale networks rather
than scale-free networks.
As hyponymy is primarily semantic and does not
produce lexical ambiguity, adding hyponym edges
weakens the constraint on ambiguity, producing a
scale-free network. Generalising synonymy to in-
clude topicality weakens the constraints, but at the
same time reduces preference in attachment. The
results of this is the gaussian-like distribution with
very few low degree nodes. The difference between
this thesaurus based topicality and that found in hu-
man association data is that human association data
only includes the most similar words.
</bodyText>
<sectionHeader confidence="0.987832" genericHeader="method">
5 Distributional Similarity Networks
</sectionHeader>
<bodyText confidence="0.99990175">
Lexical semantic resources can be automatically ex-
tracted using distributional similarity. Here words
are projected into a vector space using the contexts
in which they appear as axes. Contexts can be as
</bodyText>
<figure confidence="0.725011">
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.997042">
Figure 3: Degree distributions of Jaccard
</figureCaption>
<bodyText confidence="0.999408823529412">
wide as document (Landauer and Dumais, 1997)
or close as grammatical dependencies (Grefenstette,
1994). The distance between words in this space ap-
proximates the similarity measured by synonymy.
We use the noun similarities produced by Gor-
man and Curran (2006) using the weighted Jac-
card measure and the t-test weight and grammat-
ical relations extracted from their LARGE corpus,
the method found to perform best against their gold-
standard evaluation. Only words with a corpus fre-
quency higher than 100 are included. This method
is comparable to that used in LSA, although using
grammatical relations as context produces similar-
ity much more like synonymy than those taken at a
document level (Kilgarriff and Yallop, 2000).
Distributional similarity produces a list of vocab-
ulary words, their similar neighbours and the sim-
ilarity to the neighbours. These lists approximate
synonymy by measuring substitutability in context,
and do not only find synonyms as near neighbours
as both antonyms and hyponyms are frequently sub-
stitutable in a grammatical context (Weeds, 2003).
From this we generate graphs by taking either the k
nearest neighbours to each word (k-NN), or by us-
ing a threshold. To produce a threshold we take the
mean similarity of the kth neighbour of all words (*k-
NN). We compare both these methods.
Figure 3 compares the degree distributions of
these. Using k-NN produces a degree distribution
that is close to a Gaussian, where as *k-NN pro-
duces a distribution much more like that of our ex-
pert compiled resources. This is unsurprising when
the distribution of distributional distances is consid-
ered. Some words will have many near neighbours,
</bodyText>
<figure confidence="0.995778863636364">
0.001
1e-04
1e-05
0.01
0.1
1
Roget’s
Paragraph
WordNet
Hyponym
1
0.1
0.01
0.001
1e-04
1e-05
k=5
*k=5
k=50
*k=50
P(k)
P(k)
</figure>
<page confidence="0.99098">
78
</page>
<table confidence="0.999511666666667">
Roget’s WordNet Hyp k-NN *k-NN
n 15,517 11,746 118,264 35,592 19,642
(k) 8.97 4.58 6.61 8.26 13.86
L 6.5 9.8 6.3 6.2 6.4
D 21.4 27 16.4 12 25.6
C 0.74 0.63 0.74 0.18 0.37
</table>
<tableCaption confidence="0.972133">
Table 3: Comparing nouns in expert and distribu-
tional resources
</tableCaption>
<bodyText confidence="0.999715128205128">
and other few. In the first case, k-NN will fail to in-
clude some near neighbours, and in the second will
include some distant neighbours that are note se-
mantically related. This result is consistent between
k = 5 and 50. Introduction of random edges from
the noise of distant neighbours reduces the diameter
and missing near neighbours reduces the clustering
coefficient (Table 3).
In Table 3 we also compare these to noun syn-
onymy in Roget’s, and to synonymy and hyponymy
in WordNet. Distributional similarity (*k-NN) pro-
duces a network with similar degree, characteristic
path length and diameter. The clustering coefficient
is much less than that from expert resources, is still
several orders of magnitude larger than an equivalent
random graph (Table 1).
Figure 4 compares a distributional network to net-
works WordNet and Moby. We can see the same
broad-scale in the distributional and synonym net-
works, and a distinct difference with the scale-free
WordNet hyponym distribution.
The distributional similarity distribution is sim-
ilar to that found in networks formed from LSA
by Steyvers and Tenenbaum (2005). Steyvers and
Tenenbaum hypothesise that the distributions pro-
duced by LSA might be due more to frequency dis-
tribution effects that correct language modelling.
In light of our analysis of synonymy relations,
we propose a new explanation. Given that: dis-
tributional similarity has been shown to approx-
imate the semantic similarity in synonymy rela-
tions found in thesaurus type resources (Curran,
2004); distributional similarity produces networks
with similar statistical properties to those formed by
synonym and homonymy relations; and, the syn-
onym and homonymy relations found in thesauri
produce networks with different statistical proper-
ties to those found in the association networks anal-
ysed by Steyvers and Tenenbaum; it can be plausibly
</bodyText>
<figure confidence="0.9044295">
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.999147">
Figure 4: Degree distributions for nouns
</figureCaption>
<bodyText confidence="0.998880578947368">
hypothesised that distributional techniques are mod-
eling the acquisition of synonyms and homonyms,
rather than all semantic relationships.
This is given further credence by experimental
findings that acquisition of homonyms occurs at a
different rate to the acquisition of vocabulary. This
indicates that there are different mechanisms for
learning the meaning of lexical items and learning
to relate the meanings of lexical items. Any whole-
language model would then be composed of a com-
mon set of lexical items related by disparate rela-
tions, such as synonymy, homonymy and hyponymy.
This type of model is predicted by spreading activa-
tion (Collins and Loftus, 1975).
It is unfortunate that there is a lack of data
with which to validate this model, or our constraint
model, empirically. This should not prevent further
analysis of network models that distiguish semantic
relations, so long as this limitation is understood.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9997715">
Semantic networks have been used successfully to
explain access to the mental lexicon. We use both
expert-compiled and automatically extracted seman-
tic resources, we compare the networks formed from
semantic association and synonymy and homonymy.
These relations produce small-world networks, but
do not share the same scale-free properties as for se-
mantic association.
We find that this difference can be explained using
a constrained attachment model informed by child-
hood language acquisition experiments. It is also
predicted by spreading-activation theories of seman-
</bodyText>
<figure confidence="0.9970215">
0.001
1e-04
0.01
0.1
1
WordNet
Hyponym
Moby
*k=5
P(k)
</figure>
<page confidence="0.993932">
79
</page>
<bodyText confidence="0.9998745">
tic access where a common set of lexical items is
connected by a disparate set of relations. We further
find that distributional models of language acquisi-
tion produce relations that approximate synonymy
and networks topologically similar to synonymy-
homonymy networks.
</bodyText>
<sectionHeader confidence="0.999181" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994474">
We would like to thank the anonymous reviewers
for their helpful feedback and corrections. This
work has been supported by the Australian Research
Council under Discovery Projects DP0453131 and
DP0665973.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999951890243902">
R´eka Albert, Hawoong Jeong, and Albert-L´aszl´o Barab´asi.
2000. Error and attack tolerance of complex networks. Na-
ture, 406:378–381.
Luis A. Nunes Amaral, Antonio Scala, Marc Barth´el´emy, and
H. Eugene Stanley. 2000. Classes of small-world net-
works. Proceedings of the National Academy of Sciences,
97(21):11149–11152, October 10.
Ramon F. i Cancho and Ricard V. Sol´e. 2001. The small
world of human language. Proceedings of The Royal Society
of London. Series B, Biological Sciences, 268(1482):2261–
2265, November.
Devin M. Casenhiser. 2005. Children’s resistance to
homonymy: an experimental study of pseudohomonyms.
Journal of Child Language, 32:319–343.
Allan M. Collins and Elizabeth F. Loftus. 1975. A spreading-
activation theory of semantic processing. Psychological re-
view, 82(6):407–428.
Allan M. Collins and M. Ross Quillian. 1969. Retrieval time
from semantic memory. Journal of Verbal Learning and Ver-
bal Behavior, 8:240–247.
James R. Curran. 2004. From Distributional to Semantic Simi-
larity. Ph.D. thesis, University of Edinburgh.
Martin Doherty and Josef Perner. 1998. Metalinguistic aware-
ness and theory of mind: just two words for the same thing?
Congitive Development, 13:279–305.
Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos.
1999. On power-law relationships of the internet topology.
In Proceedings of the conference on Applications, technolo-
gies, architectures, and protocols for computer communica-
tion, pages 251–262.
Christiane Fellbaum, editor. 1998. WordNet: an electronic lex-
ical database. The MIT Press, Cambridge, MA, USA.
Wendy A. Garnham, Julie Brooks, Alan Garnham, and Anne-
Marie Ostenfeld. 2000. From synonyms to homonyms:
exploring the role of metarepresentation in language under-
standing. Developmental Science, 3(4):428–441.
James Gorman and James R. Curran. 2006. Scaling distribu-
tional similarity to large corpora. In Proceedings of the 44th
Annual Meeting of the Association for Computational Lin-
guistics, Sydney, Australia, 17–21 July.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, Boston.
Evelyn F. Keller. 2005. Revisiting ”scale-free” networks.
Bioessays, 27(10):1060–1068, October.
Adam Kilgarriff and Colin Yallop. 2000. What’s in a the-
saurus? In Proceedings of the Second International Confer-
ence on Language Resources and Evaluation, pages 1371–
1379.
Thomas K. Landauer and Susan T. Dumais. 1997. A solu-
tion to plato’s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211–240, April.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The university of south florida
word association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/.
L. John Old. 2003. The Semantic Structure ofRoget’s, a Whole-
Language Thesaurus. Ph.D. thesis, Indiana University.
Adwait Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages 133–
142, 17–18 May.
Mariano Sigman and Guillermo A. Cecchi. 2002. Global or-
ganization of the WordNet lexicon. Proceedings of the Na-
tional Academy of Sciences, 99(3):1742–1747.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The large-
scale structure of semantic networks: statistical analyses and
a model of semantic growth. Cognitive Science, 29(1):41–
78.
Christopher Stokoe. 2005. Differentiating homonymy and
polysemy in information retrieval. In Proceedings of the
Conference on Human Language Technology and Empirical
Methods in Natural Language Processing, pages 403–410.
Amos Tversky. 1977. Features of similarity. Psychological
Review, 84(4):327–352, July.
Duncan J. Watts and Steven H. Strogatz. 1998. Collective dy-
namics of small-world networks, 393:440–442, 4 June.
Duncan J. Watts. 1999. Small Worlds: The Dynamics of Net-
works between Order and Randomness. Princeton Univer-
sity Press, Princeton, NJ, USA.
Julie E. Weeds. 2003. Measures and Applications of Lexical
Distributional Similarity. Ph.D. thesis, University of Sussex.
</reference>
<page confidence="0.998246">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.509140">
<title confidence="0.999937">The Topology of Synonymy and Homonymy Networks</title>
<author confidence="0.999973">James Gorman</author>
<author confidence="0.999973">R James</author>
<affiliation confidence="0.9869015">School of Information University of</affiliation>
<address confidence="0.514498">NSW 2006,</address>
<abstract confidence="0.999377266666667">Semantic networks have been used successfully to explain access to the mental lexicon. Topological analyses of these networks have focused on acquisition and generation. We extend this work to look at models that distinguish semantic relations. We find the scale-free properties of association networks are not found in synonymy-homonymy networks, and that this is consistent with studies of childhood acquisition of these relationships. We further find that distributional models of language acquisition display similar topological properties to these networks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R´eka Albert</author>
<author>Hawoong Jeong</author>
<author>Albert-L´aszl´o Barab´asi</author>
</authors>
<title>Error and attack tolerance of complex networks.</title>
<date>2000</date>
<journal>Nature,</journal>
<pages>406--378</pages>
<marker>Albert, Jeong, Barab´asi, 2000</marker>
<rawString>R´eka Albert, Hawoong Jeong, and Albert-L´aszl´o Barab´asi. 2000. Error and attack tolerance of complex networks. Nature, 406:378–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis A Nunes Amaral</author>
<author>Antonio Scala</author>
<author>Marc Barth´el´emy</author>
<author>H Eugene Stanley</author>
</authors>
<title>Classes of small-world networks.</title>
<date>2000</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>97</volume>
<issue>21</issue>
<marker>Amaral, Scala, Barth´el´emy, Stanley, 2000</marker>
<rawString>Luis A. Nunes Amaral, Antonio Scala, Marc Barth´el´emy, and H. Eugene Stanley. 2000. Classes of small-world networks. Proceedings of the National Academy of Sciences, 97(21):11149–11152, October 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon F i Cancho</author>
<author>Ricard V Sol´e</author>
</authors>
<title>The small world of human language.</title>
<date>2001</date>
<journal>Proceedings of The Royal Society of London. Series B, Biological Sciences,</journal>
<volume>268</volume>
<issue>1482</issue>
<pages>2265</pages>
<marker>Cancho, Sol´e, 2001</marker>
<rawString>Ramon F. i Cancho and Ricard V. Sol´e. 2001. The small world of human language. Proceedings of The Royal Society of London. Series B, Biological Sciences, 268(1482):2261– 2265, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Devin M Casenhiser</author>
</authors>
<title>Children’s resistance to homonymy: an experimental study of pseudohomonyms.</title>
<date>2005</date>
<journal>Journal of Child Language,</journal>
<pages>32--319</pages>
<contexts>
<context position="2282" citStr="Casenhiser, 2005" startWordPosition="337" endWordPosition="338">ties of whole semantic networks, can be used to model phenomena not easily explained from the smaller scale data found in human experiments. These networks are typically formed from corpora, expert compiled lexical resources, or human wordassociation data. Existing work has focused language acquisition (Steyvers and Tenenbaum, 2005) and generation (Cancho and Sol´e, 2001). These models use the general notion of semantic association which subsumes all specific semantic relations, e.g. synonymy. There is evidence that there are distinct cognitive processes for different semantic relations (e.g. Casenhiser, 2005). We perform a graph analysis of synonymy, nearness of meaning, and homonymy, shared lexicalisation. We find that synonymy and homonymy produce graphs that are topologically distinct from those produced using association. They still produce smallworld networks with short path lengths but lack scale-free properties. Adding edges of different semantic relations, in particular hyponymy, produces graphs more similar to the association networks. We argue our analyses consistent with other semantic network models where nodes of a common type share edges of different types (e.g. Collins and Loftus, 1</context>
<context position="11360" citStr="Casenhiser, 2005" startWordPosition="1796" endWordPosition="1797">s is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefree networks as their association models. 3.1 Synonymy and Homonymy While there have been many studies using human subjects on the acquisition of particular semantic relations, there have been no topological studies differentiating these from the general notion of semantic association. This is interesting as psycholinguistic studies have shown that semantic relationships are distinguishable (e.g. Casenhiser, 2005). Here we consider synonymy and homonymy. There are very few cases of true synonymy, where two words are substitutable in all contexts. Nearsynonymy, where two words share some close common meaning, is more common. Sets of synonyms can be grouped together into synsets, representing a common idea. Homonymy occurs when a word has multiple meanings. Formally, homonymy is occurs when words do not share an etymological root (in linguistics) or when the distinction between meanings is coarse (in cognitive science). When the words share a root or meanings are close, the relationship is called polysem</context>
<context position="13311" citStr="Casenhiser, 2005" startWordPosition="2103" endWordPosition="2104">nd vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despite knowing each of the things independently. Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005). This is balanced by the utility of ambiguous relations for mental navigation (Sigman and Cecchi, 2002) which goes some way to explaining why they still play such a large role in language. 4 The Topology of Synonymy and Homonymy Relations For each of our resources we form a graph based on the relations between lexical items. This differs to the earlier work of Sigman and Cecchi (2002), who use synsets as vertices, and Steyvers and Tenenbaum (2005) who use both lexical items and synsets.. This is motivated largely by our automatic acquisition techniques, and also by human studies, in which we </context>
</contexts>
<marker>Casenhiser, 2005</marker>
<rawString>Devin M. Casenhiser. 2005. Children’s resistance to homonymy: an experimental study of pseudohomonyms. Journal of Child Language, 32:319–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan M Collins</author>
<author>Elizabeth F Loftus</author>
</authors>
<title>A spreadingactivation theory of semantic processing.</title>
<date>1975</date>
<journal>Psychological review,</journal>
<volume>82</volume>
<issue>6</issue>
<contexts>
<context position="1435" citStr="Collins and Loftus, 1975" startWordPosition="211" endWordPosition="214">portant role in the modelling of the organisation of lexical knowledge. In these networks, words are connected by graph edges based on their semantic relations. In recent years, researchers have found that many semantic networks are small-world, scale-free networks, having a high degree of structure and a short distance between nodes (Steyvers and Tenenbaum, 2005). Early models were taxonomic and explained some aspects of human reasoning (Collins and Quillian, 1969) (and are still used in artificial reasoning systems), but were replaced by models that focused on general graph structures (e.g. Collins and Loftus, 1975). These better modelled many observed phenomena but explained only the searching of seman73 tic space, not its generation or properties that exist at a whole-network level. Topological analyses, looking at the statistical regularities of whole semantic networks, can be used to model phenomena not easily explained from the smaller scale data found in human experiments. These networks are typically formed from corpora, expert compiled lexical resources, or human wordassociation data. Existing work has focused language acquisition (Steyvers and Tenenbaum, 2005) and generation (Cancho and Sol´e, 2</context>
<context position="2886" citStr="Collins and Loftus, 1975" startWordPosition="425" endWordPosition="429">.g. Casenhiser, 2005). We perform a graph analysis of synonymy, nearness of meaning, and homonymy, shared lexicalisation. We find that synonymy and homonymy produce graphs that are topologically distinct from those produced using association. They still produce smallworld networks with short path lengths but lack scale-free properties. Adding edges of different semantic relations, in particular hyponymy, produces graphs more similar to the association networks. We argue our analyses consistent with other semantic network models where nodes of a common type share edges of different types (e.g. Collins and Loftus, 1975). We further analyse the distributional model of language acquisition. We find that it does not well explain whole-language acquisition, but provides a model for synonym and homonym acquisition. Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73–80, Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics 2 Graph Theory Our overview of graph theory follows Watts (1999). A graph consists of a set of n vertices (nodes) and a set of edges, or arcs, which join pairs of vertices. Edges are undirected and arcs are directed. Edge</context>
<context position="8561" citStr="Collins and Loftus, 1975" startWordPosition="1359" endWordPosition="1362"> representation of knowledge, where words are connected by hyponym relations, like in the WordNet noun hierarchy (Fellbaum, 1998). While this structure predicted human reaction times for verifying facts it allows only a limited portion of knowledge to be expressed. Later models represented knowledge as semi-structured networks, and focused on explaining performance in memory retrieval tasks. One such model is spreading-activation, in which the degree to which a concept is able to be recalled is related to its similarity both to other concepts in general and to some particular prime or primes (Collins and Loftus, 1975). In this way, if one is asked to name a red vehicle, fire truck is more likely response than car: while both are strongly associated with vehicle, fire truck is more strongly associated with red than is car. More recently, graph theoretic approaches have examined the topologies of various semantic networks. Cancho and Sol´e (2001) examine graphs of English modelled from the British National Corpus. Since co-occurrence is non-trivial — words in a sentence must share some semantic content for the sentence to be coherent — edges were formed between adjacent words, with punctuation skipped. Two g</context>
<context position="26403" citStr="Collins and Loftus, 1975" startWordPosition="4222" endWordPosition="4225">s are modeling the acquisition of synonyms and homonyms, rather than all semantic relationships. This is given further credence by experimental findings that acquisition of homonyms occurs at a different rate to the acquisition of vocabulary. This indicates that there are different mechanisms for learning the meaning of lexical items and learning to relate the meanings of lexical items. Any wholelanguage model would then be composed of a common set of lexical items related by disparate relations, such as synonymy, homonymy and hyponymy. This type of model is predicted by spreading activation (Collins and Loftus, 1975). It is unfortunate that there is a lack of data with which to validate this model, or our constraint model, empirically. This should not prevent further analysis of network models that distiguish semantic relations, so long as this limitation is understood. 6 Conclusion Semantic networks have been used successfully to explain access to the mental lexicon. We use both expert-compiled and automatically extracted semantic resources, we compare the networks formed from semantic association and synonymy and homonymy. These relations produce small-world networks, but do not share the same scale-fre</context>
</contexts>
<marker>Collins, Loftus, 1975</marker>
<rawString>Allan M. Collins and Elizabeth F. Loftus. 1975. A spreadingactivation theory of semantic processing. Psychological review, 82(6):407–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan M Collins</author>
<author>M Ross Quillian</author>
</authors>
<title>Retrieval time from semantic memory.</title>
<date>1969</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>8--240</pages>
<contexts>
<context position="1280" citStr="Collins and Quillian, 1969" startWordPosition="186" endWordPosition="189">at distributional models of language acquisition display similar topological properties to these networks. 1 Introduction Semantic networks have played an important role in the modelling of the organisation of lexical knowledge. In these networks, words are connected by graph edges based on their semantic relations. In recent years, researchers have found that many semantic networks are small-world, scale-free networks, having a high degree of structure and a short distance between nodes (Steyvers and Tenenbaum, 2005). Early models were taxonomic and explained some aspects of human reasoning (Collins and Quillian, 1969) (and are still used in artificial reasoning systems), but were replaced by models that focused on general graph structures (e.g. Collins and Loftus, 1975). These better modelled many observed phenomena but explained only the searching of seman73 tic space, not its generation or properties that exist at a whole-network level. Topological analyses, looking at the statistical regularities of whole semantic networks, can be used to model phenomena not easily explained from the smaller scale data found in human experiments. These networks are typically formed from corpora, expert compiled lexical </context>
<context position="7915" citStr="Collins and Quillian (1969)" startWordPosition="1257" endWordPosition="1260"> attach to highly connected nodes. Scale-free networks result when there are no constraints. Broad-scale networks are produced when ageing and cost-to-add-link constraints are added, making it more difficult to produce very high degree hubs. Single-scale networks occur when (kv ) 2 74 these constraints are strengthened. This is one of several models for scale-free network generation, and different models will result in different internal structures and properties (Keller, 2005). 3 Semantics Networks Semantic networks represent the structure of human knowledge through the connections of words. Collins and Quillian (1969) proposed a taxonomic representation of knowledge, where words are connected by hyponym relations, like in the WordNet noun hierarchy (Fellbaum, 1998). While this structure predicted human reaction times for verifying facts it allows only a limited portion of knowledge to be expressed. Later models represented knowledge as semi-structured networks, and focused on explaining performance in memory retrieval tasks. One such model is spreading-activation, in which the degree to which a concept is able to be recalled is related to its similarity both to other concepts in general and to some particu</context>
</contexts>
<marker>Collins, Quillian, 1969</marker>
<rawString>Allan M. Collins and M. Ross Quillian. 1969. Retrieval time from semantic memory. Journal of Verbal Learning and Verbal Behavior, 8:240–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="25328" citStr="Curran, 2004" startWordPosition="4058" endWordPosition="4059">tworks, and a distinct difference with the scale-free WordNet hyponym distribution. The distributional similarity distribution is similar to that found in networks formed from LSA by Steyvers and Tenenbaum (2005). Steyvers and Tenenbaum hypothesise that the distributions produced by LSA might be due more to frequency distribution effects that correct language modelling. In light of our analysis of synonymy relations, we propose a new explanation. Given that: distributional similarity has been shown to approximate the semantic similarity in synonymy relations found in thesaurus type resources (Curran, 2004); distributional similarity produces networks with similar statistical properties to those formed by synonym and homonymy relations; and, the synonym and homonymy relations found in thesauri produce networks with different statistical properties to those found in the association networks analysed by Steyvers and Tenenbaum; it can be plausibly 1 10 100 1000 10000 k Figure 4: Degree distributions for nouns hypothesised that distributional techniques are modeling the acquisition of synonyms and homonyms, rather than all semantic relationships. This is given further credence by experimental findin</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Doherty</author>
<author>Josef Perner</author>
</authors>
<title>Metalinguistic awareness and theory of mind: just two words for the same thing? Congitive Development,</title>
<date>1998</date>
<pages>13--279</pages>
<contexts>
<context position="12740" citStr="Doherty and Perner, 1998" startWordPosition="2007" endWordPosition="2010">of polysemes (Casenhiser, 2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despite knowing each of the things independently. Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005). This is balanced by the uti</context>
</contexts>
<marker>Doherty, Perner, 1998</marker>
<rawString>Martin Doherty and Josef Perner. 1998. Metalinguistic awareness and theory of mind: just two words for the same thing? Congitive Development, 13:279–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michalis Faloutsos</author>
<author>Petros Faloutsos</author>
<author>Christos Faloutsos</author>
</authors>
<title>On power-law relationships of the internet topology.</title>
<date>1999</date>
<booktitle>In Proceedings of the conference on Applications, technologies, architectures, and</booktitle>
<pages>251--262</pages>
<contexts>
<context position="6669" citStr="Faloutsos et al., 1999" startWordPosition="1074" endWordPosition="1077">und a vertex v: JE(Γv)J Cv = where JE(Γv)J is the number of edges in the neighbourhood Γv and (2) is the total number of possible edges in Γv. The clustering coefficient C of a graph is the average over the coefficients of all the vertices. 2.2 The Scale of Networks Amaral et al. (2000) describe three classes of small world networks based on their degree distributions: Scale-free networks are characterised by their degree distribution decaying as a power law, having a small number of vertices with many links (hubs) and many vertices with few links. Networks in this class include the internet (Faloutsos et al., 1999) and semantic networks (Steyvers and Tenenbaum, 2005). Broad-scale networks are characterised by their degree distribution decaying as a power law followed by a sharp cut-off. This class includes collaborative networks (Watts and Strogatz, 1998). Single-scale networks are characterised by fast decaying degree distribution, such exponential or Gaussian, in which hubs are scarce or nonexistent. This class includes power grids (Watts and Strogatz, 1998) and airport traffic (Amaral et al., 2000). Amaral et al. (2000) model these differences using a constrained preferential attachment model, where </context>
</contexts>
<marker>Faloutsos, Faloutsos, Faloutsos, 1999</marker>
<rawString>Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos. 1999. On power-law relationships of the internet topology. In Proceedings of the conference on Applications, technologies, architectures, and protocols for computer communication, pages 251–262.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="10490" citStr="(1998)" startWordPosition="1664" endWordPosition="1664">ounal polysemy on graph navigation. Beginning with synsets and the hyponym tree, they find adding polysemy both reduces the characteristic path length and increases the clustering coefficient, producing a smallworld network. They propose, citing word priming experiments as evidence, that these changes in structure give polysemy a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks. Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several resources: the free association index collected by Nelson et al. (1998), Wordnet and the 1911 Roget’s thesaurus. All these produced scale-free networks, and, using an age of acquisition and frequency weighted preferential attachement model, show that this corresponds to age-ofacquisition norms for a small set of words. This is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefree networks as their association models. 3.1 Synonymy and Homonymy While there have been many studies using human subjects on the acquisition of p</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy A Garnham</author>
<author>Julie Brooks</author>
<author>Alan Garnham</author>
<author>AnneMarie Ostenfeld</author>
</authors>
<title>From synonyms to homonyms: exploring the role of metarepresentation in language understanding.</title>
<date>2000</date>
<journal>Developmental Science,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="12763" citStr="Garnham et al., 2000" startWordPosition="2011" endWordPosition="2014">2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despite knowing each of the things independently. Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005). This is balanced by the utility of ambiguous relat</context>
</contexts>
<marker>Garnham, Brooks, Garnham, Ostenfeld, 2000</marker>
<rawString>Wendy A. Garnham, Julie Brooks, Alan Garnham, and AnneMarie Ostenfeld. 2000. From synonyms to homonyms: exploring the role of metarepresentation in language understanding. Developmental Science, 3(4):428–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Gorman</author>
<author>James R Curran</author>
</authors>
<title>Scaling distributional similarity to large corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="22055" citStr="Gorman and Curran (2006)" startWordPosition="3518" endWordPosition="3522">man association data only includes the most similar words. 5 Distributional Similarity Networks Lexical semantic resources can be automatically extracted using distributional similarity. Here words are projected into a vector space using the contexts in which they appear as axes. Contexts can be as 1 10 100 1000 10000 k Figure 3: Degree distributions of Jaccard wide as document (Landauer and Dumais, 1997) or close as grammatical dependencies (Grefenstette, 1994). The distance between words in this space approximates the similarity measured by synonymy. We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their LARGE corpus, the method found to perform best against their goldstandard evaluation. Only words with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarriff and Yallop, 2000). Distributional similarity produces a list of vocabulary words, their similar neighbours and the similarity to the neighbours. These lists</context>
</contexts>
<marker>Gorman, Curran, 2006</marker>
<rawString>James Gorman and James R. Curran. 2006. Scaling distributional similarity to large corpora. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, 17–21 July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="21897" citStr="Grefenstette, 1994" startWordPosition="3495" endWordPosition="3496">e distribution with very few low degree nodes. The difference between this thesaurus based topicality and that found in human association data is that human association data only includes the most similar words. 5 Distributional Similarity Networks Lexical semantic resources can be automatically extracted using distributional similarity. Here words are projected into a vector space using the contexts in which they appear as axes. Contexts can be as 1 10 100 1000 10000 k Figure 3: Degree distributions of Jaccard wide as document (Landauer and Dumais, 1997) or close as grammatical dependencies (Grefenstette, 1994). The distance between words in this space approximates the similarity measured by synonymy. We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their LARGE corpus, the method found to perform best against their goldstandard evaluation. Only words with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarrif</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evelyn F Keller</author>
</authors>
<title>Revisiting ”scale-free” networks.</title>
<date>2005</date>
<journal>Bioessays,</journal>
<volume>27</volume>
<issue>10</issue>
<contexts>
<context position="7770" citStr="Keller, 2005" startWordPosition="1238" endWordPosition="1239">., 2000). Amaral et al. (2000) model these differences using a constrained preferential attachment model, where new nodes prefer to attach to highly connected nodes. Scale-free networks result when there are no constraints. Broad-scale networks are produced when ageing and cost-to-add-link constraints are added, making it more difficult to produce very high degree hubs. Single-scale networks occur when (kv ) 2 74 these constraints are strengthened. This is one of several models for scale-free network generation, and different models will result in different internal structures and properties (Keller, 2005). 3 Semantics Networks Semantic networks represent the structure of human knowledge through the connections of words. Collins and Quillian (1969) proposed a taxonomic representation of knowledge, where words are connected by hyponym relations, like in the WordNet noun hierarchy (Fellbaum, 1998). While this structure predicted human reaction times for verifying facts it allows only a limited portion of knowledge to be expressed. Later models represented knowledge as semi-structured networks, and focused on explaining performance in memory retrieval tasks. One such model is spreading-activation,</context>
</contexts>
<marker>Keller, 2005</marker>
<rawString>Evelyn F. Keller. 2005. Revisiting ”scale-free” networks. Bioessays, 27(10):1060–1068, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Colin Yallop</author>
</authors>
<title>What’s in a thesaurus?</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1371--1379</pages>
<contexts>
<context position="12509" citStr="Kilgarriff and Yallop, 2000" startWordPosition="1972" endWordPosition="1975">he words share a root or meanings are close, the relationship is called polysemy. This distinction is significant 75 in language acquisition, but as yet little research 4.1 Lexical Semantic Resources has been performed on the learning of polysemes (Casenhiser, 2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despi</context>
<context position="22516" citStr="Kilgarriff and Yallop, 2000" startWordPosition="3594" endWordPosition="3597">te, 1994). The distance between words in this space approximates the similarity measured by synonymy. We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their LARGE corpus, the method found to perform best against their goldstandard evaluation. Only words with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarriff and Yallop, 2000). Distributional similarity produces a list of vocabulary words, their similar neighbours and the similarity to the neighbours. These lists approximate synonymy by measuring substitutability in context, and do not only find synonyms as near neighbours as both antonyms and hyponyms are frequently substitutable in a grammatical context (Weeds, 2003). From this we generate graphs by taking either the k nearest neighbours to each word (k-NN), or by using a threshold. To produce a threshold we take the mean similarity of the kth neighbour of all words (*kNN). We compare both these methods. Figure 3</context>
</contexts>
<marker>Kilgarriff, Yallop, 2000</marker>
<rawString>Adam Kilgarriff and Colin Yallop. 2000. What’s in a thesaurus? In Proceedings of the Second International Conference on Language Resources and Evaluation, pages 1371– 1379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="10838" citStr="Landauer and Dumais, 1997" startWordPosition="1715" endWordPosition="1718">y a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks. Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several resources: the free association index collected by Nelson et al. (1998), Wordnet and the 1911 Roget’s thesaurus. All these produced scale-free networks, and, using an age of acquisition and frequency weighted preferential attachement model, show that this corresponds to age-ofacquisition norms for a small set of words. This is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefree networks as their association models. 3.1 Synonymy and Homonymy While there have been many studies using human subjects on the acquisition of particular semantic relations, there have been no topological studies differentiating these from the general notion of semantic association. This is interesting as psycholinguistic studies have shown that semantic relationships are distinguishable (e.g. Casenhiser, 2005). Here we consider synonymy and homonymy. There are very few cases of true syn</context>
<context position="21839" citStr="Landauer and Dumais, 1997" startWordPosition="3486" endWordPosition="3489">preference in attachment. The results of this is the gaussian-like distribution with very few low degree nodes. The difference between this thesaurus based topicality and that found in human association data is that human association data only includes the most similar words. 5 Distributional Similarity Networks Lexical semantic resources can be automatically extracted using distributional similarity. Here words are projected into a vector space using the contexts in which they appear as axes. Contexts can be as 1 10 100 1000 10000 k Figure 3: Degree distributions of Jaccard wide as document (Landauer and Dumais, 1997) or close as grammatical dependencies (Grefenstette, 1994). The distance between words in this space approximates the similarity measured by synonymy. We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their LARGE corpus, the method found to perform best against their goldstandard evaluation. Only words with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more lik</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The university of south florida word association, rhyme, and word fragment norms.</title>
<date>1998</date>
<note>http://www.usf.edu/FreeAssociation/.</note>
<contexts>
<context position="10490" citStr="Nelson et al. (1998)" startWordPosition="1661" endWordPosition="1664">e effects of nounal polysemy on graph navigation. Beginning with synsets and the hyponym tree, they find adding polysemy both reduces the characteristic path length and increases the clustering coefficient, producing a smallworld network. They propose, citing word priming experiments as evidence, that these changes in structure give polysemy a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks. Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several resources: the free association index collected by Nelson et al. (1998), Wordnet and the 1911 Roget’s thesaurus. All these produced scale-free networks, and, using an age of acquisition and frequency weighted preferential attachement model, show that this corresponds to age-ofacquisition norms for a small set of words. This is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefree networks as their association models. 3.1 Synonymy and Homonymy While there have been many studies using human subjects on the acquisition of p</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 1998. The university of south florida word association, rhyme, and word fragment norms. http://www.usf.edu/FreeAssociation/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L John Old</author>
</authors>
<title>The Semantic Structure ofRoget’s, a WholeLanguage Thesaurus.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Indiana University.</institution>
<contexts>
<context position="15270" citStr="Old, 2003" startWordPosition="2411" endWordPosition="2412">ases, pronouns, interjections, conjunctions, and interrogatives. Paragraphs with similar meaning are collated by part of speech into labeled categories. Categories are then collected into classes using a three-tiered hierarchy, with the most general concept at the top. Where a word has several senses, it will appear in several synsets. Several editions of Roget’s have been released representing the change in language since the first edition in 1852. The last freely available edition is the 1911, which uses outdated vocabulary, but the global topology has not changed with more recent editions (Old, 2003). As our analysis is not concerned with the specifics of the vocabulary, this is the edition we will use. It consists of a vocabulary of 29,460 nouns, 15,173 verbs, 13,052 adjectives and 3,005 adverbs. WordNet (Fellbaum, 1998) is an electronic lexical database. Like Roget’s, it main unit of organisation is the synset, and a word with several senses will appear in several synsets. These are divided into four parts of speech: nouns, verbs, adjectives and adverbs. Synsets are connected by semantic relationships, e.g antonymy, hyponymy and meronym. WordNet 2.1 provides a vocabulary of 117,097 noun</context>
</contexts>
<marker>Old, 2003</marker>
<rawString>L. John Old. 2003. The Semantic Structure ofRoget’s, a WholeLanguage Thesaurus. Ph.D. thesis, Indiana University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-ofspeech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="14232" citStr="Ratnaparkhi, 1996" startWordPosition="2253" endWordPosition="2254">s between lexical items. This differs to the earlier work of Sigman and Cecchi (2002), who use synsets as vertices, and Steyvers and Tenenbaum (2005) who use both lexical items and synsets.. This is motivated largely by our automatic acquisition techniques, and also by human studies, in which we can only directly access relationships between words. This also allows us to directly compare resources where we have information about synsets to those without. We distinguish parts of speech as disambiguation across them is relatively easy psychologically (Casenhiser, 2005) and computationally (e.g. Ratnaparkhi, 1996). 76 A typical resource for providing this information are manually constructed lexical semantic resources. We will consider three: Roget’s, WordNet and Moby Roget’s thesaurus is a common language thesaurus providing a hierarchy of synsets. Synsets with the same general or overlapping meaning and part of speech are collected into paragraphs. The parts of speech covered are nouns, verbs, adjectives, adverbs, prepositions, phrases, pronouns, interjections, conjunctions, and interrogatives. Paragraphs with similar meaning are collated by part of speech into labeled categories. Categories are then</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy part-ofspeech tagger. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133– 142, 17–18 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Sigman</author>
<author>Guillermo A Cecchi</author>
</authors>
<title>Global organization of the WordNet lexicon.</title>
<date>2002</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>99--3</pages>
<contexts>
<context position="9821" citStr="Sigman and Cecchi (2002)" startWordPosition="1559" endWordPosition="1562">ccurrences and the other from only those co-occurrences with a frequency greater than chance. Both models produced scale-free networks. They find this model compelling for word choice during speech, noting function words are the most highly connected. These give structure without conveying significant meaning, so can be omitted without rendering a sentence incoherent, but when unavailable render speech non-fluent. This is consistent with work by Albert et al. (2000) showing that scale-free networks are tolerant to random deletion but sensitive to targeted removal of highly connected vertices. Sigman and Cecchi (2002) investigate the structure of WordNet to study the effects of nounal polysemy on graph navigation. Beginning with synsets and the hyponym tree, they find adding polysemy both reduces the characteristic path length and increases the clustering coefficient, producing a smallworld network. They propose, citing word priming experiments as evidence, that these changes in structure give polysemy a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks. Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several re</context>
<context position="13415" citStr="Sigman and Cecchi, 2002" startWordPosition="2117" endWordPosition="2120">h rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit. Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despite knowing each of the things independently. Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005). This is balanced by the utility of ambiguous relations for mental navigation (Sigman and Cecchi, 2002) which goes some way to explaining why they still play such a large role in language. 4 The Topology of Synonymy and Homonymy Relations For each of our resources we form a graph based on the relations between lexical items. This differs to the earlier work of Sigman and Cecchi (2002), who use synsets as vertices, and Steyvers and Tenenbaum (2005) who use both lexical items and synsets.. This is motivated largely by our automatic acquisition techniques, and also by human studies, in which we can only directly access relationships between words. This also allows us to directly compare resources </context>
</contexts>
<marker>Sigman, Cecchi, 2002</marker>
<rawString>Mariano Sigman and Guillermo A. Cecchi. 2002. Global organization of the WordNet lexicon. Proceedings of the National Academy of Sciences, 99(3):1742–1747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>The largescale structure of semantic networks: statistical analyses and a model of semantic growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>78</pages>
<contexts>
<context position="1176" citStr="Steyvers and Tenenbaum, 2005" startWordPosition="171" endWordPosition="174">d that this is consistent with studies of childhood acquisition of these relationships. We further find that distributional models of language acquisition display similar topological properties to these networks. 1 Introduction Semantic networks have played an important role in the modelling of the organisation of lexical knowledge. In these networks, words are connected by graph edges based on their semantic relations. In recent years, researchers have found that many semantic networks are small-world, scale-free networks, having a high degree of structure and a short distance between nodes (Steyvers and Tenenbaum, 2005). Early models were taxonomic and explained some aspects of human reasoning (Collins and Quillian, 1969) (and are still used in artificial reasoning systems), but were replaced by models that focused on general graph structures (e.g. Collins and Loftus, 1975). These better modelled many observed phenomena but explained only the searching of seman73 tic space, not its generation or properties that exist at a whole-network level. Topological analyses, looking at the statistical regularities of whole semantic networks, can be used to model phenomena not easily explained from the smaller scale dat</context>
<context position="5332" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="854" endWordPosition="857">t D steps are required to reach any vertex from any other vertex but, on average, only L are required. For very large graphs, calculating the values for L and D is computationally difficult. We instead sample n&apos; &lt;&lt; n nodes and find the mean values of L and D across the samples. The diameter produced will always be less than or equal to the true diameter. We found n&apos; = 100 to be most efficient. It is not a requirement that every vertex be reachable from every other vertex and in these cases both L and D will be infinite. In these cases we analyse the largest connected subgraph. 1Here we follow Steyvers and Tenenbaum (2005) as it is more commonly used in the cognitive science literature. Watts (1999) defines the characteristic path length as the median of the means of shortest path lengths for each vertex. 2.1 Small-world Networks Traditional network models assume that networks are either completely random or completely regular. Many natural networks are somewhere between these two extremes. These small-world networks a have the high degree of clustering of a regular lattice and the short average path length of a random network (Watts and Strogatz, 1998). The clustering is indicative of organisation, and the sho</context>
<context position="6722" citStr="Steyvers and Tenenbaum, 2005" startWordPosition="1081" endWordPosition="1084">he number of edges in the neighbourhood Γv and (2) is the total number of possible edges in Γv. The clustering coefficient C of a graph is the average over the coefficients of all the vertices. 2.2 The Scale of Networks Amaral et al. (2000) describe three classes of small world networks based on their degree distributions: Scale-free networks are characterised by their degree distribution decaying as a power law, having a small number of vertices with many links (hubs) and many vertices with few links. Networks in this class include the internet (Faloutsos et al., 1999) and semantic networks (Steyvers and Tenenbaum, 2005). Broad-scale networks are characterised by their degree distribution decaying as a power law followed by a sharp cut-off. This class includes collaborative networks (Watts and Strogatz, 1998). Single-scale networks are characterised by fast decaying degree distribution, such exponential or Gaussian, in which hubs are scarce or nonexistent. This class includes power grids (Watts and Strogatz, 1998) and airport traffic (Amaral et al., 2000). Amaral et al. (2000) model these differences using a constrained preferential attachment model, where new nodes prefer to attach to highly connected nodes.</context>
<context position="10345" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="1639" endWordPosition="1642"> random deletion but sensitive to targeted removal of highly connected vertices. Sigman and Cecchi (2002) investigate the structure of WordNet to study the effects of nounal polysemy on graph navigation. Beginning with synsets and the hyponym tree, they find adding polysemy both reduces the characteristic path length and increases the clustering coefficient, producing a smallworld network. They propose, citing word priming experiments as evidence, that these changes in structure give polysemy a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks. Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several resources: the free association index collected by Nelson et al. (1998), Wordnet and the 1911 Roget’s thesaurus. All these produced scale-free networks, and, using an age of acquisition and frequency weighted preferential attachement model, show that this corresponds to age-ofacquisition norms for a small set of words. This is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefre</context>
<context position="13763" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="2179" endWordPosition="2182">ings independently. Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005). This is balanced by the utility of ambiguous relations for mental navigation (Sigman and Cecchi, 2002) which goes some way to explaining why they still play such a large role in language. 4 The Topology of Synonymy and Homonymy Relations For each of our resources we form a graph based on the relations between lexical items. This differs to the earlier work of Sigman and Cecchi (2002), who use synsets as vertices, and Steyvers and Tenenbaum (2005) who use both lexical items and synsets.. This is motivated largely by our automatic acquisition techniques, and also by human studies, in which we can only directly access relationships between words. This also allows us to directly compare resources where we have information about synsets to those without. We distinguish parts of speech as disambiguation across them is relatively easy psychologically (Casenhiser, 2005) and computationally (e.g. Ratnaparkhi, 1996). 76 A typical resource for providing this information are manually constructed lexical semantic resources. We will consider three:</context>
<context position="20633" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="3302" endWordPosition="3305">ph is omitted for clarity. We can see that the origP(k) 0.001 1e-04 0.01 0.1 1 Roget’s WordNet Moby Random 77 1 10 100 1000 10000 k Figure 2: Degree distributions adding hyponym relations to nouns inally broad-scale structure of the Roget’s distribution is tending to have a more gaussian distribution. The addition of hyponyms produces a power law distribution for k &gt; 10 of P(k) 7z� k−1 7. Additional constraints on attachment reduce the ability of networks to be scale-free (Amaral et al., 2000). The difference between synonymyhomonymy networks and association networks can be explained by this. Steyvers and Tenenbaum (2005) propose a plausible attachment model for their association networks which has no additional constraint function. If we use the tendency for languages to avoid lexical ambiguity from synonymy and homonymy as a constraint to the production of edges we will produce broad-scale networks rather than scale-free networks. As hyponymy is primarily semantic and does not produce lexical ambiguity, adding hyponym edges weakens the constraint on ambiguity, producing a scale-free network. Generalising synonymy to include topicality weakens the constraints, but at the same time reduces preference in attach</context>
<context position="24927" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="3994" endWordPosition="3997"> Distributional similarity (*k-NN) produces a network with similar degree, characteristic path length and diameter. The clustering coefficient is much less than that from expert resources, is still several orders of magnitude larger than an equivalent random graph (Table 1). Figure 4 compares a distributional network to networks WordNet and Moby. We can see the same broad-scale in the distributional and synonym networks, and a distinct difference with the scale-free WordNet hyponym distribution. The distributional similarity distribution is similar to that found in networks formed from LSA by Steyvers and Tenenbaum (2005). Steyvers and Tenenbaum hypothesise that the distributions produced by LSA might be due more to frequency distribution effects that correct language modelling. In light of our analysis of synonymy relations, we propose a new explanation. Given that: distributional similarity has been shown to approximate the semantic similarity in synonymy relations found in thesaurus type resources (Curran, 2004); distributional similarity produces networks with similar statistical properties to those formed by synonym and homonymy relations; and, the synonym and homonymy relations found in thesauri produce </context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Joshua B. Tenenbaum. 2005. The largescale structure of semantic networks: statistical analyses and a model of semantic growth. Cognitive Science, 29(1):41– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Stokoe</author>
</authors>
<title>Differentiating homonymy and polysemy in information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="12319" citStr="Stokoe, 2005" startWordPosition="1944" endWordPosition="1945">s. Formally, homonymy is occurs when words do not share an etymological root (in linguistics) or when the distinction between meanings is coarse (in cognitive science). When the words share a root or meanings are close, the relationship is called polysemy. This distinction is significant 75 in language acquisition, but as yet little research 4.1 Lexical Semantic Resources has been performed on the learning of polysemes (Casenhiser, 2005). It is also significant for Natural Language Processing. The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005). We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000). For simplicity, will conflate the categories under homonymy. There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000). A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny i</context>
</contexts>
<marker>Stokoe, 2005</marker>
<rawString>Christopher Stokoe. 2005. Differentiating homonymy and polysemy in information retrieval. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<volume>84</volume>
<issue>4</issue>
<contexts>
<context position="3741" citStr="Tversky, 1977" startWordPosition="561" endWordPosition="562">ts of Computational Language Acquisition, pages 73–80, Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics 2 Graph Theory Our overview of graph theory follows Watts (1999). A graph consists of a set of n vertices (nodes) and a set of edges, or arcs, which join pairs of vertices. Edges are undirected and arcs are directed. Edges and arcs can be weighted or unweighted, with weights indicating the relative strength or importance of the edges. We will only consider unweighted, undirected networks. Although there is evidence that semantic relations are both directed (Tversky, 1977) and weighted (Collins and Loftus, 1975), we do not have access to this information in a consistent and meaningful format for all our resources. Two vertices connected by an edge are called neighbours. The degree k of a vertex is the count of it neighbours. From this we measure the average degree (k) for the graph and the degree distribution P(k) for all values of k. The degree distribution is the probability of a vertex having a degree k. The neighbourhood Γv of a vertex v is the set of all neighbours of v not including v. The neighbourhood ΓS of a subgraph S is the set of all neighbours of S</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Amos Tversky. 1977. Features of similarity. Psychological Review, 84(4):327–352, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan J Watts</author>
<author>Steven H Strogatz</author>
</authors>
<title>Collective dynamics of small-world networks,</title>
<date>1998</date>
<volume>393</volume>
<contexts>
<context position="5873" citStr="Watts and Strogatz, 1998" startWordPosition="940" endWordPosition="943">nalyse the largest connected subgraph. 1Here we follow Steyvers and Tenenbaum (2005) as it is more commonly used in the cognitive science literature. Watts (1999) defines the characteristic path length as the median of the means of shortest path lengths for each vertex. 2.1 Small-world Networks Traditional network models assume that networks are either completely random or completely regular. Many natural networks are somewhere between these two extremes. These small-world networks a have the high degree of clustering of a regular lattice and the short average path length of a random network (Watts and Strogatz, 1998). The clustering is indicative of organisation, and the short paths make for easier navigation. The clustering coefficient Cv is used to measure the degree of clustering around a vertex v: JE(Γv)J Cv = where JE(Γv)J is the number of edges in the neighbourhood Γv and (2) is the total number of possible edges in Γv. The clustering coefficient C of a graph is the average over the coefficients of all the vertices. 2.2 The Scale of Networks Amaral et al. (2000) describe three classes of small world networks based on their degree distributions: Scale-free networks are characterised by their degree d</context>
<context position="7123" citStr="Watts and Strogatz, 1998" startWordPosition="1139" endWordPosition="1142"> power law, having a small number of vertices with many links (hubs) and many vertices with few links. Networks in this class include the internet (Faloutsos et al., 1999) and semantic networks (Steyvers and Tenenbaum, 2005). Broad-scale networks are characterised by their degree distribution decaying as a power law followed by a sharp cut-off. This class includes collaborative networks (Watts and Strogatz, 1998). Single-scale networks are characterised by fast decaying degree distribution, such exponential or Gaussian, in which hubs are scarce or nonexistent. This class includes power grids (Watts and Strogatz, 1998) and airport traffic (Amaral et al., 2000). Amaral et al. (2000) model these differences using a constrained preferential attachment model, where new nodes prefer to attach to highly connected nodes. Scale-free networks result when there are no constraints. Broad-scale networks are produced when ageing and cost-to-add-link constraints are added, making it more difficult to produce very high degree hubs. Single-scale networks occur when (kv ) 2 74 these constraints are strengthened. This is one of several models for scale-free network generation, and different models will result in different in</context>
</contexts>
<marker>Watts, Strogatz, 1998</marker>
<rawString>Duncan J. Watts and Steven H. Strogatz. 1998. Collective dynamics of small-world networks, 393:440–442, 4 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan J Watts</author>
</authors>
<title>Small Worlds: The Dynamics of Networks between Order and Randomness.</title>
<date>1999</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ, USA.</location>
<contexts>
<context position="3329" citStr="Watts (1999)" startWordPosition="491" endWordPosition="492">networks. We argue our analyses consistent with other semantic network models where nodes of a common type share edges of different types (e.g. Collins and Loftus, 1975). We further analyse the distributional model of language acquisition. We find that it does not well explain whole-language acquisition, but provides a model for synonym and homonym acquisition. Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73–80, Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics 2 Graph Theory Our overview of graph theory follows Watts (1999). A graph consists of a set of n vertices (nodes) and a set of edges, or arcs, which join pairs of vertices. Edges are undirected and arcs are directed. Edges and arcs can be weighted or unweighted, with weights indicating the relative strength or importance of the edges. We will only consider unweighted, undirected networks. Although there is evidence that semantic relations are both directed (Tversky, 1977) and weighted (Collins and Loftus, 1975), we do not have access to this information in a consistent and meaningful format for all our resources. Two vertices connected by an edge are calle</context>
<context position="5410" citStr="Watts (1999)" startWordPosition="869" endWordPosition="870">quired. For very large graphs, calculating the values for L and D is computationally difficult. We instead sample n&apos; &lt;&lt; n nodes and find the mean values of L and D across the samples. The diameter produced will always be less than or equal to the true diameter. We found n&apos; = 100 to be most efficient. It is not a requirement that every vertex be reachable from every other vertex and in these cases both L and D will be infinite. In these cases we analyse the largest connected subgraph. 1Here we follow Steyvers and Tenenbaum (2005) as it is more commonly used in the cognitive science literature. Watts (1999) defines the characteristic path length as the median of the means of shortest path lengths for each vertex. 2.1 Small-world Networks Traditional network models assume that networks are either completely random or completely regular. Many natural networks are somewhere between these two extremes. These small-world networks a have the high degree of clustering of a regular lattice and the short average path length of a random network (Watts and Strogatz, 1998). The clustering is indicative of organisation, and the short paths make for easier navigation. The clustering coefficient Cv is used to </context>
</contexts>
<marker>Watts, 1999</marker>
<rawString>Duncan J. Watts. 1999. Small Worlds: The Dynamics of Networks between Order and Randomness. Princeton University Press, Princeton, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie E Weeds</author>
</authors>
<date>2003</date>
<booktitle>Measures and Applications of Lexical Distributional Similarity. Ph.D. thesis,</booktitle>
<institution>University of Sussex.</institution>
<contexts>
<context position="22865" citStr="Weeds, 2003" startWordPosition="3648" endWordPosition="3649">rds with a corpus frequency higher than 100 are included. This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarriff and Yallop, 2000). Distributional similarity produces a list of vocabulary words, their similar neighbours and the similarity to the neighbours. These lists approximate synonymy by measuring substitutability in context, and do not only find synonyms as near neighbours as both antonyms and hyponyms are frequently substitutable in a grammatical context (Weeds, 2003). From this we generate graphs by taking either the k nearest neighbours to each word (k-NN), or by using a threshold. To produce a threshold we take the mean similarity of the kth neighbour of all words (*kNN). We compare both these methods. Figure 3 compares the degree distributions of these. Using k-NN produces a degree distribution that is close to a Gaussian, where as *k-NN produces a distribution much more like that of our expert compiled resources. This is unsurprising when the distribution of distributional distances is considered. Some words will have many near neighbours, 0.001 1e-04</context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie E. Weeds. 2003. Measures and Applications of Lexical Distributional Similarity. Ph.D. thesis, University of Sussex.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>