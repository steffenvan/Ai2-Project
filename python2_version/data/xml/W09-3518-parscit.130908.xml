<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.119369">
<title confidence="0.9985185">
Improving transliteration accuracy using word-origin detection and
lexicon lookup
</title>
<author confidence="0.985081">
Mitesh M. Khapra Pushpak Bhattacharyya
</author>
<affiliation confidence="0.984114">
IIT Bombay IIT Bombay
</affiliation>
<email confidence="0.989764">
miteshk@cse.iitb.ac.in pb@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.99361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969">
We propose a framework for translit-
eration which uses (i) a word-origin
detection engine (pre-processing) (ii) a
CRF based transliteration engine and (iii)
a re-ranking model based on lexicon-
lookup (post-processing). The results
obtained for English-Hindi and English-
Kannada transliteration show that the pre-
processing and post-processing modules
improve the top-1 accuracy by 7.1%.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98953606060606">
Machine transliteration is the method of automati-
cally converting Out-Of-Vocabulary (OOV) words
in one language to their phonetic equivalents in
another language. An attempt is made to retain
the original pronunciation of the source word to
as great an extent as allowed by the orthographic
and phonological rules of the target language. This
is not a great challenge for language pairs like
Hindi-Marathi which have very similar alphabetic
and phonetic sets. However, the problem becomes
non-trivial for language pairs like English-Hindi
and English-Kannada which have reasonably dif-
ferent alphabet sets and sound systems.
Machine transliteration find its application in
Cross-Lingual Information Retrieval (CLIR) and
Machine Translation (MT). In CLIR, machine
transliteration can help in translating the OOV
terms like proper names and technical terms which
frequently appear in the source language queries
(e.g. Jaipur in “Jaipur palace”). Similarly it can
help improve the performance of MT by translat-
ing proper names and technical terms which are
not present in the translation dictionary.
Current models for transliteration can be clas-
sified as grapheme-based models, phoneme-based
models and hybrid models. Grapheme-based mod-
els like source channel model (Lee and Choi,
1998), Maximum Entropy Model (Goto et al.,
2003), Conditional Random Fields (Veeravalli et
al., 2008) and Decision Trees (Kang and Choi,
2000) treat transliteration as an orthographic pro-
cess and try to map the source graphemes di-
rectly to the target graphemes. Phoneme based
models like the ones based on Weighted Finite
State Transducers (WFST) (Knight and Graehl,
1997) and extended Markov window (Jung et al.,
2000) treat transliteration as a phonetic process
rather than an orthographic process. Under this
framework, transliteration is treated as a conver-
sion from source grapheme to source phoneme
followed by a conversion from source phoneme
to target grapheme. Hybrid models either use a
combination of a grapheme based model and a
phoneme based model (Stalls and Knight, 1998)
or capture the correspondence between source
graphemes and source phonemes to produce target
language graphemes (Oh and Choi, 2002).
Combining any of the above transliteration en-
gines with pre-processing modules like word-
origin detection (Oh and Choi, 2002) and/or
post-processing modules like re-ranking using
clues from monolingual resources (Al-Onaizan
and Knight, 2002) can enhance the performance of
the system. We propose such a framework which
uses (i) language model based word-origin detec-
tion (ii) CRF based transliteration engine and (iii)
a re-ranking model based on lexicon lookup on the
target language (Hindi and Kannada in our case).
The roadmap of the paper is as follows. In
section 2 we describe the 3 components of the
proposed framework. In section 3 we present
the results for English-Hindi and English-Kannada
transliteration on the datasets (Kumaran and
Kellner, 2007) released for NEWS 2009 Ma-
chine Transliteration Shared Task1(Haizhou et al.,
2009). Section 4 concludes the paper.
</bodyText>
<footnote confidence="0.992705">
1https://translit.i2r.a-star.edu.sg/news2009/
</footnote>
<page confidence="0.982678">
84
</page>
<note confidence="0.9916315">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 84–87,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.719767" genericHeader="method">
2 Proposed framework for
Transliteration
</sectionHeader>
<figureCaption confidence="0.999434">
Figure 1: Proposed framework for transliteration.
</figureCaption>
<subsectionHeader confidence="0.992922">
2.1 Word Origin Detection
</subsectionHeader>
<bodyText confidence="0.999969">
To emphasize the importance of Word Origin De-
tection we consider the example of letter d’.
When d’ appears in a name of Western origin (e.g.
Daniel, Durban) and is not followed by the letter
V, it invariably gets transliterated as Hindi letter
T, whereas, if it appears in a name of Indic origin
(e.g. Indore, Jharkhand) then it is equally likely to
be transliterated as T or T. This shows that the de-
cision is influenced by the origin of the word. The
Indic dataset (Hindi, Kannada, and Tamil) for the
Shared Task consisted of a mix of Indic and West-
ern names. We therefore felt the need of train-
ing separate models for words of Indic origin and
words of Western origin.
For this we needed to separate the words in
the training data based on their origin. We first
manually classified 3000 words from the training
set into words of Indic origin and Western origin.
These words were used as seed input for the boot-
strapping algorithm described below:
</bodyText>
<listItem confidence="0.996741176470588">
1. Build two n-gram language models: one for
the already classified names of Indic origin
and another for the names of Western origin.
Here, by n-gram we mean n-character ob-
tained by splitting the words into a sequence
of characters.
2. Split each of the remaining words into a se-
quence of characters and find the probability
of this sequence using the two language mod-
els constructed in step 1.
3. If the probability of a word (i.e. a sequence
of characters) is higher in the Indic language
model than in the Western language model
then classify it as Indic word else classify it
as Western word.
4. Repeat steps 1-3 till all words have been clas-
sified.
</listItem>
<bodyText confidence="0.999981111111111">
Thus, we classified the entire training set into
words of Indic origin and words of Western origin.
The two language models (one for words of Indic
origin and another for words of Western origin)
thus obtained were then used to classify the test
data using steps 2 and 3 of the above algorithm.
Manual verification showed that this method was
able to determine the origin of the words in the test
data with an accuracy of 97%.
</bodyText>
<subsectionHeader confidence="0.995607">
2.2 CRF based transliteration engine
</subsectionHeader>
<bodyText confidence="0.9982556">
Conditional Random Fields (Lafferty et al., 2001)
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given
the source word is given by,
</bodyText>
<equation confidence="0.9907556">
P(Y |X, A) = 1 eEt1EK
Z(X) AkA(Yt−1,Yt,X,t)
(1)
where,
X = source word (English)
Y = target word (Hindi, Kannada)
T = length of source word (English)
K = number of features
Ak = feature weight
Z(X) = normalization constant
</equation>
<bodyText confidence="0.999474615384615">
CRF++2 which is an open source implemen-
tation of CRF was used for training and decod-
ing. GIZA++ (Och and Ney, 2000), which is a
freely available implementation of the IBM align-
ment models (Brown et al., 1993) was used to get
character level alignments for English-Hindi word
pairs in the training data. Under this alignment,
each character in the English word is aligned to
zero or more characters in the corresponding Hindi
word. The following features are then generated
using this character-aligned data (here ei and hi
are the characters at position i of the source word
and target word respectively):
</bodyText>
<listItem confidence="0.981654666666667">
• hi and ej such that i − 2 G j G i + 2
• hi and source character bigrams ( {ei_1, ei}
or {ei, ei+1})
• hi and source character trigrams ( {ei_2,
ei_1, ei} or {ei_1, ei, ei+1} or {ei, ei+1,
ei+2})
</listItem>
<footnote confidence="0.972802">
2http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.99831">
85
</page>
<listItem confidence="0.99884">
• hi, hi_1 and ej such that i − 2 &lt; j &lt; i + 2
• hi, hi_1 and source character bigrams
• hi, hi_1 and source character trigrams
</listItem>
<bodyText confidence="0.998442888888889">
Two separate models were trained: one for the
words of Indic origin and another for the words
of Western origin. At the time of testing, the
words were first classified as Indic origin words
and Western origin words using the classifier de-
scribed in section 2.1. The top-10 transliterations
for each word were then generated using the cor-
rect CRF model depending on the origin of the
word.
</bodyText>
<subsectionHeader confidence="0.999188">
2.3 Re-ranking using lexicon lookup
</subsectionHeader>
<bodyText confidence="0.999983772727273">
Since the dataset for the Shared Task contains
words of Indic origin there is a possibility that the
correct transliteration of some of these words may
be found in a Hindi lexicon. Such a lexicon con-
taining 90677 unique words was constructed by
extracting words from the Hindi Wordnet3. If a
candidate transliteration generated by the CRF en-
gine is found in this lexicon then its rank is in-
creased and it is moved towards the top of the list.
If multiple outputs are found in the lexicon then all
such outputs are moved towards the top of the list
and the relative ranking of these outputs remains
the same as that assigned by the CRF engine. For
example, if the 4th and 6th candidate generated by
the CRF engine are found in the lexicon then these
two candidates will be moved to positions 1 and 2
respectively. We admit that this way of moving
candidates to the top of the list is adhoc. Ideally, if
the lexicon also stored the frequency of each word
then the candidates could be re-ranked using these
frequencies. But unfortunately the lexicon does
not store such frequency counts.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.9999651">
The system was tested for English-Hindi and
English-Kannada transliteration using the dataset
(Kumaran and Kellner, 2007) released for NEWS
2009 Machine Transliteration Shared Task. We
submitted one standard run and one non-standard
run for the English-Hindi task and one standard
run for the English-Kannada task. The re-ranking
module was used only for the non-standard run as
it uses resources (lexicon) other than those pro-
vided for the task. We did not have a lexicon
</bodyText>
<footnote confidence="0.620621">
3http://www.cfilt.iitb.ac.in/wordnet/webhwn
</footnote>
<bodyText confidence="0.9590286">
for Kannada so were not able to apply the re-
ranking module for English-Kannada task. The
performance of the system was evaluated us-
ing 6 measures, viz., Word Accuracy in Top-
1 (ACC), Fuzziness in Top-1 (Mean F-score),
Mean Reciprocal Rank (MRR), MAPTef, MAP10
and MAPsys. Please refer to the white paper of
NEWS 2009 Machine Transliteration Shared Task
(Haizhou et al., 2009) for more details of these
measures.
Table 1 and Table 2 report the results4 for
English-Hindi and English-Kannada translitera-
tion respectively. For English-Hindi we report
3 results: (i) without any pre-processing (word-
origin detection) or post-processing (re-ranking)
(ii) with pre-processing but no post-processing and
(iii) with both pre-processing and post-processing.
The results clearly show that the addition of these
modules boosts the performance. The use of
word-origin detection boosts the top-1 accuracy by
around 0.9% and the use of lexicon lookup based
re-ranking boosts the accuracy by another 6.2%.
Thus, together these two modules give an incre-
ment of 7.1% in the accuracy. Corresponding im-
provements are also seen in the other 5 metrics.
</bodyText>
<sectionHeader confidence="0.995939" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9997816">
We presented a framework for transliteration
which uses (i) a word-origin detection engine
(pre-processing) (ii) a CRF based transliteration
engine and (iii) a re-ranking model based on
lexicon-lookup (post-processing). The results
show that this kind of pre-processing and post-
processing helps to boost the performance of
the transliteration engine. The re-ranking using
lexicon lookup is slightly adhoc as ideally the
re-ranking should take into account the frequency
of the words in the lexicon. Since such frequency
counts are not available it would be useful to find
the web counts for each transliteration candidate
using a search engine and use these web counts to
re-rank the candidates.
</bodyText>
<footnote confidence="0.82645975">
4Please note that the results reported in this paper are bet-
ter than the results we submitted to the shared task. This im-
provement was due to the correction of an error in the tem-
plate file given as input to CRF++.
</footnote>
<page confidence="0.98543">
86
</page>
<table confidence="0.997253307692308">
Method ACC Mean MRR MAPref MAPIo MAPsys
F-score
CRF Engine 0.408 0.878 0.534 0.403 0.188 0.188
(no word origin detection, no re-
ranking)
CRF Engine + 0.417 0.877 0.546 0.409 0.192 0.192
Word-Origin detection
(no re-ranking)
Standard run
CRF Engine + 0.479 0.884 0.588 0.475 0.208 0.208
Word-Origin detection +
Re-ranking
Non-Standard run
</table>
<tableCaption confidence="0.90366">
Table 1: Results for English-Kannada transliteration.
</tableCaption>
<table confidence="0.998626">
Method Accuracy Mean MRR MAPref MAPIO MAPsys
(top1) F-score
CRF Engine + 0.335 0.859 0.453 0.327 0.154 0.154
Word-Origin detection
(no re-ranking)
Standard run
</table>
<tableCaption confidence="0.956055">
Table 2: Results for English-Kannada transliteration.
</tableCaption>
<sectionHeader confidence="0.992373" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999100666666667">
B. J. Kang and K. S. Choi 2000. Automatic translitera-
tion and back-transliteration by decision tree learn-
ing. Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation, 1135-
1411.
Bonnie Glover Stalls and Kevin Knight 1998. Trans-
lating Names and Technical Terms in Arabic Text.
Proceedings of COLING/ACL Workshop on Com-
putational Approaches to Semitic Languages, 34-41.
Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervou-
chine 2009. Whitepaper of NEWS 2009 Machine
Transliteration Shared Task. Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009).
I. Goto and N. Kato and N. Uratani and T. Ehara
2003. Transliteration considering context informa-
tion based on the maximum entropy method. Pro-
ceedings of MT-Summit IX, 125132.
J. S. Lee and K. S. Choi. 1998. English to Korean
statistical transliteration for information retrieval.
Computer Processing of Oriental Languages, 17-37.
John Lafferty, Andrew McCallum, Fernando Pereira
2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence
Data. In Proceedings ofthe Eighteenth International
Conference on Machine Learning.
Jong-hoon Oh and Key-sun Choi 2002. An English-
Korean Transliteration Model Using Pronunciation
and Contextual Rules. Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING), 758-764.
Kevin Knight and Jonathan Graehl 1997. Machine
transliteration. Computational Linguistics, 128-
135.
Kumaran, A. and Kellner, Tobias 2007. A generic
framework for machine transliteration. SIGIR ’07:
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, 721-722.
Och Franz Josef and Hermann Ney 2000. Improved
Statistical Alignment Models. Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pp. 440-447
P. F. Brown, S. A. Della Pietra, and R. L. Mercer 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263-311.
Sung Young Jung and SungLim Hong and Eunok Paek
2000. An English to Korean transliteration model of
extended Markov window. Proceedings of the 18th
conference on Computational linguistics, 383-389.
Suryaganesh Veeravalli and Sreeharsha Yella and
Prasad Pingali and Vasudeva Varma 2008. Statisti-
cal Transliteration for Cross Language Information
Retrieval using HMM alignment model and CRF.
Proceedings of the 2nd workshop on Cross Lingual
Information Access (CLIA) Addressing the Infor-
mation Need of Multilingual Societies.
Yaser Al-Onaizan and Kevin Knight 2001. Translating
named entities using monolingual and bilingual re-
sources. ACL ’02: Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, 400-408.
</reference>
<page confidence="0.999474">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.375844">
<title confidence="0.926921">Improving transliteration accuracy using word-origin detection lexicon lookup</title>
<author confidence="0.996471">Mitesh M Khapra Pushpak Bhattacharyya</author>
<affiliation confidence="0.955368">IIT Bombay IIT Bombay</affiliation>
<email confidence="0.468717">miteshk@cse.iitb.ac.inpb@cse.iitb.ac.in</email>
<abstract confidence="0.996782636363636">We propose a framework for transliteration which uses (i) a word-origin engine (ii) a CRF based transliteration engine and (iii) a re-ranking model based on lexicon- The results for Englishshow that the preprocessing and post-processing modules improve the top-1 accuracy by 7.1%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B J Kang</author>
<author>K S Choi</author>
</authors>
<title>Automatic transliteration and back-transliteration by decision tree learning.</title>
<date>2000</date>
<booktitle>Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1135--1411</pages>
<contexts>
<context position="2011" citStr="Kang and Choi, 2000" startWordPosition="283" endWordPosition="286">g the OOV terms like proper names and technical terms which frequently appear in the source language queries (e.g. Jaipur in “Jaipur palace”). Similarly it can help improve the performance of MT by translating proper names and technical terms which are not present in the translation dictionary. Current models for transliteration can be classified as grapheme-based models, phoneme-based models and hybrid models. Grapheme-based models like source channel model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a p</context>
</contexts>
<marker>Kang, Choi, 2000</marker>
<rawString>B. J. Kang and K. S. Choi 2000. Automatic transliteration and back-transliteration by decision tree learning. Proceedings of the 2nd International Conference on Language Resources and Evaluation, 1135-1411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating Names and Technical Terms in Arabic Text.</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>34--41</pages>
<contexts>
<context position="2655" citStr="Stalls and Knight, 1998" startWordPosition="385" endWordPosition="388">on as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). Combining any of the above transliteration engines with pre-processing modules like wordorigin detection (Oh and Choi, 2002) and/or post-processing modules like re-ranking using clues from monolingual resources (Al-Onaizan and Knight, 2002) can enhance the performance of the system. We propose such a framework which uses (i) language model based word-origin detection (ii) CRF based transliteration engine and (iii) a re-ranking model based on lexicon lookup on th</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Glover Stalls and Kevin Knight 1998. Translating Names and Technical Terms in Arabic Text. Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic Languages, 34-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
</authors>
<title>A Kumaran, Min Zhang, Vladimir Pervouchine</title>
<date>2009</date>
<booktitle>Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<marker>Li, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, Vladimir Pervouchine 2009. Whitepaper of NEWS 2009 Machine Transliteration Shared Task. Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>N Kato</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<title>Transliteration considering context information based on the maximum entropy method.</title>
<date>2003</date>
<booktitle>Proceedings of MT-Summit IX,</booktitle>
<pages>125132</pages>
<contexts>
<context position="1917" citStr="Goto et al., 2003" startWordPosition="269" endWordPosition="272">(CLIR) and Machine Translation (MT). In CLIR, machine transliteration can help in translating the OOV terms like proper names and technical terms which frequently appear in the source language queries (e.g. Jaipur in “Jaipur palace”). Similarly it can help improve the performance of MT by translating proper names and technical terms which are not present in the translation dictionary. Current models for transliteration can be classified as grapheme-based models, phoneme-based models and hybrid models. Grapheme-based models like source channel model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phonem</context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>I. Goto and N. Kato and N. Uratani and T. Ehara 2003. Transliteration considering context information based on the maximum entropy method. Proceedings of MT-Summit IX, 125132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Lee</author>
<author>K S Choi</author>
</authors>
<title>English to Korean statistical transliteration for information retrieval.</title>
<date>1998</date>
<booktitle>Computer Processing of Oriental Languages,</booktitle>
<pages>17--37</pages>
<contexts>
<context position="1874" citStr="Lee and Choi, 1998" startWordPosition="262" endWordPosition="265">tion in Cross-Lingual Information Retrieval (CLIR) and Machine Translation (MT). In CLIR, machine transliteration can help in translating the OOV terms like proper names and technical terms which frequently appear in the source language queries (e.g. Jaipur in “Jaipur palace”). Similarly it can help improve the performance of MT by translating proper names and technical terms which are not present in the translation dictionary. Current models for transliteration can be classified as grapheme-based models, phoneme-based models and hybrid models. Grapheme-based models like source channel model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme </context>
</contexts>
<marker>Lee, Choi, 1998</marker>
<rawString>J. S. Lee and K. S. Choi. 1998. English to Korean statistical transliteration for information retrieval. Computer Processing of Oriental Languages, 17-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
</authors>
<title>Fernando Pereira</title>
<date>2001</date>
<booktitle>In Proceedings ofthe Eighteenth International Conference on Machine Learning.</booktitle>
<marker>Lafferty, McCallum, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, Fernando Pereira 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings ofthe Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-hoon Oh</author>
<author>Key-sun Choi</author>
</authors>
<title>An EnglishKorean Transliteration Model Using Pronunciation and Contextual Rules.</title>
<date>2002</date>
<booktitle>Proceedings of the 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>758--764</pages>
<contexts>
<context position="2787" citStr="Oh and Choi, 2002" startWordPosition="404" endWordPosition="407">ed on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). Combining any of the above transliteration engines with pre-processing modules like wordorigin detection (Oh and Choi, 2002) and/or post-processing modules like re-ranking using clues from monolingual resources (Al-Onaizan and Knight, 2002) can enhance the performance of the system. We propose such a framework which uses (i) language model based word-origin detection (ii) CRF based transliteration engine and (iii) a re-ranking model based on lexicon lookup on the target language (Hindi and Kannada in our case). The roadmap of the paper is as follows. In section 2 we describe the 3 components</context>
</contexts>
<marker>Oh, Choi, 2002</marker>
<rawString>Jong-hoon Oh and Key-sun Choi 2002. An EnglishKorean Transliteration Model Using Pronunciation and Contextual Rules. Proceedings of the 19th International Conference on Computational Linguistics (COLING), 758-764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="2241" citStr="Knight and Graehl, 1997" startWordPosition="320" endWordPosition="323"> technical terms which are not present in the translation dictionary. Current models for transliteration can be classified as grapheme-based models, phoneme-based models and hybrid models. Grapheme-based models like source channel model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). Combining any of the above transliteration engines w</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl 1997. Machine transliteration. Computational Linguistics, 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--722</pages>
<contexts>
<context position="3548" citStr="Kumaran and Kellner, 2007" startWordPosition="521" endWordPosition="524">ost-processing modules like re-ranking using clues from monolingual resources (Al-Onaizan and Knight, 2002) can enhance the performance of the system. We propose such a framework which uses (i) language model based word-origin detection (ii) CRF based transliteration engine and (iii) a re-ranking model based on lexicon lookup on the target language (Hindi and Kannada in our case). The roadmap of the paper is as follows. In section 2 we describe the 3 components of the proposed framework. In section 3 we present the results for English-Hindi and English-Kannada transliteration on the datasets (Kumaran and Kellner, 2007) released for NEWS 2009 Machine Transliteration Shared Task1(Haizhou et al., 2009). Section 4 concludes the paper. 1https://translit.i2r.a-star.edu.sg/news2009/ 84 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 84–87, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP 2 Proposed framework for Transliteration Figure 1: Proposed framework for transliteration. 2.1 Word Origin Detection To emphasize the importance of Word Origin Detection we consider the example of letter d’. When d’ appears in a name of Western origin (e.g. Daniel, Durban) and is not followed by the l</context>
<context position="9092" citStr="Kumaran and Kellner, 2007" startWordPosition="1486" endWordPosition="1489">remains the same as that assigned by the CRF engine. For example, if the 4th and 6th candidate generated by the CRF engine are found in the lexicon then these two candidates will be moved to positions 1 and 2 respectively. We admit that this way of moving candidates to the top of the list is adhoc. Ideally, if the lexicon also stored the frequency of each word then the candidates could be re-ranked using these frequencies. But unfortunately the lexicon does not store such frequency counts. 3 Results The system was tested for English-Hindi and English-Kannada transliteration using the dataset (Kumaran and Kellner, 2007) released for NEWS 2009 Machine Transliteration Shared Task. We submitted one standard run and one non-standard run for the English-Hindi task and one standard run for the English-Kannada task. The re-ranking module was used only for the non-standard run as it uses resources (lexicon) other than those provided for the task. We did not have a lexicon 3http://www.cfilt.iitb.ac.in/wordnet/webhwn for Kannada so were not able to apply the reranking module for English-Kannada task. The performance of the system was evaluated using 6 measures, viz., Word Accuracy in Top1 (ACC), Fuzziness in Top-1 (Me</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>Kumaran, A. and Kellner, Tobias 2007. A generic framework for machine transliteration. SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, 721-722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Och Franz Josef</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<marker>Josef, Ney, 2000</marker>
<rawString>Och Franz Josef and Hermann Ney 2000. Improved Statistical Alignment Models. Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="6694" citStr="Brown et al., 1993" startWordPosition="1060" endWordPosition="1063">, 2001) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P(Y |X, A) = 1 eEt1EK Z(X) AkA(Yt−1,Yt,X,t) (1) where, X = source word (English) Y = target word (Hindi, Kannada) T = length of source word (English) K = number of features Ak = feature weight Z(X) = normalization constant CRF++2 which is an open source implementation of CRF was used for training and decoding. GIZA++ (Och and Ney, 2000), which is a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for English-Hindi word pairs in the training data. Under this alignment, each character in the English word is aligned to zero or more characters in the corresponding Hindi word. The following features are then generated using this character-aligned data (here ei and hi are the characters at position i of the source word and target word respectively): • hi and ej such that i − 2 G j G i + 2 • hi and source character bigrams ( {ei_1, ei} or {ei, ei+1}) • hi and source character trigrams ( {ei_2, ei_1, ei} or {ei_1, ei, ei+1} or {ei, ei+1, ei+2}) 2http</context>
</contexts>
<marker>Brown, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, and R. L. Mercer 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sung Young</author>
</authors>
<title>Jung and SungLim Hong and Eunok Paek 2000. An English to Korean transliteration model of extended Markov window.</title>
<booktitle>Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>383--389</pages>
<marker>Young, </marker>
<rawString>Sung Young Jung and SungLim Hong and Eunok Paek 2000. An English to Korean transliteration model of extended Markov window. Proceedings of the 18th conference on Computational linguistics, 383-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suryaganesh Veeravalli</author>
<author>Sreeharsha Yella</author>
<author>Prasad Pingali</author>
<author>Vasudeva Varma</author>
</authors>
<title>Statistical Transliteration for Cross Language Information Retrieval using HMM alignment model and CRF.</title>
<date>2008</date>
<booktitle>Proceedings of the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies.</booktitle>
<contexts>
<context position="1970" citStr="Veeravalli et al., 2008" startWordPosition="276" endWordPosition="279">achine transliteration can help in translating the OOV terms like proper names and technical terms which frequently appear in the source language queries (e.g. Jaipur in “Jaipur palace”). Similarly it can help improve the performance of MT by translating proper names and technical terms which are not present in the translation dictionary. Current models for transliteration can be classified as grapheme-based models, phoneme-based models and hybrid models. Grapheme-based models like source channel model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source graphemes directly to the target graphemes. Phoneme based models like the ones based on Weighted Finite State Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under this framework, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a comb</context>
</contexts>
<marker>Veeravalli, Yella, Pingali, Varma, 2008</marker>
<rawString>Suryaganesh Veeravalli and Sreeharsha Yella and Prasad Pingali and Vasudeva Varma 2008. Statistical Transliteration for Cross Language Information Retrieval using HMM alignment model and CRF. Proceedings of the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Translating named entities using monolingual and bilingual resources.</title>
<date>2001</date>
<booktitle>ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>400--408</pages>
<marker>Al-Onaizan, Knight, 2001</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight 2001. Translating named entities using monolingual and bilingual resources. ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, 400-408.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>