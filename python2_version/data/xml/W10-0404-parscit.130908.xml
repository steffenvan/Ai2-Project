<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000617">
<title confidence="0.975269">
The Design of a Proofreading Software Service
</title>
<author confidence="0.913686">
Raphael Mudge
</author>
<affiliation confidence="0.497343">
Automattic
</affiliation>
<address confidence="0.392858">
Washington, DC 20036
</address>
<email confidence="0.934778">
raffi@automattic.com
</email>
<sectionHeader confidence="0.998181" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995664375">
Web applications have the opportunity to
check spelling, style, and grammar using a
software service architecture. A software ser-
vice authoring aid can offer contextual spell
checking, detect real word errors, and avoid
poor grammar checker suggestions through
the use of large language models. Here we
present After the Deadline, an open source au-
thoring aid, used in production on Word-
Press.com, a blogging platform with over ten
million writers. We discuss the benefits of the
software service environment and how it af-
fected our choice of algorithms. We summa-
rize our design principles as speed over
accuracy, simplicity over complexity, and do
what works.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889166666667">
On the web, tools to check writing lag behind those
offered on the desktop. No online word processing
suite has a grammar checker yet. Few major web
applications offer contextual spell checking. This
is a shame because web applications have an op-
portunity to offer authoring aids that are a genera-
tion beyond the non-contextual spell-check most
applications offer.
Here we present After the Deadline, a production
software service that checks spelling, style, and
grammar on WordPress.com1, one of the most
popular blogging platforms. Our system uses a
</bodyText>
<footnote confidence="0.988308666666667">
1 An After the Deadline add-on for the Firefox web browser is
available. We also provide client libraries for embedding into
other applications. See http://www.afterthedeadline.com.
</footnote>
<page confidence="0.996463">
24
</page>
<bodyText confidence="0.999682166666667">
software service architecture. In this paper we dis-
cuss how this system works, the trade-offs of the
software service environment, and the benefits.
We conclude with a discussion of our design prin-
ciples: speed over accuracy, simplicity over com-
plexity, and do what works.
</bodyText>
<subsectionHeader confidence="0.973578">
1.1 What is a Software Service?
</subsectionHeader>
<bodyText confidence="0.9999092">
A software service (Turner et al., 2003) is an ap-
plication that runs on a server. Client applications
post the expected inputs to the server and receive
the output as XML.
Our software service checks spelling, style, and
grammar. A client connects to our server, posts
the text, and receives the errors and suggestions as
XML. Figure 1 shows this process. It is the cli-
ent’s responsibility to display the errors and pre-
sent the suggestions to the user.
</bodyText>
<figureCaption confidence="0.991108">
Figure 1. After the Deadline Client/Server Interaction.
</figureCaption>
<note confidence="0.9626875">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 24–32,
Los Angeles, California, June 2010. @2010 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.851507">
1.2 Applications
</subsectionHeader>
<bodyText confidence="0.999985692307693">
One could argue that web browsers should provide
spell and grammar check features for their users.
Internet Explorer, the most used browser (Stat-
Counter, 2010), offers no checking. Firefox offers
spell checking only. Apple’s Safari web browser
has non-contextual spell and grammar checking.
Application developers should not wait for the
browsers to catch up. Using a software service
architecture, applications can provide the same
quality checking to their users regardless of the
client they connect with. This is especially rele-
vant as more users begin to use web applications
from mobile and tablet devices.
</bodyText>
<subsectionHeader confidence="0.884581">
1.3 Benefits
</subsectionHeader>
<bodyText confidence="0.999979413793104">
A software service application has the advantage
that it can use the complete CPU and memory re-
sources of the server. Clients hoping to offer the
same level of proofreading, without a software ser-
vice, will use more resources on the local system to
store and process the language models.
Our system uses large memory-resident lan-
guage models to offer contextually relevant spell-
ing suggestions, detect real word errors, and
automatically find exceptions to our grammar
rules.
On disk our language model for English is
165MB uncompressed, 32MB compressed. We
use hash tables to allow constant time access to the
language model data. In memory our English lan-
guage model expands to 1GB of RAM. The mem-
ory footprint of our language model is too large for
a web browser or a mobile client.
A software service also has maintenance advan-
tages. The grammar rules and spell checker dic-
tionary are maintained in one place. Updates to
these immediately benefit all clients that use the
service.
In this environment, users lose the ability to up-
date their spell checker dictionary directly. To
compensate, clients can offer users a way to al-
ways ignore errors. Our WordPress plugin allows
users to ignore any error. Ignored errors are not
highlighted in future checks.
</bodyText>
<subsectionHeader confidence="0.998578">
1.4 Operating Requirements
</subsectionHeader>
<bodyText confidence="0.999891913043478">
A software service authoring aid must be able to
respond to multiple clients using the service at the
same time. Our service regularly processes over
100,000 requests a day on a single server.
Our goal is to process one thousand words per
second under this load.
Since our system works in the web environment,
it must process both text and HTML. We use a
regular expression to remove HTML from text sent
to the service.
It’s important that our service report errors in a
way that the client can locate them. The error
phrase alone is not enough because suggestions
may differ based on the context of the error.
We take a shortcut and provide clients with the
text used to match the error and the word that pre-
cedes the error phrase. For example, for indefinite
article errors, the text used to match the error is the
misused article and the word following it. The
client searches for this marker word followed by
the error text to find the error and present the cor-
rect suggestions. This scheme is not perfect, but it
simplifies our client and server implementations.
</bodyText>
<sectionHeader confidence="0.996119" genericHeader="method">
2 Language Model
</sectionHeader>
<bodyText confidence="0.99991864">
Our system derives its smarts from observed lan-
guage use. We construct our language model by
counting the number of times we see each se-
quence of two words in a corpus of text. These
sequences are known as bigrams. Our language
model is case sensitive.
We trained our bigram language model using
text from the Simple English edition of Wikipedia
(Wikimedia, 2010), Project Gutenberg (Hart,
2008), and several blogs. We bootstrapped this
process by using Wikipedia and Project Gutenberg
data. We then evaluated the contents of several
blogs looking for low occurrences of commonly
misspelled words and real word errors. Blogs that
had a low occurrence of errors were then added to
our corpus. Our corpus has about 75 million
words.
We also store counts for sequences of three
words that end or begin with a potentially confused
word. A potentially confused word is a word asso-
ciated with a confusion set (see section 4.1). The
real word error detector feature relies on these con-
fusion sets. These counts are known as trigrams.
We limit the number of trigrams stored to reduce
the memory requirements.
</bodyText>
<page confidence="0.996508">
25
</page>
<bodyText confidence="0.999444666666667">
tions for the word. The final step is to sort these
suggestions with the goal of placing the intended
word in the first position.
</bodyText>
<subsectionHeader confidence="0.945623">
2.1 Functions
</subsectionHeader>
<bodyText confidence="0.998989947368421">
Throughout this paper we will use the following
functions to refer to our language model.
P(word): This function is the probability of a word.
We divide the number of times the word occurs by
the total number of words observed in our corpus
to calculate the probability of a word.
P(wordn , wordn+1): This function is the probability
of the sequence wordn wordn+1. We divide the
number of times the sequence occurs by the total
number of words observed in our corpus to calcu-
late the probability of the sequence.
Pn(wordn|wordn-1): This function is the probability
of a word given the previous word. We calculate
this with the count of the wordn-1 wordn sequence
divided by the count of the occurrences of wordn.
Pp(wordn|wordn+1): This function is the probability
of a word given the next word. We use Bayes’
Theorem to flip the conditional probability. We
calculate this result as: Pp(wordn|wordn+1) =
</bodyText>
<equation confidence="0.793314">
Pn(wordn+1|wordn) * P(wordn) / P(wordn+1).
</equation>
<bodyText confidence="0.966803642857143">
Pn(wordn|wordn-1, wordn-2): This function is the
probability of a word given the previous two
words. The function is calculated as the count of
the wordn-2 wordn-1 wordn sequence divided by the
count of the wordn-2 wordn-1 sequence.
Pn(wordn+1, wordn+2|wordn): is the probability of a
sequence of two words given the word that pre-
cedes them. This is calculated as the count of
wordn wordn+1 wordn+2 sequence divided by the
count of the occurrences of wordn.
Pp(wordn|wordn+1, wordn+2): This function is the
probability of a word given the next two words.
We calculate this result with Pn(wordn+1,
wordn+2|wordn) * P(wordn) / P(wordn+1, wordn+2).
</bodyText>
<sectionHeader confidence="0.975976" genericHeader="method">
3 Spell Checking
</sectionHeader>
<bodyText confidence="0.9999656">
Spell checkers scan a document word by word and
follow a three-step process. The first step is to
check if the word is in the spell checker’s diction-
ary. If it is, then the word is spelled correctly. The
second step is to generate a set of possible sugges-
</bodyText>
<subsectionHeader confidence="0.999274">
3.1 The Spell Checker Dictionary
</subsectionHeader>
<bodyText confidence="0.999574105263158">
The dictionary size is a matter of balance. Too
many words and misspelled words will go unno-
ticed. Too few words and the user will see more
false positive suggestions.
We used public domain word-lists (Atkinson,
2008) to create a master word list to generate our
spell checker dictionary. We added to this list by
analyzing popular blogs for frequently occurring
words that were missing from our dictionary. This
analysis lets us include new words in our master
word list of 760,211 words.
Our spell checker dictionary is the intersection of
this master word list and words found in our cor-
pus. We do this to prevent some misspelled words
from making it into our spell checker dictionary.
We only allow words that pass a minimal count
threshold into our dictionary. We adjust this
threshold to keep our dictionary size around
125,000 words.
</bodyText>
<table confidence="0.999196833333333">
Threshold Words Present Words Accuracy
1 161,879 233 87.9%
2 116,876 149 87.8%
3 95,910 104 88.0%
4 82,782 72 88.3%
5 73,628 59 88.6%
</table>
<tableCaption confidence="0.999146">
Table 1. Dictionary Inclusion Threshold.
</tableCaption>
<bodyText confidence="0.997362428571429">
Table 1 shows the effect of this threshold on the
dictionary size, the number of present words from
Wikipedia’s List of Common Misspellings
(Wikipedia, 2009), and the accuracy of a non-
contextual version of our spell checker. We will
refer to the Wikipedia Common Misspellings list
as WPCM through the rest of this paper.
</bodyText>
<subsectionHeader confidence="0.999414">
3.2 Generating Suggestions
</subsectionHeader>
<bodyText confidence="0.9996926">
To generate suggestions our system first considers
all words within an edit distance of two. An edit is
defined as inserting a letter, deleting a letter, sub-
stituting a letter, or transposing two letters (Dam-
erau, 1964).
</bodyText>
<page confidence="0.972629">
26
</page>
<bodyText confidence="0.995168">
Consider the word post. Here are several words
that are within one edit:
</bodyText>
<construct confidence="0.932899166666667">
cost substitute p, c
host substitute p, h
most substitute p, m
past substitute o, a
pest substitute o, e
poet substitute s, e
</construct>
<bodyText confidence="0.99999312">
The naïve approach to finding words within one
edit involves making all possible edits to the mis-
spelled word using our edit operations. You may
remove any words that are not in the dictionary to
arrive at the final result. Apply the same algorithm
to all word and non-word results within one edit of
the misspelled word to find all words within two
edits.
We store our dictionary as a Trie and generate
edits by walking the Trie looking for words that
are reachable in a specified number of edits. While
this is faster than the naïve approach, generating
suggestions is the slowest part of our spell checker.
We cache these results in a global least-recently-
used cache to mitigate this performance hit.
We find that an edit distance of two is sufficient
as 97.3% of the typos in the WPCM list are two
edits from the intended word. When no sugges-
tions are available within two edits, we consider
suggestions three edits from the typo. 99% of the
typos from the WPCM list are within three edits.
By doing this we avoid affecting the accuracy of
the sorting step in a negative way and make it pos-
sible for the system to suggest the correct word for
severe typos.
</bodyText>
<subsectionHeader confidence="0.999837">
3.3 Sorting Suggestions
</subsectionHeader>
<bodyText confidence="0.999823608695652">
The sorting step relies on a score function that ac-
cepts a typo and suggestion as parameters. The
perfect score function calculates the probability of
a suggestion given the misspelled word (Brill and
Moore, 2000).
We approximate our scoring function using a
neural network. Our neural network is a multi-
layer perceptron network, implemented as de-
scribed in Chapter 4 of Programming Collective
Intelligence (Segaran, 2007). We created a train-
ing data set for our spelling corrector by combining
misspelled words from the WPCM list with ran-
dom sentences from Wikipedia.
Our neural network sees each typo (wordn) and
suggestion pair as several features with values
ranging from 0.0 to 1.0. During training, the neu-
ral network is presented with examples of sugges-
tions and typos with the expected score. From
these examples the neural network converges on an
approximation of our score function.
We use the following features to train a neural
network to calculate our suggestion scoring func-
tion:
</bodyText>
<equation confidence="0.979539">
editDistance(suggestion, wordn)
firstLetterMatch(suggestion, wordn)
Pn(suggestion|wordn-1)
Pp(suggestion|wordn+1)
P(suggestion)
</equation>
<bodyText confidence="0.997189870967742">
We calculate the edit distance using the Dam-
erau–Levenshtein algorithm (Wagner and Fischer,
1974). This algorithm recognizes insertions, sub-
stitutions, deletions, and transpositions as a single
edit. We normalize this value for the neural net-
work by assigning 1.0 to an edit distance of 1 and
0.0 to any other edit distance. We do this to pre-
vent the occasional introduction of a correct word
with an edit distance of three from skewing the
neural network.
The firstLetterMatch function returns 1.0 when
the first letters of the suggestion and the typo
match. This is based on the observation that most
writers get the first letter correct when attempting
to a spell a word. In the WPCM list, this is true for
96.0% of the mistakes. We later realized this cor-
rector performed poorly for errors that swapped the
first and second letter (e.g., oyu ĺ you). We then
updated this feature to return 1.0 if the first and
second letters were swapped.
We also use the contextual fit of the suggestion
from the language model. Both the previous and
next word are used. Consider the following exam-
ple:
The written wrd.
Here wrd is a typo for word. Now consider two
suggestions word and ward. Both are an edit dis-
tance of one from wrd. Both words also have a
first letter match. Pp(ward|written) is 0.00% while
Pp(word|written) is 0.17%. Context makes the
difference in this example.
</bodyText>
<table confidence="0.788200666666667">
pose substitute t, e
posit insert i
posts insert s
pot delete e
pots transpose s, t
pout substitute s, u
</table>
<page confidence="0.970601">
27
</page>
<subsectionHeader confidence="0.773597">
3.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999442071428572">
To evaluate our spelling corrector we created two
testing data sets. We used the typo and word pairs
from the WPCM list merged with random sen-
tences from our Project Gutenberg corpus. We
also used the typo and word pairs from the ASpell
data set (Atkinson, 2002) merged with sentences
from the Project Gutenberg corpus.
We measure our accuracy with the method de-
scribed in Deorowicz and Ciura (2005). For com-
parison we present their numbers for ASpell and
several versions of Microsoft Word along with
ours in Tables 2 and 3. We also show the number
of misspelled words present in each system’s spell
checker dictionary.
</bodyText>
<table confidence="0.999243333333333">
Present Words Accuracy
ASpell (normal) 14 56.9%
MS Word 97 18 59.0%
MS Word 2000 20 62.6%
MS Word 2003 20 62.8%
After the Deadline 53 66.1%
</table>
<tableCaption confidence="0.997471">
Table 2. Corrector Accuracy: ASpell Data.
</tableCaption>
<table confidence="0.9998755">
Present Words Accuracy
ASpell (normal) 44 84.7%
MS Word 97 31 89.0%
MS Word 2000 42 92.5%
MS Word 2003 41 92.6%
After the Deadline 143 92.7%
</table>
<tableCaption confidence="0.999812">
Table 3. Corrector Accuracy: WPCM Data.
</tableCaption>
<bodyText confidence="0.9999275">
The accuracy number measures both the sugges-
tion generation and sorting steps. As with the ref-
erenced experiment, we excluded misspelled
entries that existed in the spell checker dictionary.
Note that the present words number from Table 1
differs from Table 3 as these experiments were
carried out at different times in the development of
our technology.
</bodyText>
<sectionHeader confidence="0.998959" genericHeader="method">
4 Real Word Errors
</sectionHeader>
<bodyText confidence="0.9997776">
Spell checkers are unable to detect an error when a
typo results in a word contained in the dictionary.
These are called real word errors. A good over-
view of real word error detection and correction is
Pedler (2007).
</bodyText>
<subsectionHeader confidence="0.996327">
4.1 Confusion Sets
</subsectionHeader>
<bodyText confidence="0.999967666666667">
Our real word error detector checks 1,603 words,
grouped into 741 confusion sets. A confusion set
is two or more words that are often confused for
each other (e.g., right and write). Our confusion
sets were built by hand using a list of English
homophones as a starting point.
</bodyText>
<subsectionHeader confidence="0.992388">
4.2 Real Word Error Correction
</subsectionHeader>
<bodyText confidence="0.99909925">
The real word error detector scans the document
finding words associated with a confusion set. For
each of these words the real word error detector
uses a score function to sort the confusion set. The
score function approximates the likelihood of a
word given the context. Any words that score
higher than the current word are presented to the
user as suggestions.
When determining an error, we bias heavily for
precision at the expense of recall. We want users
to trust the errors when they’re presented.
We implement the score function as a neural
network. We inserted errors into sentences from
our Wikipedia corpus to create a training corpus.
The neural network calculates the score function
using:
</bodyText>
<equation confidence="0.9956728">
Pn(suggestion|wordn-1)
Pp(suggestion|wordn+1)
Pn(suggestion|wordn-1, wordn-2)
Pp(suggestion|wordn+1, wordn+2)
P(suggestion)
</equation>
<bodyText confidence="0.999984">
With the neural network our software is able to
consolidate the information from these statistical
features. The neural network also gives us a back-
off method, as the neural network will deal with
situations that have trigrams and those that don’t.
While using our system, we’ve found some
words experience a higher false positive rate than
others (e.g., to/too). Our approach is to remove
these difficult-to-correct words from our confusion
sets and use hand-made grammar rules to detect
when they are misused.
</bodyText>
<subsectionHeader confidence="0.994673">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.997645333333333">
We use the dyslexic spelling error corpus from
Pedler’s PhD thesis (2007) to evaluate the real
word error correction ability of our system. 97.8%
</bodyText>
<page confidence="0.996764">
28
</page>
<bodyText confidence="0.999736266666666">
of the 835 errors in this corpus are real-word er-
rors.
Our method is to provide all sentences to each
evaluated system, accept the first suggestion, and
compare the corrected text to the expected an-
swers. For comparison we present numbers for
Microsoft Word 2007 Windows, Microsoft Word
2008 on MacOS X, and the MacOS X 10.6 built-in
grammar and spell checker. Table 4 shows the
results.
Microsoft Word 2008 and the MacOS X built-in
proofreading tools do not have the benefit of a sta-
tistical technique for real-word error detection.
Microsoft Word 2007 has a contextual spell-
checking feature.
</bodyText>
<table confidence="0.9997524">
Precision Recall
MS Word 07 - Win 90.0% 40.8%
After the Deadline 89.4% 27.1%
MS Word 08 - Mac 79.7% 17.7%
MacOS X built-in 88.5% 9.3%
</table>
<tableCaption confidence="0.999332">
Table 4. Real Word Error Correction Performance.
</tableCaption>
<bodyText confidence="0.999930142857143">
Most grammar checkers (including After the
Deadline) use grammar rules to detect common
real-word errors (e.g., a/an). Table 4 shows the
systems with statistical real-word error correctors
are advantageous to users. These systems correct
far more errors than those that only rely on a rule-
based grammar checker.
</bodyText>
<sectionHeader confidence="0.992628" genericHeader="evaluation">
5 Grammar and Style Checking
</sectionHeader>
<bodyText confidence="0.99974165625">
The grammar and style checker works with
phrases. Our rule-based grammar checker finds
verb and determiner agreement errors, locates
some missing prepositions, and flags plural phrases
that should indicate possession. The grammar
checker also adds to the real-word error detection,
using a rule-based approach to detect misused
words. The style checker points out complex ex-
pressions, redundant phrases, clichés, double nega-
tives, and it flags passive voice and hidden verbs.
Our system prepares text for grammar checking
by segmenting the raw text into sentences and
words. Each word is tagged with its relevant part-
of-speech (adjective, noun, verb, etc.). The system
then applies several grammar and style rules to this
marked up text looking for matches. Grammar
rules consist of regular expressions that match on
parts-of-speech, word patterns, and sentence begin
and end markers.
Our grammar checker does not do a deep parse
of the sentence. This prevents us from writing
rules that reference the sentence subject, verb, and
object directly. In practice this means we’re un-
able to rewrite passive voice for users and create
general rules to catch many subject-verb agreement
errors.
Functionally, our grammar and style checker is
similar to Language Tool (Naber, 2003) with the
exception that it uses the language model to filter
suggestions that don’t fit the context of the text
they replace, similar to work from Microsoft Re-
search (Gamon, et al 2008).
</bodyText>
<subsectionHeader confidence="0.99539">
5.1 Text Segmentation
</subsectionHeader>
<bodyText confidence="0.999982">
Our text segmentation function uses a rule-based
approach similar to Yona (2002) to split raw text
into paragraphs, sentences, and words. The seg-
mentation is good enough for most purposes.
Because our sentence segmentation is wrong at
times, we do not notify a user when they fail to
capitalize the first word in a sentence.
</bodyText>
<subsectionHeader confidence="0.999827">
5.2 Part-of-Speech Tagger
</subsectionHeader>
<bodyText confidence="0.999860571428572">
A tagger labels each word with its relevant part-of-
speech. These labels are called tags. A tag is a
hint about the grammatical category of the word.
Such tagging allows grammar and style rules to
reference all nouns or all verbs rather than having
to account for individual words. Our system uses
the Penn Tagset (Marcus et al, 1993).
</bodyText>
<equation confidence="0.799298">
The/DT little/JJ dog/NN
laughed/VBD
</equation>
<bodyText confidence="0.999966083333333">
Here we have tagged the sentence The little dog
laughed. The is labeled as a determiner, little is an
adjective, dog is a noun, and laughed is a past
tense verb.
We can reference little, large, and mean laugh-
ing dogs with the pattern The .*/JJ dog laughed.
Our grammar checker separates phrases and tags
with a forward slash character. This is a common
convention.
The part-of-speech tagger uses a mixed statisti-
cal and rule-based approach. If a word is known
and has tags associated with it, the tagger tries to
</bodyText>
<page confidence="0.995056">
29
</page>
<bodyText confidence="0.938043">
find the tag that maximizes the following probabil-
ity:
</bodyText>
<equation confidence="0.995685">
P(tagn|wordn) * P(tagn|tagn-1, tagn-2)
</equation>
<bodyText confidence="0.999863777777778">
For words that are not known, an alternate
model containing tag probabilities based on word
endings is consulted. This alternate model uses the
last three letters of the word. Again the goal is to
maximize this probability.
We apply rules from Brill’s tagger (Brill, 1995)
to fix some cases of known incorrect tagging. Ta-
ble 5 compares our tagger accuracy for known and
unknown words to a probabilistic tagger that
</bodyText>
<table confidence="0.86579525">
maximizes P(tagn|wordn) only.
Tagger Known Unknown
Probability Tagger 91.9% 72.9%
Trigram Tagger 94.0% 76.7%
</table>
<tableCaption confidence="0.998836">
Table 5. POS Tagger Accuracy.
</tableCaption>
<bodyText confidence="0.99984525">
To train the tagger we created training and test-
ing data sets by running the Stanford POS tagger
(Toutanova and Manning, 2000) against the
Wikipedia and Project Gutenberg corpus data.
</bodyText>
<subsectionHeader confidence="0.999334">
5.3 Rule Engine
</subsectionHeader>
<bodyText confidence="0.900888">
It helps to think of a grammar checker as a lan-
guage for describing phrases. Phrases that match a
grammar rule return suggestions that are trans-
forms of the matched phrase.
Some rules are simple string substitutions (e.g.,
utilized ĺ used). Others are more complex. Con-
sider the following phrase:
I wonder if this is your com-
panies way of providing sup-
port?
This phrase contains an error. The word compa-
nies should be possessive not plural. To create a
rule to find this error, we first look at how our sys-
tem sees it:
I/PRP wonder/VBP if/IN
this/DT is/VBZ your/PRP$ com-
panies/NNS way/NN of/IN pro-
viding/VBG support/NN
A rule to capture this error is:
your .*/NNS .*/NN
This rule looks for a phrase that begins with the
word your, followed by a plural noun, followed by
another noun. When this rule matches a phrase,
suggestions are generated using a template speci-
fied with the rule. The suggestion for this rule is:
your \1:possessive \2
Suggestions may reference matched words with
\n, where n is the nth word starting from zero.
This suggestion references the second and third
words. It also specifies that the second word
should be transformed to possessive form. Our
system converts the plural word to a possessive
form using the \1:possessive transform.
Phrase Score
your companies way 0.000004%
your company’s way 0.000030%
Table 6. Grammar Checker Statistical Filtering.
Before presenting suggestions to the user, our
system queries the language model to decide which
suggestions fit in the context of the original text.
Rules may specify which context fit function
they want to use. The default context fit function
</bodyText>
<equation confidence="0.783716333333333">
is: Pn(wordn|wordn-1) + Pp(wordn|wordn+1) &gt;
(0.5 x [Pn(wordn|wordn-1) + Pp(wordn|wordn+1)]) +
0.00001.
</equation>
<bodyText confidence="0.993931466666667">
This simple context fit function gets rid of many
suggestions. Table 6 shows the scores from our
example. Here we see that the suggestion scores
nearly ten times higher than the original text.
This statistical filtering is helpful as it relieves
the rule developer from the burden of finding ex-
ceptions to the rule. Consider the rules to identify
the wrong indefinite article:
a [aeiouyhAEIOUYH18]\w+
an [^aeiAEIMNRSX8]\w+
One uses a when the next word has a consonant
sound and an when it has a vowel sound. Writing
rules to capture this is wrought with exceptions. A
rule can’t capture a sound without hard coding
each exception. For this situation we use a context
</bodyText>
<page confidence="0.993768">
30
</page>
<bodyText confidence="0.994443">
fit function that calculates the statistical fit of the
indefinite article with the following word. This
saves us from having to manually find exceptions.
</bodyText>
<figureCaption confidence="0.999092">
Figure 2. Rule Tree Example.
</figureCaption>
<bodyText confidence="0.999985846153846">
Each rule describes a phrase one word and tag
pattern at a time. For performance reasons, the
first token must be a word or part-of-speech tag.
No pattern matching is allowed in the first token.
We group rules with a common first word or tag
into an n-ary rule tree. Rules with common pattern
elements are grouped together until the word/tag
patterns described by the rule diverges from exist-
ing patterns. Figure 2 illustrates this.
When evaluating text, our system checks if there
is a rule tree associated with the current word or
tag. If there is, our system walks the tree looking
for the deepest match. Each shaded node in Figure
2 represents a potential match. Associated with
each node are suggestions and hints for the statisti-
cal checker.
We measure the number of rules in our system
by counting the number of nodes that result in a
grammar rule match. Figure 2 represents six dif-
ferent grammar rules. Our system has 33,732 rules
to check for grammar and style errors.
The capabilities of the grammar checker are lim-
ited by our imagination and ability to create new
rules. We do not present the precision and recall
of the grammar checker, as the coverage of our
hand-made rules is not the subject of this paper.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999055666666667">
Our approach to developing a software service
proofreader is summarized with the following
principles:
</bodyText>
<listItem confidence="0.999935666666667">
• Speed over accuracy
• Simplicity over complexity
• Do what works
</listItem>
<bodyText confidence="0.999702459459459">
In natural language processing there are many
opportunities to choose speed over accuracy. For
example, when tagging a sentence one can use a
Hidden Markov Model tagger or a simple trigram
tagger. In these instances we made the choice to
trade accuracy for speed.
When implementing the smarts of our system,
we’ve opted to use simpler algorithms and focus
on acquiring more data and increasing the quality
of data our system learns from. As others have
pointed out (Banko and Brill, 2001), with enough
data the complex algorithms with their tricks cease
to have an advantage over the simpler methods.
Our real-word error detector is an example of
simplicity over complexity. With our simple tri-
gram language model, we were able to correct
nearly a quarter of the errors in the dyslexic writer
corpus. We could improve the performance of our
real-word error corrector simply by adding more
confusion sets.
We define “do what works” as favoring mixed
strategies for finding and correcting errors. We
use both statistical and rule-based methods to de-
tect real word errors and correct grammar mis-
takes.
Here we’ve shown a production software service
system used for proofreading documents. While
designing this system for production we’ve noted
several areas of improvement. We’ve explained
how we implemented a comprehensive proofread-
ing solution using a simple language model and a
few neural networks. We’ve also shown that there
are advantages to a software service from the use
of large language models.
After the Deadline is available under the GNU
General Public License. The code and models are
available at http://open.afterthedeadline.com.
</bodyText>
<sectionHeader confidence="0.998022" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.978987">
The author would like to acknowledge the review
committee for their questions and suggestions.
The author would also like to acknowledge Niko-
lay Bachiyski, Michael Yoshitaka Erlewine, and
Dr. Charles Wallace who offered comments on
drafts of this paper.
</bodyText>
<page confidence="0.999841">
31
</page>
<sectionHeader confidence="0.998289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987729113924">
Kevin Atkinson. 2008, Kevin’s Wordlist Page.
http://wordlist.sourceforge.net/, last accessed: 4 April
2010.
Kevin Atkinson, Spellchecker Test Kernel Results.
2002. http://aspell.net/test/orig/, last accessed: 28
February 2010.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics and the
10th Conference of the European Chapter of the As-
sociation for Computational Linguistics, Toulouse.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part of speech tagging. Computational Lin-
guistics, 21:543–565.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, Hong Kong,
pp. 286–293.
Fred J. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tions of the ACM, 7(3): 659-664.
Sebastian Deorowicz and Marcin G. Ciura. 2005. Cor-
recting spelling errors by modelling their causes. In-
ternational Journal of Applied Mathematics and
Computer Science, 15(2):275Ð285.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
der Klementiev, William Dolan, Dmitriy Belenko,
and Lucy Vanderwende. 2008. Using Contextual
Speller Techniques and Language Modeling for ESL
Error Correction. Proceedings of IJCNLP, Hydera-
bad, India, Asia Federation of Natural Language
Processing.
Michael Hart. 2008. Project Gutenberg.
http://www.gutenberg.org/, last accessed: 28 Febru-
ary 2010.
Abby Levenberg. 2007. Bloom filter and lossy diction-
ary based language models. Master of Science Dis-
sertation, School of Informatics, University of
Edinburgh.
Mitchell Marcus, Beatrice Santorini, and Maryann Mar-
cinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Lin-
guistics, 19(2).
Daniel Naber. 2003. A Rule-Based Style and Grammar
Checker. Diplomarbeit Technis FakultŠt, UniversitŠt
Bielefeld, Germany.
Jennifer Pedler. 2007. Computer Correction of Real-
word Spelling Errors in Dyslexic Text. PhD thesis,
Birkbeck, London University.
Segaran, T. 2007 Programming Collective Intelligence.
First. O&apos;Reilly. pp. 74-85
StatCounter, 2010. Top 5 Browsers from Feb 09 to Mar
10. http://gs.statcounter.com/, last accessed: 28 Feb-
ruary 2010.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large
Corpora (EMNLP/VLC-2000), pp. 63-70.
Mark Turner, David Budgen, and Pearl Brereton. 2003.
Turning software into a service. Computer,
36(10):38Ð44.
Robert A. Wagner and Michael J. Fischer. 1974. The
string-to-string correction problem. Journal of ACM,
21(1):168Ð173.
Wikipedia, 2009. List of Common Misspellings.
http://en.wikipedia.org/wiki/Wikipedia:Lists_of_com
mon_misspellings, last accessed: 28 February 2010.
Wikimedia Inc. 2010. Wikimedia Downloads.
http://download.wikipedia.org/, last accessed: 28
February 2010.
Shloma Yona, 2002. Lingua::EN::Sentence Module,
CPAN. http://search.cpan.org/~shlomoy/Lingua-EN-
Sentence-0.25/lib/Lingua/EN/Sentence.pm, last ac-
cessed: 28 February 2010.
</reference>
<page confidence="0.999297">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803240">
<title confidence="0.99974">The Design of a Proofreading Software Service</title>
<author confidence="0.999257">Raphael Mudge</author>
<affiliation confidence="0.819868">Automattic</affiliation>
<address confidence="0.996565">Washington, DC 20036</address>
<email confidence="0.999791">raffi@automattic.com</email>
<abstract confidence="0.99891105882353">Web applications have the opportunity to check spelling, style, and grammar using a software service architecture. A software service authoring aid can offer contextual spell checking, detect real word errors, and avoid poor grammar checker suggestions through the use of large language models. Here we present After the Deadline, an open source authoring aid, used in production on Word- Press.com, a blogging platform with over ten million writers. We discuss the benefits of the software service environment and how it affected our choice of algorithms. We summarize our design principles as speed over accuracy, simplicity over complexity, and do what works.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kevin Atkinson</author>
</authors>
<title>Kevin’s Wordlist Page. http://wordlist.sourceforge.net/, last accessed: 4</title>
<date>2008</date>
<contexts>
<context position="8908" citStr="Atkinson, 2008" startWordPosition="1465" endWordPosition="1466">. We calculate this result with Pn(wordn+1, wordn+2|wordn) * P(wordn) / P(wordn+1, wordn+2). 3 Spell Checking Spell checkers scan a document word by word and follow a three-step process. The first step is to check if the word is in the spell checker’s dictionary. If it is, then the word is spelled correctly. The second step is to generate a set of possible sugges3.1 The Spell Checker Dictionary The dictionary size is a matter of balance. Too many words and misspelled words will go unnoticed. Too few words and the user will see more false positive suggestions. We used public domain word-lists (Atkinson, 2008) to create a master word list to generate our spell checker dictionary. We added to this list by analyzing popular blogs for frequently occurring words that were missing from our dictionary. This analysis lets us include new words in our master word list of 760,211 words. Our spell checker dictionary is the intersection of this master word list and words found in our corpus. We do this to prevent some misspelled words from making it into our spell checker dictionary. We only allow words that pass a minimal count threshold into our dictionary. We adjust this threshold to keep our dictionary siz</context>
</contexts>
<marker>Atkinson, 2008</marker>
<rawString>Kevin Atkinson. 2008, Kevin’s Wordlist Page. http://wordlist.sourceforge.net/, last accessed: 4 April 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Atkinson</author>
</authors>
<title>Spellchecker Test Kernel Results.</title>
<date>2002</date>
<pages>28</pages>
<note>http://aspell.net/test/orig/, last accessed:</note>
<contexts>
<context position="14543" citStr="Atkinson, 2002" startWordPosition="2422" endWordPosition="2423">onsider two suggestions word and ward. Both are an edit distance of one from wrd. Both words also have a first letter match. Pp(ward|written) is 0.00% while Pp(word|written) is 0.17%. Context makes the difference in this example. pose substitute t, e posit insert i posts insert s pot delete e pots transpose s, t pout substitute s, u 27 3.4 Evaluation To evaluate our spelling corrector we created two testing data sets. We used the typo and word pairs from the WPCM list merged with random sentences from our Project Gutenberg corpus. We also used the typo and word pairs from the ASpell data set (Atkinson, 2002) merged with sentences from the Project Gutenberg corpus. We measure our accuracy with the method described in Deorowicz and Ciura (2005). For comparison we present their numbers for ASpell and several versions of Microsoft Word along with ours in Tables 2 and 3. We also show the number of misspelled words present in each system’s spell checker dictionary. Present Words Accuracy ASpell (normal) 14 56.9% MS Word 97 18 59.0% MS Word 2000 20 62.6% MS Word 2003 20 62.8% After the Deadline 53 66.1% Table 2. Corrector Accuracy: ASpell Data. Present Words Accuracy ASpell (normal) 44 84.7% MS Word 97 </context>
</contexts>
<marker>Atkinson, 2002</marker>
<rawString>Kevin Atkinson, Spellchecker Test Kernel Results. 2002. http://aspell.net/test/orig/, last accessed: 28 February 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="26852" citStr="Banko and Brill, 2001" startWordPosition="4456" endWordPosition="4459">e service proofreader is summarized with the following principles: • Speed over accuracy • Simplicity over complexity • Do what works In natural language processing there are many opportunities to choose speed over accuracy. For example, when tagging a sentence one can use a Hidden Markov Model tagger or a simple trigram tagger. In these instances we made the choice to trade accuracy for speed. When implementing the smarts of our system, we’ve opted to use simpler algorithms and focus on acquiring more data and increasing the quality of data our system learns from. As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. Our real-word error detector is an example of simplicity over complexity. With our simple trigram language model, we were able to correct nearly a quarter of the errors in the dyslexic writer corpus. We could improve the performance of our real-word error corrector simply by adding more confusion sets. We define “do what works” as favoring mixed strategies for finding and correcting errors. We use both statistical and rule-based methods to detect real word errors and correct grammar</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 10th Conference of the European Chapter of the Association for Computational Linguistics, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--543</pages>
<contexts>
<context position="21892" citStr="Brill, 1995" startWordPosition="3628" endWordPosition="3629">hecker separates phrases and tags with a forward slash character. This is a common convention. The part-of-speech tagger uses a mixed statistical and rule-based approach. If a word is known and has tags associated with it, the tagger tries to 29 find the tag that maximizes the following probability: P(tagn|wordn) * P(tagn|tagn-1, tagn-2) For words that are not known, an alternate model containing tag probabilities based on word endings is consulted. This alternate model uses the last three letters of the word. Again the goal is to maximize this probability. We apply rules from Brill’s tagger (Brill, 1995) to fix some cases of known incorrect tagging. Table 5 compares our tagger accuracy for known and unknown words to a probabilistic tagger that maximizes P(tagn|wordn) only. Tagger Known Unknown Probability Tagger 91.9% 72.9% Trigram Tagger 94.0% 76.7% Table 5. POS Tagger Accuracy. To train the tagger we created training and testing data sets by running the Stanford POS tagger (Toutanova and Manning, 2000) against the Wikipedia and Project Gutenberg corpus data. 5.3 Rule Engine It helps to think of a grammar checker as a language for describing phrases. Phrases that match a grammar rule return </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging. Computational Linguistics, 21:543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="11876" citStr="Brill and Moore, 2000" startWordPosition="1979" endWordPosition="1982"> in the WPCM list are two edits from the intended word. When no suggestions are available within two edits, we consider suggestions three edits from the typo. 99% of the typos from the WPCM list are within three edits. By doing this we avoid affecting the accuracy of the sorting step in a negative way and make it possible for the system to suggest the correct word for severe typos. 3.3 Sorting Suggestions The sorting step relies on a score function that accepts a typo and suggestion as parameters. The perfect score function calculates the probability of a suggestion given the misspelled word (Brill and Moore, 2000). We approximate our scoring function using a neural network. Our neural network is a multilayer perceptron network, implemented as described in Chapter 4 of Programming Collective Intelligence (Segaran, 2007). We created a training data set for our spelling corrector by combining misspelled words from the WPCM list with random sentences from Wikipedia. Our neural network sees each typo (wordn) and suggestion pair as several features with values ranging from 0.0 to 1.0. During training, the neural network is presented with examples of suggestions and typos with the expected score. From these e</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong, pp. 286–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>659--664</pages>
<contexts>
<context position="10274" citStr="Damerau, 1964" startWordPosition="1694" endWordPosition="1696">88.6% Table 1. Dictionary Inclusion Threshold. Table 1 shows the effect of this threshold on the dictionary size, the number of present words from Wikipedia’s List of Common Misspellings (Wikipedia, 2009), and the accuracy of a noncontextual version of our spell checker. We will refer to the Wikipedia Common Misspellings list as WPCM through the rest of this paper. 3.2 Generating Suggestions To generate suggestions our system first considers all words within an edit distance of two. An edit is defined as inserting a letter, deleting a letter, substituting a letter, or transposing two letters (Damerau, 1964). 26 Consider the word post. Here are several words that are within one edit: cost substitute p, c host substitute p, h most substitute p, m past substitute o, a pest substitute o, e poet substitute s, e The naïve approach to finding words within one edit involves making all possible edits to the misspelled word using our edit operations. You may remove any words that are not in the dictionary to arrive at the final result. Apply the same algorithm to all word and non-word results within one edit of the misspelled word to find all words within two edits. We store our dictionary as a Trie and g</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Fred J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3): 659-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Deorowicz</author>
<author>Marcin G Ciura</author>
</authors>
<title>Correcting spelling errors by modelling their causes.</title>
<date>2005</date>
<journal>International Journal of Applied Mathematics and Computer Science,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="14680" citStr="Deorowicz and Ciura (2005)" startWordPosition="2442" endWordPosition="2445">p(ward|written) is 0.00% while Pp(word|written) is 0.17%. Context makes the difference in this example. pose substitute t, e posit insert i posts insert s pot delete e pots transpose s, t pout substitute s, u 27 3.4 Evaluation To evaluate our spelling corrector we created two testing data sets. We used the typo and word pairs from the WPCM list merged with random sentences from our Project Gutenberg corpus. We also used the typo and word pairs from the ASpell data set (Atkinson, 2002) merged with sentences from the Project Gutenberg corpus. We measure our accuracy with the method described in Deorowicz and Ciura (2005). For comparison we present their numbers for ASpell and several versions of Microsoft Word along with ours in Tables 2 and 3. We also show the number of misspelled words present in each system’s spell checker dictionary. Present Words Accuracy ASpell (normal) 14 56.9% MS Word 97 18 59.0% MS Word 2000 20 62.6% MS Word 2003 20 62.8% After the Deadline 53 66.1% Table 2. Corrector Accuracy: ASpell Data. Present Words Accuracy ASpell (normal) 44 84.7% MS Word 97 31 89.0% MS Word 2000 42 92.5% MS Word 2003 41 92.6% After the Deadline 143 92.7% Table 3. Corrector Accuracy: WPCM Data. The accuracy nu</context>
</contexts>
<marker>Deorowicz, Ciura, 2005</marker>
<rawString>Sebastian Deorowicz and Marcin G. Ciura. 2005. Correcting spelling errors by modelling their causes. International Journal of Applied Mathematics and Computer Science, 15(2):275Ð285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexander Klementiev</author>
<author>William Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<journal>Federation of Natural Language Processing.</journal>
<booktitle>Proceedings of IJCNLP,</booktitle>
<location>Hyderabad, India, Asia</location>
<contexts>
<context position="20264" citStr="Gamon, et al 2008" startWordPosition="3352" endWordPosition="3355">tterns, and sentence begin and end markers. Our grammar checker does not do a deep parse of the sentence. This prevents us from writing rules that reference the sentence subject, verb, and object directly. In practice this means we’re unable to rewrite passive voice for users and create general rules to catch many subject-verb agreement errors. Functionally, our grammar and style checker is similar to Language Tool (Naber, 2003) with the exception that it uses the language model to filter suggestions that don’t fit the context of the text they replace, similar to work from Microsoft Research (Gamon, et al 2008). 5.1 Text Segmentation Our text segmentation function uses a rule-based approach similar to Yona (2002) to split raw text into paragraphs, sentences, and words. The segmentation is good enough for most purposes. Because our sentence segmentation is wrong at times, we do not notify a user when they fail to capitalize the first word in a sentence. 5.2 Part-of-Speech Tagger A tagger labels each word with its relevant part-ofspeech. These labels are called tags. A tag is a hint about the grammatical category of the word. Such tagging allows grammar and style rules to reference all nouns or all ve</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. Proceedings of IJCNLP, Hyderabad, India, Asia Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hart</author>
</authors>
<date>2008</date>
<pages>28</pages>
<note>Project Gutenberg. http://www.gutenberg.org/, last accessed:</note>
<contexts>
<context position="5940" citStr="Hart, 2008" startWordPosition="966" endWordPosition="967">t searches for this marker word followed by the error text to find the error and present the correct suggestions. This scheme is not perfect, but it simplifies our client and server implementations. 2 Language Model Our system derives its smarts from observed language use. We construct our language model by counting the number of times we see each sequence of two words in a corpus of text. These sequences are known as bigrams. Our language model is case sensitive. We trained our bigram language model using text from the Simple English edition of Wikipedia (Wikimedia, 2010), Project Gutenberg (Hart, 2008), and several blogs. We bootstrapped this process by using Wikipedia and Project Gutenberg data. We then evaluated the contents of several blogs looking for low occurrences of commonly misspelled words and real word errors. Blogs that had a low occurrence of errors were then added to our corpus. Our corpus has about 75 million words. We also store counts for sequences of three words that end or begin with a potentially confused word. A potentially confused word is a word associated with a confusion set (see section 4.1). The real word error detector feature relies on these confusion sets. Thes</context>
</contexts>
<marker>Hart, 2008</marker>
<rawString>Michael Hart. 2008. Project Gutenberg. http://www.gutenberg.org/, last accessed: 28 February 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
</authors>
<title>Bloom filter and lossy dictionary based language models.</title>
<date>2007</date>
<institution>Master of Science Dissertation, School of Informatics, University of Edinburgh.</institution>
<marker>Levenberg, 2007</marker>
<rawString>Abby Levenberg. 2007. Bloom filter and lossy dictionary based language models. Master of Science Dissertation, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Maryann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="20972" citStr="Marcus et al, 1993" startWordPosition="3472" endWordPosition="3475"> to Yona (2002) to split raw text into paragraphs, sentences, and words. The segmentation is good enough for most purposes. Because our sentence segmentation is wrong at times, we do not notify a user when they fail to capitalize the first word in a sentence. 5.2 Part-of-Speech Tagger A tagger labels each word with its relevant part-ofspeech. These labels are called tags. A tag is a hint about the grammatical category of the word. Such tagging allows grammar and style rules to reference all nouns or all verbs rather than having to account for individual words. Our system uses the Penn Tagset (Marcus et al, 1993). The/DT little/JJ dog/NN laughed/VBD Here we have tagged the sentence The little dog laughed. The is labeled as a determiner, little is an adjective, dog is a noun, and laughed is a past tense verb. We can reference little, large, and mean laughing dogs with the pattern The .*/JJ dog laughed. Our grammar checker separates phrases and tags with a forward slash character. This is a common convention. The part-of-speech tagger uses a mixed statistical and rule-based approach. If a word is known and has tags associated with it, the tagger tries to 29 find the tag that maximizes the following prob</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Maryann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Naber</author>
</authors>
<title>A Rule-Based Style and Grammar Checker. Diplomarbeit Technis FakultŠt,</title>
<date>2003</date>
<location>UniversitŠt Bielefeld, Germany.</location>
<contexts>
<context position="20078" citStr="Naber, 2003" startWordPosition="3321" endWordPosition="3322"> system then applies several grammar and style rules to this marked up text looking for matches. Grammar rules consist of regular expressions that match on parts-of-speech, word patterns, and sentence begin and end markers. Our grammar checker does not do a deep parse of the sentence. This prevents us from writing rules that reference the sentence subject, verb, and object directly. In practice this means we’re unable to rewrite passive voice for users and create general rules to catch many subject-verb agreement errors. Functionally, our grammar and style checker is similar to Language Tool (Naber, 2003) with the exception that it uses the language model to filter suggestions that don’t fit the context of the text they replace, similar to work from Microsoft Research (Gamon, et al 2008). 5.1 Text Segmentation Our text segmentation function uses a rule-based approach similar to Yona (2002) to split raw text into paragraphs, sentences, and words. The segmentation is good enough for most purposes. Because our sentence segmentation is wrong at times, we do not notify a user when they fail to capitalize the first word in a sentence. 5.2 Part-of-Speech Tagger A tagger labels each word with its rele</context>
</contexts>
<marker>Naber, 2003</marker>
<rawString>Daniel Naber. 2003. A Rule-Based Style and Grammar Checker. Diplomarbeit Technis FakultŠt, UniversitŠt Bielefeld, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Pedler</author>
</authors>
<title>Computer Correction of Realword Spelling Errors in Dyslexic Text.</title>
<date>2007</date>
<tech>PhD thesis,</tech>
<institution>Birkbeck, London University.</institution>
<contexts>
<context position="15854" citStr="Pedler (2007)" startWordPosition="2648" endWordPosition="2649">or Accuracy: WPCM Data. The accuracy number measures both the suggestion generation and sorting steps. As with the referenced experiment, we excluded misspelled entries that existed in the spell checker dictionary. Note that the present words number from Table 1 differs from Table 3 as these experiments were carried out at different times in the development of our technology. 4 Real Word Errors Spell checkers are unable to detect an error when a typo results in a word contained in the dictionary. These are called real word errors. A good overview of real word error detection and correction is Pedler (2007). 4.1 Confusion Sets Our real word error detector checks 1,603 words, grouped into 741 confusion sets. A confusion set is two or more words that are often confused for each other (e.g., right and write). Our confusion sets were built by hand using a list of English homophones as a starting point. 4.2 Real Word Error Correction The real word error detector scans the document finding words associated with a confusion set. For each of these words the real word error detector uses a score function to sort the confusion set. The score function approximates the likelihood of a word given the context</context>
</contexts>
<marker>Pedler, 2007</marker>
<rawString>Jennifer Pedler. 2007. Computer Correction of Realword Spelling Errors in Dyslexic Text. PhD thesis, Birkbeck, London University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Segaran</author>
</authors>
<date>2007</date>
<journal>Programming Collective Intelligence. First. O&apos;Reilly.</journal>
<pages>74--85</pages>
<contexts>
<context position="12085" citStr="Segaran, 2007" startWordPosition="2012" endWordPosition="2013">. By doing this we avoid affecting the accuracy of the sorting step in a negative way and make it possible for the system to suggest the correct word for severe typos. 3.3 Sorting Suggestions The sorting step relies on a score function that accepts a typo and suggestion as parameters. The perfect score function calculates the probability of a suggestion given the misspelled word (Brill and Moore, 2000). We approximate our scoring function using a neural network. Our neural network is a multilayer perceptron network, implemented as described in Chapter 4 of Programming Collective Intelligence (Segaran, 2007). We created a training data set for our spelling corrector by combining misspelled words from the WPCM list with random sentences from Wikipedia. Our neural network sees each typo (wordn) and suggestion pair as several features with values ranging from 0.0 to 1.0. During training, the neural network is presented with examples of suggestions and typos with the expected score. From these examples the neural network converges on an approximation of our score function. We use the following features to train a neural network to calculate our suggestion scoring function: editDistance(suggestion, wo</context>
</contexts>
<marker>Segaran, 2007</marker>
<rawString>Segaran, T. 2007 Programming Collective Intelligence. First. O&apos;Reilly. pp. 74-85</rawString>
</citation>
<citation valid="true">
<authors>
<author>StatCounter</author>
</authors>
<date>2010</date>
<booktitle>Top 5 Browsers from Feb</booktitle>
<volume>09</volume>
<pages>28</pages>
<note>to Mar 10. http://gs.statcounter.com/, last accessed:</note>
<contexts>
<context position="2708" citStr="StatCounter, 2010" startWordPosition="420" endWordPosition="422">o our server, posts the text, and receives the errors and suggestions as XML. Figure 1 shows this process. It is the client’s responsibility to display the errors and present the suggestions to the user. Figure 1. After the Deadline Client/Server Interaction. Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 24–32, Los Angeles, California, June 2010. @2010 Association for Computational Linguistics 1.2 Applications One could argue that web browsers should provide spell and grammar check features for their users. Internet Explorer, the most used browser (StatCounter, 2010), offers no checking. Firefox offers spell checking only. Apple’s Safari web browser has non-contextual spell and grammar checking. Application developers should not wait for the browsers to catch up. Using a software service architecture, applications can provide the same quality checking to their users regardless of the client they connect with. This is especially relevant as more users begin to use web applications from mobile and tablet devices. 1.3 Benefits A software service application has the advantage that it can use the complete CPU and memory resources of the server. Clients hoping </context>
</contexts>
<marker>StatCounter, 2010</marker>
<rawString>StatCounter, 2010. Top 5 Browsers from Feb 09 to Mar 10. http://gs.statcounter.com/, last accessed: 28 February 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="22300" citStr="Toutanova and Manning, 2000" startWordPosition="3692" endWordPosition="3695">taining tag probabilities based on word endings is consulted. This alternate model uses the last three letters of the word. Again the goal is to maximize this probability. We apply rules from Brill’s tagger (Brill, 1995) to fix some cases of known incorrect tagging. Table 5 compares our tagger accuracy for known and unknown words to a probabilistic tagger that maximizes P(tagn|wordn) only. Tagger Known Unknown Probability Tagger 91.9% 72.9% Trigram Tagger 94.0% 76.7% Table 5. POS Tagger Accuracy. To train the tagger we created training and testing data sets by running the Stanford POS tagger (Toutanova and Manning, 2000) against the Wikipedia and Project Gutenberg corpus data. 5.3 Rule Engine It helps to think of a grammar checker as a language for describing phrases. Phrases that match a grammar rule return suggestions that are transforms of the matched phrase. Some rules are simple string substitutions (e.g., utilized ĺ used). Others are more complex. Consider the following phrase: I wonder if this is your companies way of providing support? This phrase contains an error. The word companies should be possessive not plural. To create a rule to find this error, we first look at how our system sees it: I/PRP w</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Turner</author>
<author>David Budgen</author>
<author>Pearl Brereton</author>
</authors>
<title>Turning software into a service.</title>
<date>2003</date>
<journal>Computer,</journal>
<volume>36</volume>
<issue>10</issue>
<contexts>
<context position="1881" citStr="Turner et al., 2003" startWordPosition="289" endWordPosition="292"> grammar on WordPress.com1, one of the most popular blogging platforms. Our system uses a 1 An After the Deadline add-on for the Firefox web browser is available. We also provide client libraries for embedding into other applications. See http://www.afterthedeadline.com. 24 software service architecture. In this paper we discuss how this system works, the trade-offs of the software service environment, and the benefits. We conclude with a discussion of our design principles: speed over accuracy, simplicity over complexity, and do what works. 1.1 What is a Software Service? A software service (Turner et al., 2003) is an application that runs on a server. Client applications post the expected inputs to the server and receive the output as XML. Our software service checks spelling, style, and grammar. A client connects to our server, posts the text, and receives the errors and suggestions as XML. Figure 1 shows this process. It is the client’s responsibility to display the errors and present the suggestions to the user. Figure 1. After the Deadline Client/Server Interaction. Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 24–32, Los Angeles, California, June 201</context>
</contexts>
<marker>Turner, Budgen, Brereton, 2003</marker>
<rawString>Mark Turner, David Budgen, and Pearl Brereton. 2003. Turning software into a service. Computer, 36(10):38Ð44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The string-to-string correction problem.</title>
<date>1974</date>
<journal>Journal of ACM,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="12883" citStr="Wagner and Fischer, 1974" startWordPosition="2126" endWordPosition="2129">ch typo (wordn) and suggestion pair as several features with values ranging from 0.0 to 1.0. During training, the neural network is presented with examples of suggestions and typos with the expected score. From these examples the neural network converges on an approximation of our score function. We use the following features to train a neural network to calculate our suggestion scoring function: editDistance(suggestion, wordn) firstLetterMatch(suggestion, wordn) Pn(suggestion|wordn-1) Pp(suggestion|wordn+1) P(suggestion) We calculate the edit distance using the Damerau–Levenshtein algorithm (Wagner and Fischer, 1974). This algorithm recognizes insertions, substitutions, deletions, and transpositions as a single edit. We normalize this value for the neural network by assigning 1.0 to an edit distance of 1 and 0.0 to any other edit distance. We do this to prevent the occasional introduction of a correct word with an edit distance of three from skewing the neural network. The firstLetterMatch function returns 1.0 when the first letters of the suggestion and the typo match. This is based on the observation that most writers get the first letter correct when attempting to a spell a word. In the WPCM list, this</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1974. The string-to-string correction problem. Journal of ACM, 21(1):168Ð173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>List of Common Misspellings. http://en.wikipedia.org/wiki/Wikipedia:Lists_of_com mon_misspellings, last accessed: 28</title>
<date>2009</date>
<contexts>
<context position="9864" citStr="Wikipedia, 2009" startWordPosition="1626" endWordPosition="1627">d list and words found in our corpus. We do this to prevent some misspelled words from making it into our spell checker dictionary. We only allow words that pass a minimal count threshold into our dictionary. We adjust this threshold to keep our dictionary size around 125,000 words. Threshold Words Present Words Accuracy 1 161,879 233 87.9% 2 116,876 149 87.8% 3 95,910 104 88.0% 4 82,782 72 88.3% 5 73,628 59 88.6% Table 1. Dictionary Inclusion Threshold. Table 1 shows the effect of this threshold on the dictionary size, the number of present words from Wikipedia’s List of Common Misspellings (Wikipedia, 2009), and the accuracy of a noncontextual version of our spell checker. We will refer to the Wikipedia Common Misspellings list as WPCM through the rest of this paper. 3.2 Generating Suggestions To generate suggestions our system first considers all words within an edit distance of two. An edit is defined as inserting a letter, deleting a letter, substituting a letter, or transposing two letters (Damerau, 1964). 26 Consider the word post. Here are several words that are within one edit: cost substitute p, c host substitute p, h most substitute p, m past substitute o, a pest substitute o, e poet su</context>
</contexts>
<marker>Wikipedia, 2009</marker>
<rawString>Wikipedia, 2009. List of Common Misspellings. http://en.wikipedia.org/wiki/Wikipedia:Lists_of_com mon_misspellings, last accessed: 28 February 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikimedia Inc</author>
</authors>
<title>Wikimedia Downloads. http://download.wikipedia.org/, last accessed: 28</title>
<date>2010</date>
<marker>Inc, 2010</marker>
<rawString>Wikimedia Inc. 2010. Wikimedia Downloads. http://download.wikipedia.org/, last accessed: 28 February 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shloma Yona</author>
</authors>
<date>2002</date>
<booktitle>Lingua::EN::Sentence Module, CPAN. http://search.cpan.org/~shlomoy/Lingua-ENSentence-0.25/lib/Lingua/EN/Sentence.pm, last accessed: 28</booktitle>
<contexts>
<context position="20368" citStr="Yona (2002)" startWordPosition="3369" endWordPosition="3370">prevents us from writing rules that reference the sentence subject, verb, and object directly. In practice this means we’re unable to rewrite passive voice for users and create general rules to catch many subject-verb agreement errors. Functionally, our grammar and style checker is similar to Language Tool (Naber, 2003) with the exception that it uses the language model to filter suggestions that don’t fit the context of the text they replace, similar to work from Microsoft Research (Gamon, et al 2008). 5.1 Text Segmentation Our text segmentation function uses a rule-based approach similar to Yona (2002) to split raw text into paragraphs, sentences, and words. The segmentation is good enough for most purposes. Because our sentence segmentation is wrong at times, we do not notify a user when they fail to capitalize the first word in a sentence. 5.2 Part-of-Speech Tagger A tagger labels each word with its relevant part-ofspeech. These labels are called tags. A tag is a hint about the grammatical category of the word. Such tagging allows grammar and style rules to reference all nouns or all verbs rather than having to account for individual words. Our system uses the Penn Tagset (Marcus et al, 1</context>
</contexts>
<marker>Yona, 2002</marker>
<rawString>Shloma Yona, 2002. Lingua::EN::Sentence Module, CPAN. http://search.cpan.org/~shlomoy/Lingua-ENSentence-0.25/lib/Lingua/EN/Sentence.pm, last accessed: 28 February 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>