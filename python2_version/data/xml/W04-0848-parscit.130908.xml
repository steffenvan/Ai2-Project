<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001747">
<note confidence="0.8226545">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<subsectionHeader confidence="0.786845">
Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.987315391304348">
SemCor next word - the same as the pre-
vious heuristic but this heuristic forms a pair
from the target word in a gloss and the next
word and searches for this pair in the SemCor
corpus. If the target word has the same sense
in all occurrences of the pair, then the heuristic
assigns this sense to the target word.
Cross reference - given an ambiguous word
W in the synset S, this method looks for a ref-
erence to the synset S in all the glosses corre-
sponding to word W&apos;s senses. By reference to a
word W we denote a word or a part of a com-
pound concept that has the same lemma as the
word W.
Reversed cross reference - given a word
W in the gloss G of the synset S, this method
assigns to the word W the sense that contains
in its set of synonyms one of the words from the
gloss G.
Distance among glosses - determines the
number of common word lemmas between two
glosses. For an ambiguous word W in a gloss G
this method selects the sense with the greatest
number of common word lemmas with the gloss
G.
Common Domain - using the domain labels
assigned by (Magnini and Cavaglia, 2000) this
method selects the sense of a word in a gloss
that has the same domain as the synset of the
gloss.
Patterns - exploits the idiosyncratic nature
of the WordNet glosses. The patterns of the
form &amp;quot;N successive words&amp;quot; and &amp;quot;M words ...
N words&amp;quot; are extracted offline from the Word-
Net glosses and manually disambiguated. This
method matches the patterns against glosses
and assigns to the words the corresponding
sense in the pattern.
First Sense Restricted - this method as-
signs sense 1 to a noun or verb if this sense
has the smallest number of ancestors in the ISA
hierarchy from all senses (it is the most gen-
eral sense). The method selects sense 1 for an
adjective if this sense has the greatest number
of similarity pointers compared to all the other
senses.
</bodyText>
<subsectionHeader confidence="0.9399335">
2.2 Disambiguation methods for
English All Words task
</subsectionHeader>
<bodyText confidence="0.9992506">
The disambiguation methods in this task can be
classified in three types described below: meth-
ods based on heuristics, machine learning dis-
ambiguation methods, and incremental disam-
biguation methods.
</bodyText>
<sectionHeader confidence="0.7497045" genericHeader="method">
2.2.1 WSD Methods based on
Heuristics
</sectionHeader>
<bodyText confidence="0.99957925">
In this category the methods can be further clas-
sified in methods that use hand coded rules,
methods that use WordNet and methods that
use SemCor (Miller et al., 1994). Some methods
applied to Semantic Disambiguation of Word-
Net glosses were also applied on open text: Sem-
cor Previous Word, Semcor Next Word, Pat-
terns and Lexical Parallelism.
</bodyText>
<subsectionHeader confidence="0.557677">
2.2.2 Machine Learning
Disambiguation met hods
</subsectionHeader>
<bodyText confidence="0.99958625">
We used Support Vector Machines (Cortes and
Vapnik, 1995) for disambiguating verbs, adjec-
tives, and adverbs, and C5.0 (Quinlan, 2003)
rules for disambiguating nouns.
</bodyText>
<subsectionHeader confidence="0.5869925">
Support Vector Machines (SVM)
Method
</subsectionHeader>
<bodyText confidence="0.999972055555556">
We used the following set of features (Mi-
halcea, 2002): current word form and part of
speech, contextual features, collocations in a
window of (-3,3) words, and keywords and bi-
grams in a window of (-3,3) sentences. For dis-
ambiguating verbs we used an additional set of
features: Verb mode (which can take 4 values:
ACTIVE, INFINITIVE, PAST, GERUND),
verb voice (which can take 2 values ACTIVE,
PASSIVE), the parent of the current verb in
the parse tree (ex: VP, NP), The first ancestor
that is not VP in the parse tree (like S, NP, PP,
SBAR).
We generated feature values using the Sense-
val 2 Lexical Sample training corpus and Sem-
Cor corpus combined, and we trained the SVM
classifier on those words that had at least 10
training examples.
</bodyText>
<subsectionHeader confidence="0.679369">
C5.0 Rules Method
</subsectionHeader>
<bodyText confidence="0.9998895">
The second machine learning method, used
for word sense disambiguating nouns, is the
C5.0 decision tree learning (Quinlan, 2003).
For generating training examples we used the
features described in (Mihalcea and Moldovan,
2001b). We used the SemCor 1.7.1 collection
and Senseval 2 All Words collection as training
corpus.
</bodyText>
<subsubsectionHeader confidence="0.731449">
2.2.3 Incremental Disambiguation
</subsubsectionHeader>
<bodyText confidence="0.999487833333333">
The main idea of incremental disambiguation is
to disambiguate new words using senses of the
previous disambiguated words. We used some
of the procedures presented in (Mihalcea and
Moldovan, 2001b) briefly described below:
WordNet distance 0 with disam-
biguated words - find words that are in the
same synset with already disambiguated words.
WordNet distance 1 with disam-
biguated words - find words that are in a
hypernymy/hyponymy relation with the words
already disambiguated.
</bodyText>
<sectionHeader confidence="0.998345" genericHeader="method">
3 Combining Methods using Rules
</sectionHeader>
<bodyText confidence="0.99997748">
At this point each WSD system has a pool of
disambiguation methods. An approach using
rules for selecting the right sense was described
in (Novischi, 2004) and is summarized below.
For a given disambiguated word we create a
training example for each of its senses. Given a
manually-disambiguated corpus, training exam-
ples are generated for each sense of the tagged
words with their correct classification. Then
we can train a machine learning algorithm on
this set of training examples.If we use C4.5 or
C5.0, where each rule has an associated accu-
racy value, we can output the single sense that
is classified as CORRECT by the rule with the
best accuracy. For Semantic disambiguation of
WordNet glosses task, we used a set of rules
given by C4.5 program trained on a set of train-
ing examples generated from 3196 goldstandard
glosses. For the English All Words task, how-
ever, we created the rules manually, each rule
corresponding to a disambiguation method and
assigned an accuracy value equal to the preci-
sion of the disambiguation method. The rules
were tested in the decreasing order of their ac-
curacy.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999987333333333">
We measured the accuraccy of the LCC WSD
systems using precision, recall and coverage.
Precision is defined as the number of correct
disambiguated words over the number of at-
tempted words. Recall is defined as the number
of correct disambiguated words over the total
number of words for disambiguation. Coverage
is defined as the number of attempted words
over the total number of words.
</bodyText>
<subsectionHeader confidence="0.918969">
4.1 English All Words
</subsectionHeader>
<bodyText confidence="0.9999784">
In order to illustrate the performance of the sys-
tem, we examine it from three different perspec-
tives. First we ran the system without default-
ing to first sense and computed the results ex-
cluding monosemous words (row a in table 1).
Second we ran the system including defaulting
to sense 1, but we still excluded monosemous
words when we computed the results (row b
in table 1). Third we included both the first
sense for words that were not otherwise disam-
biguated by the system and monosemous words
(row c in table 1). Table 1 presents the results
given by the Python scorer script. Precision
and recall in rows a and b do not include the
monosemous words.
</bodyText>
<subsectionHeader confidence="0.9923605">
4.2 Semantic Disambiguation of
WordNet glosses
</subsectionHeader>
<bodyText confidence="0.999841666666667">
In order to illustrate the performance of the sys-
tem, we examined it from two different perspec-
tives. First we ran the system including assign-
ing sense 1 for words that were not otherwise
disambiguated by the system (row a. in table
2). Second we ran the system without assign-
ing sense 1 to words that were not otherwise
disambiguated by the system (row b. in table
2).
</bodyText>
<sectionHeader confidence="0.986129" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999870785714286">
The LCC WSD systems for Senseval 3 employ
sets of disambiguation methods. Some methods
are based on hand coded rules, some use infor-
mation in WordNet, and some exploit the statis-
tics of the SemCor corpus. Others use the senses
of previous disambiguated words and still others
are based on supervised machine learning algo-
rithms. All these methods are combined using
rules that optimize their output. The way the
WSD systems are designed using multiple meth-
ods for disambiguation ensures a greater cover-
age than a single method by itself. Combining
methods using machine learning algorithms im-
proves the precision of the system.
</bodyText>
<sectionHeader confidence="0.901457" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.841367333333333">
Eric Brill. 1995. Unsupervised learning of dis-
ambiguation rules for part of speech tagging.
In David Yarovsky and Kenneth Church, ed-
itors, Proceedings of the Third Workshop on
Very Large Corpora, pages 1-13, Somerset,
New Jersey. Association for Computational
Linguistics.
Michael Collins. 1997. Three generative lex-
icalized models for statistical parsing. In
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics (ACL 1997).
</bodyText>
<reference confidence="0.972244833333333">
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273-297.
S. Harabagiu, G. Miller, and D. Moldovan.
1999. WordNet 2 - a morphologically and
semantically enhanced resource. In Proceed-
</reference>
<figure confidence="0.59974225">
Experiment Precision Recall Coverage
a LCC WSD alone 53.6% 41.4% 77.3%
b LCC WSD ± First Sense 53.9% 54.9% 101.9%
c LCC WSD ± First Sense ± Monosemous 60.7% 60.7% 100.0%
</figure>
<tableCaption confidence="0.989098">
Table 1: Performance of the LCC WSD system in English All Words Task
</tableCaption>
<table confidence="0.554245333333333">
Experiment Precision Recall Coverage
a LCC WSD 65.1% 62.2% 95.5%
b LCC WSD without First Sense 70.1% 50.4% 71.9%
</table>
<tableCaption confidence="0.990046">
Table 2: Performance of the LCC WSD system in WordNet glosses task
</tableCaption>
<bodyText confidence="0.968935384615385">
ings of SIGLEX-99, pages 1-8, University of
Maryland.
B. Magnini and G. Cavaglia. 2000. Integrating
subject field codes into WordNet. In Proceed-
ings of the LREC-2000, Athens, Greece.
Rada Mihalcea and Dan Moldovan. 2001a. eX-
tended WordNet: progress report. In Pro-
ceedings of NAACL Workshop on WordNet
and Other Lexical Resources, pages 95-100,
Pittsburgh, PA.
Rada Mihalcea and Dan I. Moldovan. 2001b.
A highly accurate bootstrapping algorithm
for word sense disambiguation. International
</bodyText>
<reference confidence="0.99683184375">
Journal on Artificial Intelligence Tools, 10(1-
2):5-21.
Rada Mihalcea. 2002. Instance based learning
with automatic feature selection applied to
word sense disambiguation. In Proceedings of
the 19th International Conference on Compu-
tational Linguistics COLING 2002, Taiwan.
M. Miller, G.and Chodorow, S. Landes, C. Lea-
cock, and R Thomas. 1994. Using a semantic
concordance for sense identication. In Pro-
ceedings of the ARPA Human Language Tech-
nology Workshop, pages 240-243.
G. Miller. 1995. Wordnet: a lexical database.
Communications of the ACM, 38(11):39-41.
Adrian Novischi. 2002. Accurate semantic an-
notations via pattern matching. In Pro-
ceedings of Florida Artificial Intelligence Re-
search Society, pages 375-379, Pensacola
Beach, Florida, USA.
Adrian Novischi. 2004. Combining methods
for word sense disambiguation of WordNet
glosses. In Proceedings of Florida Artificial
Intelligence Research Society, Miami Beach,
Florida, USA.
Ross Quinlan. 2003. Data Mining Tools See5
and C5.0., http://www.rulequest.com/see5-
info.html.
Mihai Surdeanu and Sanda Harabagiu. 2002.
Infrastructure for open-domain information
extraction. In Proceedings of the Human
Language Technology Conference, San Diego,
California.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000042">
<note confidence="0.884069666666667">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<abstract confidence="0.993042905759162">next word same as the previous heuristic but this heuristic forms a pair from the target word in a gloss and the next word and searches for this pair in the SemCor corpus. If the target word has the same sense in all occurrences of the pair, then the heuristic assigns this sense to the target word. reference an ambiguous word in the synset method looks for a refto the synset all the glosses corresponding to word W&apos;s senses. By reference to a word W we denote a word or a part of a compound concept that has the same lemma as the word W. cross reference a word in the gloss the synset method assigns to the word W the sense that contains in its set of synonyms one of the words from the among glosses the number of common word lemmas between two For an ambiguous word W in a gloss this method selects the sense with the greatest number of common word lemmas with the gloss G. Domain the domain labels assigned by (Magnini and Cavaglia, 2000) this method selects the sense of a word in a gloss that has the same domain as the synset of the gloss. the idiosyncratic nature of the WordNet glosses. The patterns of the form &amp;quot;N successive words&amp;quot; and &amp;quot;M words ... N words&amp;quot; are extracted offline from the Word- Net glosses and manually disambiguated. This method matches the patterns against glosses and assigns to the words the corresponding sense in the pattern. Sense Restricted method assigns sense 1 to a noun or verb if this sense has the smallest number of ancestors in the ISA hierarchy from all senses (it is the most general sense). The method selects sense 1 for an adjective if this sense has the greatest number of similarity pointers compared to all the other senses. 2.2 Disambiguation methods for English All Words task The disambiguation methods in this task can be classified in three types described below: methods based on heuristics, machine learning disambiguation methods, and incremental disambiguation methods. 2.2.1 WSD Methods based on Heuristics In this category the methods can be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994). Some methods applied to Semantic Disambiguation of Word- Net glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism. 2.2.2 Machine Learning Disambiguation met hods We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns. Support Vector Machines (SVM) Method We used the following set of features (Mihalcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and bigrams in a window of (-3,3) sentences. For disambiguating verbs we used an additional set of features: Verb mode (which can take 4 values: ACTIVE, INFINITIVE, PAST, GERUND), verb voice (which can take 2 values ACTIVE, PASSIVE), the parent of the current verb in the parse tree (ex: VP, NP), The first ancestor that is not VP in the parse tree (like S, NP, PP, SBAR). We generated feature values using the Senseval 2 Lexical Sample training corpus and Sem- Cor corpus combined, and we trained the SVM classifier on those words that had at least 10 training examples. C5.0 Rules Method The second machine learning method, used for word sense disambiguating nouns, is the C5.0 decision tree learning (Quinlan, 2003). For generating training examples we used the features described in (Mihalcea and Moldovan, 2001b). We used the SemCor 1.7.1 collection and Senseval 2 All Words collection as training corpus. 2.2.3 Incremental Disambiguation The main idea of incremental disambiguation is to disambiguate new words using senses of the previous disambiguated words. We used some of the procedures presented in (Mihalcea and Moldovan, 2001b) briefly described below: WordNet distance 0 with disamwords words that are in the same synset with already disambiguated words. WordNet distance 1 with disamwords words that are in a hypernymy/hyponymy relation with the words already disambiguated. 3 Combining Methods using Rules At this point each WSD system has a pool of disambiguation methods. An approach using rules for selecting the right sense was described in (Novischi, 2004) and is summarized below. For a given disambiguated word we create a training example for each of its senses. Given a manually-disambiguated corpus, training examples are generated for each sense of the tagged words with their correct classification. Then we can train a machine learning algorithm on this set of training examples.If we use C4.5 or C5.0, where each rule has an associated accuracy value, we can output the single sense that is classified as CORRECT by the rule with the best accuracy. For Semantic disambiguation of WordNet glosses task, we used a set of rules given by C4.5 program trained on a set of training examples generated from 3196 goldstandard glosses. For the English All Words task, however, we created the rules manually, each rule corresponding to a disambiguation method and assigned an accuracy value equal to the precision of the disambiguation method. The rules were tested in the decreasing order of their accuracy. 4 Results We measured the accuraccy of the LCC WSD systems using precision, recall and coverage. Precision is defined as the number of correct disambiguated words over the number of attempted words. Recall is defined as the number of correct disambiguated words over the total number of words for disambiguation. Coverage is defined as the number of attempted words over the total number of words. 4.1 English All Words In order to illustrate the performance of the system, we examine it from three different perspectives. First we ran the system without defaulting to first sense and computed the results excluding monosemous words (row a in table 1). Second we ran the system including defaulting to sense 1, but we still excluded monosemous words when we computed the results (row b in table 1). Third we included both the first sense for words that were not otherwise disambiguated by the system and monosemous words (row c in table 1). Table 1 presents the results given by the Python scorer script. Precision and recall in rows a and b do not include the monosemous words. 4.2 Semantic Disambiguation of WordNet glosses In order to illustrate the performance of the system, we examined it from two different perspectives. First we ran the system including assigning sense 1 for words that were not otherwise disambiguated by the system (row a. in table 2). Second we ran the system without assigning sense 1 to words that were not otherwise disambiguated by the system (row b. in table 2). 5 Conclusion The LCC WSD systems for Senseval 3 employ sets of disambiguation methods. Some methods are based on hand coded rules, some use information in WordNet, and some exploit the statistics of the SemCor corpus. Others use the senses of previous disambiguated words and still others are based on supervised machine learning algorithms. All these methods are combined using rules that optimize their output. The way the WSD systems are designed using multiple methods for disambiguation ensures a greater coverage than a single method by itself. Combining methods using machine learning algorithms improves the precision of the system. References Eric Brill. 1995. Unsupervised learning of disambiguation rules for part of speech tagging.</abstract>
<note confidence="0.911257617647059">In David Yarovsky and Kenneth Church, edof the Third Workshop on Large Corpora, 1-13, Somerset, New Jersey. Association for Computational Linguistics. Michael Collins. 1997. Three generative lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL 1997). Corinna Cortes and Vladimir Vapnik. 1995. networks. Learning, 20(3):273-297. S. Harabagiu, G. Miller, and D. Moldovan. 1999. WordNet 2 a morphologically and enhanced resource. In Proceed- Experiment Precision Recall Coverage a LCC WSD alone 53.6% 41.4% 77.3% b LCC WSD ± First Sense 53.9% 54.9% 101.9% c LCC WSD ± First Sense ± Monosemous 60.7% 60.7% 100.0% Table 1: Performance of the LCC WSD system in English All Words Task Experiment Precision Recall Coverage a LCC WSD 65.1% 62.2% 95.5% b LCC WSD without First Sense 70.1% 50.4% 71.9% Table 2: Performance of the LCC WSD system in WordNet glosses task of SIGLEX-99, 1-8, University of Maryland. B. Magnini and G. Cavaglia. 2000. Integrating field codes into WordNet. In Proceedof the LREC-2000, Greece. Rada Mihalcea and Dan Moldovan. 2001a. eX- WordNet: progress report. In Proceedings of NAACL Workshop on WordNet Other Lexical Resources, 95-100,</note>
<address confidence="0.690651">Pittsburgh, PA.</address>
<author confidence="0.432096">b</author>
<abstract confidence="0.933970714285714">A highly accurate bootstrapping algorithm word sense disambiguation. on Artificial Intelligence Tools, 10(1- 2):5-21. Rada Mihalcea. 2002. Instance based learning with automatic feature selection applied to sense disambiguation. In of</abstract>
<note confidence="0.592680125">the 19th International Conference on Compu- Linguistics COLING 2002, M. Miller, G.and Chodorow, S. Landes, C. Leacock, and R Thomas. 1994. Using a semantic for sense identication. In Proceedings of the ARPA Human Language Tech- Workshop, 240-243. G. Miller. 1995. Wordnet: a lexical database.</note>
<title confidence="0.382014">of the ACM,</title>
<author confidence="0.7158375">Accurate semantic anvia pattern matching In Pro-</author>
<affiliation confidence="0.933042">ceedings of Florida Artificial Intelligence Re-</affiliation>
<address confidence="0.984795">Society, 375-379, Pensacola Beach, Florida, USA.</address>
<author confidence="0.625476">Combining methods</author>
<affiliation confidence="0.671777666666667">for word sense disambiguation of WordNet In of Florida Artificial Research Society, Beach,</affiliation>
<address confidence="0.987054">Florida, USA.</address>
<author confidence="0.534135">Data Mining Tools See</author>
<note confidence="0.639743142857143">and C5.0., http://www.rulequest.com/see5info.html. Mihai Surdeanu and Sanda Harabagiu. 2002. Infrastructure for open-domain information In of the Human Technology Conference, Diego, California.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="2760" citStr="Cortes and Vapnik, 1995" startWordPosition="486" endWordPosition="489"> three types described below: methods based on heuristics, machine learning disambiguation methods, and incremental disambiguation methods. 2.2.1 WSD Methods based on Heuristics In this category the methods can be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994). Some methods applied to Semantic Disambiguation of WordNet glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism. 2.2.2 Machine Learning Disambiguation met hods We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns. Support Vector Machines (SVM) Method We used the following set of features (Mihalcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and bigrams in a window of (-3,3) sentences. For disambiguating verbs we used an additional set of features: Verb mode (which can take 4 values: ACTIVE, INFINITIVE, PAST, GERUND), verb voice (which can take 2 values ACTIVE, PASSIVE), the parent of the current verb in the parse tree (</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273-297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>G Miller</author>
<author>D Moldovan</author>
</authors>
<title>WordNet 2 - a morphologically and semantically enhanced resource.</title>
<date>1999</date>
<booktitle>In ProceedJournal on Artificial Intelligence Tools,</booktitle>
<pages>10--1</pages>
<marker>Harabagiu, Miller, Moldovan, 1999</marker>
<rawString>S. Harabagiu, G. Miller, and D. Moldovan. 1999. WordNet 2 - a morphologically and semantically enhanced resource. In ProceedJournal on Artificial Intelligence Tools, 10(1-2):5-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics COLING</booktitle>
<contexts>
<context position="2960" citStr="Mihalcea, 2002" startWordPosition="517" endWordPosition="519">n be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994). Some methods applied to Semantic Disambiguation of WordNet glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism. 2.2.2 Machine Learning Disambiguation met hods We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns. Support Vector Machines (SVM) Method We used the following set of features (Mihalcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and bigrams in a window of (-3,3) sentences. For disambiguating verbs we used an additional set of features: Verb mode (which can take 4 values: ACTIVE, INFINITIVE, PAST, GERUND), verb voice (which can take 2 values ACTIVE, PASSIVE), the parent of the current verb in the parse tree (ex: VP, NP), The first ancestor that is not VP in the parse tree (like S, NP, PP, SBAR). We generated feature values using the Senseval 2 Lexical Sample training corpus and SemCor corpus combined, and</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Instance based learning with automatic feature selection applied to word sense disambiguation. In Proceedings of the 19th International Conference on Computational Linguistics COLING 2002, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Miller</author>
<author>G and Chodorow</author>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Thomas</author>
</authors>
<title>Using a semantic concordance for sense identication.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>240--243</pages>
<contexts>
<context position="2481" citStr="Miller et al., 1994" startWordPosition="443" endWordPosition="446"> most general sense). The method selects sense 1 for an adjective if this sense has the greatest number of similarity pointers compared to all the other senses. 2.2 Disambiguation methods for English All Words task The disambiguation methods in this task can be classified in three types described below: methods based on heuristics, machine learning disambiguation methods, and incremental disambiguation methods. 2.2.1 WSD Methods based on Heuristics In this category the methods can be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994). Some methods applied to Semantic Disambiguation of WordNet glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism. 2.2.2 Machine Learning Disambiguation met hods We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns. Support Vector Machines (SVM) Method We used the following set of features (Mihalcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and b</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>M. Miller, G.and Chodorow, S. Landes, C. Leacock, and R Thomas. 1994. Using a semantic concordance for sense identication. In Proceedings of the ARPA Human Language Technology Workshop, pages 240-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: a lexical database.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--11</pages>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. Wordnet: a lexical database. Communications of the ACM, 38(11):39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Novischi</author>
</authors>
<title>Accurate semantic annotations via pattern matching.</title>
<date>2002</date>
<booktitle>In Proceedings of Florida Artificial Intelligence Research Society,</booktitle>
<pages>375--379</pages>
<location>Pensacola Beach, Florida, USA.</location>
<marker>Novischi, 2002</marker>
<rawString>Adrian Novischi. 2002. Accurate semantic annotations via pattern matching. In Proceedings of Florida Artificial Intelligence Research Society, pages 375-379, Pensacola Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Novischi</author>
</authors>
<title>Combining methods for word sense disambiguation of WordNet glosses.</title>
<date>2004</date>
<booktitle>In Proceedings of Florida Artificial Intelligence Research Society,</booktitle>
<location>Miami Beach, Florida, USA.</location>
<contexts>
<context position="4684" citStr="Novischi, 2004" startWordPosition="798" endWordPosition="799">isambiguate new words using senses of the previous disambiguated words. We used some of the procedures presented in (Mihalcea and Moldovan, 2001b) briefly described below: WordNet distance 0 with disambiguated words - find words that are in the same synset with already disambiguated words. WordNet distance 1 with disambiguated words - find words that are in a hypernymy/hyponymy relation with the words already disambiguated. 3 Combining Methods using Rules At this point each WSD system has a pool of disambiguation methods. An approach using rules for selecting the right sense was described in (Novischi, 2004) and is summarized below. For a given disambiguated word we create a training example for each of its senses. Given a manually-disambiguated corpus, training examples are generated for each sense of the tagged words with their correct classification. Then we can train a machine learning algorithm on this set of training examples.If we use C4.5 or C5.0, where each rule has an associated accuracy value, we can output the single sense that is classified as CORRECT by the rule with the best accuracy. For Semantic disambiguation of WordNet glosses task, we used a set of rules given by C4.5 program </context>
</contexts>
<marker>Novischi, 2004</marker>
<rawString>Adrian Novischi. 2004. Combining methods for word sense disambiguation of WordNet glosses. In Proceedings of Florida Artificial Intelligence Research Society, Miami Beach, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<date>2003</date>
<booktitle>Data Mining Tools See5 and C5.0.,</booktitle>
<pages>5</pages>
<contexts>
<context position="2836" citStr="Quinlan, 2003" startWordPosition="499" endWordPosition="500">tion methods, and incremental disambiguation methods. 2.2.1 WSD Methods based on Heuristics In this category the methods can be further classified in methods that use hand coded rules, methods that use WordNet and methods that use SemCor (Miller et al., 1994). Some methods applied to Semantic Disambiguation of WordNet glosses were also applied on open text: Semcor Previous Word, Semcor Next Word, Patterns and Lexical Parallelism. 2.2.2 Machine Learning Disambiguation met hods We used Support Vector Machines (Cortes and Vapnik, 1995) for disambiguating verbs, adjectives, and adverbs, and C5.0 (Quinlan, 2003) rules for disambiguating nouns. Support Vector Machines (SVM) Method We used the following set of features (Mihalcea, 2002): current word form and part of speech, contextual features, collocations in a window of (-3,3) words, and keywords and bigrams in a window of (-3,3) sentences. For disambiguating verbs we used an additional set of features: Verb mode (which can take 4 values: ACTIVE, INFINITIVE, PAST, GERUND), verb voice (which can take 2 values ACTIVE, PASSIVE), the parent of the current verb in the parse tree (ex: VP, NP), The first ancestor that is not VP in the parse tree (like S, NP</context>
</contexts>
<marker>Quinlan, 2003</marker>
<rawString>Ross Quinlan. 2003. Data Mining Tools See5 and C5.0., http://www.rulequest.com/see5-info.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Infrastructure for open-domain information extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>San Diego, California.</location>
<marker>Surdeanu, Harabagiu, 2002</marker>
<rawString>Mihai Surdeanu and Sanda Harabagiu. 2002. Infrastructure for open-domain information extraction. In Proceedings of the Human Language Technology Conference, San Diego, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>