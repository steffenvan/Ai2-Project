<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040790">
<title confidence="0.965788">
The Need for Intentionally-Based Approaches to Language*
</title>
<author confidence="0.796376">
Karen E. Lochbaum
</author>
<bodyText confidence="0.856803">
Aiken Computation Lab
Harvard University
Cambridge, MA 02138
kelOdas.harvard.edu
</bodyText>
<subsectionHeader confidence="0.889344">
The Claim
</subsectionHeader>
<bodyText confidence="0.999733">
Discourses are inherently intentional; conversational participants engage in them for a reason. Sys-
tems for natural language interpretation and generation that do not account, with every utterance,
for the purposeful nature of discourse cannot adequately participate in collaborative dialogues.
</bodyText>
<subsectionHeader confidence="0.832427">
The Approach: A SharedPlan Analysis of Subdialogues
</subsectionHeader>
<bodyText confidence="0.999970882352941">
Evidence for the above claim comes from my recent work on understanding subdialogues in conver-
sation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an in-
stance of the more general phenomenon of collaboration. For agents to successfully collaborate on a
task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish
that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93]
specify the requisite components of the mental states of collaborating agents and consequently
provide an important context for interpreting their utterances. In particular, because agents are
aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be
understood as contributing information towards the establishment of the appropriate mental states,
and thus the building of such a plan. The process by which utterances are understood has been
formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and
used to explain utterances concerning the performance of actions [LGS90, Loc91].
My current work [Loc93], based on Grosz and Sidner&apos;s theory of discourse structure [GS86],
provides a new approach to the problem of understanding subdialogues and their relationship to
the discourse in which they are embedded. The basic approach entails treating each subdialogue
or discourse segment as a separate collaboration between the conversational participants; each ut-
terance of a subdialogue is understood as contributing some information towards the completion
of a SharedPlan. Because subdialogues do not occur in isolation, each subdialogue itself is under-
stood in terms of the role its corresponding SharedPlan plays in satisfying the other SharedPlans
underlying the dialogue. In particular, if the completion of the new SharedPlan contributes to
the establishment of one of the beliefs or intentions necessary for the completion of another, then
the first SharedPlan is said to be subsidiary to the second. A subsidiary relationship between
SharedPlans corresponds to a dominance relationship [GS86] between discourse segment purposes;
it provides an explanation [SI81] for why the dominated segment was initiated by a conversational
participantl.
An Example The excerpt in Figure 1, taken from a larger discourse concerned with the replace-
ment of an air compressor pump by an Expert and an Apprentice [GS86], will be used to illustrate
the approach. Because the participants in this discourse are collaborating on the replacement of the
pump, the first utterance of this excerpt can be understood as establishing mutual belief that the
action described therein, i.e. the removal of the flywheel by the Apprentice, is part of the recipe the
agents will use to accomplish their task [LGS90]. Utterance (2), however, begins a new discourse
segment, the purpose of which is to bring it about that the Apprentice knows how to perform
(i.e. has a recipe for) the act of removing the flywheel. This segment is recognized as a separate
collaboration using a conversational default rule for recognizing an agent&apos;s desire to collaborate on
</bodyText>
<footnote confidence="0.584595333333333">
el would like to thank Barbara Grosz, Christine Nakatani, and Candy Sidner for their comments on this paper.
This research has been supported by U S West Advanced Technologies and by a Bellcore Graduate Fellowship.
&apos;A detailed description of the model can be found in [Loc93).
</footnote>
<page confidence="0.999295">
64
</page>
<bodyText confidence="0.999403">
the performance of an act [GS90]. The individual utterances of this segment are then understood
as directed towards completing a SharedPlan to achieve the purpose of the segment, namely that
the Apprentice have a recipe for removing the flywheel. For example, in utterance (3), the Expert&apos;s
telling the Apprentice the steps in removing the flywheel and an ordering constraint on them (i.e.
&amp;quot;First, loosen ..., then pull ...&amp;quot;) constitutes a way of achieving that the Apprentice has the
requisite recipe.
</bodyText>
<figure confidence="0.990438923076923">
1 E:. How
do I re
First you he flywheel?riemove
theflwheel.
2 A
3 E: First, loosen the two alien head setscrews holding it the shaft,
then pull it off.
4 A: 0K.
can
5 I
only
find one screw. Where&apos;s the other one?
6 E: On the hub of the flywheel.
</figure>
<figureCaption confidence="0.999988">
Figure 1: Sample Multi-segment Discourse
</figureCaption>
<bodyText confidence="0.999985777777778">
To explain the new discourse segment itself, the Expert must determine how the SharedPlan
corresponding to the segment is related to the previous SharedPlan. Because the Expert knows
that part of having a SharedPlan is mutually believing that the agent of each act is able to perform
that act, the Expert can infer that the Apprentice has engaged in the new SharedPlan in order to
bring about an instance of that condition required by the previous or &amp;quot;interrupted&amp;quot; SharedPlan.
In particular, because utterance (1) has established that removing the flywheel is part of the recipe
for replacing the pump, the Expert can infer that the Apprentice has engaged in the subsidiary
SharedPlan to bring about a knowledge precondition [Mor87] required of the dominating SharedPlan
to replace the pump.
</bodyText>
<sectionHeader confidence="0.991548" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.999848142857143">
Agents engage in subdialogues for many reasons. For example, they may engage in them based on
the need for information to perform actions or to weigh options, the need to correct problems that
arise during plan execution, or simply as the result of the normal decomposition of a task. My
claim is that all subdialogues can be understood in terms of collaborations between agents, and
thus modelled using SharedPlans. The type of plan does not vary with the type of subdialogue, or
an agent&apos;s reason for engaging in it, only the object of that plan does2. The same methods used
in our previous work to understand utterances concerned with &amp;quot;domain goals&amp;quot; [LA87], can also be
used to understand utterances aimed at achieving other types of goals; the only difference is in the
object of the recipes and SharedPlans used by the algorithms.
A Rhetorical Relations Approach This approach contrasts sharply with the less goal-directed
account of subdialogues given by Litman and Allen [Lit85, LA87]. In their model, the process of un-
derstanding an utterance entails recognizing a discourse plan from the utterance and then relating
that discourse plan to some domain plan. While domain plans represent knowledge about a task,
discourse plans represent knowledge about relationships between utterances and plans; for example,
an agent may use an utterance to introduce, continue, or clarify a plan. Litman and Allen propose
their discourse plans as plan-based correlates of rhetorical relations [Lit86, LA87]. Although these
plans address some of the problems with computationally based RST [MT87] analyses (i.e. the for-
malization and recognition of rhetorical relations), they are extremely rigid in nature and narrow
in scope. Each discourse plan (or more specifically the constraints of its decomposition) represents
only one specific way in which an utterance can relate to a plan. For example, the only way some-
thing can go wrong and be corrected according to the CORRECT-PLAN discourse plan is if a
</bodyText>
<footnote confidence="0.9577965">
2By the object of the plan, I mean the act on which the agents are collaborating, i.e. the a in SharedPlan({GI,
G2J,a,TI,T2)•
</footnote>
<page confidence="0.999677">
65
</page>
<bodyText confidence="0.998978157894737">
speaker, not being able to perform the next step in a plan, requests that the hearer do something
else first so that he can. In addition, because a new discourse plan is recognized from every ut-
terance, CORRECT-PLAN itself does not model extended problem-solving subdialogues, but only
one utterance within such subdialogues. Further utterances of the subdialogue are only understood
in terms of their relationship to preceding ones. This approach cannot adequately capture the con-
tribution an utterance of a subdialogue makes to the higher-level purpose of the subdialogue. For
example, in the dialogue of Figure 2, utterances (3)-(4) seem intuitively to comprise a subdialogue
the purpose of which is to correct a problem that has occurred during plan execution. Utterance
(3) identifies the problem, while utterance (4) suggests a way of fixing it. Under Litman and Allen&apos;s
analysis, however, utterance (3) is understood as an instance of the CORRECT-PLAN discourse
plan (with the utterance, the User is correcting the domain plan to add data to a network), while
utterance (4) is understood as an instance of IDENTIFY-PARAMETER. The parameter that ut-
terance (4) is understood to be identifying is one in the CORRECT-PLAN discourse plan, namely
the parameter that specifies what new step is being added to a domain plan to correct it. Not
only does this analysis run counter to intuitions as to what utterances (3) and (4) both individu-
ally and collectively are about, but, as used in a model of plan recognition, it constitutes a claim
that the speaker (i.e. User) (i) produces utterances (3) and (4) intending to perform the above
CORRECT-PLAN and IDENTIFY-PARAMETER actions respectively, and (ii) intends that the
hearer (System) recognize these intentions.
</bodyText>
<listItem confidence="0.827684833333333">
(1 User: Show me the generic concept called &amp;quot;employee&amp;quot;.
(2 System: OK. &lt;system displays network&gt;
3 User: I can&apos;t fit a new ic below it.
4 Can you move it up?
5 System: Yes. &lt;system displays network&gt;
(6 User: OK, now make...
</listItem>
<figureCaption confidence="0.99386">
Figure 2: A Correction Subdialogue (taken from Litman[Lit85])
</figureCaption>
<bodyText confidence="0.995253523809524">
The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that
it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to
not recognizing discourse segments as separate units with an overall purpose, the model also fails
to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it
cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91]
that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same
shortcomings3.
Evidence from Generation Work in generation has recognized a similar problem with respect
to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have
argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an inten-
tional structure in order to respond to follow-up questions. The problem is that although solely
RST-based approaches associate a communicative goal with each schema, they do not represent
the intended effect of each component of the schema, nor the role that each component plays in
satisfying the overall communicative goal associated with the schema. Without such information,
a system cannot respond effectively if the hearer does not understand or accept its utterances.
In response to this problem, Moore and Paris have devised a planner that constructs text plans
containing both intentional and rhetorical information. By recording these text plans as part of
the dialogue history, their system is able to reason about its previous utterances in interpreting
and responding to users&apos; follow-up questions.
Conclusions Both the interpretation process and the generation process need intentionally-based
approaches to language. In the former, a solely intentional approach provides a more general
</bodyText>
<note confidence="0.279181">
&apos;Analyses of all of these approaches can also be found in [Loc93].
</note>
<page confidence="0.939497">
66
</page>
<bodyText confidence="0.9999205">
model for understanding subdialogues and their relationships. In the latter, intentional information
augments RST information to allow more effective participation in explanation dialogues. Although
rhetorical relations have proved useful in machine-based natural language generation (see Hovy&apos;s
recent survey [Hov93]), their cognitive role remains unclear. Does a speaker actually have them &amp;quot;in
mind&amp;quot; when he produces utterances? Or are they only &amp;quot;compilations&amp;quot; of intentional information
that are computationally efficient for generation systems [MP9111 And if a speaker does have
rhetorical relations in mind, does a hearer actually infer them? On that matter, I&apos;d argue, based
on the above discussion (and following Grosz and Sidner [GS86]), that a discourse can be understood
even if the hearer (be it machine or person) cannot infer, construct, or name any such relations
used by the speaker.
</bodyText>
<sectionHeader confidence="0.998845" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989415">
[GK93] B. J. Grosz and S. Kraus. Collaborative plans for group activities. In Proceedings of IJCA1-93,
Chambery, Savoie, France, 1993.
[GS86] B.J. Grosz and C.L. Sidner. Attention, intentions, and the structure of discourse. Computational
Linguistics, 12(3), 1986.
[GS90] B.J. Grosz and C.L. Sidner. Plans for discourse. In P.R. Cohen, J.L. Morgan, and M.E. Pollack,
editors, Intentions in Communication. MIT Press, 1990.
[Hov88] E. H. Hovy. Planning coherent mulitsentential text. In Proceedings of ACL-88.
[Hov93] E.H. Hovy. Automated discourse generation using discourse structure relations. Aritificial Intelli-
gence, 1993. To appear.
[LA87] D.J. Litman and J.F. Allen. A plan recognition model for subdialogues in conversations. Cognitive
Science, 11:163-200,1987.
[LC91] L. Lambert and S. Carberry. A tripartite plan-based model of dialogue. In Proceedings of ACL-91.
[LC92] L. Lambert and S. Carberry. Modeling negotiation subdialogues. In Proceedings of ACL-92.
[LGS90] K. E. Lochbaum, B. J. Grosz, and C. L. Sidner. Models of plans to support communication: An
initial report. In Proceedings of AAAI-90, Boston, MA, 1990.
[Lit85] D. J. Litman. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding
Dialogues. PhD thesis, University of Rochester, 1985.
[Lit86] D. J. Litman. Linguistic coherence: A plan-based alternative. In Proceedings of ACL-86.
[Loc91] K. E. Lochbaum. An algorithm for plan recognition in collaborative discourse. In Proceedings of
ACL-91.
[Loc93] K. E. Lochbaum. A collaborative planning approach to understanding subdialogues in conversation.
Technical report, Harvard University, 1993.
[McK85] K. R. McKeown. Text Generation: Using Discourse Strategies and Focus Constraints to Generate
Natural Languge Text. Cambridge University Press, Cambridge, England, 1985.
[Mor87] L. Morgenstern. Knowledge preconditions for actions and plans. In Proceedings of IJCAI-87.
[MP91] J. D. Moore and C. L. Paris. Discourse structure for explanatory dialogues. In AAAI Fall 1991
Symposium on Discourse Structure in Natural Language Understanding and Generation, Asilomar.
[MP92] J.D. Moore and M.E. Pollack. A problem for RST: The need for multi-level discourse analysis.
Computational Linguistics, 18(4), December 1992.
[MT87] W.C. Mann and S.A. Thompson. Rhetorical structure theory: A theory of text organization. In
L. Polanyi, editor, The Structure of Discourse. Ablex Publishing Corp., 1987.
[Ram91] L. A. Ramshaw. A three-level model for plan exploration. In Proceedings of ACL-91.
[SI81] C.L. Sidner and D. J. Israel. Recognizing intended meaning and speakers&apos; plans. In Proceedings of
JC A 1-81.
</reference>
<page confidence="0.999475">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.064588">
<title confidence="0.999001">The Need for Intentionally-Based Approaches to Language*</title>
<author confidence="0.990168">E Karen</author>
<affiliation confidence="0.866718">Aiken Computation Harvard</affiliation>
<address confidence="0.991334">Cambridge, MA</address>
<email confidence="0.992332">kelOdas.harvard.edu</email>
<abstract confidence="0.987946675">The Claim Discourses are inherently intentional; conversational participants engage in them for a reason. Systems for natural language interpretation and generation that do not account, with every utterance, for the purposeful nature of discourse cannot adequately participate in collaborative dialogues. The Approach: A SharedPlan Analysis of Subdialogues Evidence for the above claim comes from my recent work on understanding subdialogues in conversation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an instance of the more general phenomenon of collaboration. For agents to successfully collaborate on a task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93] specify the requisite components of the mental states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be understood as contributing information towards the establishment of the appropriate mental states, and thus the building of such a plan. The process by which utterances are understood has been formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and used to explain utterances concerning the performance of actions [LGS90, Loc91]. My current work [Loc93], based on Grosz and Sidner&apos;s theory of discourse structure [GS86], provides a new approach to the problem of understanding subdialogues and their relationship to the discourse in which they are embedded. The basic approach entails treating each subdialogue or discourse segment as a separate collaboration between the conversational participants; each utterance of a subdialogue is understood as contributing some information towards the completion of a SharedPlan. Because subdialogues do not occur in isolation, each subdialogue itself is understood in terms of the role its corresponding SharedPlan plays in satisfying the other SharedPlans underlying the dialogue. In particular, if the completion of the new SharedPlan contributes to the establishment of one of the beliefs or intentions necessary for the completion of another, then first SharedPlan is said to be the second. A subsidiary relationship between SharedPlans corresponds to a dominance relationship [GS86] between discourse segment purposes; it provides an explanation [SI81] for why the dominated segment was initiated by a conversational participantl. Example excerpt in Figure 1, taken from a larger discourse concerned with the replacement of an air compressor pump by an Expert and an Apprentice [GS86], will be used to illustrate the approach. Because the participants in this discourse are collaborating on the replacement of the pump, the first utterance of this excerpt can be understood as establishing mutual belief that the described therein, i.e. the removal of the flywheel by the Apprentice, is part of the agents will use to accomplish their task [LGS90]. Utterance (2), however, begins a new discourse segment, the purpose of which is to bring it about that the Apprentice knows how to perform (i.e. has a recipe for) the act of removing the flywheel. This segment is recognized as a separate collaboration using a conversational default rule for recognizing an agent&apos;s desire to collaborate on would like to thank Barbara Grosz, Christine Nakatani, and Candy Sidner for their comments on this paper.</abstract>
<note confidence="0.848133666666667">research supported by U S West Advanced Technologies and by a Bellcore Graduate Fellowship. &apos;A detailed description of the model can be found in [Loc93). 64</note>
<abstract confidence="0.990136973214286">the performance of an act [GS90]. The individual utterances of this segment are then understood as directed towards completing a SharedPlan to achieve the purpose of the segment, namely that the Apprentice have a recipe for removing the flywheel. For example, in utterance (3), the Expert&apos;s telling the Apprentice the steps in removing the flywheel and an ordering constraint on them (i.e. ..., pull ...&amp;quot;) a way of achieving that the Apprentice has the requisite recipe. 1 E:. How I re you he theflwheel. A E: loosen the two alien head setscrews holding it the shaft, then pull it off. 0K. can 5I only find one screw. Where&apos;s the other one? 6 E: On the hub of the flywheel. Figure 1: Sample Multi-segment Discourse To explain the new discourse segment itself, the Expert must determine how the SharedPlan corresponding to the segment is related to the previous SharedPlan. Because the Expert knows that part of having a SharedPlan is mutually believing that the agent of each act is able to perform that act, the Expert can infer that the Apprentice has engaged in the new SharedPlan in order to bring about an instance of that condition required by the previous or &amp;quot;interrupted&amp;quot; SharedPlan. In particular, because utterance (1) has established that removing the flywheel is part of the recipe for replacing the pump, the Expert can infer that the Apprentice has engaged in the subsidiary to bring about a precondition required of the dominating SharedPlan to replace the pump. Discussion Agents engage in subdialogues for many reasons. For example, they may engage in them based on the need for information to perform actions or to weigh options, the need to correct problems that arise during plan execution, or simply as the result of the normal decomposition of a task. My claim is that all subdialogues can be understood in terms of collaborations between agents, and modelled using SharedPlans. The type of not vary with the type of subdialogue, or agent&apos;s reason for engaging in it, only the object of that plan The same methods used in our previous work to understand utterances concerned with &amp;quot;domain goals&amp;quot; [LA87], can also be used to understand utterances aimed at achieving other types of goals; the only difference is in the object of the recipes and SharedPlans used by the algorithms. Rhetorical Relations Approach approach contrasts sharply with the less goal-directed account of subdialogues given by Litman and Allen [Lit85, LA87]. In their model, the process of unan utterance entails recognizing a from the utterance and then relating discourse plan to some domain plans represent knowledge about a task, discourse plans represent knowledge about relationships between utterances and plans; for example, agent may use an utterance to continue, plan. Litman and Allen propose their discourse plans as plan-based correlates of rhetorical relations [Lit86, LA87]. Although these plans address some of the problems with computationally based RST [MT87] analyses (i.e. the formalization and recognition of rhetorical relations), they are extremely rigid in nature and narrow in scope. Each discourse plan (or more specifically the constraints of its decomposition) represents only one specific way in which an utterance can relate to a plan. For example, the only way something can go wrong and be corrected according to the CORRECT-PLAN discourse plan is if a the object of the plan, I mean the act on which the agents are collaborating, i.e. the a in SharedPlan({GI, G2J,a,TI,T2)• 65 speaker, not being able to perform the next step in a plan, requests that the hearer do something else first so that he can. In addition, because a new discourse plan is recognized from every utterance, CORRECT-PLAN itself does not model extended problem-solving subdialogues, but only one utterance within such subdialogues. Further utterances of the subdialogue are only understood in terms of their relationship to preceding ones. This approach cannot adequately capture the contribution an utterance of a subdialogue makes to the higher-level purpose of the subdialogue. For example, in the dialogue of Figure 2, utterances (3)-(4) seem intuitively to comprise a subdialogue the purpose of which is to correct a problem that has occurred during plan execution. Utterance (3) identifies the problem, while utterance (4) suggests a way of fixing it. Under Litman and Allen&apos;s analysis, however, utterance (3) is understood as an instance of the CORRECT-PLAN discourse plan (with the utterance, the User is correcting the domain plan to add data to a network), while utterance (4) is understood as an instance of IDENTIFY-PARAMETER. The parameter that utterance (4) is understood to be identifying is one in the CORRECT-PLAN discourse plan, namely the parameter that specifies what new step is being added to a domain plan to correct it. Not only does this analysis run counter to intuitions as to what utterances (3) and (4) both individually and collectively are about, but, as used in a model of plan recognition, it constitutes a claim that the speaker (i.e. User) (i) produces utterances (3) and (4) intending to perform the above CORRECT-PLAN and IDENTIFY-PARAMETER actions respectively, and (ii) intends that the hearer (System) recognize these intentions. (1 User: Show me the generic concept called &amp;quot;employee&amp;quot;. System: OK. displays network&gt; 3 User: can&apos;t fit a new ic below 4 Can move it System: Yes. displays network&gt; (6 User: OK, now make... Figure 2: A Correction Subdialogue (taken from Litman[Lit85]) The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it account for engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same from Generation in generation has recognized a similar problem with respect RST-based approaches. In particular, Moore [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each component of the schema, nor the role that each component plays in satisfying the overall communicative goal associated with the schema. Without such information, a system cannot respond effectively if the hearer does not understand or accept its utterances. In response to this problem, Moore and Paris have devised a planner that constructs text plans containing both intentional and rhetorical information. By recording these text plans as part of the dialogue history, their system is able to reason about its previous utterances in interpreting and responding to users&apos; follow-up questions. the interpretation process and the generation process need intentionally-based approaches to language. In the former, a solely intentional approach provides a more general &apos;Analyses of all of these approaches can also be found in [Loc93]. 66 model for understanding subdialogues and their relationships. In the latter, intentional information augments RST information to allow more effective participation in explanation dialogues. Although rhetorical relations have proved useful in machine-based natural language generation (see Hovy&apos;s recent survey [Hov93]), their cognitive role remains unclear. Does a speaker actually have them &amp;quot;in mind&amp;quot; when he produces utterances? Or are they only &amp;quot;compilations&amp;quot; of intentional information that are computationally efficient for generation systems [MP9111 And if a speaker does have rhetorical relations in mind, does a hearer actually infer them? On that matter, I&apos;d argue, based on the above discussion (and following Grosz and Sidner [GS86]), that a discourse can be understood even if the hearer (be it machine or person) cannot infer, construct, or name any such relations used by the speaker.</abstract>
<note confidence="0.972325457142857">References B. J. Grosz and S. Kraus. Collaborative plans for group activities. In of IJCA1-93, Chambery, Savoie, France, 1993. B.J. Grosz and C.L. Sidner. Attention, intentions, and the structure of discourse. 1986. [GS90] B.J. Grosz and C.L. Sidner. Plans for discourse. In P.R. Cohen, J.L. Morgan, and M.E. Pollack, in Communication. Press, 1990. E. H. Hovy. Planning coherent mulitsentential text. In of ACL-88. E.H. Hovy. Automated discourse generation using discourse structure relations. Intelli- To appear. D.J. Litman and J.F. Allen. A plan recognition model for subdialogues in conversations. L. Lambert and S. Carberry. A tripartite plan-based model of dialogue. In of ACL-91. L. Lambert and S. Carberry. Modeling negotiation subdialogues. In of ACL-92. [LGS90] K. E. Lochbaum, B. J. Grosz, and C. L. Sidner. Models of plans to support communication: An report. In of AAAI-90, MA, 1990. D. J. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding thesis, University of Rochester, 1985. D. J. Litman. Linguistic coherence: A plan-based alternative. In of ACL-86. K. E. Lochbaum. An algorithm for plan recognition in collaborative discourse. In of ACL-91. [Loc93] K. E. Lochbaum. A collaborative planning approach to understanding subdialogues in conversation. Technical report, Harvard University, 1993. K. R. McKeown. Generation: Using Discourse Strategies and Focus Constraints to Generate Languge Text. University Press, Cambridge, England, 1985. L. Morgenstern. Knowledge preconditions for actions and plans. In of IJCAI-87. J. D. Moore and C. L. Paris. Discourse structure for explanatory dialogues. In Fall 1991 on Discourse Structure in Natural Language Understanding and Generation, [MP92] J.D. Moore and M.E. Pollack. A problem for RST: The need for multi-level discourse analysis. Linguistics, December 1992. [MT87] W.C. Mann and S.A. Thompson. Rhetorical structure theory: A theory of text organization. In Polanyi, editor, Structure of Discourse. Publishing Corp., 1987. L. A. Ramshaw. A three-level model for plan exploration. In of ACL-91. C.L. Sidner and D. J. Israel. Recognizing intended meaning and speakers&apos; plans. In of JC A 1-81. 67</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>S Kraus</author>
</authors>
<title>Collaborative plans for group activities.</title>
<date>1993</date>
<booktitle>In Proceedings of IJCA1-93, Chambery,</booktitle>
<location>Savoie, France,</location>
<contexts>
<context position="1003" citStr="[GS90, LGS90, GK93]" startWordPosition="141" endWordPosition="143">ature of discourse cannot adequately participate in collaborative dialogues. The Approach: A SharedPlan Analysis of Subdialogues Evidence for the above claim comes from my recent work on understanding subdialogues in conversation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an instance of the more general phenomenon of collaboration. For agents to successfully collaborate on a task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93] specify the requisite components of the mental states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be understood as contributing information towards the establishment of the appropriate mental states, and thus the building of such a plan. The process by which utterances are understood has been formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and used to explain ut</context>
</contexts>
<marker>[GK93]</marker>
<rawString>B. J. Grosz and S. Kraus. Collaborative plans for group activities. In Proceedings of IJCA1-93, Chambery, Savoie, France, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="1755" citStr="[GS86]" startWordPosition="254" endWordPosition="254">r utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be understood as contributing information towards the establishment of the appropriate mental states, and thus the building of such a plan. The process by which utterances are understood has been formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and used to explain utterances concerning the performance of actions [LGS90, Loc91]. My current work [Loc93], based on Grosz and Sidner&apos;s theory of discourse structure [GS86], provides a new approach to the problem of understanding subdialogues and their relationship to the discourse in which they are embedded. The basic approach entails treating each subdialogue or discourse segment as a separate collaboration between the conversational participants; each utterance of a subdialogue is understood as contributing some information towards the completion of a SharedPlan. Because subdialogues do not occur in isolation, each subdialogue itself is understood in terms of the role its corresponding SharedPlan plays in satisfying the other SharedPlans underlying the dialog</context>
<context position="2991" citStr="[GS86]" startWordPosition="439" endWordPosition="439">letion of the new SharedPlan contributes to the establishment of one of the beliefs or intentions necessary for the completion of another, then the first SharedPlan is said to be subsidiary to the second. A subsidiary relationship between SharedPlans corresponds to a dominance relationship [GS86] between discourse segment purposes; it provides an explanation [SI81] for why the dominated segment was initiated by a conversational participantl. An Example The excerpt in Figure 1, taken from a larger discourse concerned with the replacement of an air compressor pump by an Expert and an Apprentice [GS86], will be used to illustrate the approach. Because the participants in this discourse are collaborating on the replacement of the pump, the first utterance of this excerpt can be understood as establishing mutual belief that the action described therein, i.e. the removal of the flywheel by the Apprentice, is part of the recipe the agents will use to accomplish their task [LGS90]. Utterance (2), however, begins a new discourse segment, the purpose of which is to bring it about that the Apprentice knows how to perform (i.e. has a recipe for) the act of removing the flywheel. This segment is reco</context>
</contexts>
<marker>[GS86]</marker>
<rawString>B.J. Grosz and C.L. Sidner. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Plans for discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication.</booktitle>
<editor>In P.R. Cohen, J.L. Morgan, and M.E. Pollack, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="668" citStr="[GS90]" startWordPosition="91" endWordPosition="91"> E. Lochbaum Aiken Computation Lab Harvard University Cambridge, MA 02138 kelOdas.harvard.edu The Claim Discourses are inherently intentional; conversational participants engage in them for a reason. Systems for natural language interpretation and generation that do not account, with every utterance, for the purposeful nature of discourse cannot adequately participate in collaborative dialogues. The Approach: A SharedPlan Analysis of Subdialogues Evidence for the above claim comes from my recent work on understanding subdialogues in conversation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an instance of the more general phenomenon of collaboration. For agents to successfully collaborate on a task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93] specify the requisite components of the mental states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan</context>
<context position="4026" citStr="[GS90]" startWordPosition="610" endWordPosition="610"> segment, the purpose of which is to bring it about that the Apprentice knows how to perform (i.e. has a recipe for) the act of removing the flywheel. This segment is recognized as a separate collaboration using a conversational default rule for recognizing an agent&apos;s desire to collaborate on el would like to thank Barbara Grosz, Christine Nakatani, and Candy Sidner for their comments on this paper. This research has been supported by U S West Advanced Technologies and by a Bellcore Graduate Fellowship. &apos;A detailed description of the model can be found in [Loc93). 64 the performance of an act [GS90]. The individual utterances of this segment are then understood as directed towards completing a SharedPlan to achieve the purpose of the segment, namely that the Apprentice have a recipe for removing the flywheel. For example, in utterance (3), the Expert&apos;s telling the Apprentice the steps in removing the flywheel and an ordering constraint on them (i.e. &amp;quot;First, loosen ..., then pull ...&amp;quot;) constitutes a way of achieving that the Apprentice has the requisite recipe. 1 E:. How do I re First you he flywheel?riemove theflwheel. 2 A 3 E: First, loosen the two alien head setscrews holding it the sh</context>
</contexts>
<marker>[GS90]</marker>
<rawString>B.J. Grosz and C.L. Sidner. Plans for discourse. In P.R. Cohen, J.L. Morgan, and M.E. Pollack, editors, Intentions in Communication. MIT Press, 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E H Hovy</author>
</authors>
<title>Planning coherent mulitsentential text.</title>
<booktitle>In Proceedings of ACL-88.</booktitle>
<contexts>
<context position="10610" citStr="[Hov88, McK85]" startWordPosition="1673" endWordPosition="1674">rate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each component of the schema, nor the role that each component plays in satisfying the overall communicative goal associated with the schema. Without such information, a system cannot respond effectively if the hearer does not understand or accept its utterances. In response to this problem, Moore and Paris have devised a planner that constructs text plans containing both</context>
</contexts>
<marker>[Hov88]</marker>
<rawString>E. H. Hovy. Planning coherent mulitsentential text. In Proceedings of ACL-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Aritificial Intelligence,</journal>
<note>To appear.</note>
<marker>[Hov93]</marker>
<rawString>E.H. Hovy. Automated discourse generation using discourse structure relations. Aritificial Intelligence, 1993. To appear.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D J Litman</author>
<author>J F Allen</author>
</authors>
<title>A plan recognition model for subdialogues in conversations.</title>
<journal>Cognitive Science,</journal>
<pages>11--163</pages>
<contexts>
<context position="6246" citStr="[LA87]" startWordPosition="982" endWordPosition="982">xample, they may engage in them based on the need for information to perform actions or to weigh options, the need to correct problems that arise during plan execution, or simply as the result of the normal decomposition of a task. My claim is that all subdialogues can be understood in terms of collaborations between agents, and thus modelled using SharedPlans. The type of plan does not vary with the type of subdialogue, or an agent&apos;s reason for engaging in it, only the object of that plan does2. The same methods used in our previous work to understand utterances concerned with &amp;quot;domain goals&amp;quot; [LA87], can also be used to understand utterances aimed at achieving other types of goals; the only difference is in the object of the recipes and SharedPlans used by the algorithms. A Rhetorical Relations Approach This approach contrasts sharply with the less goal-directed account of subdialogues given by Litman and Allen [Lit85, LA87]. In their model, the process of understanding an utterance entails recognizing a discourse plan from the utterance and then relating that discourse plan to some domain plan. While domain plans represent knowledge about a task, discourse plans represent knowledge abou</context>
</contexts>
<marker>[LA87]</marker>
<rawString>D.J. Litman and J.F. Allen. A plan recognition model for subdialogues in conversations. Cognitive Science, 11:163-200,1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Lambert</author>
<author>S Carberry</author>
</authors>
<title>A tripartite plan-based model of dialogue.</title>
<booktitle>In Proceedings of ACL-91.</booktitle>
<contexts>
<context position="10241" citStr="[LC91, LC92, Ram91]" startWordPosition="1614" endWordPosition="1616"> move it up? 5 System: Yes. &lt;system displays network&gt; (6 User: OK, now make... Figure 2: A Correction Subdialogue (taken from Litman[Lit85]) The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each </context>
</contexts>
<marker>[LC91]</marker>
<rawString>L. Lambert and S. Carberry. A tripartite plan-based model of dialogue. In Proceedings of ACL-91.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Lambert</author>
<author>S Carberry</author>
</authors>
<title>Modeling negotiation subdialogues.</title>
<booktitle>In Proceedings of ACL-92.</booktitle>
<contexts>
<context position="10241" citStr="[LC91, LC92, Ram91]" startWordPosition="1614" endWordPosition="1616"> move it up? 5 System: Yes. &lt;system displays network&gt; (6 User: OK, now make... Figure 2: A Correction Subdialogue (taken from Litman[Lit85]) The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each </context>
</contexts>
<marker>[LC92]</marker>
<rawString>L. Lambert and S. Carberry. Modeling negotiation subdialogues. In Proceedings of ACL-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Lochbaum</author>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Models of plans to support communication: An initial report.</title>
<date>1990</date>
<booktitle>In Proceedings of AAAI-90,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="1003" citStr="[GS90, LGS90, GK93]" startWordPosition="141" endWordPosition="143">ature of discourse cannot adequately participate in collaborative dialogues. The Approach: A SharedPlan Analysis of Subdialogues Evidence for the above claim comes from my recent work on understanding subdialogues in conversation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an instance of the more general phenomenon of collaboration. For agents to successfully collaborate on a task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93] specify the requisite components of the mental states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be understood as contributing information towards the establishment of the appropriate mental states, and thus the building of such a plan. The process by which utterances are understood has been formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and used to explain ut</context>
<context position="3372" citStr="[LGS90]" startWordPosition="501" endWordPosition="501">dominated segment was initiated by a conversational participantl. An Example The excerpt in Figure 1, taken from a larger discourse concerned with the replacement of an air compressor pump by an Expert and an Apprentice [GS86], will be used to illustrate the approach. Because the participants in this discourse are collaborating on the replacement of the pump, the first utterance of this excerpt can be understood as establishing mutual belief that the action described therein, i.e. the removal of the flywheel by the Apprentice, is part of the recipe the agents will use to accomplish their task [LGS90]. Utterance (2), however, begins a new discourse segment, the purpose of which is to bring it about that the Apprentice knows how to perform (i.e. has a recipe for) the act of removing the flywheel. This segment is recognized as a separate collaboration using a conversational default rule for recognizing an agent&apos;s desire to collaborate on el would like to thank Barbara Grosz, Christine Nakatani, and Candy Sidner for their comments on this paper. This research has been supported by U S West Advanced Technologies and by a Bellcore Graduate Fellowship. &apos;A detailed description of the model can be</context>
</contexts>
<marker>[LGS90]</marker>
<rawString>K. E. Lochbaum, B. J. Grosz, and C. L. Sidner. Models of plans to support communication: An initial report. In Proceedings of AAAI-90, Boston, MA, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
</authors>
<title>Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues.</title>
<date>1985</date>
<tech>PhD thesis,</tech>
<institution>University of Rochester,</institution>
<contexts>
<context position="6578" citStr="[Lit85, LA87]" startWordPosition="1033" endWordPosition="1034">, and thus modelled using SharedPlans. The type of plan does not vary with the type of subdialogue, or an agent&apos;s reason for engaging in it, only the object of that plan does2. The same methods used in our previous work to understand utterances concerned with &amp;quot;domain goals&amp;quot; [LA87], can also be used to understand utterances aimed at achieving other types of goals; the only difference is in the object of the recipes and SharedPlans used by the algorithms. A Rhetorical Relations Approach This approach contrasts sharply with the less goal-directed account of subdialogues given by Litman and Allen [Lit85, LA87]. In their model, the process of understanding an utterance entails recognizing a discourse plan from the utterance and then relating that discourse plan to some domain plan. While domain plans represent knowledge about a task, discourse plans represent knowledge about relationships between utterances and plans; for example, an agent may use an utterance to introduce, continue, or clarify a plan. Litman and Allen propose their discourse plans as plan-based correlates of rhetorical relations [Lit86, LA87]. Although these plans address some of the problems with computationally based RST [MT87] a</context>
<context position="9761" citStr="[Lit85]" startWordPosition="1544" endWordPosition="1544">ly and collectively are about, but, as used in a model of plan recognition, it constitutes a claim that the speaker (i.e. User) (i) produces utterances (3) and (4) intending to perform the above CORRECT-PLAN and IDENTIFY-PARAMETER actions respectively, and (ii) intends that the hearer (System) recognize these intentions. (1 User: Show me the generic concept called &amp;quot;employee&amp;quot;. (2 System: OK. &lt;system displays network&gt; 3 User: I can&apos;t fit a new ic below it. 4 Can you move it up? 5 System: Yes. &lt;system displays network&gt; (6 User: OK, now make... Figure 2: A Correction Subdialogue (taken from Litman[Lit85]) The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence f</context>
</contexts>
<marker>[Lit85]</marker>
<rawString>D. J. Litman. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues. PhD thesis, University of Rochester, 1985.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D J Litman</author>
</authors>
<title>Linguistic coherence: A plan-based alternative.</title>
<booktitle>In Proceedings of ACL-86.</booktitle>
<contexts>
<context position="7087" citStr="[Lit86, LA87]" startWordPosition="1109" endWordPosition="1110">asts sharply with the less goal-directed account of subdialogues given by Litman and Allen [Lit85, LA87]. In their model, the process of understanding an utterance entails recognizing a discourse plan from the utterance and then relating that discourse plan to some domain plan. While domain plans represent knowledge about a task, discourse plans represent knowledge about relationships between utterances and plans; for example, an agent may use an utterance to introduce, continue, or clarify a plan. Litman and Allen propose their discourse plans as plan-based correlates of rhetorical relations [Lit86, LA87]. Although these plans address some of the problems with computationally based RST [MT87] analyses (i.e. the formalization and recognition of rhetorical relations), they are extremely rigid in nature and narrow in scope. Each discourse plan (or more specifically the constraints of its decomposition) represents only one specific way in which an utterance can relate to a plan. For example, the only way something can go wrong and be corrected according to the CORRECT-PLAN discourse plan is if a 2By the object of the plan, I mean the act on which the agents are collaborating, i.e. the a in SharedP</context>
</contexts>
<marker>[Lit86]</marker>
<rawString>D. J. Litman. Linguistic coherence: A plan-based alternative. In Proceedings of ACL-86.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K E Lochbaum</author>
</authors>
<title>An algorithm for plan recognition in collaborative discourse.</title>
<booktitle>In Proceedings of ACL-91.</booktitle>
<contexts>
<context position="1664" citStr="[LGS90, Loc91]" startWordPosition="239" endWordPosition="240"> states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and intentions they must hold to have a SharedPlan, their utterances can be understood as contributing information towards the establishment of the appropriate mental states, and thus the building of such a plan. The process by which utterances are understood has been formalized in algorithms for augmenting the beliefs and intentions of an evolving SharedPlan, and used to explain utterances concerning the performance of actions [LGS90, Loc91]. My current work [Loc93], based on Grosz and Sidner&apos;s theory of discourse structure [GS86], provides a new approach to the problem of understanding subdialogues and their relationship to the discourse in which they are embedded. The basic approach entails treating each subdialogue or discourse segment as a separate collaboration between the conversational participants; each utterance of a subdialogue is understood as contributing some information towards the completion of a SharedPlan. Because subdialogues do not occur in isolation, each subdialogue itself is understood in terms of the role i</context>
</contexts>
<marker>[Loc91]</marker>
<rawString>K. E. Lochbaum. An algorithm for plan recognition in collaborative discourse. In Proceedings of ACL-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Lochbaum</author>
</authors>
<title>A collaborative planning approach to understanding subdialogues in conversation.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="621" citStr="[Loc93]" startWordPosition="83" endWordPosition="83">ntentionally-Based Approaches to Language* Karen E. Lochbaum Aiken Computation Lab Harvard University Cambridge, MA 02138 kelOdas.harvard.edu The Claim Discourses are inherently intentional; conversational participants engage in them for a reason. Systems for natural language interpretation and generation that do not account, with every utterance, for the purposeful nature of discourse cannot adequately participate in collaborative dialogues. The Approach: A SharedPlan Analysis of Subdialogues Evidence for the above claim comes from my recent work on understanding subdialogues in conversation [Loc93]. Following the work of Grosz and Sidner [GS90], I view discourse behavior as an instance of the more general phenomenon of collaboration. For agents to successfully collaborate on a task, they must hold certain beliefs and intentions regarding the acts they will perform to accomplish that task. The definitions of the SharedPlan model of collaborative activity [GS90, LGS90, GK93] specify the requisite components of the mental states of collaborating agents and consequently provide an important context for interpreting their utterances. In particular, because agents are aware of the beliefs and</context>
<context position="11691" citStr="[Loc93]" startWordPosition="1837" endWordPosition="1837">its utterances. In response to this problem, Moore and Paris have devised a planner that constructs text plans containing both intentional and rhetorical information. By recording these text plans as part of the dialogue history, their system is able to reason about its previous utterances in interpreting and responding to users&apos; follow-up questions. Conclusions Both the interpretation process and the generation process need intentionally-based approaches to language. In the former, a solely intentional approach provides a more general &apos;Analyses of all of these approaches can also be found in [Loc93]. 66 model for understanding subdialogues and their relationships. In the latter, intentional information augments RST information to allow more effective participation in explanation dialogues. Although rhetorical relations have proved useful in machine-based natural language generation (see Hovy&apos;s recent survey [Hov93]), their cognitive role remains unclear. Does a speaker actually have them &amp;quot;in mind&amp;quot; when he produces utterances? Or are they only &amp;quot;compilations&amp;quot; of intentional information that are computationally efficient for generation systems [MP9111 And if a speaker does have rhetorical r</context>
</contexts>
<marker>[Loc93]</marker>
<rawString>K. E. Lochbaum. A collaborative planning approach to understanding subdialogues in conversation. Technical report, Harvard University, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Languge Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<contexts>
<context position="10610" citStr="[Hov88, McK85]" startWordPosition="1673" endWordPosition="1674">rate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each component of the schema, nor the role that each component plays in satisfying the overall communicative goal associated with the schema. Without such information, a system cannot respond effectively if the hearer does not understand or accept its utterances. In response to this problem, Moore and Paris have devised a planner that constructs text plans containing both</context>
</contexts>
<marker>[McK85]</marker>
<rawString>K. R. McKeown. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Languge Text. Cambridge University Press, Cambridge, England, 1985.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Morgenstern</author>
</authors>
<title>Knowledge preconditions for actions and plans.</title>
<booktitle>In Proceedings of IJCAI-87.</booktitle>
<contexts>
<context position="5516" citStr="[Mor87]" startWordPosition="860" endWordPosition="860">ated to the previous SharedPlan. Because the Expert knows that part of having a SharedPlan is mutually believing that the agent of each act is able to perform that act, the Expert can infer that the Apprentice has engaged in the new SharedPlan in order to bring about an instance of that condition required by the previous or &amp;quot;interrupted&amp;quot; SharedPlan. In particular, because utterance (1) has established that removing the flywheel is part of the recipe for replacing the pump, the Expert can infer that the Apprentice has engaged in the subsidiary SharedPlan to bring about a knowledge precondition [Mor87] required of the dominating SharedPlan to replace the pump. Discussion Agents engage in subdialogues for many reasons. For example, they may engage in them based on the need for information to perform actions or to weigh options, the need to correct problems that arise during plan execution, or simply as the result of the normal decomposition of a task. My claim is that all subdialogues can be understood in terms of collaborations between agents, and thus modelled using SharedPlans. The type of plan does not vary with the type of subdialogue, or an agent&apos;s reason for engaging in it, only the o</context>
</contexts>
<marker>[Mor87]</marker>
<rawString>L. Morgenstern. Knowledge preconditions for actions and plans. In Proceedings of IJCAI-87.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J D Moore</author>
<author>C L Paris</author>
</authors>
<title>Discourse structure for explanatory dialogues.</title>
<booktitle>In AAAI Fall 1991 Symposium on Discourse Structure in Natural Language Understanding and Generation,</booktitle>
<location>Asilomar.</location>
<contexts>
<context position="10502" citStr="[MP91]" startWordPosition="1656" endWordPosition="1656">-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each component of the schema, nor the role that each component plays in satisfying the overall communicative goal associated with the schema. Without such information, a system cannot respond effectively if the hearer does not understand or accept its utterances. In</context>
</contexts>
<marker>[MP91]</marker>
<rawString>J. D. Moore and C. L. Paris. Discourse structure for explanatory dialogues. In AAAI Fall 1991 Symposium on Discourse Structure in Natural Language Understanding and Generation, Asilomar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Moore</author>
<author>M E Pollack</author>
</authors>
<title>A problem for RST: The need for multi-level discourse analysis.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>[MP92]</marker>
<rawString>J.D. Moore and M.E. Pollack. A problem for RST: The need for multi-level discourse analysis. Computational Linguistics, 18(4), December 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<booktitle>The Structure of Discourse.</booktitle>
<editor>In L. Polanyi, editor,</editor>
<publisher>Ablex Publishing Corp.,</publisher>
<contexts>
<context position="7176" citStr="[MT87]" startWordPosition="1123" endWordPosition="1123">5, LA87]. In their model, the process of understanding an utterance entails recognizing a discourse plan from the utterance and then relating that discourse plan to some domain plan. While domain plans represent knowledge about a task, discourse plans represent knowledge about relationships between utterances and plans; for example, an agent may use an utterance to introduce, continue, or clarify a plan. Litman and Allen propose their discourse plans as plan-based correlates of rhetorical relations [Lit86, LA87]. Although these plans address some of the problems with computationally based RST [MT87] analyses (i.e. the formalization and recognition of rhetorical relations), they are extremely rigid in nature and narrow in scope. Each discourse plan (or more specifically the constraints of its decomposition) represents only one specific way in which an utterance can relate to a plan. For example, the only way something can go wrong and be corrected according to the CORRECT-PLAN discourse plan is if a 2By the object of the plan, I mean the act on which the agents are collaborating, i.e. the a in SharedPlan({GI, G2J,a,TI,T2)• 65 speaker, not being able to perform the next step in a plan, req</context>
</contexts>
<marker>[MT87]</marker>
<rawString>W.C. Mann and S.A. Thompson. Rhetorical structure theory: A theory of text organization. In L. Polanyi, editor, The Structure of Discourse. Ablex Publishing Corp., 1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L A Ramshaw</author>
</authors>
<title>A three-level model for plan exploration.</title>
<booktitle>In Proceedings of ACL-91.</booktitle>
<contexts>
<context position="10241" citStr="[LC91, LC92, Ram91]" startWordPosition="1614" endWordPosition="1616"> move it up? 5 System: Yes. &lt;system displays network&gt; (6 User: OK, now make... Figure 2: A Correction Subdialogue (taken from Litman[Lit85]) The problem with Litman and Allen&apos;s approach, like RST-based approaches in general, is that it essentially provides only an utterance-to-utterance based analysis of discourse. In addition to not recognizing discourse segments as separate units with an overall purpose, the model also fails to recognize a subdialogue&apos;s relationship to the discourse in which it is embedded. That is, it cannot account for why agents engage in subdialogues. More recent models [LC91, LC92, Ram91] that augment Litman and Allen&apos;s two types of plans with other types also suffer from the same shortcomings3. Evidence from Generation Work in generation has recognized a similar problem with respect to RST-based approaches. In particular, Moore St Paris [MP91] (see also [MP92, Hov931) have argued for the need to augment RST-based text plans or schemas [Hov88, McK85] with an intentional structure in order to respond to follow-up questions. The problem is that although solely RST-based approaches associate a communicative goal with each schema, they do not represent the intended effect of each </context>
</contexts>
<marker>[Ram91]</marker>
<rawString>L. A. Ramshaw. A three-level model for plan exploration. In Proceedings of ACL-91.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C L Sidner</author>
<author>D J Israel</author>
</authors>
<title>Recognizing intended meaning and speakers&apos; plans.</title>
<booktitle>In Proceedings of JC A</booktitle>
<pages>1--81</pages>
<contexts>
<context position="2752" citStr="[SI81]" startWordPosition="399" endWordPosition="399">aredPlan. Because subdialogues do not occur in isolation, each subdialogue itself is understood in terms of the role its corresponding SharedPlan plays in satisfying the other SharedPlans underlying the dialogue. In particular, if the completion of the new SharedPlan contributes to the establishment of one of the beliefs or intentions necessary for the completion of another, then the first SharedPlan is said to be subsidiary to the second. A subsidiary relationship between SharedPlans corresponds to a dominance relationship [GS86] between discourse segment purposes; it provides an explanation [SI81] for why the dominated segment was initiated by a conversational participantl. An Example The excerpt in Figure 1, taken from a larger discourse concerned with the replacement of an air compressor pump by an Expert and an Apprentice [GS86], will be used to illustrate the approach. Because the participants in this discourse are collaborating on the replacement of the pump, the first utterance of this excerpt can be understood as establishing mutual belief that the action described therein, i.e. the removal of the flywheel by the Apprentice, is part of the recipe the agents will use to accomplis</context>
</contexts>
<marker>[SI81]</marker>
<rawString>C.L. Sidner and D. J. Israel. Recognizing intended meaning and speakers&apos; plans. In Proceedings of JC A 1-81.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>