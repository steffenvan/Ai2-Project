<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003269">
<title confidence="0.9873535">
Zhijun Wu: Chinese Semantic Dependency Parsing with Third-Order
Features
</title>
<author confidence="0.999844">
Zhijun Wu Xuan Wang Xinxin Li
</author>
<affiliation confidence="0.998351333333333">
Computer Application Research Center
School of Computer Science and Technology
Harbin Institute of Technology Shenzhen Graduate School
</affiliation>
<address confidence="0.683435">
Shenzhen 518055, P.R.China
</address>
<email confidence="0.988998">
mattwu305@gmail.com wangxuan@insun.hit.edu.cn lixin2@gmail.com
</email>
<sectionHeader confidence="0.997679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.8601985">
This paper presents our system participated on
SemEval-2012 task: Chinese Semantic De-
pendency Parsing. Our system extends the
second-order MST model by adding two
third-order features. The two third-order fea-
tures are grand-sibling and tri-sibling. In the
decoding phase, we keep the k best results for
each span. After using the selected third-order
features, our system presently achieves LAS
of 61.58% ignoring punctuation tokens which
is 0.15% higher than the result of purely
second-order model on the test dataset.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998794322580645">
Recently, semantic role labeling (SRL) has been a
hot research topic. CoNLL shared tasks for joint
parsing for syntactic and semantic dependencies
both in the year 2008 and 2009, cf. (Surdeanu et al.,
2008; Hajič et al., 2009; Bohnet, 2009). Same
shared tasks in SemEval-2007 (Sameer S., 2007).
The SRL is traditionally implemented as two sub-
tasks, argument identification and classification.
However, there are some problems for the seman-
tic representation method used by the semantic role
labeling. For example, the SRL only considers the
predicate-argument relations and ignores the rela-
tions between a noun and its modifier, the meaning
of semantic roles is related with special predicates.
In order to overcome those problems, semantic
dependency parsing (SDP) is introduced. Semantic
dependencies express semantic links between pre-
dicates and arguments and represent relations be-
tween entities and events in text. The SDP is a kind
of dependency parsing, and its task is to build a
dependency structure for an input sentence and to
label the semantic relation between a word and its
head. However, semantic relations are different
from syntactic relations, such as position indepen-
dent. Table 1 shows the position independent of
semantic relations for the sentence XiaoMing hit
XiaoBai with a book today.
Today, XiaoMing hit XiaoBai with a book.
XiaoBai was hit by XiaoMing today with a book.
With a book, XiaoMing hit XiaoBai today.
XiaoMing hit XiaoBai with a book today.
</bodyText>
<tableCaption confidence="0.998075">
Table 1: An example position dependency
</tableCaption>
<bodyText confidence="0.9994359">
Although semantic relations are different from
syntactic relations, yet they are identical in the de-
pendency tree. That means the methods used in
syntactic dependency parsing can also be applied
in SDP.
Two main approaches to syntactic dependency
paring are Maximum Spanning Tree (MST) based
dependency parsing and Transition based depen-
dency parsing (Eisner, 1996; Nivre et al., 2004;
McDonald and Pereira, 2006). The main idea of
</bodyText>
<page confidence="0.969212">
430
</page>
<note confidence="0.526331">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430–434,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999835111111111">
MSTParser is to take dependency parsing as a
problem of searching a maximum spanning tree
(MST) in a directed graph (Dependency Tree). We
see MSTParser a better chance to improve the
parsing speed and MSTParser provides the state-
of-the-art performance for both projective and non-
projective tree banks. For the reasons above, we
choose MSTParser as our SemEval-2012 shared
task participating system basic framework.
</bodyText>
<sectionHeader confidence="0.928324" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999951555555555">
Our parser is based on the projective MSTParser
using all the features described by (McDonald et
al., 2006) as well as some third-order features de-
scribed in the following sections. Semantic depen-
dency paring is introduced in Section 3. We
explain the reasons why we choose projective
MSTParser in Section 4 which also contains the
experiment result analysis in various conditions.
Section 5 gives our conclusion and future work.
</bodyText>
<sectionHeader confidence="0.924717" genericHeader="method">
3 Semantic Dependency parsers
3.1 First-Order Model
</sectionHeader>
<bodyText confidence="0.9997904">
Dependency tree parsing as the search for the max-
imum spanning tree in a directed graph was pro-
posed by McDonald et al. (2005c). This
formulation leads to efficient parsing algorithms
for both projective and non-projective dependency
trees with the Eisner algorithm (Eisner, 1996) and
the Chu-Liu-Edmonds algorithm (Chu and Liu,
1965; Edmonds, 1967) respectively. The formula-
tion works by defining in McDonald et al (2005a).
The score of a dependency tree y for sentence x is
</bodyText>
<equation confidence="0.99915">
S (x ,Y ) = ∑ S o,J)=∑w f u,j)
( Q)∈ Y
</equation>
<bodyText confidence="0.994023">
f(i, j) is a multidimensional feature vector repre-
sentation of the edge from node i to node j. We set
the value of f(i, j) as 1 if there an edge from node i
to node j. w is the corresponding weight vector
between the two nodes that will be learned during
training. Hence, finding a dependency tree with
highest score is equivalent to finding a maximum
spanning tree. Obviously, the scores are restricted
to a single edge in the dependency tree, thus we
call this first-order dependency parsing. This is a
standard linear classifier. The features used in the
first-order dependency parser are based on those
listed in (Johansson, 2008). Table 2 shows the fea-
tures we choose in the first-order parsing. We use
some shorthand notations in order to simplify the
feature representations: h is the abbreviation for
head, d for dependent, s for nearby nodes (may not
be siblings), f for form, le for the lemmas, pos for
part-of-speech tags, dir for direction, dis for dis-
tance, ‘+1’ and ‘-1’ for right and left position re-
spectively. Additional features are built by adding
the direction and the distance plus the direction.
The direction is left if the dependent is left to its
head otherwise right. The distance is the number of
words minus one between the head and the depen-
dent in a certain sentence, if ≤ 5, 5 if &gt; 5, 10 if &gt;
10. ◎ means that previous part is built once and
the additional part follow ◎ together with the pre-
vious part is built again.
</bodyText>
<table confidence="0.877988076923077">
Head and Dependent
h-f, h-l, d-pos ◎dir(h, d) ◎dis(h, d)
h-l, h-pos, d-f ◎dir(h, d) ◎dis(h, d)
h-pos, h-f, d-l ◎dir(h, d) ◎dis(h, d)
h-f, d-l, d-pos ◎dir(h, d) ◎dis(h, d)
h-f, d-f, d-l ◎dir(h, d) ◎dis(h, d)
h-f, h-l, d-f, d-l ◎dir(h, d) ◎dis(h, d)
h-f, h-l, d-f, d-pos ◎dir(h, d) ◎dis(h, d)
h-f, h-pos, d-f, d-pos ◎dir(h, d) ◎dis(h, d)
h-l, h-pos, d-l, d-pos ◎dir(h, d) ◎dis(h, d)
Dependent and Nearby
d-pos-1, d-pos, s-pos ◎dir(d, s) ◎dis(d, s)
d-pos-1, s-pos, s-pos+1 ◎dir(d, s) ◎dis(d, s)
d-pos-1, d-pos, s-pos+1 ◎dir(d, s) ◎dis(d, s)
d-pos, s-pos, s-pos+1 ◎dir(d, s) ◎dis(d, s)
d-pos, d-pos+1, s-pos-1 ◎dir(d, s) ◎dis(d, s)
d-pos-1, d-pos, s-pos-1 ◎dir(d, s) ◎dis(d, s)
d-pos, d-pos+1, s-pos ◎dir(d, s) ◎dis(d, s)
d-pos, s-pos-1, s-pos ◎dir(d, s) ◎dis(d, s)
d-pos+1, s-pos-1, s-pos ◎dir(d, s) ◎dis(d, s)
d-pos-1, d-pos, s-pos-1, s-pos ◎ dir(d, s) ◎
dis(d, s)
d-pos, d-pos+1, s-pos-1, s-pos ◎dir(d, s) ◎
dis(d, s)
d-pos-1, d-pos, s-pos, s-pos+1 ◎dir(d, s) ◎
dis(d, s)
</table>
<tableCaption confidence="0.998819">
Table 2: Selected features in first order parsing
</tableCaption>
<page confidence="0.996605">
431
</page>
<subsectionHeader confidence="0.772087">
3.2 Second-Order Model
</subsectionHeader>
<bodyText confidence="0.999904545454545">
A second order model proposed by McDonald
(McDonald and Pereira, 2006) alleviates some of
the first order factorization limitations. Because the
first order parsing restricts scores to a single edge
in a dependency tree, the procedure is sufficient.
However, in the second order parsing scenario
where more than one edge are considered by the
parsing algorithm, combinations of two edges
might be more accurate which will be described in
the Section 4. The second-order parsing can be
defined as below:
</bodyText>
<equation confidence="0.972312333333333">
= E s i k j
( , , )
( Q) y
</equation>
<bodyText confidence="0.999627181818182">
where k and j are adjacent, same-side children of i
in the tree y. The shortcoming of this definition is
that it restricts i on the same side of its sibling. In
our system, we extend this restriction by adding
the feature that as long as i is another child of k or j.
In that case, i may be the child or grandchild of k
or j which is shown in Figure 1.
second-order parsing, the features selected are
shown in Table 3. We divide the dependency dis-
tance into six parts which are 1 if &gt; 1, 2 if &gt; 2, ... ,
5 if &gt; 5, 10 if &gt; 10.
</bodyText>
<subsectionHeader confidence="0.990096">
3.3 Third-Order Features
</subsectionHeader>
<bodyText confidence="0.999812727272727">
The order of parsing is defined according to the
number of dependencies it contains (Koo and Col-
lins, 2010). Collins classifies the third-order as two
models, Model 1 is all grand-siblings, and Model 2
is grand-siblings and tri-siblings. A grand-sibling
is a 4-tuple of indices (g, h, m, s) where g is grand-
father. (h, m, s) is a sibling part and (g, h, m) is a
grandchild part as well as (g, h, s). A tri-sibling
part is also a 4-tuple of indices (h, m, s, t). Both (h,
m, s) and (h, s, t) are siblings. Figure 2 clearly
shows these relations.
</bodyText>
<figure confidence="0.8981645">
g h s m ,h t s m
s(x,y)
</figure>
<figureCaption confidence="0.987054">
Figure 2: Grand-siblings and tri-siblings dependency.
</figureCaption>
<figure confidence="0.393226">
k i j , k i j
</figure>
<figureCaption confidence="0.999725">
Figure 1: Sibling and grand-child relations.
</figureCaption>
<table confidence="0.985893923076923">
Siblings
c1-pos, c2-posOdir(c1, c2)Odis(c1, c2)
c1-f, c2-fOdir(c1, c2)
c1-f, c2-posOdir(c1, c2)
c1-pos, c2-fOdir(c1, c2)
Parent and Two Children
p-pos, c1-pos, c2-posOdir(c1, c2)Odis(c1, c2)
p-f, c1-pos, c2-posOdir(c1, c2)Odis(c1, c2)
p-f, c1-f, c2-posOdir(c1, c2) Odis(c1, c2)
p-f, c1-f, c2-f Odir(c1, c2) Odis(c1, c2)
p-pos, c1-f, c2-fOdir(c1, c2) Odis(c1, c2)
p-pos, c1-f, c2-posOdir(c1, c2) Odis(c1, c2)
p-pos, c1-pos, c2-fOdir(c1, c2) Odis(c1, c2)
</table>
<tableCaption confidence="0.999823">
Table 3: Selected features in second-order parsing
</tableCaption>
<bodyText confidence="0.974539666666667">
Shorthand notations are almost the same with the
Section 3.1 except for that we use c1 and c2 to
represent the two children and p for parent. In
Collins and Koo implement an efficient third-
order dependency parsing algorithm, but still time
consuming compared with the second-order
(McDonald, 2006). For that reason, we only add
third-order relation features into our system instead
of implementing the third-order dependency pars-
ing model. These features shown in Table 4 are
grand-sibling and tri-sibling described above.
Shorthand notations are almost the same with the
Section 3.1 and 3.2 except that we use c3 for the
third sibling and g represent the grandfather. We
attempt to add features of words form and parts-of-
speech as well as directions into our system, which
is used both in first-order and second-order as fea-
tures, but result shows that these decrease the sys-
tem performance.
Tri-Sibling
c1-pos, c2-pos, c3-posOdir(c1, c2)
</bodyText>
<subsectionHeader confidence="0.645403">
Grandfather and Two Children
</subsectionHeader>
<bodyText confidence="0.8318505">
g-pos, c1-pos, c2-posOdir(c1, c2)
g-pos, p-pos, c1-pos, c2-posOdir(c1, c2)
</bodyText>
<tableCaption confidence="0.995784">
Table 4: Third-order features.
</tableCaption>
<page confidence="0.997921">
432
</page>
<sectionHeader confidence="0.984463" genericHeader="evaluation">
4 Experiment result analysis
</sectionHeader>
<bodyText confidence="0.99996962962963">
As we all know that projective dependency parsing
using edge based factorization can be processed by
the Einster algorithm (Einster, 1996). The corpus
given by SemEval-2012 is consists of 10000 sen-
tences converting into dependency structures from
Chinese Penn Treebank randomly. We find that
none of non-projective sentence existing by testing
the 8301 sentences in training data. For this reason,
we set the MSTParser into projective parsing mode.
We perform a number of experiments where we
compare the first-order, second-order and second-
order by adding third-order features proposed in
the previous sections. We train the model on the
full training set which contains 8301 sentences to-
tally. We use 10 training iterations and projective
decoding in the experiments. Experimental results
show that 10 training iterations are better than oth-
ers. After adjusting the features of third-order, our
best result reaches the labeled attachment score of
62.48% on the developing dataset which ignores
punctuation. We submitted our currently best result
to SemEval-2012 which is 61.58% on the test data-
set. The results in Table 5 show that by adding
third-order features to second-order model, we im-
prove the dependency parsing accuracies by 1.21%
comparing to first-order model and 0.15% compar-
ing to second-order model.
</bodyText>
<table confidence="0.99248725">
Models LAS UAS
First-Order 61.26 80.18
Second-Order 62.33 81.40
Second-Order+ 62.48 81.43
</table>
<tableCaption confidence="0.9748064">
Table 5: Experimental results. Second-Order+ means
second-order model by adding third-order features.
Results are tested under the developping dataset which
contains the heads and semantic relations given by
organizer.
</tableCaption>
<sectionHeader confidence="0.99654" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99996775">
In this paper, we have presented the semantic de-
pendency parsing and shown it works on the first-
order model, second-order model and second-order
model by adding third-order features. Our experi-
mental results show more significant improve-
ments than the conventional approaches of third-
order model.
In the future, we firstly plan to implement the
third-order model by adding higher-order features,
such as forth-order features. We have found that
both in the first-order and second-order model of
MSTParser, words form and lemmas are recog-
nized as two different features. These features are
essential in languages that have different grid,
however, which are the same in Chinese in the giv-
en dataset. Things are the same in POS (part-of-
speech tags) and CPOS (fine-grid POS) which are
viewed as different features. For the applications of
syntactic and semantic parsing, the parsing time
and memory footprint are very important. There-
fore, secondly, we decide to remove these repeated
features in order to reduce to training time as well
as the space if it does not lower the system perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99641">
The authors would like to thank the reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995971875">
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Márquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic andse-
mantic dependencies. In Proceedings of the 12th
CoNLL-2008.
Jan Hajič, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martí, Llu’is
Márquez, Adam Meyers, Joakim Nivre, SebastianPa-
do, Jan Štepánek, Pavel Stranák, Miahi Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of the
13th CoNLL-2009, June 4-5, Boulder, Colorado,
USA.
Bohnet, Bernd. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of
CoNLL-09.
Ryan McDonald. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Al-
grithms. In In Proc. of EACL, pages 81–88.
Ryan. McDonald, K. Crammer, and F. Pereira. 2005a.
Online large-margin training of dependency parsers.
In Proc. of the 43rd Annual Meeting of the ACL.
Ryan. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc.
2005c. Non-projective dependency parsing using
spanning tree algorithms. In Proc. HLT-EMNLP.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
</reference>
<page confidence="0.992148">
433
</page>
<reference confidence="0.999759846153846">
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of the 16th International Conference on Computa-
tional Linguistics (COLING-96), pages 340–345,
Copenhaen.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In Proceedings
of the 8th CoNLL, pages 49–56, Boston, Massachu-
setts.
Terry Koo, Michael Collins. 2010. Efficient Third-order
Dependency Parsers. Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1–11.
</reference>
<page confidence="0.999111">
434
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.786289">
<title confidence="0.9980295">Zhijun Wu: Chinese Semantic Dependency Parsing with Third-Order Features</title>
<author confidence="0.999144">Zhijun Wu Xuan Wang Xinxin Li</author>
<affiliation confidence="0.997755333333333">Computer Application Research Center School of Computer Science and Technology Harbin Institute of Technology Shenzhen Graduate School</affiliation>
<address confidence="0.849224">Shenzhen 518055, P.R.China</address>
<email confidence="0.907553">mattwu305@gmail.comwangxuan@insun.hit.edu.cnlixin2@gmail.com</email>
<abstract confidence="0.998201230769231">This paper presents our system participated on task: Semantic De- Our system extends the second-order MST model by adding two third-order features. The two third-order features are grand-sibling and tri-sibling. In the decoding phase, we keep the k best results for each span. After using the selected third-order features, our system presently achieves LAS of 61.58% ignoring punctuation tokens which is 0.15% higher than the result of purely second-order model on the test dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Márquez</author>
<author>JoakimNivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic andsemantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th CoNLL-2008.</booktitle>
<contexts>
<context position="1078" citStr="Surdeanu et al., 2008" startWordPosition="150" endWordPosition="153">s the second-order MST model by adding two third-order features. The two third-order features are grand-sibling and tri-sibling. In the decoding phase, we keep the k best results for each span. After using the selected third-order features, our system presently achieves LAS of 61.58% ignoring punctuation tokens which is 0.15% higher than the result of purely second-order model on the test dataset. 1 Introduction Recently, semantic role labeling (SRL) has been a hot research topic. CoNLL shared tasks for joint parsing for syntactic and semantic dependencies both in the year 2008 and 2009, cf. (Surdeanu et al., 2008; Hajič et al., 2009; Bohnet, 2009). Same shared tasks in SemEval-2007 (Sameer S., 2007). The SRL is traditionally implemented as two subtasks, argument identification and classification. However, there are some problems for the semantic representation method used by the semantic role labeling. For example, the SRL only considers the predicate-argument relations and ignores the relations between a noun and its modifier, the meaning of semantic roles is related with special predicates. In order to overcome those problems, semantic dependency parsing (SDP) is introduced. Semantic dependencies ex</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Márquez, JoakimNivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Márquez, and JoakimNivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic andsemantic dependencies. In Proceedings of the 12th CoNLL-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajič</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Antonia Martí</author>
<author>Llu’is Márquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Jan Štepánek SebastianPado</author>
<author>Pavel Stranák</author>
<author>Miahi Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<date>2009</date>
<booktitle>The CoNLL2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Proceedings of the 13th CoNLL-2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="1098" citStr="Hajič et al., 2009" startWordPosition="154" endWordPosition="157">model by adding two third-order features. The two third-order features are grand-sibling and tri-sibling. In the decoding phase, we keep the k best results for each span. After using the selected third-order features, our system presently achieves LAS of 61.58% ignoring punctuation tokens which is 0.15% higher than the result of purely second-order model on the test dataset. 1 Introduction Recently, semantic role labeling (SRL) has been a hot research topic. CoNLL shared tasks for joint parsing for syntactic and semantic dependencies both in the year 2008 and 2009, cf. (Surdeanu et al., 2008; Hajič et al., 2009; Bohnet, 2009). Same shared tasks in SemEval-2007 (Sameer S., 2007). The SRL is traditionally implemented as two subtasks, argument identification and classification. However, there are some problems for the semantic representation method used by the semantic role labeling. For example, the SRL only considers the predicate-argument relations and ignores the relations between a noun and its modifier, the meaning of semantic roles is related with special predicates. In order to overcome those problems, semantic dependency parsing (SDP) is introduced. Semantic dependencies express semantic links</context>
</contexts>
<marker>Hajič, Ciaramita, Johansson, Kawahara, Martí, Márquez, Meyers, Nivre, SebastianPado, Stranák, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajič, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Martí, Llu’is Márquez, Adam Meyers, Joakim Nivre, SebastianPado, Jan Štepánek, Pavel Stranák, Miahi Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Proceedings of the 13th CoNLL-2009, June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Efficient parsing of syntactic and semantic dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-09.</booktitle>
<contexts>
<context position="1113" citStr="Bohnet, 2009" startWordPosition="158" endWordPosition="159">third-order features. The two third-order features are grand-sibling and tri-sibling. In the decoding phase, we keep the k best results for each span. After using the selected third-order features, our system presently achieves LAS of 61.58% ignoring punctuation tokens which is 0.15% higher than the result of purely second-order model on the test dataset. 1 Introduction Recently, semantic role labeling (SRL) has been a hot research topic. CoNLL shared tasks for joint parsing for syntactic and semantic dependencies both in the year 2008 and 2009, cf. (Surdeanu et al., 2008; Hajič et al., 2009; Bohnet, 2009). Same shared tasks in SemEval-2007 (Sameer S., 2007). The SRL is traditionally implemented as two subtasks, argument identification and classification. However, there are some problems for the semantic representation method used by the semantic role labeling. For example, the SRL only considers the predicate-argument relations and ignores the relations between a noun and its modifier, the meaning of semantic roles is related with special predicates. In order to overcome those problems, semantic dependency parsing (SDP) is introduced. Semantic dependencies express semantic links between predic</context>
</contexts>
<marker>Bohnet, 2009</marker>
<rawString>Bohnet, Bernd. 2009. Efficient parsing of syntactic and semantic dependency structures. In Proceedings of CoNLL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="9494" citStr="McDonald, 2006" startWordPosition="1575" endWordPosition="1576"> c2) p-f, c1-pos, c2-posOdir(c1, c2)Odis(c1, c2) p-f, c1-f, c2-posOdir(c1, c2) Odis(c1, c2) p-f, c1-f, c2-f Odir(c1, c2) Odis(c1, c2) p-pos, c1-f, c2-fOdir(c1, c2) Odis(c1, c2) p-pos, c1-f, c2-posOdir(c1, c2) Odis(c1, c2) p-pos, c1-pos, c2-fOdir(c1, c2) Odis(c1, c2) Table 3: Selected features in second-order parsing Shorthand notations are almost the same with the Section 3.1 except for that we use c1 and c2 to represent the two children and p for parent. In Collins and Koo implement an efficient thirdorder dependency parsing algorithm, but still time consuming compared with the second-order (McDonald, 2006). For that reason, we only add third-order relation features into our system instead of implementing the third-order dependency parsing model. These features shown in Table 4 are grand-sibling and tri-sibling described above. Shorthand notations are almost the same with the Section 3.1 and 3.2 except that we use c3 for the third sibling and g represent the grandfather. We attempt to add features of words form and parts-ofspeech as well as directions into our system, which is used both in first-order and second-order as features, but result shows that these decrease the system performance. Tri-</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Learning of Approximate Dependency Parsing Algrithms. In</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2806" citStr="McDonald and Pereira, 2006" startWordPosition="420" endWordPosition="423">day. Today, XiaoMing hit XiaoBai with a book. XiaoBai was hit by XiaoMing today with a book. With a book, XiaoMing hit XiaoBai today. XiaoMing hit XiaoBai with a book today. Table 1: An example position dependency Although semantic relations are different from syntactic relations, yet they are identical in the dependency tree. That means the methods used in syntactic dependency parsing can also be applied in SDP. Two main approaches to syntactic dependency paring are Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). The main idea of 430 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430–434, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics MSTParser is to take dependency parsing as a problem of searching a maximum spanning tree (MST) in a directed graph (Dependency Tree). We see MSTParser a better chance to improve the parsing speed and MSTParser provides the stateof-the-art performance for both projective and nonprojective tree banks. For the reasons above, we choose MSTParser as our SemEval-2012 shared task participating system basic fram</context>
<context position="7000" citStr="McDonald and Pereira, 2006" startWordPosition="1128" endWordPosition="1131"> d-pos-1, d-pos, s-pos+1 ◎dir(d, s) ◎dis(d, s) d-pos, s-pos, s-pos+1 ◎dir(d, s) ◎dis(d, s) d-pos, d-pos+1, s-pos-1 ◎dir(d, s) ◎dis(d, s) d-pos-1, d-pos, s-pos-1 ◎dir(d, s) ◎dis(d, s) d-pos, d-pos+1, s-pos ◎dir(d, s) ◎dis(d, s) d-pos, s-pos-1, s-pos ◎dir(d, s) ◎dis(d, s) d-pos+1, s-pos-1, s-pos ◎dir(d, s) ◎dis(d, s) d-pos-1, d-pos, s-pos-1, s-pos ◎ dir(d, s) ◎ dis(d, s) d-pos, d-pos+1, s-pos-1, s-pos ◎dir(d, s) ◎ dis(d, s) d-pos-1, d-pos, s-pos, s-pos+1 ◎dir(d, s) ◎ dis(d, s) Table 2: Selected features in first order parsing 431 3.2 Second-Order Model A second order model proposed by McDonald (McDonald and Pereira, 2006) alleviates some of the first order factorization limitations. Because the first order parsing restricts scores to a single edge in a dependency tree, the procedure is sufficient. However, in the second order parsing scenario where more than one edge are considered by the parsing algorithm, combinations of two edges might be more accurate which will be described in the Section 4. The second-order parsing can be defined as below: = E s i k j ( , , ) ( Q) y where k and j are adjacent, same-side children of i in the tree y. The shortcoming of this definition is that it restricts i on the same sid</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online Learning of Approximate Dependency Parsing Algrithms. In In Proc. of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the ACL.</booktitle>
<marker>McDonald, Pereira, 2005</marker>
<rawString>Ryan. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. of the 43rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira McDonald</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP.</booktitle>
<marker>McDonald, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005c. Non-projective dependency parsing using spanning tree algorithms. In Proc. HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Dependency-based Semantic Analysis of Natural-language Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Lund University.</institution>
<contexts>
<context position="5066" citStr="Johansson, 2008" startWordPosition="792" endWordPosition="793">dimensional feature vector representation of the edge from node i to node j. We set the value of f(i, j) as 1 if there an edge from node i to node j. w is the corresponding weight vector between the two nodes that will be learned during training. Hence, finding a dependency tree with highest score is equivalent to finding a maximum spanning tree. Obviously, the scores are restricted to a single edge in the dependency tree, thus we call this first-order dependency parsing. This is a standard linear classifier. The features used in the first-order dependency parser are based on those listed in (Johansson, 2008). Table 2 shows the features we choose in the first-order parsing. We use some shorthand notations in order to simplify the feature representations: h is the abbreviation for head, d for dependent, s for nearby nodes (may not be siblings), f for form, le for the lemmas, pos for part-of-speech tags, dir for direction, dis for distance, ‘+1’ and ‘-1’ for right and left position respectively. Additional features are built by adding the direction and the distance plus the direction. The direction is left if the dependent is left to its head otherwise right. The distance is the number of words minu</context>
</contexts>
<marker>Johansson, 2008</marker>
<rawString>Richard Johansson. 2008. Dependency-based Semantic Analysis of Natural-language Text. Ph.D. thesis, Lund University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="2757" citStr="Eisner, 1996" startWordPosition="414" endWordPosition="415">iaoMing hit XiaoBai with a book today. Today, XiaoMing hit XiaoBai with a book. XiaoBai was hit by XiaoMing today with a book. With a book, XiaoMing hit XiaoBai today. XiaoMing hit XiaoBai with a book today. Table 1: An example position dependency Although semantic relations are different from syntactic relations, yet they are identical in the dependency tree. That means the methods used in syntactic dependency parsing can also be applied in SDP. Two main approaches to syntactic dependency paring are Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). The main idea of 430 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430–434, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics MSTParser is to take dependency parsing as a problem of searching a maximum spanning tree (MST) in a directed graph (Dependency Tree). We see MSTParser a better chance to improve the parsing speed and MSTParser provides the stateof-the-art performance for both projective and nonprojective tree banks. For the reasons above, we choose MSTParser as our SemEval</context>
<context position="4197" citStr="Eisner, 1996" startWordPosition="635" endWordPosition="636">ribed in the following sections. Semantic dependency paring is introduced in Section 3. We explain the reasons why we choose projective MSTParser in Section 4 which also contains the experiment result analysis in various conditions. Section 5 gives our conclusion and future work. 3 Semantic Dependency parsers 3.1 First-Order Model Dependency tree parsing as the search for the maximum spanning tree in a directed graph was proposed by McDonald et al. (2005c). This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) respectively. The formulation works by defining in McDonald et al (2005a). The score of a dependency tree y for sentence x is S (x ,Y ) = ∑ S o,J)=∑w f u,j) ( Q)∈ Y f(i, j) is a multidimensional feature vector representation of the edge from node i to node j. We set the value of f(i, j) as 1 if there an edge from node i to node j. w is the corresponding weight vector between the two nodes that will be learned during training. Hence, finding a dependency tree with highest score is equivalent to finding a maximum spanning tree</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340–345, Copenhaen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-Based Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th CoNLL,</booktitle>
<pages>49--56</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="2777" citStr="Nivre et al., 2004" startWordPosition="416" endWordPosition="419">aoBai with a book today. Today, XiaoMing hit XiaoBai with a book. XiaoBai was hit by XiaoMing today with a book. With a book, XiaoMing hit XiaoBai today. XiaoMing hit XiaoBai with a book today. Table 1: An example position dependency Although semantic relations are different from syntactic relations, yet they are identical in the dependency tree. That means the methods used in syntactic dependency parsing can also be applied in SDP. Two main approaches to syntactic dependency paring are Maximum Spanning Tree (MST) based dependency parsing and Transition based dependency parsing (Eisner, 1996; Nivre et al., 2004; McDonald and Pereira, 2006). The main idea of 430 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430–434, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics MSTParser is to take dependency parsing as a problem of searching a maximum spanning tree (MST) in a directed graph (Dependency Tree). We see MSTParser a better chance to improve the parsing speed and MSTParser provides the stateof-the-art performance for both projective and nonprojective tree banks. For the reasons above, we choose MSTParser as our SemEval-2012 shared task pa</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-Based Dependency Parsing. In Proceedings of the 8th CoNLL, pages 49–56, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient Third-order Dependency Parsers.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="8118" citStr="Koo and Collins, 2010" startWordPosition="1344" endWordPosition="1348">e children of i in the tree y. The shortcoming of this definition is that it restricts i on the same side of its sibling. In our system, we extend this restriction by adding the feature that as long as i is another child of k or j. In that case, i may be the child or grandchild of k or j which is shown in Figure 1. second-order parsing, the features selected are shown in Table 3. We divide the dependency distance into six parts which are 1 if &gt; 1, 2 if &gt; 2, ... , 5 if &gt; 5, 10 if &gt; 10. 3.3 Third-Order Features The order of parsing is defined according to the number of dependencies it contains (Koo and Collins, 2010). Collins classifies the third-order as two models, Model 1 is all grand-siblings, and Model 2 is grand-siblings and tri-siblings. A grand-sibling is a 4-tuple of indices (g, h, m, s) where g is grandfather. (h, m, s) is a sibling part and (g, h, m) is a grandchild part as well as (g, h, s). A tri-sibling part is also a 4-tuple of indices (h, m, s, t). Both (h, m, s) and (h, s, t) are siblings. Figure 2 clearly shows these relations. g h s m ,h t s m s(x,y) Figure 2: Grand-siblings and tri-siblings dependency. k i j , k i j Figure 1: Sibling and grand-child relations. Siblings c1-pos, c2-posOd</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo, Michael Collins. 2010. Efficient Third-order Dependency Parsers. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>