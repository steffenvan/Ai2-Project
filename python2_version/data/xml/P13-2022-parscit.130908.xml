<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004434">
<title confidence="0.887467">
Benefactive/Malefactive Event and Writer Attitude Annotation
</title>
<author confidence="0.892038">
Lingjia Deng †, Yoonjung Choi *, Janyce Wiebe †*
</author>
<affiliation confidence="0.823764">
† Intelligent System Program, University of Pittsburgh
* Department of Computer Science, University of Pittsburgh
</affiliation>
<email confidence="0.996514">
†lid29@pitt.edu, *{yjchoi,wiebe}@cs.pitt.edu
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892785714286">
This paper presents an annotation scheme
for events that negatively or positively
affect entities (benefactive/malefactive
events) and for the attitude of the writer
toward their agents and objects. Work on
opinion and sentiment tends to focus on
explicit expressions of opinions. However,
many attitudes are conveyed implicitly,
and benefactive/malefactive events are
important for inferring implicit attitudes.
We describe an annotation scheme and
give the results of an inter-annotator
agreement study. The annotated corpus is
available online.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985187081081081">
Work in NLP on opinion mining and sentiment
analysis tends to focus on explicit expressions of
opinions. Consider, however, the following sen-
tence from the MPQA corpus (Wiebe et al., 2005)
discussed by (Wilson and Wiebe, 2005):
(1) I think people are happy because
Chavez has fallen.
The explicit sentiment expression, happy, is pos-
itive. Yet (according to the writer), the people
are negative toward Chavez. As noted by (Wil-
son and Wiebe, 2005), the attitude toward Chavez
is inferred from the explicit sentiment toward the
event. An opinion-mining system that recognizes
only explicit sentiments would not be able to per-
ceive the negative attitude toward Chavez con-
veyed in (1). Such inferences must be addressed
for NLP systems to be able to recognize the full
range of opinions conveyed in language.
The inferences arise from interactions be-
tween sentiment expressions and events such as
fallen, which negatively affect entities (malefac-
tive events), and events such as help, which pos-
itively affect entities (benefactive events). While
some corpora have been annotated for explicit
opinion expressions (for example, (Kessler et
al., 2010; Wiebe et al., 2005)), there isn’t a
previously published corpus annotated for bene-
factive/malefactive events. While (Anand and
Reschke, 2010) conducted a related annotation
study, their data are artificially constructed sen-
tences incorporating event predicates from a fixed
list, and their annotations are of the writer’s
attitude toward the events. The scheme pre-
sented here is the first scheme for annotating, in
naturally-occurring text, benefactive/malefactive
events themselves as well as the writer’s attitude
toward the agents and objects of those events.
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999932588235294">
For ease of communication, we use the terms
goodFor and badFor for benefactive and malefac-
tive events, respectively, and use the abbreviation
gfbf for an event that is one or the other. There are
many varieties of gfbf events, including destruc-
tion (as in kill Bill, which is bad for Bill), cre-
ation (as in bake a cake, which is good for the
cake), gain or loss (as in increasing costs, which
is good for the costs), and benefit or injury (as in
comforted the child, which is good for the child)
(Anand and Reschke, 2010).
The scheme targets clear cases of gfbf events.
The event must be representable as a triple of con-
tiguous text spans, (agent, gfbf, object). The
agent must be a noun phrase, or it may be implicit
(as in the constituent will be destroyed). The ob-
ject must be a noun phrase.
</bodyText>
<page confidence="0.938276">
120
</page>
<bodyText confidence="0.7222576">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 120–125,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
Another component of the scheme is the influ-
encer, a word whose effect is to either retain or
reverse the polarity of a gfbf event. For example:
</bodyText>
<listItem confidence="0.99967825">
(2) Luckily Bill didn’t kill him.
(3) The reform prevented companies
from hurting patients.
(4) John helped Mary to save Bill.
</listItem>
<bodyText confidence="0.998612461538462">
In (2) and (3), didn’t and prevented, respectively,
reverse the polarity from badFor to goodFor (not
killing Bill is good for Bill; preventing companies
from hurting patients is good for the patients). In
(4), helped is an influencer which retains the polar-
ity (i.e., helping Mary to save Bill is good for Bill).
Examples (3) and (4) illustrate the case where an
influencer introduces an additional agent (reform
in (3) and John in (4)).
The agent of an influencer must be a noun
phrase or implicit. The object must be another in-
fluencer or a gfbf event.
Note that, semantically, an influencer can be
seen as good for or bad for its object. A reverser
influencer makes its object irrealis (i.e., not hap-
pen). Thus, it is bad for it. In (3), for example,
prevent is bad for the hurting event. A retainer in-
fluencer maintains its object, and thus is good for
it. In (4), for example, helped maintains the sav-
ing event. For this reason, influencers and gfbf
events are sometimes combined in the evaluations
presented below (see Section 4.2).
Finally, the annotators are asked to mark the
writer’s attitude towards the agents of the influ-
encers and gfbf events and the objects of the gfbf
events. For example:
</bodyText>
<listItem confidence="0.995343">
(5) GOP Attack on Reform Is a Fight
Against Justice.
(6) Jettison any reference to end-of-life
counselling.
</listItem>
<bodyText confidence="0.986724">
In (5), there are two badFor events: (GOP, Attack
on, Reform) and (GOP Attack on Reform,Fight
Against, Justice). The writer’s attitude toward
both agents is negative, and his or her attitude
toward both objects is positive. In (6), the
writer conveys a negative attitude toward end-of-
life counselling. The coding manual instructs the
annotators to consider whether an attitude of the
writer is communicated or revealed in the particu-
lar sentence which contains the gfbf event.
</bodyText>
<sectionHeader confidence="0.906271" genericHeader="method">
3 Annotation Scheme
</sectionHeader>
<bodyText confidence="0.999986333333333">
There are four types of annotations: gfbf event,
influencer, agent, and object. For gfbf events, the
agent, object, and polarity (goodFor or badFor) are
identified. For influencers, the agent, object and
effect (reverse or retain) are identified. For agents
and objects, the writer’s attitude is marked (posi-
tive, negative, or none). The annotator links agents
and objects to their gfbf and influencer annotations
via explicit IDs. When an agent is not mentioned
explicitly, the annotator should indicate that it is
implicit. For any span the annotator is not certain
about, he or she can set the uncertain option to be
true.
The annotation manual includes guidelines to
help clarify which events should be annotated.
Though it often is, the gfbf span need not be a
verb or verb phrase. We saw an example above,
namely (5). Even though attack on and fight
against are not verbs, we still mark them because
they represent events that are bad for the object.
Note that, Goyal et al. (2012) present a method for
automatically generating a lexicon of what they
call patient polarity verbs. Such verbs correspond
to gfbf events, except that gfbf events are, concep-
tually, events, not verbs, and gfbf spans are not
limited to verbs (as just noted).
Recall from Section 2 that annotators should
only mark gfbf events that may be represented as a
triple, (agent,gfbf,object). The relationship should
be perceptible by looking only at the spans in the
triple. If, for example, another argument of the
verb is needed to perceive the relationship, the an-
notators should not mark that event.
</bodyText>
<listItem confidence="0.998276">
(7) His uncle left him a massive amount
of debt.
(8) His uncle left him a treasure.
</listItem>
<bodyText confidence="0.999930692307692">
There is no way to break these sentences into
triples that follow our rules. (His uncle, left, him)
doesn’t work because we cannot perceive the po-
larity looking only at the triple; the polarity de-
pends on what his uncle left him. (His uncle, left
him, a massive amount of debt) isn’t correct: the
event is not bad for the debt, it is bad for him. Fi-
nally, (His uncle, left him a massive amount of
debt, Null) isn’t correct, since no object is iden-
tified.
Note that him in (7) and (8) are both consid-
ered benefactive semantic roles (Z´u˜niga and Kit-
til¨a, 2010). In general, gfbf objects are not equiva-
</bodyText>
<page confidence="0.990289">
121
</page>
<bodyText confidence="0.999979257142857">
lent to benefactive/malefactive semantic roles. For
example, in our scheme, (7) is a badFor event and
(8) is a goodFor event, while him fills the benefac-
tive semantic role in both. Further, according to
(Z´u˜niga and Kittil¨a, 2010), me is the filler of the
benefactive role in She baked a cake for me. Yet,
in our scheme, a cake is the object of the good-
For event; me is not included in the annotations.
The objects of gfbf events are what (Z´u˜niga and
Kittil¨a, 2010) refer to as the primary targets of the
events, whereas, they state, beneficiary semantic
roles are typically optional arguments. The reason
we annotate only the primary objects (and agents)
is that the clear cases of attitude implicatures mo-
tivating this work (see Section 1) are inferences
toward agents and primary objects of gfbf events.
Turning to influencers, there may be chains of
them, where the ultimate polarity and agent must
be determined compositionally. For example, the
structure of Jack stopped Mary from trying to kill
Bill is a reverser influencer (stopped) whose object
is a retainer influencer (trying) whose object is, in
turn, a badFor event (kill). The ultimate polarity of
this event is goodFor and the “highest level” agent
is Jack. In our scheme, all such chains of length N
are treated as N −1 influencers followed by a sin-
gle gfbf event. It will be up to an automatic system
to calculate the ultimate polarity and agent using
rules such as those presented in, e.g., (Moilanen
and Pulman, 2007; Neviarouskaya et al., 2010).
To save some effort, the annotators are not
asked to mark retainer influencers which do not in-
troduce new agents. For example, for Jack stopped
trying to kill Bill, there is no need to mark “trying.”
Of course, all reverser influencers must be marked.
</bodyText>
<sectionHeader confidence="0.969711" genericHeader="method">
4 Agreement Study
</sectionHeader>
<bodyText confidence="0.999978714285714">
To validate the reliability of the annotation
scheme, we conducted an agreement study. In this
section we introduce how we designed the agree-
ment study, present the evaluation method and
give the agreement results. Besides, we conduct
a second-step consensus study to further analyze
the disagreement.
</bodyText>
<subsectionHeader confidence="0.981262">
4.1 Data and Agreement Study Design
</subsectionHeader>
<bodyText confidence="0.99996335">
For this study, we want to use data that is rich in
opinions and implicatures. Thus we used the cor-
pus from (Conrad et al., 2012), which consists of
134 documents from blogs and editorials about a
controversial topic, “the Affordable Care Act”.
To measure agreement on various aspects of
the annotation scheme, two annotators, who are
co-authors, participated in the agreement study;
one of the two wasn’t involved in developing the
scheme. The new annotator first read the anno-
tation manual and discussed it with the first an-
notator. Then, the annotators labelled 6 docu-
ments and discussed their disagreements to recon-
cile their differences. For the formal agreement
study, we randomly selected 15 documents, which
have a total of 725 sentences. These documents do
not contain any examples in the manual, and they
are different from the documents discussed during
training. The annotators then independently anno-
tated the 15 selected documents.
</bodyText>
<subsectionHeader confidence="0.995201">
4.2 Agreement Study Evaluation
</subsectionHeader>
<bodyText confidence="0.9997855">
We annotate four types of items (gfbf event, influ-
encer, agent, and object) and their corresponding
attributes. As noted above in Section 2, influencers
can also be viewed as gfbf events. Also, the two
may be combined together in chains. Thus, we
measure agreement for gfbf and influencer spans
together, treating them as one type. Then we
choose the subset of gfbf and influencer annota-
tions that both annotators identified, and measure
agreement on the corresponding agents and ob-
jects.
Sometimes the annotations differ even though
the annotators recognize the same gfbf event.
Consider the following sentence:
</bodyText>
<listItem confidence="0.802572">
(9) Obama helped reform curb costs.
Suppose the annotations given by the annotators
were:
</listItem>
<figureCaption confidence="0.890536">
Ann 1. (Obama, helped, curb)
(reform, curb, costs)
Ann 2. (Obama, helped, reform)
</figureCaption>
<bodyText confidence="0.947033875">
The two annotators do agree on the (Obama,
helped, reform) triple, the first one marking helped
as a retainer and the other marking it as a goodFor
event. To take such cases into consideration in our
evaluation of agreement, if two spans overlap and
one is marked as gfbf and the other as influencer,
we use the following rules to match up their agents
and objects:
</bodyText>
<listItem confidence="0.9915515">
• for a gfbf event, consider its agent and object
as annotated;
</listItem>
<page confidence="0.978429">
122
</page>
<bodyText confidence="0.947818705882353">
• for an influencer, assign the agent of the in-
fluencer’s object to be the influencer’s object,
and consider its agent as annotated and the
newly-assigned object. In (9), Ann 2’s anno-
tations remain the same and Ann 1’s become
(Obama, helped, reform) and (reform, curb,
costs).
We use the same measurement for agreement
for all types of spans. Suppose A is a set of an-
notations of a particular type and B is the set of
annotations of the same type from the other anno-
tator. For any text span a E A and b E B, the span
coverage c measures the overlap between a and b.
Two measures of c are adopted here.
Binary: As in (Wilson and Wiebe, 2003), if two
spans a and b overlap, the pair is counted as 1,
otherwise 0.
</bodyText>
<equation confidence="0.988066">
c1(a,b) = 1 if |a n b |&gt; 0
</equation>
<bodyText confidence="0.998981">
Numerical: (Johansson and Moschitti, 2013)
propose, for the pairs that are counted as 1 by c1, a
measure of the percentage of overlapping tokens,
</bodyText>
<equation confidence="0.9962675">
c2 (a, b) = |a n
|b |b�
</equation>
<bodyText confidence="0.989855545454546">
where |a |is the number of tokens in span a, and n
gives the tokens that two spans have in common.
As (Breck et al., 2007) point out, c2 avoids the
problem of c1, namely that c1 does not penalize a
span covering the whole sentence, so it potentially
inflates the results.
Following (Wilson and Wiebe, 2003), treat-
ing each set A and B in turn as the gold-
standard, we calculate the average F-measure, de-
noted agr(A, B). agr(A, B) is calculated twice,
once with c = c1 and once with c = c2.
</bodyText>
<equation confidence="0.988044666666667">
match(A, B) = � c(a, b)
aEA,bEB,
|a∩b|&gt;0
match(A,B)
agr(A||B) =
|B|
agr(A||B) + agr(B||A)
agr(A, B) =
2
</equation>
<bodyText confidence="0.999965833333333">
Now that we have the sets of annotations on
which the annotators agree, we use n (Artstein
and Poesio, 2008) to measure agreement for the
attributes. We report two n values: one for the
polarities of the gfbf events, together with the ef-
fects of the influencers, and one for the writer’s
</bodyText>
<table confidence="0.99904325">
gfbf &amp; agent object
influencer
all anno- c1 0.70 0.92 1.00
tations c2 0.69 0.87 0.97
only c1 0.75 0.92 1.00
certain c2 0.72 0.87 0.98
consensus c1 0.85 0.93 0.99
study c2 0.81 0.88 0.98
</table>
<tableCaption confidence="0.990468">
Table 1: Span overlapping agreement agr(A, B)
in agreement study and consensus study.
</tableCaption>
<table confidence="0.998765333333333">
polarity &amp; effect attitude
all 0.97 0.89
certain 0.97 0.89
</table>
<tableCaption confidence="0.996241">
Table 2: n for attribute agreement.
</tableCaption>
<bodyText confidence="0.999887857142857">
attitude toward the agents and objects. Note that,
as in Example (9), sometimes one annotator marks
a span as gfbf and the other marks it as an influ-
encer; in such cases we regard retain and goodfor
as the same attribute value and reverse and badfor
as the same value. Table 1 gives the agr values
and Table 2 gives the n values.
</bodyText>
<subsectionHeader confidence="0.999925">
4.3 Agreement Study Results
</subsectionHeader>
<bodyText confidence="0.983654588235294">
Recall that the annotator could choose whether
(s)he is certain about the annotation. Thus, we
evaluate two sets: all annotations and only those
annotations that both annotators are certain about.
The results are shown in the top four rows in Table
1.
The results for agents and objects in Table 1 are
all quite good, indicating that, given a gfbf or in-
fluencer, the annotators are able to correctly iden-
tify the agent and object.
Table 1 also shows that results are not signifi-
cantly worse when measured using c2 rather than
c1. This suggests that, in general, the annotators
have good agreement concerning the boundaries
of spans.
Table 2 shows that the n values are high for both
sets of attributes.
</bodyText>
<subsectionHeader confidence="0.999578">
4.4 Consensus Analysis
</subsectionHeader>
<bodyText confidence="0.9907466">
Following (Medlock and Briscoe, 2007), we ex-
amined what percentage of disagreement is due to
negligence on behalf of one or the other annota-
tor (i.e., cases of clear gfbfs or influencers that
were missed), though we conducted our consensus
</bodyText>
<page confidence="0.997605">
123
</page>
<bodyText confidence="0.999988782608696">
study in a more independent manner than face-to-
face discussion between the annotators. For anno-
tator Ann1, we highlighted sentences for which
only Ann2 marked a gfbf event, and gave Ann1’s
annotations back to him or her with the highlights
added on top. For Ann2 we did the same thing.
The annotators reconsidered their highlighted sen-
tences, making any changes they felt they should,
without communicating with each other. There
could be more than one annotation in a highlighted
sentence; the annotators were not told the specific
number.
After re-annotating the highlighted sentences,
we calculate the agreement score for all the an-
notations. As shown in the last two rows in Table
1, the agreement for gfbf and influencer annota-
tions increases quite a bit. Similar to the claim
in (Medlock and Briscoe, 2007), it is reasonable
to conclude that the actual agreement is approx-
imately lower bounded by the initial values and
upper bounded by the consensus values, though,
compared to face-to-face consensus, we provide a
tighter upper bound.
</bodyText>
<sectionHeader confidence="0.925022" genericHeader="evaluation">
5 Corpus and Examples
</sectionHeader>
<bodyText confidence="0.999575136363636">
Recall from in Section 4.1 that we use the corpus
from (Conrad et al., 2012), which consists of 134
documents with a total of 8,069 sentences from
blogs and editorials about “the Affordable Care
Act”. There are 1,762 gfbf and influencer annota-
tions. On average, more than 20 percent of the sen-
tences contain a gfbf event or an influencer. Out of
all gfbf and influencer annotations, 40 percent are
annotated as goodFor or retain and 60 percent are
annotated as badFor or reverse. For agents and ob-
jects, 52 percent are annotated as positive and 47
percent as negative. Only 1 percent are annotated
as none, showing that almost all the sentences (in
this corpus of editorials and blogs) which con-
tain gfbf annotations are subjective. The annotated
corpus is available online1.
To illustrate various aspects of the annotation
scheme, in this section we give several examples
from the corpus. In the examples below, words
in square brackets are agents or objects, words in
italics are influencers, and words in boldface are
gfbf events.
</bodyText>
<footnote confidence="0.952538333333333">
1. And [it] will enable [Obama and the
Democrats] - who run Washington - to get
1http://mpqa.cs.pitt.edu/
</footnote>
<bodyText confidence="0.817213411764706">
back to creating [jobs].
(a) Creating is goodFor jobs; the agent is
Obama and the Democrats.
(b) The phrase to get back to is a retainer in-
fluencer. But, the agent span is also Obama
and the Democrats, as the same with the
goodFor, so we don’t have to give an anno-
tation for it.
(c) The phrase enable is a retainer influencer.
Since its agent span is different (namely, it),
we do create an annotation for it.
2. [Repealing [the Affordable Care Act]] would
hurt [families, businesses, and our econ-
omy].
(a) Repealing is a badFor event since it de-
prives the object, the Affordable Care Act, of
its existence. In this case the agent is implicit.
(b) The agent of the badFor event hurt is the
whole phrase Repealing the Affordable Care
Act. Note that the agent span is in fact a noun
phrase (even though it refers to an event).
Thus, it doesn’t break the rule that all agent
gfbf spans should be noun phrases.
3. It is a moral obligation to end this indefensi-
ble neglect of [hard-working Americans].
(a) This example illustrates a gfbf that cen-
ters on a noun (neglect) rather than on a verb.
(b) It also illustrates the case when two words
can be seen as gfbf events: both end and ne-
glect of can be seen as badFor events. Fol-
lowing our specification, they are annotated
as a chain ending in a single gfbf event: end
is an influencer that reverses the polarity of
the badFor event neglect of.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999831285714286">
Attitude inferences arise from interactions
between sentiment expressions and benefac-
tive/malefactive events. Corpora have been
annotated in the past for explicit sentiment ex-
pressions; this paper fills in a gap by presenting
an annotation scheme for benefactive/malefactive
events and the writer’s attitude toward the agents
and objects of those events. We conducted an
agreement study, the results of which are positive.
Acknowledgement This work was supported
in part by DARPA-BAA-12-47 DEFT grant
#12475008 and National Science Foundation
grant #IIS-0916046. We would like to thank the
anonymous reviewers for their helpful feedback.
</bodyText>
<page confidence="0.997856">
124
</page>
<sectionHeader confidence="0.998329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775704918033">
Pranav Anand and Kevin Reschke. 2010. Verb classes
as evaluativity functor classes. In Interdisciplinary
Workshop on Verbs. The Identification and Repre-
sentation of Verb Features.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Comput.
Linguist., 34(4):555–596, December.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI’07, pages 2683–
2688, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ’12, pages 80–88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A
computational model for plot units. Computational
Intelligence, pages no–no.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3).
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 icwsm jdpa sen-
timent corpus for the automotive domain. In 4th
Int’l AAAI Conference on Weblogs and Social Media
Data Workshop Challenge (ICWSM-DWC 2010).
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Alena Neviarouskaya, Helmut Prendinger, and Mit-
suru Ishizuka. 2010. Recognition of affect, judg-
ment, and appreciation in text. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ’10, pages 806–814, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3):164–210.
Theresa Wilson and Janyce Wiebe. 2003. Annotating
opinions in the world press. In Proceedings of the
4th ACL SIGdial Workshop on Discourse and Dia-
logue (SIGdial-03), pages 13–22.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
F. Z´u˜niga and S. Kittil¨a. 2010. Introduction. In
F. Z´u˜niga and S. Kittil¨a, editors, Benefactives and
malefactives, Typological studies in language. J.
Benjamins Publishing Company.
</reference>
<page confidence="0.998492">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315351">
<title confidence="0.998811">Benefactive/Malefactive Event and Writer Attitude Annotation</title>
<author confidence="0.745418">Deng Yoonjung Choi Janyce Wiebe</author>
<affiliation confidence="0.3626515">System Program, University of * Department of Computer Science, University of</affiliation>
<abstract confidence="0.9996104">This paper presents an annotation scheme for events that negatively or positively entities and for the attitude of the writer toward their agents and objects. Work on opinion and sentiment tends to focus on explicit expressions of opinions. However, many attitudes are conveyed implicitly, and benefactive/malefactive events are important for inferring implicit attitudes. We describe an annotation scheme and give the results of an inter-annotator agreement study. The annotated corpus is available online.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Kevin Reschke</author>
</authors>
<title>Verb classes as evaluativity functor classes. In Interdisciplinary Workshop on Verbs. The Identification and Representation of Verb Features.</title>
<date>2010</date>
<contexts>
<context position="2126" citStr="Anand and Reschke, 2010" startWordPosition="308" endWordPosition="311">ez conveyed in (1). Such inferences must be addressed for NLP systems to be able to recognize the full range of opinions conveyed in language. The inferences arise from interactions between sentiment expressions and events such as fallen, which negatively affect entities (malefactive events), and events such as help, which positively affect entities (benefactive events). While some corpora have been annotated for explicit opinion expressions (for example, (Kessler et al., 2010; Wiebe et al., 2005)), there isn’t a previously published corpus annotated for benefactive/malefactive events. While (Anand and Reschke, 2010) conducted a related annotation study, their data are artificially constructed sentences incorporating event predicates from a fixed list, and their annotations are of the writer’s attitude toward the events. The scheme presented here is the first scheme for annotating, in naturally-occurring text, benefactive/malefactive events themselves as well as the writer’s attitude toward the agents and objects of those events. 2 Overview For ease of communication, we use the terms goodFor and badFor for benefactive and malefactive events, respectively, and use the abbreviation gfbf for an event that is</context>
</contexts>
<marker>Anand, Reschke, 2010</marker>
<rawString>Pranav Anand and Kevin Reschke. 2010. Verb classes as evaluativity functor classes. In Interdisciplinary Workshop on Verbs. The Identification and Representation of Verb Features.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="13831" citStr="Artstein and Poesio, 2008" startWordPosition="2323" endWordPosition="2326">wo spans have in common. As (Breck et al., 2007) point out, c2 avoids the problem of c1, namely that c1 does not penalize a span covering the whole sentence, so it potentially inflates the results. Following (Wilson and Wiebe, 2003), treating each set A and B in turn as the goldstandard, we calculate the average F-measure, denoted agr(A, B). agr(A, B) is calculated twice, once with c = c1 and once with c = c2. match(A, B) = � c(a, b) aEA,bEB, |a∩b|&gt;0 match(A,B) agr(A||B) = |B| agr(A||B) + agr(B||A) agr(A, B) = 2 Now that we have the sets of annotations on which the annotators agree, we use n (Artstein and Poesio, 2008) to measure agreement for the attributes. We report two n values: one for the polarities of the gfbf events, together with the effects of the influencers, and one for the writer’s gfbf &amp; agent object influencer all anno- c1 0.70 0.92 1.00 tations c2 0.69 0.87 0.97 only c1 0.75 0.92 1.00 certain c2 0.72 0.87 0.98 consensus c1 0.85 0.93 0.99 study c2 0.81 0.88 0.98 Table 1: Span overlapping agreement agr(A, B) in agreement study and consensus study. polarity &amp; effect attitude all 0.97 0.89 certain 0.97 0.89 Table 2: n for attribute agreement. attitude toward the agents and objects. Note that, as</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist., 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>2683--2688</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="13253" citStr="Breck et al., 2007" startWordPosition="2215" endWordPosition="2218">set of annotations of the same type from the other annotator. For any text span a E A and b E B, the span coverage c measures the overlap between a and b. Two measures of c are adopted here. Binary: As in (Wilson and Wiebe, 2003), if two spans a and b overlap, the pair is counted as 1, otherwise 0. c1(a,b) = 1 if |a n b |&gt; 0 Numerical: (Johansson and Moschitti, 2013) propose, for the pairs that are counted as 1 by c1, a measure of the percentage of overlapping tokens, c2 (a, b) = |a n |b |b� where |a |is the number of tokens in span a, and n gives the tokens that two spans have in common. As (Breck et al., 2007) point out, c2 avoids the problem of c1, namely that c1 does not penalize a span covering the whole sentence, so it potentially inflates the results. Following (Wilson and Wiebe, 2003), treating each set A and B in turn as the goldstandard, we calculate the average F-measure, denoted agr(A, B). agr(A, B) is calculated twice, once with c = c1 and once with c = c2. match(A, B) = � c(a, b) aEA,bEB, |a∩b|&gt;0 match(A,B) agr(A||B) = |B| agr(A||B) + agr(B||A) agr(A, B) = 2 Now that we have the sets of annotations on which the annotators agree, we use n (Artstein and Poesio, 2008) to measure agreement </context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 2683– 2688, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Conrad</author>
<author>Janyce Wiebe</author>
<author>Hwa</author>
<author>Rebecca</author>
</authors>
<title>Recognizing arguing subjectivity and argument tags.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, ExProM ’12,</booktitle>
<pages>80--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10156" citStr="Conrad et al., 2012" startWordPosition="1676" endWordPosition="1679"> example, for Jack stopped trying to kill Bill, there is no need to mark “trying.” Of course, all reverser influencers must be marked. 4 Agreement Study To validate the reliability of the annotation scheme, we conducted an agreement study. In this section we introduce how we designed the agreement study, present the evaluation method and give the agreement results. Besides, we conduct a second-step consensus study to further analyze the disagreement. 4.1 Data and Agreement Study Design For this study, we want to use data that is rich in opinions and implicatures. Thus we used the corpus from (Conrad et al., 2012), which consists of 134 documents from blogs and editorials about a controversial topic, “the Affordable Care Act”. To measure agreement on various aspects of the annotation scheme, two annotators, who are co-authors, participated in the agreement study; one of the two wasn’t involved in developing the scheme. The new annotator first read the annotation manual and discussed it with the first annotator. Then, the annotators labelled 6 documents and discussed their disagreements to reconcile their differences. For the formal agreement study, we randomly selected 15 documents, which have a total </context>
<context position="16847" citStr="Conrad et al., 2012" startWordPosition="2842" endWordPosition="2845">he specific number. After re-annotating the highlighted sentences, we calculate the agreement score for all the annotations. As shown in the last two rows in Table 1, the agreement for gfbf and influencer annotations increases quite a bit. Similar to the claim in (Medlock and Briscoe, 2007), it is reasonable to conclude that the actual agreement is approximately lower bounded by the initial values and upper bounded by the consensus values, though, compared to face-to-face consensus, we provide a tighter upper bound. 5 Corpus and Examples Recall from in Section 4.1 that we use the corpus from (Conrad et al., 2012), which consists of 134 documents with a total of 8,069 sentences from blogs and editorials about “the Affordable Care Act”. There are 1,762 gfbf and influencer annotations. On average, more than 20 percent of the sentences contain a gfbf event or an influencer. Out of all gfbf and influencer annotations, 40 percent are annotated as goodFor or retain and 60 percent are annotated as badFor or reverse. For agents and objects, 52 percent are annotated as positive and 47 percent as negative. Only 1 percent are annotated as none, showing that almost all the sentences (in this corpus of editorials a</context>
</contexts>
<marker>Conrad, Wiebe, Hwa, Rebecca, 2012</marker>
<rawString>Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca. 2012. Recognizing arguing subjectivity and argument tags. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, ExProM ’12, pages 80–88, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Ellen Riloff</author>
<author>Hal Daum</author>
</authors>
<title>A computational model for plot units.</title>
<date>2012</date>
<journal>Computational Intelligence,</journal>
<pages>pages no–no.</pages>
<contexts>
<context position="6615" citStr="Goyal et al. (2012)" startWordPosition="1064" endWordPosition="1067">ects to their gfbf and influencer annotations via explicit IDs. When an agent is not mentioned explicitly, the annotator should indicate that it is implicit. For any span the annotator is not certain about, he or she can set the uncertain option to be true. The annotation manual includes guidelines to help clarify which events should be annotated. Though it often is, the gfbf span need not be a verb or verb phrase. We saw an example above, namely (5). Even though attack on and fight against are not verbs, we still mark them because they represent events that are bad for the object. Note that, Goyal et al. (2012) present a method for automatically generating a lexicon of what they call patient polarity verbs. Such verbs correspond to gfbf events, except that gfbf events are, conceptually, events, not verbs, and gfbf spans are not limited to verbs (as just noted). Recall from Section 2 that annotators should only mark gfbf events that may be represented as a triple, (agent,gfbf,object). The relationship should be perceptible by looking only at the spans in the triple. If, for example, another argument of the verb is needed to perceive the relationship, the annotators should not mark that event. (7) His</context>
</contexts>
<marker>Goyal, Riloff, Daum, 2012</marker>
<rawString>Amit Goyal, Ellen Riloff, and Hal Daum III. 2012. A computational model for plot units. Computational Intelligence, pages no–no.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational features in fine-grained opinion analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="13003" citStr="Johansson and Moschitti, 2013" startWordPosition="2162" endWordPosition="2165">gned object. In (9), Ann 2’s annotations remain the same and Ann 1’s become (Obama, helped, reform) and (reform, curb, costs). We use the same measurement for agreement for all types of spans. Suppose A is a set of annotations of a particular type and B is the set of annotations of the same type from the other annotator. For any text span a E A and b E B, the span coverage c measures the overlap between a and b. Two measures of c are adopted here. Binary: As in (Wilson and Wiebe, 2003), if two spans a and b overlap, the pair is counted as 1, otherwise 0. c1(a,b) = 1 if |a n b |&gt; 0 Numerical: (Johansson and Moschitti, 2013) propose, for the pairs that are counted as 1 by c1, a measure of the percentage of overlapping tokens, c2 (a, b) = |a n |b |b� where |a |is the number of tokens in span a, and n gives the tokens that two spans have in common. As (Breck et al., 2007) point out, c2 avoids the problem of c1, namely that c1 does not penalize a span covering the whole sentence, so it potentially inflates the results. Following (Wilson and Wiebe, 2003), treating each set A and B in turn as the goldstandard, we calculate the average F-measure, denoted agr(A, B). agr(A, B) is calculated twice, once with c = c1 and on</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Computational Linguistics, 39(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Kessler</author>
<author>Miriam Eckert</author>
<author>Lyndsay Clark</author>
<author>Nicolas Nicolov</author>
</authors>
<title>icwsm jdpa sentiment corpus for the automotive domain.</title>
<date>2010</date>
<booktitle>In 4th Int’l AAAI Conference on Weblogs and Social Media Data Workshop Challenge (ICWSM-DWC</booktitle>
<publisher>The</publisher>
<contexts>
<context position="1983" citStr="Kessler et al., 2010" startWordPosition="288" endWordPosition="291">he event. An opinion-mining system that recognizes only explicit sentiments would not be able to perceive the negative attitude toward Chavez conveyed in (1). Such inferences must be addressed for NLP systems to be able to recognize the full range of opinions conveyed in language. The inferences arise from interactions between sentiment expressions and events such as fallen, which negatively affect entities (malefactive events), and events such as help, which positively affect entities (benefactive events). While some corpora have been annotated for explicit opinion expressions (for example, (Kessler et al., 2010; Wiebe et al., 2005)), there isn’t a previously published corpus annotated for benefactive/malefactive events. While (Anand and Reschke, 2010) conducted a related annotation study, their data are artificially constructed sentences incorporating event predicates from a fixed list, and their annotations are of the writer’s attitude toward the events. The scheme presented here is the first scheme for annotating, in naturally-occurring text, benefactive/malefactive events themselves as well as the writer’s attitude toward the agents and objects of those events. 2 Overview For ease of communicatio</context>
</contexts>
<marker>Kessler, Eckert, Clark, Nicolov, 2010</marker>
<rawString>Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and Nicolas Nicolov. 2010. The 2010 icwsm jdpa sentiment corpus for the automotive domain. In 4th Int’l AAAI Conference on Weblogs and Social Media Data Workshop Challenge (ICWSM-DWC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="15499" citStr="Medlock and Briscoe, 2007" startWordPosition="2617" endWordPosition="2620">y those annotations that both annotators are certain about. The results are shown in the top four rows in Table 1. The results for agents and objects in Table 1 are all quite good, indicating that, given a gfbf or influencer, the annotators are able to correctly identify the agent and object. Table 1 also shows that results are not significantly worse when measured using c2 rather than c1. This suggests that, in general, the annotators have good agreement concerning the boundaries of spans. Table 2 shows that the n values are high for both sets of attributes. 4.4 Consensus Analysis Following (Medlock and Briscoe, 2007), we examined what percentage of disagreement is due to negligence on behalf of one or the other annotator (i.e., cases of clear gfbfs or influencers that were missed), though we conducted our consensus 123 study in a more independent manner than face-toface discussion between the annotators. For annotator Ann1, we highlighted sentences for which only Ann2 marked a gfbf event, and gave Ann1’s annotations back to him or her with the highlights added on top. For Ann2 we did the same thing. The annotators reconsidered their highlighted sentences, making any changes they felt they should, without </context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment composition.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP</booktitle>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="9388" citStr="Moilanen and Pulman, 2007" startWordPosition="1547" endWordPosition="1550">here the ultimate polarity and agent must be determined compositionally. For example, the structure of Jack stopped Mary from trying to kill Bill is a reverser influencer (stopped) whose object is a retainer influencer (trying) whose object is, in turn, a badFor event (kill). The ultimate polarity of this event is goodFor and the “highest level” agent is Jack. In our scheme, all such chains of length N are treated as N −1 influencers followed by a single gfbf event. It will be up to an automatic system to calculate the ultimate polarity and agent using rules such as those presented in, e.g., (Moilanen and Pulman, 2007; Neviarouskaya et al., 2010). To save some effort, the annotators are not asked to mark retainer influencers which do not introduce new agents. For example, for Jack stopped trying to kill Bill, there is no need to mark “trying.” Of course, all reverser influencers must be marked. 4 Agreement Study To validate the reliability of the annotation scheme, we conducted an agreement study. In this section we introduce how we designed the agreement study, present the evaluation method and give the agreement results. Besides, we conduct a second-step consensus study to further analyze the disagreemen</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment composition. In Proceedings of RANLP 2007, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena Neviarouskaya</author>
<author>Helmut Prendinger</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Recognition of affect, judgment, and appreciation in text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>806--814</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9417" citStr="Neviarouskaya et al., 2010" startWordPosition="1551" endWordPosition="1554">and agent must be determined compositionally. For example, the structure of Jack stopped Mary from trying to kill Bill is a reverser influencer (stopped) whose object is a retainer influencer (trying) whose object is, in turn, a badFor event (kill). The ultimate polarity of this event is goodFor and the “highest level” agent is Jack. In our scheme, all such chains of length N are treated as N −1 influencers followed by a single gfbf event. It will be up to an automatic system to calculate the ultimate polarity and agent using rules such as those presented in, e.g., (Moilanen and Pulman, 2007; Neviarouskaya et al., 2010). To save some effort, the annotators are not asked to mark retainer influencers which do not introduce new agents. For example, for Jack stopped trying to kill Bill, there is no need to mark “trying.” Of course, all reverser influencers must be marked. 4 Agreement Study To validate the reliability of the annotation scheme, we conducted an agreement study. In this section we introduce how we designed the agreement study, present the evaluation method and give the agreement results. Besides, we conduct a second-step consensus study to further analyze the disagreement. 4.1 Data and Agreement Stu</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2010</marker>
<rawString>Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. 2010. Recognition of affect, judgment, and appreciation in text. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 806–814, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1029" citStr="Wiebe et al., 2005" startWordPosition="138" endWordPosition="141">nd for the attitude of the writer toward their agents and objects. Work on opinion and sentiment tends to focus on explicit expressions of opinions. However, many attitudes are conveyed implicitly, and benefactive/malefactive events are important for inferring implicit attitudes. We describe an annotation scheme and give the results of an inter-annotator agreement study. The annotated corpus is available online. 1 Introduction Work in NLP on opinion mining and sentiment analysis tends to focus on explicit expressions of opinions. Consider, however, the following sentence from the MPQA corpus (Wiebe et al., 2005) discussed by (Wilson and Wiebe, 2005): (1) I think people are happy because Chavez has fallen. The explicit sentiment expression, happy, is positive. Yet (according to the writer), the people are negative toward Chavez. As noted by (Wilson and Wiebe, 2005), the attitude toward Chavez is inferred from the explicit sentiment toward the event. An opinion-mining system that recognizes only explicit sentiments would not be able to perceive the negative attitude toward Chavez conveyed in (1). Such inferences must be addressed for NLP systems to be able to recognize the full range of opinions convey</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating opinions in the world press.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th ACL SIGdial Workshop on Discourse and Dialogue (SIGdial-03),</booktitle>
<pages>13--22</pages>
<contexts>
<context position="12863" citStr="Wilson and Wiebe, 2003" startWordPosition="2133" endWordPosition="2136">er, assign the agent of the influencer’s object to be the influencer’s object, and consider its agent as annotated and the newly-assigned object. In (9), Ann 2’s annotations remain the same and Ann 1’s become (Obama, helped, reform) and (reform, curb, costs). We use the same measurement for agreement for all types of spans. Suppose A is a set of annotations of a particular type and B is the set of annotations of the same type from the other annotator. For any text span a E A and b E B, the span coverage c measures the overlap between a and b. Two measures of c are adopted here. Binary: As in (Wilson and Wiebe, 2003), if two spans a and b overlap, the pair is counted as 1, otherwise 0. c1(a,b) = 1 if |a n b |&gt; 0 Numerical: (Johansson and Moschitti, 2013) propose, for the pairs that are counted as 1 by c1, a measure of the percentage of overlapping tokens, c2 (a, b) = |a n |b |b� where |a |is the number of tokens in span a, and n gives the tokens that two spans have in common. As (Breck et al., 2007) point out, c2 avoids the problem of c1, namely that c1 does not penalize a span covering the whole sentence, so it potentially inflates the results. Following (Wilson and Wiebe, 2003), treating each set A and </context>
</contexts>
<marker>Wilson, Wiebe, 2003</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2003. Annotating opinions in the world press. In Proceedings of the 4th ACL SIGdial Workshop on Discourse and Dialogue (SIGdial-03), pages 13–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</booktitle>
<contexts>
<context position="1067" citStr="Wilson and Wiebe, 2005" startWordPosition="144" endWordPosition="147">toward their agents and objects. Work on opinion and sentiment tends to focus on explicit expressions of opinions. However, many attitudes are conveyed implicitly, and benefactive/malefactive events are important for inferring implicit attitudes. We describe an annotation scheme and give the results of an inter-annotator agreement study. The annotated corpus is available online. 1 Introduction Work in NLP on opinion mining and sentiment analysis tends to focus on explicit expressions of opinions. Consider, however, the following sentence from the MPQA corpus (Wiebe et al., 2005) discussed by (Wilson and Wiebe, 2005): (1) I think people are happy because Chavez has fallen. The explicit sentiment expression, happy, is positive. Yet (according to the writer), the people are negative toward Chavez. As noted by (Wilson and Wiebe, 2005), the attitude toward Chavez is inferred from the explicit sentiment toward the event. An opinion-mining system that recognizes only explicit sentiments would not be able to perceive the negative attitude toward Chavez conveyed in (1). Such inferences must be addressed for NLP systems to be able to recognize the full range of opinions conveyed in language. The inferences arise f</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Z´u˜niga</author>
<author>S Kittil¨a</author>
</authors>
<date>2010</date>
<booktitle>Benefactives and malefactives, Typological studies in</booktitle>
<editor>Introduction. In F. Z´u˜niga and S. Kittil¨a, editors,</editor>
<publisher>Benjamins Publishing Company.</publisher>
<marker>Z´u˜niga, Kittil¨a, 2010</marker>
<rawString>F. Z´u˜niga and S. Kittil¨a. 2010. Introduction. In F. Z´u˜niga and S. Kittil¨a, editors, Benefactives and malefactives, Typological studies in language. J. Benjamins Publishing Company.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>