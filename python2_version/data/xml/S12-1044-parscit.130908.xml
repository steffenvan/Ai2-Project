<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051693">
<title confidence="0.987037">
UWashington: Negation Resolution using Machine Learning Methods
</title>
<author confidence="0.995743">
James Paul White
</author>
<affiliation confidence="0.9950665">
University of Washington
Department of Linguistics, Box 354340
</affiliation>
<address confidence="0.761672">
Seattle, WA 98195, USA
</address>
<email confidence="0.999589">
jimwhite@uw.edu
</email>
<sectionHeader confidence="0.995554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992334714285714">
This paper reports on a simple system for
resolving the scope of negation in the closed
track of the *SEM 2012 Shared Task. Cue
detection is performed using regular
expression rules extracted from the training
data. Both scope tokens and negated event
tokens are resolved using a Conditional
Random Field (CRF) sequence tagger –
namely the SimpleTagger library in the
MALLET machine learning toolkit. The full
negation F1 score obtained for the task
evaluation is 48.09% (P=74.02%, R=35.61%)
which ranks this system fourth among the six
submitted for the closed track.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998136875">
Resolving the scope of negation is an interesting
area of research for Natural Language Processing
(NLP) systems because many such systems have
used methods that are insensitive to polarity. As a
result it is fairly common to have a system that
treats “X does Y” and “X does not Y” as having
the same, or very nearly the same, meaning1. A
few application areas that have been addressing
this issue recently are in sentiment analysis, bio-
medical NLP, and recognition of textual entail-
ment. Sentiment analysis systems are frequently
used in corporate and product marketing, call cen-
ter quality control, and within “recommender” sys-
tems which are all contexts where it is important to
recognize that “X does like Y” is contrary to “X
does not like Y”. Similarly in biomedical text such
</bodyText>
<footnote confidence="0.845852">
1A one token difference between the strings surely indicating
at least an inexact match.
</footnote>
<bodyText confidence="0.999952233333333">
as research papers and abstracts, diagnostic proce-
dure reports, and medical records it is important to
differentiate between statements about what is the
case and what is not the case.
The *SEM 2012 Shared Task is actually two re-
lated tasks run in parallel. The one this system was
developed for is the identification of three features
of negation: the cue, the scope, and the factual
negated event (if any). The other task is concerned
with the focus of negation. Detailed description of
both subtasks, including definition of the relevant
concepts and terminology (negation, cue, scope,
event, and focus) appears in this volume (Morante
and Blanco, 2012). Roser Morante and Eduardo
Blanco describe the corpora provided to partici-
pants with numbers and examples, methods used
used to process the data, and briefly describes each
participant and analyzes the overall results.
Annotation of the corpus was undertaken at the
University of Antwerp and was performed on sev-
eral Sherlock Holmes works of fiction written by
Sir Arthur Conan Doyle. The corpus includes all
sentences from the original text, not just those em-
ploying negation. Roser Morante and Walter
Daelemans provide a thorough explanation of
those gold annotations of negation cue, scope, and
negated event (if any) (Morante and Daelemans,
2012). Their paper explains the motivations for the
particular annotation decisions and describes in de-
tail the guidelines, including many examples.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9989975">
Recognition of phrases containing negation, partic-
ularly in the medical domain, using regular expres-
sions has been described using several different ap-
proaches. Systems such as Negfinder (Mutalik et
</bodyText>
<page confidence="0.996655">
335
</page>
<note confidence="0.531498">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335–339,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999415571428571">
al, 2001) and NegEx (Chapman et al, 2001) use
manually constructed rules to extract phrases from
text and classify them as to whether they contain
an expression of negation. Rokach et al evaluate
several methods and show their highest level of
performance (an F1 of 95.9 ± 1.9%) by using cas-
caded decision trees of regular expressions learned
from labelled narrative medical reports (Rokach et
al, 2008).
Those systems perform a different function than
that required for this task though. They classify
phrases extracted from plain text as to whether
they contain negation or not, while the requirement
of this shared task for negation cue detection is to
identify the particular token(s) or part of a token
that signals the presence of negation. Furthermore,
those systems only identify the scope of negation
at the level of phrasal constituents, which is differ-
ent than what is required for this task in which the
scopes are not necessarily contiguous.
Conditional Random Field (CRF) sequence tag-
gers have been successfully applied to many scope
resolution problems, including those of negation.
The NegScope system (Agarwal and Yu, 2010)
trains a CRF sequence tagger on labelled data to
identify both the cue and scope of negation. How-
ever, that system only recognizes a whole word as
a cue and does not recognize nor generalize nega-
tion cues which are affixes. There are also systems
that use CRF sequence taggers for detection of
hedge scopes (Tang et al, 2010, Zhao et al, 2010).
Morante and Daelemans describe a method for im-
proving resolution of the scope of negation by
combining IGTREE, CRF, and Support Vector Ma-
chines (SVM) (Morante and Daelemans, 2009).
</bodyText>
<sectionHeader confidence="0.988435" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999974583333333">
This system is implemented as a three stage cas-
cade with the output from each of the first two
stages included as input to the subsequent stage.
The stages are ordered as cue detection, scope de-
tection, and finally negated event detection. The
format of the inputs and outputs for each stage use
the shared task’s CoNLL-style file format. That
simplifies the use of the supplied gold-standard
data for training of each stage separately.
Because this system was designed for the
closed track of the shared task, it makes minimal
language-specific assumptions and learns (nearly)
all language-specific rules from the gold-labelled
training data (which includes the development set
for the final system).
The CRF sequence tagger used by the system is
that implemented in the SimpleTagger class of the
MALLET toolkit, which is a Java library distrib-
uted under the Common Public License2.
The system is implemented in the Groovy pro-
gramming language, an agile and dynamic lan-
guage for the Java Virtual Machine3. The source
code is available under the GNU Public License on
GitHub4.
</bodyText>
<subsectionHeader confidence="0.999511">
3.1 Cue Detection
</subsectionHeader>
<bodyText confidence="0.999940818181818">
Cues are recognized by four different regular ex-
pression rule patterns: affixes (partial token), single
(whole) token, contiguous multiple token, and
gappy (discontiguous) multiple token. The rules
are learned by a two pass process. In the first pass,
for each positive example of a negation cue in the
training data, a rule that matches that example is
added to the prospective rule set. Then, in the sec-
ond pass, the rules are applied to the training data
and the counts of correct and incorrect matches are
accumulated. Rules that are wrong more often than
they are right are removed from the set used by the
system.
A further filtering of the prospective rules is
done in which gappy multiple token rules that
match the same word type more than once are re-
moved. Those prospective rules are created to
match cases in the supplied training data where the
a repetition has occurred and then encoded by the
annotators as a single cue (and thus scope) of nega-
tion5.
The single token and multiple token rules match
both the word string feature (ignoring case) and the
part-of-speech (POS) feature of each token. And
because a single token rule might also match a cue
that belongs to a multiple token rule, multiple to-
ken rules are checked first.
Affix rules are of two types: prefix cues and
non-prefix cues. The distinction is that while pre-
fix cues must match starting at the beginning of the
word string, the non-prefix cues may have a suffix
following them in the word string that is not part of
the cue. Affix rules only match against the word
</bodyText>
<footnote confidence="0.9993876">
2http://mallet.cs.umass.edu/
3http://groovy.codehaus.org/
4https://github.com/jimwhite/SEMST2012
5Such as baskervilles12 174: “Not a whisper, not a rustle,
rose...” which has a cue annotation of “Not” gap “not”.
</footnote>
<page confidence="0.997338">
336
</page>
<bodyText confidence="0.999879742857143">
string feature of the tokens and are insensitive to
the POS feature.
In order to generalize the affix rules, sets are ac-
cumulated of both base word strings (the substring
following a prefix cue or substring preceding a
non-prefix cue) and suffixes (the substring follow-
ing non-prefix cues, if any). In addition, all other
word strings and lemma strings in the training cor-
pus that are at least four characters long are added
to the set of possible base word strings6. A set of
negative word strings is also accumulated in the
second pass of the rule training to condition against
false positive matches for each affix rule.
A prefix cue rule will match a token with a
word string that starts with the cue string and is
followed by any of the strings in the base word set.
Similarly a suffix cue rule will match a token
whose word string contains the cue string preceded
by a string in the base word set and is either at the
end of the string or is followed by one of the
strings in the suffix string set. Affix rules, unlike
the other cue-matching rules, also output the string
for matched base word as the value of the scope for
the matched token. In any case, if the token’s word
string is in the negative word string set for the rule
then it will not be matched.
Following submission of the system outputs for
the shared tasked I discovered that a hand written
regular expression rule that filters out the (poten-
tial) cues detected for “(be|have) no doubt” and
“none the (worse|less)” was inadvertently included
in the system. Although those rules could be
learned automatically from the training data (and
such was my intention), the system as reported
here does not currently do so.
</bodyText>
<subsectionHeader confidence="0.999296">
3.2 Negation Scope Resolution
</subsectionHeader>
<bodyText confidence="0.999892444444444">
For each cue detected, scope resolution is per-
formed as a ternary classification of each token in
the sentence as to whether it is part of a cue, part of
a scope, or neither. The classifier is the CRF se-
quence tagger implemented in the SimpleTagger
class of the MALLET toolkit (McCallum, 2002).
Training is performed using the gold-standard data
including the gold cues. The output of the tagger is
not used to determine the scope value of a token in
</bodyText>
<footnote confidence="0.9206058">
6This “longer than four character” rule was manually created
to correct for over-generalization observed in the training data.
If the affix rule learner selected this value using the correct/in-
correct counts as it does with the other rule parameters then
this bit of language-specific tweaking would be unnecessary.
</footnote>
<bodyText confidence="0.9804055">
those cases where an affix rule in the cue detector
has matched a token and therefore has supplied the
matched base word string as the value of the scope
for the token.
For features that are computed in terms of the
cue token, the first (lowest numbered) token
marked as a cue is used when there is more than
one cue token for the scope.
Features used by the scope CRF sequence tag-
ger are:
</bodyText>
<listItem confidence="0.6939175">
• Of the per-token data: word string in low-
ercase, lemma string in lowercase, part-of-
speech (POS) tag, binary flag indicating
whether the token is a cue, a binary flag in-
dicating whether the token is at the edge of
its parent non-terminal node or an internal
sibling, a binary flag indicating whether
the token is a cue token, and relative posi-
tion to the cue token in number of tokens.
• Of the cue token data: word string in low-
ercase, lemma string in lowercase, and
POS tag.
• Of the path through the syntax tree from
the cue token: an ordered list of the non-
</listItem>
<bodyText confidence="0.973995777777778">
terminal labels of each node up the tree to
the lowest common parent, an ordered list
of the non-terminal labels of each node
down the tree from that lowest common
parent, a path relation value consisting of
the label of the lowest common parent
node concatenated with an indication of
the relative position of the paths to the cue
and token in terms of sibling order.
</bodyText>
<subsectionHeader confidence="0.985578">
3.3 Negated Event Resolution
</subsectionHeader>
<bodyText confidence="0.999983333333333">
Detection of the negated event or property is per-
formed using the same CRF sequence tagger and
features used for scope detection. The only differ-
ence is that the token classification is in terms of
whether each token in the sentence is part of a fac-
tual negated event for each negation cue.
</bodyText>
<subsectionHeader confidence="0.984368">
3.4 Feature Set Selection
</subsectionHeader>
<bodyText confidence="0.999885857142857">
A comparison of the end-to-end performance of
this system using several different sets of per token
feature choices for the scope and negated event
classifiers is shown in Table 1. In each case the
training data is the entire training data and the dev
data is the entire dev data supplied by the organiz-
ers for this shared task. The scores are computed
</bodyText>
<page confidence="0.994156">
337
</page>
<table confidence="0.999339111111111">
Gold System TP FP FN Precision (%) Recall (%) F1 (%)
Baseline (train) 984 1034 382 56 602 87.21 38.82 53.73
(dev) 173 164 34 9 139 79.07 19.65 31.48
Set 1 (train) 984 1034 524 56 460 90.34 53.25 67.00
(dev) 173 164 60 9 113 86.96 34.68 49.59
Set 2 (train) 984 1034 666 56 318 92.24 67.68 78.07
(dev) 173 164 61 9 112 87.14 35.26 50.21
System (train) 984 1034 644 56 340 92.00 65.45 76.49
(dev) 173 164 68 9 105 88.31 39.31 54.40
</table>
<tableCaption confidence="0.999979">
Table 1: Comparison of full negation scores for various feature sets.
</tableCaption>
<bodyText confidence="0.999940153846154">
by the evaluation program also supplied by the or-
ganizers. The baseline features are those provided
in the data, with the exception of the syntactic tree
fragment: word string in lowercase, lemma in low-
ercase, and POS tag. The “set 1” features are the
remainder of the features described in section 3.2,
with the exception of those of the path through the
syntax tree from the cue token. The “set 2” fea-
tures are the three baseline features plus the three
features of the path through the syntax tree from
the cue token: list of non-terminal labels from cue
up to the lowest common parent, lowest common
parent label concatenated with the relative distance
in nodes between the siblings, list of non-terminals
from the lowest common parent down to the token.
The “system” feature set is the union of set 1 and
set 2, and is the one used by the submitted system.
The baseline score is an F1 of 31.5% (P=79.1%,
R=19.7%) on the dev data. Using either feature set
1 or 2 results in substantially better performance.
They achieve nearly the same score on the dev set
with an F1 of 50±0.5% (P=87±0.2%, R=35±0.3%)
in which the difference is that between one case of
true positive vs. false negative out of 173. The
combination of those feature sets is better still
though with an F1 of 54.4% (P=88.3%, R=39.3%).
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999970884615385">
Table 2 presents the scores computed for the sys-
tem output on the held-out evaluation data. The F1
for full negation is 48.1% (P=74%, R=35.6%),
which is noticeably lower than that seen for the
dev data (54.4%). That reduction is to be expected
because the dev data was used for system tuning.
There was also evidence of significant over-fitting
to the training data because the F1 for that was
76.5% (P=92%, R=65.5%). The largest compo-
nent of the fall off in performance is in the recall.
The worst performing component of the system
is the negated event detection which has an F1 of
54.3% (P=58%, R=51%) on the evaluation data.
One contributor to low precision for the negated
event detector is that the root word of an affix cue
is always output as a negated event, bypassing the
negated event CRF sequence classifier. In the
combined training and dev data there is a total of
1157 gold cues (and scopes) of which 738 (63.8%)
are annotated as having a negated event. Of the
1198 cues the system outputs for that data, 188
(15.7%) are affix cues, each of which will also be
output as a negated event. Therefore it would be
reasonable to expect that approximately 16
(27.7%) of the false positives for the negated event
in the evaluation (60) are due to that behavior.
</bodyText>
<table confidence="0.9987047">
Gold System TP FP FN Precision (%) Recall (%) F1 (%)
Cues 264 285 243 33 21 88.04 92.05 90.00
Scopes (no cue match) 249 270 158 33 89 82.90 64.26 72.40
Scope tokens (no cue match) 1805 1816 1512 304 293 83.26 83.77 83.51
Negated (no cue match) 173 154 83 60 80 58.04 50.92 54.25
Full negation 264 285 94 33 170 74.02 35.61 48.09
Cues B 264 285 243 33 21 85.26 92.05 88.52
Scopes B (no cue match) 249 270 158 33 89 59.26 64.26 61.66
Negated B (no cue match) 173 154 83 60 80 53.9 50.92 52.37
Full negation B 264 285 94 33 170 32.98 35.61 34.24
</table>
<tableCaption confidence="0.99871">
Table 2: System evaluation on held-out data.
</tableCaption>
<page confidence="0.998052">
338
</page>
<sectionHeader confidence="0.998572" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99997068">
This paper describes the system I implemented for
the closed track of the *SEM 2012 Shared Task for
negation cue, scope, and event resolution. The sys-
tem’s performance on the held-out evaluation data,
an F1 of 48.09% (P=74.02%, R=35.61%) for the
full negation, relative to the other entries for the
task is fourth among the six teams that partici-
pated.
The strongest part of this system is the scope re-
solver which performs at a level near that of the
best-performing systems in this shared task. I
think it is likely that the performance on scope res-
olution would be equivalent to them with a better
negation cue detector. That is supported by the “no
cue match” version of the scope resolution evalua-
tion for which this system has the highest F1
(72.4%).
Clearly the weakest link is the negated event
detector. Since one obvious source of error is that
the root word extracted when an affix cue is de-
tected is always output as a negated event, a
promising approach for improvement would be to
instead utilize that as a feature for the negated
event’s CRF sequence tagger so that they have a
chance to be filtered out in non-factual contexts.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99979375">
I want to thank Roser Morante and Eduardo
Blanco for organizing this task, the reviewers for
their thorough and very helpful suggestions, and
Emily Bender for her guidance.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999277568627451">
Shashank Agarwal and Hong Yu. 2010. Biomedical
negation scope detection with conditional random
fields. Journal of the American Medical Informatics As-
sociation, 17(6), 696–701.
doi:10.1136/jamia.2010.003228
Chapman, W. W., Bridewell, W., Hanbury, P., Cooper,
G. F., &amp; Buchanan, B. G.. 2001. A simple algorithm
for identifying negated findings and diseases in dis-
charge summaries. Journal of Biomedical Informatics,
34(5), 301–310. doi:10.1006/jbin.2001.1029
Andrew McCallum. 2002. MALLET: A Machine
Learning for Language Toolkit. Retrieved from
http://mallet.cs.umass.edu
Roser Morante and Eduardo Blanco. 2012. *SEM
2012 Shared Task: Resolving the Scope and Focus of
Negation. Proceedings of the First Joint Conference on
Lexical and Computational Semantics. Presented at the
*SEM 2012, Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A Met-
alearning Approach to Processing the Scope of Nega-
tion. Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning (CoNLL-2009)
(pp. 21–29). Boulder, Colorado: Association for Com-
putational Linguistics.
Roser Morante and Walter Daelemans. 2012. Conan-
Doyle-neg: Annotation of negation in Conan Doyle sto-
ries. Proceedings of the Eighth International Confer-
ence on Language Resources and Evaluation (LREC).
Pradeep G. Mutalik, Aniruddha Deshpande, and Prakash
M. Nadkarni. 2001. Use of general-purpose negation
detection to augment concept indexing of medical docu-
ments: a quantitative study using the UMLS. Journal of
the American Medical Informatics Association: JAMIA,
8(6), 598–609.
Lior Rokach, Roni Romano, and Oded Maimon. 2008.
Negation recognition in medical narrative reports.
Information Retrieval, 11(6), 499–538.
doi:10.1007/s10791-008-9061-0
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A Cascade Method for Detecting
Hedges and their Scope in Natural Language Text. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning (pp. 13–17). Upp-
sala, Sweden: Association for Computational Linguis-
tics.
Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong Cheng.
2010. Learning to Detect Hedges and their Scope Using
CRF. Proceedings of the Fourteenth Conference on
Computational Natural Language Learning (pp. 100–
105). Uppsala, Sweden: Association for Computational
Linguistics.
</reference>
<page confidence="0.999209">
339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918984">
<title confidence="0.999898">UWashington: Negation Resolution using Machine Learning Methods</title>
<author confidence="0.999925">James Paul</author>
<affiliation confidence="0.9980725">University of Department of Linguistics, Box</affiliation>
<address confidence="0.999926">Seattle, WA 98195, USA</address>
<email confidence="0.99986">jimwhite@uw.edu</email>
<abstract confidence="0.9946174">This paper reports on a simple system for resolving the scope of negation in the closed track of the *SEM 2012 Shared Task. Cue detection is performed using regular expression rules extracted from the training data. Both scope tokens and negated event tokens are resolved using a Conditional Random Field (CRF) sequence tagger – namely the SimpleTagger library in the MALLET machine learning toolkit. The full obtained for the task evaluation is 48.09% (P=74.02%, R=35.61%) which ranks this system fourth among the six submitted for the closed track.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shashank Agarwal</author>
<author>Hong Yu</author>
</authors>
<title>Biomedical negation scope detection with conditional random fields.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>6</issue>
<pages>696--701</pages>
<contexts>
<context position="4626" citStr="Agarwal and Yu, 2010" startWordPosition="727" endWordPosition="730">d from plain text as to whether they contain negation or not, while the requirement of this shared task for negation cue detection is to identify the particular token(s) or part of a token that signals the presence of negation. Furthermore, those systems only identify the scope of negation at the level of phrasal constituents, which is different than what is required for this task in which the scopes are not necessarily contiguous. Conditional Random Field (CRF) sequence taggers have been successfully applied to many scope resolution problems, including those of negation. The NegScope system (Agarwal and Yu, 2010) trains a CRF sequence tagger on labelled data to identify both the cue and scope of negation. However, that system only recognizes a whole word as a cue and does not recognize nor generalize negation cues which are affixes. There are also systems that use CRF sequence taggers for detection of hedge scopes (Tang et al, 2010, Zhao et al, 2010). Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). 3 System Description This system is implemented as a three stage cascade </context>
</contexts>
<marker>Agarwal, Yu, 2010</marker>
<rawString>Shashank Agarwal and Hong Yu. 2010. Biomedical negation scope detection with conditional random fields. Journal of the American Medical Informatics Association, 17(6), 696–701. doi:10.1136/jamia.2010.003228</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Chapman</author>
<author>W Bridewell</author>
<author>P Hanbury</author>
<author>G F Cooper</author>
<author>B G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>34</volume>
<issue>5</issue>
<pages>301--310</pages>
<contexts>
<context position="3527" citStr="Chapman et al, 2001" startWordPosition="551" endWordPosition="554">ted event (if any) (Morante and Daelemans, 2012). Their paper explains the motivations for the particular annotation decisions and describes in detail the guidelines, including many examples. 2 Related Work Recognition of phrases containing negation, particularly in the medical domain, using regular expressions has been described using several different approaches. Systems such as Negfinder (Mutalik et 335 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335–339, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics al, 2001) and NegEx (Chapman et al, 2001) use manually constructed rules to extract phrases from text and classify them as to whether they contain an expression of negation. Rokach et al evaluate several methods and show their highest level of performance (an F1 of 95.9 ± 1.9%) by using cascaded decision trees of regular expressions learned from labelled narrative medical reports (Rokach et al, 2008). Those systems perform a different function than that required for this task though. They classify phrases extracted from plain text as to whether they contain negation or not, while the requirement of this shared task for negation cue d</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Chapman, W. W., Bridewell, W., Hanbury, P., Cooper, G. F., &amp; Buchanan, B. G.. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 34(5), 301–310. doi:10.1006/jbin.2001.1029</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit. Retrieved from http://mallet.cs.umass.edu</title>
<date>2002</date>
<contexts>
<context position="10044" citStr="McCallum, 2002" startWordPosition="1658" endWordPosition="1659">ters out the (potential) cues detected for “(be|have) no doubt” and “none the (worse|less)” was inadvertently included in the system. Although those rules could be learned automatically from the training data (and such was my intention), the system as reported here does not currently do so. 3.2 Negation Scope Resolution For each cue detected, scope resolution is performed as a ternary classification of each token in the sentence as to whether it is part of a cue, part of a scope, or neither. The classifier is the CRF sequence tagger implemented in the SimpleTagger class of the MALLET toolkit (McCallum, 2002). Training is performed using the gold-standard data including the gold cues. The output of the tagger is not used to determine the scope value of a token in 6This “longer than four character” rule was manually created to correct for over-generalization observed in the training data. If the affix rule learner selected this value using the correct/incorrect counts as it does with the other rule parameters then this bit of language-specific tweaking would be unnecessary. those cases where an affix rule in the cue detector has matched a token and therefore has supplied the matched base word strin</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. Retrieved from http://mallet.cs.umass.edu</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>SEM 2012 Shared Task: Resolving the Scope and Focus of Negation.</title>
<date>2012</date>
<booktitle>Proceedings of the First Joint Conference on Lexical and Computational Semantics. Presented at the *SEM 2012,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2304" citStr="Morante and Blanco, 2012" startWordPosition="367" endWordPosition="370">s, diagnostic procedure reports, and medical records it is important to differentiate between statements about what is the case and what is not the case. The *SEM 2012 Shared Task is actually two related tasks run in parallel. The one this system was developed for is the identification of three features of negation: the cue, the scope, and the factual negated event (if any). The other task is concerned with the focus of negation. Detailed description of both subtasks, including definition of the relevant concepts and terminology (negation, cue, scope, event, and focus) appears in this volume (Morante and Blanco, 2012). Roser Morante and Eduardo Blanco describe the corpora provided to participants with numbers and examples, methods used used to process the data, and briefly describes each participant and analyzes the overall results. Annotation of the corpus was undertaken at the University of Antwerp and was performed on several Sherlock Holmes works of fiction written by Sir Arthur Conan Doyle. The corpus includes all sentences from the original text, not just those employing negation. Roser Morante and Walter Daelemans provide a thorough explanation of those gold annotations of negation cue, scope, and n</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. *SEM 2012 Shared Task: Resolving the Scope and Focus of Negation. Proceedings of the First Joint Conference on Lexical and Computational Semantics. Presented at the *SEM 2012, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>A Metalearning Approach to Processing the Scope of Negation.</title>
<date>2009</date>
<booktitle>Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</booktitle>
<pages>21--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado:</location>
<contexts>
<context position="5151" citStr="Morante and Daelemans, 2009" startWordPosition="819" endWordPosition="822">many scope resolution problems, including those of negation. The NegScope system (Agarwal and Yu, 2010) trains a CRF sequence tagger on labelled data to identify both the cue and scope of negation. However, that system only recognizes a whole word as a cue and does not recognize nor generalize negation cues which are affixes. There are also systems that use CRF sequence taggers for detection of hedge scopes (Tang et al, 2010, Zhao et al, 2010). Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). 3 System Description This system is implemented as a three stage cascade with the output from each of the first two stages included as input to the subsequent stage. The stages are ordered as cue detection, scope detection, and finally negated event detection. The format of the inputs and outputs for each stage use the shared task’s CoNLL-style file format. That simplifies the use of the supplied gold-standard data for training of each stage separately. Because this system was designed for the closed track of the shared task, it makes minimal language-specific assumptions and learns (nearly)</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. A Metalearning Approach to Processing the Scope of Negation. Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009) (pp. 21–29). Boulder, Colorado: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>ConanDoyle-neg: Annotation of negation in Conan Doyle stories.</title>
<date>2012</date>
<booktitle>Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="2955" citStr="Morante and Daelemans, 2012" startWordPosition="469" endWordPosition="472">ardo Blanco describe the corpora provided to participants with numbers and examples, methods used used to process the data, and briefly describes each participant and analyzes the overall results. Annotation of the corpus was undertaken at the University of Antwerp and was performed on several Sherlock Holmes works of fiction written by Sir Arthur Conan Doyle. The corpus includes all sentences from the original text, not just those employing negation. Roser Morante and Walter Daelemans provide a thorough explanation of those gold annotations of negation cue, scope, and negated event (if any) (Morante and Daelemans, 2012). Their paper explains the motivations for the particular annotation decisions and describes in detail the guidelines, including many examples. 2 Related Work Recognition of phrases containing negation, particularly in the medical domain, using regular expressions has been described using several different approaches. Systems such as Negfinder (Mutalik et 335 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335–339, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics al, 2001) and NegEx (Chapman et al, 2001) use manually constructed ru</context>
</contexts>
<marker>Morante, Daelemans, 2012</marker>
<rawString>Roser Morante and Walter Daelemans. 2012. ConanDoyle-neg: Annotation of negation in Conan Doyle stories. Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep G Mutalik</author>
<author>Aniruddha Deshpande</author>
<author>Prakash M Nadkarni</author>
</authors>
<title>Use of general-purpose negation detection to augment concept indexing of medical documents: a quantitative study using the UMLS.</title>
<date>2001</date>
<journal>Journal of the American Medical Informatics Association: JAMIA,</journal>
<volume>8</volume>
<issue>6</issue>
<pages>598--609</pages>
<marker>Mutalik, Deshpande, Nadkarni, 2001</marker>
<rawString>Pradeep G. Mutalik, Aniruddha Deshpande, and Prakash M. Nadkarni. 2001. Use of general-purpose negation detection to augment concept indexing of medical documents: a quantitative study using the UMLS. Journal of the American Medical Informatics Association: JAMIA, 8(6), 598–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lior Rokach</author>
<author>Roni Romano</author>
<author>Oded Maimon</author>
</authors>
<title>Negation recognition in medical narrative reports.</title>
<date>2008</date>
<journal>Information Retrieval,</journal>
<volume>11</volume>
<issue>6</issue>
<pages>499--538</pages>
<contexts>
<context position="3889" citStr="Rokach et al, 2008" startWordPosition="610" endWordPosition="613">hes. Systems such as Negfinder (Mutalik et 335 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 335–339, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics al, 2001) and NegEx (Chapman et al, 2001) use manually constructed rules to extract phrases from text and classify them as to whether they contain an expression of negation. Rokach et al evaluate several methods and show their highest level of performance (an F1 of 95.9 ± 1.9%) by using cascaded decision trees of regular expressions learned from labelled narrative medical reports (Rokach et al, 2008). Those systems perform a different function than that required for this task though. They classify phrases extracted from plain text as to whether they contain negation or not, while the requirement of this shared task for negation cue detection is to identify the particular token(s) or part of a token that signals the presence of negation. Furthermore, those systems only identify the scope of negation at the level of phrasal constituents, which is different than what is required for this task in which the scopes are not necessarily contiguous. Conditional Random Field (CRF) sequence taggers </context>
</contexts>
<marker>Rokach, Romano, Maimon, 2008</marker>
<rawString>Lior Rokach, Roni Romano, and Oded Maimon. 2008. Negation recognition in medical narrative reports. Information Retrieval, 11(6), 499–538. doi:10.1007/s10791-008-9061-0</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buzhou Tang</author>
<author>Xiaolong Wang</author>
<author>Xuan Wang</author>
<author>Bo Yuan</author>
<author>Shixi Fan</author>
</authors>
<title>A Cascade Method for Detecting Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning</booktitle>
<pages>13--17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden:</location>
<contexts>
<context position="4951" citStr="Tang et al, 2010" startWordPosition="786" endWordPosition="789">hich is different than what is required for this task in which the scopes are not necessarily contiguous. Conditional Random Field (CRF) sequence taggers have been successfully applied to many scope resolution problems, including those of negation. The NegScope system (Agarwal and Yu, 2010) trains a CRF sequence tagger on labelled data to identify both the cue and scope of negation. However, that system only recognizes a whole word as a cue and does not recognize nor generalize negation cues which are affixes. There are also systems that use CRF sequence taggers for detection of hedge scopes (Tang et al, 2010, Zhao et al, 2010). Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). 3 System Description This system is implemented as a three stage cascade with the output from each of the first two stages included as input to the subsequent stage. The stages are ordered as cue detection, scope detection, and finally negated event detection. The format of the inputs and outputs for each stage use the shared task’s CoNLL-style file format. That simplifies the use of the supplie</context>
</contexts>
<marker>Tang, Wang, Wang, Yuan, Fan, 2010</marker>
<rawString>Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, and Shixi Fan. 2010. A Cascade Method for Detecting Hedges and their Scope in Natural Language Text. Proceedings of the Fourteenth Conference on Computational Natural Language Learning (pp. 13–17). Uppsala, Sweden: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhao</author>
<author>Chengjie Sun</author>
<author>Bingquan Liu</author>
<author>Yong Cheng</author>
</authors>
<title>Learning to Detect Hedges and their Scope Using CRF.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning</booktitle>
<pages>100--105</pages>
<institution>Uppsala, Sweden: Association for Computational Linguistics.</institution>
<contexts>
<context position="4970" citStr="Zhao et al, 2010" startWordPosition="790" endWordPosition="793">than what is required for this task in which the scopes are not necessarily contiguous. Conditional Random Field (CRF) sequence taggers have been successfully applied to many scope resolution problems, including those of negation. The NegScope system (Agarwal and Yu, 2010) trains a CRF sequence tagger on labelled data to identify both the cue and scope of negation. However, that system only recognizes a whole word as a cue and does not recognize nor generalize negation cues which are affixes. There are also systems that use CRF sequence taggers for detection of hedge scopes (Tang et al, 2010, Zhao et al, 2010). Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). 3 System Description This system is implemented as a three stage cascade with the output from each of the first two stages included as input to the subsequent stage. The stages are ordered as cue detection, scope detection, and finally negated event detection. The format of the inputs and outputs for each stage use the shared task’s CoNLL-style file format. That simplifies the use of the supplied gold-standard dat</context>
</contexts>
<marker>Zhao, Sun, Liu, Cheng, 2010</marker>
<rawString>Qi Zhao, Chengjie Sun, Bingquan Liu, and Yong Cheng. 2010. Learning to Detect Hedges and their Scope Using CRF. Proceedings of the Fourteenth Conference on Computational Natural Language Learning (pp. 100– 105). Uppsala, Sweden: Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>