<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.9754645">
PART-OF-SPEECH TAGGING USING
A VARIABLE MEMORY MARKOV MODEL
</title>
<author confidence="0.934656">
Hinrich Schiitze
</author>
<affiliation confidence="0.836615">
Center for the Study of
Language and Information
</affiliation>
<address confidence="0.769439">
Stanford, CA 94305-4115
</address>
<email confidence="0.70796">
Internet: schuetzencsli.stanford.edu
</email>
<author confidence="0.976853">
Yoram Singer
</author>
<affiliation confidence="0.876641666666667">
Institute of Computer Science and
Center for Neural Computation
Hebrew University, Jerusalem 91904
</affiliation>
<email confidence="0.579606">
Internet: singerOcs.huji.ac.il
</email>
<sectionHeader confidence="0.985207" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999751">
We present a new approach to disambiguating syn-
tactically ambiguous words in context, based on
Variable Memory Markov (VMM) models. In con-
trast to fixed-length Markov models, which predict
based on fixed-length histories, variable memory
Markov models dynamically adapt their history
length based on the training data, and hence may
use fewer parameters. In a test of a VMM based
tagger on the Brown corpus, 95.81% of tokens are
correctly classified.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999969">
Many words in English have several parts of speech
(P05). For example &amp;quot;book&amp;quot; is used as a noun in
&amp;quot;She read a book.&amp;quot; and as a verb in &amp;quot;She didn&apos;t
book a trip.&amp;quot; Part-of-speech tagging is the prob-
lem of determining the syntactic part of speech of
an occurrence of a word in context. In any given
English text, most tokens are syntactically am-
biguous since most of the high-frequency English
words have several parts of speech. Therefore, a
correct syntactic classification of words in context
is important for most syntactic and other higher-
level processing of natural language text.
Two stochastic methods have been widely
used for POS tagging: fixed order Markov models
and Hidden Markov models. Fixed order Markov
models are used in (Church, 1989) and (Charniak
et al., 1993). Since the order of the model is as-
sumed to be fixed, a short memory (small order) is
typically used, since the number of possible combi-
nations grows exponentially. For example, assum-
ing there are 184 different tags, as in the Brown
corpus, there are 1843 6, 229, 504 different or-
der 3 combinations of tags (of course not all of
these will actually occur, see (Weischedel et al.,
1993)). Because of the large number of param-
eters higher-order fixed length models are hard
to estimate. (See (Brill, 1993) for a rule-based
approach to incorporating higher-order informa-
tion.) In a Hidden Markov Model (HMM) (Jelinek,
1985; Kupiec, 1992), a different state is defined
for each POS tag and the transition probabilities
and the output probabilities are estimated using
the EM (Dempster et al., 1977) algorithm, which
guarantees convergence to a local minimum (Wu,
1983). The advantage of an HMM is that it can be
trained using untagged text. On the other hand,
the training procedure is time consuming, and a
fixed model (topology) is assumed. Another dis-
advantage is due to the local convergence proper-
ties of the EM algorithm. The solution obtained
depends on the initial setting of the model&apos;s pa-
rameters, and different solutions are obtained for
different parameter initialization schemes. This
phenomenon discourages linguistic analysis based
on the output of the model.
We present a new method based on vari-
able memory Markov models (VMM) (Ron et al.,
1993; Ron et al., 1994). The VMM is an approx-
imation of an unlimited order Markov source. It
can incorporate both the static (order 0) and dy-
namic (higher-order) information systematically,
while keeping the ability to change the model due
to future observations. This approach is easy to
implement, the learning algorithm and classifica-
tion of new tags are computationally efficient, and
the results achieved, using simplified assumptions
for the static tag probabilities, are encouraging.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
VARIABLE MEMORY MARKOV
MODELS
</sectionHeader>
<bodyText confidence="0.999871083333333">
Markov models are a natural candidate for lan-
guage modeling and temporal pattern recognition,
mostly due to their mathematical simplicity. How-
ever, it is obvious that finite memory Markov mod-
els cannot capture the recursive nature of lan-
guage, nor can they be trained effectively with
long memories. The notion of variable context
length also appears naturally in the context of uni-
versal coding (Rissanen, 1978; Rissanen and Lang-
don, 1981). This information theoretic notion is
now known to be closely related to efficient mod-
eling (Rissanen, 1988). The natural measure that
</bodyText>
<page confidence="0.997544">
181
</page>
<bodyText confidence="0.998524096491228">
appears in information theory is the description
length, as measured by the statistical predictabil-
ity via the Kullback-Leibler (KL) divergence.
The VMM learning algorithm is based on min-
imizing the statistical prediction error of a Markov
model, measured by the instantaneous KL diver-
gence of the following symbols, the current statisti-
cal surprise of the model. The memory is extended
precisely when such a surprise is significant, until
the overall statistical prediction of the stochastic
model is sufficiently good. For the sake of sim-
plicity, a POS tag is termed a symbol and a se-
quence of tags is called a string. We now briefly de-
scribe the algorithm for learning a variable mem-
ory Markov model. See (Ron et al., 1993; Ron et
al., 1994) for a more detailed description of the
algorithm.
We first introduce notational conventions and
define some basic concepts. Let E be a finite al-
phabet. Denote by E* the set of all strings over
E. A string s, over E* of length n, is denoted
by s = sis2 ...sn. We denote by e the empty
string. The length of a string s is denoted by
is&apos; and the size of an alphabet E is denoted by
lEi• Let Pre f ix(s) = s1s2 . sn_ I denote the
longest prefix of a string s, and let Pre fix*(s)
denote the set of all prefixes of s, including the
empty string. Similarly, Su f fix(s) = s2s3 sn
and Suffix* (s) is the set of all suffixes of s. A set
of strings is called a suffix (prefix) free set if, Vs E
S : SnSuffix*(s) = 0 (SnPrefie(s) = 0).
We call a probability measure P, over the strings
in E* proper if P(e) = 1, and for every string s,
EuEs P(so) = P(s). Hence, for every prefix free
set S, EdEs P(s) &lt; 1, and specifically for every
integer n &gt; 0, E,EE,, P(s) = 1.
A prediction suffix tree T over E, is a tree
of degree iEi. The edges of the tree are labeled
by symbols from E, such that from every internal
node there is at most one outgoing edge labeled
by each symbol. The nodes of the tree are labeled
by pairs (s, 7,) where s is the string associated
with the walk starting from that node and end-
ing in the root of the tree, and -y, : E [0, 1]
is the output probability function of s satisfying
EaeE -y(i) = 1. A prediction suffix tree induces
probabilities on arbitrarily long strings in the fol-
lowing manner. The probability that T gener-
ates a string w = to]. w2 wn in E&apos;, denoted by
Pr(w), is fI173.-1(wi), where s°= e, and for
1 &lt; i &lt; n — 1, si is the string labeling the deep-
est node reached by taking the walk corresponding
to w1 wi starting at the root of T. By defini-
tion, a prediction suffix tree induces a proper mea-
sure over E*, and hence for every prefix free set
of strings {wl, , wm}, (wi) &lt; 1, and
specifically for n &gt; 1, then E.,EE&apos;PT(s) = 1.
A Probabilistic Finite Automaton (PFA) A is
a 5-tuple (Q, E, 7, 7,7r), where Q is a finite set of
n states, E is an alphabet of size k,T:QxE Q
is the transition function, 7 Q x E [0, 1] is the
output probability function, and Ir : Q [0, 1] is
the probability distribution over the start states.
The functions ^y and 7r must satisfy the following
requirements: for every q E Q, a) =
1, and EgEQ 7r(q) = 1. The probability that
A generates a string s = sis2 ...sn E En
is PA(s) = E9o€C1 (q° Fr_ -y (qi , si), where
qi+1 = r(qi , si). T can be extended to be de-
fined on Q x E* as follows: r(q, s1s2 . • • SI) =
r(r(q, si se) = r(r(q, Pre fix(s)), se).
The distribution over the states, 7r, can be re-
placed by a single start state, denoted by c such
that r(c, s) = 7r(q), where s is the label of the state
q. Therefore, 7r(c) = 1 and 7r(q) = 0 if q c.
For POS tagging, we are interested in learning
a sub-class of finite state machines which have the
following property. Each state in a machine M
belonging to this sub-class is labeled by a string
of length at most L over E, for some L &gt; 0. The
set of strings labeling the states is suffix free. We
require that for every two states q1, q2 E Q and
for every symbol a E E, if r(q1 , a) = q2 and q1
is labeled by a string s1, then q2 is labeled by
a string S2 which is a suffix of s1 • a. Since the
set of strings labeling the states is suffix free, if
there exists a string having this property then it
is unique. Thus, in order that r be well defined on
a given set of string S, not only must the set be
suffix free, but it must also have the property, that
for every string s in the set and every symbol a,
there exists a string which is a suffix of so. For our
convenience, from this point on, if q is a state in
Q then q will also denote the string labeling that
state.
A special case of these automata is the case
in which Q includes all IEIL strings of length L.
These automata are known as Markov processes of
order L. We are interested in learning automata
for which the number of states, n, is much smaller
than IV&apos;, which means that few states have long
memory and most states have a short one. We re-
fer to these automata as variable memory Markov
(VMM) processes. In the case of Markov processes
of order L, the identity of the states (i.e. the iden-
tity of the strings labeling the states) is known and
learning such a process reduces to approximating
the output probability function.
Given a sample consisting of m POS tag se-
quences of lengths /1,12, ,l,, we would like to
find a prediction suffix tree that will have the
same statistical properties as the sample and thus
can be used to predict the next outcome for se-
c.„uences generated by the same source. At each
</bodyText>
<page confidence="0.991285">
182
</page>
<bodyText confidence="0.996279875">
stage we can transform the tree into a variable
memory Markov process. The key idea is to iter-
atively build a prediction tree whose probability
measure equals the empirical probability measure
calculated from the sample.
We start with a tree consisting of a single
node and add nodes which we have reason to be-
lieve should be in the tree. A node a-s, must be
added to the tree if it statistically differs from its
parent node s. A natural measure to check the
statistical difference is the relative entropy (also
known as the Kullback-Leibler (KL) divergence)
(Kullback, 1959), between the conditional proba-
bilities P(.Js) and P(1a.$). Let X be an obser-
vation space and Pi, P2 be probability measures
over X then the KL divergence between P1 and
</bodyText>
<equation confidence="0.944617">
P2 1S, DKL(P1I1P2) ExEX Pi (X) log pPZ. In
</equation>
<bodyText confidence="0.999956444444444">
our case, the KL divergence measures how much
additional information is gained by using the suf-
fix as for prediction instead of the shorter suffix s.
There are cases where the statistical difference is
large yet the probability of observing the suffix as
itself is so small that we can neglect those cases.
Hence we weigh the statistical error by the prior
probability of observing us. The statistical error
measure in our case is,
</bodyText>
<equation confidence="0.400164">
Err(as, s)
</equation>
<listItem confidence="0.994170333333333">
• P(0&amp;quot; s)Dx (13(las)I1P(.ls))
• P(6 s) EcriEE P(Gr/ las) log Pp(r0.1,117))
• Ea&apos;EE P(0-so- log p(aP,S:)Pas) .
</listItem>
<bodyText confidence="0.9999883">
Therefore, a node s is added to the tree if the sta-
tistical difference (defined by Err(o-s, s)) between
the node and its parrent s is larger than a prede-
termined accuracy c. The tree is grown level by
level, adding a son of a given leaf in the tree when-
ever the statistical error is large. The problem is
that the requirement that a node statistically dif-
fers from its parent node is a necessary condition
for belonging to the tree, but is not sufficient. The
leaves of a prediction suffix tree must differ from
their parents (or they are redundant) but internal
nodes might not have this property. Therefore,
we must continue testing further potential descen-
dants of the leaves in the tree up to depth L. In
order to avoid exponential grow in the number of
strings tested, we do not test strings which belong
to branches which are reached with small prob-
ability. The set of strings, tested at each step,
is denoted by S, and can be viewed as a kind of
frontier of the growing tree T.
</bodyText>
<sectionHeader confidence="0.7020805" genericHeader="method">
USING A VMM FOR POS
TAGGING
</sectionHeader>
<bodyText confidence="0.999860333333333">
We used a tagged corpus to train a VMM. The
syntactic information, i.e. the probability of a spe-
cific word belonging to a tag class, was estimated
using maximum likelihood estimation from the in-
dividual word counts. The states and the transi-
tion probabilities of the Markov model were de-
termined by the learning algorithm and tag out-
put probabilities were estimated from word counts
(the static information present in the training cor-
pus). The whole structure, for two states, is de-
picted in Fig. 1. Si and S1+1 are strings of tags cor-
responding to states of the automaton. P(tilSi)
is the probability that tag ti will be output by
state Si and P(ti+i iSi+i) is the probability that
the next tag ti+1, is the output of state S1+1.
</bodyText>
<figure confidence="0.515555">
P(Si+11Si)
</figure>
<figureCaption confidence="0.9594805">
Figure 1: The structure of the VMM based POS
tagger.
</figureCaption>
<listItem confidence="0.7636877">
When tagging a sequence of words wi,„, we
want to find the tag sequence ti,„ that is most
likely for wi,„. We can maximize the joint proba-
bility of wi,n and t1,,, to find this sequence:i
• arg
poi
• arg maxi, p(&apos;v 1.. )
• arg maxti.,,P(ti,n) Wl,n)
P(ti,, win) can be expressed as a product of con-
ditional probabilities as follows:
</listItem>
<equation confidence="0.974268666666667">
Wl,n) =
P(ti)P(wilti)P(i2Iti, W1)P(W21t1,2) tV1)
Wl,n-1)P(W0 Itl,n,
= P(ti wi i)P(wi It w_
1)
1=1
</equation>
<bodyText confidence="0.9998985">
With the simplifying assumption that the proba-
bility of a tag only depends on previous tags and
that the probability of a word only depends on its
tags, we get:
</bodyText>
<equation confidence="0.757454">
P(ti,n, Wl,n) =
</equation>
<bodyText confidence="0.9567365">
Given a variable memory Markov model M,
P(tilti,i_i) is estimated by P(tiiSi_i, M) where
&apos;Part of the following derivation is adapted from
(Charniak et al., 1993).
</bodyText>
<equation confidence="0.678089">
T(w1)
</equation>
<page confidence="0.988766">
183
</page>
<bodyText confidence="0.998358166666667">
= r(e, t1,1), since the dynamics of the sequence
are represented by the transition probabilities of
the corresponding automaton. The tags 11,„ for
a sequence of words wi,,, are therefore chosen ac-
cording to the following equation using the Viterbi
algorithm:
</bodyText>
<equation confidence="0.996554">
Tm(Wi,n) = arg maxi, P(ti ISi- M)P(wi Ili)
i=1
</equation>
<bodyText confidence="0.716360333333333">
We estimate P(wilti) indirectly from P(ti Iwi) us-
ing Bayes&apos; Theorem:
p (dio = P(tvip)Vit)ilwi)
</bodyText>
<subsubsectionHeader confidence="0.449901">
In
</subsubsectionHeader>
<bodyText confidence="0.999883704545454">
The terms P(w1) are constant for a given sequence
wi and can therefore be omitted from the maxi-
mization. We perform a maximum likelihood es-
timation for P(ti) by calculating the relative fre-
quency of Li in the training corpus. The estima-
tion of the static parameters P(ti(wi) is described
in the next section.
We trained the variable memory Markov
model on the Brown corpus (Francis and KuCera,
1982), with every tenth sentence removed (a total
of 1,022,462 tags). The four stylistic tag modifiers
&amp;quot;FW&amp;quot; (foreign word), &amp;quot;TL&amp;quot; (title), &amp;quot;NC&amp;quot; (cited
word), and &amp;quot;HL&amp;quot; (headline) were ignored reduc-
ing the complete set of 471 tags to 184 different
tags.
The resulting automaton has 49 states: the
null state (0, 43 first order states (one symbol
long) and 5 second order states (two symbols
long). This means that 184-43=141 states were
not (statistically) different enough to be included
as separate states in the automaton. An analy-
sis reveals two possible reasons. Frequent symbols
such as &amp;quot;ABN&amp;quot; (&amp;quot;half&amp;quot;, &amp;quot;all&amp;quot;, &amp;quot;many&amp;quot; used as pre-
quantifiers, e.g. in &amp;quot;many a younger man&amp;quot;) and
&amp;quot;DTI&amp;quot; (determiners that can be singular or plu-
ral, &amp;quot;any&amp;quot; and &amp;quot;some&amp;quot;) were not included because
they occur in a variety of diverse contexts or often
precede unambiguous words. For example, when
tagged as &amp;quot;ABN&amp;quot; &amp;quot;half&amp;quot;, &amp;quot;all&amp;quot;, and &amp;quot;many&amp;quot; tend
to occur before the unambiguous determiners &amp;quot;a&amp;quot;,
&amp;quot;an&amp;quot; and &amp;quot;the&amp;quot;.
Some rare tags were not included because they
did not improve the optimization criterion, min-
imum description length (measured by the KL-
divergence). For example, &amp;quot;HVZ*&amp;quot; (&amp;quot;hasn&apos;t&amp;quot;) is
not a state although a following &amp;quot;- ed&amp;quot; form is al-
ways disambiguated as belonging to class &amp;quot;VBN&amp;quot;
(past participle). But since this is a rare event, de-
scribing all &amp;quot;HVZ* VBN&amp;quot; sequences separately is
cheaper than the added complexity of an automa-
ton with state &amp;quot;HVZ*&amp;quot;. We in fact lost some ac-
curacy in tagging because of the optimization cri-
terion: Several &amp;quot;-ed&amp;quot; forms after forms of &amp;quot;have&amp;quot;
were mistagged as &amp;quot;VBD&amp;quot; (past tense).
</bodyText>
<table confidence="0.99928125">
transition to one-symbol two-symbol
state state
NN JJ: 0.45 AT JJ: 0.69
IN JJ: 0.06 AT JJ: 0.004
IN NN: 0.27 AT NN: 0.35
NN: 0.14 AT NN: 0.10
NN VBN: 0.08 AT VBN: 0.48
IN VBN: 0.35 AT VBN: 0.003
NN CC: 0.12 JJ CC: 0.04
JJ CC: 0.09 JJ CC: 0.58
VB RB: 0.05 MD RB: 0.48
VBN RB: 0.08 MD RB: 0.0009
</table>
<tableCaption confidence="0.984059">
Table 1: States for which the statistical predic-
</tableCaption>
<bodyText confidence="0.997579272727273">
tion is significantly different when using a longer
suffix for prediction. Those states are identified
automatically by the VMM learning algorithm. A
better prediction and classification of POS-tags is
achieved by adding those states with only a small
increase in the computation time.
The two-symbol states were &amp;quot;AT JJ&amp;quot;, &amp;quot;AT
NN&amp;quot;, &amp;quot;AT VBN&amp;quot;, &amp;quot;JJ CC&amp;quot;, and &amp;quot;MD RB&amp;quot; (ar-
ticle adjective, article noun, article past partici-
ple, adjective conjunction, modal adverb). Ta-
ble 1 lists two of the largest differences in transi-
tion probabilities for each state. The varying tran-
sition probabilities are based on differences be-
tween the syntactic constructions in which the two
competing states occur. For example, adjectives
after articles (&amp;quot;AT JJ&amp;quot;) are almost always used
attributively which makes a following preposition
impossible and a following noun highly probable,
whereas a predicative use favors modifying prepo-
sitional phrases. Similarly, an adverb preceded by
a modal (&amp;quot;MD RB&amp;quot;) is followed by an infinitive
(&amp;quot;VB&amp;quot;) half the time, whereas other adverbs oc-
cur less often in pre-infinitival position. On the
other hand, a past participle is virtually impossi-
ble after &amp;quot;MD RB&amp;quot; whereas adverbs that are not
preceded by modals modify past participles quite
often.
While it is known that Markov models of order
2 give a slight improvement over order-1 models
(Charniak et al., 1993), the number of parameters
in our model is much smaller than in a full order-2
Markov model (49*184 = 9016 vs. 184*184*184 =
6,229,504).
</bodyText>
<sectionHeader confidence="0.9991705" genericHeader="method">
ESTIMATION OF THE STATIC
PARAMETERS
</sectionHeader>
<bodyText confidence="0.8428926">
We have to estimate the conditional probabilities
P(tilw3), the probability that a given word w3 will
appear with tag in order to compute the static
parameters P(wilti) used in the tagging equations
described above. A first approximation would be
</bodyText>
<page confidence="0.987847">
184
</page>
<bodyText confidence="0.9674401">
C(wi)
where C(ti, wj) is the number of times ti is tagged
as ug in the training text and C(ug) is the num-
ber of times iv./ occurs in the training text. How-
ever, some form of smoothing is necessary, since
any new text will contain new words, for which
C(ug) is zero. Also, words that are rare will only
occur with some of their possible parts of speech
in the training text. One solution to this problem
is Good-Turing estimation:
</bodyText>
<listItem confidence="0.711591">
• • C(ti, wj) + 1
</listItem>
<equation confidence="0.977666">
P(taiwl) c(tvi)
</equation>
<bodyText confidence="0.999979103448276">
where I is the number of tags, 184 in our case.
It turns out that Good-Turing is not appropri-
ate for our problem. The reason is the distinction
between closed-class and open-class words. Some
syntactic classes like verbs and nouns are produc-
tive, others like articles are not. As a consequence,
the probability that a new word is an article is
zero, whereas it is high for verbs and nouns. We
need a smoothing scheme that takes this fact into
account.
Extending an idea in (Charniak et al., 1993),
we estimate the probability of tag conversion to
find an adequate smoothing scheme. Open and
closed classes differ in that words often add a tag
from an open class, but rarely from a closed class.
For example, a word that is first used as a noun
will often be used as a verb subsequently, but
closed classes such as possessive pronouns (&amp;quot;my&amp;quot;,
&amp;quot;her&amp;quot;, &amp;quot;his&amp;quot;) are rarely used with new syntactic
categories after the first few thousand words of the
Brown corpus. We only have to take stock of these
&amp;quot;tag conversions&amp;quot; to make informed predictions on
new tags when confronted with unseen text. For-
mally, let WIk be the set of words that have been
seen with ti , but not with ik in the training text up
to word wi. Then we can estimate the probability
that a word with tag t&apos; will later be seen with tag
ik as the proportion of words allowing tag ti but
not tk that later add ik :
</bodyText>
<equation confidence="0.9175975">
Pin(i • k) =
1{n11&lt;n&lt;m A W.EWrknW:fik A tn=tk}i
</equation>
<bodyText confidence="0.999440307692308">
This formula also applies to words we haven&apos;t seen
so far, if we regard such words as having occurred
with a special tag &amp;quot;U&amp;quot; for &amp;quot;unseen&amp;quot;. (In this case,
Wirj&apos;-&apos;k is the set of words that haven&apos;t occurred up
to 1.) An-4(U --+ k) then estimates the probability
that an unseen word has tag i&amp;quot;. Table 2 shows
the estimates of tag conversion we derived from
our training text for 1 = 1022462 — 100000, m =
1022462, where 1022462 is the number of words in
the training text. To avoid sparse data problems
we assumed zero probability for types of tag con-
version with less than 100 instances in the training
set.
</bodyText>
<table confidence="0.998714">
tag conversion estimated probability
U NN 0.29
•—o JJ 0.13
U —o NNS 0.12
U --o NP 0.08
U --o VBD 0.07
U •—■ VBG 0.07
U --■ VBN 0.06
U --o VB 0.05
U—. RB 0.05
U —o VBZ 0.01
U NP$ 0.01
VBD VBN 0.09
VBN VBD 0.05
VB --o NN 0.05
NN --o VB 0.01
</table>
<tableCaption confidence="0.990273">
Table 2: Estimates for tag conversion
</tableCaption>
<bodyText confidence="0.98903">
Our smoothing scheme is then the following
heuristic modification of Good-Turing:
</bodyText>
<equation confidence="0.992375333333333">
C(ti, wj) + EkiETI Phn(ki
P(tilwi) .\
+ EkiET1,k2ET Pirn(ki k2)
</equation>
<bodyText confidence="0.999952">
where Ti is the set of tags that wj has in the train-
ing set and T is the set of all tags. This scheme
has the following desirable properties:
</bodyText>
<listItem confidence="0.868961583333333">
• As with Good-Turing, smoothing has a small ef-
fect on estimates that are based on large counts.
• The difference between closed-class and open-
class words is respected: The probability for
conversion to a closed class is zero and is not
affected by smoothing.
• Prior knowledge about the probabilities of con-
version to different tag classes is incorporated.
For example, an unseen word ug is five times as
likely to be a noun than an adverb. Our esti-
mate for P(tt1w3) is correspondingly five times
higher for &amp;quot;NN&amp;quot; than for &amp;quot;RB&amp;quot;.
</listItem>
<sectionHeader confidence="0.963318" genericHeader="method">
ANALYSIS OF RESULTS
</sectionHeader>
<bodyText confidence="0.98945">
Our result on the test set of 114392 words (the
tenth of the Brown corpus not used for training)
was 95.81%. Table 3 shows the 20 most frequent
errors.
Three typical examples for the most common
error (tagging nouns as adjectives) are &amp;quot;Commu-
nist&amp;quot;, &amp;quot;public&amp;quot; and &amp;quot;homerun&amp;quot; in the following
sentences.
to use the maximum likelihood estimator:
</bodyText>
<equation confidence="0.807951">
P(tijug) C(ti, wj)
</equation>
<page confidence="0.994166">
185
</page>
<table confidence="0.999252928571428">
VMM: JJ VBN NN VBD IN CS NP RP QL RB VB VBG
correct:
NN 259 102 100 69 66
VBD &apos; 228
NNS 227
VBN 219
JJ 165 71
VB * 142
CS 112
NP 110 194
IN 103
VBG 94
RB 63 63 76
QL 64
</table>
<tableCaption confidence="0.998012">
Table 3: Most common errors.
</tableCaption>
<listItem confidence="0.99971275">
• the Cuban fiasco and the Communist military
victories in Laos
• to increase public awareness of the movement
• the best homerun hitter
</listItem>
<bodyText confidence="0.999068">
The words &amp;quot;public&amp;quot; and &amp;quot;communist&amp;quot; can be used
as adjectives or nouns. Since in the above sen-
tences an adjective is syntactically more likely,
this was the tagging chosen by the VMM. The
noun &amp;quot;homerun&amp;quot; didn&apos;t occur in the training set,
therefore the priors for unknown words biased the
tagging towards adjectives, again because the po-
sition is more typical of an adjective than of a
noun.
Two examples of the second most common er-
ror (tagging past tense forms (&amp;quot;VBD&amp;quot;) as past
participles (&amp;quot;VBN&amp;quot;)) are &amp;quot;called&amp;quot; and &amp;quot;elected&amp;quot;
in the following sentences:
</bodyText>
<listItem confidence="0.95058525">
• the party called for government operation of all
utilities
• When I come back here after the November elec-
tion you&apos;ll think, you&apos;re my man — elected.
</listItem>
<bodyText confidence="0.9998182">
Most of the VBD/VBN errors were caused by
words that have a higher prior for &amp;quot;VBN&amp;quot; so that
in a situation in which both forms are possible ac-
cording to local syntactic context, &amp;quot;VBN&amp;quot; is cho-
sen. More global syntactic context is necessary
to find the right tag &amp;quot;VBD&amp;quot; in the first sentence.
The second sentence is an example for one of the
tagging mistakes in the Brown corpus, &amp;quot;elected&amp;quot;
is clearly used as a past participle, not as a past
tense form.
</bodyText>
<subsectionHeader confidence="0.864585">
Comparison with other Results
</subsectionHeader>
<bodyText confidence="0.999962">
Charniak et al.&apos;s result of 95.97% (Charniak et al.,
1993) is slightly better than ours. This difference
is probably due to the omission of rare tags that
permit reliable prediction of the following tag (the
case of &amp;quot;HVZ*&amp;quot; for &amp;quot;hasn&apos;t&amp;quot;).
Kupiec achieves up to 96.36% correctness
(Kupiec, 1992), without using a tagged corpus for
training as we do. But the results are not eas-
ily comparable with ours since a lexicon is used
that lists only possible tags. This can result in in-
creasing the error rate when tags are listed in the
lexicon that do not occur in the corpus. But it can
also decrease the error rate when errors due to bad
tags for rare words are avoided by looking them up
in the lexicon. Our error rate on words that do not
occur in the training text is 57%, since only the
general priors are used for these words in decod-
ing. This error rate could probably be reduced
substantially by incorporating outside lexical in-
formation.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="discussions">
DISCUSSION
</sectionHeader>
<bodyText confidence="0.999830041666667">
While the learning algorithm of a VMM is efficient
and the resulting tagging algorithm is very simple,
the accuracy achieved is rather moderate. This is
due to several reasons. As mentioned in the intro-
ductory sections, any finite memory Markov model
cannot capture the recursive nature of natural lan-
guage. The VMM can accommodate longer sta-
tistical dependencies than a traditional full-order
Markov model, but due to its Markovian nature
long-distance statistical correlations are neglected.
Therefore, a VMM based tagger can be used for
pruning many of the tagging alternatives using its
prediction probability, but not as a complete tag-
ging system. Furthermore, the VMM power can
be better utilized in low level language process-
ing tasks such as cleaning up corrupted text as
demonstrated in (Ron et al., 1993).
We currently investigate other stochastic
models that can accommodate long distance sta-
tistical correlation (see (Singer and Tishby, 1994)
for preliminary results). However, there are theo-
retical clues that those models are much harder to
learn (Kearns et al., 1993), including HMM based
models (Abe and Warmuth, 1992).
</bodyText>
<page confidence="0.996293">
186
</page>
<bodyText confidence="0.9999865">
Another drawback of the current tagging
scheme is the independence assumption of the un-
derlying tags and the observed words, and the ad-
hoc estimation of the static probabilities. We are
pursuing a systematic scheme to estimate those
probabilities based on Bayesian statistics, by as-
signing a discrete probability distribution, such as
the Dirichlet distribution (Berger, 1985), to each
tag class. The a-posteriori probability estimation
of the individual words can be estimated from the
word counts and the tag class priors. Those priors
can be modeled as a mixture of Dirichlet distribu-
tions (Antoniak, 1974), where each mixture com-
ponent would correspond to a different tag class.
Currently we estimate the state transition prob-
abilities from the conditional counts assuming a
uniform prior. The same technique can be used to
estimate those parameters as well.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="acknowledgments">
ACKNOWLEDGMENT
</sectionHeader>
<bodyText confidence="0.999884833333333">
Part of this work was done while the second au-
thor was visiting the Department of Computer
and Information Sciences, University of California,
Santa-Cruz, supported by NSF grant IRI-9123692.
We would like to thank Jan Pedersen and Naf-
tali Tishby for helpful suggestions and discussions
of this material. Yoram Singer would like to thank
the Charles Clore foundation for supporting this
research. We express our appreciation to faculty
and students for the stimulating atmosphere at
the 1993 Connectionist Models Summer School at
which the idea for this paper took shape.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880577464789">
N. Abe and M. Warmuth, On the computational
complexity of approximating distributionsby
probabilistic automata, Machine Learning,
Vol. 9, pp. 205-260, 1992.
C. Antoniak, Mixture of Dirichlet processes with
applications to Bayesian nonparametric prob-
lems, Annals of Statistics, Vol. 2, pp. 1152-
174, 1974.
J. Berger, Statistical decision theory and Bayesian
analysis, New-York: Springer-Verlag, 1985.
E. Brill. Automatic grammar induction and pars-
ing free text: A transformation-based ap-
proach. In Proceedings of ACL 31, pp. 259-
265, 1993.
E. Charniak, Curtis Hendrickson, Neil Jacobson,
and Mike Perkowitz, Equations for Part-of-
Speech Tagging, Proceedings of the Eleventh
National Conference on Artificial Intelligence,
pp. 784-789, 1993.
K.W. Church, A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text,
Proceedings of ICASSP, 1989.
A. Dempster, N. Laird, and D. Rubin, Maximum
Likelihood estimation from Incomplete Data
via the EM algorithm, J. Roy. Statist. Soc.,
Vol. 39(B), pp. 1-38, 1977.
W.N. Francis and F. Kue&apos;era, Frequency Analysis
of English Usage, Houghton Mifflin, Boston
MA, 1982.
F. Jelinek, Robust part-of-speech tagging using
a hidden Markov model, IBM Tech. Report,
1985.
M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld,
R. Schapire, L. Sellie, On the Learnability of
Discrete Distributions, The 25th Annual ACM
Symposium on Theory of Computing, 1994.
S. Kullback, Information Theory and Statistics,
New-York: Wiley, 1959.
J. Kupiec, Robust part-of-speech tagging using a
hidden Markov model, Computer Speech and
Language, Vol. 6, pp. 225-242, 1992.
L.R. Rabiner and B. H. Juang, An Introduction
to Hidden Markov Models, IEEE ASSP Mag-
azine, Vol. 3, No. 1, pp. 4-16, 1986.
J. Rissanen, Modeling by shortest data discription,
Automatica, Vol. 14, pp. 465-471, 1978.
J. Rissanen, Stochastic complexity and modeling,
The Annals of Statistics, Vol. 14, No. 3, pp.
1080-1100, 1986.
J. Rissanen and G. G. Langdon, Universal model-
ing and coding, IEEE Trans. on Info. Theory,
IT-27, No. 3, pp. 12-23, 1981.
D. Ron, Y. Singer, and N. Tishby, The power
of Amnesia, Advances in Neural Information
Processing Systems 6, 1993.
D. Ron, Y. Singer, and N. Tishby, Learning
Probabilistic Automata with Variable Memory
Length, Proceedings of the 1994 Workshop on
Computational Learning Theory, 1994.
Y. Singer and N. Tishby, Inferring Probabilis-
tic Acyclic Automata Using the Minimum
Description Length Principle, Proceedings of
IEEE Intl. Symp. on Info. Theory, 1994.
R. Weischedel, M. Meteer, R. Schwartz, L.
Ramshaw, and 3. Palmucci. Coping with am-
biguity and unknown words through prob-
abilistic models. Computational Linguistics,
19(2):359-382, 1993.
J. Wu, On the convergence properties of the EM
algorithm, Annals of Statistics, Vol. 11, pp.
95-103, 1983.
</reference>
<page confidence="0.997925">
187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197059">
<title confidence="0.66268">PART-OF-SPEECH TAGGING USING A VARIABLE MEMORY MARKOV MODEL Hinrich Schiitze Center for the Study of Language and Information</title>
<address confidence="0.995152">Stanford, CA 94305-4115</address>
<email confidence="0.96682">Internet:schuetzencsli.stanford.edu</email>
<author confidence="0.992404">Yoram Singer</author>
<affiliation confidence="0.9987055">Institute of Computer Science and Center for Neural Computation</affiliation>
<address confidence="0.71448">Hebrew University, Jerusalem 91904</address>
<email confidence="0.767655">Internet:singerOcs.huji.ac.il</email>
<abstract confidence="0.995154909090909">We present a new approach to disambiguating syntactically ambiguous words in context, based on Memory Markov models. In contrast to fixed-length Markov models, which predict based on fixed-length histories, variable memory Markov models dynamically adapt their history length based on the training data, and hence may use fewer parameters. In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>M Warmuth</author>
</authors>
<title>On the computational complexity of approximating distributionsby probabilistic automata,</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<volume>9</volume>
<pages>205--260</pages>
<contexts>
<context position="25589" citStr="Abe and Warmuth, 1992" startWordPosition="4564" endWordPosition="4567">ased tagger can be used for pruning many of the tagging alternatives using its prediction probability, but not as a complete tagging system. Furthermore, the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in (Ron et al., 1993). We currently investigate other stochastic models that can accommodate long distance statistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992). 186 Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the adhoc estimation of the static probabilities. We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics, by assigning a discrete probability distribution, such as the Dirichlet distribution (Berger, 1985), to each tag class. The a-posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors. Those priors can be modeled as a mixture of Dirichlet distributions (</context>
</contexts>
<marker>Abe, Warmuth, 1992</marker>
<rawString>N. Abe and M. Warmuth, On the computational complexity of approximating distributionsby probabilistic automata, Machine Learning, Vol. 9, pp. 205-260, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Antoniak</author>
</authors>
<title>Mixture of Dirichlet processes with applications to Bayesian nonparametric problems,</title>
<date>1974</date>
<journal>Annals of Statistics,</journal>
<volume>2</volume>
<pages>1152--174</pages>
<contexts>
<context position="26204" citStr="Antoniak, 1974" startWordPosition="4662" endWordPosition="4663">. 186 Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the adhoc estimation of the static probabilities. We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics, by assigning a discrete probability distribution, such as the Dirichlet distribution (Berger, 1985), to each tag class. The a-posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors. Those priors can be modeled as a mixture of Dirichlet distributions (Antoniak, 1974), where each mixture component would correspond to a different tag class. Currently we estimate the state transition probabilities from the conditional counts assuming a uniform prior. The same technique can be used to estimate those parameters as well. ACKNOWLEDGMENT Part of this work was done while the second author was visiting the Department of Computer and Information Sciences, University of California, Santa-Cruz, supported by NSF grant IRI-9123692. We would like to thank Jan Pedersen and Naftali Tishby for helpful suggestions and discussions of this material. Yoram Singer would like to </context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. Antoniak, Mixture of Dirichlet processes with applications to Bayesian nonparametric problems, Annals of Statistics, Vol. 2, pp. 1152-174, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berger</author>
</authors>
<title>Statistical decision theory and Bayesian analysis,</title>
<date>1985</date>
<publisher>Springer-Verlag,</publisher>
<location>New-York:</location>
<contexts>
<context position="25971" citStr="Berger, 1985" startWordPosition="4624" endWordPosition="4625">tistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992). 186 Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the adhoc estimation of the static probabilities. We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics, by assigning a discrete probability distribution, such as the Dirichlet distribution (Berger, 1985), to each tag class. The a-posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors. Those priors can be modeled as a mixture of Dirichlet distributions (Antoniak, 1974), where each mixture component would correspond to a different tag class. Currently we estimate the state transition probabilities from the conditional counts assuming a uniform prior. The same technique can be used to estimate those parameters as well. ACKNOWLEDGMENT Part of this work was done while the second author was visiting the Department of Computer and Inf</context>
</contexts>
<marker>Berger, 1985</marker>
<rawString>J. Berger, Statistical decision theory and Bayesian analysis, New-York: Springer-Verlag, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Automatic grammar induction and parsing free text: A transformation-based approach.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL 31,</booktitle>
<pages>259--265</pages>
<contexts>
<context position="2083" citStr="Brill, 1993" startWordPosition="332" endWordPosition="333">s and Hidden Markov models. Fixed order Markov models are used in (Church, 1989) and (Charniak et al., 1993). Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties </context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>E. Brill. Automatic grammar induction and parsing free text: A transformation-based approach. In Proceedings of ACL 31, pp. 259-265, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>Curtis Hendrickson</author>
<author>Neil Jacobson</author>
<author>Mike Perkowitz</author>
</authors>
<title>Equations for Part-ofSpeech Tagging,</title>
<date>1993</date>
<booktitle>Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>784--789</pages>
<contexts>
<context position="1579" citStr="Charniak et al., 1993" startWordPosition="241" endWordPosition="244">rip.&amp;quot; Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context. In any given English text, most tokens are syntactically ambiguous since most of the high-frequency English words have several parts of speech. Therefore, a correct syntactic classification of words in context is important for most syntactic and other higherlevel processing of natural language text. Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Hidden Markov models. Fixed order Markov models are used in (Church, 1989) and (Charniak et al., 1993). Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model </context>
<context position="13480" citStr="Charniak et al., 1993" startWordPosition="2433" endWordPosition="2436">f wi,n and t1,,, to find this sequence:i • arg poi • arg maxi, p(&apos;v 1.. ) • arg maxti.,,P(ti,n) Wl,n) P(ti,, win) can be expressed as a product of conditional probabilities as follows: Wl,n) = P(ti)P(wilti)P(i2Iti, W1)P(W21t1,2) tV1) Wl,n-1)P(W0 Itl,n, = P(ti wi i)P(wi It w_ 1) 1=1 With the simplifying assumption that the probability of a tag only depends on previous tags and that the probability of a word only depends on its tags, we get: P(ti,n, Wl,n) = Given a variable memory Markov model M, P(tilti,i_i) is estimated by P(tiiSi_i, M) where &apos;Part of the following derivation is adapted from (Charniak et al., 1993). T(w1) 183 = r(e, t1,1), since the dynamics of the sequence are represented by the transition probabilities of the corresponding automaton. The tags 11,„ for a sequence of words wi,,, are therefore chosen according to the following equation using the Viterbi algorithm: Tm(Wi,n) = arg maxi, P(ti ISi- M)P(wi Ili) i=1 We estimate P(wilti) indirectly from P(ti Iwi) using Bayes&apos; Theorem: p (dio = P(tvip)Vit)ilwi) In The terms P(w1) are constant for a given sequence wi and can therefore be omitted from the maximization. We perform a maximum likelihood estimation for P(ti) by calculating the relativ</context>
<context position="17595" citStr="Charniak et al., 1993" startWordPosition="3123" endWordPosition="3126">ways used attributively which makes a following preposition impossible and a following noun highly probable, whereas a predicative use favors modifying prepositional phrases. Similarly, an adverb preceded by a modal (&amp;quot;MD RB&amp;quot;) is followed by an infinitive (&amp;quot;VB&amp;quot;) half the time, whereas other adverbs occur less often in pre-infinitival position. On the other hand, a past participle is virtually impossible after &amp;quot;MD RB&amp;quot; whereas adverbs that are not preceded by modals modify past participles quite often. While it is known that Markov models of order 2 give a slight improvement over order-1 models (Charniak et al., 1993), the number of parameters in our model is much smaller than in a full order-2 Markov model (49*184 = 9016 vs. 184*184*184 = 6,229,504). ESTIMATION OF THE STATIC PARAMETERS We have to estimate the conditional probabilities P(tilw3), the probability that a given word w3 will appear with tag in order to compute the static parameters P(wilti) used in the tagging equations described above. A first approximation would be 184 C(wi) where C(ti, wj) is the number of times ti is tagged as ug in the training text and C(ug) is the number of times iv./ occurs in the training text. However, some form of sm</context>
<context position="18979" citStr="Charniak et al., 1993" startWordPosition="3370" endWordPosition="3373">rts of speech in the training text. One solution to this problem is Good-Turing estimation: • • C(ti, wj) + 1 P(taiwl) c(tvi) where I is the number of tags, 184 in our case. It turns out that Good-Turing is not appropriate for our problem. The reason is the distinction between closed-class and open-class words. Some syntactic classes like verbs and nouns are productive, others like articles are not. As a consequence, the probability that a new word is an article is zero, whereas it is high for verbs and nouns. We need a smoothing scheme that takes this fact into account. Extending an idea in (Charniak et al., 1993), we estimate the probability of tag conversion to find an adequate smoothing scheme. Open and closed classes differ in that words often add a tag from an open class, but rarely from a closed class. For example, a word that is first used as a noun will often be used as a verb subsequently, but closed classes such as possessive pronouns (&amp;quot;my&amp;quot;, &amp;quot;her&amp;quot;, &amp;quot;his&amp;quot;) are rarely used with new syntactic categories after the first few thousand words of the Brown corpus. We only have to take stock of these &amp;quot;tag conversions&amp;quot; to make informed predictions on new tags when confronted with unseen text. Formally, </context>
<context position="23562" citStr="Charniak et al., 1993" startWordPosition="4226" endWordPosition="4229"> I come back here after the November election you&apos;ll think, you&apos;re my man — elected. Most of the VBD/VBN errors were caused by words that have a higher prior for &amp;quot;VBN&amp;quot; so that in a situation in which both forms are possible according to local syntactic context, &amp;quot;VBN&amp;quot; is chosen. More global syntactic context is necessary to find the right tag &amp;quot;VBD&amp;quot; in the first sentence. The second sentence is an example for one of the tagging mistakes in the Brown corpus, &amp;quot;elected&amp;quot; is clearly used as a past participle, not as a past tense form. Comparison with other Results Charniak et al.&apos;s result of 95.97% (Charniak et al., 1993) is slightly better than ours. This difference is probably due to the omission of rare tags that permit reliable prediction of the following tag (the case of &amp;quot;HVZ*&amp;quot; for &amp;quot;hasn&apos;t&amp;quot;). Kupiec achieves up to 96.36% correctness (Kupiec, 1992), without using a tagged corpus for training as we do. But the results are not easily comparable with ours since a lexicon is used that lists only possible tags. This can result in increasing the error rate when tags are listed in the lexicon that do not occur in the corpus. But it can also decrease the error rate when errors due to bad tags for rare words are av</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>E. Charniak, Curtis Hendrickson, Neil Jacobson, and Mike Perkowitz, Equations for Part-ofSpeech Tagging, Proceedings of the Eleventh National Conference on Artificial Intelligence, pp. 784-789, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text,</title>
<date>1989</date>
<booktitle>Proceedings of ICASSP,</booktitle>
<contexts>
<context position="1551" citStr="Church, 1989" startWordPosition="238" endWordPosition="239">She didn&apos;t book a trip.&amp;quot; Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context. In any given English text, most tokens are syntactically ambiguous since most of the high-frequency English words have several parts of speech. Therefore, a correct syntactic classification of words in context is important for most syntactic and other higherlevel processing of natural language text. Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Hidden Markov models. Fixed order Markov models are used in (Church, 1989) and (Charniak et al., 1993). Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information</context>
</contexts>
<marker>Church, 1989</marker>
<rawString>K.W. Church, A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text, Proceedings of ICASSP, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum Likelihood estimation from Incomplete Data via the EM algorithm,</title>
<date>1977</date>
<journal>J. Roy. Statist. Soc.,</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="2374" citStr="Dempster et al., 1977" startWordPosition="375" endWordPosition="378">mple, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different solutions are obtained for different parameter initialization schemes. This phenomenon discourages linguistic analysis based on the output of the model. We present a new method</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin, Maximum Likelihood estimation from Incomplete Data via the EM algorithm, J. Roy. Statist. Soc., Vol. 39(B), pp. 1-38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>F Kue&apos;era</author>
</authors>
<title>Frequency Analysis of English Usage,</title>
<date>1982</date>
<location>Houghton Mifflin, Boston MA,</location>
<marker>Francis, Kue&apos;era, 1982</marker>
<rawString>W.N. Francis and F. Kue&apos;era, Frequency Analysis of English Usage, Houghton Mifflin, Boston MA, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model,</title>
<date>1985</date>
<tech>IBM Tech. Report,</tech>
<contexts>
<context position="2199" citStr="Jelinek, 1985" startWordPosition="349" endWordPosition="350"> the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different s</context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>F. Jelinek, Robust part-of-speech tagging using a hidden Markov model, IBM Tech. Report, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kearns</author>
<author>Y Mansour</author>
<author>D Ron</author>
<author>R Rubinfeld</author>
<author>R Schapire</author>
<author>L Sellie</author>
</authors>
<date>1994</date>
<booktitle>On the Learnability of Discrete Distributions, The 25th Annual ACM Symposium on Theory of Computing,</booktitle>
<marker>Kearns, Mansour, Ron, Rubinfeld, Schapire, Sellie, 1994</marker>
<rawString>M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, L. Sellie, On the Learnability of Discrete Distributions, The 25th Annual ACM Symposium on Theory of Computing, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
</authors>
<title>Information Theory and Statistics,</title>
<date>1959</date>
<publisher>Wiley,</publisher>
<location>New-York:</location>
<contexts>
<context position="10138" citStr="Kullback, 1959" startWordPosition="1835" endWordPosition="1836">erated by the same source. At each 182 stage we can transform the tree into a variable memory Markov process. The key idea is to iteratively build a prediction tree whose probability measure equals the empirical probability measure calculated from the sample. We start with a tree consisting of a single node and add nodes which we have reason to believe should be in the tree. A node a-s, must be added to the tree if it statistically differs from its parent node s. A natural measure to check the statistical difference is the relative entropy (also known as the Kullback-Leibler (KL) divergence) (Kullback, 1959), between the conditional probabilities P(.Js) and P(1a.$). Let X be an observation space and Pi, P2 be probability measures over X then the KL divergence between P1 and P2 1S, DKL(P1I1P2) ExEX Pi (X) log pPZ. In our case, the KL divergence measures how much additional information is gained by using the suffix as for prediction instead of the shorter suffix s. There are cases where the statistical difference is large yet the probability of observing the suffix as itself is so small that we can neglect those cases. Hence we weigh the statistical error by the prior probability of observing us. T</context>
</contexts>
<marker>Kullback, 1959</marker>
<rawString>S. Kullback, Information Theory and Statistics, New-York: Wiley, 1959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model,</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<volume>6</volume>
<pages>225--242</pages>
<contexts>
<context position="2214" citStr="Kupiec, 1992" startWordPosition="351" endWordPosition="352">he model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different solutions are ob</context>
<context position="23797" citStr="Kupiec, 1992" startWordPosition="4266" endWordPosition="4267">yntactic context, &amp;quot;VBN&amp;quot; is chosen. More global syntactic context is necessary to find the right tag &amp;quot;VBD&amp;quot; in the first sentence. The second sentence is an example for one of the tagging mistakes in the Brown corpus, &amp;quot;elected&amp;quot; is clearly used as a past participle, not as a past tense form. Comparison with other Results Charniak et al.&apos;s result of 95.97% (Charniak et al., 1993) is slightly better than ours. This difference is probably due to the omission of rare tags that permit reliable prediction of the following tag (the case of &amp;quot;HVZ*&amp;quot; for &amp;quot;hasn&apos;t&amp;quot;). Kupiec achieves up to 96.36% correctness (Kupiec, 1992), without using a tagged corpus for training as we do. But the results are not easily comparable with ours since a lexicon is used that lists only possible tags. This can result in increasing the error rate when tags are listed in the lexicon that do not occur in the corpus. But it can also decrease the error rate when errors due to bad tags for rare words are avoided by looking them up in the lexicon. Our error rate on words that do not occur in the training text is 57%, since only the general priors are used for these words in decoding. This error rate could probably be reduced substantially</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>J. Kupiec, Robust part-of-speech tagging using a hidden Markov model, Computer Speech and Language, Vol. 6, pp. 225-242, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B H Juang</author>
</authors>
<title>An Introduction to Hidden Markov Models,</title>
<date>1986</date>
<journal>IEEE ASSP Magazine,</journal>
<volume>3</volume>
<pages>4--16</pages>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L.R. Rabiner and B. H. Juang, An Introduction to Hidden Markov Models, IEEE ASSP Magazine, Vol. 3, No. 1, pp. 4-16, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by shortest data discription,</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<pages>465--471</pages>
<contexts>
<context position="3965" citStr="Rissanen, 1978" startWordPosition="629" endWordPosition="630">ithm and classification of new tags are computationally efficient, and the results achieved, using simplified assumptions for the static tag probabilities, are encouraging. VARIABLE MEMORY MARKOV MODELS Markov models are a natural candidate for language modeling and temporal pattern recognition, mostly due to their mathematical simplicity. However, it is obvious that finite memory Markov models cannot capture the recursive nature of language, nor can they be trained effectively with long memories. The notion of variable context length also appears naturally in the context of universal coding (Rissanen, 1978; Rissanen and Langdon, 1981). This information theoretic notion is now known to be closely related to efficient modeling (Rissanen, 1988). The natural measure that 181 appears in information theory is the description length, as measured by the statistical predictability via the Kullback-Leibler (KL) divergence. The VMM learning algorithm is based on minimizing the statistical prediction error of a Markov model, measured by the instantaneous KL divergence of the following symbols, the current statistical surprise of the model. The memory is extended precisely when such a surprise is significan</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>J. Rissanen, Modeling by shortest data discription, Automatica, Vol. 14, pp. 465-471, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Stochastic complexity and modeling,</title>
<date>1986</date>
<journal>The Annals of Statistics,</journal>
<volume>14</volume>
<pages>1080--1100</pages>
<marker>Rissanen, 1986</marker>
<rawString>J. Rissanen, Stochastic complexity and modeling, The Annals of Statistics, Vol. 14, No. 3, pp. 1080-1100, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
<author>G G Langdon</author>
</authors>
<title>Universal modeling and coding,</title>
<date>1981</date>
<journal>IEEE Trans. on Info. Theory,</journal>
<volume>27</volume>
<pages>12--23</pages>
<contexts>
<context position="3994" citStr="Rissanen and Langdon, 1981" startWordPosition="631" endWordPosition="635">ication of new tags are computationally efficient, and the results achieved, using simplified assumptions for the static tag probabilities, are encouraging. VARIABLE MEMORY MARKOV MODELS Markov models are a natural candidate for language modeling and temporal pattern recognition, mostly due to their mathematical simplicity. However, it is obvious that finite memory Markov models cannot capture the recursive nature of language, nor can they be trained effectively with long memories. The notion of variable context length also appears naturally in the context of universal coding (Rissanen, 1978; Rissanen and Langdon, 1981). This information theoretic notion is now known to be closely related to efficient modeling (Rissanen, 1988). The natural measure that 181 appears in information theory is the description length, as measured by the statistical predictability via the Kullback-Leibler (KL) divergence. The VMM learning algorithm is based on minimizing the statistical prediction error of a Markov model, measured by the instantaneous KL divergence of the following symbols, the current statistical surprise of the model. The memory is extended precisely when such a surprise is significant, until the overall statisti</context>
</contexts>
<marker>Rissanen, Langdon, 1981</marker>
<rawString>J. Rissanen and G. G. Langdon, Universal modeling and coding, IEEE Trans. on Info. Theory, IT-27, No. 3, pp. 12-23, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>The power of Amnesia,</title>
<date>1993</date>
<booktitle>Advances in Neural Information Processing Systems 6,</booktitle>
<contexts>
<context position="3037" citStr="Ron et al., 1993" startWordPosition="483" endWordPosition="486">ocal minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different solutions are obtained for different parameter initialization schemes. This phenomenon discourages linguistic analysis based on the output of the model. We present a new method based on variable memory Markov models (VMM) (Ron et al., 1993; Ron et al., 1994). The VMM is an approximation of an unlimited order Markov source. It can incorporate both the static (order 0) and dynamic (higher-order) information systematically, while keeping the ability to change the model due to future observations. This approach is easy to implement, the learning algorithm and classification of new tags are computationally efficient, and the results achieved, using simplified assumptions for the static tag probabilities, are encouraging. VARIABLE MEMORY MARKOV MODELS Markov models are a natural candidate for language modeling and temporal pattern re</context>
<context position="4859" citStr="Ron et al., 1993" startWordPosition="776" endWordPosition="779">e Kullback-Leibler (KL) divergence. The VMM learning algorithm is based on minimizing the statistical prediction error of a Markov model, measured by the instantaneous KL divergence of the following symbols, the current statistical surprise of the model. The memory is extended precisely when such a surprise is significant, until the overall statistical prediction of the stochastic model is sufficiently good. For the sake of simplicity, a POS tag is termed a symbol and a sequence of tags is called a string. We now briefly describe the algorithm for learning a variable memory Markov model. See (Ron et al., 1993; Ron et al., 1994) for a more detailed description of the algorithm. We first introduce notational conventions and define some basic concepts. Let E be a finite alphabet. Denote by E* the set of all strings over E. A string s, over E* of length n, is denoted by s = sis2 ...sn. We denote by e the empty string. The length of a string s is denoted by is&apos; and the size of an alphabet E is denoted by lEi• Let Pre f ix(s) = s1s2 . sn_ I denote the longest prefix of a string s, and let Pre fix*(s) denote the set of all prefixes of s, including the empty string. Similarly, Su f fix(s) = s2s3 sn and Su</context>
<context position="25269" citStr="Ron et al., 1993" startWordPosition="4516" endWordPosition="4519">introductory sections, any finite memory Markov model cannot capture the recursive nature of natural language. The VMM can accommodate longer statistical dependencies than a traditional full-order Markov model, but due to its Markovian nature long-distance statistical correlations are neglected. Therefore, a VMM based tagger can be used for pruning many of the tagging alternatives using its prediction probability, but not as a complete tagging system. Furthermore, the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in (Ron et al., 1993). We currently investigate other stochastic models that can accommodate long distance statistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992). 186 Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the adhoc estimation of the static probabilities. We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistic</context>
</contexts>
<marker>Ron, Singer, Tishby, 1993</marker>
<rawString>D. Ron, Y. Singer, and N. Tishby, The power of Amnesia, Advances in Neural Information Processing Systems 6, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>Learning Probabilistic Automata with Variable Memory Length,</title>
<date>1994</date>
<booktitle>Proceedings of the 1994 Workshop on Computational Learning Theory,</booktitle>
<contexts>
<context position="3056" citStr="Ron et al., 1994" startWordPosition="487" endWordPosition="490">1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different solutions are obtained for different parameter initialization schemes. This phenomenon discourages linguistic analysis based on the output of the model. We present a new method based on variable memory Markov models (VMM) (Ron et al., 1993; Ron et al., 1994). The VMM is an approximation of an unlimited order Markov source. It can incorporate both the static (order 0) and dynamic (higher-order) information systematically, while keeping the ability to change the model due to future observations. This approach is easy to implement, the learning algorithm and classification of new tags are computationally efficient, and the results achieved, using simplified assumptions for the static tag probabilities, are encouraging. VARIABLE MEMORY MARKOV MODELS Markov models are a natural candidate for language modeling and temporal pattern recognition, mostly d</context>
<context position="4878" citStr="Ron et al., 1994" startWordPosition="780" endWordPosition="783"> (KL) divergence. The VMM learning algorithm is based on minimizing the statistical prediction error of a Markov model, measured by the instantaneous KL divergence of the following symbols, the current statistical surprise of the model. The memory is extended precisely when such a surprise is significant, until the overall statistical prediction of the stochastic model is sufficiently good. For the sake of simplicity, a POS tag is termed a symbol and a sequence of tags is called a string. We now briefly describe the algorithm for learning a variable memory Markov model. See (Ron et al., 1993; Ron et al., 1994) for a more detailed description of the algorithm. We first introduce notational conventions and define some basic concepts. Let E be a finite alphabet. Denote by E* the set of all strings over E. A string s, over E* of length n, is denoted by s = sis2 ...sn. We denote by e the empty string. The length of a string s is denoted by is&apos; and the size of an alphabet E is denoted by lEi• Let Pre f ix(s) = s1s2 . sn_ I denote the longest prefix of a string s, and let Pre fix*(s) denote the set of all prefixes of s, including the empty string. Similarly, Su f fix(s) = s2s3 sn and Suffix* (s) is the se</context>
</contexts>
<marker>Ron, Singer, Tishby, 1994</marker>
<rawString>D. Ron, Y. Singer, and N. Tishby, Learning Probabilistic Automata with Variable Memory Length, Proceedings of the 1994 Workshop on Computational Learning Theory, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle,</title>
<date>1994</date>
<booktitle>Proceedings of IEEE Intl. Symp. on Info. Theory,</booktitle>
<contexts>
<context position="25409" citStr="Singer and Tishby, 1994" startWordPosition="4535" endWordPosition="4538">e longer statistical dependencies than a traditional full-order Markov model, but due to its Markovian nature long-distance statistical correlations are neglected. Therefore, a VMM based tagger can be used for pruning many of the tagging alternatives using its prediction probability, but not as a complete tagging system. Furthermore, the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in (Ron et al., 1993). We currently investigate other stochastic models that can accommodate long distance statistical correlation (see (Singer and Tishby, 1994) for preliminary results). However, there are theoretical clues that those models are much harder to learn (Kearns et al., 1993), including HMM based models (Abe and Warmuth, 1992). 186 Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words, and the adhoc estimation of the static probabilities. We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics, by assigning a discrete probability distribution, such as the Dirichlet distribution (Berger, 1985), to each tag class. The a-posteriori </context>
</contexts>
<marker>Singer, Tishby, 1994</marker>
<rawString>Y. Singer and N. Tishby, Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle, Proceedings of IEEE Intl. Symp. on Info. Theory, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weischedel</author>
<author>M Meteer</author>
<author>R Schwartz</author>
<author>L Ramshaw</author>
</authors>
<title>Coping with ambiguity and unknown words through probabilistic models.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1965" citStr="Weischedel et al., 1993" startWordPosition="311" endWordPosition="314">rlevel processing of natural language text. Two stochastic methods have been widely used for POS tagging: fixed order Markov models and Hidden Markov models. Fixed order Markov models are used in (Church, 1989) and (Charniak et al., 1993). Since the order of the model is assumed to be fixed, a short memory (small order) is typically used, since the number of possible combinations grows exponentially. For example, assuming there are 184 different tags, as in the Brown corpus, there are 1843 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is tim</context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, 1993</marker>
<rawString>R. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and 3. Palmucci. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359-382, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wu</author>
</authors>
<title>On the convergence properties of the EM algorithm,</title>
<date>1983</date>
<journal>Annals of Statistics,</journal>
<volume>11</volume>
<pages>95--103</pages>
<contexts>
<context position="2444" citStr="Wu, 1983" startWordPosition="387" endWordPosition="388"> 6, 229, 504 different order 3 combinations of tags (of course not all of these will actually occur, see (Weischedel et al., 1993)). Because of the large number of parameters higher-order fixed length models are hard to estimate. (See (Brill, 1993) for a rule-based approach to incorporating higher-order information.) In a Hidden Markov Model (HMM) (Jelinek, 1985; Kupiec, 1992), a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM (Dempster et al., 1977) algorithm, which guarantees convergence to a local minimum (Wu, 1983). The advantage of an HMM is that it can be trained using untagged text. On the other hand, the training procedure is time consuming, and a fixed model (topology) is assumed. Another disadvantage is due to the local convergence properties of the EM algorithm. The solution obtained depends on the initial setting of the model&apos;s parameters, and different solutions are obtained for different parameter initialization schemes. This phenomenon discourages linguistic analysis based on the output of the model. We present a new method based on variable memory Markov models (VMM) (Ron et al., 1993; Ron e</context>
</contexts>
<marker>Wu, 1983</marker>
<rawString>J. Wu, On the convergence properties of the EM algorithm, Annals of Statistics, Vol. 11, pp. 95-103, 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>