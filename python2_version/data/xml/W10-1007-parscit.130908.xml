<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005772">
<title confidence="0.976902">
Predicting Cloze Task Quality for Vocabulary Training
</title>
<author confidence="0.998598">
Adam Skory Maxine Eskenazi
</author>
<affiliation confidence="0.878968666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh PA 15213, USA
</affiliation>
<email confidence="0.992967">
{askory,max}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994697" genericHeader="abstract">
Abstract
</sectionHeader>
<subsectionHeader confidence="0.999339">
1.1 Cloze Tasks in Assessment
</subsectionHeader>
<bodyText confidence="0.999613045454546">
Computer generation of cloze tasks still falls
short of full automation; most current systems
are used by teachers as authoring aids.
Improved methods to estimate cloze quality are
needed for full automation. We investigated
lexical reading difficulty as a novel automatic
estimator of cloze quality, to which co-
occurrence frequency of words was compared
as an alternate estimator. Rather than relying on
expert evaluation of cloze quality, we submitted
open cloze tasks to workers on Amazon
Mechanical Turk (AMT) and discuss ways to
measure of the results of these tasks. Results
show one statistically significant correlation
between the above measures and estimators,
which was lexical co-occurrence and Cloze
Easiness. Reading difficulty was not found to
correlate significantly. We gave subsets of
cloze sentences to an English teacher as a gold
standard. Sentences selected by co-occurrence
and Cloze Easiness were ranked most highly,
corroborating the evidence from AMT.
</bodyText>
<sectionHeader confidence="0.972115" genericHeader="method">
1 Cloze Tasks
</sectionHeader>
<bodyText confidence="0.999948212121212">
Cloze tasks, described in Taylor (1953), are
activities in which one or several words are
removed from a sentence and a student is asked to
fill in the missing content. That sentence can be
referred to as the &apos;stem&apos;, and the removed term
itself as the &apos;key&apos;. (Higgins, 2006) The portion of
the sentence from which the key has been removed
is the &apos;blank&apos;. &apos;Open cloze&apos; tasks are those in which
the student can propose any answer. &apos;Closed cloze&apos;
describes multiple choice tasks in which the key is
presented along with a set of several &apos;distractors&apos;.
Assessment is the best known application of cloze
tasks. As described in (Alderson, 1979), the “cloze
procedure” is that in which multiple words are
removed at intervals from a text. This is mostly
used in first language (L1) education. Alderson
describes three deletion strategies: random
deletion, deletion of every nth word, and targeted
deletion, in which certain words are manually
chosen and deleted by an instructor. Theories of
lexical quality (Perfetti &amp; Hart, 2001) and word
knowledge levels (Dale, 1965) illustrate why cloze
tasks can effectively assess multiple dimensions of
vocabulary knowledge.
Perfetti &amp; Hart explain that lexical knowledge
can be decomposed into orthographic, phonetic,
syntactic, and semantic constituents. The lexical
quality of a given word can then be defined as a
measure based on both the depth of knowledge of
each constituent and the degree to which those
constituents are bonded together. Cloze tasks allow
a test author to select for specific combinations of
constituents to assess (Bachman, 1982).
</bodyText>
<subsectionHeader confidence="0.762543">
1.2 Instructional Cloze Tasks
</subsectionHeader>
<bodyText confidence="0.999907555555556">
Cloze tasks can be employed for instruction as well
as assessment. Jongsma (1980) showed that
targeted deletion is an effective use of instructional
passage-based cloze tasks. Repeated exposure to
frequent words leads first to familiarity with those
words, and increasingly to suppositions about their
semantic and syntactic constituents. Producing
cloze tasks through targeted deletion takes implicit,
receptive word knowledge, and forces the student
</bodyText>
<page confidence="0.992331">
49
</page>
<note confidence="0.9906805">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 49–56,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.977858666666667">
to consider explicitly how to match features of the
stem with what is known about features of any
keys she may consider.
</bodyText>
<sectionHeader confidence="0.432359" genericHeader="method">
2 Automatic Generation of Cloze Tasks
</sectionHeader>
<bodyText confidence="0.999951678571429">
Most cloze task “generation” systems are really
cloze task identification systems. That is, given a
set of requirements, such as a specific key and
syntactic structure (Higgins 2006) for the stem, a
system looks into a database of pre-processed text
and attempts to identify sentences matching those
criteria. Thus, the content generated for a closed
cloze is the stem (by deletion of the key), and a set
of distractors. In the case of some systems, a
human content author may manually tailor the
resulting stems to meet further needs.
Identifying suitable sentences from natural
language corpora is desirable because the
sentences that are found will be authentic.
Depending on the choice of corpora, sentences
should also be well-formed and suitable in terms of
reading level and content. Newspaper text is one
popular source (Hoshino &amp; Nakagawa, 2005; Liu
et al., 2005; Lee &amp; Seneff, 2007). Pino et al.
(2008) use documents from a corpus of texts
retrieved from the internet and subsequently
filtered according to readability level, category,
and appropriateness of content. Using a broader
corpus increases the number and variability of
potential matching sentences, but also lowers the
confidence that sentences will be well-formed and
contain appropriate language (Brown &amp; Eskenazi,
2004).
</bodyText>
<subsectionHeader confidence="0.998265">
2.1 Tag-based Sentence Search
</subsectionHeader>
<bodyText confidence="0.99993996875">
Several cloze item authoring tools (Liu et al. 2005;
Higgins, 2006) implement specialized tag-based
sentence search. This goes back to the original
distribution of the Penn Treebank and the
corresponding tgrep program. Developed by Pito
in 1992 (Pito, 1994) this program allows
researchers to search for corpus text according to
sequences of part of speech (POS) tags and tree
structure.
The linguists&apos; Search Engine (Resnik &amp; Elkiss,
2005) takes the capabilities of tgrep yet further,
providing a simplified interface for linguists to
search within tagged corpora along both syntactic
and lexical features.
Both tgrep and the Linguists&apos; Search Engine
were not designed as cloze sentence search tools,
but they paved the way for similar tools specialized
for this task. For example, Higgins&apos; (2006) system
uses a regular expression engine that can work
either on the tag level, the text level or both. This
allows test content creators to quickly find
sentences within very narrow criteria. They can
then alter these sentences as necessary.
Liu et al. (2005) use sentences from a corpus of
newspaper text tagged for POS and lemma.
Candidate sentences are found by searching on the
key and its POS as well as the POS sequence of
surrounding terms. In their system results are
filtered for proper word sense by comparing other
words in the stem with data from WordNet and
HowNet, databases of inter-word semantic
relations.
</bodyText>
<subsectionHeader confidence="0.999658">
2.2 Statistical Sentence Search
</subsectionHeader>
<bodyText confidence="0.999936807692308">
Pino et al (2009) use co-occurrence frequencies to
identify candidate sentences. They used the
Stanford Parser (Klein &amp; Manning, 2003) to detect
sentences within a desired range of complexity and
likely well-formedness. Co-occurrence frequencies
of words in the corpus were calculated and keys
were compared to other words in the stem to
determine cloze quality, producing suitable cloze
questions 66.53% of the time. This method
operates on the theory that the quality of the
context of a stem is based on the co-occurrence
scores of other words in the sentence. Along with
this result, Pino et al. incorporated syntactic
complexity in terms of the number of parses found.
Hoshino &amp; Nakagawa (2005) use machine
learning techniques to train a cloze task search
system. Their system, rather than finding sentences
suitable for cloze tasks, attempts to automate
deletion for passage-based cloze. The features used
include sentence length and POS of keys and
surrounding words. Both a Naïve Bayes and a K-
Nearest Neighbor classifier were trained to find the
most likely words for deletion within news articles.
To train the system they labeled cloze sentences
from a TOEIC training test as true, then shifted the
position of the blanks from those sentences and
</bodyText>
<page confidence="0.99239">
50
</page>
<bodyText confidence="0.99984525">
labeled the resulting sentences as false. Manual
evaluation of the results showed that, for both
classifiers, experts saw over 90% of the deletions
as either easy to solve or merely possible to solve.
</bodyText>
<sectionHeader confidence="0.534141" genericHeader="method">
3 Reading Level and Information Theory
</sectionHeader>
<bodyText confidence="0.999985126984127">
An information-theoretical basis for an entirely
novel approach to automated cloze sentence search
is found in Finn (1978). Finn defines Cloze
Easiness as “the percent of subjects filling in the
correct word in a cloze task.” Another metric of the
quality of a cloze task is context restriction; the
number of solutions perceived as acceptable keys
for a given stem. Finn&apos;s theory of lexical feature
transfer provides one mechanism to explain
context restriction. The theory involves the
information content of a blank.
According to Shannon&apos;s (1948) seminal work
on information theory, the information contained
in a given term is inverse to its predictability. In
other words, if a term appears despite following a
history after which is it considered very unlikely to
occur, that word has high information content. For
example, consider the partial sentence “She drives
a nice...”. A reader forms hypotheses about the
next word before seeing it, and thus expects an
overall meaning of the sentence. A word that
conforms to this hypothesis, such as the word &apos;car&apos;,
does little to change a reader&apos;s knowledge and thus
has little information. If instead the next word is
&apos;taxi&apos;, &apos;tank&apos;, or &apos;ambulance&apos;, unforeseen knowledge
is gained and relative information is higher.
According to Finn (1978) the applicability of
this theory to Cloze Easiness can be explained
though lexical transfer features. These features can
be both syntactic and semantic, and they serve to
interrelate words within a sentence. If a large
number of lexical transfer features are within a
given proximity of a blank, then the set of words
matching those features will be highly restricted.
Given that each choice of answer will be from a
smaller pool of options, the probability of that
answer will be much higher. Thus, a highly
probable key has correspondingly low information
content.
Predicting context restriction is of benefit to
automatic generation of cloze tasks. Cloze
Easiness improves if a student chooses from a
smaller set of possibilities. The instructional value
of a highly context-restricted cloze task is also
higher by providing a richer set of lexical transfer
features with which to associate vocabulary.
Finn&apos;s application of information theory to
Cloze Easiness and context restriction provides one
possible new avenue to improve the quality of
generated cloze tasks. We hypothesize that words
of higher reading levels contain higher numbers of
transfer features and thus their presence in a
sentence can be correlated with its degree of
context restriction. To the authors&apos; knowledge
reading level has not been previously applied to
this problem.
We can use a unigram reading level model to
investigate this hypothesis. Returning to the
example words for the partial sentence “She drives
a nice...”, we can see that our current model
classifies the highly expected word, &apos;car&apos;, at reading
level 1, while &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at
reading levels 5, 6, and 11 respectively.
</bodyText>
<subsectionHeader confidence="0.999379">
3.1 Reading Level Estimators
</subsectionHeader>
<bodyText confidence="0.99997936">
The estimation of reading level is a complex topic
unto itself. Early work used heuristics based on
average sentence length and the percentage of
words deemed unknown to a baseline reader. (Dale
&amp; Chall, 1948; Dale, 1965) Another early measure,
the Flesch-Kincaid measure, (Kincaid et al., 1975)
uses a function of the syllable length of words in a
document and the average sentence length.
More recent work on the topic also focuses on
readability classification at the document level.
Collins-Thompson &amp; Callan (2005) use unigram
language models without syntactic features.
Heilman et al. (2008) use a probabilistic parser and
unigram language models to combine grammatical
and lexical features. (Petersen &amp; Ostendorf, 2006)
add higher-order n-gram features to the above to
train support vector machine classifiers for each
grade level.
These recent methods perform well to
characterize the level of an entire document, but
they are untested for single sentences. We wish to
investigate if a robust unigram model of reading
level can be employed to improve the estimation of
cloze quality at the sentence level. By extension of
Finn&apos;s (1978) hypothesis, it is in fact not the
</bodyText>
<page confidence="0.99317">
51
</page>
<bodyText confidence="0.999866857142857">
overall level of the sentence that has a predicted
effect on cloze context restriction, but rather the
reading level of the words in proximity to the
blank. Thus we propose that it should be possible
to find a correlation between cloze quality and the
reading levels of words in near context to the blank
of a cloze task.
</bodyText>
<sectionHeader confidence="0.982597" genericHeader="method">
4 The Approach
</sectionHeader>
<bodyText confidence="0.999925833333333">
We investigate a multi-staged filtering approach to
cloze sentence generation. Several variations of the
final filtering step of this approach were employed
and correlations sought between the resulting sets
of each filter variation. The subset predicted to
contain the best sentences by each filter was finally
submitted to expert review as a gold standard test
of cloze quality.
This study compares two features of sentences,
finding the levels of context restriction
experimentally. The first feature in question is the
maximum reading level found in near-context to
the blank. The second feature is the mean skip
bigram co-occurrence score of words within that
context.
Amazon Mechanical Turk (AMT) is used as a
novel cloze quality evaluation method. This
method is validated by both positive correlation
with the known-valid (Pino et al., 2008) co-
occurrence score predictor, and an expert gold
standard. Experimental results from AMT are then
used to evaluate the hypothesis that reading level
can be used as a new, alternative predictor of cloze
quality.
</bodyText>
<subsectionHeader confidence="0.989669">
4.1 Cloze Sentence Filtering
</subsectionHeader>
<bodyText confidence="0.999788666666667">
The first step in preparing material for this study
was to obtain a set of keys. We expect that in most
applications of sentence-based cloze tasks the set
of keys is pre-determined by instructional goals.
Due to this constraint, we choose a set of keys
distributed across several reading levels and hold it
as fixed. Four words were picked from the set of
words common in texts labeled as grades four, six,
eight, ten, and twelve respectively.
</bodyText>
<table confidence="0.9502422">
4th: &apos;little&apos;, &apos;thought&apos;, &apos;voice&apos;, &apos;animals&apos;
6th: ‘president&apos;, &apos;sportsmanship&apos;, &apos;national&apos;, experience&apos;
8th: &apos;college&apos;, &apos;wildlife&apos;, &apos;beautiful&apos;, &apos;competition&apos;
10th: &apos;medical&apos;, &apos;elevations&apos;,&apos;qualities&apos;, &apos;independent&apos;
12th: &apos;scientists&apos;, &apos;citizens&apos;, &apos;discovered&apos;, &apos;university&apos;
</table>
<figureCaption confidence="0.9653">
Figure 1: common words per grade level.
</figureCaption>
<bodyText confidence="0.999854642857143">
201,025 sentences containing these keys were
automatically extracted from a corpus of web
documents as the initial filtering step. This
collection of sentences was then limited to
sentences of length 25 words or less. Filtering by
sentence length reduced the set to 136,837
sentences.
A probabilistic parser was used to score each
sentence. This parser gives log-probability values
corresponding to confidence of the best parse. A
threshold for this confidence score was chosen
manually and sentences with scores below the
threshold were removed, reducing the number of
sentences to 29,439.
</bodyText>
<subsectionHeader confidence="0.983107">
4.2 Grade Level
</subsectionHeader>
<bodyText confidence="0.999971">
Grade level in this study is determined by a
smoothed unigram model based on normalized
concentrations within labeled documents. A
sentence is assigned the grade level of the highest
level word in context of the key.
</bodyText>
<subsectionHeader confidence="0.985137">
4.3 Co-occurrence Scores
</subsectionHeader>
<bodyText confidence="0.999958352941177">
Skip bigram co-occurrence counts were calculated
from the Brown (Francis &amp; Kucera, 1979) and
OANC (OANC, 2009) corpora. A given sentence&apos;s
score is calculated as the mean of the probabilities
of finding that sentence&apos;s context for the key.
These probabilities are defined on the triplet
(key, word, window size), in which key is the target
word to be removed, word any term in the corpus,
and window size is a positive integer less than or
equal to the length of the sentence.
This probability is estimated as the number of
times word is found within the same sentence as
key and within an absolute window size of 2
positions from key, divided by the total number of
times all terms are found in that window. These
scores are thus maximum likelihood estimators of
the probability of word given key and window size:
</bodyText>
<page confidence="0.992771">
52
</page>
<listItem confidence="0.84086725">
(1) For some key k , word w, and window-size m :
Cj(w, k) := count of times w found j words from the
position of k, within the same sentence.
(2) For a vocabulary V and for some positive integer
</listItem>
<equation confidence="0.728649166666667">
window-size m, let n = (m-1) / 2, then:
i.e. if our corpus consisted of the single sentence
“This is a good example sentence.”:
C−1 (w = good, k = example) = 1
C1 (w = sentence, k = example) = 1
P (w = good  |k = example, m = 3) = 1 / (1+1)= .5
</equation>
<bodyText confidence="0.998159333333333">
Finally, the overall score of the sentence is taken
to be the mean of the skip bigram probabilities of
all words in context of the key.
</bodyText>
<subsectionHeader confidence="0.965661">
4.4 Variable Filtering by Grade and Score
</subsectionHeader>
<bodyText confidence="0.999973243243243">
Skip bigram scores were calculated for all words
co-occurrent in a sentence with each of our 20
keys. To maximize the observable effect of the two
dimensions of grade level and co-occurrence score,
the goal was to find sentences representing
combinations of ranges within those dimensions.
To achieve this it was necessary to pick the
window size that best balances variance of these
dimensions with a reasonably flat distribution of
sentences.
In terms of grade level, smaller window sizes
resulted in very few sentences with at least one
high-level word, while larger window sizes
resulted in few sentences with no high-level words.
Variance in co-occurrence score, on the other
hand, was maximal at a window size of 3 words,
and dropped off until nearly flattening out at a
window size of 20 words. A window size of 15
words was found to offer a reasonable distribution
of grade level while preserving sufficient variance
of co-occurrence score.
Using the above window-size, we created filters
according to maximum grade level: one each for
the grade ranges 5-6, 7-8, 9-10, and 11-12. Four
more filters were created according to co-
occurrence score: one selecting the highest-scoring
quartile of sentences, one the second highest-
scoring quartile, and so on. Each grade level filter
was combined with each co-occurrence score filter
creating 4x4=16 composite filters. By combining
these filters we can create a final set of sentences
for analysis with high confidence of having a
significant number of sentences representing all
possible values of grade level and co-occurrence
score. At most two sentences were chosen for each
of the 20 keys using these composite filters. The
final number of sentences was 540.
</bodyText>
<subsectionHeader confidence="0.937345">
4.5 Experimental Cloze Quality
</subsectionHeader>
<bodyText confidence="0.998947580645162">
Previous evaluation of automatically generated
cloze tasks has relied on expert judgments. (Pino et
al., 2008; Liu et al., 2005) We present the use of
crowdsourcing techniques as a new approach for
this evaluation. We believe the approach can be
validated by statistically significant correlations
with predicted cloze quality and comparison with
expert judgments.
The set of 540 sentences were presented to
workers from Amazon Mechanical Turk (AMT),
an online marketplace for “human intelligence
tasks.” Each worker was shown up to twenty of the
stems of these sentences as open cloze tasks. No
worker was allowed to see more than one stem for
the same key. Workers were instructed to enter
only those words that “absolutely make sense in
this context”, but were not encouraged to submit
any particular number of answers. Workers were
paid US$.04 per sentence, and the task was limited
to workers with approval ratings on past tasks at or
above 90%.
For each sentence under review each worker
contributes one subset of answers. Cloze Easiness,
as defined by Finn (1978) is calculated as the
percentage of these subsets containing the original
key. We define context restriction on n as the
percentage of answer subsets containing n or fewer
words.
Using the example sentence: “Take this cloze
sentence, for (example) .” We can find the set of
answer subsets A:
</bodyText>
<figure confidence="0.934070333333333">
A = { A1={example, free, fun, me}
A2={example,instance}
A3={instance} }
Then, Cloze Easiness is |{A1,A2} |/ |A |≈ .67 and
Context restriction (on one or two words) is |
{A2,A3} |/ |A |≈ .67
</figure>
<page confidence="0.99743">
53
</page>
<sectionHeader confidence="0.99947" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999786684210526">
Each sentence in the final set was seen, on average,
by 27 Mechanical Turk workers. We wish to
correlate measures of Cloze Easiness and context
restriction with cloze quality predictors of
maximum grade level and score. We use the
Pearson correlation coefficient (PCC) to test the
linear relationship between each measure of cloze
quality and each predictor.
Table (1) shows these PCC values. All of the
values are positive, meaning there is a correlation
showing that one value will tend to increase as the
other increases. The strongest correlation is that of
co-occurrence and Cloze Easiness. This is also the
only statistically significant correlation. The value
of P(H0) represents the likelihood of the null
hypothesis: that two random distributions
generated the same correlation. Values of P(H0)
under 0.05 can be considered statistically
significant.
</bodyText>
<table confidence="0.9984794">
Cloze Easiness PCC = 0.2043 PCC = 0.0671
P(H0)=1.6965e-06 P(H0)=0.1193
Context PCC = 0.0649 PCC = 0.07
Restriction (2) P(H0)=0.1317 P(H0)=0.1038
Co-occurrence Maximum Grade
</table>
<tableCaption confidence="0.972206333333333">
Table (1): Pearson Correlation Coefficient and
probability of null hypothesis for estimators and
measures of cloze quality.
</tableCaption>
<figureCaption confidence="0.579404">
Figure (3) shows scatter plots of these four
correlations in which each dot represents one
sentence.
</figureCaption>
<bodyText confidence="0.999781">
The top-leftmost plot shows the correlation of
co-occurrence score (on the x-axis), and Cloze
Easiness (on the y-axis). Co-occurrence scores are
shown on a log-scale. The line through these points
represents a linear regression, which is in this case
statistically significant.
The bottom-left plot shows correlation of co-
occurrence score (x-axis) with context restriction.
In this case context restriction was calculated on
n=2, i.e. the percent of answers containing only
</bodyText>
<figureCaption confidence="0.957551">
Figure (3): Scatter plots of all sentences with cloze quality measure as y-axis, and cloze quality estimator as x-axis.
The linear regression of each distribution is shown.
</figureCaption>
<page confidence="0.994762">
54
</page>
<bodyText confidence="0.999973517241379">
one or two words. The linear regression shows
there is a small (statistically insignificant)
correlation.
The top-right plot shows Cloze Easiness (y-
axis) per grade level (x-axis). The bottom left
shows context restriction (y-axis) as a function of
grade level. In both cases linear regressions here
also show small, statistically insignificant positive
correlations.
The lack of significant correlations for three out
of four combinations of measures and estimators is
not grounds to dismiss these measures. Across all
sentences, the measure of context restriction is
highly variant, at 47.9%. This is possibly the result
of the methodology; in an attempt to avoid biasing
the AMT workers, we did not specify the desirable
number of answers. This led to many workers
interpreting the task differently.
In terms of maximum grade level, the lack of a
significant correlation with context restriction does
not absolutely refute Finn (1978)&apos;s hypothesis.
Finn specifies that semantic transfer features
should be in “lexical scope” of a blank. A clear
definition of “lexical scope” was not presented. We
generalized scope to mean proximity within a fixed
contextual window size. It is possible that a more
precise definition of “lexical scope” will provide a
stronger correlation of reading level and context
restriction.
</bodyText>
<subsectionHeader confidence="0.858389">
5.1 Expert Validation
</subsectionHeader>
<bodyText confidence="0.999690208333333">
Finally, while we have shown a statistically
significant positive correlation between co-
occurrence scores and Cloze Easiness, we still
need to demonstrate that Cloze Easiness is a valid
measure of cloze quality. To do so, we selected the
set of 20 sentences that ranked highest by co-
occurrence score and by Cloze Easiness to submit
to expert evaluation. Due to overlap between these
two sets, choosing distinct sentences for both
would require choosing some sentences ranked
below the top 20 for each category. Accordingly,
we chose to submit just one set based on both
criteria in combination.
Along with these 20 sentences, as controls, we
also selected two more distinct sets of 20
sentences: one set of sentences measuring most
highly in context restriction, and one set most
highly estimated by maximum grade level.
We asked a former English teacher to read each
open cloze, without the key, and rate, on a five
point Likert scale, her agreement with the
statement “This is a very good fill-in-the-blank
sentence.” where 1 means strong agreement, and 5
means strong disagreement.
</bodyText>
<tableCaption confidence="0.956029">
Table (2): Mean ratings for each sentence category.
</tableCaption>
<bodyText confidence="0.999857888888889">
The results in Table (2) show that, on average,
the correlated results of selecting sentences based
on Cloze Easiness and co-occurrence score are in
fact rated more highly by our expert as compared
to sentences selected based on context restriction,
which is, in turn, rated more highly than sentences
selected by maximum grade level. Using a one-
sample t-test and a population mean of 2.5, we find
a p-value of .0815 for our expert&apos;s ratings.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991333333333">
We present a multi-step filter-based paradigm
under which diverse estimators of cloze quality can
be applied towards the goal of full automation of
cloze task generation. In our implementation of
this approach sentences were found for a set of
keys, and then filtered by maximum length and
likelihood of well-formedness. We then tested
combinations of two estimators and two
experimental measures of cloze quality for the next
filtering step.
We presented an information-theoretical basis
for the use of reading level as a novel estimator for
cloze quality. The hypothesis that maximum grade
level should be correlated with context restriction
was not, however, shown with statistical
significance. A stronger correlation might be
shown with a different experimental methodology
and a more refined definition of lexical scope.
</bodyText>
<figure confidence="0.800903">
Expert evaluation on
5-point Scale
Mean Standard
Deviation
20 Cloze Easiness and co- 2.25 1.37
best occurrence score
sentences
as
determined
by:
Context restriction 3.05 1.36
Maximum grade level 3.15 1.2
</figure>
<page confidence="0.99486">
55
</page>
<bodyText confidence="0.999985333333333">
As an alternative to expert evaluation of cloze
quality, we investigated the use of non-expert
workers on AMT. A statistically significant
correlation was found between the co-occurrence
score of a sentence and its experimental measure of
Cloze Easiness. This is evidence that
crowdsourcing techniques agree with expert
evaluation of co-occurrence scores in past studies.
To gain further evidence of the validity of these
experimental results, sentences selected by a
composite filter of co-occurrence score and Cloze
Easiness were compared to sentences selected by
context restriction and reading level. An expert
evaluation showed a preference for sentences
selected by the composite filter.
We believe that this method of cloze task
selection is promising. It will now be tested in a
real learning situation. This work contributes
insight into methods for improving technologies
such as intelligent tutoring systems and language
games.
</bodyText>
<sectionHeader confidence="0.998085" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999740802631579">
Alderson, J. C. (1979). The Cloze Procedure and
Proficiency in English as a Foreign Language. TESOL
Quarterly, 13(2), 219-227. doi: 10.2307/3586211.
Bachman, L. F. (1982). The Trait Structure of Cloze Test
Scores. TESOL Quarterly, 16(1), 61.
Brown, J., &amp; Eskenazi, M. (2004). Retrieval of Authentic
Documents for Reader-Specific Lexical Practice. In
InSTIL/ICALL Symposium (Vol. 2). Venice, Italy.
Collins-Thompson, K., &amp; Callan, J. (2005). Predicting
reading difficulty with statistical language models. Journal
of the American Society for Information Science and
Technology, 56(13), 1448-1462.
Dale, E. (1965). Vocabulary measurement: Techniques and
major findings. Elementary English, 42, 395-401.
Dale, E., &amp; Chall, J. S. (1948). A Formula for Predicting
Readability: Instructions. Educational Research Bulletin,
Vol. 27(2), 37-54.
Finn, P. J. (1978). Word frequency, information theory,
and cloze performance: A transfer feature theory of
processing in reading. Reading Research Quarterly, 13(4),
508-537.
Francis, W. N. &amp; Kucera, H. (1979). Brown Corpus
Manual, Brown University Department of Linguistics.
Providence, RI
Heilman, M., Collins-Thompson, K., &amp; Eskenazi, M.
(2008). An Analysis of Statistical Models and Features for
Reading Difficulty Prediction. 3rd Workshop on
Innovative Use of NLP for Building Educational
Applications. Assoc. for Computational Linguistics.
Higgins, D. (2006). Item Distiller: Text retrieval for
computer-assisted test item creation. ETS, Princeton, NJ.
Hoshino, A., &amp; Nakagawa, H. (2005). A real-time
multiple-choice question generation for language testing –
a preliminary study–. In 2nd Workshop on Building
Educational Applications Using NLP (pp. 17-20). Ann
Arbor, MI: Association for Computational Linguistics.
Jongsma, E. (1980). Cloze instructional research: A second
look. Newark, DE: International Reading Association.
Urbana, IL.
Kincaid, J., Fishburne, R., Rodgers, R., &amp; Chissom, B.
(1975). Derivation of new readability formulas for navy
enlisted personnel. Research Branch Report. Millington,
TN.
Klein, D. &amp; Manning, C. (2003). Accurate Unlexicalized
Parsing. (pp. 423-430) In Proceedings of the 41st Meeting
of the Assoc. for Computational Linguistics.
Lee, J., &amp; Seneff, S. (2007). Automatic Generation of
Cloze Items for Prepositions. Proceedings of In.
Liu, C., Wang, C., Gao, Z., &amp; Huang, S. (2005).
Applications of Lexical Information for Algorithmically
Composing Multiple-Choice Cloze Items. In Proceedings
of the 2nd Workshop on Building Educational Applications
Using NLP (p. 1–8). Ann Arbor, MI: Association for
Computational Linguistics.
Open American National Corpus (2009)
americannationalcorpus.org/OANC/
Perfetti, C., &amp; Hart, L. (2001). Lexical bases of
comprehension skill. (D. Gorfein) (pp. 67-86). Washington
D.C.: American Psychological Association.
Petersen, S. E., &amp; Ostendorf, M. (2006). Assessing the
reading level of web pages. In ICSLP (Vol. pages, pp. 833-
836).
Pino, J., Heilman, M., &amp; Eskenazi, M. (2008). A Selection
Strategy to Improve Cloze Question Quality. In
Proceedings of the Workshop on Intelligent Tutoring
Systems for Ill-Defined Domains.
Pito, R. (1994). tgrep README
www.ldc.upenn.edu/ldc/online/treebank/README.long
Resnik, P., &amp; Elkiss, A. (2005). The Linguist’s Search
Engine: An Overview. Association for Computational
Linguistics, (June), 33-36.
Shannon, C. (1948). A Mathematical Theory of
Communication. Bell System Technical Journal, 27, 379–
423, 623–656.
Taylor, W. L. (1953). Cloze procedure: a new tool for
measuring readability. Journalism Quarterly, 30, 415-453.
</reference>
<page confidence="0.998416">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000138">
<title confidence="0.999988">Predicting Cloze Task Quality for Vocabulary Training</title>
<author confidence="0.999503">Adam Skory Maxine Eskenazi</author>
<affiliation confidence="0.9992355">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.989276">Pittsburgh PA 15213,</address>
<email confidence="0.999011">askory@cs.cmu.edu</email>
<email confidence="0.999011">max@cs.cmu.edu</email>
<abstract confidence="0.997466386813188">1.1 Cloze Tasks in Assessment Computer generation of cloze tasks still falls short of full automation; most current systems are used by teachers as authoring aids. Improved methods to estimate cloze quality are needed for full automation. We investigated lexical reading difficulty as a novel automatic estimator of cloze quality, to which cooccurrence frequency of words was compared as an alternate estimator. Rather than relying on expert evaluation of cloze quality, we submitted open cloze tasks to workers on Amazon Mechanical Turk (AMT) and discuss ways to measure of the results of these tasks. Results show one statistically significant correlation between the above measures and estimators, which was lexical co-occurrence and Cloze Easiness. Reading difficulty was not found to correlate significantly. We gave subsets of cloze sentences to an English teacher as a gold standard. Sentences selected by co-occurrence and Cloze Easiness were ranked most highly, corroborating the evidence from AMT. 1 Cloze Tasks Cloze tasks, described in Taylor (1953), are activities in which one or several words are removed from a sentence and a student is asked to fill in the missing content. That sentence can be referred to as the &apos;stem&apos;, and the removed term itself as the &apos;key&apos;. (Higgins, 2006) The portion of the sentence from which the key has been removed is the &apos;blank&apos;. &apos;Open cloze&apos; tasks are those in which the student can propose any answer. &apos;Closed cloze&apos; describes multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979), the “cloze procedure” is that in which multiple words are removed at intervals from a text. This is mostly used in first language (L1) education. Alderson describes three deletion strategies: random deletion of every word, and targeted deletion, in which certain words are manually chosen and deleted by an instructor. Theories of lexical quality (Perfetti &amp; Hart, 2001) and word knowledge levels (Dale, 1965) illustrate why cloze tasks can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthographic, phonetic, syntactic, and semantic constituents. The lexical quality of a given word can then be defined as a measure based on both the depth of knowledge of each constituent and the degree to which those constituents are bonded together. Cloze tasks allow a test author to select for specific combinations of constituents to assess (Bachman, 1982). 1.2 Instructional Cloze Tasks Cloze tasks can be employed for instruction as well as assessment. Jongsma (1980) showed that targeted deletion is an effective use of instructional passage-based cloze tasks. Repeated exposure to frequent words leads first to familiarity with those words, and increasingly to suppositions about their semantic and syntactic constituents. Producing cloze tasks through targeted deletion takes implicit, receptive word knowledge, and forces the student 49 of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational pages 49–56, Angeles, California, June 2010. Association for Computational Linguistics to consider explicitly how to match features of the stem with what is known about features of any keys she may consider. 2 Automatic Generation of Cloze Tasks Most cloze task “generation” systems are really task That is, given a set of requirements, such as a specific key and syntactic structure (Higgins 2006) for the stem, a system looks into a database of pre-processed text and attempts to identify sentences matching those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of the Penn Treebank and the Developed by Pito in 1992 (Pito, 1994) this program allows researchers to search for corpus text according to sequences of part of speech (POS) tags and tree structure. The linguists&apos; Search Engine (Resnik &amp; Elkiss, takes the capabilities of further, providing a simplified interface for linguists to search within tagged corpora along both syntactic and lexical features. the Linguists&apos; Search Engine were not designed as cloze sentence search tools, but they paved the way for similar tools specialized for this task. For example, Higgins&apos; (2006) system uses a regular expression engine that can work either on the tag level, the text level or both. This allows test content creators to quickly find sentences within very narrow criteria. They can then alter these sentences as necessary. Liu et al. (2005) use sentences from a corpus of newspaper text tagged for POS and lemma. Candidate sentences are found by searching on the key and its POS as well as the POS sequence of surrounding terms. In their system results are filtered for proper word sense by comparing other words in the stem with data from WordNet and HowNet, databases of inter-word semantic relations. 2.2 Statistical Sentence Search Pino et al (2009) use co-occurrence frequencies to identify candidate sentences. They used the Stanford Parser (Klein &amp; Manning, 2003) to detect sentences within a desired range of complexity and likely well-formedness. Co-occurrence frequencies of words in the corpus were calculated and keys were compared to other words in the stem to determine cloze quality, producing suitable cloze questions 66.53% of the time. This method operates on the theory that the quality of the context of a stem is based on the co-occurrence scores of other words in the sentence. Along with this result, Pino et al. incorporated syntactic complexity in terms of the number of parses found. Hoshino &amp; Nakagawa (2005) use machine learning techniques to train a cloze task search system. Their system, rather than finding sentences suitable for cloze tasks, attempts to automate deletion for passage-based cloze. The features used include sentence length and POS of keys and surrounding words. Both a Naïve Bayes and a K- Nearest Neighbor classifier were trained to find the most likely words for deletion within news articles. To train the system they labeled cloze sentences a TOEIC training test as then shifted the position of the blanks from those sentences and 50 the resulting sentences as Manual evaluation of the results showed that, for both classifiers, experts saw over 90% of the deletions as either easy to solve or merely possible to solve. 3 Reading Level and Information Theory An information-theoretical basis for an entirely novel approach to automated cloze sentence search found in Finn (1978). Finn defines “the percent of subjects filling in the correct word in a cloze task.” Another metric of the quality of a cloze task is context restriction; the number of solutions perceived as acceptable keys for a given stem. Finn&apos;s theory of lexical feature transfer provides one mechanism to explain context restriction. The theory involves the of a blank. According to Shannon&apos;s (1948) seminal work information theory, the in a given term is inverse to its predictability. In other words, if a term appears despite following a history after which is it considered very unlikely to that word has high For consider the partial sentence drives A reader forms hypotheses about the next word before seeing it, and thus expects an overall meaning of the sentence. A word that conforms to this hypothesis, such as the word &apos;car&apos;, does little to change a reader&apos;s knowledge and thus little If instead the next word is &apos;taxi&apos;, &apos;tank&apos;, or &apos;ambulance&apos;, unforeseen knowledge gained and relative higher. According to Finn (1978) the applicability of this theory to Cloze Easiness can be explained though lexical transfer features. These features can be both syntactic and semantic, and they serve to interrelate words within a sentence. If a large number of lexical transfer features are within a given proximity of a blank, then the set of words matching those features will be highly restricted. Given that each choice of answer will be from a smaller pool of options, the probability of that answer will be much higher. Thus, a highly key has correspondingly low content. Predicting context restriction is of benefit to automatic generation of cloze tasks. Cloze Easiness improves if a student chooses from a smaller set of possibilities. The instructional value of a highly context-restricted cloze task is also higher by providing a richer set of lexical transfer features with which to associate vocabulary. Finn&apos;s application of information theory to Cloze Easiness and context restriction provides one possible new avenue to improve the quality of generated cloze tasks. We hypothesize that words of higher reading levels contain higher numbers of transfer features and thus their presence in a sentence can be correlated with its degree of context restriction. To the authors&apos; knowledge reading level has not been previously applied to this problem. We can use a unigram reading level model to investigate this hypothesis. Returning to the words for the partial sentence drives we can see that our current model classifies the highly expected word, &apos;car&apos;, at reading &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at levels and 3.1 Reading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for each grade level. These recent methods perform well to characterize the level of an entire document, but they are untested for single sentences. We wish to investigate if a robust unigram model of reading level can be employed to improve the estimation of cloze quality at the sentence level. By extension of Finn&apos;s (1978) hypothesis, it is in fact not the 51 overall level of the sentence that has a predicted effect on cloze context restriction, but rather the reading level of the words in proximity to the blank. Thus we propose that it should be possible to find a correlation between cloze quality and the reading levels of words in near context to the blank of a cloze task. 4 The Approach We investigate a multi-staged filtering approach to cloze sentence generation. Several variations of the final filtering step of this approach were employed and correlations sought between the resulting sets of each filter variation. The subset predicted to contain the best sentences by each filter was finally submitted to expert review as a gold standard test of cloze quality. This study compares two features of sentences, finding the levels of context restriction experimentally. The first feature in question is the maximum reading level found in near-context to the blank. The second feature is the mean skip bigram co-occurrence score of words within that context. Amazon Mechanical Turk (AMT) is used as a novel cloze quality evaluation method. This method is validated by both positive correlation with the known-valid (Pino et al., 2008) cooccurrence score predictor, and an expert gold standard. Experimental results from AMT are then used to evaluate the hypothesis that reading level can be used as a new, alternative predictor of cloze quality. 4.1 Cloze Sentence Filtering The first step in preparing material for this study was to obtain a set of keys. We expect that in most applications of sentence-based cloze tasks the set of keys is pre-determined by instructional goals. Due to this constraint, we choose a set of keys distributed across several reading levels and hold it as fixed. Four words were picked from the set of words common in texts labeled as grades four, six, eight, ten, and twelve respectively. 4th: &apos;little&apos;, &apos;thought&apos;, &apos;voice&apos;, &apos;animals&apos; 6th: ‘president&apos;, &apos;sportsmanship&apos;, &apos;national&apos;, experience&apos; 8th: &apos;college&apos;, &apos;wildlife&apos;, &apos;beautiful&apos;, &apos;competition&apos; 10th: &apos;medical&apos;, &apos;elevations&apos;,&apos;qualities&apos;, &apos;independent&apos; &apos;scientists&apos;, &apos;citizens&apos;, &apos;discovered&apos;, Figure 1: common words per grade level. 201,025 sentences containing these keys were automatically extracted from a corpus of web documents as the initial filtering step. This collection of sentences was then limited to sentences of length 25 words or less. Filtering by sentence length reduced the set to 136,837 sentences. A probabilistic parser was used to score each sentence. This parser gives log-probability values corresponding to confidence of the best parse. A threshold for this confidence score was chosen manually and sentences with scores below the threshold were removed, reducing the number of sentences to 29,439. 4.2 Grade Level Grade level in this study is determined by a smoothed unigram model based on normalized concentrations within labeled documents. A sentence is assigned the grade level of the highest level word in context of the key. 4.3 Co-occurrence Scores Skip bigram co-occurrence counts were calculated from the Brown (Francis &amp; Kucera, 1979) and OANC (OANC, 2009) corpora. A given sentence&apos;s score is calculated as the mean of the probabilities of finding that sentence&apos;s context for the key. These probabilities are defined on the triplet in which the target to be removed, term in the corpus, size a positive integer less than or equal to the length of the sentence. This probability is estimated as the number of found within the same sentence as within an absolute size from by the total number of times all terms are found in that window. These scores are thus maximum likelihood estimators of probability of size: 52 For some key word and window-size k) count of times from the of within the same sentence. For a vocabulary for some positive integer let = (m-1) / then: i.e. if our corpus consisted of the single sentence is a good example (w = k = = 1 (w = k = = 1 (w = k = m = 3) = 1 / (1+1)= .5 Finally, the overall score of the sentence is taken to be the mean of the skip bigram probabilities of all words in context of the key. 4.4 Variable Filtering by Grade and Score Skip bigram scores were calculated for all words co-occurrent in a sentence with each of our 20 keys. To maximize the observable effect of the two dimensions of grade level and co-occurrence score, the goal was to find sentences representing combinations of ranges within those dimensions. To achieve this it was necessary to pick the window size that best balances variance of these dimensions with a reasonably flat distribution of sentences. In terms of grade level, smaller window sizes resulted in very few sentences with at least one high-level word, while larger window sizes resulted in few sentences with no high-level words. Variance in co-occurrence score, on the other hand, was maximal at a window size of 3 words, and dropped off until nearly flattening out at a window size of 20 words. A window size of 15 words was found to offer a reasonable distribution of grade level while preserving sufficient variance of co-occurrence score. Using the above window-size, we created filters according to maximum grade level: one each for the grade ranges 5-6, 7-8, 9-10, and 11-12. Four more filters were created according to cooccurrence score: one selecting the highest-scoring quartile of sentences, one the second highestscoring quartile, and so on. Each grade level filter was combined with each co-occurrence score filter creating 4x4=16 composite filters. By combining these filters we can create a final set of sentences for analysis with high confidence of having a significant number of sentences representing all possible values of grade level and co-occurrence score. At most two sentences were chosen for each of the 20 keys using these composite filters. The final number of sentences was 540. 4.5 Experimental Cloze Quality Previous evaluation of automatically generated cloze tasks has relied on expert judgments. (Pino et al., 2008; Liu et al., 2005) We present the use of crowdsourcing techniques as a new approach for this evaluation. We believe the approach can be validated by statistically significant correlations with predicted cloze quality and comparison with expert judgments. The set of 540 sentences were presented to workers from Amazon Mechanical Turk (AMT), an online marketplace for “human intelligence tasks.” Each worker was shown up to twenty of the stems of these sentences as open cloze tasks. No worker was allowed to see more than one stem for the same key. Workers were instructed to enter only those words that “absolutely make sense in this context”, but were not encouraged to submit any particular number of answers. Workers were paid US$.04 per sentence, and the task was limited to workers with approval ratings on past tasks at or above 90%. For each sentence under review each worker contributes one subset of answers. Cloze Easiness, as defined by Finn (1978) is calculated as the percentage of these subsets containing the original We define restriction the of answer subsets containing fewer words. Using the example sentence: “Take this cloze for (example) .” We can find the set of answer subsets A: = { free, fun, } Cloze Easiness is / |A |≈ .67 restriction (on one or two words) is / |A |≈ .67 53 5 Results Each sentence in the final set was seen, on average, by 27 Mechanical Turk workers. We wish to correlate measures of Cloze Easiness and context restriction with cloze quality predictors of maximum grade level and score. We use the Pearson correlation coefficient (PCC) to test the linear relationship between each measure of cloze quality and each predictor. Table (1) shows these PCC values. All of the values are positive, meaning there is a correlation showing that one value will tend to increase as the other increases. The strongest correlation is that of co-occurrence and Cloze Easiness. This is also the only statistically significant correlation. The value represents the likelihood of the null hypothesis: that two random distributions the same correlation. Values of under 0.05 can be considered statistically significant.</abstract>
<note confidence="0.816845">Cloze Easiness PCC = 0.2043 PCC = 0.0671 Context PCC = 0.0649 PCC = 0.07 Restriction (2)</note>
<title confidence="0.604416">Co-occurrence Maximum Grade</title>
<abstract confidence="0.991546553846154">Table (1): Pearson Correlation Coefficient and probability of null hypothesis for estimators and measures of cloze quality. Figure (3) shows scatter plots of these four correlations in which each dot represents one sentence. The top-leftmost plot shows the correlation of co-occurrence score (on the x-axis), and Cloze Easiness (on the y-axis). Co-occurrence scores are shown on a log-scale. The line through these points represents a linear regression, which is in this case statistically significant. The bottom-left plot shows correlation of cooccurrence score (x-axis) with context restriction. In this case context restriction was calculated on i.e. the percent of answers containing only Figure (3): Scatter plots of all sentences with cloze quality measure as y-axis, and cloze quality estimator as x-axis. The linear regression of each distribution is shown. 54 one or two words. The linear regression shows there is a small (statistically insignificant) correlation. The top-right plot shows Cloze Easiness (yaxis) per grade level (x-axis). The bottom left shows context restriction (y-axis) as a function of grade level. In both cases linear regressions here also show small, statistically insignificant positive correlations. The lack of significant correlations for three out of four combinations of measures and estimators is not grounds to dismiss these measures. Across all sentences, the measure of context restriction is highly variant, at 47.9%. This is possibly the result of the methodology; in an attempt to avoid biasing the AMT workers, we did not specify the desirable number of answers. This led to many workers interpreting the task differently. In terms of maximum grade level, the lack of a significant correlation with context restriction does not absolutely refute Finn (1978)&apos;s hypothesis. Finn specifies that semantic transfer features should be in “lexical scope” of a blank. A clear definition of “lexical scope” was not presented. We generalized scope to mean proximity within a fixed contextual window size. It is possible that a more precise definition of “lexical scope” will provide a stronger correlation of reading level and context restriction. 5.1 Expert Validation Finally, while we have shown a statistically significant positive correlation between cooccurrence scores and Cloze Easiness, we still need to demonstrate that Cloze Easiness is a valid measure of cloze quality. To do so, we selected the set of 20 sentences that ranked highest by cooccurrence score and by Cloze Easiness to submit to expert evaluation. Due to overlap between these two sets, choosing distinct sentences for both would require choosing some sentences ranked below the top 20 for each category. Accordingly, we chose to submit just one set based on both criteria in combination. Along with these 20 sentences, as controls, we also selected two more distinct sets of 20 sentences: one set of sentences measuring most highly in context restriction, and one set most highly estimated by maximum grade level. We asked a former English teacher to read each open cloze, without the key, and rate, on a five point Likert scale, her agreement with the is a very good fill-in-the-blank where 1 means strong agreement, and 5 means strong disagreement. Table (2): Mean ratings for each sentence category. The results in Table (2) show that, on average, the correlated results of selecting sentences based on Cloze Easiness and co-occurrence score are in fact rated more highly by our expert as compared to sentences selected based on context restriction, which is, in turn, rated more highly than sentences selected by maximum grade level. Using a onesample t-test and a population mean of 2.5, we find a p-value of .0815 for our expert&apos;s ratings. 6 Conclusion We present a multi-step filter-based paradigm under which diverse estimators of cloze quality can be applied towards the goal of full automation of cloze task generation. In our implementation of this approach sentences were found for a set of keys, and then filtered by maximum length and likelihood of well-formedness. We then tested combinations of two estimators and two experimental measures of cloze quality for the next filtering step. We presented an information-theoretical basis for the use of reading level as a novel estimator for cloze quality. The hypothesis that maximum grade level should be correlated with context restriction was not, however, shown with statistical significance. A stronger correlation might be shown with a different experimental methodology and a more refined definition of lexical scope. Expert evaluation on 5-point Scale Mean Standard Deviation 20 best sentences as determined by: Cloze Easiness and co-occurrence score 2.25 1.37 Context restriction 3.05 1.36 Maximum grade level 3.15 1.2 55 As an alternative to expert evaluation of cloze quality, we investigated the use of non-expert workers on AMT. A statistically significant correlation was found between the co-occurrence score of a sentence and its experimental measure of Cloze Easiness. This is evidence that crowdsourcing techniques agree with expert evaluation of co-occurrence scores in past studies. To gain further evidence of the validity of these experimental results, sentences selected by a composite filter of co-occurrence score and Cloze Easiness were compared to sentences selected by context restriction and reading level. An expert evaluation showed a preference for sentences selected by the composite filter. We believe that this method of cloze task selection is promising. It will now be tested in a real learning situation. This work contributes insight into methods for improving technologies such as intelligent tutoring systems and language games.</abstract>
<note confidence="0.738784878787879">References Alderson, J. C. (1979). The Cloze Procedure and in English as a Foreign Language. 219-227. doi: 10.2307/3586211. Bachman, L. F. (1982). The Trait Structure of Cloze Test 61. Brown, J., &amp; Eskenazi, M. (2004). Retrieval of Authentic Documents for Reader-Specific Lexical Practice. In Symposium 2). Venice, Italy. Collins-Thompson, K., &amp; Callan, J. (2005). Predicting difficulty with statistical language models. of the American Society for Information Science and 1448-1462. Dale, E. (1965). Vocabulary measurement: Techniques and findings. 395-401. Dale, E., &amp; Chall, J. S. (1948). A Formula for Predicting Instructions. Research 37-54. Finn, P. J. (1978). Word frequency, information theory, and cloze performance: A transfer feature theory of in reading. Research 508-537. Francis, W. N. &amp; Kucera, H. (1979). Brown Corpus University Department of Linguistics. Providence, RI Heilman, M., Collins-Thompson, K., &amp; Eskenazi, M. (2008). An Analysis of Statistical Models and Features for Difficulty Prediction. Workshop on Innovative Use of NLP for Building Educational Assoc. for Computational Linguistics. Higgins, D. (2006). Item Distiller: Text retrieval for computer-assisted test item creation. ETS, Princeton, NJ. Hoshino, A., &amp; Nakagawa, H. (2005). A real-time</note>
<abstract confidence="0.765">multiple-choice question generation for language testing – preliminary study–. In Workshop on Building</abstract>
<note confidence="0.890682066666667">Applications Using NLP 17-20). Ann Arbor, MI: Association for Computational Linguistics. Jongsma, E. (1980). Cloze instructional research: A second look. Newark, DE: International Reading Association. Urbana, IL. Kincaid, J., Fishburne, R., Rodgers, R., &amp; Chissom, B. (1975). Derivation of new readability formulas for navy personnel. Branch Millington, TN. Klein, D. &amp; Manning, C. (2003). Accurate Unlexicalized (pp. 423-430) In of the 41st Meeting of the Assoc. for Computational Linguistics. Lee, J., &amp; Seneff, S. (2007). Automatic Generation of Items for Prepositions. of Liu, C., Wang, C., Gao, Z., &amp; Huang, S. (2005).</note>
<title confidence="0.722291">Applications of Lexical Information for Algorithmically</title>
<author confidence="0.687584">In</author>
<note confidence="0.863241615384616">of the 2nd Workshop on Building Educational Applications NLP 1–8). Ann Arbor, MI: Association for Computational Linguistics. Open American National Corpus (2009) americannationalcorpus.org/OANC/ C., &amp; Hart, L. (2001). bases of (D. Gorfein) (pp. 67-86). Washington D.C.: American Psychological Association. Petersen, S. E., &amp; Ostendorf, M. (2006). Assessing the level of web pages. In pages, pp. 833- 836). Pino, J., Heilman, M., &amp; Eskenazi, M. (2008). A Selection Strategy to Improve Cloze Question Quality. In Proceedings of the Workshop on Intelligent Tutoring for Ill-Defined Pito, R. (1994). tgrep README www.ldc.upenn.edu/ldc/online/treebank/README.long Resnik, P., &amp; Elkiss, A. (2005). The Linguist’s Search An Overview. for Computational (June), 33-36. Shannon, C. (1948). A Mathematical Theory of System Technical 379– 423, 623–656. Taylor, W. L. (1953). Cloze procedure: a new tool for readability. 415-453. 56</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Alderson</author>
</authors>
<title>The Cloze Procedure and Proficiency in English as a Foreign Language.</title>
<date>1979</date>
<journal>TESOL Quarterly,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>219--227</pages>
<contexts>
<context position="1858" citStr="Alderson, 1979" startWordPosition="286" endWordPosition="287">in Taylor (1953), are activities in which one or several words are removed from a sentence and a student is asked to fill in the missing content. That sentence can be referred to as the &apos;stem&apos;, and the removed term itself as the &apos;key&apos;. (Higgins, 2006) The portion of the sentence from which the key has been removed is the &apos;blank&apos;. &apos;Open cloze&apos; tasks are those in which the student can propose any answer. &apos;Closed cloze&apos; describes multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979), the “cloze procedure” is that in which multiple words are removed at intervals from a text. This is mostly used in first language (L1) education. Alderson describes three deletion strategies: random deletion, deletion of every nth word, and targeted deletion, in which certain words are manually chosen and deleted by an instructor. Theories of lexical quality (Perfetti &amp; Hart, 2001) and word knowledge levels (Dale, 1965) illustrate why cloze tasks can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthograp</context>
</contexts>
<marker>Alderson, 1979</marker>
<rawString>Alderson, J. C. (1979). The Cloze Procedure and Proficiency in English as a Foreign Language. TESOL Quarterly, 13(2), 219-227. doi: 10.2307/3586211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Bachman</author>
</authors>
<title>The Trait Structure of Cloze Test Scores.</title>
<date>1982</date>
<journal>TESOL Quarterly,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>61</pages>
<contexts>
<context position="2810" citStr="Bachman, 1982" startWordPosition="432" endWordPosition="433">ories of lexical quality (Perfetti &amp; Hart, 2001) and word knowledge levels (Dale, 1965) illustrate why cloze tasks can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthographic, phonetic, syntactic, and semantic constituents. The lexical quality of a given word can then be defined as a measure based on both the depth of knowledge of each constituent and the degree to which those constituents are bonded together. Cloze tasks allow a test author to select for specific combinations of constituents to assess (Bachman, 1982). 1.2 Instructional Cloze Tasks Cloze tasks can be employed for instruction as well as assessment. Jongsma (1980) showed that targeted deletion is an effective use of instructional passage-based cloze tasks. Repeated exposure to frequent words leads first to familiarity with those words, and increasingly to suppositions about their semantic and syntactic constituents. Producing cloze tasks through targeted deletion takes implicit, receptive word knowledge, and forces the student 49 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, </context>
</contexts>
<marker>Bachman, 1982</marker>
<rawString>Bachman, L. F. (1982). The Trait Structure of Cloze Test Scores. TESOL Quarterly, 16(1), 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Brown</author>
<author>M Eskenazi</author>
</authors>
<title>Retrieval of Authentic Documents for Reader-Specific Lexical Practice.</title>
<date>2004</date>
<booktitle>In InSTIL/ICALL Symposium</booktitle>
<volume>2</volume>
<location>Venice, Italy.</location>
<contexts>
<context position="4961" citStr="Brown &amp; Eskenazi, 2004" startWordPosition="758" endWordPosition="761">ding on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of the Penn Treebank and the corresponding tgrep program. Developed by Pito in 1992 (Pito, 1994) this program allows researchers to search for corpus text according to sequences of part of speech (POS) tags and tree structure. The linguists&apos; Search Engine (Resnik &amp; Elkiss, 2005) takes the capabilities of tgrep yet further, providing a simplified interface for linguists to search within tagged corpora alo</context>
</contexts>
<marker>Brown, Eskenazi, 2004</marker>
<rawString>Brown, J., &amp; Eskenazi, M. (2004). Retrieval of Authentic Documents for Reader-Specific Lexical Practice. In InSTIL/ICALL Symposium (Vol. 2). Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<pages>1448--1462</pages>
<contexts>
<context position="11478" citStr="Collins-Thompson &amp; Callan (2005)" startWordPosition="1797" endWordPosition="1800">le &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at reading levels 5, 6, and 11 respectively. 3.1 Reading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for each grade level. These recent methods perform well to characterize the level of an entire document, but they are untested for single sentences. We wish to investigate if a robust unigram model of reading level can be employed to improve the estimation of cloze quality at the sentence level. By ex</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Collins-Thompson, K., &amp; Callan, J. (2005). Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13), 1448-1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dale</author>
</authors>
<title>Vocabulary measurement: Techniques and major findings.</title>
<date>1965</date>
<journal>Elementary English,</journal>
<volume>42</volume>
<pages>395--401</pages>
<contexts>
<context position="2283" citStr="Dale, 1965" startWordPosition="352" endWordPosition="353">s multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979), the “cloze procedure” is that in which multiple words are removed at intervals from a text. This is mostly used in first language (L1) education. Alderson describes three deletion strategies: random deletion, deletion of every nth word, and targeted deletion, in which certain words are manually chosen and deleted by an instructor. Theories of lexical quality (Perfetti &amp; Hart, 2001) and word knowledge levels (Dale, 1965) illustrate why cloze tasks can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthographic, phonetic, syntactic, and semantic constituents. The lexical quality of a given word can then be defined as a measure based on both the depth of knowledge of each constituent and the degree to which those constituents are bonded together. Cloze tasks allow a test author to select for specific combinations of constituents to assess (Bachman, 1982). 1.2 Instructional Cloze Tasks Cloze tasks can be employed for instructi</context>
<context position="11180" citStr="Dale, 1965" startWordPosition="1753" endWordPosition="1754">plied to this problem. We can use a unigram reading level model to investigate this hypothesis. Returning to the example words for the partial sentence “She drives a nice...”, we can see that our current model classifies the highly expected word, &apos;car&apos;, at reading level 1, while &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at reading levels 5, 6, and 11 respectively. 3.1 Reading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for </context>
</contexts>
<marker>Dale, 1965</marker>
<rawString>Dale, E. (1965). Vocabulary measurement: Techniques and major findings. Elementary English, 42, 395-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dale</author>
<author>J S Chall</author>
</authors>
<title>A Formula for Predicting Readability: Instructions.</title>
<date>1948</date>
<journal>Educational Research Bulletin,</journal>
<volume>27</volume>
<issue>2</issue>
<pages>37--54</pages>
<contexts>
<context position="11167" citStr="Dale &amp; Chall, 1948" startWordPosition="1749" endWordPosition="1752">t been previously applied to this problem. We can use a unigram reading level model to investigate this hypothesis. Returning to the example words for the partial sentence “She drives a nice...”, we can see that our current model classifies the highly expected word, &apos;car&apos;, at reading level 1, while &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at reading levels 5, 6, and 11 respectively. 3.1 Reading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine cla</context>
</contexts>
<marker>Dale, Chall, 1948</marker>
<rawString>Dale, E., &amp; Chall, J. S. (1948). A Formula for Predicting Readability: Instructions. Educational Research Bulletin, Vol. 27(2), 37-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Finn</author>
</authors>
<title>Word frequency, information theory, and cloze performance: A transfer feature theory of processing in reading.</title>
<date>1978</date>
<journal>Reading Research Quarterly,</journal>
<volume>13</volume>
<issue>4</issue>
<pages>508--537</pages>
<contexts>
<context position="8069" citStr="Finn (1978)" startWordPosition="1257" endWordPosition="1258">ighbor classifier were trained to find the most likely words for deletion within news articles. To train the system they labeled cloze sentences from a TOEIC training test as true, then shifted the position of the blanks from those sentences and 50 labeled the resulting sentences as false. Manual evaluation of the results showed that, for both classifiers, experts saw over 90% of the deletions as either easy to solve or merely possible to solve. 3 Reading Level and Information Theory An information-theoretical basis for an entirely novel approach to automated cloze sentence search is found in Finn (1978). Finn defines Cloze Easiness as “the percent of subjects filling in the correct word in a cloze task.” Another metric of the quality of a cloze task is context restriction; the number of solutions perceived as acceptable keys for a given stem. Finn&apos;s theory of lexical feature transfer provides one mechanism to explain context restriction. The theory involves the information content of a blank. According to Shannon&apos;s (1948) seminal work on information theory, the information contained in a given term is inverse to its predictability. In other words, if a term appears despite following a histor</context>
<context position="19425" citStr="Finn (1978)" startWordPosition="3103" endWordPosition="3104">e marketplace for “human intelligence tasks.” Each worker was shown up to twenty of the stems of these sentences as open cloze tasks. No worker was allowed to see more than one stem for the same key. Workers were instructed to enter only those words that “absolutely make sense in this context”, but were not encouraged to submit any particular number of answers. Workers were paid US$.04 per sentence, and the task was limited to workers with approval ratings on past tasks at or above 90%. For each sentence under review each worker contributes one subset of answers. Cloze Easiness, as defined by Finn (1978) is calculated as the percentage of these subsets containing the original key. We define context restriction on n as the percentage of answer subsets containing n or fewer words. Using the example sentence: “Take this cloze sentence, for (example) .” We can find the set of answer subsets A: A = { A1={example, free, fun, me} A2={example,instance} A3={instance} } Then, Cloze Easiness is |{A1,A2} |/ |A |≈ .67 and Context restriction (on one or two words) is | {A2,A3} |/ |A |≈ .67 53 5 Results Each sentence in the final set was seen, on average, by 27 Mechanical Turk workers. We wish to correlate </context>
<context position="22764" citStr="Finn (1978)" startWordPosition="3621" endWordPosition="3622">ally insignificant positive correlations. The lack of significant correlations for three out of four combinations of measures and estimators is not grounds to dismiss these measures. Across all sentences, the measure of context restriction is highly variant, at 47.9%. This is possibly the result of the methodology; in an attempt to avoid biasing the AMT workers, we did not specify the desirable number of answers. This led to many workers interpreting the task differently. In terms of maximum grade level, the lack of a significant correlation with context restriction does not absolutely refute Finn (1978)&apos;s hypothesis. Finn specifies that semantic transfer features should be in “lexical scope” of a blank. A clear definition of “lexical scope” was not presented. We generalized scope to mean proximity within a fixed contextual window size. It is possible that a more precise definition of “lexical scope” will provide a stronger correlation of reading level and context restriction. 5.1 Expert Validation Finally, while we have shown a statistically significant positive correlation between cooccurrence scores and Cloze Easiness, we still need to demonstrate that Cloze Easiness is a valid measure of </context>
</contexts>
<marker>Finn, 1978</marker>
<rawString>Finn, P. J. (1978). Word frequency, information theory, and cloze performance: A transfer feature theory of processing in reading. Reading Research Quarterly, 13(4), 508-537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<date>1979</date>
<institution>Brown Corpus Manual, Brown University Department of Linguistics.</institution>
<location>Providence, RI</location>
<contexts>
<context position="15263" citStr="Francis &amp; Kucera, 1979" startWordPosition="2385" endWordPosition="2388">ed to score each sentence. This parser gives log-probability values corresponding to confidence of the best parse. A threshold for this confidence score was chosen manually and sentences with scores below the threshold were removed, reducing the number of sentences to 29,439. 4.2 Grade Level Grade level in this study is determined by a smoothed unigram model based on normalized concentrations within labeled documents. A sentence is assigned the grade level of the highest level word in context of the key. 4.3 Co-occurrence Scores Skip bigram co-occurrence counts were calculated from the Brown (Francis &amp; Kucera, 1979) and OANC (OANC, 2009) corpora. A given sentence&apos;s score is calculated as the mean of the probabilities of finding that sentence&apos;s context for the key. These probabilities are defined on the triplet (key, word, window size), in which key is the target word to be removed, word any term in the corpus, and window size is a positive integer less than or equal to the length of the sentence. This probability is estimated as the number of times word is found within the same sentence as key and within an absolute window size of 2 positions from key, divided by the total number of times all terms are f</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>Francis, W. N. &amp; Kucera, H. (1979). Brown Corpus Manual, Brown University Department of Linguistics. Providence, RI</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>M Eskenazi</author>
</authors>
<title>An Analysis of Statistical Models and Features for Reading Difficulty Prediction.</title>
<date>2008</date>
<booktitle>3rd Workshop on Innovative Use of NLP for Building Educational Applications. Assoc. for Computational Linguistics.</booktitle>
<contexts>
<context position="11556" citStr="Heilman et al. (2008)" startWordPosition="1808" endWordPosition="1811">eading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for each grade level. These recent methods perform well to characterize the level of an entire document, but they are untested for single sentences. We wish to investigate if a robust unigram model of reading level can be employed to improve the estimation of cloze quality at the sentence level. By extension of Finn&apos;s (1978) hypothesis, it is in fact not the 51 overall level of</context>
</contexts>
<marker>Heilman, Collins-Thompson, Eskenazi, 2008</marker>
<rawString>Heilman, M., Collins-Thompson, K., &amp; Eskenazi, M. (2008). An Analysis of Statistical Models and Features for Reading Difficulty Prediction. 3rd Workshop on Innovative Use of NLP for Building Educational Applications. Assoc. for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
</authors>
<title>Item Distiller: Text retrieval for computer-assisted test item creation. ETS,</title>
<date>2006</date>
<location>Princeton, NJ.</location>
<contexts>
<context position="1494" citStr="Higgins, 2006" startWordPosition="225" endWordPosition="226">es and estimators, which was lexical co-occurrence and Cloze Easiness. Reading difficulty was not found to correlate significantly. We gave subsets of cloze sentences to an English teacher as a gold standard. Sentences selected by co-occurrence and Cloze Easiness were ranked most highly, corroborating the evidence from AMT. 1 Cloze Tasks Cloze tasks, described in Taylor (1953), are activities in which one or several words are removed from a sentence and a student is asked to fill in the missing content. That sentence can be referred to as the &apos;stem&apos;, and the removed term itself as the &apos;key&apos;. (Higgins, 2006) The portion of the sentence from which the key has been removed is the &apos;blank&apos;. &apos;Open cloze&apos; tasks are those in which the student can propose any answer. &apos;Closed cloze&apos; describes multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979), the “cloze procedure” is that in which multiple words are removed at intervals from a text. This is mostly used in first language (L1) education. Alderson describes three deletion strategies: random deletion, deletion of every nth wor</context>
<context position="3849" citStr="Higgins 2006" startWordPosition="585" endWordPosition="586">mplicit, receptive word knowledge, and forces the student 49 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 49–56, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics to consider explicitly how to match features of the stem with what is known about features of any keys she may consider. 2 Automatic Generation of Cloze Tasks Most cloze task “generation” systems are really cloze task identification systems. That is, given a set of requirements, such as a specific key and syntactic structure (Higgins 2006) for the stem, a system looks into a database of pre-processed text and attempts to identify sentences matching those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and </context>
</contexts>
<marker>Higgins, 2006</marker>
<rawString>Higgins, D. (2006). Item Distiller: Text retrieval for computer-assisted test item creation. ETS, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hoshino</author>
<author>H Nakagawa</author>
</authors>
<title>A real-time multiple-choice question generation for language testing – a preliminary study–.</title>
<date>2005</date>
<booktitle>In 2nd Workshop on Building Educational Applications Using NLP</booktitle>
<pages>17--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, MI:</location>
<contexts>
<context position="4520" citStr="Hoshino &amp; Nakagawa, 2005" startWordPosition="692" endWordPosition="695">f pre-processed text and attempts to identify sentences matching those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes </context>
<context position="7146" citStr="Hoshino &amp; Nakagawa (2005)" startWordPosition="1107" endWordPosition="1110">ences. They used the Stanford Parser (Klein &amp; Manning, 2003) to detect sentences within a desired range of complexity and likely well-formedness. Co-occurrence frequencies of words in the corpus were calculated and keys were compared to other words in the stem to determine cloze quality, producing suitable cloze questions 66.53% of the time. This method operates on the theory that the quality of the context of a stem is based on the co-occurrence scores of other words in the sentence. Along with this result, Pino et al. incorporated syntactic complexity in terms of the number of parses found. Hoshino &amp; Nakagawa (2005) use machine learning techniques to train a cloze task search system. Their system, rather than finding sentences suitable for cloze tasks, attempts to automate deletion for passage-based cloze. The features used include sentence length and POS of keys and surrounding words. Both a Naïve Bayes and a KNearest Neighbor classifier were trained to find the most likely words for deletion within news articles. To train the system they labeled cloze sentences from a TOEIC training test as true, then shifted the position of the blanks from those sentences and 50 labeled the resulting sentences as fals</context>
</contexts>
<marker>Hoshino, Nakagawa, 2005</marker>
<rawString>Hoshino, A., &amp; Nakagawa, H. (2005). A real-time multiple-choice question generation for language testing – a preliminary study–. In 2nd Workshop on Building Educational Applications Using NLP (pp. 17-20). Ann Arbor, MI: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Jongsma</author>
</authors>
<title>Cloze instructional research: A second look.</title>
<date>1980</date>
<publisher>International</publisher>
<location>Newark, DE:</location>
<contexts>
<context position="2923" citStr="Jongsma (1980)" startWordPosition="449" endWordPosition="450">s can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthographic, phonetic, syntactic, and semantic constituents. The lexical quality of a given word can then be defined as a measure based on both the depth of knowledge of each constituent and the degree to which those constituents are bonded together. Cloze tasks allow a test author to select for specific combinations of constituents to assess (Bachman, 1982). 1.2 Instructional Cloze Tasks Cloze tasks can be employed for instruction as well as assessment. Jongsma (1980) showed that targeted deletion is an effective use of instructional passage-based cloze tasks. Repeated exposure to frequent words leads first to familiarity with those words, and increasingly to suppositions about their semantic and syntactic constituents. Producing cloze tasks through targeted deletion takes implicit, receptive word knowledge, and forces the student 49 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 49–56, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics to consider exp</context>
</contexts>
<marker>Jongsma, 1980</marker>
<rawString>Jongsma, E. (1980). Cloze instructional research: A second look. Newark, DE: International Reading Association. Urbana, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kincaid</author>
<author>R Fishburne</author>
<author>R Rodgers</author>
<author>B Chissom</author>
</authors>
<title>Derivation of new readability formulas for navy enlisted personnel. Research Branch Report.</title>
<date>1975</date>
<location>Millington, TN.</location>
<contexts>
<context position="11254" citStr="Kincaid et al., 1975" startWordPosition="1761" endWordPosition="1764">to investigate this hypothesis. Returning to the example words for the partial sentence “She drives a nice...”, we can see that our current model classifies the highly expected word, &apos;car&apos;, at reading level 1, while &apos;taxi&apos;,&apos;tank&apos;, and &apos;ambulance&apos;, are at reading levels 5, 6, and 11 respectively. 3.1 Reading Level Estimators The estimation of reading level is a complex topic unto itself. Early work used heuristics based on average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for each grade level. These recent methods perform well to characterize the le</context>
</contexts>
<marker>Kincaid, Fishburne, Rodgers, Chissom, 1975</marker>
<rawString>Kincaid, J., Fishburne, R., Rodgers, R., &amp; Chissom, B. (1975). Derivation of new readability formulas for navy enlisted personnel. Research Branch Report. Millington, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Assoc. for Computational Linguistics.</booktitle>
<pages>423--430</pages>
<contexts>
<context position="6581" citStr="Klein &amp; Manning, 2003" startWordPosition="1015" endWordPosition="1018">in very narrow criteria. They can then alter these sentences as necessary. Liu et al. (2005) use sentences from a corpus of newspaper text tagged for POS and lemma. Candidate sentences are found by searching on the key and its POS as well as the POS sequence of surrounding terms. In their system results are filtered for proper word sense by comparing other words in the stem with data from WordNet and HowNet, databases of inter-word semantic relations. 2.2 Statistical Sentence Search Pino et al (2009) use co-occurrence frequencies to identify candidate sentences. They used the Stanford Parser (Klein &amp; Manning, 2003) to detect sentences within a desired range of complexity and likely well-formedness. Co-occurrence frequencies of words in the corpus were calculated and keys were compared to other words in the stem to determine cloze quality, producing suitable cloze questions 66.53% of the time. This method operates on the theory that the quality of the context of a stem is based on the co-occurrence scores of other words in the sentence. Along with this result, Pino et al. incorporated syntactic complexity in terms of the number of parses found. Hoshino &amp; Nakagawa (2005) use machine learning techniques to</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. &amp; Manning, C. (2003). Accurate Unlexicalized Parsing. (pp. 423-430) In Proceedings of the 41st Meeting of the Assoc. for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Automatic Generation of Cloze Items for Prepositions.</title>
<date>2007</date>
<booktitle>Proceedings of In.</booktitle>
<contexts>
<context position="4559" citStr="Lee &amp; Seneff, 2007" startWordPosition="700" endWordPosition="703">y sentences matching those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of th</context>
</contexts>
<marker>Lee, Seneff, 2007</marker>
<rawString>Lee, J., &amp; Seneff, S. (2007). Automatic Generation of Cloze Items for Prepositions. Proceedings of In.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Liu</author>
<author>C Wang</author>
<author>Z Gao</author>
<author>S Huang</author>
</authors>
<title>Applications of Lexical Information for Algorithmically Composing Multiple-Choice Cloze Items.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP (p. 1–8).</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, MI:</location>
<contexts>
<context position="4538" citStr="Liu et al., 2005" startWordPosition="696" endWordPosition="699">ttempts to identify sentences matching those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the origin</context>
<context position="6051" citStr="Liu et al. (2005)" startWordPosition="929" endWordPosition="932">takes the capabilities of tgrep yet further, providing a simplified interface for linguists to search within tagged corpora along both syntactic and lexical features. Both tgrep and the Linguists&apos; Search Engine were not designed as cloze sentence search tools, but they paved the way for similar tools specialized for this task. For example, Higgins&apos; (2006) system uses a regular expression engine that can work either on the tag level, the text level or both. This allows test content creators to quickly find sentences within very narrow criteria. They can then alter these sentences as necessary. Liu et al. (2005) use sentences from a corpus of newspaper text tagged for POS and lemma. Candidate sentences are found by searching on the key and its POS as well as the POS sequence of surrounding terms. In their system results are filtered for proper word sense by comparing other words in the stem with data from WordNet and HowNet, databases of inter-word semantic relations. 2.2 Statistical Sentence Search Pino et al (2009) use co-occurrence frequencies to identify candidate sentences. They used the Stanford Parser (Klein &amp; Manning, 2003) to detect sentences within a desired range of complexity and likely w</context>
<context position="18483" citStr="Liu et al., 2005" startWordPosition="2947" endWordPosition="2950">on. Each grade level filter was combined with each co-occurrence score filter creating 4x4=16 composite filters. By combining these filters we can create a final set of sentences for analysis with high confidence of having a significant number of sentences representing all possible values of grade level and co-occurrence score. At most two sentences were chosen for each of the 20 keys using these composite filters. The final number of sentences was 540. 4.5 Experimental Cloze Quality Previous evaluation of automatically generated cloze tasks has relied on expert judgments. (Pino et al., 2008; Liu et al., 2005) We present the use of crowdsourcing techniques as a new approach for this evaluation. We believe the approach can be validated by statistically significant correlations with predicted cloze quality and comparison with expert judgments. The set of 540 sentences were presented to workers from Amazon Mechanical Turk (AMT), an online marketplace for “human intelligence tasks.” Each worker was shown up to twenty of the stems of these sentences as open cloze tasks. No worker was allowed to see more than one stem for the same key. Workers were instructed to enter only those words that “absolutely ma</context>
</contexts>
<marker>Liu, Wang, Gao, Huang, 2005</marker>
<rawString>Liu, C., Wang, C., Gao, Z., &amp; Huang, S. (2005). Applications of Lexical Information for Algorithmically Composing Multiple-Choice Cloze Items. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP (p. 1–8). Ann Arbor, MI: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<date>2009</date>
<institution>Open American National Corpus</institution>
<note>americannationalcorpus.org/OANC/</note>
<contexts>
<context position="6464" citStr="(2009)" startWordPosition="1002" endWordPosition="1002">e tag level, the text level or both. This allows test content creators to quickly find sentences within very narrow criteria. They can then alter these sentences as necessary. Liu et al. (2005) use sentences from a corpus of newspaper text tagged for POS and lemma. Candidate sentences are found by searching on the key and its POS as well as the POS sequence of surrounding terms. In their system results are filtered for proper word sense by comparing other words in the stem with data from WordNet and HowNet, databases of inter-word semantic relations. 2.2 Statistical Sentence Search Pino et al (2009) use co-occurrence frequencies to identify candidate sentences. They used the Stanford Parser (Klein &amp; Manning, 2003) to detect sentences within a desired range of complexity and likely well-formedness. Co-occurrence frequencies of words in the corpus were calculated and keys were compared to other words in the stem to determine cloze quality, producing suitable cloze questions 66.53% of the time. This method operates on the theory that the quality of the context of a stem is based on the co-occurrence scores of other words in the sentence. Along with this result, Pino et al. incorporated synt</context>
</contexts>
<marker>2009</marker>
<rawString>Open American National Corpus (2009) americannationalcorpus.org/OANC/</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Perfetti</author>
<author>L Hart</author>
</authors>
<title>Lexical bases of comprehension skill.</title>
<date>2001</date>
<journal>(D. Gorfein)</journal>
<pages>67--86</pages>
<publisher>American Psychological Association.</publisher>
<location>Washington D.C.:</location>
<contexts>
<context position="2244" citStr="Perfetti &amp; Hart, 2001" startWordPosition="344" endWordPosition="347">nt can propose any answer. &apos;Closed cloze&apos; describes multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979), the “cloze procedure” is that in which multiple words are removed at intervals from a text. This is mostly used in first language (L1) education. Alderson describes three deletion strategies: random deletion, deletion of every nth word, and targeted deletion, in which certain words are manually chosen and deleted by an instructor. Theories of lexical quality (Perfetti &amp; Hart, 2001) and word knowledge levels (Dale, 1965) illustrate why cloze tasks can effectively assess multiple dimensions of vocabulary knowledge. Perfetti &amp; Hart explain that lexical knowledge can be decomposed into orthographic, phonetic, syntactic, and semantic constituents. The lexical quality of a given word can then be defined as a measure based on both the depth of knowledge of each constituent and the degree to which those constituents are bonded together. Cloze tasks allow a test author to select for specific combinations of constituents to assess (Bachman, 1982). 1.2 Instructional Cloze Tasks Cl</context>
</contexts>
<marker>Perfetti, Hart, 2001</marker>
<rawString>Perfetti, C., &amp; Hart, L. (2001). Lexical bases of comprehension skill. (D. Gorfein) (pp. 67-86). Washington D.C.: American Psychological Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>Assessing the reading level of web pages.</title>
<date>2006</date>
<journal>In ICSLP</journal>
<volume>(Vol.</volume>
<pages>833--836</pages>
<contexts>
<context position="11685" citStr="Petersen &amp; Ostendorf, 2006" startWordPosition="1826" endWordPosition="1829"> average sentence length and the percentage of words deemed unknown to a baseline reader. (Dale &amp; Chall, 1948; Dale, 1965) Another early measure, the Flesch-Kincaid measure, (Kincaid et al., 1975) uses a function of the syllable length of words in a document and the average sentence length. More recent work on the topic also focuses on readability classification at the document level. Collins-Thompson &amp; Callan (2005) use unigram language models without syntactic features. Heilman et al. (2008) use a probabilistic parser and unigram language models to combine grammatical and lexical features. (Petersen &amp; Ostendorf, 2006) add higher-order n-gram features to the above to train support vector machine classifiers for each grade level. These recent methods perform well to characterize the level of an entire document, but they are untested for single sentences. We wish to investigate if a robust unigram model of reading level can be employed to improve the estimation of cloze quality at the sentence level. By extension of Finn&apos;s (1978) hypothesis, it is in fact not the 51 overall level of the sentence that has a predicted effect on cloze context restriction, but rather the reading level of the words in proximity to</context>
</contexts>
<marker>Petersen, Ostendorf, 2006</marker>
<rawString>Petersen, S. E., &amp; Ostendorf, M. (2006). Assessing the reading level of web pages. In ICSLP (Vol. pages, pp. 833-836).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pino</author>
<author>M Heilman</author>
<author>M Eskenazi</author>
</authors>
<title>A Selection Strategy to Improve Cloze Question Quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Intelligent Tutoring Systems for Ill-Defined Domains.</booktitle>
<contexts>
<context position="4579" citStr="Pino et al. (2008)" startWordPosition="704" endWordPosition="707">those criteria. Thus, the content generated for a closed cloze is the stem (by deletion of the key), and a set of distractors. In the case of some systems, a human content author may manually tailor the resulting stems to meet further needs. Identifying suitable sentences from natural language corpora is desirable because the sentences that are found will be authentic. Depending on the choice of corpora, sentences should also be well-formed and suitable in terms of reading level and content. Newspaper text is one popular source (Hoshino &amp; Nakagawa, 2005; Liu et al., 2005; Lee &amp; Seneff, 2007). Pino et al. (2008) use documents from a corpus of texts retrieved from the internet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of the Penn Treebank and </context>
<context position="13326" citStr="Pino et al., 2008" startWordPosition="2094" endWordPosition="2097">n. The subset predicted to contain the best sentences by each filter was finally submitted to expert review as a gold standard test of cloze quality. This study compares two features of sentences, finding the levels of context restriction experimentally. The first feature in question is the maximum reading level found in near-context to the blank. The second feature is the mean skip bigram co-occurrence score of words within that context. Amazon Mechanical Turk (AMT) is used as a novel cloze quality evaluation method. This method is validated by both positive correlation with the known-valid (Pino et al., 2008) cooccurrence score predictor, and an expert gold standard. Experimental results from AMT are then used to evaluate the hypothesis that reading level can be used as a new, alternative predictor of cloze quality. 4.1 Cloze Sentence Filtering The first step in preparing material for this study was to obtain a set of keys. We expect that in most applications of sentence-based cloze tasks the set of keys is pre-determined by instructional goals. Due to this constraint, we choose a set of keys distributed across several reading levels and hold it as fixed. Four words were picked from the set of wor</context>
<context position="18464" citStr="Pino et al., 2008" startWordPosition="2943" endWordPosition="2946">g quartile, and so on. Each grade level filter was combined with each co-occurrence score filter creating 4x4=16 composite filters. By combining these filters we can create a final set of sentences for analysis with high confidence of having a significant number of sentences representing all possible values of grade level and co-occurrence score. At most two sentences were chosen for each of the 20 keys using these composite filters. The final number of sentences was 540. 4.5 Experimental Cloze Quality Previous evaluation of automatically generated cloze tasks has relied on expert judgments. (Pino et al., 2008; Liu et al., 2005) We present the use of crowdsourcing techniques as a new approach for this evaluation. We believe the approach can be validated by statistically significant correlations with predicted cloze quality and comparison with expert judgments. The set of 540 sentences were presented to workers from Amazon Mechanical Turk (AMT), an online marketplace for “human intelligence tasks.” Each worker was shown up to twenty of the stems of these sentences as open cloze tasks. No worker was allowed to see more than one stem for the same key. Workers were instructed to enter only those words </context>
</contexts>
<marker>Pino, Heilman, Eskenazi, 2008</marker>
<rawString>Pino, J., Heilman, M., &amp; Eskenazi, M. (2008). A Selection Strategy to Improve Cloze Question Quality. In Proceedings of the Workshop on Intelligent Tutoring Systems for Ill-Defined Domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pito</author>
</authors>
<date>1994</date>
<note>tgrep README www.ldc.upenn.edu/ldc/online/treebank/README.long</note>
<contexts>
<context position="5250" citStr="Pito, 1994" startWordPosition="803" endWordPosition="804">ernet and subsequently filtered according to readability level, category, and appropriateness of content. Using a broader corpus increases the number and variability of potential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of the Penn Treebank and the corresponding tgrep program. Developed by Pito in 1992 (Pito, 1994) this program allows researchers to search for corpus text according to sequences of part of speech (POS) tags and tree structure. The linguists&apos; Search Engine (Resnik &amp; Elkiss, 2005) takes the capabilities of tgrep yet further, providing a simplified interface for linguists to search within tagged corpora along both syntactic and lexical features. Both tgrep and the Linguists&apos; Search Engine were not designed as cloze sentence search tools, but they paved the way for similar tools specialized for this task. For example, Higgins&apos; (2006) system uses a regular expression engine that can work eith</context>
</contexts>
<marker>Pito, 1994</marker>
<rawString>Pito, R. (1994). tgrep README www.ldc.upenn.edu/ldc/online/treebank/README.long</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>A Elkiss</author>
</authors>
<title>The Linguist’s Search Engine: An Overview. Association for Computational Linguistics,</title>
<date>2005</date>
<pages>33--36</pages>
<contexts>
<context position="5433" citStr="Resnik &amp; Elkiss, 2005" startWordPosition="830" endWordPosition="833">ential matching sentences, but also lowers the confidence that sentences will be well-formed and contain appropriate language (Brown &amp; Eskenazi, 2004). 2.1 Tag-based Sentence Search Several cloze item authoring tools (Liu et al. 2005; Higgins, 2006) implement specialized tag-based sentence search. This goes back to the original distribution of the Penn Treebank and the corresponding tgrep program. Developed by Pito in 1992 (Pito, 1994) this program allows researchers to search for corpus text according to sequences of part of speech (POS) tags and tree structure. The linguists&apos; Search Engine (Resnik &amp; Elkiss, 2005) takes the capabilities of tgrep yet further, providing a simplified interface for linguists to search within tagged corpora along both syntactic and lexical features. Both tgrep and the Linguists&apos; Search Engine were not designed as cloze sentence search tools, but they paved the way for similar tools specialized for this task. For example, Higgins&apos; (2006) system uses a regular expression engine that can work either on the tag level, the text level or both. This allows test content creators to quickly find sentences within very narrow criteria. They can then alter these sentences as necessary.</context>
</contexts>
<marker>Resnik, Elkiss, 2005</marker>
<rawString>Resnik, P., &amp; Elkiss, A. (2005). The Linguist’s Search Engine: An Overview. Association for Computational Linguistics, (June), 33-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A Mathematical Theory of Communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>379--423</pages>
<marker>Shannon, 1948</marker>
<rawString>Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27, 379– 423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Taylor</author>
</authors>
<title>Cloze procedure: a new tool for measuring readability.</title>
<date>1953</date>
<journal>Journalism Quarterly,</journal>
<volume>30</volume>
<pages>415--453</pages>
<contexts>
<context position="1259" citStr="Taylor (1953)" startWordPosition="182" endWordPosition="183">tion of cloze quality, we submitted open cloze tasks to workers on Amazon Mechanical Turk (AMT) and discuss ways to measure of the results of these tasks. Results show one statistically significant correlation between the above measures and estimators, which was lexical co-occurrence and Cloze Easiness. Reading difficulty was not found to correlate significantly. We gave subsets of cloze sentences to an English teacher as a gold standard. Sentences selected by co-occurrence and Cloze Easiness were ranked most highly, corroborating the evidence from AMT. 1 Cloze Tasks Cloze tasks, described in Taylor (1953), are activities in which one or several words are removed from a sentence and a student is asked to fill in the missing content. That sentence can be referred to as the &apos;stem&apos;, and the removed term itself as the &apos;key&apos;. (Higgins, 2006) The portion of the sentence from which the key has been removed is the &apos;blank&apos;. &apos;Open cloze&apos; tasks are those in which the student can propose any answer. &apos;Closed cloze&apos; describes multiple choice tasks in which the key is presented along with a set of several &apos;distractors&apos;. Assessment is the best known application of cloze tasks. As described in (Alderson, 1979),</context>
</contexts>
<marker>Taylor, 1953</marker>
<rawString>Taylor, W. L. (1953). Cloze procedure: a new tool for measuring readability. Journalism Quarterly, 30, 415-453.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>