<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.961116">
Incremental Tree Substitution Grammar
for Parsing and Sentence Prediction
</title>
<author confidence="0.99622">
Federico Sangati and Frank Keller
</author>
<affiliation confidence="0.999652">
Institute for Language, Cognition, and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.982693">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.992603">
federico.sangati@gmail.com keller@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999715857142857">
In this paper, we present the first incremental
parser for Tree Substitution Grammar (TSG).
A TSG allows arbitrarily large syntactic frag-
ments to be combined into complete trees;
we show how constraints (including lexical-
ization) can be imposed on the shape of the
TSG fragments to enable incremental process-
ing. We propose an efficient Earley-based al-
gorithm for incremental TSG parsing and re-
port an F-score competitive with other incre-
mental parsers. In addition to whole-sentence
F-score, we also evaluate the partial trees that
the parser constructs for sentence prefixes;
partial trees play an important role in incre-
mental interpretation, language modeling, and
psycholinguistics. Unlike existing parsers, our
incremental TSG parser can generate partial
trees that include predictions about the up-
coming words in a sentence. We show that
it outperforms an n-gram model in predicting
more than one upcoming word.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999716645833333">
When humans listen to speech, the input becomes
available gradually as the speech signal unfolds.
Reading happens in a similarly gradual manner
when the eyes scan a text. There is good evidence
that the human language processor is adapted to this
and works incrementally, i.e., computes an interpre-
tation for an incoming sentence on a word-by-word
basis (Tanenhaus et al., 1995; Altmann and Kamide,
1999). Also language processing systems often deal
with speech as it is spoken, or text as it is being
typed. A dialogue system should start interpreting
a sentence while it is being spoken, and a question
answering system should start retrieving answers be-
fore the user has finished typing the question.
Incremental processing is therefore essential both
for realistic models of human language processing
and for NLP applications that react to user input
in real time. In response to this, a number of in-
cremental parsers have been developed, which use
context-free grammar (Roark, 2001; Schuler et al.,
2010), dependency grammar (Chelba and Jelinek,
2000; Nivre, 2007; Huang and Sagae, 2010), or tree-
adjoining grammar (Demberg et al., 2014). Typical
applications of incremental parsers include speech
recognition (Chelba and Jelinek, 2000; Roark, 2001;
Xu et al., 2002), machine translation (Schwartz
et al., 2011; Tan et al., 2011), reading time modeling
(Demberg and Keller, 2008), or dialogue systems
(Stoness et al., 2004). Another potential use of incre-
mental parsers is sentence prediction, i.e., the task
of predicting upcoming words in a sentence given a
prefix. However, so far only n-gram models and clas-
sifiers have been used for this task (Fazly and Hirst,
2003; Eng and Eisner, 2004; Grabski and Scheffer,
2004; Bickel et al., 2005; Li and Hirst, 2005).
In this paper, we present an incremental parser for
Tree Substitution Grammar (TSG). A TSG contains
a set of arbitrarily large tree fragments, which can be
combined into new syntax trees by means of a sub-
stitution operation. An extensive tradition of parsing
with TSG (also referred to as data-oriented parsing)
exists (Bod, 1995; Bod et al., 2003), but none of the
existing TSG parsers are incremental. We show how
constraints can be imposed on the shape of the TSG
fragments to enable incremental processing. We pro-
pose an efficient Earley-based algorithm for incre-
mental TSG parsing and report an F-score competi-
tive with other incremental parsers.
</bodyText>
<page confidence="0.647454">
111
</page>
<note confidence="0.7811875">
Transactions of the Association for Computational Linguistics, 1 (2013) 111–124. Action Editor: David Chiang.
Submitted 10/2012; Revised 2/2013; Published 5/2013. c�2013 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999589933333333">
TSG fragments can be arbitrarily large and can
contain multiple lexical items. This property enables
our incremental TSG parser to generate partial parse
trees that include predictions about the upcoming
words in a sentence. It can therefore be applied di-
rectly to the task of sentence prediction, simply by
reading off the predicted items in a partial tree. We
show that our parser outperforms an n-gram model
in predicting more than one upcoming word.
The rest of the paper is structured as follows. In
Section 2, we introduce the ITSG framework and re-
late it to the original TSG formalism. Section 3 de-
scribes the chart-parser algorithm, while Section 4
details the experimental setup and results. Sections 5
and 6 present related work and conclusions.
</bodyText>
<sectionHeader confidence="0.992895" genericHeader="introduction">
2 Incremental Tree Substitution Grammar
</sectionHeader>
<bodyText confidence="0.999948714285714">
The current work is based on Tree Substitu-
tion Grammar (TSG, Schabes 1990; for a recent
overview see Bod et al. 2003). A TSG is composed
of (i) a set of arbitrarily large fragments, usually ex-
tracted from an annotated phrase-structure treebank,
and (ii) the substitution operation by means of which
fragments can be combined into complete syntactic
analyses (derivations) of novel sentences.
Every fragment’s node is either a lexical node
(word), a substitution site (a non-lexical node in the
yield of the structure),1 or an internal node. An inter-
nal node must always keep the same daughter nodes
as in the original tree. For an example of a binarized2
tree and a fragment extracted from it see Figure 1.
A TSG derivation is constructed in a top-down
generative process starting from a fragment in the
grammar rooted in S (the unique non-lexical node
all syntactic analysis are rooted in). A partial deriva-
tion is extended by subsequently introducing more
fragments: if X is the left-most substitution site in
the yield of the current partial derivation, a fragment
</bodyText>
<footnote confidence="0.95320575">
1For example nodes NP, VP, S@ are the substitution sites of
the right fragment in Figure 1.
2The tree is right-binarized via artificial nodes with @ sym-
bols, as explained in Section 4.1. The original tree is
</footnote>
<figure confidence="0.981282">
S
S
S@
S@
.
“.
VP
VP
VBD
NP
NNS
.
“.”
“Terms” “were” VBN
“disclosed”
</figure>
<page confidence="0.499273">
”
112
</page>
<bodyText confidence="0.999453222222222">
The generative process starts with a fragment an-
chored in the first word of the sentence being gener-
ated. At each subsequent step, a lexicalized fragment
is introduced (by means of the substitution opera-
tion) to extend the current partial derivation in such
a way that the prefix of the yield of the partial struc-
ture is lengthened by one word (the lexical anchor of
the fragment being introduced). The lexicalization
constraint allows a fragment to have multiple lexical
items, not necessarily adjacent to one another. This
is useful to capture the general ability of TSG to pro-
duce in one single step an arbitrarily big syntactic
construction ranging from phrasal verbs (e.g., ask
someone out), to parallel constructions (e.g., either
X or Y), and idiomatic expressions (e.g., took me
to the cleaners). For an example of a fragment with
multiple lexical anchors see the fragment in the mid-
dle of Figure 2.
</bodyText>
<subsectionHeader confidence="0.999623">
2.2 Symbolic Grammar
</subsectionHeader>
<bodyText confidence="0.992399111111111">
An ITSG is a tuple (N ,L ,F,®,®,®), where N
and L are the set of non-lexical and lexical nodes
respectively, F is a collection of lexicalized frag-
ments, ® and ® are two variants of the substitution
operation (backward and forward) used to combine
fragments into derivations, and ® is the stop opera-
tion which terminates the generative process.
Fragments A fragment f E F belongs to one of
the three sets Finit,FXlex,FYsub:
</bodyText>
<listItem confidence="0.9481398">
• An initial fragment (finit) has the lexical anchor
in the first position of the yield, being the initial
word of a sentence (the left-most lexical node
of the parse tree from which it was extracted).
• A lex-first fragment (fXlex) has the lexical anchor
(non sentence-initial) in the first position of the
yield, and is rooted in X.3
• A sub-first fragment (fYsub) has the lexical an-
chor in the second position of its yield, and a
substitution site Y in the first.
</listItem>
<bodyText confidence="0.946944">
Fringes We will use fringes (Demberg et al.,
2014) as a compressed representation of fragments,
3A fragment can be both an initial and a lex-first fragment
(e.g., if the lexical anchor is a proper noun). We will make use
of two separate instances of the same fragment in the two sets.
</bodyText>
<equation confidence="0.741171">
VBD VP
“were” “.”
</equation>
<figureCaption confidence="0.9439565">
Figure 2: An example of an ITSG derivation yielding the
tree on the left side of Figure 1. The second and third frag-
ment are introduced by means of forward and backward
substitution, respectively.
</figureCaption>
<bodyText confidence="0.99973303030303">
in which the internal structure is replaced by a trian-
gle (0 or ✁) and only the root and the yield are vis-
ible. It is possible in a grammar that multiple frag-
ments map to the same fringe; we will refer to those
as ambiguous fringes. We use both vertical (0, e.g.,
in Figure 3 and 4) and horizontal (✁) fringe nota-
tion. The latter is used for describing the states in our
chart-parsing algorithm in Section 3. For instance,
the horizontal fringe representation of the right frag-
ment in Figure 1 is S ✁ NP “were” VP S@.
Incremental Derivation An incremental deriva-
tion is a sequence of lexicalized fragments
(f1, f2,..., fn) which, combined together in the
specified order, give rise to a complete parse tree
(see Figure 2 for an example). The first fragment f1
being introduced in the derivation must be an initial
fragment, and its lexical anchor constitutes the one-
word prefix of the sentence being generated. Sub-
sequent fragments are introduced by means of the
substitution operation, which has two variants: back-
ward substitution (@), which is used to substitute
lex-first fragments into the partial derivation gener-
ated so far, and forward substitution (®), which is
used to substitute sub-first fragments into the partial
derivation. After a number of fragments are intro-
duced, a stop operation (®) may terminate the gen-
erative process.
Operations The three ITSG operations take place
under specific conditions within an incremental
derivation, as illustrated in Figure 3 and explained
hereafter. At a given stage of the generative process
(after an initial fragment has been inserted), the con-
nected partial structure may or may not have sub-
</bodyText>
<figure confidence="0.968787">
NP
NNS
“Terms”
® S
® VP ®S
VBN
“disclosed”
NP
VP
S@
S@
113
</figure>
<figureCaption confidence="0.860210666666667">
Figure 3: Schemata of the three ITSG operations. All tree structures (partial structure and fragments) are represented
in a compact notation, which displays only the root nodes and the yields. The i-th words in the structure’s yield is
represented as `i, while α and β stands for any (possibly empty) sequence of words and substitution sites.
</figureCaption>
<figure confidence="0.87991172">
Partial Structure Operation Accepted Fragment Resulting Structure Terminated
`1 lex... `i Y `i+1 α... `1 lex... `i `i+1 α...
NO
`i+1β
...
NO
® X
(backward)
® X
(forward)
Y
`1 lex... `i+1 β
... α...
X
Y
`1 lex... `i X α...
Y
Y
YES
® 0
(stop)
Y #
0
`1 lex... `n #
`1 lex... `n
</figure>
<bodyText confidence="0.998561466666667">
stitution sites present in its yield. In the first case,
a backward substitution (®) must take place in the
following generative step: if X is the left-most sub-
stitution site, a new fragment of type fXlex is chosen
from the grammar and substituted into X. If the par-
tially derived structure has no substitution site (all
the nodes in its yield are lexical nodes) and it is
rooted in Y, two possible choices exist: either the
generative process terminates by means of the stop
operation (®Y), or the generative process contin-
ues. In the latter case a forward substitution (®) is
performed: a new fYsub fragment is chosen from the
grammar, and the partial structure is substituted into
the left-most substitution site Y of the fragment.4
Multiple Derivations As in TSG, an ITSG may
be able to generate the same parse tree in multiple
ways: multiple incremental derivations yielding the
same tree. Figure 4 shows one such example.
Generative Capacity It is useful to clarify the dif-
ference between ITSG and the more general TSG
formalism in terms of generative capacity. Although
both types of grammar make use of the substitu-
tion operation to combine fragments, an ITSG im-
poses more constraints on (i) the type of fragments
which are allowed in the grammar (initial, lex-first,
4A stop operation can be viewed as a forward substitution
when using an artificial sub-first fragment ∅ ✁Y# (stop frag-
ment), where # is an artificial lexical node indicating the termi-
nation of the sentence. For simplicity, stop fragments are omit-
ted in Figure 2 and 4 and Y is attached to the stop symbol (®Y).
</bodyText>
<equation confidence="0.5887325">
® ® ®S
® ® ®S
</equation>
<bodyText confidence="0.871175923076923">
Figure 4: Above: an example of a set of fragments ex-
tracted from the tree in Figure 1. Below: two incremental
derivations that generate it. Colors (and lines strokes) in-
dicate which derivation fragments belong to.
and sub-first fragments), and (ii) the generative pro-
cess with which fragments are combined (incremen-
tally left to right instead of top-down). If we com-
pare a TSG and an ITSG on the same set of (ITSG-
compatible) fragments, then there are cases in which
the TSG can generate more tree structures than the
ITSG.
In the following, we provide a more formal char-
acterization of the strong and weak generative power
</bodyText>
<equation confidence="0.988242170731707">
114
S
“a”
S
X
X
X “c”
X “c”
X
X “c”
X
“b”
“c”
115
Start(`0)
X ✁`0ν
0 : 0X ✁ •`0ν [α,γ,β]
α = γ = P(X ✁`0ν) β = β(1 : 0X ✁`0 •ν)
Backward Substitution(`i)
i : kX ✁λ•Yµ [α,γ,β] Y ✁`iν
i : iY ✁ •`iν [α0,γ0,β0]
α0 =+ α·P(Y✁`iν) γ0 = P(Y✁`iν)
Forward Substitution(`i)
i : 0Y ✁ν• [α,γ,β] X ✁Y`iµ
i : 0X ✁Y • `iµ [α0,γ0,β0]
α0 = + α·P(X✁Y`iµ) = β0·P(X✁Y`iµ)
γ0 = + γ·P(X✁Y`iµ) β +
Completion
i : jY ✁ `jν • [α,γ,β] j : kX ✁ λ •Yµ [α0,γ0,β0]
i : kX ✁λY •µ [α00,γ00,β00]
Scan(`i) α00 =+ α0·γ β+ = β00 · γ0
γ00 =+ γ0·γ β0 += β00 · γ00
i : kX ✁λ• `iµ [α,γ,β]
i+ 1 : kX ✁λ`i •µ [α0,γ0,β0]
α0 = α β = β0
n : 0Y ✁ν• [α = γ,β] Ø✁Y#
n : 0Ø✁Y •# [α0,γ0,β0]
α0 = γ0 = α · P((DY) β0 = 1
β = P(®Y)
γ0 = γ
Stop(#)
</equation>
<figureCaption confidence="0.9842115">
Figure 6: Chart operations with forward (α), inner (γ),
and outer (β) probabilities.
</figureCaption>
<bodyText confidence="0.99945725">
Each state is composed of a fringe and some ad-
ditional information which keeps track of where the
fringe is located within a path. A chart state can be
generally represented as
</bodyText>
<equation confidence="0.980318">
i : kX ✁λ•µ (6)
</equation>
<bodyText confidence="0.990644">
where X ✁ λµ is the state’s fringe, Greek letters are
(possibly empty) sequences of words and substitu-
tion sites, and • is a placeholder indicating to which
extent the fragment’s yield has been consumed: all
the elements in the yield preceding the dot have
been already accepted. Finally, i and k are indices
of words in the input sentence:
</bodyText>
<listItem confidence="0.7334602">
• i signifies that the current state is introduced
after the first i words in the sentence have
been scanned. All states in the chart will be
grouped according to this index, and will con-
stitute state-set i.
• k indicates that the fringe associated with the
current state was first introduced in the chart
after the first k words in the input sentence had
been scanned. The index k is therefore called
the start index.
</listItem>
<bodyText confidence="0.999812333333333">
For instance when generating the first incremental
derivation in Figure 4, the parser will pass through
state 1 : 1S ✁ NP • “were” VP “.” indicating that
the second fringe is introduced right after the parser
has scanned the first word in the sentence and before
having scanned the second word.
</bodyText>
<subsectionHeader confidence="0.99987">
3.2 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999402074074074">
We will first introduce the symbolic part of the
parsing algorithm, and then discuss its probabilistic
component in Section 3.3. In line with the generative
process illustrated in Section 2.2, the parser operates
on the chart states in order to keep track of all pos-
sible ITSG derivations as new words are fed in. It
starts by reading the first word `0 and introducing
new states to state-set 0 in the chart, those mapping
to initial fragments in the grammar with `0 as lexi-
cal anchor. At a given stage, after i words have been
scanned, the parser reads the next word (`i) and in-
troduces new states in state-sets i and i+1 by apply-
ing specific operations on states present in the chart,
and fringes in the grammar.
Parser Operations The parser begins with the
start operation just described, and continues with a
cycle of four operations for every word in the input
sentence `i (for i ≥ 0). The order of the four opera-
tions is the following: completion, backward substi-
tution (®), forward substitution (®), and scan. When
there are no more words in input, the parser termi-
nates with a stop operation. We will now describe
the parser operations (see Figure 6 for their formal
definition), ignoring the probabilities for now.
Start(`0): For every initial fringe in the grammar
anchored in `0, the parser inserts a (scan) state for
that fringe in the state-set 0.
</bodyText>
<page confidence="0.753091">
116
</page>
<bodyText confidence="0.998762225">
Backward Substitution(`i) applies to acceptor
states, i.e., those with a substitution site following
the dot, say X. For each acceptor state in state-set i,
and any lex-first fringe in the grammar rooted in X
and anchored in `i, the parser inserts a (scan) state
for that fringe in state-set i.
Forward Substitution(`i) applies to donor
states, i.e., those that have no elements following
the dot and with start index 0. For each donor state
in state-set i, rooted in Y, and any sub-first fringe in
the grammar with Y as the left-most element in its
yield, the parser inserts a (scan) state for that fringe
in state-set i, with the dot placed after Y.
Completion applies to complete states, i.e., those
with no elements following the dot and with start
index j &gt; 0. For every complete state in state-set i,
rooted in Y, with starting index j, and every acceptor
state in set j with Y following the dot, the parser
inserts a copy of the acceptor state in state-set i, and
advances the dot.
Scan(`i) applies to scan states, i.e., those with a
word after the dot. For every scan state in state-set
i having `i after the dot, the parser inserts a copy of
that state in state-set (i+ 1), and advances the dot.
Stop(#) is a special type of forward substitution
and applies to donor states, but only when the input
word is the terminal symbol #. For every donor state
in state-set n (the length of the sentence), if the root
of the fringe’s state is Y, the parser introduces a stop
state whose fringe is a stop fringe with Y as the left
most substitution site.
Comparison with the Earley Algorithm It is
useful to clarify the differences between the pro-
posed ITSG parsing algorithm and the original Ear-
ley algorithm. Primarily, the ITSG parser is based
on a left-right processing order, whereas the Ear-
ley algorithm uses a top-down generative process.
Moreover, our parser presupposes a restricted in-
ventory of fragments in the grammar (the ones al-
lowed by an ITSG) as opposed to the general CFG
rules allowed by the Earley algorithm. In particular,
the Backward Substitution operation is more limited
than the corresponding Prediction step of the Earley
algorithm: only lex-first fragments can be introduced
using Backward Substitution, and therefore left re-
cursion (allowed by the Earley algorithm) is not pos-
sible here.6 This restriction is compensated for by
the existence of the Forward Substitution operation,
which has no analog in the Earley algorithm.7 The
worst case complexity of Earley algorithm is domi-
nated by the Completion operation which is identical
to that in our parser, and therefore the original total
time complexity applies, i.e., O(l3) for an input sen-
tence of length l, and O(n3) in terms of the number
of non-lexical nodes n in the grammar.
Derivations Incremental (partial) derivations are
represented in the chart as (partial) paths along
states. Each state can lead to one or more succes-
sors, and come from one or more antecedents. Scan
is the only operation which introduces, for every
scan state, a new single successor state (which can
be of any of the four types) in the following state-
set. Complete states may lead to several states within
the current state-set, which may belong to any of the
four types. An acceptor state may lead to a number
of scan states via backward substitution (depending
on the number of lex-first fringes that can combine
with it). Similarly, a donor state may lead to a num-
ber of scan states via forward substitution.
After i words have been scanned, we can retrieve
(partial) paths from the chart. This is done in a back-
ward direction starting from scan states in state-set i
all the way back to the initial states. This is possible
since all the operations are reversible, i.e., given a
state it is possible to retrieve its antecedent state(s).
As an example, consider the ITSG grammar con-
sisting of the fragments in Figure 7 and the two de-
rivations of the same parse tree in the same figure;
Figure 7 represents the parsing chart of the same
grammar, containing the two corresponding paths.
</bodyText>
<subsectionHeader confidence="0.998824">
3.3 Probabilistic Parser
</subsectionHeader>
<bodyText confidence="0.787078818181818">
In the probabilistic version of the parser, each fringe
in the grammar has a given probability, such that
Equations (1)–(3) are satisfied.8 In the probabilistic
chart, every state i : kX ✁λ•µ is decorated with three
6This further simplifies the probabilistic version of our
parser, as there is no need to resort to the probabilistic reflex-
ive, transitive left-corner relation described by Stolcke (1995).
7This operation would violate Earley’s top-down constraint;
donor states are in fact the terminal states in Earley algorithm.
8The probability of an ambiguous fringe is the marginal
probability of the fragments mapping to it.
</bodyText>
<table confidence="0.9807779375">
117
0 – “Terms”
S  |0NP✁• “Terms” [1/2, 1/2, 1]
 ||0S S@ [1/2, 1/2, 1]
✁
“Terms”
•
3 – “.”
S  |0S✁NP “were” VP • “.” [1/2, 1/2, 1]
 ||1S@ ✁ “were” VP • “.” [1/2, 1, 1/2]
C 2VP✁“disclosed” • [1, 1, 1] ||***  |**
4 – #
S 0∅ ✁ S • # [1, 1, 1]
®  ||oS✁ “Terms” S@ • [1/2, 1/2, 1] ∅✁S # [1 ]
� 0S✁NP “were” VP “.” • [1/2, 1/2, 1]
C  ||1S@ ✁ “were” VP “.” • [1/2, 1, 1/2]  ||*
</table>
<figureCaption confidence="0.985419">
Figure 7: The parsing chart of the two derivations in Figure 4. Blue states or fringes (also marked with |) are the ones in
</figureCaption>
<bodyText confidence="0.986413833333333">
the first derivation, red (||) in the second, and yellow (no marks) are the ones in common. Each state-set is represented
as a separate block in the chart, headed by the state-set index and the next word. Each row maps to a chart operation
(specified in the first column, with S and C standing for ‘scan’ and ‘complete’ respectively) and follows the same
notation of figure 6. Symbols ∗ are used as state placeholders.
probabilities [α,γ,β] as shown in the chart example
in Figure 7.
</bodyText>
<listItem confidence="0.986459181818182">
• The forward probability α is the marginal prob-
ability of all the paths starting with an initial
state, scanning all initial words in the sentence
until `i−1 included, and passing through the
current state.
• The inner probability γ is the marginal proba-
bility of all the paths passing through the state
k : kX ✁ •λµ, scanning words `k,...,`i−1 and
passing through the current state.
• The outer probability β is the marginal prob-
ability of all the paths starting with an initial
</listItem>
<bodyText confidence="0.938198066666667">
state, scanning all initial words in the sentence
until `k−1 included, passing through the current
state, and reaching a stop state.
Forward (α) and inner (γ) probabilities are propa-
gated while filling the chart incrementally, whereas
outer probabilities (β) are back-propagated from the
stop states, for which β = 1 (see Figure 6). These
probabilities are used for computing prefix and sen-
tence probabilities, and for obtaining the most prob-
able partial derivation (MPD) of a prefix, the MPD
of a sentence, its minimum risk parse (MRP), and to
approximate its most probable parse (MPP).
Prefix probabilities are obtained by summing over
the forward probabilities of all scan states instate-set
i having `i after the dot:9
</bodyText>
<equation confidence="0.999569">
P(`0,...,`i) = ∑ α(s) (7)
</equation>
<bodyText confidence="0.7841295">
state s
i:kX✁λ•`iµ
</bodyText>
<subsectionHeader confidence="0.971901">
3.4 Most Probable Derivation (MPD)
</subsectionHeader>
<bodyText confidence="0.99931">
The Most Probable (partial) Derivation (MPD) can
be obtained from the chart by backtracking the
Viterbi path. Viterbi forward and inner probabilities
</bodyText>
<equation confidence="0.86200475">
1 – “were”
S  |0S✁NP • “were” VP “.” [1/2, 1/2, 1]
 ||1S@ ✁ • “were” VP “.” [1/2, 1, 1/2]
®  ||0S✁ “Terms” • S@ [1/2, 1/2, 1] ∗  ||S@ ✁ “were” VP “.” [1]
®  |0NP✁“Terms” • [1/2, 1/2, 1]  |S✁NP “were” VP “.” [1]
2
S
[1, 1,
</equation>
<figure confidence="0.559022294117647">
S
NP
were
• VP
[1/2, 1/2, 1]
–“disclosed”
2VP✁•“disclosed”
1]|0
✁
“
”
“.”
∗∗VP ✁ “disclosed” [1]
 ||1S@ ✁ “were” • VP “.” [1/2, 1, 1/2] ∗ ∗ ∗
9Sentence probability is obtained by marginalizing the for-
ward probabilities of the stop states in the last state-set n.
118
</figure>
<bodyText confidence="0.9993219375">
(α*,γ*) are propagated as standard forward and in-
ner probabilities except that summation is replaced
by maximization, and the probability of an ambigu-
ous fringe is the maximum probability among all the
fragments mapping into it (instead of the marginal
one). The Viterbi partial path for the prefix t0,...,ii
can then be retrieved by backtracking from the scan
state in state-set i with max α*: for each state, the
most probable preceding state is retrieved, i.e., the
state among its antecedents with maximum α*. The
Viterbi complete path of a sentence can be obtained
by backtracking the Viterbi path from the stop state
with max α*. Given a Viterbi path, it is possible to
obtain the corresponding MPD. This is done by re-
trieving the associated sequence of fragments10 and
connecting them.
</bodyText>
<subsectionHeader confidence="0.962003">
3.5 Most Probable Parse (MPP)
</subsectionHeader>
<bodyText confidence="0.999971272727273">
According to Equation (5), if we want to compute
the MPP we need to retrieve all possible derivations
of the current sentence, sum up the probabilities of
those generating the same tree, and returning the
tree with max marginal probability. Unfortunately
the number of possible derivations grows exponen-
tially with the length of the sentence, and computing
the exact MPP is NP-hard (Sima’an, 1996). In our
implementation, we approximate the MPP by per-
forming this marginalization over the Viterbi-best
derivations obtained from all stop states in the chart.
</bodyText>
<subsectionHeader confidence="0.988536">
3.6 Minimum Risk Parse (MRP)
</subsectionHeader>
<bodyText confidence="0.998936875">
MPD and MPP aim at obtaining the structure of a
sentence which is more likely as a whole under the
current probabilistic model. Alternatively, we may
want to focus on the single components of a tree
structures, e.g., CFG rules covering a certain span of
the sentence, and search for the structure which has
the highest number of correct constituents, as pro-
posed by Goodman (1996). Such structure is more
likely to obtain higher results according to standard
parsing evaluations, as the objective being maxi-
mized is closely related to the metric used for eval-
uation (recall/precision on the number of correct la-
beled constituents).
10For each scan state in the path, we obtain the fragment in
the grammar that maps into the state’s fringe. For ambiguous
fringes the most probable fragment that maps into it is selected.
In order to obtain the minimum risk parse (MRP)
we utilize both inner (γ) and outer (β) probabilities.
The product of these two probabilities equals the
marginal probability of all paths generating the en-
tire current sentence and passing through the current
state. We can therefore compute the probability of a
fringe f = X ✁ λ • µ covering a specific span [s,t] of
the sentence:
</bodyText>
<equation confidence="0.997834">
P(f,[s,t]) = γ(t : s f•) · β(t : s f•) (8)
</equation>
<bodyText confidence="0.999649">
We can then compute the probability of each frag-
ment spanning [s,t],11 and the probability P(r,[s,t])
of a CFG-rule r spanning [s,t].12 Finally the MRP is
computed as
</bodyText>
<equation confidence="0.9986475">
MRP = argmax ∏ P(r,[s,t]) (9)
T rcT
</equation>
<sectionHeader confidence="0.996431" genericHeader="background">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99998125">
For training and evaluating the ITSG parser, we em-
ploy the Penn WSJ Treebank (Marcus et al., 1993).
We use sections 2–21 for training, section 22 and 24
for development and section 23 for testing.
</bodyText>
<subsectionHeader confidence="0.990969">
4.1 Grammar Extraction
</subsectionHeader>
<bodyText confidence="0.999830541666667">
Following standard practice, we start with some pre-
processing of the treebank. After removing traces
and functional tags, we apply right binarization on
the training treebank (Klein and Manning, 2003),
with no horizontal and vertical conditioning. This
means that when a node X has more than two chil-
dren, new artificial constituents labeled X@ are cre-
ated in a right recursive fashion (see Figure 1).13 We
then replace words appearing less than five times in
the training data by one of 50 unknown word cate-
gories based on the presence of lexical features as
described in Petrov (2009).
Fragment Extraction In order to equip the gram-
mar with a representative set of lexicalized frag-
ments, we use the extraction algorithm of Sangati
11For an ambiguous fringe, the spanning probability of each
fragment mapping into it is the fraction of the fringe’s spanning
probability with respect to the marginal fringe probability.
12Marginalizing the probabilities of all fragments having r
spanning [s,t].
13This shallow binarization (H0V1) was used based on gold
coverage of the unsmoothed grammar (extracted from the train-
ing set) on trees in section 22: H0V1 binarization results on a
coverage of 88.0% of the trees, compared to 79.2% for H1V1.
</bodyText>
<page confidence="0.892478">
119
</page>
<bodyText confidence="0.999930193548387">
et al. (2010) which finds maximal fragments recur-
ring twice or more in the training treebank. To en-
sure better coverage, we additionally extract one-
word fragments from each training parse tree: for
each lexical node f in the parse tree we percolate
up till the root node, and for every encountered in-
ternal node X0,X1,...,Xi we extract the lexicalized
fragment whose spine is Xi − Xi−1 − ... − X0 − f,
and where all the remaining children of the inter-
nal nodes are substitution sites (see for instance the
right fragment in Figure 1). Finally, we remove all
fragments which do not comply with the restrictions
presented in Section 2.1.14
For each extracted fragment we keep track of its
frequency, i.e., the number of times it occurs in the
training corpus. Each fragment’s probability is then
derived according to its relative frequency in the
corresponding set of fragments (finit,f X lex, fYsub), so
that equations(1)–(3) are satisfied. The final gram-
mar consists of 2.2M fragments mapping to 2.0M
fringes.
Smoothing Two types of smoothing are per-
formed over the grammar’s fragments: Open class
smoothing adds simple CFG rewriting rules to the
grammar for open-class15 (PoS,word) pairs not en-
countered in the training corpus, with frequency
10−6. Initial fragments smoothing adds each lex-first
fragment f to the initial fragment set with frequency
10−2 -freq(f).16
All ITSG experiments we report used exhaustive
search (no beam was used to prune the search space).
</bodyText>
<subsectionHeader confidence="0.945867">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.987986666666667">
In addition to standard full-sentence parsing results,
we propose a novel way of evaluating our ITSG on
partial trees, i.e., those that the parser constructs for
sentence prefixes. More precisely, for each prefix of
the input sentence (length two words or longer) we
compute the parsing accuracy on the minimal struc-
ture spanning that prefix. The minimal structure is
obtained from the subtree rooted in the minimum
14The fragment with no lexical items, and those with more
than one substitution site at the beginning of the yield.
15A PoS belongs to the open class if it rewrites to at least 50
different words in the training corpus. A word belongs to the
open class if it has been seen only with open-class PoS tags.
16The parameters were tuned on section 24 of the WSJ.
common ancestor of the prefix nodes, after pruning
those nodes not yielding any word in the prefix.
As observed in the example derivations of Fig-
ure 4, our ITSG generates partial trees for a given
prefix which may include predictions about unseen
parts of the sentence. We propose three new mea-
sures for evaluating sentence prediction:17
Word prediction PRD(m): For every prefix of
each test sentence, if the model predicts m&apos; &gt; m
words, the prediction is correct if the first m pre-
dicted words are identical to the m words following
the prefix in the original sentence.
Word presence PRS(m): For every prefix of each
test sentence, if the model predicts m&apos; &gt; m words,
the prediction is correct if the first m predicted words
are present, in the same order, in the words following
the prefix in the original sentence (i.e., the predicted
word sequence is a subsequence of the sequence of
words following the prefix).18
Longest common subsequence LCS: For every
prefix of each test sentence, it computes the longest
common subsequence between the sequence of pre-
dicted words (possibly none) and the words follow-
ing the prefix in the original sentence.
Recall and precision can be computed in the usual
way for these three measures. Recall is the total
number (over all prefixes) of correctly predicted
words (as defined by PRD(m), PRS(m), or LCS)
over the total number of words expected to be pre-
dicted (according to m), while precision is the num-
ber of correctly predicted words over the number of
words predicted by the model.
We compare the ITSG parser with the incremental
parsers of Schuler et al. (2010) and Demberg et al.
(2014) for full-sentence parsing, with the Roark
(2001) parser19 for full-sentence and partial pars-
17We also evaluated our ITSG model using perplexity; the
results obtained were substantially worse than those obtained
using Roark’s parsers.
18Note that neither PRD(m) nor PRS(m) correspond to word
error rate (WER). PRD requires the predicted word sequence to
be identical to the original sequence, while PRS only requires
the predicted words to be present in the original. In contrast,
WER measures the minimum number of substitutions, inser-
tions, and deletions needed to transform the predicted sequence
into the original sequence.
19Apart from reporting the results in Roark (2001), we also
run the latest version of Roark’s parser, used in Roark et al.
(2009), which has higher results compared to the original work.
</bodyText>
<table confidence="0.947241923076923">
120
R P
Demberg et al. (2014) 79.4 79.4
Schuler et al. (2010) 83.4 83.7
Roark (2001) 86.6 86.5
Roark et al. (2009) 87.7 87.5
ITSG (MPD) 81.5 83.5
ITSG (MPP) 81.6 83.6
ITSG (MRP) 82.6 85.8
ITSG Smoothing (MPD) 83.0 83.5
88 83.2 83.6
ITSG Smoothing (MPP)
ITSG Smoothing (MRP) 83.9 85.6
</table>
<figure confidence="0.983805231884058">
79.4
94
83.5
86.5
87.6
92
re
83.2
88
83.4
84.8
86
82.5
90
82.6
84.1
sc -
F1
99
98
97
96
95
94
93
92
91
98
96
94
F-score
92
90
88
86
84
82
80
78
2 3 4 5 6 7 8 9 10
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40
Roark et al. (2009)
Roark (last) ITSG ITSG Smooth. Smooth. (MPD) (MPD)
97
96
95
94
93
92
91
90
89
88
87
86
85
98
96
94
92
90
88
86
84
82
80
78
</figure>
<tableCaption confidence="0.998727">
Table 1: Full-sentence parsing results for sentences in the
</tableCaption>
<bodyText confidence="0.906660181818182">
2345678910
test set of length up to 40 words.
ing, and with a language model built using SRILM
(Stolcke, 2002) for sentence prediction. We used a
standard 3-gram model trained on the sentences of
the training set using the default setting and smooth-
ing (Kneser-Ney) provided by the SRILM pack-
age. (Higher n-gram model do not seem appropriate,
given the small size of the training corpus.) For ev-
ery prefix in the test set we compute the most prob-
able continuation predicted by the n-gram model.20
</bodyText>
<subsectionHeader confidence="0.975519">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999838470588236">
Table 1 reports full-sentence parsing results for our
parser and three comparable incremental parsers
from the literature. While Roark (2001) obtains the
best results, the ITSG parser without smoothing per-
forms on a par with Schuler et al. (2010), and out-
performs Demberg et al. (2014).21 Adding smooth-
ing results in a gain of 1.2 points F-score over the
Schuler parser. When we compare the different pars-
ing objectives of the ITSG parser, MRP is the best
one, followed by MPP and MPD.
Incremental Parsing The graphs in Figure 8 com-
pare the ITSG and Roark’s parser on the incremental
parsing evaluation, when parsing sentences of length
10, 20, 30 and 40. The performance of both models
declines as the length of the prefix increases, with
Roark’s parser outperforming the ITSG parser on
average, although the ITSG parser seems more com-
</bodyText>
<footnote confidence="0.689947">
20We used a modified version of a script by Nathaniel Smith
available at https://github.com/njsmith/pysrilm.
21Note that the scores reported by Demberg et al. (2014) are
for TAG structures, not for the original Penn Treebank trees.
xPrefix Length
</footnote>
<figureCaption confidence="0.9802285">
Figure 8: Partial parsing results for sentences of length
10, 20, 30, and 40 (from upper left to lower right).
</figureCaption>
<bodyText confidence="0.999870684210526">
petitive when parsing prefixes for longer (and there-
fore more difficult) sentences.
Sentence Prediction Table 2 compares the sen-
tence prediction results of the ITSG and the lan-
guage model (SRILM). The latter is outperforming
the former when predicting the next word of a pre-
fix, i.e. PRD(1), whereas ITSG is better than the lan-
guage model at predicting a single future word, i.e.
PRS(1). When more than one (consecutive) word
is considered, the SRILM model exhibits a slightly
better recall while ITSG achieves a large gain in pre-
cision. This illustrates the complementary nature of
the two models: while the language model is better
at predicting the next word, the ITSG predicts future
words (rarely adjacent to the prefix) with high con-
fidence (89.4% LCS precision). However, it makes
predictions for only a small number of words (5.9%
LCS recall). Examples of sentence predictions can
be found in Table 3.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999674777777778">
To the best of our knowledge, there are no other in-
cremental TSG parsers in the literature. The parser
of Demberg et al. (2014) is closely related, but uses
tree-adjoining grammar, which includes both sub-
stitution and adjunction. That parser makes predic-
tions, but only for upcoming structure, not for up-
coming words, and thus cannot be used directly
for sentence prediction. The incremental parser of
Roark (2001) uses a top-down algorithm and works
</bodyText>
<table confidence="0.959394333333333">
121
ITSG SRILM
Correct R P Correct R P
4,637 8.7 12.5 11,430 21.5 21.6
864 1.7 13.9 2,686 5.3 5.7
414 0.9 20.9 911 1.9 2.1
236 0.5 23.4 387 0.8 1.0
34,831 65.4 93.9 21,954 41.2 41.5
4,062 8.0 65.3 5,726 11.3 12.2
1,066 2.2 53.7 1,636 3.4 3.8
541 1.2 53.7 654 1.4 1.7
44,454 5.9 89.4 92,587 12.2 18.4
</table>
<tableCaption confidence="0.974225">
Table 2: Sentence prediction results.
</tableCaption>
<note confidence="0.6480015">
Prefix Shares of UAL, the parent PRD(3) PRS(3)
ITSG company of United Airlines , − −
</note>
<table confidence="0.574993333333333">
SRILM company , which is the − −
Goldstd of United Airlines , were extremely active all day
Friday .
Prefix PSE said it expects to report earnings of $ 1.3
million to $ 1.7 million , or 14
ITSG cents a share , − +
</table>
<tableCaption confidence="0.759219">
SRILM % to $ UNK − −
Goldstd cents to 18 cents a share .
Table 3: Examples comparing sentence predictions for
ITSG and SRILM (UNK: unknown word).
</tableCaption>
<bodyText confidence="0.999913615384615">
on the basis of context-free rules. These are aug-
mented with a large number of non-local fea-
tures (e.g., grandparent categories). Our approach
avoids the need for such additional features, as
TSG fragments naturally contain non-local informa-
tion. Roark’s parser outperforms ours in both full-
sentence and incremental F-score (see Section 4),
but cannot be used for sentence prediction straight-
forwardly: to obtain a prediction for the next word,
we would need to compute an argmax over the
whole vocabulary, then iterate this for each word af-
ter that (the same is true for the parsers of Schuler
et al., 2010 and Demberg et al., 2014). Most in-
cremental dependency parsers use a discriminative
model over parse actions (Nivre, 2007), and there-
fore cannot predict upcoming words either (but see
Huang and Sagae 2010).
Turning to the literature on sentence prediction,
we note that ours is the first attempt to use a parser
for this task. Existing approaches either use n-gram
models (Eng and Eisner, 2004; Bickel et al., 2005) or
a retrieval approach in which the best matching sen-
tence is identified from a sentence collection given a
set of features (Grabski and Scheffer, 2004). There
is also work combining n-gram models with lexical
semantics (Li and Hirst, 2005) or part-of-speech in-
formation (Fazly and Hirst, 2003).
In the language modeling literature, more sophis-
ticated models than simple n-gram models have
been developed in the past few years, and these
could potentially improve sentence prediction. Ex-
amples include syntactic language models which
have applied successfully for speech recognition
(Chelba and Jelinek, 2000; Xu et al., 2002) and ma-
chine translation (Schwartz et al., 2011; Tan et al.,
2011), as well as discriminative language models
(Mikolov et al., 2010; Roark et al., 2007). Future
work should evaluate these approaches against the
ITSG model proposed here.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999967611111111">
We have presented the first incremental parser for
tree substitution grammar. Incrementality is moti-
vated by psycholinguistic findings, and by the need
for real-time interpretation in NLP. We have shown
that our parser performs competitively on both full
sentence and sentence prefix F-score. We also intro-
duced sentence prediction as a new way of evaluat-
ing incremental parsers, and demonstrated that our
parser outperforms an n-gram model in predicting
more than one upcoming word.
The performance of our approach is likely to im-
prove by implementing better binarization and more
advanced smoothing. Also, our model currently con-
tains no conditioning on lexical information, which
is also likely to yield a performance gain. Finally,
future work could involve replacing the relative fre-
quency estimator that we use with more sophisti-
cated estimation schemes.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999852555555556">
This work was funded by EPSRC grant
EP/I032916/1 “An integrated model of syntac-
tic and semantic prediction in human language
processing”. We are grateful to Brian Roark for
clarifying correspondence and for guidance in using
his incremental parser. We would also like to thank
Katja Abramova, Vera Demberg, Mirella Lapata,
Andreas van Cranenburgh, and three anonymous
reviewers for useful comments.
</bodyText>
<equation confidence="0.980978555555555">
PRD(1)
PRD(2)
PRD(3)
PRD(4)
PRS(1)
PRS(2)
PRS(3)
PRS(4)
LCS
</equation>
<page confidence="0.767773">
122
</page>
<sectionHeader confidence="0.992184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996083887640449">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
theory of parsing, translation, and compiling.
Prentice-Hall, Inc., Upper Saddle River, NJ.
Gerry T. M. Altmann and Yuki Kamide. 1999. Incre-
mental interpretation at verbs: Restricting the do-
main of subsequent reference. Cognition, 73:247–
264.
Steffen Bickel, Peter Haider, and Tobias Scheffer.
2005. Predicting sentences using n-gram lan-
guage models. In Proceedings of the Conference
on Human Language Technology and Empirical
Methods in Natural Language Processing, pages
193–200. Vancouver.
Rens Bod. 1995. The problem of computing the
most probable tree in data-oriented parsing and
stochastic tree grammars. In Proceedings of the
7th Conference of the European Chapter of the
Association for Computational Linguistics, pages
104–111. Association for Computer Linguistics,
Dublin.
Rens Bod, Khalil Sima’an, and Remko Scha. 2003.
Data-Oriented Parsing. University of Chicago
Press, Chicago, IL.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14:283–332.
Vera Demberg and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition,
101(2):193–210.
Vera Demberg, Frank Keller, and Alexander Koller.
2014. Parsing with psycholinguistically moti-
vated tree-adjoining grammar. Computational
Linguistics, 40(1). In press.
Jay Earley. 1970. An efficient context-free pars-
ing algorithm. Communications of the ACM,
13(2):94–102.
John Eng and Jason M. Eisner. 2004. Radiology
report entry with automatic phrase completion
driven by language modeling. Radiographics,
24(5):1493–1501.
Afsaneh Fazly and Graeme Hirst. 2003. Testing
the efficacy of part-of-speech information in word
completion. In Proceedings of the EACL Work-
shop on Language Modeling for Text Entry Meth-
ods, pages 9–16. Budapest.
Joshua Goodman. 1996. Parsing algorithms and
metrics. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
pages 177–183. Association for Computational
Linguistics, Santa Cruz.
Korinna Grabski and Tobias Scheffer. 2004. Sen-
tence completion. In Proceedings of the 27th An-
nual International ACM SIR Conference on Re-
search and Development in Information Retrieval,
pages 433–439. Sheffield.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1077–1086. Association for Computational Lin-
guistics, Uppsala.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423–430. Associa-
tion for Computational Linguistics, Sapporo.
Jianhua Li and Graeme Hirst. 2005. Semantic
knowledge in a word completion task. In Pro-
ceedings of the 7th International ACM SIGAC-
CESS Conference on Computers and Accessibil-
ity, pages 121–128. Baltimore.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313–330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and
Sanjeev. 2010. Recurrent neural network based
language model. In Proceedings of the 11th
Annual Conference of the International Speech
Communication Association, pages 2877–2880.
Florence.
Joakim Nivre. 2007. Incremental non-projective
dependency parsing. In Proceedings of Human
Language Technologies: The Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 396–
403. Association for Computational Linguistics,
Rochester.
</reference>
<page confidence="0.642561">
123
</page>
<reference confidence="0.998916426966293">
Slav Petrov. 2009. Coarse-to-Fine Natural Lan-
guage Processing. Ph.D. thesis, University of
California at Bekeley, Berkeley, CA.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tistics, 27:249–276.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psy-
cholinguistic modeling via incremental top-down
parsing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 324–333. Association for Computational
Linguistics, Singapore.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2):373–392.
D. J. Rosenkrantz and P. M. Lewis. 1970. Deter-
ministic left corner parsing. In Proceedings of
the 11th Annual Symposium on Switching and Au-
tomata Theory, pages 139–152. IEEE Computer
Society, Washington, DC.
Federico Sangati, Willem Zuidema, and Rens Bod.
2010. Efficiently extract recurring tree fragments
from large treebanks. In Nicoletta Calzolari,
Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner,
and Daniel Tapias, editors, Proceedings of the 7th
InternationalConference on Language Resources
and Evaluation. European Language Resources
Association, Valletta, Malta.
Yves Schabes. 1990. Mathematical and Computa-
tional Aspects of Lexicalized Grammars. Ph.D.
thesis, University of Pennsylvania, Philadelphia,
PA.
William Schuler, Samir AbdelRahman, Tim Miller,
and Lane Schwartz. 2010. Broad-coverage pars-
ing using human-like memory constraints. Com-
putational Linguististics, 36(1):1–30.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental syn-
tactic language models for phrase-based transla-
tion. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies, Volume 1, pages
620–631. Association for Computational Linguis-
tics, Portland, OR.
Khalil Sima’an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of the 16th Confer-
ence on Computational Linguistics, pages 1175–
1180. Association for Computational Linguistics,
Copenhagen.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Process-
ing, pages 257–286. Denver, CO.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceed-
ings of the ACL Workshop Incremental Parsing:
Bringing Engineering and Cognition Together,
pages 18–25. Association for Computational Lin-
guistics, Barcelona.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun
Wang. 2011. A large scale distributed syntac-
tic, semantic and lexical language model for ma-
chine translation. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1, pages 201–210. Association for
Computational Linguistics, Portland, OR.
Michael K. Tanenhaus, Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguistic
information in spoken language comprehension.
Science, 268:1632–1634.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceedings
of the 40th Annual Meeting on Association for
Computational Linguistics, pages 191–198. As-
sociation for Computational Linguistics, Philadel-
phia.
</reference>
<page confidence="0.931328">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697769">
<title confidence="0.9990935">Incremental Tree Substitution for Parsing and Sentence Prediction</title>
<author confidence="0.952999">Sangati</author>
<affiliation confidence="0.986766">Institute for Language, Cognition, and School of Informatics, University of</affiliation>
<address confidence="0.733978">10 Crichton Street, Edinburgh EH8 9AB,</address>
<email confidence="0.994175">federico.sangati@gmail.comkeller@inf.ed.ac.uk</email>
<abstract confidence="0.999792954545455">In this paper, we present the first incremental parser for Tree Substitution Grammar (TSG). A TSG allows arbitrarily large syntactic fragments to be combined into complete trees; we show how constraints (including lexicalization) can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. In addition to whole-sentence F-score, we also evaluate the partial trees that the parser constructs for sentence prefixes; partial trees play an important role in incremental interpretation, language modeling, and psycholinguistics. Unlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The theory of parsing, translation, and compiling. Prentice-Hall, Inc., Upper Saddle River,</title>
<date>1972</date>
<location>NJ.</location>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The theory of parsing, translation, and compiling. Prentice-Hall, Inc., Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerry T M Altmann</author>
<author>Yuki Kamide</author>
</authors>
<title>Incremental interpretation at verbs: Restricting the domain of subsequent reference.</title>
<date>1999</date>
<journal>Cognition,</journal>
<volume>73</volume>
<pages>264</pages>
<contexts>
<context position="1642" citStr="Altmann and Kamide, 1999" startWordPosition="240" endWordPosition="243">our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word. 1 Introduction When humans listen to speech, the input becomes available gradually as the speech signal unfolds. Reading happens in a similarly gradual manner when the eyes scan a text. There is good evidence that the human language processor is adapted to this and works incrementally, i.e., computes an interpretation for an incoming sentence on a word-by-word basis (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., </context>
</contexts>
<marker>Altmann, Kamide, 1999</marker>
<rawString>Gerry T. M. Altmann and Yuki Kamide. 1999. Incremental interpretation at verbs: Restricting the domain of subsequent reference. Cognition, 73:247– 264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Bickel</author>
<author>Peter Haider</author>
<author>Tobias Scheffer</author>
</authors>
<title>Predicting sentences using n-gram language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--200</pages>
<location>Vancouver.</location>
<contexts>
<context position="2983" citStr="Bickel et al., 2005" startWordPosition="454" endWordPosition="457">g et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG</context>
<context position="38615" citStr="Bickel et al., 2005" startWordPosition="6701" endWordPosition="6704">to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jel</context>
</contexts>
<marker>Bickel, Haider, Scheffer, 2005</marker>
<rawString>Steffen Bickel, Peter Haider, and Tobias Scheffer. 2005. Predicting sentences using n-gram language models. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 193–200. Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>The problem of computing the most probable tree in data-oriented parsing and stochastic tree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<institution>Association for Computer Linguistics,</institution>
<location>Dublin.</location>
<contexts>
<context position="3335" citStr="Bod, 1995" startWordPosition="514" endWordPosition="515">ediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report an F-score competitive with other incremental parsers. 111 Transactions of the Association for Computational Linguistics, 1 (2013) 111–124. Action Editor: David Chiang. Submitted 10/2012; Revised 2/2013; Published 5/2013. c�2013 Association for Computational Linguistics. TSG fragments can be arbitrarily large and can contain multi</context>
</contexts>
<marker>Bod, 1995</marker>
<rawString>Rens Bod. 1995. The problem of computing the most probable tree in data-oriented parsing and stochastic tree grammars. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, pages 104–111. Association for Computer Linguistics, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
<author>Khalil Sima’an</author>
<author>Remko Scha</author>
</authors>
<title>Data-Oriented Parsing.</title>
<date>2003</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<marker>Bod, Sima’an, Scha, 2003</marker>
<rawString>Rens Bod, Khalil Sima’an, and Remko Scha. 2003. Data-Oriented Parsing. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<date>2000</date>
<booktitle>Structured language modeling. Computer Speech and Language,</booktitle>
<pages>14--283</pages>
<contexts>
<context position="2293" citStr="Chelba and Jelinek, 2000" startWordPosition="344" endWordPosition="347">systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fa</context>
<context position="39225" citStr="Chelba and Jelinek, 2000" startWordPosition="6794" endWordPosition="6797"> et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14:283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>101</volume>
<issue>2</issue>
<contexts>
<context position="2621" citStr="Demberg and Keller, 2008" startWordPosition="393" endWordPosition="396"> realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution o</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition, 101(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
<author>Alexander Koller</author>
</authors>
<title>Parsing with psycholinguistically motivated tree-adjoining grammar.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<note>In press.</note>
<contexts>
<context position="2379" citStr="Demberg et al., 2014" startWordPosition="358" endWordPosition="361">system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., </context>
<context position="7891" citStr="Demberg et al., 2014" startWordPosition="1278" endWordPosition="1281"> the generative process. Fragments A fragment f E F belongs to one of the three sets Finit,FXlex,FYsub: • An initial fragment (finit) has the lexical anchor in the first position of the yield, being the initial word of a sentence (the left-most lexical node of the parse tree from which it was extracted). • A lex-first fragment (fXlex) has the lexical anchor (non sentence-initial) in the first position of the yield, and is rooted in X.3 • A sub-first fragment (fYsub) has the lexical anchor in the second position of its yield, and a substitution site Y in the first. Fringes We will use fringes (Demberg et al., 2014) as a compressed representation of fragments, 3A fragment can be both an initial and a lex-first fragment (e.g., if the lexical anchor is a proper noun). We will make use of two separate instances of the same fragment in the two sets. VBD VP “were” “.” Figure 2: An example of an ITSG derivation yielding the tree on the left side of Figure 1. The second and third fragment are introduced by means of forward and backward substitution, respectively. in which the internal structure is replaced by a triangle (0 or ✁) and only the root and the yield are visible. It is possible in a grammar that multi</context>
<context position="32119" citStr="Demberg et al. (2014)" startWordPosition="5532" endWordPosition="5535">t common subsequence between the sequence of predicted words (possibly none) and the words following the prefix in the original sentence. Recall and precision can be computed in the usual way for these three measures. Recall is the total number (over all prefixes) of correctly predicted words (as defined by PRD(m), PRS(m), or LCS) over the total number of words expected to be predicted (according to m), while precision is the number of correctly predicted words over the number of words predicted by the model. We compare the ITSG parser with the incremental parsers of Schuler et al. (2010) and Demberg et al. (2014) for full-sentence parsing, with the Roark (2001) parser19 for full-sentence and partial pars17We also evaluated our ITSG model using perplexity; the results obtained were substantially worse than those obtained using Roark’s parsers. 18Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER). PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original. In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transform the predicted sequence </context>
<context position="34524" citStr="Demberg et al. (2014)" startWordPosition="5993" endWordPosition="5996">ed on the sentences of the training set using the default setting and smoothing (Kneser-Ney) provided by the SRILM package. (Higher n-gram model do not seem appropriate, given the small size of the training corpus.) For every prefix in the test set we compute the most probable continuation predicted by the n-gram model.20 4.3 Results Table 1 reports full-sentence parsing results for our parser and three comparable incremental parsers from the literature. While Roark (2001) obtains the best results, the ITSG parser without smoothing performs on a par with Schuler et al. (2010), and outperforms Demberg et al. (2014).21 Adding smoothing results in a gain of 1.2 points F-score over the Schuler parser. When we compare the different parsing objectives of the ITSG parser, MRP is the best one, followed by MPP and MPD. Incremental Parsing The graphs in Figure 8 compare the ITSG and Roark’s parser on the incremental parsing evaluation, when parsing sentences of length 10, 20, 30 and 40. The performance of both models declines as the length of the prefix increases, with Roark’s parser outperforming the ITSG parser on average, although the ITSG parser seems more com20We used a modified version of a script by Natha</context>
<context position="36485" citStr="Demberg et al. (2014)" startWordPosition="6323" endWordPosition="6326">nsidered, the SRILM model exhibits a slightly better recall while ITSG achieves a large gain in precision. This illustrates the complementary nature of the two models: while the language model is better at predicting the next word, the ITSG predicts future words (rarely adjacent to the prefix) with high confidence (89.4% LCS precision). However, it makes predictions for only a small number of words (5.9% LCS recall). Examples of sentence predictions can be found in Table 3. 5 Related Work To the best of our knowledge, there are no other incremental TSG parsers in the literature. The parser of Demberg et al. (2014) is closely related, but uses tree-adjoining grammar, which includes both substitution and adjunction. That parser makes predictions, but only for upcoming structure, not for upcoming words, and thus cannot be used directly for sentence prediction. The incremental parser of Roark (2001) uses a top-down algorithm and works 121 ITSG SRILM Correct R P Correct R P 4,637 8.7 12.5 11,430 21.5 21.6 864 1.7 13.9 2,686 5.3 5.7 414 0.9 20.9 911 1.9 2.1 236 0.5 23.4 387 0.8 1.0 34,831 65.4 93.9 21,954 41.2 41.5 4,062 8.0 65.3 5,726 11.3 12.2 1,066 2.2 53.7 1,636 3.4 3.8 541 1.2 53.7 654 1.4 1.7 44,454 5.</context>
<context position="38225" citStr="Demberg et al., 2014" startWordPosition="6637" endWordPosition="6640"> of context-free rules. These are augmented with a large number of non-local features (e.g., grandparent categories). Our approach avoids the need for such additional features, as TSG fragments naturally contain non-local information. Roark’s parser outperforms ours in both fullsentence and incremental F-score (see Section 4), but cannot be used for sentence prediction straightforwardly: to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexica</context>
</contexts>
<marker>Demberg, Keller, Koller, 2014</marker>
<rawString>Vera Demberg, Frank Keller, and Alexander Koller. 2014. Parsing with psycholinguistically motivated tree-adjoining grammar. Computational Linguistics, 40(1). In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Eng</author>
<author>Jason M Eisner</author>
</authors>
<title>Radiology report entry with automatic phrase completion driven by language modeling.</title>
<date>2004</date>
<journal>Radiographics,</journal>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="2934" citStr="Eng and Eisner, 2004" startWordPosition="446" endWordPosition="449">and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an eff</context>
<context position="38593" citStr="Eng and Eisner, 2004" startWordPosition="6697" endWordPosition="6700">on straightforwardly: to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recog</context>
</contexts>
<marker>Eng, Eisner, 2004</marker>
<rawString>John Eng and Jason M. Eisner. 2004. Radiology report entry with automatic phrase completion driven by language modeling. Radiographics, 24(5):1493–1501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Graeme Hirst</author>
</authors>
<title>Testing the efficacy of part-of-speech information in word completion.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods,</booktitle>
<pages>9--16</pages>
<location>Budapest.</location>
<contexts>
<context position="2912" citStr="Fazly and Hirst, 2003" startWordPosition="442" endWordPosition="445">00; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental process</context>
<context position="38911" citStr="Fazly and Hirst, 2003" startWordPosition="6749" endWordPosition="6752"> over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented</context>
</contexts>
<marker>Fazly, Hirst, 2003</marker>
<rawString>Afsaneh Fazly and Graeme Hirst. 2003. Testing the efficacy of part-of-speech information in word completion. In Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods, pages 9–16. Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz.</location>
<contexts>
<context position="25685" citStr="Goodman (1996)" startWordPosition="4447" endWordPosition="4448">ing the exact MPP is NP-hard (Sima’an, 1996). In our implementation, we approximate the MPP by performing this marginalization over the Viterbi-best derivations obtained from all stop states in the chart. 3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a sentence which is more likely as a whole under the current probabilistic model. Alternatively, we may want to focus on the single components of a tree structures, e.g., CFG rules covering a certain span of the sentence, and search for the structure which has the highest number of correct constituents, as proposed by Goodman (1996). Such structure is more likely to obtain higher results according to standard parsing evaluations, as the objective being maximized is closely related to the metric used for evaluation (recall/precision on the number of correct labeled constituents). 10For each scan state in the path, we obtain the fragment in the grammar that maps into the state’s fringe. For ambiguous fringes the most probable fragment that maps into it is selected. In order to obtain the minimum risk parse (MRP) we utilize both inner (γ) and outer (β) probabilities. The product of these two probabilities equals the margina</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, pages 177–183. Association for Computational Linguistics, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korinna Grabski</author>
<author>Tobias Scheffer</author>
</authors>
<title>Sentence completion.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>433--439</pages>
<publisher>Sheffield.</publisher>
<contexts>
<context position="2962" citStr="Grabski and Scheffer, 2004" startWordPosition="450" endWordPosition="453">reeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorith</context>
<context position="38769" citStr="Grabski and Scheffer, 2004" startWordPosition="6727" endWordPosition="6730">(the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al.,</context>
</contexts>
<marker>Grabski, Scheffer, 2004</marker>
<rawString>Korinna Grabski and Tobias Scheffer. 2004. Sentence completion. In Proceedings of the 27th Annual International ACM SIR Conference on Research and Development in Information Retrieval, pages 433–439. Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<location>Uppsala.</location>
<contexts>
<context position="2330" citStr="Huang and Sagae, 2010" startWordPosition="350" endWordPosition="353"> spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, </context>
<context position="38405" citStr="Huang and Sagae 2010" startWordPosition="6665" endWordPosition="6668">TSG fragments naturally contain non-local information. Roark’s parser outperforms ours in both fullsentence and incremental F-score (see Section 4), but cannot be used for sentence prediction straightforwardly: to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models hav</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077–1086. Association for Computational Linguistics, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo.</location>
<contexts>
<context position="27185" citStr="Klein and Manning, 2003" startWordPosition="4702" endWordPosition="4705">ompute the probability of each fragment spanning [s,t],11 and the probability P(r,[s,t]) of a CFG-rule r spanning [s,t].12 Finally the MRP is computed as MRP = argmax ∏ P(r,[s,t]) (9) T rcT 4 Experiments For training and evaluating the ITSG parser, we employ the Penn WSJ Treebank (Marcus et al., 1993). We use sections 2–21 for training, section 22 and 24 for development and section 23 for testing. 4.1 Grammar Extraction Following standard practice, we start with some preprocessing of the treebank. After removing traces and functional tags, we apply right binarization on the training treebank (Klein and Manning, 2003), with no horizontal and vertical conditioning. This means that when a node X has more than two children, new artificial constituents labeled X@ are created in a right recursive fashion (see Figure 1).13 We then replace words appearing less than five times in the training data by one of 50 unknown word categories based on the presence of lexical features as described in Petrov (2009). Fragment Extraction In order to equip the grammar with a representative set of lexicalized fragments, we use the extraction algorithm of Sangati 11For an ambiguous fringe, the spanning probability of each fragmen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 423–430. Association for Computational Linguistics, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Li</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic knowledge in a word completion task.</title>
<date>2005</date>
<booktitle>In Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility,</booktitle>
<pages>121--128</pages>
<location>Baltimore.</location>
<contexts>
<context position="3004" citStr="Li and Hirst, 2005" startWordPosition="458" endWordPosition="461">cal applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing with TSG (also referred to as data-oriented parsing) exists (Bod, 1995; Bod et al., 2003), but none of the existing TSG parsers are incremental. We show how constraints can be imposed on the shape of the TSG fragments to enable incremental processing. We propose an efficient Earley-based algorithm for incremental TSG parsing and report a</context>
<context position="38857" citStr="Li and Hirst, 2005" startWordPosition="6741" endWordPosition="6744">ental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITS</context>
</contexts>
<marker>Li, Hirst, 2005</marker>
<rawString>Jianhua Li and Graeme Hirst. 2005. Semantic knowledge in a word completion task. In Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility, pages 121–128. Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="26863" citStr="Marcus et al., 1993" startWordPosition="4652" endWordPosition="4655">these two probabilities equals the marginal probability of all paths generating the entire current sentence and passing through the current state. We can therefore compute the probability of a fringe f = X ✁ λ • µ covering a specific span [s,t] of the sentence: P(f,[s,t]) = γ(t : s f•) · β(t : s f•) (8) We can then compute the probability of each fragment spanning [s,t],11 and the probability P(r,[s,t]) of a CFG-rule r spanning [s,t].12 Finally the MRP is computed as MRP = argmax ∏ P(r,[s,t]) (9) T rcT 4 Experiments For training and evaluating the ITSG parser, we employ the Penn WSJ Treebank (Marcus et al., 1993). We use sections 2–21 for training, section 22 and 24 for development and section 23 for testing. 4.1 Grammar Extraction Following standard practice, we start with some preprocessing of the treebank. After removing traces and functional tags, we apply right binarization on the training treebank (Klein and Manning, 2003), with no horizontal and vertical conditioning. This means that when a node X has more than two children, new artificial constituents labeled X@ are created in a right recursive fashion (see Figure 1).13 We then replace words appearing less than five times in the training data </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiat</author>
<author>Jan Cernocky</author>
<author>Sanjeev</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>2877--2880</pages>
<location>Florence.</location>
<contexts>
<context position="39374" citStr="Mikolov et al., 2010" startWordPosition="6820" endWordPosition="6823">Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction as a new way of evaluating incremental parsers, and demonstrated that our parser outperforms an n-gram model in predicting more than one upcoming wo</context>
</contexts>
<marker>Mikolov, Karafiat, Cernocky, Sanjeev, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiat, Jan Cernocky, and Sanjeev. 2010. Recurrent neural network based language model. In Proceedings of the 11th Annual Conference of the International Speech Communication Association, pages 2877–2880. Florence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incremental non-projective dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 396– 403. Association for Computational Linguistics,</booktitle>
<location>Rochester.</location>
<contexts>
<context position="2306" citStr="Nivre, 2007" startWordPosition="348" endWordPosition="349">eech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst</context>
<context position="38322" citStr="Nivre, 2007" startWordPosition="6653" endWordPosition="6654">ategories). Our approach avoids the need for such additional features, as TSG fragments naturally contain non-local information. Roark’s parser outperforms ours in both fullsentence and incremental F-score (see Section 4), but cannot be used for sentence prediction straightforwardly: to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the la</context>
</contexts>
<marker>Nivre, 2007</marker>
<rawString>Joakim Nivre. 2007. Incremental non-projective dependency parsing. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 396– 403. Association for Computational Linguistics, Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Bekeley,</institution>
<location>Berkeley, CA.</location>
<contexts>
<context position="27571" citStr="Petrov (2009)" startWordPosition="4772" endWordPosition="4773">ng. 4.1 Grammar Extraction Following standard practice, we start with some preprocessing of the treebank. After removing traces and functional tags, we apply right binarization on the training treebank (Klein and Manning, 2003), with no horizontal and vertical conditioning. This means that when a node X has more than two children, new artificial constituents labeled X@ are created in a right recursive fashion (see Figure 1).13 We then replace words appearing less than five times in the training data by one of 50 unknown word categories based on the presence of lexical features as described in Petrov (2009). Fragment Extraction In order to equip the grammar with a representative set of lexicalized fragments, we use the extraction algorithm of Sangati 11For an ambiguous fringe, the spanning probability of each fragment mapping into it is the fraction of the fringe’s spanning probability with respect to the marginal fringe probability. 12Marginalizing the probabilities of all fragments having r spanning [s,t]. 13This shallow binarization (H0V1) was used based on gold coverage of the unsmoothed grammar (extracted from the training set) on trees in section 22: H0V1 binarization results on a coverage</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>Slav Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, University of California at Bekeley, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguististics,</journal>
<pages>27--249</pages>
<contexts>
<context position="2224" citStr="Roark, 2001" startWordPosition="336" endWordPosition="337">95; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so fa</context>
<context position="32168" citStr="Roark (2001)" startWordPosition="5541" endWordPosition="5542">ds (possibly none) and the words following the prefix in the original sentence. Recall and precision can be computed in the usual way for these three measures. Recall is the total number (over all prefixes) of correctly predicted words (as defined by PRD(m), PRS(m), or LCS) over the total number of words expected to be predicted (according to m), while precision is the number of correctly predicted words over the number of words predicted by the model. We compare the ITSG parser with the incremental parsers of Schuler et al. (2010) and Demberg et al. (2014) for full-sentence parsing, with the Roark (2001) parser19 for full-sentence and partial pars17We also evaluated our ITSG model using perplexity; the results obtained were substantially worse than those obtained using Roark’s parsers. 18Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER). PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original. In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transform the predicted sequence into the original sequence. 19Apart from reportin</context>
<context position="34380" citStr="Roark (2001)" startWordPosition="5969" endWordPosition="5970"> words. ing, and with a language model built using SRILM (Stolcke, 2002) for sentence prediction. We used a standard 3-gram model trained on the sentences of the training set using the default setting and smoothing (Kneser-Ney) provided by the SRILM package. (Higher n-gram model do not seem appropriate, given the small size of the training corpus.) For every prefix in the test set we compute the most probable continuation predicted by the n-gram model.20 4.3 Results Table 1 reports full-sentence parsing results for our parser and three comparable incremental parsers from the literature. While Roark (2001) obtains the best results, the ITSG parser without smoothing performs on a par with Schuler et al. (2010), and outperforms Demberg et al. (2014).21 Adding smoothing results in a gain of 1.2 points F-score over the Schuler parser. When we compare the different parsing objectives of the ITSG parser, MRP is the best one, followed by MPP and MPD. Incremental Parsing The graphs in Figure 8 compare the ITSG and Roark’s parser on the incremental parsing evaluation, when parsing sentences of length 10, 20, 30 and 40. The performance of both models declines as the length of the prefix increases, with R</context>
<context position="36772" citStr="Roark (2001)" startWordPosition="6369" endWordPosition="6370">ith high confidence (89.4% LCS precision). However, it makes predictions for only a small number of words (5.9% LCS recall). Examples of sentence predictions can be found in Table 3. 5 Related Work To the best of our knowledge, there are no other incremental TSG parsers in the literature. The parser of Demberg et al. (2014) is closely related, but uses tree-adjoining grammar, which includes both substitution and adjunction. That parser makes predictions, but only for upcoming structure, not for upcoming words, and thus cannot be used directly for sentence prediction. The incremental parser of Roark (2001) uses a top-down algorithm and works 121 ITSG SRILM Correct R P Correct R P 4,637 8.7 12.5 11,430 21.5 21.6 864 1.7 13.9 2,686 5.3 5.7 414 0.9 20.9 911 1.9 2.1 236 0.5 23.4 387 0.8 1.0 34,831 65.4 93.9 21,954 41.2 41.5 4,062 8.0 65.3 5,726 11.3 12.2 1,066 2.2 53.7 1,636 3.4 3.8 541 1.2 53.7 654 1.4 1.7 44,454 5.9 89.4 92,587 12.2 18.4 Table 2: Sentence prediction results. Prefix Shares of UAL, the parent PRD(3) PRS(3) ITSG company of United Airlines , − − SRILM company , which is the − − Goldstd of United Airlines , were extremely active all day Friday . Prefix PSE said it expects to report ea</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguististics, 27:249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>324--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Singapore.</institution>
<contexts>
<context position="32876" citStr="Roark et al. (2009)" startWordPosition="5649" endWordPosition="5652">lexity; the results obtained were substantially worse than those obtained using Roark’s parsers. 18Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER). PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original. In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transform the predicted sequence into the original sequence. 19Apart from reporting the results in Roark (2001), we also run the latest version of Roark’s parser, used in Roark et al. (2009), which has higher results compared to the original work. 120 R P Demberg et al. (2014) 79.4 79.4 Schuler et al. (2010) 83.4 83.7 Roark (2001) 86.6 86.5 Roark et al. (2009) 87.7 87.5 ITSG (MPD) 81.5 83.5 ITSG (MPP) 81.6 83.6 ITSG (MRP) 82.6 85.8 ITSG Smoothing (MPD) 83.0 83.5 88 83.2 83.6 ITSG Smoothing (MPP) ITSG Smoothing (MRP) 83.9 85.6 79.4 94 83.5 86.5 87.6 92 re 83.2 88 83.4 84.8 86 82.5 90 82.6 84.1 sc - F1 99 98 97 96 95 94 93 92 91 98 96 94 F-score 92 90 88 86 84 82 80 78 2 3 4 5 6 7 8 9 10 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2 4 6</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 324–333. Association for Computational Linguistics, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="39395" citStr="Roark et al., 2007" startWordPosition="6824" endWordPosition="6827"> is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction as a new way of evaluating incremental parsers, and demonstrated that our parser outperforms an n-gram model in predicting more than one upcoming word. The performance o</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M Lewis</author>
</authors>
<title>Deterministic left corner parsing.</title>
<date>1970</date>
<booktitle>In Proceedings of the 11th Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<publisher>IEEE Computer Society,</publisher>
<location>Washington, DC.</location>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>D. J. Rosenkrantz and P. M. Lewis. 1970. Deterministic left corner parsing. In Proceedings of the 11th Annual Symposium on Switching and Automata Theory, pages 139–152. IEEE Computer Society, Washington, DC.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Federico Sangati</author>
<author>Willem Zuidema</author>
<author>Rens Bod</author>
</authors>
<title>Efficiently extract recurring tree fragments from large treebanks.</title>
<date>2010</date>
<booktitle>Proceedings of the 7th InternationalConference on Language Resources and Evaluation. European Language Resources Association,</booktitle>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta.</location>
<marker>Sangati, Zuidema, Bod, 2010</marker>
<rawString>Federico Sangati, Willem Zuidema, and Rens Bod. 2010. Efficiently extract recurring tree fragments from large treebanks. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the 7th InternationalConference on Language Resources and Evaluation. European Language Resources Association, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4744" citStr="Schabes 1990" startWordPosition="736" endWordPosition="737">ctly to the task of sentence prediction, simply by reading off the predicted items in a partial tree. We show that our parser outperforms an n-gram model in predicting more than one upcoming word. The rest of the paper is structured as follows. In Section 2, we introduce the ITSG framework and relate it to the original TSG formalism. Section 3 describes the chart-parser algorithm, while Section 4 details the experimental setup and results. Sections 5 and 6 present related work and conclusions. 2 Incremental Tree Substitution Grammar The current work is based on Tree Substitution Grammar (TSG, Schabes 1990; for a recent overview see Bod et al. 2003). A TSG is composed of (i) a set of arbitrarily large fragments, usually extracted from an annotated phrase-structure treebank, and (ii) the substitution operation by means of which fragments can be combined into complete syntactic analyses (derivations) of novel sentences. Every fragment’s node is either a lexical node (word), a substitution site (a non-lexical node in the yield of the structure),1 or an internal node. An internal node must always keep the same daughter nodes as in the original tree. For an example of a binarized2 tree and a fragmen</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computational Aspects of Lexicalized Grammars. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
<author>Samir AbdelRahman</author>
<author>Tim Miller</author>
<author>Lane Schwartz</author>
</authors>
<title>Broad-coverage parsing using human-like memory constraints.</title>
<date>2010</date>
<journal>Computational Linguististics,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="2247" citStr="Schuler et al., 2010" startWordPosition="338" endWordPosition="341">nd Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models an</context>
<context position="32093" citStr="Schuler et al. (2010)" startWordPosition="5527" endWordPosition="5530">ce, it computes the longest common subsequence between the sequence of predicted words (possibly none) and the words following the prefix in the original sentence. Recall and precision can be computed in the usual way for these three measures. Recall is the total number (over all prefixes) of correctly predicted words (as defined by PRD(m), PRS(m), or LCS) over the total number of words expected to be predicted (according to m), while precision is the number of correctly predicted words over the number of words predicted by the model. We compare the ITSG parser with the incremental parsers of Schuler et al. (2010) and Demberg et al. (2014) for full-sentence parsing, with the Roark (2001) parser19 for full-sentence and partial pars17We also evaluated our ITSG model using perplexity; the results obtained were substantially worse than those obtained using Roark’s parsers. 18Note that neither PRD(m) nor PRS(m) correspond to word error rate (WER). PRD requires the predicted word sequence to be identical to the original sequence, while PRS only requires the predicted words to be present in the original. In contrast, WER measures the minimum number of substitutions, insertions, and deletions needed to transfo</context>
<context position="34485" citStr="Schuler et al. (2010)" startWordPosition="5986" endWordPosition="5989">. We used a standard 3-gram model trained on the sentences of the training set using the default setting and smoothing (Kneser-Ney) provided by the SRILM package. (Higher n-gram model do not seem appropriate, given the small size of the training corpus.) For every prefix in the test set we compute the most probable continuation predicted by the n-gram model.20 4.3 Results Table 1 reports full-sentence parsing results for our parser and three comparable incremental parsers from the literature. While Roark (2001) obtains the best results, the ITSG parser without smoothing performs on a par with Schuler et al. (2010), and outperforms Demberg et al. (2014).21 Adding smoothing results in a gain of 1.2 points F-score over the Schuler parser. When we compare the different parsing objectives of the ITSG parser, MRP is the best one, followed by MPP and MPD. Incremental Parsing The graphs in Figure 8 compare the ITSG and Roark’s parser on the incremental parsing evaluation, when parsing sentences of length 10, 20, 30 and 40. The performance of both models declines as the length of the prefix increases, with Roark’s parser outperforming the ITSG parser on average, although the ITSG parser seems more com20We used </context>
<context position="38199" citStr="Schuler et al., 2010" startWordPosition="6632" endWordPosition="6635">known word). on the basis of context-free rules. These are augmented with a large number of non-local features (e.g., grandparent categories). Our approach avoids the need for such additional features, as TSG fragments naturally contain non-local information. Roark’s parser outperforms ours in both fullsentence and incremental F-score (see Section 4), but cannot be used for sentence prediction straightforwardly: to obtain a prediction for the next word, we would need to compute an argmax over the whole vocabulary, then iterate this for each word after that (the same is true for the parsers of Schuler et al., 2010 and Demberg et al., 2014). Most incremental dependency parsers use a discriminative model over parse actions (Nivre, 2007), and therefore cannot predict upcoming words either (but see Huang and Sagae 2010). Turning to the literature on sentence prediction, we note that ours is the first attempt to use a parser for this task. Existing approaches either use n-gram models (Eng and Eisner, 2004; Bickel et al., 2005) or a retrieval approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining</context>
</contexts>
<marker>Schuler, AbdelRahman, Miller, Schwartz, 2010</marker>
<rawString>William Schuler, Samir AbdelRahman, Tim Miller, and Lane Schwartz. 2010. Broad-coverage parsing using human-like memory constraints. Computational Linguististics, 36(1):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental syntactic language models for phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>620--631</pages>
<location>Portland, OR.</location>
<contexts>
<context position="2552" citStr="Schwartz et al., 2011" startWordPosition="382" endWordPosition="385"> question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, whi</context>
<context position="39290" citStr="Schwartz et al., 2011" startWordPosition="6806" endWordPosition="6809">ntence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction as a new way of evaluating incremental parsers, and demonstrated</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language models for phrase-based translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 620–631. Association for Computational Linguistics, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of treegrammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics,</booktitle>
<pages>1175--1180</pages>
<location>Copenhagen.</location>
<marker>Sima’an, 1996</marker>
<rawString>Khalil Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of treegrammars. In Proceedings of the 16th Conference on Computational Linguistics, pages 1175– 1180. Association for Computational Linguistics, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="20780" citStr="Stolcke (1995)" startWordPosition="3579" endWordPosition="3580">agments in Figure 7 and the two derivations of the same parse tree in the same figure; Figure 7 represents the parsing chart of the same grammar, containing the two corresponding paths. 3.3 Probabilistic Parser In the probabilistic version of the parser, each fringe in the grammar has a given probability, such that Equations (1)–(3) are satisfied.8 In the probabilistic chart, every state i : kX ✁λ•µ is decorated with three 6This further simplifies the probabilistic version of our parser, as there is no need to resort to the probabilistic reflexive, transitive left-corner relation described by Stolcke (1995). 7This operation would violate Earley’s top-down constraint; donor states are in fact the terminal states in Earley algorithm. 8The probability of an ambiguous fringe is the marginal probability of the fragments mapping to it. 117 0 – “Terms” S |0NP✁• “Terms” [1/2, 1/2, 1] ||0S S@ [1/2, 1/2, 1] ✁ “Terms” • 3 – “.” S |0S✁NP “were” VP • “.” [1/2, 1/2, 1] ||1S@ ✁ “were” VP • “.” [1/2, 1, 1/2] C 2VP✁“disclosed” • [1, 1, 1] ||*** |** 4 – # S 0∅ ✁ S • # [1, 1, 1] ® ||oS✁ “Terms” S@ • [1/2, 1/2, 1] ∅✁S # [1 ] � 0S✁NP “were” VP “.” • [1/2, 1/2, 1] C ||1S@ ✁ “were” VP “.” • [1/2, 1, 1/2] ||* Figure 7:</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference on Spoken Language Processing,</booktitle>
<pages>257--286</pages>
<location>Denver, CO.</location>
<contexts>
<context position="33840" citStr="Stolcke, 2002" startWordPosition="5881" endWordPosition="5882">83.5 86.5 87.6 92 re 83.2 88 83.4 84.8 86 82.5 90 82.6 84.1 sc - F1 99 98 97 96 95 94 93 92 91 98 96 94 F-score 92 90 88 86 84 82 80 78 2 3 4 5 6 7 8 9 10 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 Roark et al. (2009) Roark (last) ITSG ITSG Smooth. Smooth. (MPD) (MPD) 97 96 95 94 93 92 91 90 89 88 87 86 85 98 96 94 92 90 88 86 84 82 80 78 Table 1: Full-sentence parsing results for sentences in the 2345678910 test set of length up to 40 words. ing, and with a language model built using SRILM (Stolcke, 2002) for sentence prediction. We used a standard 3-gram model trained on the sentences of the training set using the default setting and smoothing (Kneser-Ney) provided by the SRILM package. (Higher n-gram model do not seem appropriate, given the small size of the training corpus.) For every prefix in the test set we compute the most probable continuation predicted by the n-gram model.20 4.3 Results Table 1 reports full-sentence parsing results for our parser and three comparable incremental parsers from the literature. While Roark (2001) obtains the best results, the ITSG parser without smoothing</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing, pages 257–286. Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>Joel Tetreault</author>
<author>James Allen</author>
</authors>
<title>Incremental parsing with reference interaction.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>18--25</pages>
<editor>In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors,</editor>
<location>Barcelona.</location>
<contexts>
<context position="2665" citStr="Stoness et al., 2004" startWordPosition="400" endWordPosition="403">nd for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined into new syntax trees by means of a substitution operation. An extensive tradition of parsing </context>
</contexts>
<marker>Stoness, Tetreault, Allen, 2004</marker>
<rawString>Scott C. Stoness, Joel Tetreault, and James Allen. 2004. Incremental parsing with reference interaction. In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, pages 18–25. Association for Computational Linguistics, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Tan</author>
<author>Wenli Zhou</author>
<author>Lei Zheng</author>
<author>Shaojun Wang</author>
</authors>
<title>A large scale distributed syntactic, semantic and lexical language model for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>201--210</pages>
<location>Portland, OR.</location>
<contexts>
<context position="2571" citStr="Tan et al., 2011" startWordPosition="386" endWordPosition="389">processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a set of arbitrarily large tree fragments, which can be combined </context>
<context position="39309" citStr="Tan et al., 2011" startWordPosition="6810" endWordPosition="6813">om a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction as a new way of evaluating incremental parsers, and demonstrated that our parser ou</context>
</contexts>
<marker>Tan, Zhou, Zheng, Wang, 2011</marker>
<rawString>Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang. 2011. A large scale distributed syntactic, semantic and lexical language model for machine translation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1, pages 201–210. Association for Computational Linguistics, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J SpiveyKnowlton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<pages>268--1632</pages>
<contexts>
<context position="1615" citStr="Tanenhaus et al., 1995" startWordPosition="236" endWordPosition="239">nlike existing parsers, our incremental TSG parser can generate partial trees that include predictions about the upcoming words in a sentence. We show that it outperforms an n-gram model in predicting more than one upcoming word. 1 Introduction When humans listen to speech, the input becomes available gradually as the speech signal unfolds. Reading happens in a similarly gradual manner when the eyes scan a text. There is good evidence that the human language processor is adapted to this and works incrementally, i.e., computes an interpretation for an incoming sentence on a word-by-word basis (Tanenhaus et al., 1995; Altmann and Kamide, 1999). Also language processing systems often deal with speech as it is spoken, or text as it is being typed. A dialogue system should start interpreting a sentence while it is being spoken, and a question answering system should start retrieving answers before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Ro</context>
</contexts>
<marker>Tanenhaus, SpiveyKnowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K. Tanenhaus, Michael J. SpiveyKnowlton, Kathleen M. Eberhard, and Julie C. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>191--198</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="2508" citStr="Xu et al., 2002" startWordPosition="376" endWordPosition="379">before the user has finished typing the question. Incremental processing is therefore essential both for realistic models of human language processing and for NLP applications that react to user input in real time. In response to this, a number of incremental parsers have been developed, which use context-free grammar (Roark, 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or treeadjoining grammar (Demberg et al., 2014). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Another potential use of incremental parsers is sentence prediction, i.e., the task of predicting upcoming words in a sentence given a prefix. However, so far only n-gram models and classifiers have been used for this task (Fazly and Hirst, 2003; Eng and Eisner, 2004; Grabski and Scheffer, 2004; Bickel et al., 2005; Li and Hirst, 2005). In this paper, we present an incremental parser for Tree Substitution Grammar (TSG). A TSG contains a </context>
<context position="39243" citStr="Xu et al., 2002" startWordPosition="6798" endWordPosition="6801">val approach in which the best matching sentence is identified from a sentence collection given a set of features (Grabski and Scheffer, 2004). There is also work combining n-gram models with lexical semantics (Li and Hirst, 2005) or part-of-speech information (Fazly and Hirst, 2003). In the language modeling literature, more sophisticated models than simple n-gram models have been developed in the past few years, and these could potentially improve sentence prediction. Examples include syntactic language models which have applied successfully for speech recognition (Chelba and Jelinek, 2000; Xu et al., 2002) and machine translation (Schwartz et al., 2011; Tan et al., 2011), as well as discriminative language models (Mikolov et al., 2010; Roark et al., 2007). Future work should evaluate these approaches against the ITSG model proposed here. 6 Conclusions We have presented the first incremental parser for tree substitution grammar. Incrementality is motivated by psycholinguistic findings, and by the need for real-time interpretation in NLP. We have shown that our parser performs competitively on both full sentence and sentence prefix F-score. We also introduced sentence prediction as a new way of e</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 191–198. Association for Computational Linguistics, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>