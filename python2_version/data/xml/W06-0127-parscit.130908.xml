<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008998">
<title confidence="0.998956">
An Improved Chinese Word Segmentation System
with Conditional Random Field
</title>
<author confidence="0.997716">
Hai Zhao, Chang-Ning Huang and Mu Li
</author>
<affiliation confidence="0.994212">
Microsoft Research Asia,
</affiliation>
<address confidence="0.9844255">
49, Zhichun Road, Haidian District,
Beijing, P. R. China, 100080
</address>
<email confidence="0.9846425">
Email: {f-hzhao,cnhuangl@msrchina.research.microsoft.com,
muli@microsoft.com
</email>
<sectionHeader confidence="0.994695" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999684555555556">
In this paper, we describe a Chinese
word segmentation system that we de-
veloped for the Third SIGHAN Chinese
Language Processing Bakeoff (Bakeoff-
2006). We took part in six tracks, namely
the closed and open track on three cor-
pora, Academia Sinica (CKIP), City Uni-
versity of Hong Kong (CityU), and Uni-
versity of Pennsylvania/University of Col-
orado (UPUC). Based on a conditional
random field based approach, our word
segmenter achieved the highest F mea-
sures in four tracks, and the third highest
in the other two tracks. We found that the
use of a 6-tag set, tone feature of Chinese
character and assistant segmenters trained
on other corpora further improve Chinese
word segmentation performance.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884454545454">
Conditional random field (CRF) is a statistical se-
quence modeling framework first introduced into
language processing in (Lafferty et al., 2001). Work
by Peng et al. first used this framework for Chinese
word segmentation by treating it as a binary decision
task, such that each Chinese character is labeled ei-
ther as the beginning of a word or not (Peng et al.,
2004).
Since two participants, Ng and Tseng in Bakeoff-
2005, gave the best results in almost all test corpora
(Low et al., 2005), (Tseng et al., 2005), we continue
to improve CRF-based tagging method of Chinese
word segmentation on their track. Our implemen-
tation used CRF++ package Version 0.411 by Taku
Kudo.
In our system, a Chinese character is labeled by a
tag which stands for its position in the Chinese word
that the character belongs to. We handle closed test
and open test in the same way. The difference is that
those features concerned with additional linguistic
resources are added in the feature set of closed test
to produce the feature set used in open test.
</bodyText>
<sectionHeader confidence="0.795522" genericHeader="method">
2 Tag Set Selection
</sectionHeader>
<bodyText confidence="0.9996959">
Character based tagging method for Chinese word
segmentation, either based on maximum entropy or
CRF, views Chinese word segmentation as a label
tagging problem, which is described in detail in
(Ratnaparkhi, 1996).
The probability model and corresponding feature
function is defined over the set H x T, where H is
the set of possible contexts (or any predefined con-
dition) and T is the set of possible tags. Generally, a
feature function can be defined as follows,
</bodyText>
<equation confidence="0.998203">
f (h , t)
1, if h = hi is satisfied and t — ti
=
0, otherwise,
(1)
</equation>
<bodyText confidence="0.998132166666667">
where hi, c H and t3 E T.
For convenience, features are generally organized
into some groups, which used to be called feature
templates. For example, a bigram feature template
C1 stands for the next character occurring in the cor-
pus after each character.
</bodyText>
<footnote confidence="0.677407">
&apos;http://chasen.org/ taku/software/CRF++/
</footnote>
<page confidence="0.960312">
162
</page>
<bodyText confidence="0.953173333333333">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162-165,
Sydney, July 2006. C)2006 Association for Computational Linguistics
As for tag set, there are two kinds of schemes
that are used to distinguish the character position in
a word in the previous work, i.e., 4-tag set and 2-
tag set. The details are listed in Table 1. Notice
Xue and Ng use a 4-tag set in maximum entropy
model. While Peng and Tseng used a 2-tag set in
CRF model.
</bodyText>
<tableCaption confidence="0.8979765">
Table 1: Tag sets in Chinese word segmentation in
the previous work
</tableCaption>
<table confidence="0.993360428571428">
4-tag set 2-tag set
Ng/(Xue) Peng/Tseng
Function Tag Function Tag
begin B(LL) start Start
middle M(MM) continuation NoStart
end E(RR)
single S(LR)
</table>
<bodyText confidence="0.995441888888889">
Generally speaking, activated feature functions in
practice like (1) are determined by both feature tem-
plate and tag set. In the existing work, tag set is spec-
ified aforehand. To effectively perform tagging for
those long words, we extend the 4-tag set of Ng/Xue
into a 6-tag set. Two tags, &apos;B2&apos; and &apos;B3&apos;, are added
into a 4-tag set to form a 6-tag set, each additional
tag stands for the second and the third character po-
sition in a Chinese word, respectively.
</bodyText>
<sectionHeader confidence="0.963227" genericHeader="method">
3 Feature Templates for Closed Test
</sectionHeader>
<bodyText confidence="0.984579470588235">
The feature template set we selected for closed test is
shown in Table 2. We give an explanation to feature
template (e) and (f).
Feature template (e) is improved from the corre-
sponding one in (Low et al., 2005). Tr?, , n = —1, 0, 1
stands for predefined class. There are four classes
defined: numbers represent class 1, those characters
whose meanings are dates represent class 2, English
letters represent class 3, and other characters repre-
sent class 4.
As for feature template (f), To(C0) stands for
the tone of current character. There are five possi-
ble types of tones for Chinese characters in man-
darin, we just assign 0, 1, 2, 3 and 4 as feature
values. For example, consider some characters,
X.&apos; and &apos; To(C0) is 1, 2, 3, 4 and
0, respectively.
</bodyText>
<sectionHeader confidence="0.917262" genericHeader="method">
4 Feature Templates for Open Test
</sectionHeader>
<bodyText confidence="0.999282">
In open test, we use two kinds of additional feature
templates to improve the performance upon closed
test.
</bodyText>
<subsectionHeader confidence="0.983799">
4.1 External Dictionary
</subsectionHeader>
<bodyText confidence="0.9998268">
This method was firstly introduced in (Low et al.,
2005). We continue to use the online dictionary from
Peking University downloadable from the Internet 2,
consisting of about 108,000 words of length one to
four characters. If there is some sequence of neigh-
boring characters around Co in the sentence that
matches a word in this dictionary, then we greed-
ily choose the longest such matching word W in the
dictionary. The following features derived from the
dictionary are added:
</bodyText>
<listItem confidence="0.943177">
(g) Lto
(h) Cnto (n = —1, 0, 1)
</listItem>
<bodyText confidence="0.998658">
where to is the boundary tag of Co in W, and L is
the number of characters in W.
</bodyText>
<subsectionHeader confidence="0.998015">
4.2 Assistant Segmenter
</subsectionHeader>
<bodyText confidence="0.9946774">
We observed that although there exists different seg-
mentation standards, most words are still segmented
in the same way according to different segmentation
standards. Thus, though those segmenters trained
on different corpora will give some different seg-
mentation results, they agree on most cases. In fact,
we find that it is feasible to customize a pre-defined
standard into any other standards with TBL method
in (Gao, 2005). And it is also valuable to incorpo-
rate different segmenters into one segmenter based
on the current standard. For convenience, we call
the segmenter subjected to the current standard main
segmenter, and the other assistant segmenters.
A feature template will be added for a assistant
segmenter:
</bodyText>
<listItem confidence="0.550962">
(i) t(Co)
</listItem>
<footnote confidence="0.997878">
2http://ccl.pku.edu.cn/doubtfire/Course/Chinese %20Infor-
mation%20Processing/Source Code/ Chapter 8/Lexicon full
2000.zip
</footnote>
<page confidence="0.999205">
163
</page>
<tableCaption confidence="0.980136">
Table 2: Feature templates
</tableCaption>
<table confidence="0.994238285714286">
Code Type Feature Function
a Unigram Cr), n = —1, 0, 1 The previous (current, next) character
b Bigram CnCn+i, n = —1, 0 The previous (next) character and current character
c Jump C_1C1 The previous character and next character
d Punctuation Pu(Co) Current character is a punctuation or not
e Date, Digital and Letter T_iToTi Types of previous, current and next character
f Tone To(C0) Tone of current character
</table>
<bodyText confidence="0.99994175">
where t(Co) is the output tag of the assistant seg-
menter for the current character Co. For exam-
ple, consider character sequence, &apos; &amp;41&apos;1-n111-1-1
A&apos;, an assistant segmenter gives the tag sequence
&apos;BESSBES&apos; according to its output segmentation,
then t (Co) by this assistant segmenter is &apos;B&apos;, &apos;E&apos;,
&apos;S&apos;, &apos;S&apos;, &apos;B&apos;, &apos;E&apos;, and &apos;S&apos; for each current character,
respectively.
In our system, we integrate all other segmenters
that are trained on all corpora from Bakeoff-2003,
2005 and 2006 with the feature set used in closed
test. The segmenter, MSRSeg, described in (Gao,
2003) is also integrated, too.
Our assistant segmenter method is more conve-
nient compared to the additional training corpus
method in (Low et al., 2005). Firstly, the perfor-
mance of additional corpus method depends on the
performance of the trained segmenter that carries out
the corpus extraction task. If the segmenter is not
well-trained, then it cannot effectively extract the
most wanted additional corpus to some extent. Sec-
ondly, additional corpus method is only able to inte-
grate useful corpus, but it cannot integrate a well-
trained segmenter while the corpus cannot be ac-
cessed. Finally, additional corpus method is very
difficult to use in CRF model, the reason is that the
increase of corpus can lead to a dramatic increase of
memory and time consuming in this case, while as-
sistant segmenters just lead to little increase of mem-
ory and time consuming in training.
It is more interesting that we may also regard the
external dictionary method as another assistant seg-
menter in some degree, that is, a maximal match-
ing segmenter with the specified external dictionary.
Thus, all of our additional methods in open test can
be viewed as assistant segmenter ones.
</bodyText>
<sectionHeader confidence="0.980427" genericHeader="conclusions">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.992556529411765">
We took part in six segmentation tasks in Bakeoff-
2006, namely the closed and open track on three cor-
pora, Academia Sinica (CKIP), City University of
Hong Kong (CityU), and University of Pennsylva-
nia/University of Colorado (UPUC).
The comparison between our official results and
best results in Bakeoff-2006 are shown in Table 3.
Our system achieved the highest F measures in
four tracks, and the third highest in the other two
tracks. However, a format error unfortunately oc-
curred in the open test of UPUC corpus as we sub-
mitted our final results. Thus an abnormal result in
this task is obtained, the official F measure in open
test is the same as that in closed test. We get the
actual F measure of 0.953 after the bug is fixed.
The results in MSRA corpus from our evaluation
are listed in Table 4.
</bodyText>
<tableCaption confidence="0.9272195">
Table 4: Comparison between our results and best
results of Bakeoff-2006 on MSRA corpora
</tableCaption>
<table confidence="0.9049485">
Type F Measures
Bakeoff-2006 Ours
Closed Test 0.963 0.970
Open Test 0.979 0.982
</table>
<bodyText confidence="0.833167">
The sizes of training corpora (in number of char-
acters) and difference of our results between open
</bodyText>
<page confidence="0.998898">
164
</page>
<tableCaption confidence="0.999958">
Table 3: Comparison between our official results and best results of Bakeoff-2006
</tableCaption>
<table confidence="0.99645725">
Type Participant F measures on D ifferent Corpora
CKIP CityU UPUC
Closed Best results of Bakeoff-2006 0.958 0.972 0.933
Test
Our results 0.958 0.971 0.933
Open Best results of Bakeoff-2006 0.959 0.977 0.944
Test
Our results 0.959 0.977 0.933
</table>
<figure confidence="0.972010307692308">
F Measures
CKIP CityU
0.001 0.006
9M 2.9M
MSRA
0.012
2.3M
UPUC
0.020
0.88M
Fopen Fclosed
Size of
training corpus
</figure>
<bodyText confidence="0.519659125">
test and closed test are shown in Table 5. This illus-
trates how much assistant segmenters improve the
performance of segmentation in different sizes of
training corpora, also, this shows how the size of
training corpus affects the improvement contributed
by assistant segmenters.
Table 5: The sizes of training corpora and difference
of our results between open test and closed test
</bodyText>
<sectionHeader confidence="0.996357" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999751816326531">
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang.
2005. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, Vol. 31(4): 531-574.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, Vol. 8(1): 29-48.
Nianwen Xue and Libin Shen. 2003. Chinese Word Seg-
mentation as LMR Tagging. In Proceedings gf. the 2nd
SIGHAN Workshop on Chinese Language Processing,
in conjunction with ACL&apos;03, 176-179. Sapporo, Japan
Richard Sproat and Thomas Emerson. 2003. The
First International Chinese Word Segmentation Bake-
off. The Second SIGHAN Workshop on Chinese Lan-
guage Processing, 133-143. Sapporo, Japan.
Thomas Emerson. 2005. The Second International Chi-
nese Word Segmentation Bakeoff. Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing, 123-133. Jeju Island, Korea.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, 161-164. Jeju
Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing, 168-171. Jeju Is-
land, Korea.
Fuchun Peng, Fangfang Feng and Andrew McCallum.
2004. Chinese Segmentation and New Word Detec-
tion Using Conditional Random Fields. In COLING
2004, 562-568. August 23-27, 2004, Geneva, Switzer-
land
John Lafferty, A. McCallum and F. Pereira. 2001. Con-
ditional Random Field: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the Eighteenth International Conference on Ma-
chine Learning, 282-289. June 28-July 01, 2001
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
of-speech Tagger. In Proceedings of the Empirical
Method in Natural Language Processing Conference,
133-142. University of Pennsylvania.
Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. In Proceedings 4 I nd Annual Meeting
of the Association for Computational Linguistics, 272-
279. Sapporo, Japan, July 7-12, 2003.
</reference>
<page confidence="0.998731">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.675249">
<title confidence="0.9647695">An Improved Chinese Word Segmentation with Conditional Random Field</title>
<author confidence="0.978342">Hai Zhao</author>
<author confidence="0.978342">Chang-Ning Huang</author>
<author confidence="0.978342">Mu</author>
<affiliation confidence="0.99984">Microsoft Research</affiliation>
<address confidence="0.8934415">49, Zhichun Road, Haidian Beijing, P. R. China,</address>
<email confidence="0.9987295">muli@microsoft.com</email>
<abstract confidence="0.991192105263158">In this paper, we describe a Chinese word segmentation system that we developed for the Third SIGHAN Chinese Language Processing Bakeoff (Bakeoff- 2006). We took part in six tracks, namely the closed and open track on three corpora, Academia Sinica (CKIP), City University of Hong Kong (CityU), and University of Pennsylvania/University of Colorado (UPUC). Based on a conditional random field based approach, our word segmenter achieved the highest F measures in four tracks, and the third highest in the other two tracks. We found that the use of a 6-tag set, tone feature of Chinese character and assistant segmenters trained on other corpora further improve Chinese word segmentation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Andi Wu</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>4</issue>
<pages>531--574</pages>
<marker>Gao, Li, Wu, Huang, 2005</marker>
<rawString>Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang. 2005. Chinese Word Segmentation and Named Entity Recognition: A Pragmatic Approach. Computational Linguistics, Vol. 31(4): 531-574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, Vol. 8(1): 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese Word Segmentation as LMR Tagging.</title>
<date>2003</date>
<booktitle>In Proceedings gf. the 2nd SIGHAN Workshop on Chinese Language Processing, in conjunction with ACL&apos;03,</booktitle>
<pages>176--179</pages>
<location>Sapporo, Japan</location>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese Word Segmentation as LMR Tagging. In Proceedings gf. the 2nd SIGHAN Workshop on Chinese Language Processing, in conjunction with ACL&apos;03, 176-179. Sapporo, Japan</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<date>2003</date>
<booktitle>The First International Chinese Word Segmentation Bakeoff. The Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<location>Sapporo, Japan.</location>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The First International Chinese Word Segmentation Bakeoff. The Second SIGHAN Workshop on Chinese Language Processing, 133-143. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The Second International Chinese Word Segmentation Bakeoff.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The Second International Chinese Word Segmentation Bakeoff. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, 123-133. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
</authors>
<title>Hwee Tou Ng and Wenyuan Guo</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<location>Jeju Island,</location>
<marker>Low, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo 2005. A Maximum Entropy Approach to Chinese Word Segmentation. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, 161-164. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A Conditional Random Field Word Segmenter for Sighan Bakeoff</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1513" citStr="Tseng et al., 2005" startWordPosition="236" endWordPosition="239"> segmenters trained on other corpora further improve Chinese word segmentation performance. 1 Introduction Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in (Lafferty et al., 2001). Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is labeled either as the beginning of a word or not (Peng et al., 2004). Since two participants, Ng and Tseng in Bakeoff2005, gave the best results in almost all test corpora (Low et al., 2005), (Tseng et al., 2005), we continue to improve CRF-based tagging method of Chinese word segmentation on their track. Our implementation used CRF++ package Version 0.411 by Taku Kudo. In our system, a Chinese character is labeled by a tag which stands for its position in the Chinese word that the character belongs to. We handle closed test and open test in the same way. The difference is that those features concerned with additional linguistic resources are added in the feature set of closed test to produce the feature set used in open test. 2 Tag Set Selection Character based tagging method for Chinese word segment</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, Christopher Manning. 2005. A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, 168-171. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese Segmentation and New Word Detection Using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In COLING 2004,</booktitle>
<pages>562--568</pages>
<location>Geneva, Switzerland</location>
<contexts>
<context position="1369" citStr="Peng et al., 2004" startWordPosition="210" endWordPosition="213">ur tracks, and the third highest in the other two tracks. We found that the use of a 6-tag set, tone feature of Chinese character and assistant segmenters trained on other corpora further improve Chinese word segmentation performance. 1 Introduction Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in (Lafferty et al., 2001). Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is labeled either as the beginning of a word or not (Peng et al., 2004). Since two participants, Ng and Tseng in Bakeoff2005, gave the best results in almost all test corpora (Low et al., 2005), (Tseng et al., 2005), we continue to improve CRF-based tagging method of Chinese word segmentation on their track. Our implementation used CRF++ package Version 0.411 by Taku Kudo. In our system, a Chinese character is labeled by a tag which stands for its position in the Chinese word that the character belongs to. We handle closed test and open test in the same way. The difference is that those features concerned with additional linguistic resources are added in the feat</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng and Andrew McCallum. 2004. Chinese Segmentation and New Word Detection Using Conditional Random Fields. In COLING 2004, 562-568. August 23-27, 2004, Geneva, Switzerland</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1145" citStr="Lafferty et al., 2001" startWordPosition="168" endWordPosition="171">emia Sinica (CKIP), City University of Hong Kong (CityU), and University of Pennsylvania/University of Colorado (UPUC). Based on a conditional random field based approach, our word segmenter achieved the highest F measures in four tracks, and the third highest in the other two tracks. We found that the use of a 6-tag set, tone feature of Chinese character and assistant segmenters trained on other corpora further improve Chinese word segmentation performance. 1 Introduction Conditional random field (CRF) is a statistical sequence modeling framework first introduced into language processing in (Lafferty et al., 2001). Work by Peng et al. first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each Chinese character is labeled either as the beginning of a word or not (Peng et al., 2004). Since two participants, Ng and Tseng in Bakeoff2005, gave the best results in almost all test corpora (Low et al., 2005), (Tseng et al., 2005), we continue to improve CRF-based tagging method of Chinese word segmentation on their track. Our implementation used CRF++ package Version 0.411 by Taku Kudo. In our system, a Chinese character is labeled by a tag which stands for</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, 282-289. June 28-July 01, 2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Partof-speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the Empirical Method in Natural Language Processing Conference,</booktitle>
<pages>133--142</pages>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2271" citStr="Ratnaparkhi, 1996" startWordPosition="364" endWordPosition="365"> 0.411 by Taku Kudo. In our system, a Chinese character is labeled by a tag which stands for its position in the Chinese word that the character belongs to. We handle closed test and open test in the same way. The difference is that those features concerned with additional linguistic resources are added in the feature set of closed test to produce the feature set used in open test. 2 Tag Set Selection Character based tagging method for Chinese word segmentation, either based on maximum entropy or CRF, views Chinese word segmentation as a label tagging problem, which is described in detail in (Ratnaparkhi, 1996). The probability model and corresponding feature function is defined over the set H x T, where H is the set of possible contexts (or any predefined condition) and T is the set of possible tags. Generally, a feature function can be defined as follows, f (h , t) 1, if h = hi is satisfied and t — ti = 0, otherwise, (1) where hi, c H and t3 E T. For convenience, features are generally organized into some groups, which used to be called feature templates. For example, a bigram feature template C1 stands for the next character occurring in the corpus after each character. &apos;http://chasen.org/ taku/s</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Partof-speech Tagger. In Proceedings of the Empirical Method in Natural Language Processing Conference, 133-142. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved Source-Channel Models for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings 4 I nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>272--279</pages>
<location>Sapporo, Japan,</location>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Jianfeng Gao, Mu Li, and Chang-Ning Huang. 2003. Improved Source-Channel Models for Chinese Word Segmentation. In Proceedings 4 I nd Annual Meeting of the Association for Computational Linguistics, 272-279. Sapporo, Japan, July 7-12, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>