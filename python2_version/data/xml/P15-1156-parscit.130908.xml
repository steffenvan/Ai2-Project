<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.911194">
Unsupervised Prediction of Acceptability Judgements
</title>
<author confidence="0.665966">
Jey Han Lau, Alexander Clark, and Shalom Lappin
</author>
<email confidence="0.773039">
jeyhan.lau@gmail.com, alexsclark@gmail.com, shalom.lappin@kcl.ac.uk
</email>
<author confidence="0.246483">
King’s College London
</author>
<bodyText confidence="0.999141526315789">
2014), there is limited research on the relationship
between acceptability and probability. In this pa-
per, we consider the the task of unsupervised pre-
diction of acceptability.
Speakers have robust intuitions about accept-
ability, and acceptability has been consistently
rated on various scales (Sprouse and Almeida,
2012). The acceptability of a sentence appears to
be relatively unaffected by its length (within cer-
tain bounds), or the frequency of its words, prop-
erties that we have confirmed experimentally. By
contrast sentence probability does depend on these
factors. To filter the effects of sentence length and
word frequency, we devise normalising functions
to map the probability of a sentence (inferred by
our unsupervised language models) to an accept-
ability score.
Keller (2001) and Lau et al. (2014) present ev-
idence that acceptability exhibits gradience. Ac-
cordingly, we treat acceptability as a continuous
variable here. We train a variety of unsuper-
vised models for the acceptability prediction task,
and we assess the performance of these models
by measuring the correlation between their nor-
malised acceptability scores and the mean crowd-
sourced acceptability judgements on a set of test
sentences.
There are a number of NLP tasks to which our
work can be fruitfully applied. It can be used
to evaluate the fluency of the output for machine
translation and other language generation systems.
It could also contribute to automatic essay scoring,
and to second language tutorial systems.
There are several reasons to favour unsuper-
vised models. From an engineering perspective,
unsupervised models offer greater portability to
other domains and languages. Our methodology
takes only unannotated text as input. Extending
</bodyText>
<page confidence="0.832221">
1618
</page>
<sectionHeader confidence="0.822802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999681269230769">
In this paper we present the task of un-
supervised prediction of speakers’ accept-
ability judgements. We use a test set
generated from the British National Cor-
pus (BNC) containing both grammatical
sentences and sentences containing a va-
riety of syntactic infelicities introduced
by round trip machine translation. This
set was annotated for acceptability judge-
ments through crowd sourcing. We trained
a variety of unsupervised language mod-
els on the original BNC, and tested them
to see the extent to which they could pre-
dict mean speakers’ judgements on the test
set. To map probability to acceptability,
we experimented with several normalisa-
tion functions to neutralise the effects of
sentence length and word frequencies. We
found encouraging results with the unsu-
pervised models predicting acceptability
across two different datasets. Our method-
ology is highly portable to other domains
and languages, and the approach has po-
tential implications for the representation
and the acquisition of linguistic knowl-
edge.
</bodyText>
<sectionHeader confidence="0.985746" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994991375">
Language modelling involves predicting the prob-
ability of a sentence. Given a trained model, we
can infer the quantitative likelihood that a sentence
occurs. Acceptability, on the other hand, indicates
the extent to which a sentence is permissible or ac-
ceptable to native speakers of the language. While
acceptability is affected by frequency and exhibits
gradience (Keller, 2001; Sprouse, 2007; Lau et al.,
</bodyText>
<note confidence="0.941022666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1618–1628,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999830551020408">
our methodology to other domains/languages is
therefore straightforward, as it requires only a raw
training corpus in that domain/language.
Our work may also have significant implica-
tions for the cognitive foundations of the repre-
sentation and acquisition of linguistic knowledge.
The unannotated training corpora of our language
models are impoverished input in comparison to
the data available to humans language learners,
who learn from a variety of data sources (vi-
sual and auditory cues, interaction with adults and
peers in a non-linguistic environment, etc). If an
unsupervised language model can reliably predict
human acceptability judgements, then it provides
a benchmark of what humans could, in principle,
achieve with the same learning algorithm.
Success in this task raises interesting questions
about the nature of grammatical knowledge. If ac-
ceptability judgments can be accurately modeled
through these techniques, then it seems unneces-
sary to posit an underlying binary model of syn-
tax which enumerates all and only the set of well-
formed sentences. Instead it is reasonable to sug-
gest that humans represent linguistic knowledge
as a probabilistic, rather than as a binary system.
Probability distributions provide a natural expla-
nation of the gradience that characterises accept-
ability judgements. Gradience is intrinsic to prob-
ability distributions, and to the acceptability scores
that we derive from these distributions.
While our results raise important questions con-
cerning the nature of syntactic representation and
of language acquisition, we leave them open for
further research. We refrain from making strong
claims on cognitive issues here. Clearly addi-
tional psychological evidence is required to moti-
vate substantive conclusions on these issues, even
if our results suggest them.
Our focus in this paper is on the task of predict-
ing speakers’ acceptability judgements through
unsupervised language models. We take this to be
a problem in natural language processing, whose
solution has useful applications in language tech-
nology. All models described in this paper are im-
plemented in an open source toolkit.1
We describe our dataset in Section 2, which
consists of crowd sourced acceptability judg-
ments applied to sentences with errors introduced
through round trip machine translation. We de-
</bodyText>
<footnote confidence="0.9465915">
1This toolkit can be accessed at https://github.
com/jhlau/acceptability_prediction.
</footnote>
<bodyText confidence="0.999814">
scribe the models and their results in Section 3. In
Section 4 we present results with a different cor-
pus based on English Wikipedia. The new dataset
shows that our observations generalise to another
domain. We compare our methodology to a super-
vised system in the acceptability prediction task in
Section 5. We look more closely at the influence
of sentence length and lexical frequency in Sec-
tion 6, and we show that the normalising functions
succeed in neutralising these effects. Finally, we
discuss the implications of our results, and draw
conclusions from them in Section 7 and Section 8.
</bodyText>
<sectionHeader confidence="0.850001" genericHeader="introduction">
2 Dataset and Methodology
</sectionHeader>
<bodyText confidence="0.999986771428572">
For our experiments, we require a collection of
sentences that exhibit varying degrees of gram-
matical well-formedness. We use the dataset that
we discuss in Lau et al. (2014). We translated
British National Corpus (BNC Consortium, 2007)
English sentences to four other languages – Nor-
wegian, Spanish, Japanese and Chinese – and then
back to English using Google Translate. To collect
human judgements of acceptability for the sen-
tences, we used Amazon Mechanical Turk. A total
of 2,500 sentences were annotated.
Three modes of presentation were used for rat-
ing a sentence: (1) binary with two options (un-
natural vs. natural); (2) four options (extremely
unnatural, somewhat unnatural, somewhat natural
and extremely natural); and (3) a sliding scale with
two extremes (extremely unnatural and extremely
natural). To aggregate the ratings over multiple
speakers for each sentence, we computed the arith-
metic mean. As there is a high correlation of mean
ratings among different modes of presentation, we
take the judgements for the four-option mode of
presentation as the gold-standard for our experi-
ments.
To predict the ratings of the 2,500 test sen-
tences, we trained several probabilistic models on
the BNC, and then used the trained models to infer
the probabilities of the test sentences. Models are
trained on the written portion of the BNC, which
has approximately 100 million words (henceforth
referred to as BNC-100M).2 We used only the
words, and no forms of annotation information in
the BNC, as input to training.
We first experiment with simple lexical N-gram
models, and then move to Bayesian and neural
</bodyText>
<footnote confidence="0.933807">
2We removed sentences with less than 8 words, as well as
the 2,500 test sentences, from the training data.
</footnote>
<page confidence="0.991702">
1619
</page>
<bodyText confidence="0.9999245">
network models, increasing the complexity of the
models to better capture word dependencies.
To translate probability into acceptability
scores, we compute several acceptability mea-
sures extracted from the model parameters. The
acceptability measures are variants of the sen-
tence’s log probability, devised to normalise sen-
tence length and low frequency words. These
measures are summarised in Table 1. For each
measure (including LogProb as a baseline) we
compute its Pearson correlation coefficient with
the gold standard sentence mean rating to evalu-
ate its effectiveness in predicting acceptability.
We tokenised the training data (BNC-100M) and
the test sentences using OpenNLP, and we con-
verted all words to lower case. To address out of
vocabulary (OOV) words that are seen in the test
sentences but not in the training data, we adopt
the Berkeley Parser approach, where we replace
low frequency or OOV words using the UNK sig-
nature. We capture additional surface characteris-
tics of the original word by attaching features at
the end of the signature (e.g. the OOV word 1949
would be replaced by the signature UNK-NUM).3
</bodyText>
<sectionHeader confidence="0.992553" genericHeader="method">
3 Unsupervised Models
</sectionHeader>
<subsectionHeader confidence="0.996618">
3.1 Lexical N-gram Model
</subsectionHeader>
<bodyText confidence="0.999613368421053">
Lexical N-gram models were variously explored
in tasks related to acceptability estimation (Heil-
man et al., 2014; Clark et al., 2013; Pauls and
Klein, 2012). We use an N-gram model with
Kneser-Ney interpolation (Goodman, 2001), and
we train bigram, trigram, and 4-gram models on
BNC-100M. The trained models are then used to
compute the acceptability measures of the test sen-
tences.
The results are detailed in Table 2 (columns:
“2-gram”, “3-gram” and “4-gram”).4 In general
across all models, the Norm LP (Div) and SLOR
measures consistently produce the best correla-
tions.
We see a significant improvement when the con-
text window is increased from 2-gram to 3-gram,
but not so from 3-gram to 4-gram (2-gram best:
0.34; 3-gram best: 0.42; 4-gram best: 0.42). This
result implies that increasing the context window
</bodyText>
<footnote confidence="0.997784333333333">
3Low frequency words are defined as words occurring less
than 4 times in the BNC training data. A total of 15 features
are used for the UNK signature.
4We do not present model perplexity values in the results,
as we did not find any correlation between perplexity and task
performance.
</footnote>
<sectionHeader confidence="0.405414" genericHeader="method">
Acc. Measure Equation
</sectionHeader>
<equation confidence="0.980435625">
LogProb log Pm(ξ)
log Pm(ξ)
|ξ|
log Pm(ξ)
log Pu(ξ)
Norm LP (Sub) log Pm(ξ) − log Pu(ξ)
log Pm(ξ) − log Pu(ξ)
|ξ|
</equation>
<tableCaption confidence="0.733108">
Table 1: Acceptability measures for predicting the
</tableCaption>
<bodyText confidence="0.989052923076923">
acceptability of a sentence. Notations: SLOR is
the syntactic log-odds ratio, introduced by Pauls
and Klein (2012); ξ is the sentence (|ξ |is the sen-
tence length); Pm(ξ) is the probability of the sen-
tence given by the model; Pu(ξ) is the unigram
probability of the sentence. Note that the negative
sign in Norm LP (Div) is given to reverse the sign
change introduced by the division of log unigram
probabilities.
of the lexical N-gram model does not correspond
to a better representation of grammatical structure
(insofar as the size of the dataset is fixed), and a
more sophisticated model is necessary.
</bodyText>
<subsectionHeader confidence="0.999365">
3.2 Bayesian HMM
</subsectionHeader>
<bodyText confidence="0.999922947368421">
Seeing that local context is insufficient for pre-
dicting acceptability, we explore various Bayesian
models that incorporate richer latent structures.
We chose a Bayesian implementation because its
“rich gets richer” dynamics tends to work well for
languages (Goldwater and Griffiths, 2007; Gold-
water et al., 2009; Newman et al., 2012; Lau et al.,
2012).
Lexical N-grams model the generation of a
word based on its preceding words. We introduce
a layer of latent variables on top of the words,
which can be interpreted as the word classes,
and we model the transitions between the latent
variables and observed words using Markov pro-
cesses. In this model we first generate a (latent)
word class based on its preceding word classes,
and we then generate the word based on its word
class. Figure 1(b) illustrates the structure of
a second order Hidden Markov model (HMM).
</bodyText>
<page confidence="0.909756">
1620
</page>
<figure confidence="0.912065583333333">
Mean LP
Norm LP (Div)
SLOR
wi−2 wi−1 wi wi+1
(a) Lexical 3-gram
(b) Bayesian HMM (2nd Order)
(c) Two-Tier BHMM
si−2 si−1 si si+1
wi−2 wi−1 wi wi+1
ti−2 ti−1 ti ti+1
si−2 si−1 si si+1
wi−2 wi−1 wi wi+1
</figure>
<figureCaption confidence="0.973064">
Figure 1: A comparison of word structures for 3-gram, BHMM and Two-Tier BHMM. w = observed
words; s = tier-1 latent states (“word classes”); t = tier-2 latent states (“phrase classes”).
</figureCaption>
<bodyText confidence="0.999183916666667">
For comparison, the structure of a lexical 3-gram
model is given in Figure 1(a).
Goldwater and Griffiths (2007) propose a
Bayesian approach for learning the HMM struc-
ture. The authors found that their Bayesian
HMM (BHMM) significantly outperforms a
HMM trained with Maximum Likelihood Estima-
tion in unsupervised part-of-speech tagging. We
adopt the methodology of Goldwater and Griffiths
(2007), and train a 2nd order BHMM for our task,
using collapsed Gibbs sampling for inference.
BHMM has two sets of multinomials: the state
transition multinomials and the word emission
multinomials. To generalise the state transition
probabilities for start probabilities, we use dummy
words/states for empty preceding words/states.
BHMM has 3 parameters: (1) S, the number of
latent states; (2) &apos;y, the Dirichlet hyper-parameter
for the state transition multinomials; and (3) δ,
the Dirichlet hyper-parameter for the word emis-
sion multinomials. We assume symmetric Dirich-
let priors for the hyper-parameters, and optimise
the 3 parameters based on test perplexity using a
greedy search approach, i.e. we optimise locally
for one parameter at each stage, while keeping the
default or previously optimised values for other
parameters.5 For the optimisation step models are
trained using 10% of the full BNC (BNC-10M) for
2,000 iterations.6
Using the optimised parameters, we train
BHMM on BNC-100M for 10,000 iterations. For
test inference, we run the Gibbs sampler using
the trained model for 5,000 iterations, and take 50
samples from the final 500 iterations (with a lag
of 10 iterations). In each of the samples, we com-
pute the test probabilities and acceptability mea-
</bodyText>
<footnote confidence="0.97973125">
5When optimising for a parameter, we experimented with
4–6 values of various orders of magnitudes.
6The optimised parameters are: S = 100, -y = 1.0 and
6 = 0.01.
</footnote>
<bodyText confidence="0.998829333333334">
sures using the MAP estimated states.7 The final
probabilities are computed as a harmonic mean of
probabilities over the 50 samples.
We summarise the correlation results in Table 2
(column: “BHMM”). Compared to the N-gram
models, we see an improvement in the correlation,
indicating that the introduction of a layer of (la-
tent) word classes produces a better structure for
modelling acceptability.
</bodyText>
<subsectionHeader confidence="0.995401">
3.3 LDAHMM and LDA
</subsectionHeader>
<bodyText confidence="0.998070384615385">
To better understand the role of semantics in
acceptability, we experimented with LDAHMM
(Griffiths et al., 2004), a model that combines syn-
tactic and semantic dependencies between words.
The generative method of LDAHMM to gener-
ate a word in a document is to first decide whether
to generate a syntactic state or a semantic state for
the word. For the former, follow the HMM process
to generate a state, and generate the word based
on the chosen state. For the latter, follow the LDA
(Blei et al., 2003) process to generate a topic based
on the document’s topic mixture, and generate the
word based on the chosen topic.
We use a second order HMM for the HMM part
and Collapsed Gibbs sampling for performing in-
ference. LDAHMM has 4 sets of multinomials:
the HMM multinomials (state transition and word
emission) and the LDA multinomials (document-
topic and topic-word).
LDAHMM has 6 parameters to optimise: (1)
K the number of topics; (2) S the number
of syntactic states (semantic state has only 1
state, designated as state 0); (3) α, the Dirichlet
hyper-parameter for document-topic multinomi-
als; (4) Q, the Dirichlet hyper-parameter for topic-
word multinomials; (5) &apos;y, the Dirichlet hyper-
</bodyText>
<footnote confidence="0.979732666666667">
7As computing full probabilities gave little difference in
the final outcome, we adopted the computationally more effi-
cient MAP approach.
</footnote>
<page confidence="0.828465">
1621
</page>
<table confidence="0.9998675">
Measure 2-gram 3-gram 4-gram BHMM LDA LDAHMM 2T Chunker RNNLM PCFG*
LogProb 0.24 0.30 0.32 0.25 0.09 0.21 0.26 0.32 0.32 0.21
Mean LP 0.26 0.35 0.37 0.26 0.14 0.19 0.31 0.42 0.39 0.18
Norm LP (Div) 0.33 0.42 0.42 0.44 0.05 0.33 0.50 0.43 0.53 0.26
Norm LP (Sub) 0.12 0.20 0.23 0.33 0.01 0.19 0.46 0.14 0.31 0.22
SLOR 0.34 0.41 0.41 0.45 0.03 0.33 0.50 0.42 0.53 0.25
</table>
<tableCaption confidence="0.9904895">
Table 2: Pearson’s r of acceptability measure and mean sentence rating for all experimented models in BNC. Boldface
indicates the best performing measure. Note that PCFG is a supervised model unlike the others.
</tableCaption>
<bodyText confidence="0.999240666666667">
parameter for state transition multinomials; and
(6) 6, the Dirichlet hyper-parameter for word
emission multinomials. We follow the same ap-
proach as with BHMM for optimising, training,
and testing the model.8 Note that as LDAHMM
operates with documents, the training data is par-
titioned into documents, and each test sentence is
treated as a document.
The results are summarised in Table 2 (column:
“LDAHMM”). The result shows that LDAHMM
underperforms in comparison to BHMM, indicat-
ing that the incorporation of LDA did not improve
the model. To understand the impact of LDA
alone, we repeat the experiments using LDA and
find that it performs very poorly. Results are sum-
marised in Table 2 (column: LDA). We suspect
that the low performance of LDA and LDAHMM
is due to the small context window of the test doc-
uments. The LDA part is unable to generate any
meaningful topic mixtures, as there is insufficient
context.
</bodyText>
<subsectionHeader confidence="0.523409">
3.4 Two-Tier BHMM
</subsectionHeader>
<bodyText confidence="0.999860416666667">
BHMM uses (latent) word classes to drive word
generation. Exploring a richer structure, we intro-
duce another layer of latent variables on top of the
word classes. This second layer can be interpreted
as phrase classes. The idea behind this model is
to use these phrase classes to drive word class and
word generation. An illustration of its word struc-
ture is given in Figure 1(c).
We use collapsed Gibbs sampling for perform-
ing inference. We sample the tier-1 state s and tier-
2 state t separately, and the sampling equations are
given as follows:
</bodyText>
<footnote confidence="0.836951">
8The final optimised parameters are: K = 100, S = 80,
α = 0.1, β = 0.0001, 7 = 0.1, and 6 = 0.01.
</footnote>
<bodyText confidence="0.999963117647059">
where si, ti are the tier-1 and tier-2 state indices;
s, t, w are the assignments for all tier-1 states,
tier-2 states and words, respectively (subscript −i
means the current assignment is excluded); α, γ
and 6 are the Dirichlet hyper-parameters; S =
number of tier-1 states; T = number of tier-2
states; W = vocabulary size; and #(x, [y], [z]) are
the multinomial counts.
We follow the same process for optimising,
training, and testing the model, and we summarise
the results in Table 2 (column: “2T”).9 We see an
improved correlation relative to BHMM (BHMM
best: 0.45, Two-Tier BHMM best: 0.50). In fact
it has the best performance of all models thus far.
This is encouraging, as it implies that the introduc-
tion of the phrase layer produces a more optimal
structure for representing acceptability.
</bodyText>
<subsectionHeader confidence="0.988917">
3.5 Bayesian Chunker
</subsectionHeader>
<bodyText confidence="0.9998805">
Goldwater et al. (2009) propose a Bayesian ap-
proach to segment words in speech streams. New-
man et al. (2012) extend the approach to segment
phrases – i.e. multiword units – in sentences, and
they apply it to the task of index term identification
and keyphrase extraction.
The core machinery of the methodology is
driven by the Dirichlet Process, where segments
(words in Goldwater et al. (2009) or phrases in
Newman et al. (2012)) are retrieved from a cache,
</bodyText>
<footnote confidence="0.535747">
9The optimised parameters: S = 100, T = 60, α = 1.0,
</footnote>
<equation confidence="0.988812111111111">
7 = 1.0, 6 = 0.01.
1622
P(ti|t−i, s, w, α, 7, 6) ∝ #(ti−1, si−1, ti) + α ×
#(ti−1, si−1) + Tα
#(ti, si, ti+1) + α
×
#(ti, si) + Tα ;
#(ti, si−1) + S7
+ &apos;y
P(si |s−i, t, w, α, 7, 6) ∝ #(ti, si−1, si) ×
#(ti, si−1) + S7
#(ti+1, si, si+1) + 7
× ×
#(ti, si) + Tα #(ti+1, si) + S7
#(si, wi) + 6
#(si) + W6
#(ti, si−1, si) + 7
#(ti, si, ti+1) + α
</equation>
<bodyText confidence="0.999165736842105">
where H− is all of the structure shared by both
hypotheses; wxy is a multiword unit consisting of
wx and wy; n is the number of multiword tokens;
α is the concentration parameter of the Dirichlet
process; n(w) is the count of multiword w; and
P0(w) is the probability of generating a novel w.
i.e. P0(wxy) = p#(1 − p#)P(wx)P(wy).
We extend their methodology to segment word
classes to do unsupervised chunking, motivated
by the idea that a well-formed sentence contains
predictable patterns of word class chunks. We
extend the sampling process to incorporate tran-
sitions between chunks. Given the word classes
“cwcxcycz”, at the boundary point between word
class cx and cy, the hypothesis H0 to not gener-
ate a boundary (therefore producing a single chunk
cxy), and the hypothesis H1 to generate a bound-
ary (therefore producing two chunks cx and cy),
are computed as follows:
</bodyText>
<equation confidence="0.9996003">
#(cw, cxy) + β (n(cxy)n««0(cxy) 1
#(cw) + mβ /1 x
#(cxy, cz) + β (n(cz )nα«0(cz) 1
#(cxy) + mβ /1
×
#(cw) + mβ
#(cx, cy) + β (n(cy)nα«0(cy)1
#(cx) + mβ \JJ x
#(cy, cz) + β(n(cz)nαα0(cz) 1
#(cy) + mβ /1
</equation>
<bodyText confidence="0.999993333333333">
where m = number of chunk types; n = num-
ber of chunk tokens; Q is the Dirichlet hyper-
parameter for the chunk transition multinomials;
and #(x, [y]) is the count for the chunk transition
multinomials.
As the model takes word classes as input, we
use the word classes induced by two-tier BHMM.
We follow the same process for optimising, train-
ing and testing the model.10 The results are sum-
marised in Table 2 (column: “Chunker”). The
model produces a moderate correlation, perform-
ing on par with the lexical 4-gram model.
</bodyText>
<subsectionHeader confidence="0.966267">
3.6 Recurrent Neural Network Language
Model
</subsectionHeader>
<bodyText confidence="0.991678208333334">
In recent years, we have seen a resurgence in the
use of neural networks for deep machine learn-
ing and NLP. Rather than designing structures or
handcrafting features that seem intuitive for a task,
deep learning introduces an entirely general ar-
chitecture for machine learning. It has yielded
some impressive results for NLP tasks: automatic
speech recognition, parsing, part of speech tag-
ging, and named entity recognition, to name a few
(Seide et al., 2011; Mikolov et al., 2011a; Col-
lobert et al., 2011; Chen and Manning, 2014).
We experiment with a recurrent neural net-
work language model (RNNLM: (Elman, 1998;
Mikolov, 2012)) for our task. We choose this
model because it has an internal state that keeps
track of previously observed sequences, which is
well suited for natural language problems. To
train the model, we use stochastic gradient descent
combined with back propagation through time.
RNNLM is optimised to reduce the error in pre-
dicting the following word, based on the current
word and its history (represented in a compressed
dimension in the size of the hidden layer). Full
details of RNNLM can be found in the original
papers (Mikolov et al., 2011b; Mikolov, 2012).11
We experimented with some of the parameters
of RNNLM using BNC-10M and found that most
parameters have an intuitive setting. Its perfor-
mance largely depends on the number of neurons
in the hidden layer. Mikolov (2012) introduced a
variant of RNNLM that does joint learning with
a Maximum Entropy model which learns direct
connections of N-gram features. We found that
although there are advantages to using the ME
model, the benefits disappear as we increase the
number of neurons in the hidden layer. We saw
optimal performance at 600 neurons, without us-
ing the ME model. All our results are based
or newly generated. Using Gibbs sampling for in-
ference, the sampler considers one boundary point
at a time, and computes the probability of two hy-
potheses: H0, for not generating a boundary; and
H1, for generating a boundary.
Borrowing the notation of Newman et al.
(2012), given p# is the probability of generating a
segment boundary, at the boundary point between
words wx and wy, the probability of the hypothe-
ses is computed as follows:
</bodyText>
<footnote confidence="0.95347475">
10The optimised parameters are: α = 0.1, β = 0.001,
p# = 0.5.
p11 We use Mikolov’s implementation of RNNLM for our
1623 eriment: ht tp : / / rnnlm . org/.
</footnote>
<equation confidence="0.996615416666667">
P(H0|H−) = n(wxy) + αP0(wxy)
;n + α
P(H1|H−) = n(wx) + αP0(wx)
n + α
n(wy) + αP0(wy)
×
n + 1 + α
P(H0|H−) =
;
P(H1|H−) =
(n(cx)+αP0(cx) �
#(cw, cx) + β n+α
</equation>
<table confidence="0.9988565">
Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLM
LogProb 0.31 0.36 0.38 0.32 0.33 0.35 0.42 0.44
Mean LP 0.28 0.36 0.37 0.28 0.28 0.35 0.45 0.46
Norm LP (Div) 0.34 0.41 0.41 0.44 0.42 0.49 0.43 0.55
Norm LP (Sub) 0.11 0.20 0.22 0.32 0.32 0.44 0.14 0.33
SLOR 0.35 0.41 0.41 0.46 0.44 0.50 0.41 0.57
</table>
<tableCaption confidence="0.9888305">
Table 3: Pearson’s r of acceptability measure and mean sentence rating for all experimented models in ENWIKI. Boldface
indicates the best performing measure.
</tableCaption>
<bodyText confidence="0.9571558">
on the original RNNLM with 600 neurons in the
hidden layer, trained on BNC-100M (Table 2 col-
umn: “RNNLM”).12 We see that RNNLM per-
forms very well. It outperforms the other models,
achieving a correlation of 0.53.
</bodyText>
<subsectionHeader confidence="0.896933">
3.7 PCFG Parser (Supervised)
</subsectionHeader>
<bodyText confidence="0.9999378">
Although we are interested in unsupervised mod-
els, for purposes of comparison we experimented
with a constituent PCFG parser for our task.
We use the Stanford Parser (Klein and Man-
ning, 2003a; Klein and Manning, 2003b), and
tested both the unlexicalised and lexicalised PCFG
parser with the supplied model. To compute the
log probability of test sentences, we experimented
with both top-1 and top-1K best parses.
We found that the unlexicalised variant gives
better performance, but we saw little difference
between using the top-1 and the top-1K best parses
for computing log probability. In Table 2 (col-
umn: “PCFG”), we report results for the unlexi-
calised variant based on the top-1 best parse. The
supervised PCFG parser performs poorly. This is
not surprising, given that the parser is trained on
a different domain.13 Moreover, the log probabil-
ity scores are not true probabilities, but arbitrary
values used for ranking the parse trees.
</bodyText>
<sectionHeader confidence="0.969088" genericHeader="method">
4 English Wikipedia
</sectionHeader>
<bodyText confidence="0.999829">
For the BNC domain we saw that SLOR and Norm
LP (Div) give the best acceptability measures,
and that BHMM, two-tier BHMM and RNNLM
are the best performing models. These findings
are limited to a particular dataset. To better un-
derstand if these observations generalise to an-
other domain, we developed an English Wikipedia
dataset (ENWIKI), following the same process de-
scribed in Lau et al. (2014) to generate test sen-
</bodyText>
<footnote confidence="0.8462446">
12Other parameter values of RNNLM: number of classes
= 550; bptt = 4; bptt-block = 100.
13The Stanford English model is trained on the parse tree
hand annotated WSJ (section 1–21), Genia, and a few other
datasets.
</footnote>
<bodyText confidence="0.99979165">
tences through round-trip machine translation ,and
to collect annotations via Mechanical Turk.14 As
before, we follow the same procedures described
in Section 3 to optimise, train, and test all models
(excluding LDA and PCFG). The Pearson corre-
lations with mean AMT annotations are presented
in Table 3.
We identify similar trends in ENWIKI: Norm LP
(Div) and SLOR are the best acceptability mea-
sures, and we see improvements when we use
a richer structure in the language model (two-
tier BHMM&gt;BHMM&gt;N-grams). Interestingly,
LDAHMM performs much better in this domain
(possibly due to increased coherence in the docu-
ment structure of ENWIKI). RNNLM has the best
performance of all models, surpassing two-tier
BHMM by a substantial margin. Overall, the cor-
relation values are very similar across the two do-
mains, indicating that the models and the accept-
ability measures are robust.
</bodyText>
<sectionHeader confidence="0.52934" genericHeader="method">
5 Comparison with a Supervised System
</sectionHeader>
<bodyText confidence="0.999958133333333">
Although not a focus of this paper, supervised
learning can further improve the correlation per-
formance of our models. The acceptability mea-
sures can be combined in a supervised context. We
experimented with this approach in a support vec-
tor regression model (with an RBF kernel). We
achieved a correlation performance of 0.64 in BNC
and of 0.69 in ENWIKI.15
Heilman et al. (2014) propose a system for pre-
dicting acceptability. They built a dataset con-
sisting of sentences from essays written by non-
native speakers for an ESL test. Acceptability
ratings were judged by the authors, and through
crowdsourcing (henceforth we refer to this anno-
tated data set as the GUG data set). They applied
</bodyText>
<footnote confidence="0.9973655">
14Both the BNC and the English Wikipedia datasets are
available at http://www.dcs.kcl.ac.uk/staff/
lappin/smog/?page=research.
15We use only the unsupervised models, excluding the su-
pervised PCFG parser. The models are trained and tested us-
1624 10-fold cross validation.
</footnote>
<bodyText confidence="0.999928282608696">
a 4-category ordinal scale for rating the sentences.
To predict sentence acceptability, they employ a
linear regression model that draws features from
spelling errors, an N-gram model, precision gram-
mar parsers, and the Stanford PCFG parser.
To better understand the performance of our
system compared to other acceptability prediction
systems, we evaluated our methodology against
that of Heilman et al. (2014) on the GUG dataset.
We preprocessed the GUG dataset minimally. We
removed 15 short sentences that have less than 5
words, lowercased all words, and tokenised the
sentences using OpenNLP. This yields 2255 sen-
tences for the training and development subset,
and 749 sentences for the test set. Using the out-
put – i.e. the acceptability measures – of our un-
supervised models (trained on BNC) as features,
we trained an SVR model using GUG training and
development subsets to predict acceptability rat-
ings on GUG test sentences. We applied the de-
fault SVR parameters, and so it was not necessary
to use the development subset separately to opti-
mise the parameters. For evaluation we computed
the correlation of the predicted ratings and mean
human ratings.
We present a comparison of results in Table 4.
We first tested the unsupervised models, with the
best correlation of 0.472 produced by the lexical
4-gram model using the Norm LP (Div) measure.
Combining the models in SVR, we achieve a cor-
relation of 0.603.
Heilman et al. (2014) note that spelling is one
of the important features in their regression model,
as the dataset often contains spelling mistakes. We
borrowed this feature, computed as the proportion
of misspelled words, and incorporated into our
model. It produced a significant improvement in
the correlation (0.636), a performance almost on
par with that of Heilman et al. (2014).16
Our results demonstrate the robustness and
portability of our system in a new domain. Our
SVR model requires significantly less supervision
than that of Heilman et al. (2014), which relies on
precision and constituent parsers. Moreover, our
methodology provides a completely unsupervised
alternative that requires only raw text for training.
</bodyText>
<footnote confidence="0.9957025">
16We use PyEnchant for spellcheck: http:
//pythonhosted.org/pyenchant/. Note that
we also tried adding the spelling feature to our original
BNC derived dataset, but it yielded no improvement in the
correlation. This is not surprising, given that it contains few
spelling errors.
</footnote>
<table confidence="0.9656188">
System Pearson’s r
Heilman et al. (2014) 0.644
Unsupervised Best 0.472
SVR: All Models 0.603
SVR: All Models+Spell 0.636
</table>
<tableCaption confidence="0.9435465">
Table 4: A comparison of results of our system and Heil-
man et al. (2014) on GUG.
</tableCaption>
<sectionHeader confidence="0.8111815" genericHeader="method">
6 Influence of Sentence Length and
Lexical Frequency
</sectionHeader>
<bodyText confidence="0.9998005625">
Our primary motivation in doing this research has
been to use acceptability predictions to explore
whether acceptability can be represented through
probability information. Unlike probability, ac-
ceptability is generally not influenced by sentence
length or low frequency words.
The acceptability measures we apply normalise
sentence length and word frequency. To evaluate
their effectiveness, we computed two correlations
in the BNC domain: (1) acceptability measure vs.
sentence length (Table 5); and (2) acceptability
measure vs. sentence minimum word frequency
(Table 6).17
For comparison we additionally computed the
correlation of these factors with human ratings.
The correlations are: +0.13 with sentence length;
and +0.07 with minimum word frequency. These
observations confirm the view that acceptability is
not affected by these two factors.
Table 5 shows that although LogProb yields a
strong negative correlation with sentence length,
Mean LP, Norm LP (Div) and SLOR all produce
low correlations. The only exception is Norm LP
(Sub), which still has a significant correlation with
sentence length.
In Table 6 we see some degree of correlation in
LogProb with the minimum word frequency, but it
is relatively small. In general, SLOR is the scoring
function that most effectively normalises word fre-
quency, producing low correlation for most mod-
els. Norm LP (Div) also does very well, for all
models except N-grams.
</bodyText>
<sectionHeader confidence="0.99961" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.906218333333333">
In principle, the upper bound of the correlation be-
tween our models’ predicted acceptability values
and mean human ratings is 1.0. But no individual
human annotator will match mean judgements per-
fectly. It is more plausible to measure our models’
17We use BNC-100M for computing word frequency.
</bodyText>
<page confidence="0.801056">
1625
</page>
<table confidence="0.999714333333333">
Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLM
LogProb −0.89 −0.80 −0.84 −0.85 −0.86 −0.86 −0.83 −0.86
Mean LP −0.16 −0.08 −0.18 +0.03 +0.05 −0.02 −0.01 +0.08
Norm LP (Div) −0.15 −0.07 −0.17 +0.10 +0.15 ±0.00 ±0.00 +0.14
Norm LP (Sub) +0.69 +0.63 +0.54 +0.46 +0.54 +0.11 +0.70 +0.62
SLOR −0.07 +0.04 −0.03 +0.12 +0.17 +0.01 ±0.00 +0.17
</table>
<tableCaption confidence="0.983754">
Table 5: Pearson’s r of acceptability measure and sentence length for all models in BNC. For comparison the correlation
</tableCaption>
<table confidence="0.999057">
with human ratings is +0.13.
Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLM
LogProb +0.27 +0.27 +0.27 +0.27 +0.27 +0.27 +0.19 +0.28
Mean LP +0.30 +0.28 +0.27 +0.29 +0.28 +0.29 +0.08 +0.26
Norm LP (Div) +0.24 +0.23 +0.21 +0.11 +0.06 +0.12 +0.06 +0.11
Norm LP (Sub) −0.04 −0.03 −0.03 −0.03 −0.09 +0.05 −0.13 −0.08
SLOR +0.16 +0.14 +0.12 +0.06 ±0.00 +0.10 +0.04 +0.03
</table>
<tableCaption confidence="0.987317">
Table 6: Pearson’s r of acceptability measure and sentence minimum word frequency for all models in BNC. The correlation
with the human ratings is +0.07.
</tableCaption>
<bodyText confidence="0.999925413793103">
rate of success against an estimated level of indi-
vidual human performance. We do this by mim-
icking an arbitrary speaker, and testing the corre-
lation of this construct’s judgements with the mean
scores of the annotators.
We simulate such an individual human judge by
randomly selecting a single annotator rating for
each sentence, and computing the Pearson corre-
lation between these judgements and the mean rat-
ings for the rest of the annotators (one vs the rest)
in our test sets. We ran this experiment 50 times
for each test set to reduce sample variation, pro-
ducing a mean correlation of 0.67 for BNC and
0.74 for ENWIKI. For comparison, the best unsu-
pervised model (RNNLM) achieves a correlation
of 0.53 in BNC and 0.57 in ENWIKI (Section 3).
The supervised model (SVR) produces a correla-
tion of 0.64 in BNC and 0.69 in ENWIKI (Section 5).
Although there is still room for improvement for
the unsupervised methodology, it is encouraging
to note that the supervised variant predicts accept-
ability at a level that approaches estimated human
performance.
To test the robustness of our methodology
across languages, we are currently developing
datasets in other languages, based on Wikipedia.
Our preliminary results show similar performance
to that which we report here for ENWIKI, suggesting
that these results hold across languages.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999951928571429">
We developed a methodology for using unsuper-
vised language models to predict human accept-
ability judgements. We experimented with a va-
riety of unsupervised models. To map proba-
bility to acceptability we proposed a set of ac-
ceptability measures to normalise sentence length
and lexical frequency. We achieved encourag-
ing results across two datasets constructed through
round trip machine translation, and the methodol-
ogy is highly portable to other domains and lan-
guages. This research has potential implications
for our understanding of human language acquisi-
tion and the way in which linguistic knowledge is
represented.
</bodyText>
<sectionHeader confidence="0.993808" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998655947368421">
The research reported here was done as part of the Statisticsal
Models of Grammar (SMOG) project at King’s College Lon-
don(www.dcs.kcl.ac.uk/staff/lappin/smog/),
funded by grant ES/J022969/1 from the Economic and So-
cial Research Council of the UK.
We are grateful to Douglas Saddy and Garry Smith at the
Centre for Integrative Neuroscience and Neurodynamics at
the University of Reading for generously giving us access to
their computing cluster, and for much helpful technical sup-
port. We thank J. David Lappin for invaluable assistance in
organising our AMT HITS. We presented part of the work
discussed here to CL/NLP, cognitive science, and machine
learning colloquia at Chalmers University of Technology,
University of Gothenburg, University of Sheffield, University
of Edinburgh, The Weizmann Institute of Science, University
of Toronto, MIT, and the ILLC at the University of Amster-
dam. We very much appreciate the comments and criticisms
that we received from these audiences, which have guided us
in our research.
</bodyText>
<page confidence="0.941265">
1626
</page>
<bodyText confidence="0.99495675">
Frank Keller. 2001. Gradience in Grammar: Exper-
imental and Computational Aspects of Degrees of
Grammaticality. Ph.D. thesis, The University of Ed-
inburgh.
</bodyText>
<sectionHeader confidence="0.944481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998199688073395">
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. Journal of Machine Learning Research,
3:993–1022.
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Ox-
ford University Computing Services on behalf of the
BNC Consortium.
D. Chen and C. Manning. 2014. A fast and accu-
rate dependency parser using neural networks. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014), pages 740–750, Doha, Qatar.
Alexander Clark, Gianluca Giorgolo, and Shalom Lap-
pin. 2013. Statistical representation of grammat-
icality judgements: The limits of n-gram models.
In Proceedings of the ACL Workshop on Cognitive
Modelling and Computational Linguistics, Sofia,
Bulgaria.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493–2537.
J. Elman. 1998. Generalization, simple recurrent net-
works, and the emergence of structure. In M. Gerns-
bacher and S. Derry, editors, Proceedings of the 20th
Annual Conference of the Cognitive Science Society.
Lawrence Erlbaum Associates, Mahway, NJ.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2007), pages 744–751, Prague, Czech Repub-
lic.
S. Goldwater, T. Griffiths, and M. Johnson. 2009. A
Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition, 112:21–
54.
J.T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech &amp; Language,
15(4):403–434.
Thomas L. Griffiths, Mark Steyvers, David M. Blei,
and Joshua B. Tenenbaum. 2004. Integrating top-
ics and syntax. In Advances in Neural Information
Processing Systems 17, pages 537–544. Vancouver,
Canada.
Michael Heilman, Aoife Cahill, Nitin Madnani,
Melissa Lopez, Matthew Mulholland, and Joel
Tetreault. 2014. Predicting grammaticality on an
ordinal scale. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2014), Volume 2: Short Papers, pages
174–180, Baltimore, Maryland.
D. Klein and C. Manning. 2003a. Accurate unlex-
icalized parsing. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL 2003), pages 423–430, Sapporo,
Japan.
D. Klein and C. Manning. 2003b. Fast exact inference
with a factored model for natural language parsing.
In Advances in Neural Information Processing Sys-
tems 15 (NIPS-03), pages 3–10, Whistler, Canada.
J.H. Lau, P. Cook, D. McCarthy, D. Newman, and
T. Baldwin. 2012. Word sense induction for novel
sense detection. In Proceedings of the 13th Con-
ference of the EACL (EACL 2012), pages 591–601,
Avignon, France.
J.H. Lau, A. Clark, and S. Lappin. 2014. Measuring
gradience in speakers’ grammaticality judgements.
In Proceedings of the 36th Annual Conference of the
Cognitive Science Society, pages 821–826, Quebec
City, Canada.
T. Mikolov, A. Deoras, S. Kombrink, L. Burget, and
J. `Eernock´y. 2011a. Empirical evaluation and com-
bination of advanced language modeling techniques.
In Proceedings of the 12th Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH 2011), pages 605–608, Flo-
rence, Italy.
T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and
J. `Eernock´y. 2011b. Rnnlm -recurrent neural net-
work language modeling toolkit. In IEEEAutomatic
Speech Recognition and Understanding Workshop,
Hawaii, US.
T. Mikolov. 2012. Statistical Language Models based
on Neural Networks. Ph.D. thesis, Brno University
of Technology.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmenta-
tion for index term identification and keyphrase ex-
traction. In Proceedings of the 24th International
Conference on Computational Linguistics (COLING
2012), pages 2077–2092, Mumbai, India.
A. Pauls and D. Klein. 2012. Large-scale syntactic
language modeling with treelets. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 959–968. Jeju, Korea.
F. Seide, G. Li, and D. Yu. 2011. Conversational
speech transcription using context-dependent deep
neural networks. In Proceedings of the 12th Annual
Conference of the International Speech Communica-
tion Association (INTERSPEECH 2011), Florence,
Italy.
1627
J. Sprouse and D. Almeida. 2012. Assessing the relia-
bility of textbook data in syntax: Adger’s core syn-
tax. Journal of Linguistics, 48(3):609–652.
J. Sprouse. 2007. Continuous acceptability, categor-
ical grammaticality, and experimental syntax. Bi-
olinguistics, 1:123–134.
</reference>
<page confidence="0.99346">
1628
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341348">
<title confidence="0.997991">Unsupervised Prediction of Acceptability Judgements</title>
<author confidence="0.977675">Jey Han Lau</author>
<author confidence="0.977675">Alexander Clark</author>
<author confidence="0.977675">Shalom</author>
<affiliation confidence="0.983506">King’s College London</affiliation>
<abstract confidence="0.996982210526316">2014), there is limited research on the relationship between acceptability and probability. In this paper, we consider the the task of unsupervised prediction of acceptability. Speakers have robust intuitions about acceptability, and acceptability has been consistently rated on various scales (Sprouse and Almeida, 2012). The acceptability of a sentence appears to be relatively unaffected by its length (within certain bounds), or the frequency of its words, properties that we have confirmed experimentally. By contrast sentence probability does depend on these factors. To filter the effects of sentence length and word frequency, we devise normalising functions to map the probability of a sentence (inferred by our unsupervised language models) to an acceptability score. Keller (2001) and Lau et al. (2014) present evidence that acceptability exhibits gradience. Accordingly, we treat acceptability as a continuous variable here. We train a variety of unsupervised models for the acceptability prediction task, and we assess the performance of these models by measuring the correlation between their normalised acceptability scores and the mean crowdsourced acceptability judgements on a set of test sentences. There are a number of NLP tasks to which our work can be fruitfully applied. It can be used to evaluate the fluency of the output for machine translation and other language generation systems. It could also contribute to automatic essay scoring, and to second language tutorial systems. There are several reasons to favour unsupervised models. From an engineering perspective, unsupervised models offer greater portability to other domains and languages. Our methodology takes only unannotated text as input. Extending</abstract>
<date confidence="0.468298">1618</date>
<abstract confidence="0.994507">In this paper we present the task of unsupervised prediction of speakers’ acceptability judgements. We use a test set generated from the British National Corcontaining both grammatical sentences and sentences containing a variety of syntactic infelicities introduced by round trip machine translation. This set was annotated for acceptability judgements through crowd sourcing. We trained a variety of unsupervised language modon the original and tested them to see the extent to which they could predict mean speakers’ judgements on the test set. To map probability to acceptability, we experimented with several normalisation functions to neutralise the effects of sentence length and word frequencies. We found encouraging results with the unsupervised models predicting acceptability across two different datasets. Our methodology is highly portable to other domains and languages, and the approach has potential implications for the representation and the acquisition of linguistic knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="15487" citStr="Blei et al., 2003" startWordPosition="2441" endWordPosition="2444">on of a layer of (latent) word classes produces a better structure for modelling acceptability. 3.3 LDAHMM and LDA To better understand the role of semantics in acceptability, we experimented with LDAHMM (Griffiths et al., 2004), a model that combines syntactic and semantic dependencies between words. The generative method of LDAHMM to generate a word in a document is to first decide whether to generate a syntactic state or a semantic state for the word. For the former, follow the HMM process to generate a state, and generate the word based on the chosen state. For the latter, follow the LDA (Blei et al., 2003) process to generate a topic based on the document’s topic mixture, and generate the word based on the chosen topic. We use a second order HMM for the HMM part and Collapsed Gibbs sampling for performing inference. LDAHMM has 4 sets of multinomials: the HMM multinomials (state transition and word emission) and the LDA multinomials (documenttopic and topic-word). LDAHMM has 6 parameters to optimise: (1) K the number of topics; (2) S the number of syntactic states (semantic state has only 1 state, designated as state 0); (3) α, the Dirichlet hyper-parameter for document-topic multinomials; (4) Q</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNC Consortium</author>
</authors>
<date>2007</date>
<booktitle>The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</booktitle>
<contexts>
<context position="6909" citStr="Consortium, 2007" startWordPosition="1041" endWordPosition="1042">upervised system in the acceptability prediction task in Section 5. We look more closely at the influence of sentence length and lexical frequency in Section 6, and we show that the normalising functions succeed in neutralising these effects. Finally, we discuss the implications of our results, and draw conclusions from them in Section 7 and Section 8. 2 Dataset and Methodology For our experiments, we require a collection of sentences that exhibit varying degrees of grammatical well-formedness. We use the dataset that we discuss in Lau et al. (2014). We translated British National Corpus (BNC Consortium, 2007) English sentences to four other languages – Norwegian, Spanish, Japanese and Chinese – and then back to English using Google Translate. To collect human judgements of acceptability for the sentences, we used Amazon Mechanical Turk. A total of 2,500 sentences were annotated. Three modes of presentation were used for rating a sentence: (1) binary with two options (unnatural vs. natural); (2) four options (extremely unnatural, somewhat unnatural, somewhat natural and extremely natural); and (3) a sliding scale with two extremes (extremely unnatural and extremely natural). To aggregate the rating</context>
</contexts>
<marker>Consortium, 2007</marker>
<rawString>BNC Consortium. 2007. The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services on behalf of the BNC Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>C Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>740--750</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="22324" citStr="Chen and Manning, 2014" startWordPosition="3650" endWordPosition="3653">performing on par with the lexical 4-gram model. 3.6 Recurrent Neural Network Language Model In recent years, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in the size of the hidden layer). Full details of RNNLM can be found in th</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>D. Chen and C. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014), pages 740–750, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Gianluca Giorgolo</author>
<author>Shalom Lappin</author>
</authors>
<title>Statistical representation of grammaticality judgements: The limits of n-gram models.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL Workshop on Cognitive Modelling and Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9708" citStr="Clark et al., 2013" startWordPosition="1487" endWordPosition="1490">e converted all words to lower case. To address out of vocabulary (OOV) words that are seen in the test sentences but not in the training data, we adopt the Berkeley Parser approach, where we replace low frequency or OOV words using the UNK signature. We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).3 3 Unsupervised Models 3.1 Lexical N-gram Model Lexical N-gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N-gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram best: 0.34; 3-gram best:</context>
</contexts>
<marker>Clark, Giorgolo, Lappin, 2013</marker>
<rawString>Alexander Clark, Gianluca Giorgolo, and Shalom Lappin. 2013. Statistical representation of grammaticality judgements: The limits of n-gram models. In Proceedings of the ACL Workshop on Cognitive Modelling and Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="22299" citStr="Collobert et al., 2011" startWordPosition="3645" endWordPosition="3649">a moderate correlation, performing on par with the lexical 4-gram model. 3.6 Recurrent Neural Network Language Model In recent years, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in the size of the hidden layer). Full details of</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Elman</author>
</authors>
<title>Generalization, simple recurrent networks, and the emergence of structure.</title>
<date>1998</date>
<booktitle>Proceedings of the 20th Annual Conference of the Cognitive Science Society. Lawrence Erlbaum Associates,</booktitle>
<editor>In M. Gernsbacher and S. Derry, editors,</editor>
<location>Mahway, NJ.</location>
<contexts>
<context position="22407" citStr="Elman, 1998" startWordPosition="3665" endWordPosition="3666"> recent years, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in the size of the hidden layer). Full details of RNNLM can be found in the original papers (Mikolov et al., 2011b; Mikolov, 2012).11 We experimented with so</context>
</contexts>
<marker>Elman, 1998</marker>
<rawString>J. Elman. 1998. Generalization, simple recurrent networks, and the emergence of structure. In M. Gernsbacher and S. Derry, editors, Proceedings of the 20th Annual Conference of the Cognitive Science Society. Lawrence Erlbaum Associates, Mahway, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>744--751</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11773" citStr="Goldwater and Griffiths, 2007" startWordPosition="1827" endWordPosition="1830">hat the negative sign in Norm LP (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. of the lexical N-gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N-grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 Mean LP Norm L</context>
<context position="13140" citStr="Goldwater and Griffiths (2007)" startWordPosition="2061" endWordPosition="2064"> ti−1 ti ti+1 si−2 si−1 si si+1 wi−2 wi−1 wi wi+1 Figure 1: A comparison of word structures for 3-gram, BHMM and Two-Tier BHMM. w = observed words; s = tier-1 latent states (“word classes”); t = tier-2 latent states (“phrase classes”). For comparison, the structure of a lexical 3-gram model is given in Figure 1(a). Goldwater and Griffiths (2007) propose a Bayesian approach for learning the HMM structure. The authors found that their Bayesian HMM (BHMM) significantly outperforms a HMM trained with Maximum Likelihood Estimation in unsupervised part-of-speech tagging. We adopt the methodology of Goldwater and Griffiths (2007), and train a 2nd order BHMM for our task, using collapsed Gibbs sampling for inference. BHMM has two sets of multinomials: the state transition multinomials and the word emission multinomials. To generalise the state transition probabilities for start probabilities, we use dummy words/states for empty preceding words/states. BHMM has 3 parameters: (1) S, the number of latent states; (2) &apos;y, the Dirichlet hyper-parameter for the state transition multinomials; and (3) δ, the Dirichlet hyper-parameter for the word emission multinomials. We assume symmetric Dirichlet priors for the hyper-paramete</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), pages 744–751, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
<author>M Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<pages>54</pages>
<contexts>
<context position="11797" citStr="Goldwater et al., 2009" startWordPosition="1831" endWordPosition="1835">P (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. of the lexical N-gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N-grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 Mean LP Norm LP (Div) SLOR wi−2 wi−1 w</context>
<context position="19323" citStr="Goldwater et al. (2009)" startWordPosition="3096" endWordPosition="3099">chlet hyper-parameters; S = number of tier-1 states; T = number of tier-2 states; W = vocabulary size; and #(x, [y], [z]) are the multinomial counts. We follow the same process for optimising, training, and testing the model, and we summarise the results in Table 2 (column: “2T”).9 We see an improved correlation relative to BHMM (BHMM best: 0.45, Two-Tier BHMM best: 0.50). In fact it has the best performance of all models thus far. This is encouraging, as it implies that the introduction of the phrase layer produces a more optimal structure for representing acceptability. 3.5 Bayesian Chunker Goldwater et al. (2009) propose a Bayesian approach to segment words in speech streams. Newman et al. (2012) extend the approach to segment phrases – i.e. multiword units – in sentences, and they apply it to the task of index term identification and keyphrase extraction. The core machinery of the methodology is driven by the Dirichlet Process, where segments (words in Goldwater et al. (2009) or phrases in Newman et al. (2012)) are retrieved from a cache, 9The optimised parameters: S = 100, T = 60, α = 1.0, 7 = 1.0, 6 = 0.01. 1622 P(ti|t−i, s, w, α, 7, 6) ∝ #(ti−1, si−1, ti) + α × #(ti−1, si−1) + Tα #(ti, si, ti+1) +</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>S. Goldwater, T. Griffiths, and M. Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112:21– 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="9802" citStr="Goodman, 2001" startWordPosition="1503" endWordPosition="1504"> test sentences but not in the training data, we adopt the Berkeley Parser approach, where we replace low frequency or OOV words using the UNK signature. We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).3 3 Unsupervised Models 3.1 Lexical N-gram Model Lexical N-gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N-gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram best: 0.34; 3-gram best: 0.42; 4-gram best: 0.42). This result implies that increasing the context window 3Low frequen</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J.T. Goodman. 2001. A bit of progress in language modeling. Computer Speech &amp; Language, 15(4):403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>537--544</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="15097" citStr="Griffiths et al., 2004" startWordPosition="2370" endWordPosition="2373">various orders of magnitudes. 6The optimised parameters are: S = 100, -y = 1.0 and 6 = 0.01. sures using the MAP estimated states.7 The final probabilities are computed as a harmonic mean of probabilities over the 50 samples. We summarise the correlation results in Table 2 (column: “BHMM”). Compared to the N-gram models, we see an improvement in the correlation, indicating that the introduction of a layer of (latent) word classes produces a better structure for modelling acceptability. 3.3 LDAHMM and LDA To better understand the role of semantics in acceptability, we experimented with LDAHMM (Griffiths et al., 2004), a model that combines syntactic and semantic dependencies between words. The generative method of LDAHMM to generate a word in a document is to first decide whether to generate a syntactic state or a semantic state for the word. For the former, follow the HMM process to generate a state, and generate the word based on the chosen state. For the latter, follow the LDA (Blei et al., 2003) process to generate a topic based on the document’s topic mixture, and generate the word based on the chosen topic. We use a second order HMM for the HMM part and Collapsed Gibbs sampling for performing infere</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2004</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, David M. Blei, and Joshua B. Tenenbaum. 2004. Integrating topics and syntax. In Advances in Neural Information Processing Systems 17, pages 537–544. Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Melissa Lopez</author>
<author>Matthew Mulholland</author>
<author>Joel Tetreault</author>
</authors>
<title>Predicting grammaticality on an ordinal scale.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Volume 2: Short Papers,</booktitle>
<pages>174--180</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="9688" citStr="Heilman et al., 2014" startWordPosition="1482" endWordPosition="1486">s using OpenNLP, and we converted all words to lower case. To address out of vocabulary (OOV) words that are seen in the test sentences but not in the training data, we adopt the Berkeley Parser approach, where we replace low frequency or OOV words using the UNK signature. We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).3 3 Unsupervised Models 3.1 Lexical N-gram Model Lexical N-gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N-gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram best</context>
<context position="27936" citStr="Heilman et al. (2014)" startWordPosition="4599" endWordPosition="4602">l models, surpassing two-tier BHMM by a substantial margin. Overall, the correlation values are very similar across the two domains, indicating that the models and the acceptability measures are robust. 5 Comparison with a Supervised System Although not a focus of this paper, supervised learning can further improve the correlation performance of our models. The acceptability measures can be combined in a supervised context. We experimented with this approach in a support vector regression model (with an RBF kernel). We achieved a correlation performance of 0.64 in BNC and of 0.69 in ENWIKI.15 Heilman et al. (2014) propose a system for predicting acceptability. They built a dataset consisting of sentences from essays written by nonnative speakers for an ESL test. Acceptability ratings were judged by the authors, and through crowdsourcing (henceforth we refer to this annotated data set as the GUG data set). They applied 14Both the BNC and the English Wikipedia datasets are available at http://www.dcs.kcl.ac.uk/staff/ lappin/smog/?page=research. 15We use only the unsupervised models, excluding the supervised PCFG parser. The models are trained and tested us1624 10-fold cross validation. a 4-category ordin</context>
<context position="29960" citStr="Heilman et al. (2014)" startWordPosition="4923" endWordPosition="4926">ed an SVR model using GUG training and development subsets to predict acceptability ratings on GUG test sentences. We applied the default SVR parameters, and so it was not necessary to use the development subset separately to optimise the parameters. For evaluation we computed the correlation of the predicted ratings and mean human ratings. We present a comparison of results in Table 4. We first tested the unsupervised models, with the best correlation of 0.472 produced by the lexical 4-gram model using the Norm LP (Div) measure. Combining the models in SVR, we achieve a correlation of 0.603. Heilman et al. (2014) note that spelling is one of the important features in their regression model, as the dataset often contains spelling mistakes. We borrowed this feature, computed as the proportion of misspelled words, and incorporated into our model. It produced a significant improvement in the correlation (0.636), a performance almost on par with that of Heilman et al. (2014).16 Our results demonstrate the robustness and portability of our system in a new domain. Our SVR model requires significantly less supervision than that of Heilman et al. (2014), which relies on precision and constituent parsers. Moreo</context>
</contexts>
<marker>Heilman, Cahill, Madnani, Lopez, Mulholland, Tetreault, 2014</marker>
<rawString>Michael Heilman, Aoife Cahill, Nitin Madnani, Melissa Lopez, Matthew Mulholland, and Joel Tetreault. 2014. Predicting grammaticality on an ordinal scale. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Volume 2: Short Papers, pages 174–180, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="25232" citStr="Klein and Manning, 2003" startWordPosition="4157" endWordPosition="4161">41 0.41 0.46 0.44 0.50 0.41 0.57 Table 3: Pearson’s r of acceptability measure and mean sentence rating for all experimented models in ENWIKI. Boldface indicates the best performing measure. on the original RNNLM with 600 neurons in the hidden layer, trained on BNC-100M (Table 2 column: “RNNLM”).12 We see that RNNLM performs very well. It outperforms the other models, achieving a correlation of 0.53. 3.7 PCFG Parser (Supervised) Although we are interested in unsupervised models, for purposes of comparison we experimented with a constituent PCFG parser for our task. We use the Stanford Parser (Klein and Manning, 2003a; Klein and Manning, 2003b), and tested both the unlexicalised and lexicalised PCFG parser with the supplied model. To compute the log probability of test sentences, we experimented with both top-1 and top-1K best parses. We found that the unlexicalised variant gives better performance, but we saw little difference between using the top-1 and the top-1K best parses for computing log probability. In Table 2 (column: “PCFG”), we report results for the unlexicalised variant based on the top-1 best parse. The supervised PCFG parser performs poorly. This is not surprising, given that the parser is</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS-03),</booktitle>
<pages>3--10</pages>
<location>Whistler, Canada.</location>
<contexts>
<context position="25232" citStr="Klein and Manning, 2003" startWordPosition="4157" endWordPosition="4161">41 0.41 0.46 0.44 0.50 0.41 0.57 Table 3: Pearson’s r of acceptability measure and mean sentence rating for all experimented models in ENWIKI. Boldface indicates the best performing measure. on the original RNNLM with 600 neurons in the hidden layer, trained on BNC-100M (Table 2 column: “RNNLM”).12 We see that RNNLM performs very well. It outperforms the other models, achieving a correlation of 0.53. 3.7 PCFG Parser (Supervised) Although we are interested in unsupervised models, for purposes of comparison we experimented with a constituent PCFG parser for our task. We use the Stanford Parser (Klein and Manning, 2003a; Klein and Manning, 2003b), and tested both the unlexicalised and lexicalised PCFG parser with the supplied model. To compute the log probability of test sentences, we experimented with both top-1 and top-1K best parses. We found that the unlexicalised variant gives better performance, but we saw little difference between using the top-1 and the top-1K best parses for computing log probability. In Table 2 (column: “PCFG”), we report results for the unlexicalised variant based on the top-1 best parse. The supervised PCFG parser performs poorly. This is not surprising, given that the parser is</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 (NIPS-03), pages 3–10, Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>P Cook</author>
<author>D McCarthy</author>
<author>D Newman</author>
<author>T Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the EACL (EACL 2012),</booktitle>
<pages>591--601</pages>
<location>Avignon, France.</location>
<contexts>
<context position="11837" citStr="Lau et al., 2012" startWordPosition="1840" endWordPosition="1843">ntroduced by the division of log unigram probabilities. of the lexical N-gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N-grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 Mean LP Norm LP (Div) SLOR wi−2 wi−1 wi wi+1 (a) Lexical 3-gram (b) Bayesian H</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>J.H. Lau, P. Cook, D. McCarthy, D. Newman, and T. Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the EACL (EACL 2012), pages 591–601, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lau</author>
<author>A Clark</author>
<author>S Lappin</author>
</authors>
<title>Measuring gradience in speakers’ grammaticality judgements.</title>
<date>2014</date>
<booktitle>In Proceedings of the 36th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>821--826</pages>
<location>Quebec City, Canada.</location>
<contexts>
<context position="1003" citStr="Lau et al. (2014)" startWordPosition="140" endWordPosition="143">t intuitions about acceptability, and acceptability has been consistently rated on various scales (Sprouse and Almeida, 2012). The acceptability of a sentence appears to be relatively unaffected by its length (within certain bounds), or the frequency of its words, properties that we have confirmed experimentally. By contrast sentence probability does depend on these factors. To filter the effects of sentence length and word frequency, we devise normalising functions to map the probability of a sentence (inferred by our unsupervised language models) to an acceptability score. Keller (2001) and Lau et al. (2014) present evidence that acceptability exhibits gradience. Accordingly, we treat acceptability as a continuous variable here. We train a variety of unsupervised models for the acceptability prediction task, and we assess the performance of these models by measuring the correlation between their normalised acceptability scores and the mean crowdsourced acceptability judgements on a set of test sentences. There are a number of NLP tasks to which our work can be fruitfully applied. It can be used to evaluate the fluency of the output for machine translation and other language generation systems. It</context>
<context position="6847" citStr="Lau et al. (2014)" startWordPosition="1031" endWordPosition="1034">eneralise to another domain. We compare our methodology to a supervised system in the acceptability prediction task in Section 5. We look more closely at the influence of sentence length and lexical frequency in Section 6, and we show that the normalising functions succeed in neutralising these effects. Finally, we discuss the implications of our results, and draw conclusions from them in Section 7 and Section 8. 2 Dataset and Methodology For our experiments, we require a collection of sentences that exhibit varying degrees of grammatical well-formedness. We use the dataset that we discuss in Lau et al. (2014). We translated British National Corpus (BNC Consortium, 2007) English sentences to four other languages – Norwegian, Spanish, Japanese and Chinese – and then back to English using Google Translate. To collect human judgements of acceptability for the sentences, we used Amazon Mechanical Turk. A total of 2,500 sentences were annotated. Three modes of presentation were used for rating a sentence: (1) binary with two options (unnatural vs. natural); (2) four options (extremely unnatural, somewhat unnatural, somewhat natural and extremely natural); and (3) a sliding scale with two extremes (extre</context>
<context position="26403" citStr="Lau et al. (2014)" startWordPosition="4348" endWordPosition="4351">is is not surprising, given that the parser is trained on a different domain.13 Moreover, the log probability scores are not true probabilities, but arbitrary values used for ranking the parse trees. 4 English Wikipedia For the BNC domain we saw that SLOR and Norm LP (Div) give the best acceptability measures, and that BHMM, two-tier BHMM and RNNLM are the best performing models. These findings are limited to a particular dataset. To better understand if these observations generalise to another domain, we developed an English Wikipedia dataset (ENWIKI), following the same process described in Lau et al. (2014) to generate test sen12Other parameter values of RNNLM: number of classes = 550; bptt = 4; bptt-block = 100. 13The Stanford English model is trained on the parse tree hand annotated WSJ (section 1–21), Genia, and a few other datasets. tences through round-trip machine translation ,and to collect annotations via Mechanical Turk.14 As before, we follow the same procedures described in Section 3 to optimise, train, and test all models (excluding LDA and PCFG). The Pearson correlations with mean AMT annotations are presented in Table 3. We identify similar trends in ENWIKI: Norm LP (Div) and SLOR </context>
</contexts>
<marker>Lau, Clark, Lappin, 2014</marker>
<rawString>J.H. Lau, A. Clark, and S. Lappin. 2014. Measuring gradience in speakers’ grammaticality judgements. In Proceedings of the 36th Annual Conference of the Cognitive Science Society, pages 821–826, Quebec City, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>A Deoras</author>
<author>S Kombrink</author>
<author>L Burget</author>
<author>J `Eernock´y</author>
</authors>
<title>Empirical evaluation and combination of advanced language modeling techniques.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH</booktitle>
<pages>605--608</pages>
<location>Florence, Italy.</location>
<marker>Mikolov, Deoras, Kombrink, Burget, `Eernock´y, 2011</marker>
<rawString>T. Mikolov, A. Deoras, S. Kombrink, L. Burget, and J. `Eernock´y. 2011a. Empirical evaluation and combination of advanced language modeling techniques. In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011), pages 605–608, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>S Kombrink</author>
<author>A Deoras</author>
<author>L Burget</author>
<author>J `Eernock´y</author>
</authors>
<title>Rnnlm -recurrent neural network language modeling toolkit.</title>
<date>2011</date>
<booktitle>In IEEEAutomatic Speech Recognition and Understanding Workshop,</booktitle>
<location>Hawaii, US.</location>
<marker>Mikolov, Kombrink, Deoras, Burget, `Eernock´y, 2011</marker>
<rawString>T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. `Eernock´y. 2011b. Rnnlm -recurrent neural network language modeling toolkit. In IEEEAutomatic Speech Recognition and Understanding Workshop, Hawaii, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
</authors>
<title>Statistical Language Models based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="22423" citStr="Mikolov, 2012" startWordPosition="3667" endWordPosition="3668">, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in the size of the hidden layer). Full details of RNNLM can be found in the original papers (Mikolov et al., 2011b; Mikolov, 2012).11 We experimented with some of the parame</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>T. Mikolov. 2012. Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Nagendra Koilada</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Bayesian text segmentation for index term identification and keyphrase extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>2077--2092</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="11818" citStr="Newman et al., 2012" startWordPosition="1836" endWordPosition="1839">rse the sign change introduced by the division of log unigram probabilities. of the lexical N-gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various Bayesian models that incorporate richer latent structures. We chose a Bayesian implementation because its “rich gets richer” dynamics tends to work well for languages (Goldwater and Griffiths, 2007; Goldwater et al., 2009; Newman et al., 2012; Lau et al., 2012). Lexical N-grams model the generation of a word based on its preceding words. We introduce a layer of latent variables on top of the words, which can be interpreted as the word classes, and we model the transitions between the latent variables and observed words using Markov processes. In this model we first generate a (latent) word class based on its preceding word classes, and we then generate the word based on its word class. Figure 1(b) illustrates the structure of a second order Hidden Markov model (HMM). 1620 Mean LP Norm LP (Div) SLOR wi−2 wi−1 wi wi+1 (a) Lexical 3-</context>
<context position="19408" citStr="Newman et al. (2012)" startWordPosition="3111" endWordPosition="3115">cabulary size; and #(x, [y], [z]) are the multinomial counts. We follow the same process for optimising, training, and testing the model, and we summarise the results in Table 2 (column: “2T”).9 We see an improved correlation relative to BHMM (BHMM best: 0.45, Two-Tier BHMM best: 0.50). In fact it has the best performance of all models thus far. This is encouraging, as it implies that the introduction of the phrase layer produces a more optimal structure for representing acceptability. 3.5 Bayesian Chunker Goldwater et al. (2009) propose a Bayesian approach to segment words in speech streams. Newman et al. (2012) extend the approach to segment phrases – i.e. multiword units – in sentences, and they apply it to the task of index term identification and keyphrase extraction. The core machinery of the methodology is driven by the Dirichlet Process, where segments (words in Goldwater et al. (2009) or phrases in Newman et al. (2012)) are retrieved from a cache, 9The optimised parameters: S = 100, T = 60, α = 1.0, 7 = 1.0, 6 = 0.01. 1622 P(ti|t−i, s, w, α, 7, 6) ∝ #(ti−1, si−1, ti) + α × #(ti−1, si−1) + Tα #(ti, si, ti+1) + α × #(ti, si) + Tα ; #(ti, si−1) + S7 + &apos;y P(si |s−i, t, w, α, 7, 6) ∝ #(ti, si−1, s</context>
<context position="23855" citStr="Newman et al. (2012)" startWordPosition="3905" endWordPosition="3908">t does joint learning with a Maximum Entropy model which learns direct connections of N-gram features. We found that although there are advantages to using the ME model, the benefits disappear as we increase the number of neurons in the hidden layer. We saw optimal performance at 600 neurons, without using the ME model. All our results are based or newly generated. Using Gibbs sampling for inference, the sampler considers one boundary point at a time, and computes the probability of two hypotheses: H0, for not generating a boundary; and H1, for generating a boundary. Borrowing the notation of Newman et al. (2012), given p# is the probability of generating a segment boundary, at the boundary point between words wx and wy, the probability of the hypotheses is computed as follows: 10The optimised parameters are: α = 0.1, β = 0.001, p# = 0.5. p11 We use Mikolov’s implementation of RNNLM for our 1623 eriment: ht tp : / / rnnlm . org/. P(H0|H−) = n(wxy) + αP0(wxy) ;n + α P(H1|H−) = n(wx) + αP0(wx) n + α n(wy) + αP0(wy) × n + 1 + α P(H0|H−) = ; P(H1|H−) = (n(cx)+αP0(cx) � #(cw, cx) + β n+α Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLM LogProb 0.31 0.36 0.38 0.32 0.33 0.35 0.42 0.44 Mean LP 0.28 0</context>
</contexts>
<marker>Newman, Koilada, Lau, Baldwin, 2012</marker>
<rawString>David Newman, Nagendra Koilada, Jey Han Lau, and Timothy Baldwin. 2012. Bayesian text segmentation for index term identification and keyphrase extraction. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 2077–2092, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Large-scale syntactic language modeling with treelets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>959--968</pages>
<location>Jeju,</location>
<contexts>
<context position="9732" citStr="Pauls and Klein, 2012" startWordPosition="1491" endWordPosition="1494">s to lower case. To address out of vocabulary (OOV) words that are seen in the test sentences but not in the training data, we adopt the Berkeley Parser approach, where we replace low frequency or OOV words using the UNK signature. We capture additional surface characteristics of the original word by attaching features at the end of the signature (e.g. the OOV word 1949 would be replaced by the signature UNK-NUM).3 3 Unsupervised Models 3.1 Lexical N-gram Model Lexical N-gram models were variously explored in tasks related to acceptability estimation (Heilman et al., 2014; Clark et al., 2013; Pauls and Klein, 2012). We use an N-gram model with Kneser-Ney interpolation (Goodman, 2001), and we train bigram, trigram, and 4-gram models on BNC-100M. The trained models are then used to compute the acceptability measures of the test sentences. The results are detailed in Table 2 (columns: “2-gram”, “3-gram” and “4-gram”).4 In general across all models, the Norm LP (Div) and SLOR measures consistently produce the best correlations. We see a significant improvement when the context window is increased from 2-gram to 3-gram, but not so from 3-gram to 4-gram (2-gram best: 0.34; 3-gram best: 0.42; 4-gram best: 0.42</context>
<context position="10977" citStr="Pauls and Klein (2012)" startWordPosition="1698" endWordPosition="1701">that increasing the context window 3Low frequency words are defined as words occurring less than 4 times in the BNC training data. A total of 15 features are used for the UNK signature. 4We do not present model perplexity values in the results, as we did not find any correlation between perplexity and task performance. Acc. Measure Equation LogProb log Pm(ξ) log Pm(ξ) |ξ| log Pm(ξ) log Pu(ξ) Norm LP (Sub) log Pm(ξ) − log Pu(ξ) log Pm(ξ) − log Pu(ξ) |ξ| Table 1: Acceptability measures for predicting the acceptability of a sentence. Notations: SLOR is the syntactic log-odds ratio, introduced by Pauls and Klein (2012); ξ is the sentence (|ξ |is the sentence length); Pm(ξ) is the probability of the sentence given by the model; Pu(ξ) is the unigram probability of the sentence. Note that the negative sign in Norm LP (Div) is given to reverse the sign change introduced by the division of log unigram probabilities. of the lexical N-gram model does not correspond to a better representation of grammatical structure (insofar as the size of the dataset is fixed), and a more sophisticated model is necessary. 3.2 Bayesian HMM Seeing that local context is insufficient for predicting acceptability, we explore various B</context>
</contexts>
<marker>Pauls, Klein, 2012</marker>
<rawString>A. Pauls and D. Klein. 2012. Large-scale syntactic language modeling with treelets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959–968. Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Seide</author>
<author>G Li</author>
<author>D Yu</author>
</authors>
<title>Conversational speech transcription using context-dependent deep neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011),</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="22252" citStr="Seide et al., 2011" startWordPosition="3637" endWordPosition="3640"> 2 (column: “Chunker”). The model produces a moderate correlation, performing on par with the lexical 4-gram model. 3.6 Recurrent Neural Network Language Model In recent years, we have seen a resurgence in the use of neural networks for deep machine learning and NLP. Rather than designing structures or handcrafting features that seem intuitive for a task, deep learning introduces an entirely general architecture for machine learning. It has yielded some impressive results for NLP tasks: automatic speech recognition, parsing, part of speech tagging, and named entity recognition, to name a few (Seide et al., 2011; Mikolov et al., 2011a; Collobert et al., 2011; Chen and Manning, 2014). We experiment with a recurrent neural network language model (RNNLM: (Elman, 1998; Mikolov, 2012)) for our task. We choose this model because it has an internal state that keeps track of previously observed sequences, which is well suited for natural language problems. To train the model, we use stochastic gradient descent combined with back propagation through time. RNNLM is optimised to reduce the error in predicting the following word, based on the current word and its history (represented in a compressed dimension in</context>
</contexts>
<marker>Seide, Li, Yu, 2011</marker>
<rawString>F. Seide, G. Li, and D. Yu. 2011. Conversational speech transcription using context-dependent deep neural networks. In Proceedings of the 12th Annual Conference of the International Speech Communication Association (INTERSPEECH 2011), Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sprouse</author>
<author>D Almeida</author>
</authors>
<title>Assessing the reliability of textbook data in syntax: Adger’s core syntax.</title>
<date>2012</date>
<journal>Journal of Linguistics,</journal>
<volume>48</volume>
<issue>3</issue>
<marker>Sprouse, Almeida, 2012</marker>
<rawString>J. Sprouse and D. Almeida. 2012. Assessing the reliability of textbook data in syntax: Adger’s core syntax. Journal of Linguistics, 48(3):609–652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sprouse</author>
</authors>
<title>Continuous acceptability, categorical grammaticality, and experimental syntax.</title>
<date>2007</date>
<journal>Biolinguistics,</journal>
<pages>1--123</pages>
<contexts>
<context position="3367" citStr="Sprouse, 2007" startWordPosition="504" endWordPosition="505"> two different datasets. Our methodology is highly portable to other domains and languages, and the approach has potential implications for the representation and the acquisition of linguistic knowledge. 1 Introduction Language modelling involves predicting the probability of a sentence. Given a trained model, we can infer the quantitative likelihood that a sentence occurs. Acceptability, on the other hand, indicates the extent to which a sentence is permissible or acceptable to native speakers of the language. While acceptability is affected by frequency and exhibits gradience (Keller, 2001; Sprouse, 2007; Lau et al., Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1618–1628, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics our methodology to other domains/languages is therefore straightforward, as it requires only a raw training corpus in that domain/language. Our work may also have significant implications for the cognitive foundations of the representation and acquisition of linguistic knowledge. The unannotated training corpora of our la</context>
</contexts>
<marker>Sprouse, 2007</marker>
<rawString>J. Sprouse. 2007. Continuous acceptability, categorical grammaticality, and experimental syntax. Biolinguistics, 1:123–134.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>