<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000908">
<title confidence="0.981293">
Very Low-Dimensional Latent Semantic Indexing for Local Query Regions
</title>
<author confidence="0.936544">
Yinghui Xu Kyoji Umemura
</author>
<affiliation confidence="0.940659">
Toyohashi Unversity of Technology Dept. of Information and Computer Sciences
</affiliation>
<address confidence="0.814403">
1-1, Hibarigaoka, Toyohashi, Aichi,Japan
</address>
<email confidence="0.995275">
xyh@ss.ics.tut.ac.jp umemura@tutics.tut.ac.jp
</email>
<sectionHeader confidence="0.998439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998148125">
In this paper, we focus on performing
LSI on very low SVD dimensions. The
results show that there is a nearly linear
surface in the local query region. Using
low-dimensional LSI on local query re-
gion we can capture such a linear surface,
obtain much better performance than
VSM and come comparably to global
LSI. The surprisingly small requirements
of the SVD dimension resolve the com-
putation restrictions. Moreover, on the
condition that several relevant sample
documents are available, application of
low-dimensional LSI to these documents
yielded comparable IR performance to
local RF but in a different manner.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988826">
The increasing size of searchable text collection
poses a great challenge to performing the informa-
tion retrieval (IR) task. Latent Semantic Index-
ing (LSI) is an enhancement of the familiar Vector
Model of IR. It satisfies the IR task through discov-
ering corpus-wide word relationship based on co-
occurrence analysis of a whole collection. LSI has
been successfully applied to various document col-
lections and has achieved favorable results, some-
times outperforming VSM (Dumais, 1996). How-
ever, the principal challenges to applying LSI to
large data collections are the cost of computing and
storing SVD.
Local analysis of the information in a set of top-
ranked documents for the query is one promising
way to solve the computationally demanding IR task
for a large collection. To solve the computational
complexity of LSI, David Hull introduced one inter-
esting method, local LSI, for routing problems(Hull,
1994). The basic idea is: apply the SVD to a set of
documents known to be relevant to the query; then
all the documents in the collection can be folded into
the reduced space of those relevant documents. By
concentrating on the local space around the query re-
sults, we may be able to compute using flexible and
efficient LSI algorithms.
In this paper we put much emphasis on local
dimensionality analysis of the local query regions
filled with relevant documents. In ideal experimen-
tal cases, local LSI involves only the documents
known to be relevant to the query. To our surprise,
in most of our experiments, local LSI obtains its best
IR performance using just one or two SVD dimen-
sions. These interesting results moved us to try per-
forming local LSI with one or two SVD dimensions
on the top return sets of VSM in ad-hoc IR experi-
ments. We found that this worked surprisingly well.
In a practical setting, local LSI may be regarded as a
variation of pseudo relevance feedback (RF). There-
fore, the comparative results with local RF are pro-
vided in this paper as well. The experiments show
that local LSI with one or two SVD dimensions can
contribute to expanding the query information in a
manner different from traditional local RF.
This paper is organized as follows: Section 2 re-
views existing related techniques. Section 3 de-
scribes the implementation architecture of the ex-
periments and gives the experiment results. Section
4 explains the result and points out characteristic of
the local LSI. Section 5 draws the conclusions.
</bodyText>
<sectionHeader confidence="0.998684" genericHeader="introduction">
2 Related works
</sectionHeader>
<subsectionHeader confidence="0.998187">
2.1 Latent Semantic Indexing
</subsectionHeader>
<bodyText confidence="0.991485818181818">
Latent semantic indexing (Berry et al., 1999) is one
kind of vector-based query-expansion methods that
use neither terms nor documents as the orthogo-
nal basis of a semantic space. Instead, it computes
the most significant orthogonal dimensions in the
term-document matrix of the corpus, via SVD, and
projects documents into the low rank subspace thus
found. LSI then computes semantic similarity based
on the proximity among projected vectors.
LSI uses SVD to factor the term-document
training matrix A into three factors: A =
</bodyText>
<equation confidence="0.91576825">
UEVT = Udiag(a1, Q2, · · · , Qn)VT Where
U = (u1, u2, ··· , um)  m×m and V =
(v1, v2, · · · , vn)  n×n are unitary matrices (i.e.
UT U = I, VT V = I), whose columns are the
</equation>
<bodyText confidence="0.998698277777778">
left and the right singular vectors of A respectively,
E  m×n is a diagonal matrix whose diagonal ele-
ments are non-negative and arranged in descending
order (a1  Q2  · · ·  Uk), and p = min(m, n).
The values a1, Q2, · · · , up are known as the singular
values of A, and are the square roots of the eigen-
values of AAT and AT A.Suppose the rank of A is
r, then r  p and only a1  Q2  · · ·  Qr are
positive, while the remaining (p-r), if r&lt;p, singular
values are zero. In LSI retrieval, researchers are only
concerned with the first r singular values of A. LSI
uses the structure from SVD to obtain the reduced-
dimension form of the training matrix A as its “la-
tent semantic space.” Notation for k  r, defines the
reduced-dimension form of A to be A = UEVT =
Udiag(a1, Q2, · · · , Qk, 0, · · · , 0)V T . That is, Ak is
obtained by discarding the r-k least significant sin-
gular values and the corresponding left and right
singular vectors of A (since they are now mul-
tiplied by zeros). Then, the first k columns of
U that correspond to the k largest singular values
of A together constitute the projection matrix for
LSI: Sim(d, ql = (ATk d) • (ATk�q). Analogous to
VSM, the vector representation of a document is
the weighted sum of the vector representation of
its constituent terms. For document vector di and
query vector qi, ATk d� and AT K4 are now the LSI vec-
tor representations of that document and query, re-
spectively, in the reduced-dimension vector space.
This process is known as “folding in” documents (or
queries) into the training space. Actually, LSI as-
sumes that the semantic associations among terms
can be found through this one-step analysis of their
statistical usage in the collection, and they are im-
plicitly stored in the singular vectors computed by
SVD.
</bodyText>
<subsectionHeader confidence="0.998428">
2.2 Relevance Feedback
</subsectionHeader>
<bodyText confidence="0.99854528">
A feedback query creation algorithm developed by
Rocchio (Rocchio, 1971) in the mid-1960s has, over
the years, proven to be one of the most successful
profile learning algorithms. The algorithm is based
upon the fact that if the relevance for a query is
known, an optimal query vector will maximize the
average query-document similarity for the relevant
documents, and will simultaneously minimize the
average query-document similarity for non-relevant
documents. Rocchio shows that an optimal query
vector is the difference vector among the centroid
vectors for the relevant and non-relevant documents.
Qo = R EDRel. D − N � R ED/Rel. D where R is
the number of relevant documents, and N is the to-
tal number of documents in the collection. Also, all
negative components of the resulting optimal query
are assigned a zero weight. To maintain focus of the
query, researchers have found that it is useful to in-
clude the original user-query in the feedback query
creation process. Also, coefficients have been intro-
duced in Rocchio’s formulation, which control the
contribution of the original query, the relevant docu-
ments, and the non-relevant documents to the feed-
back query. These modifications yield the follow-
ing query reformulation function: Qn = a × �Qo +
</bodyText>
<equation confidence="0.839334">
�× 1R Drel
E D − &apos;y × 1
N−R E D� In this paper,
D/rel
</equation>
<bodyText confidence="0.999354">
the experiment results based on the local RF were
performed for comparing with the results of Local
LSI. The terms in the query are reweighted using
the Rocchio formula with a : Q : -y = 1 : 1 : 0.
As for the local information relevant to the query,
they were obtained by extracting several top-ranked
documents through the VSM retrieving process in
the experiments. Jiang has ever used the similar
experiments (VSM+LSI) for Local LSI in his pa-
</bodyText>
<figure confidence="0.70725">
6. Back to the step 2 and continue the analysis on
the next query in the same way.
per “Approximate Dimension Reduction at NTCIR”
(Fan and Littmen, 2000).
local LSI
</figure>
<figureCaption confidence="0.999107">
Figure 1: Implementation architecture
</figureCaption>
<sectionHeader confidence="0.830671" genericHeader="method">
3 Experiment Set-up and Results
</sectionHeader>
<subsectionHeader confidence="0.999149">
3.1 Implementation Architecture
</subsectionHeader>
<bodyText confidence="0.996017">
Figure 1 shows overall the architecture of the exper-
iment. The procedure is described as follows:
</bodyText>
<listItem confidence="0.985267">
1. Indexing the document collection and query
sets
2. Given a query, retrieving some document by
the relevant sets. In some cases the relevant sets
are derived from the known relevant documents
and in other cases we regarded the top returned
documents as the relevant sets.
3. performing the singular value decomposition
on documents identified in 2.
4. Only a few dimensions for the LSI are retained.
5. Projecting the document vectors and the query
</listItem>
<bodyText confidence="0.995563875">
vector into the user-cared feature space, and
then using the standard Cosine measure to get
the final score for this query.
Step 1 is pre-processing procedure for IR system.
Only the tag removal, upper case characters trans-
verse, stoplist removal and Porter’s stemming were
adopted in this phase (Frakes and Baeza-Yates,
1992). Next, the smart “ltc” term weighting scheme
(Salton and McGill, 1983) was used to compute the
entries of the term document matrix for the collec-
tion and entries of the query vector. The second step
can be regarded as filter container. In this paper,
the three kinds of routine schemes were performed.
In the first case, the local space for each query was
represented simply by all document vectors, which
have already been judged to be relevant (appearing
in the relevant judgment file). We note that although
it is an ideal case, it may form a useful upper bound
on performance. In the second case, we assume the
condition that the user provides a reasonable num-
ber of relevant documents. In the third case, the lo-
cal space for each query was built on the top return
sets of VSM. The use of the top returned items from
VSM is similar to blind feedback or pseudo RF.
</bodyText>
<subsectionHeader confidence="0.999863">
3.2 Characteristic of test collection
</subsectionHeader>
<bodyText confidence="0.975690875">
There are three test collections in our experiments.
Two of them, Cranfield and Medlars, are small. The
third one is a large-scale test collection, NACSIS.
The Cranfield corpus consists of 1,400 documents
on aerodynamics and 225 queries, while Medlars
consists of 1,033 medical abstracts and 30 queries.
Although these two collections are very small, they
were used extensively in the past by IR researchers.
As for the NACSIS test collection for the IR 1 &amp; 2
(NTCIR 1&amp;NTCIR 2) (Kando, 2001), these docu-
ments are abstracts of academic papers presented at
meetings hosted by 65 Japanese scientists and lin-
guists. In our experiments, the English Monolingual
IR was performed. This collection consists of ap-
proximately 320,000 English documents in NTCIR-
1 and NTCIR-2.
</bodyText>
<subsectionHeader confidence="0.999159">
3.3 Local Routine Experiments (Ideal Case)
</subsectionHeader>
<bodyText confidence="0.99993675">
We first present the experimental results on the ideal
condition. The document vectors already judged to
be relevant to the query were used. SVD calcula-
tion are performed on the local region organized by
</bodyText>
<figure confidence="0.996977159090909">
topic set (query)
document set (corpus)
\HV
Qids ( )
1 ≤ s ≤ k
query vector(g,)
query vector set
Qid1 (t1, t2, ... tr)
:
Qidk (t1, t2, ... tr)
LQSXW
TXHU\ YHFWRU
doci, docj, ... ,dock
1≤ i,j,...,k≤n
G score qid = q s A k d A k
( ) ( ) ( )
T local T local
creating local query region
through the identified documents
preprocess
1. stopword removal.
2. porter&apos;s stemming.
3. smart &amp;quot;ltc&amp;quot; tw
relevant sets of Qids
reduced feature space ( )
AK local
organized by singular vectors
output the score-list for Qids
and continuefor next query
SVD
s
≤
K
term (m) X doc (n) matrix
1. &lt;docid - document vector&gt;
2. &lt;termid -term vector&gt;
doc1 vector
: :
docn vector
score table
Qid Docid score
QR
LQSXW
GRF YHFWRU
</figure>
<tableCaption confidence="0.9647325">
Table 1: Results on the Cran., Med. and NTCIR are shown in terms of ave. precision, precision at document
cutoff of 10. Results of the local LSI experiment based on three different SVD dimensions were provided.
</tableCaption>
<table confidence="0.999819333333333">
Cranfield Medlars NTCIR (E-E) (D run)
K Avr. P-R R-p K Avr. P-R R-p K Avr. P-R R-p
VSM - 0.4148 0.3885 - 0.5306 0.5359 - 0.212 0.2277
+0% +0% +0% +0% +0% +0%
G. 200 0.4543 0.4180 80 0.6680 0.6648 - - -
LSI +9% +0% +8% +0% +26% +0% +25% +0% - - -
1 0.8833 0.8243 1 0.8946 0.8139 1 0.6997 0.6508
+113% +95% +112% +97% +69% +34% +52% +22% +230% +186%
L. 2 0.8607 0.8185 2 0.8769 0.8035 2 0.7062 0.6314
LSI +108% +90% +108% +96% +67% +32% +52% +20% +233% +177%
3 0.8585 0.8102 3 0.8726 0.8019 3 0.6934 0.6293
+107% +90% +108% +96% +68% +30% +51% +20% +228% +176%
</table>
<bodyText confidence="0.999914466666667">
these relevant documents with respect to its query.
The IR performance of VSM and global LSI were
regarded as the baseline for comparison. As for the
NTCIR collection, English-English Monolingual IR
was performed and we only extracted the “D” (De-
scription) field of the topic as the query. Due to
its large size, only the result of VSM is the base-
line. Additionally, to observe the influences of SVD
factors on the IR performance for local LSI exper-
iments, results based on LSI dimension from 1 to
3 were also provided for comparison. As we ex-
pected, the majority of experimental studies are di-
rected towards obtaining better solutions for the lo-
cal routine LSI method. In table 1, K represents
the SVD dimension for LSI analysis. As for the k
value of global LSI, it is the parameter by which
LSI yields the best IR performance. The improve-
ment in the average precision of local routine LSI
is 113, 69 and 233 percent better than that of VSM
on Cranfield, Medlars and NTCIR test collections
respectively. The improvement in average precision
of local routine LSI is 95 and 34 percent better than
that of global LSI on Cranfield with 200 SVD di-
mensions and Medlars with 80 SVD dimensions re-
spectively. Moreover, in the case of SVD factors
equal to 1, we obtain the best IR performance among
all cases on the Cranfield and Medlars. While the
NTCIR collection obtained its best IR performance
with 2 SVD dimensions, there is only a slight dif-
ference between the case with 1 singular vector and
the case with 2. Such small numbers caught our at-
tention, since they indicate that there is a nearly lin-
ear surface in the local region and that the dominant
SVD dimensions can capture such surface and yield
a good IR performance for local LSI analysis.
To clarify how the local LSI space influences IR
performance, we projected the document vectors
onto the extracted local routine LSI space and fig-
ured out the distribution in figure 2. The data of
plots are based on one query from the Medlars col-
lection. Only the largest singular vector was used
for the left plot, and the two largest were used for the
right. Based on the plots, we find that these dimen-
sions do not vary significantly for the non-relevant
documents, Thus, they tend to cluster around the
origin. On the other hand, the relevant document
space illustrates that local SVD factors are designed
to capture their structure.
Since the pre-judged set of documents is generally
not available for the ad-hoc query, In this paper, to
investigate the efficiency of local LSI using very low
dimensions, we continue to do some experiments us-
ing different numbers of relevant documents, which
were selected from the relevant judgment file. The
comparative results based on four cases in which the
SVD factors equal 1, 2 and 3, respectively, were
shown in Table 2. The second column is the condi-
tion, which means that the number of relevant doc-
uments belonging to the analyzing object (query)
should exceed the value in table. Column 3 “#qry”
</bodyText>
<figure confidence="0.982543461538462">
inner product value
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.4
0.3
0.2
0.1
0.0
seco nd factor
non-relevant document
query
relevant document
Doc. ID
0 200 400 600 800 1000 1200
0.6
0.5
-0.2
-0.4
-0.6
0.4
0.2
0.0
non-relevant document
query
relevant document
the distribution of inner product of document vector largest factor
with the largest singular vector
</figure>
<figureCaption confidence="0.998165166666667">
Figure 2: Medlars: document collection distribution after represented by the Local region singular vectors.
For the left figure the X-axis is doc.ID and Y are the inner products of the doc vector with the largest singular
vectors. X and Y coordinates on the right are the inner product of the document vectors with the first and
second largest singular vector, respectively.
Figure 3: the Ave. precision-recall comparison plots between the best run of local LSI with the baseline
VSM and global LSI.
</figureCaption>
<figure confidence="0.998619466666667">
local LSI (s=20,k=1)
global LSI (k=80)
VSM
Medlars
0.0 0.2 0.4 0.6 0.8 1.0
local LSI (s=3,k=2)
global LSI (k=200)
VSN
Cranfield
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
NTCIR
vsm LSI (s=3,k=2)
VSM
1.0
0.9
0.8
Ave. precision-recall
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.9
0.8
0.7
Ave. precision-recall
0.6
0.5
0.4
0.3
0.2
0.1
0.7
0.6
0.5
Ave. precision-recall
0.4
0.3
0.2
0.1
0.0
</figure>
<bodyText confidence="0.982470744186046">
indicates the numbers of queries in the test collec-
tion which satisfy the condition appearing in the sec-
ond column. The fourth column gives the parameter
indicating the number of relevant documents to be
used for creating the local space of the correspon-
dent query. As we expected, local LSI using one or
two SVD dimensions built from the first two singu-
lar vectors resulted in the best IR performance in the
partially ideal experiments. The comparison of the
results was shown in the Table 2.
We know that the most important step in LSI is
the phase of SVD. It requires O(k x nz2) to find
the k leading eigenvectors. The parameter nz is the
non-zero entries of the term-by-document matrix.
These requirements are unacceptably high for doc-
ument data sets as the non-zero entries number tens
of thousands. According to the LSI analyzing proce-
dure, it includes the SVD phase and the subsequent
projecting treatment. For global LSI, the compu-
tation complexity can be evaluated by: O(nz2k +
#qry x k2 x nz2 x qnz2) k = (100 - 300)
While our approach can be estimated by: O(#qryx
[(nz2lockloc) + (k2loc x nz2 x qnz2)]) k = 1 or 2
In the above equation, “nzloc” represents the non-
zero entries of the local query region. “qnz” are
non-zero entities in the query vector. The value of
“nzloc” varies with the number of known relevant
documents. Note that the difference between these
two equations shows clearly that local LSI on small
SVD dimensions is much easier to compute than
global LSI. According to our observation, it is par-
ticularly fast when computing only the largest sin-
gular value.
Based on the above experiments, the interesting
results and the power of the two largest singular vec-
tors prompted us to try putting the local LSI with one
or two singular dimensions into the practical experi-
ments. In this paper, we used the simplest and most
efficient VSM method as the initial retrieval step for
extracting the relevant information around the query.
We assume that the top-ranked documents obtained
by VSM are relevant documents. The details are in-
troduced in section 3.4.
</bodyText>
<subsectionHeader confidence="0.986281">
3.4 Ad-hoc local LSI experiment
</subsectionHeader>
<bodyText confidence="0.999869857142857">
In this experiment, we note that using the top re-
turned items from VSM is sometimes called blind
feedback or pseudo RF. Hence, we borrow the idea
of local RF. The expanded query representation was
obtained by combining the original query vector
with its projecting result on the local SVD dimen-
sions. The equation for expanding the scheme is as
</bodyText>
<equation confidence="0.894379166666667">
follows Aj°C(AjOe)T gori In the equa-
gnew = gori +
tion:
Aloc
k = Ulo ckElock(V koc)T
�d•(�qori + N c(Ek c)2(Uk c)T�qori)
</equation>
<bodyText confidence="0.999943615384615">
As for the parameter k, representing the SVD dimen-
sionality of the local region, we set its value equal to
1 or 2 in this experiment. At first, to show that local
LSI on small dimensions works well is a practical
case clearly, we gave the comparable plots between
local LSI with the baseline VSM and global LSI.
The 11ppt. average precision recall plots of local
LSI were figured out for the three test collections in
figure 3. The symbol s in the figure represents the
sample size and k represents the SVD dimension.
To our satisfactions, local LSI based query expan-
sion method does much better than VSM and more
closely approaches the global LSI.
Next, to investigate the effectiveness of low di-
mensional LSI on local query region in restructur-
ing the user cared information space, local RF with
Rocchio’s weights a : Q : -y = 1 : 1 : 0, as in
Xu and Croft (Xu and Croft, 2000), was used for
comparison. Both of them were used on the same
sample documents. The difference between them is
a twofold one. First, the standard RF formula shown
in section 2.2 make use of weighting parameters for
query expansion, while our approach does not. Sec-
ondly, different combination object was used. The
local RF experiment performed in this paper makes
use of the centroid of the top s returned document
vectors. In our approach, we combine the original
query vector with its projecting results on the low
local SVD space.
Table 3 shows these results in terms of varying
feedback size with one or two SVD dimensions. The
first column ”sample size” in the table is the value of
s according to which we would select the top rank
documents . We see that local LSI outperforms lo-
cal RF for most combinations of sample size and
one or two SVD dimensions in the experiment on
Medlars. The best run on Medlars using local LSI
is 8.4% better than the best in local RF. As for the
best run on Cranfield and on NTCIR, local LSI got
comparable results with the local RF. In the exper-
iments, we note that with the increasing of sample
size, the precision of local LSI decreased more than
that of local RF. Based on our analysis, there are two
reasons for this. First, In the VSM based local LSI
experiments, we assume that the top s documents
from the initial retrieval by VSM are relevant, al-
though that assumption does not always hold. In the
case where the dominant components of the top s
return sets are non-relevant, the maintained SVD di-
mensions would deviate from the orientation that we
preferred. This will influence the following projec-
tion procedure greatly. The average precision-recall
results of VSM on Cranfield and NTCIR is 0.38 and
0.21, respectively. Neither one is ideal. The second
factor is the characteristic of the test collection. The
number of relevant documents for query sets ranges
from 2 to 40 and from 3 to 170 for the Cranfield
and NTCIR, respectively. With such wide range of
query sets, some queries don’t have enough relevant
documents for this strategy to be feasible. There-
fore, from the experiment results, it is still reason-
able for us to believe that if several relevant sample
documents of a query are available, low-dimensional
local LSI will be able to achieve comparable perfor-
mance to local RF.
</bodyText>
<sectionHeader confidence="0.866516" genericHeader="method">
4 Analysis and discussion
</sectionHeader>
<subsectionHeader confidence="0.996282">
4.1 Local dimensions
</subsectionHeader>
<bodyText confidence="0.999894333333333">
One important variable for LSI retrieval is the num-
ber of dimensions in the reduced space. In this pa-
per, we found that one or two SVD dimensions are
able to represent the structure of the local region
that corresponds to the user’s interests. The first two
largest singular vectors will represent the two major
</bodyText>
<equation confidence="0.604119">
Sim( d, �qnew) =
</equation>
<tableCaption confidence="0.9539105">
Table 2: Ave. precision-recall comparing results
based on different SVD factors.
</tableCaption>
<table confidence="0.9707149">
Coll. Cond. #qry #sel. SVD Ave.
Rel. fact. P-R
1 0.6857
10 2 0.6667
Cran. &gt;15 27 3 0.6654
(#rel) 1 0.5749
5 2 0.5692
3 0.5641
1 0.7945
10 2 0.8007
Med. &gt;15 25 3 0.7952
(#rel) 1 0.7160
5 2 0.7142
3 0.7137
1 0.3899
10 2 0.3987
NTCIR &gt;15 23 3 0.3967
(#rel) 1 0.2917
5 2 0.2913
3 0.2883
</table>
<bodyText confidence="0.9998948">
interests. The local SVD dimensions built on them
have the ability to absorb the interests of a query and
have no interest in the non-relevant information. It
indicates that there is near linear surface in the local
query region. That is why local LSI works well on
small dimensions, especially on the condition that
there is only one dominant interests in the query. Of
course, in cases where there is much noisy informa-
tion in the local region, the SVD dimension may fail
to satisfy the true needs of the user. Finally, based
on the experiments in this paper, we would like to
point out that for performing SVD on a particular
local query region, the requirement of the SVD di-
mension should not be demanding. In our opinion,
2 or less is sufficient to obtain ideal IR performance.
</bodyText>
<subsectionHeader confidence="0.997942">
4.2 Size of local region
</subsectionHeader>
<bodyText confidence="0.999988615384615">
The size of a local region is also one important pa-
rameter for local LSI. We did not do much analysis
on how to determine the best size of the local region
for local LSI. In the absence of any clear guidelines
now, we merely offer some suggestions and an anal-
ysis. The local region should be large enough so that
it will contain more relevant information. However,
there are also several reasons why the local region
should not be too large. Adding a large number of
non-relevant documents of marginal value will only
increase the number of LSI factors needed to de-
scribe the local region without improving their qual-
ity, and this will only degrade the IR performance.
Therefore, as for the size of the local region, it is a
tradeoff. According to the experimental results and
the analysis in section 3.4, since the local LSI does
well on one or two SVD dimensions, so as to avoid
influences of non-relevant information brought by
more involved documents, it is better to restrict the
size of a local region below 30. In the experiments
on Medlars, local LSI produced its best run at 20 top
return documents. Of course, the threshold for the
size of local region should be collection-dependent
and experiment-determined. It may also be possible
to set the threshold by the performance of the initial
retrieval method, but we have not yet analyzed this.
</bodyText>
<subsectionHeader confidence="0.99042">
4.3 Advantages
</subsectionHeader>
<bodyText confidence="0.99974627027027">
Finally, we would like to point out the advantages
of low-dimensional LSI analysis for local query re-
gion. Our results compared with VSM and global
LSI show clearly that local LSI with low dimensions
performs much better than VSM under some sample
sets and achieves the comparable IR performance to
global LSI. Additionally, because the largest singu-
lar vectors are essential for retrieval performance on
the local query regions, local LSI approaches the
computational complexity of global LSI by using
such small SVD dimensions. Despite the fact that
local LSI has increased the cost of separate SVD
computation for each query, the relative modest re-
quirements of SVD dimension make it feasible for
large scale IR task.
Compared with the local RF method, both the lo-
cal LSI and local RF achieve better results by pro-
viding high-centralized relevant information in the
local region. Provided that relevant sample docu-
ments are used with the same number, local RF is
able to make use of the combination of document
vectors and a heuristic procedure to improve IR per-
formance, while local LSI makes use of SVD to
extract the useful information from the information
space. In some sense, this SVD method is more
comprehensive than local RF.
gion for local LSI so as to automatically deter-
mine it.
2. Find a more efficient initial retrieval method
for obtaining high quality sample sets of each
query.
Table 3: comparative results of Local LSI and Local
relevance feedback on the local region organized by
the return sets of VSM on Med., Cran. and NTCIR,
respectively. The SVD dimension value for the local
LSI is the one from which the best IR performance
was obtained at the specific sample size.
</bodyText>
<figure confidence="0.996486777777778">
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac.
LLSI LRF LLSI LRF
3 2 0.5858 0.5977 0.5760 0.5816
5 1 0.6417 0.6243 0.6300 0.6198
10 1 0.6577 0.6152 0.6431 0.6093
20 1 0.6764 0.6044 0.6393 0.5845
30 1 0.6598 0.5854 0.6246 0.5699
40 2 0.6514 0.5776 0.6157 0.5722
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac.
LLSI LRF LLSI LRF
3 2 0.4524 0.4528 0.4206 0.4186
5 2 0.4443 0.4403 0.4203 0.4145
10 2 0.4357 0.4327 0.3981 0.3988
20 2 0.3993 0.4269 0.3571 0.3870
30 2 0.3782 0.4252 0.3345 0.3915
40 2 0.3464 0.4232 0.3106 0.3873
#ss. svd 11 ppt. Ave. P-R R-p
(s) fac.
LLSI LRF LLSI LRF
3 2 0.2367 0.2346 0.2380 0.2297
5 2 0.2292 0.2302 0.2341 0.2347
10 2 0.2119 0.2249 0.2205 0.2404
20 2 0.1728 0.2110 0.1800 0.2203
30 2 0.1458 0.2026 0.1575 0.2208
40 2 0.1404 0.1978 0.1470 0.2171
</figure>
<sectionHeader confidence="0.918629" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99997525">
In this paper, the results show that very low-
dimensional LSI on the local query region performs
IR task well. Such small dimensional requirements
of local LSI make it more attractive, enabling us to
better address the computation complexity. We can
perform the low-dimensional LSI on several known
relevant document spaces to obtain significant im-
provements in retrieval performance. Moreover,
provided that several relevant sample documents are
available, local LSI using small dimensions obtains
results comparable to the local RF although in a dif-
ferent manner. Our future work will:
</bodyText>
<listItem confidence="0.449533">
1. Continue to study the optimal size of local re-
</listItem>
<sectionHeader confidence="0.956785" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<reference confidence="0.5977965">
This work was supported in The 21st Century COE
Program ”Intelligent Human Sensing”, from the
ministry of Education, Culture,Sports,Science and
Technology.
</reference>
<sectionHeader confidence="0.946625" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999330794117647">
M. W. Berry, Zlatko Drmax, and Elizabeth R. Jessup.
1999. Matrix, vector space, and information retrieval
(technical report). SIAM Review, 41:335–362.
S. T. Dumais. 1996. Using for information filtering:
Trec-3 experiments. In In Donna K. Harman, editor,
The 3rd Text Retrieval Conference (TREC-3), pages
282–291. Department of Commerce, National Institute
of Standards and Technology.
J. Fan and M. L. Littmen. 2000. Approximate dimension
equalization in vector based information retrieval. In
Proceedings of the Seventeenth International Confer-
ence on Machine Learning. Morgan-Kauffman.
W. B. Frakes and R. Baeza-Yates. 1992. Information
retrieval - Data Structure Algorithms. Prentice Hall,
Englewood Cliffs, New Jersey 07632.
D. Hull. 1994. Improving text retrieval for the routing
problem using latent semantic indexing. In In pro-
ceedings of the 17th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 282–291. Association for com-
puting Machnery.
N. Kando. 2001. Clir syetem evaluation at ntcir work-
shop. National Information (NII) Japan.
J. Rocchio. 1971. Relevance feedback in information
retrieval. In The Smart Retrieval System-Experiments
in Automatic Document Processing, pages 313–323.
Englewood Cliffs, NJ, 1971, Prentice-Hall, Inc.
G. Salton and M. J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill, New York,
NY.
J. Xu and W. B. Croft. 2000. Improving the effec-
tiveness of informational retrieval with local context
analysis. ACM Transactions on Information Systems
(TOIS), 18(1).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891584">
<title confidence="0.999883">Very Low-Dimensional Latent Semantic Indexing for Local Query Regions</title>
<author confidence="0.999073">Yinghui Xu Kyoji</author>
<affiliation confidence="0.996011">Toyohashi Unversity of Technology Dept. of Information and Computer</affiliation>
<address confidence="0.997867">1-1, Hibarigaoka, Toyohashi,</address>
<email confidence="0.95141">xyh@ss.ics.tut.ac.jpumemura@tutics.tut.ac.jp</email>
<abstract confidence="0.996416705882353">In this paper, we focus on performing LSI on very low SVD dimensions. The results show that there is a nearly linear surface in the local query region. Using low-dimensional LSI on local query region we can capture such a linear surface, obtain much better performance than VSM and come comparably to global LSI. The surprisingly small requirements of the SVD dimension resolve the computation restrictions. Moreover, on the condition that several relevant sample documents are available, application of low-dimensional LSI to these documents yielded comparable IR performance to local RF but in a different manner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported</title>
<booktitle>in The 21st Century COE Program ”Intelligent Human Sensing”, from the ministry of Education, Culture,Sports,Science and Technology.</booktitle>
<marker></marker>
<rawString>This work was supported in The 21st Century COE Program ”Intelligent Human Sensing”, from the ministry of Education, Culture,Sports,Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>Zlatko Drmax</author>
<author>Elizabeth R Jessup</author>
</authors>
<title>Matrix, vector space, and information retrieval (technical report).</title>
<date>1999</date>
<journal>SIAM Review,</journal>
<pages>41--335</pages>
<contexts>
<context position="3431" citStr="Berry et al., 1999" startWordPosition="549" endWordPosition="552">ative results with local RF are provided in this paper as well. The experiments show that local LSI with one or two SVD dimensions can contribute to expanding the query information in a manner different from traditional local RF. This paper is organized as follows: Section 2 reviews existing related techniques. Section 3 describes the implementation architecture of the experiments and gives the experiment results. Section 4 explains the result and points out characteristic of the local LSI. Section 5 draws the conclusions. 2 Related works 2.1 Latent Semantic Indexing Latent semantic indexing (Berry et al., 1999) is one kind of vector-based query-expansion methods that use neither terms nor documents as the orthogonal basis of a semantic space. Instead, it computes the most significant orthogonal dimensions in the term-document matrix of the corpus, via SVD, and projects documents into the low rank subspace thus found. LSI then computes semantic similarity based on the proximity among projected vectors. LSI uses SVD to factor the term-document training matrix A into three factors: A = UEVT = Udiag(a1, Q2, · · · , Qn)VT Where U = (u1, u2, ··· , um)  m×m and V = (v1, v2, · · · , vn)  n×n are unitary</context>
</contexts>
<marker>Berry, Drmax, Jessup, 1999</marker>
<rawString>M. W. Berry, Zlatko Drmax, and Elizabeth R. Jessup. 1999. Matrix, vector space, and information retrieval (technical report). SIAM Review, 41:335–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Dumais</author>
</authors>
<title>Using for information filtering: Trec-3 experiments. In</title>
<date>1996</date>
<booktitle>The 3rd Text Retrieval Conference (TREC-3),</booktitle>
<pages>282--291</pages>
<editor>In Donna K. Harman, editor,</editor>
<contexts>
<context position="1382" citStr="Dumais, 1996" startWordPosition="203" endWordPosition="204">of low-dimensional LSI to these documents yielded comparable IR performance to local RF but in a different manner. 1 Introduction The increasing size of searchable text collection poses a great challenge to performing the information retrieval (IR) task. Latent Semantic Indexing (LSI) is an enhancement of the familiar Vector Model of IR. It satisfies the IR task through discovering corpus-wide word relationship based on cooccurrence analysis of a whole collection. LSI has been successfully applied to various document collections and has achieved favorable results, sometimes outperforming VSM (Dumais, 1996). However, the principal challenges to applying LSI to large data collections are the cost of computing and storing SVD. Local analysis of the information in a set of topranked documents for the query is one promising way to solve the computationally demanding IR task for a large collection. To solve the computational complexity of LSI, David Hull introduced one interesting method, local LSI, for routing problems(Hull, 1994). The basic idea is: apply the SVD to a set of documents known to be relevant to the query; then all the documents in the collection can be folded into the reduced space of</context>
</contexts>
<marker>Dumais, 1996</marker>
<rawString>S. T. Dumais. 1996. Using for information filtering: Trec-3 experiments. In In Donna K. Harman, editor, The 3rd Text Retrieval Conference (TREC-3), pages 282–291. Department of Commerce, National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fan</author>
<author>M L Littmen</author>
</authors>
<title>Approximate dimension equalization in vector based information retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning. Morgan-Kauffman.</booktitle>
<contexts>
<context position="7814" citStr="Fan and Littmen, 2000" startWordPosition="1346" endWordPosition="1349">D� In this paper, D/rel the experiment results based on the local RF were performed for comparing with the results of Local LSI. The terms in the query are reweighted using the Rocchio formula with a : Q : -y = 1 : 1 : 0. As for the local information relevant to the query, they were obtained by extracting several top-ranked documents through the VSM retrieving process in the experiments. Jiang has ever used the similar experiments (VSM+LSI) for Local LSI in his pa6. Back to the step 2 and continue the analysis on the next query in the same way. per “Approximate Dimension Reduction at NTCIR” (Fan and Littmen, 2000). local LSI Figure 1: Implementation architecture 3 Experiment Set-up and Results 3.1 Implementation Architecture Figure 1 shows overall the architecture of the experiment. The procedure is described as follows: 1. Indexing the document collection and query sets 2. Given a query, retrieving some document by the relevant sets. In some cases the relevant sets are derived from the known relevant documents and in other cases we regarded the top returned documents as the relevant sets. 3. performing the singular value decomposition on documents identified in 2. 4. Only a few dimensions for the LSI </context>
</contexts>
<marker>Fan, Littmen, 2000</marker>
<rawString>J. Fan and M. L. Littmen. 2000. Approximate dimension equalization in vector based information retrieval. In Proceedings of the Seventeenth International Conference on Machine Learning. Morgan-Kauffman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Frakes</author>
<author>R Baeza-Yates</author>
</authors>
<title>Information retrieval - Data Structure Algorithms.</title>
<date>1992</date>
<pages>07632</pages>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey</location>
<contexts>
<context position="8803" citStr="Frakes and Baeza-Yates, 1992" startWordPosition="1503" endWordPosition="1506">are derived from the known relevant documents and in other cases we regarded the top returned documents as the relevant sets. 3. performing the singular value decomposition on documents identified in 2. 4. Only a few dimensions for the LSI are retained. 5. Projecting the document vectors and the query vector into the user-cared feature space, and then using the standard Cosine measure to get the final score for this query. Step 1 is pre-processing procedure for IR system. Only the tag removal, upper case characters transverse, stoplist removal and Porter’s stemming were adopted in this phase (Frakes and Baeza-Yates, 1992). Next, the smart “ltc” term weighting scheme (Salton and McGill, 1983) was used to compute the entries of the term document matrix for the collection and entries of the query vector. The second step can be regarded as filter container. In this paper, the three kinds of routine schemes were performed. In the first case, the local space for each query was represented simply by all document vectors, which have already been judged to be relevant (appearing in the relevant judgment file). We note that although it is an ideal case, it may form a useful upper bound on performance. In the second case</context>
</contexts>
<marker>Frakes, Baeza-Yates, 1992</marker>
<rawString>W. B. Frakes and R. Baeza-Yates. 1992. Information retrieval - Data Structure Algorithms. Prentice Hall, Englewood Cliffs, New Jersey 07632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Improving text retrieval for the routing problem using latent semantic indexing. In</title>
<date>1994</date>
<booktitle>In proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>282--291</pages>
<contexts>
<context position="1810" citStr="Hull, 1994" startWordPosition="273" endWordPosition="274">rrence analysis of a whole collection. LSI has been successfully applied to various document collections and has achieved favorable results, sometimes outperforming VSM (Dumais, 1996). However, the principal challenges to applying LSI to large data collections are the cost of computing and storing SVD. Local analysis of the information in a set of topranked documents for the query is one promising way to solve the computationally demanding IR task for a large collection. To solve the computational complexity of LSI, David Hull introduced one interesting method, local LSI, for routing problems(Hull, 1994). The basic idea is: apply the SVD to a set of documents known to be relevant to the query; then all the documents in the collection can be folded into the reduced space of those relevant documents. By concentrating on the local space around the query results, we may be able to compute using flexible and efficient LSI algorithms. In this paper we put much emphasis on local dimensionality analysis of the local query regions filled with relevant documents. In ideal experimental cases, local LSI involves only the documents known to be relevant to the query. To our surprise, in most of our experim</context>
</contexts>
<marker>Hull, 1994</marker>
<rawString>D. Hull. 1994. Improving text retrieval for the routing problem using latent semantic indexing. In In proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 282–291. Association for computing Machnery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kando</author>
</authors>
<title>Clir syetem evaluation at ntcir workshop. National Information</title>
<date>2001</date>
<location>(NII)</location>
<contexts>
<context position="10201" citStr="Kando, 2001" startWordPosition="1748" endWordPosition="1749"> use of the top returned items from VSM is similar to blind feedback or pseudo RF. 3.2 Characteristic of test collection There are three test collections in our experiments. Two of them, Cranfield and Medlars, are small. The third one is a large-scale test collection, NACSIS. The Cranfield corpus consists of 1,400 documents on aerodynamics and 225 queries, while Medlars consists of 1,033 medical abstracts and 30 queries. Although these two collections are very small, they were used extensively in the past by IR researchers. As for the NACSIS test collection for the IR 1 &amp; 2 (NTCIR 1&amp;NTCIR 2) (Kando, 2001), these documents are abstracts of academic papers presented at meetings hosted by 65 Japanese scientists and linguists. In our experiments, the English Monolingual IR was performed. This collection consists of approximately 320,000 English documents in NTCIR1 and NTCIR-2. 3.3 Local Routine Experiments (Ideal Case) We first present the experimental results on the ideal condition. The document vectors already judged to be relevant to the query were used. SVD calculation are performed on the local region organized by topic set (query) document set (corpus) \HV Qids ( ) 1 ≤ s ≤ k query vector(g,)</context>
</contexts>
<marker>Kando, 2001</marker>
<rawString>N. Kando. 2001. Clir syetem evaluation at ntcir workshop. National Information (NII) Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<booktitle>In The Smart Retrieval System-Experiments in Automatic Document Processing,</booktitle>
<pages>313--323</pages>
<publisher>Prentice-Hall, Inc.</publisher>
<location>Englewood Cliffs, NJ,</location>
<contexts>
<context position="5970" citStr="Rocchio, 1971" startWordPosition="1029" endWordPosition="1030">f its constituent terms. For document vector di and query vector qi, ATk d� and AT K4 are now the LSI vector representations of that document and query, respectively, in the reduced-dimension vector space. This process is known as “folding in” documents (or queries) into the training space. Actually, LSI assumes that the semantic associations among terms can be found through this one-step analysis of their statistical usage in the collection, and they are implicitly stored in the singular vectors computed by SVD. 2.2 Relevance Feedback A feedback query creation algorithm developed by Rocchio (Rocchio, 1971) in the mid-1960s has, over the years, proven to be one of the most successful profile learning algorithms. The algorithm is based upon the fact that if the relevance for a query is known, an optimal query vector will maximize the average query-document similarity for the relevant documents, and will simultaneously minimize the average query-document similarity for non-relevant documents. Rocchio shows that an optimal query vector is the difference vector among the centroid vectors for the relevant and non-relevant documents. Qo = R EDRel. D − N � R ED/Rel. D where R is the number of relevan</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio. 1971. Relevance feedback in information retrieval. In The Smart Retrieval System-Experiments in Automatic Document Processing, pages 313–323. Englewood Cliffs, NJ, 1971, Prentice-Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="8874" citStr="Salton and McGill, 1983" startWordPosition="1514" endWordPosition="1517"> the top returned documents as the relevant sets. 3. performing the singular value decomposition on documents identified in 2. 4. Only a few dimensions for the LSI are retained. 5. Projecting the document vectors and the query vector into the user-cared feature space, and then using the standard Cosine measure to get the final score for this query. Step 1 is pre-processing procedure for IR system. Only the tag removal, upper case characters transverse, stoplist removal and Porter’s stemming were adopted in this phase (Frakes and Baeza-Yates, 1992). Next, the smart “ltc” term weighting scheme (Salton and McGill, 1983) was used to compute the entries of the term document matrix for the collection and entries of the query vector. The second step can be regarded as filter container. In this paper, the three kinds of routine schemes were performed. In the first case, the local space for each query was represented simply by all document vectors, which have already been judged to be relevant (appearing in the relevant judgment file). We note that although it is an ideal case, it may form a useful upper bound on performance. In the second case, we assume the condition that the user provides a reasonable number of</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>W B Croft</author>
</authors>
<title>Improving the effectiveness of informational retrieval with local context analysis.</title>
<date>2000</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="19869" citStr="Xu and Croft, 2000" startWordPosition="3467" endWordPosition="3470">een local LSI with the baseline VSM and global LSI. The 11ppt. average precision recall plots of local LSI were figured out for the three test collections in figure 3. The symbol s in the figure represents the sample size and k represents the SVD dimension. To our satisfactions, local LSI based query expansion method does much better than VSM and more closely approaches the global LSI. Next, to investigate the effectiveness of low dimensional LSI on local query region in restructuring the user cared information space, local RF with Rocchio’s weights a : Q : -y = 1 : 1 : 0, as in Xu and Croft (Xu and Croft, 2000), was used for comparison. Both of them were used on the same sample documents. The difference between them is a twofold one. First, the standard RF formula shown in section 2.2 make use of weighting parameters for query expansion, while our approach does not. Secondly, different combination object was used. The local RF experiment performed in this paper makes use of the centroid of the top s returned document vectors. In our approach, we combine the original query vector with its projecting results on the low local SVD space. Table 3 shows these results in terms of varying feedback size with</context>
</contexts>
<marker>Xu, Croft, 2000</marker>
<rawString>J. Xu and W. B. Croft. 2000. Improving the effectiveness of informational retrieval with local context analysis. ACM Transactions on Information Systems (TOIS), 18(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>