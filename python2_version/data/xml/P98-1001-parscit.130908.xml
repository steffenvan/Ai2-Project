<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990206">
A Quasi-Dependency Model for Structural Analysis
of Chinese BaseNPs*
</title>
<author confidence="0.989387">
Zhao Jun Huang Changning
</author>
<affiliation confidence="0.950721666666667">
Department of Computer Science &amp; Technology,
The State Key Lab of Intelligent Technology &amp; Systems,
Tsinghua University, Beijing, China, 100084
</affiliation>
<email confidence="0.99163">
Email: zj@s1000e.cs.tsinghua.edu.cn, Hcn@tsinghua.edu.cn
</email>
<bodyText confidence="0.9979364">
Abstract: The paper puts forward a quasi-
dependency model for structural analysis of Chinese
baseNPs and a MDL-based algorithm for quasi-
dependency-strength acquisition. The experiments
show that the proposed model is more suitable for
Chinese baseNP analysis and the proposed MDL-
based algorithm is superior to the traditional ML-
based algorithm. The paper also discusses the
problem of incorporating the linguistic knowledge
into the above statistical model.
</bodyText>
<sectionHeader confidence="0.997045" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.991799904761905">
The concept of baseNP is initially put forward
by Church. In English, baseNP is defined as
&apos;simple non-recursive noun phrases&apos;, which means
that there is no sub-noun-phrases contained in a
baseNP[1]. But the definition can not meet the
needs in Chinese information retrieval. The noun
phrases such as &amp;quot; PA (natural) W, (language)
J1 (process)&amp;quot;, &amp;quot;IE fl (Asian) a (finance) fe
tIt(crisis)&amp;quot; and &amp;quot;i(political) M(system)
*(reformation) AK (process)&amp;quot; are critical for
information retrieval, but they are not non-
recursive noun phrases.
In Chinese, the attribute of noun phrases can be
classified into three types, that is restrictive
attributes, distinctive attributes and descriptive
attributes, among which the restrictive attributes
have agglutinative relation with the heads. The
paper defines the Chinese baseNP using the
restrictive attributes.
[ Definition 1 Chinese baseNP (hereafter
abbreviated as baseNP):
</bodyText>
<equation confidence="0.9923498">
baseNP baseNP + baseNP
baseNP baseNP + N I VN
baseNP restrictive-attribute + baseNP
baseNP restrictive-attribute + N I VN
restrictive-attribute — AIBIVINISIXI
</equation>
<bodyText confidence="0.959707">
(M+Q)
Where, the terminal symbols A, B, V, N, VN, S, X,
M, Q stand for respectively adjective, distinctives,
verbs, nouns, norminalized verbs, locatives, non-
Chinese string, numerals and quantifiers.
According to the definition, noun phrases
falls into baseNPs and non-baseNPs (abbreviated
as —baseNP). Table-1 gives some examples.
</bodyText>
<tableCaption confidence="0.705749">
Table-1: Examples of baseNP and —baseNP
</tableCaption>
<table confidence="0.989793428571429">
Type Examples
BaseNP .&amp;quot;-&apos;1:11/air t re/corridor
BaseNP OM/politics 12M/system aJt*/reform
BaseNP ffi I=1/export 4A/commodity **/price Mga/index
— baseNP k/complicated fr‘Vde 14ff/feature
— baseNP i3f9/research —/and Ra/development
— baseNP ZIJT/teacher /write 1n/de it-sg/comment
</table>
<bodyText confidence="0.952401833333333">
Both baseNP recognition and baseNP structural
analysis are basic tasks in Chinese information
retrieval. The paper mainly discusses the problems
in structural analysis of baseNPs, which is
essential for generating the compositional
indexing units from a baseNP. The task of baseNP
</bodyText>
<footnote confidence="0.962612">
&apos;The research is supported by the key project of the National Natural Science Foundation
</footnote>
<page confidence="0.852655">
1
</page>
<bodyText confidence="0.999948956521739">
structural analysis is to determine the syntactic
structure of a baseNP. In this paper, we use
dichotomy for baseNP analysis. For example, the
structure of &amp;quot; 11 /natural rilanguage th
/process&amp;quot; is &amp;quot;( A/natural I-g/language) k12a
/process&amp;quot;. Obviously, a baseNP composed of three
or more than three words has syntactic ambiguities.
For example, baseNP &amp;quot;x y z&amp;quot; has two possible
structures, that is &amp;quot;(x y) z&amp;quot; and &amp;quot;x (y z)&amp;quot;. The task
of baseNP structural analysis is to select the
correct structure from the possible structures.
The paper mainly discusses the problems
related to Chinese baseNP structural analysis.
Section 2 puts forward a quasi-dependency model
for structure analysis of Chinese baseNPs. Section
3 gives an unsupervised quasi-dependency-
strength estimation algorithm based on the
minimum description length (MDL) principle.
Section 4 analyzes the performance of the
proposed model and the algorithm. Section 5
discusses some issues in the implementation of
baseNP structure analysis and quasi-dependency-
strength estimation. Section 6 is the conclusion.
</bodyText>
<sectionHeader confidence="0.869902" genericHeader="method">
2. The quasi-dependency model
</sectionHeader>
<bodyText confidence="0.999646903225806">
There are two kinds of structural analysis
models for English noun phrase, that is adjacency
model and dependency model. The research of
Lauer shows that the dependency model is
superior to the adjacency model for structural
analysis of English noun phrase[2]. However,
there is no model for structural analysis of Chinese
baseNP till now.
According to the dependency grammar, two
constituents can be bound together they are
determined to be dependent. The determination of
the dependency relation between two constituents
is composed of two steps. The first step is to
determine whether they have the possibility to
constituent dependency relation. The second step
is to determine whether they have dependency
relation in the given context. The former is called
the quasi-dependency-relation, which can be
acquired from collocation dictionaries or corpora.
The determination of the latter is difficult, because
multiple information in the given context should
be taken into consideration, such as syntax or
semantics information, etc.
[Definition 2] Quasi-Dependency-Relation: If
two words x and y have the possibility to
constituent dependency relation, then we say that
they have quasi-dependency-relation in the given
baseNP, formulated as x—y (where y is called the
head) or y x (where x is called the head);
Otherwise, we say that they have no quasi
dependency relation, formulated as x y and
</bodyText>
<listItem confidence="0.4677696">
y x .
[ Assumption I] In a Chinese baseNP, if two
words x and y can constituent dependency relation,
then the head is always the post-positon word y,
that is x—y.
</listItem>
<bodyText confidence="0.9925088">
According to the Definition 1, there is no
preposition phrase, verb phrase, locality phrase or
)-structure in a baseNP, so assumption-1 is
reasonable.
On the basis of assumption-1, we put forward
the quasi-dependency model for structural analysis
of Chinese baseNPs.
There are the following 3 kinds of quasi-
dependency-pattern for a tri-word-composed
baseNP xyz.
</bodyText>
<equation confidence="0.983953571428571">
X
y S31
S31 =(x y) z
X
S32
s32=x (y z)
S33
</equation>
<bodyText confidence="0.999723875">
Where, pattern s31 means x—y , y—z and x z,
which corresponds to structure (x y) z; pattern S32
means x—z, and x y, which corresponds
to the structure x (y z); However, the quasi-
dependency-strength must be used to determine
the corresponding structure for pattern s33, which
means x—y , y—z and x•-•z. For example, as for
baseNP &amp;quot; Itll &apos;Xi /politics &apos;pi /system a3z X
/reform&amp;quot;, there are quasi-dependency-relations &amp;quot;gt
Ki /politics— &amp;I/system&amp;quot;, &amp;quot;IA it /politics
/reform&amp;quot; and &amp;quot;PM/system— /reform&amp;quot;. If we
know that the quasi-dependency-relations &amp;quot;KM
/politics—PM/system&amp;quot; and &amp;quot;4$Msystem— a/*
/reform&amp;quot; are stronger than &amp;quot;a M/politics—
/reform&amp;quot;, the structure of the baseNP can be
determined to &amp;quot;(i&apos;/politics 1441/system) aX
</bodyText>
<page confidence="0.955587">
2
</page>
<bodyText confidence="0.893347333333333">
/reform&amp;quot;.
In the following, we give the definition of
quasi-dependency-strength and the formula for
determining the syntactic structure of baseNPs
based on the quasi-dependency-strengths.
[Definition 3 quasi-dependency-strength: Given
a baseNP set NP={npl,np2,...,npm) and lexicon
W={w„...,ws.} , Vwi, w E W • the quasi-
dependency-strength of is defined as:
</bodyText>
<equation confidence="0.97239575">
Edep(w, -+ wi,npk)
npkeNP
ds(w, w,) —
Eco(w, w ,npk)
</equation>
<listItem confidence="0.583293">
• npkeNP
</listItem>
<bodyText confidence="0.998571714285714">
where dep(wi —&gt; w ,np k) is the count of
dependent word pair w,—wi contained in npk,
co(w„Ivj ,np k) is the count of cooccurent word
pair (w, 19 contained in npk.
The formula for determining the syntactic
structure of baseNP based on the quasi-
dependency-strengths is as follows.
</bodyText>
<equation confidence="0.95510275">
ds(u
(u—o,v)ED(np,,Ak)
Ids(u v)+ E ds(u —&gt; v)
(u—■r)ED(np,,Ak) (u--&gt;v)eD(np,,A,)
</equation>
<bodyText confidence="0.953870222222222">
Where, belief (s I npj) represents the belief in
which the structure of np, is sj. D(np,sj) represents
the set of quasi-dependency-relations included in
the quasi-dependency-pattern corresponding to
structure s.
A tri-word-composed baseNP has two possible
syntactic structures, that is s31 and s32. Similarly, a
four-word-composed baseNP has the following
five possible structures.
</bodyText>
<equation confidence="0.999237384615385">
belief (s j(nfl,)=
x y z x y z
,/ X X X X X ,/
,/ X x X ,/ ,/
Y
,/ ,/
X
X
,/
X
X
X
s41 = ((WX)Y)Z s42 (WX)(YZ) 343 = (W(XY))Z .544 = W((lY)Z) s45 = w(x(Yz))
</equation>
<bodyText confidence="0.999832166666667">
In summary, we can compute the belief in
which the structure of np, is sj using the
correspondence between the quasi-dependency-
pattern and the baseNP structure. The acquisition
of quasi-dependency-strength between words is
the critical problem.
</bodyText>
<sectionHeader confidence="0.972784" genericHeader="method">
3. The acquisition of quasi-
dependency-strength between words
</sectionHeader>
<bodyText confidence="0.989479741935484">
If we have a large scale baseNP annotated
corpus in which the baseNPs have been assigned
the syntactic structures, the quasi-dependency-
strength between words can be acquired through a
simple statistics. However, such an annotated
corpus is not available. We only have a baseNP
corpus which has no structural information. How
to acquire the quasi-dependency-strength from
such a corpus is the main task of the section.
Given a baseNP set NP={np„np2,...,npm) and a
lexicon W={w,,w2,...,wm), the problem can be
described as learning a quasi-dependency-strength
set G (abbreviated as model) from the training set.
Where, G = Idsjj = ds(wi ),Vwe , wi E W}
Zhai Chengxiang puts forward an unsupervised
algorithm for acquiring quasi-dependency-strength
from noun phrase set[3]. The algorithm is derived
from the EM algorithm. Because the algorithm is
based on the maximum likelihood (ML) principle,
it usually leads to overfitness between the data and
the model[4]. For example, given a simple baseNP
set NP={gVM/politics PM/system al */reform,
4/economics •f21K Wsystem a*/reform, lKEI
/politics PIK m /system * /revolute , 4
/economics 4K$1,l/system /revolute}, there are
sixteen possible models for the training set, among
them G., G7, G10 and G13 have the best fitness to
NP, that is Num(NPIG)=6. However, in the
linguistic view, GI is the correct model, though it
has lower fitness to NP, that is Num(NPIG)=4 (see
the appendix).
</bodyText>
<subsectionHeader confidence="0.543252">
3.1 The estimation of the quasi-dependency-
strength under Bayesian framework
</subsectionHeader>
<bodyText confidence="0.992116666666667">
In Bayesian framework, the task of acquiring
the quasi-dependency-strength can be described as
the problem of selecting G which has the highest
</bodyText>
<page confidence="0.87382">
3
</page>
<equation confidence="0.8637175">
posterior probability p(G INP).
G = arg max p(G I NP)
</equation>
<bodyText confidence="0.990108">
According to Bayesian theorem, we have the
following inference.
</bodyText>
<equation confidence="0.90424">
G = arg max p(NP I G)p(G)
p(NP)
= arg max p(NP I G)p(G)
</equation>
<bodyText confidence="0.999884222222222">
Besides using conditional probability p(NP(G)
to measure the fitness between the training set and
the model G, Bayesian modeling gives additional
consideration to the generality of the model
through the prior probability p(G), that is simpler
model has higher probability. The central idea of
Bayesian modeling is to find a compromise
between the goodness of fit and the simplicity of
the model.
</bodyText>
<subsectionHeader confidence="0.965945">
3.2 Defining the evaluation function of Bayesian
modeling using MDL principle
</subsectionHeader>
<bodyText confidence="0.999982857142857">
The difficulty in Bayesian modeling is the
estimation of the prior probability p(G). According
to the coding theory, the lower bound of the
coding length (bit-string) of an information with
probability p is log2 lip [5]. The theorem
connects Bayesian modeling with the MDL
principle in the coding theory.
</bodyText>
<equation confidence="0.9653856">
G = arg max p(NP I G)p(G)
= arg min {- log, [p(NP I G)p(G)])
= arg min (In 2 + log2
p(NP I G) p(G)}
= arg min {L(NP I G)+ L(G))
</equation>
<bodyText confidence="0.996025111111111">
Where, L(a) is the optimal coding length of
information a. Specially, L(NPIG) is called the
data description length and L(G) is called the
model description length.
Therefore, the problem of estimating the prior
probability p(G) and the conditional probability
p(NPIG) is converted to the problem of estimating
the model description length L(G) and the data
description length L(NPIG).
</bodyText>
<subsectionHeader confidence="0.9572155">
3.3 The MDL-based quasi-dependency-strength
estimation algorithm
</subsectionHeader>
<bodyText confidence="0.99113075">
In MDL principle, the modeling problem can be
viewed as a problem of finding a model G which
has the smallest sum of the data description length
and the model description length. Because the
search space is huge, we can not find the optimal
model in a transversal manner. The model must be
improved in an iterative manner in order to arrive
at a minimum description length.
In the research, the model is composed of the
quasi-dependency-strength ds(w -+ w j) , where
each ds(w, —&gt; w) can be decomposed into two
parts: Othe structure part: the quasi-dependency-
relation (Iv; —&gt; wi ) ; ©the parameter part: the
quasi-dependency-strength ds. Therefore, the
learning process is divided into two steps: 0
Keeping the structure part fixed, optimize the
parameter part; © Keeping the parameter part
fixed, optimize the structure part. The two steps go
on alternately until the process arrives at a
convergent point.
</bodyText>
<sectionHeader confidence="0.459029" genericHeader="method">
Algorithm 1: The MDL-based algorithm for quasi-dependency-strength estimation
</sectionHeader>
<subsectionHeader confidence="0.343399">
©Initialize model G;
</subsectionHeader>
<bodyText confidence="0.9600435">
©Let L = L(NP I G) + L(G) ,G = (G s ,Gp),where Gs and Gp represent respectively the structure part and the
parameter part. Execute the following two steps alternately, until L converged.
</bodyText>
<listItem confidence="0.9996305">
• Keeping Gs fixed, optimize Gp, until L(NP I G) converges, that is L converges;
• Keeping Gp fixed, optimize G„ until L(G) converges, that is L converges.
</listItem>
<bodyText confidence="0.9967785">
On condition that the structure part of the model
is fixed, the parameter optimization means to find
the optimal sets of quasi-dependency-strength in
order that the data description length minimized,
</bodyText>
<equation confidence="0.734324333333333">
that is
G = arg min L(NP G)
Where L(NPIG) is the optimal coding length of NP
</equation>
<page confidence="0.959066">
4
</page>
<bodyText confidence="0.967947454545454">
when G is known. the gradual reduction of data description length.
The parameter optimization step can be In MDL principle, the model description length
implemented using EM algorithm[3]. In the can be gradually reduced through the modification
process of parameter optimization, the structure of the structure part of the model, therefore the
part of the model is kept fixed. The optimum overall description length of the model is reduced.
estimates of the parameters are obtained through
Algorithm 2: The structure optimization algorithm
Let the model after the parameter optimization process is G, which is composed of the quasi-
dependency-strength ds(w, w).
©Sort the quasi-dependency-strengths of model G in ascending order, that is dslii, dsm, ds131,
©Repeat the following steps, until [L(NP I + L(G. )]—[L(NP I G} + L(G)] &lt;= ThL(Thi, is the selected
</bodyText>
<listItem confidence="0.89187">
threshold). Let i=1,
• Delete the quasi-dependency-strength el from model G;
• Construct the new model G&apos; ;
• If [L(NP I ) + L(G. )1 —[L(NP G)-1- L(G)] &lt;= Th, Then the cycle ends Else let G = , i=i+1 and
continue the next cycle.
</listItem>
<sectionHeader confidence="0.456733" genericHeader="method">
4. The performance analysis
</sectionHeader>
<bodyText confidence="0.872289266666667">
This section takes the N2+N2+N2-type (where N2
represents bi-syllable noun) baseNPs as the testing
data in order to discuss the performance of the quasi-
dependency-based model for structural analysis of
baseNPs and the MDL-based algorithm for quasi-
dependency-strength acquisition. The training set
includes 7,500 N2+N2+N2-type baseNPs. The close
testing set is the 500 baseNPs included in the
training set. The open testing set is the 500 baseNPs
outside the training set. The testing target is the
precision of baseNP structural analysis, that is
precision = —a X 100%;
Where a is the count of the baseNPs which are
correctly analyzed, b is the count of the baseNPs in
the tesing set.
</bodyText>
<subsectionHeader confidence="0.86621">
4.1 The performance of the quasi-dependency
model
</subsectionHeader>
<bodyText confidence="0.991742947368421">
The experiments shows: (Din the N2+N2+N2-
type baseNPs, the left-binding structure is about two
times of the right-binding structure; ©The analysis
precision of the quasi-dependency model is about
7% higher than that of the adjacency model. This
conclusion can be explained intuitively through the
following example. The structure of baseNP &amp;quot;g±
/doctor it3t/dissertation MI/outline&amp;quot; can not be
correctly determined through the adjacency model,
because we can not find that the dependency strength
of &amp;quot;4±/doctor it/dissertation&amp;quot; is stronger than
that of &amp;quot;/dissertation AM/outline&amp;quot;. In the other
hand, the structure of the above baseNP can be
determined to &amp;quot;( ±/doctor it3Z/dissertation)
ffl/outline&amp;quot; through the quasi-dependency model,
because both &amp;quot;14±/doctor it3t/dissertation&amp;quot; and &amp;quot;
it/dissertation ffl/outline&amp;quot; are dependent word
pairs, while &amp;quot;ii ± /doctor /outline&amp;quot; is an
independent word pair. Table 2 is the testing result.
</bodyText>
<tableCaption confidence="0.348794">
Table-2: The analysis precision of N2+N2+N2- e baseNP
</tableCaption>
<table confidence="0.97952075">
Testing type Right-binding Left-binding _ .
Adjacency model Quasi-dependency model
Close test 31.5% 68.5% 84.6% 91.5%
Open test 32.7% 67.3% 81.5% 88.7%
</table>
<page confidence="0.991122">
5
</page>
<bodyText confidence="0.947824117647059">
overfitness problem inherent in the ML algorithm is
solved to a great extent. In the following, the
performance of the ML algorithm and the MDL
algorithm are compared through comparing the
baseNP analysis precision of the models constructed
using the above two algorithms. The precision is
listed in Table-3. The experiment shows that the
MDL algorithm is superior to the ML algorithm.
4.2 The performance of the MDL-based
algorithm for quasi-dependency-strength
acquisition
The ML algorithm is equivalent to the first
parameter optimization process of the MDL
algorithm. The MDL process is composed of two
iterative optimization steps. In the iterative process,
the parameters are optimized gradually and the
model is simplified gradually as well. Therefore, the
</bodyText>
<tableCaption confidence="0.655498">
Table-3: The performance of ML algorithm and MDL algorithm
</tableCaption>
<table confidence="0.9915935">
BaseNP analysis precision
Close test Open test
ML algorithm MDL algorithm ML algorithm MDL algorithm
89.0% 91.5% 82.5% 88.7%
</table>
<sectionHeader confidence="0.983344" genericHeader="method">
5. Implementation issues
</sectionHeader>
<bodyText confidence="0.998029">
The most difficult problem related to the
structural analysis of baseNPs is the acquisition of
the quasi-dependency-strength. The proposed
algorittun(Algorithm 2) is an unsupervised
algorithm, that is the parameters are estimated
over the baseNP corpus which has no structural
information. In order to improve the estimation
results and speed up the iteration process, some
measures are taken during the implementation.
</bodyText>
<subsectionHeader confidence="0.959307">
5.1 The pre-assignment of the baseNP structure
</subsectionHeader>
<bodyText confidence="0.9963664">
The structures of some baseNPs can be
determined using the linguistic knowledge. Such
knowledge includes:
0 In a baseNP, a word pair which has the
following syntactic composition is independent.
</bodyText>
<listItem confidence="0.974282375">
• Noun+Adjective: for example, &amp;quot; ffl
/ground/Noun I 0 /complicated/Adjective #
/condition&amp;quot;, &amp;quot;WA/glass/Noun 4/curved/Adjective
V/pipe&amp;quot;;
• Noun+Distinctive: for example, &amp;quot;
/elementary-school/Noun g it /of-the-right-
age/Distinctive )Lt/child&amp;quot;;
• Distinctive+Verb: for example, &amp;quot;
</listItem>
<bodyText confidence="0.942613181818182">
/large/Distinctive it a/fight/Verb /plane&amp;quot;,
&amp;quot;tat/elementary/Distinctive Nff/creepNerb
t/animal&amp;quot;.
0 If two verbs cooccur in a baseNP, then they are
dependent. For example,&amp;quot; AM/prospect/Verb fit
/design/Verb) it /group &amp;quot; , &amp;quot; ( El /Anti-
Japanese/Verb sa/save-the-nation/Verb)
/campaign&amp;quot;.
If we preprocess the baseNP corpus using the
above knowledge, it is beneficial for the estimation
process.
</bodyText>
<subsectionHeader confidence="0.995618">
5.2 The complex-feature-based modeling
</subsectionHeader>
<bodyText confidence="0.999970285714286">
If the lexicon size is I WI, then the parameter
number of the above word-based acquisition
algorithm amounts to I WI&apos;. The enormous parameter
space will lead to the data sparseness problem during
the estimation. Therefore, the paper puts forward the
complex-feature-based acquisition algorithm. First,
map each word to a complex-feature-set according to
the multiple feature of the words; Then, acquire the
quasi-dependency-strength between the complex-
feature-sets. During analyzing the structure of a
baseNP, the strength between the complex-feature-
sets is used instead of that between the words. In the
research, the multiple features include part-of-speech,
number of syllables and word sense categories.
</bodyText>
<sectionHeader confidence="0.999055" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.999956444444444">
The paper put forward a quasi-dependency model
for structural analysis of Chinese baseNPs, and a
MDL-based algorithm for the quasi-dependency-
strength acquisition. The experiments show that the
proposed model is more suitable for Chinese baseNP
analysis and the proposed MDL-based algorithm is
superior to the traditional ML-based algorithm. The
further research will focus on incorporating more
linguistic knowledge into the above statistical model.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999799">
[1] Church K., A stochastic parts program and
noun phrase parser for unrestricted text, In:
Proceedings of the Second Conference on Applied
Natural Language Processing, 1988.
</reference>
<page confidence="0.984157">
6
</page>
<reference confidence="0.9847037">
[2] Lauer M. Conceptual association for compound
noun analysis, In: Proceedings of the 32&amp;quot;d Annual
Meeting of the Association for Computational
Linguistics, Student Session, Las Cruces, NM,
1994.
[3] Zhai Chengxiang, Fast Statistical Parsing of
Noun Phrases for Document Indexing, In:
Proceedings of the 35&apos; Annual Meeting of the
Association for Computational Linguistics, USA.:
Association for Computational Linguistics. 1997.
311-318.
[4] Stolcke A. Bayesian learning of probabilistic
language models, Dissertation for Ph.D. Degree,
Berkeley, California: University of California,
1994.
[5] Solomonoff R. The mechanization of linguistic
learning, In: Proceedings of the 2nd International
Conference on Cybernetics.
Appendix: An example for quasi-dependency-relation acquisition
No. Model G PI Fitness between G and NP Num( NFIG)
</reference>
<equation confidence="0.93691405">
I fititi**1(1), klgfl*$(1(1) 4 m*sfiA*(1), got(4mm*)(0) 4
i*mast*(I), i*omt(I) 0.141**14*(1), tIgF(4$1.1ak*)(0)
gatilak*(0), klait*(0) (amom)**(1), aitt(i*$.1Vo)(0)
am*.0(0), ogF**(0) (w**0**(1),OVF(121g$11**)(0)
4 Elmi**1(1),klgF1*$11(1) 6 (fAm441i)a.m(1), xszito*mak*) (0) 6
Pictet*(1), f*$11**(1) (tIgFitic$11)&amp;*(1), klgF(4441.1&amp;*)(0)
TAK■61*(0), klPia,M(0) Mit 4$1.1)*00), fitM(4*$11**)(1)
WO**(1), .t1.6(1) 0.1M440**(1), k.1(124411*.t)(1)
7 TAM*$1.1(1), klgf14c$11(1) 6 (ait1121411)&amp;*(1), fizit(10.1&amp;*) (0) 6
1441&amp;*(1), 4$11**(1) (gHt*S1J)&amp;*(1),1)
Wtiat*(0), klaakx(1) cami*$0**(1), fitit)(4sq**)(1)
am**(1), klgF*6(0) (!l)( I), kgF(14ctimio(0)
10 TAKI4m(1), klgi1*$11(1) 6 (fAit1441.1)a.t*(1), AM(WOJek*)(1) 6
i2141&amp;*(1), 4$11*-6(1) 0.1w*$4)&amp;*(1), klaci2ma.m)(0)
fitite,t*(1), tIgfak*(0) (EAM21K$11)**(1), 001-1(4$11**) (0)
zwmta(0), tIgf*-6(1) (kM1*$11)**(1), gla(f*Sq**)(1)
13 Oifil*M0), kIVF.4441(1) 6 (Z1M1*$1.1)&amp;*(1), gatt(4$11akT) (1) 6
fli$101M(1), 101.1*.ta(1) (gal*S11)&amp;*(1), glgF(1441a,M)(0)
WOO,M(1), ggFe.M(1) (RIK1{441)**(1), (*)O)000
am*-66(0), kla**(0) (.1g#m)**(1), klgF(10Mt)(0)
</equation>
<page confidence="0.997416">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592208">
<title confidence="0.99204">A Quasi-Dependency Model for Structural Analysis of Chinese BaseNPs*</title>
<author confidence="0.984326">Zhao Jun Huang Changning</author>
<affiliation confidence="0.9983015">Department of Computer Science &amp; Technology, The State Key Lab of Intelligent Technology &amp; Systems,</affiliation>
<address confidence="0.908279">Tsinghua University, Beijing, China, 100084</address>
<email confidence="0.640526">zj@s1000e.cs.tsinghua.edu.cn,Hcn@tsinghua.edu.cn</email>
<abstract confidence="0.999723">The paper puts forward a quasidependency model for structural analysis of Chinese baseNPs and a MDL-based algorithm for quasidependency-strength acquisition. The experiments show that the proposed model is more suitable for Chinese baseNP analysis and the proposed MDLbased algorithm is superior to the traditional MLbased algorithm. The paper also discusses the problem of incorporating the linguistic knowledge into the above statistical model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text, In:</title>
<date>1988</date>
<booktitle>Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<contexts>
<context position="966" citStr="[1]" startWordPosition="135" endWordPosition="135">ral analysis of Chinese baseNPs and a MDL-based algorithm for quasidependency-strength acquisition. The experiments show that the proposed model is more suitable for Chinese baseNP analysis and the proposed MDLbased algorithm is superior to the traditional MLbased algorithm. The paper also discusses the problem of incorporating the linguistic knowledge into the above statistical model. 1. Introduction The concept of baseNP is initially put forward by Church. In English, baseNP is defined as &apos;simple non-recursive noun phrases&apos;, which means that there is no sub-noun-phrases contained in a baseNP[1]. But the definition can not meet the needs in Chinese information retrieval. The noun phrases such as &amp;quot; PA (natural) W, (language) J1 (process)&amp;quot;, &amp;quot;IE fl (Asian) a (finance) fe tIt(crisis)&amp;quot; and &amp;quot;i(political) M(system) *(reformation) AK (process)&amp;quot; are critical for information retrieval, but they are not nonrecursive noun phrases. In Chinese, the attribute of noun phrases can be classified into three types, that is restrictive attributes, distinctive attributes and descriptive attributes, among which the restrictive attributes have agglutinative relation with the heads. The paper defines the Chi</context>
</contexts>
<marker>[1]</marker>
<rawString>Church K., A stochastic parts program and noun phrase parser for unrestricted text, In: Proceedings of the Second Conference on Applied Natural Language Processing, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Conceptual association for compound noun analysis, In:</title>
<date>1994</date>
<booktitle>Proceedings of the 32&amp;quot;d Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Student Session, Las Cruces, NM,</location>
<contexts>
<context position="4214" citStr="[2]" startWordPosition="608" endWordPosition="608">rength estimation algorithm based on the minimum description length (MDL) principle. Section 4 analyzes the performance of the proposed model and the algorithm. Section 5 discusses some issues in the implementation of baseNP structure analysis and quasi-dependencystrength estimation. Section 6 is the conclusion. 2. The quasi-dependency model There are two kinds of structural analysis models for English noun phrase, that is adjacency model and dependency model. The research of Lauer shows that the dependency model is superior to the adjacency model for structural analysis of English noun phrase[2]. However, there is no model for structural analysis of Chinese baseNP till now. According to the dependency grammar, two constituents can be bound together they are determined to be dependent. The determination of the dependency relation between two constituents is composed of two steps. The first step is to determine whether they have the possibility to constituent dependency relation. The second step is to determine whether they have dependency relation in the given context. The former is called the quasi-dependency-relation, which can be acquired from collocation dictionaries or corpora. T</context>
</contexts>
<marker>[2]</marker>
<rawString>Lauer M. Conceptual association for compound noun analysis, In: Proceedings of the 32&amp;quot;d Annual Meeting of the Association for Computational Linguistics, Student Session, Las Cruces, NM, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhai Chengxiang</author>
</authors>
<title>Fast Statistical Parsing of Noun Phrases for Document Indexing, In:</title>
<date>1997</date>
<booktitle>Proceedings of the 35&apos; Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<location>USA.:</location>
<contexts>
<context position="9037" citStr="[3]" startWordPosition="1374" endWordPosition="1374">cquired through a simple statistics. However, such an annotated corpus is not available. We only have a baseNP corpus which has no structural information. How to acquire the quasi-dependency-strength from such a corpus is the main task of the section. Given a baseNP set NP={np„np2,...,npm) and a lexicon W={w,,w2,...,wm), the problem can be described as learning a quasi-dependency-strength set G (abbreviated as model) from the training set. Where, G = Idsjj = ds(wi ),Vwe , wi E W} Zhai Chengxiang puts forward an unsupervised algorithm for acquiring quasi-dependency-strength from noun phrase set[3]. The algorithm is derived from the EM algorithm. Because the algorithm is based on the maximum likelihood (ML) principle, it usually leads to overfitness between the data and the model[4]. For example, given a simple baseNP set NP={gVM/politics PM/system al */reform, 4/economics •f21K Wsystem a*/reform, lKEI /politics PIK m /system * /revolute , 4 /economics 4K$1,l/system /revolute}, there are sixteen possible models for the training set, among them G., G7, G10 and G13 have the best fitness to NP, that is Num(NPIG)=6. However, in the linguistic view, GI is the correct model, though it has low</context>
<context position="13264" citStr="[3]" startWordPosition="2067" endWordPosition="2067">, optimize Gp, until L(NP I G) converges, that is L converges; • Keeping Gp fixed, optimize G„ until L(G) converges, that is L converges. On condition that the structure part of the model is fixed, the parameter optimization means to find the optimal sets of quasi-dependency-strength in order that the data description length minimized, that is G = arg min L(NP G) Where L(NPIG) is the optimal coding length of NP 4 when G is known. the gradual reduction of data description length. The parameter optimization step can be In MDL principle, the model description length implemented using EM algorithm[3]. In the can be gradually reduced through the modification process of parameter optimization, the structure of the structure part of the model, therefore the part of the model is kept fixed. The optimum overall description length of the model is reduced. estimates of the parameters are obtained through Algorithm 2: The structure optimization algorithm Let the model after the parameter optimization process is G, which is composed of the quasidependency-strength ds(w, w). ©Sort the quasi-dependency-strengths of model G in ascending order, that is dslii, dsm, ds131, ©Repeat the following steps, u</context>
</contexts>
<marker>[3]</marker>
<rawString>Zhai Chengxiang, Fast Statistical Parsing of Noun Phrases for Document Indexing, In: Proceedings of the 35&apos; Annual Meeting of the Association for Computational Linguistics, USA.: Association for Computational Linguistics. 1997. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Bayesian learning of probabilistic language models,</title>
<date>1994</date>
<institution>Dissertation for Ph.D. Degree, Berkeley, California: University of California,</institution>
<contexts>
<context position="9225" citStr="[4]" startWordPosition="1404" endWordPosition="1404">cy-strength from such a corpus is the main task of the section. Given a baseNP set NP={np„np2,...,npm) and a lexicon W={w,,w2,...,wm), the problem can be described as learning a quasi-dependency-strength set G (abbreviated as model) from the training set. Where, G = Idsjj = ds(wi ),Vwe , wi E W} Zhai Chengxiang puts forward an unsupervised algorithm for acquiring quasi-dependency-strength from noun phrase set[3]. The algorithm is derived from the EM algorithm. Because the algorithm is based on the maximum likelihood (ML) principle, it usually leads to overfitness between the data and the model[4]. For example, given a simple baseNP set NP={gVM/politics PM/system al */reform, 4/economics •f21K Wsystem a*/reform, lKEI /politics PIK m /system * /revolute , 4 /economics 4K$1,l/system /revolute}, there are sixteen possible models for the training set, among them G., G7, G10 and G13 have the best fitness to NP, that is Num(NPIG)=6. However, in the linguistic view, GI is the correct model, though it has lower fitness to NP, that is Num(NPIG)=4 (see the appendix). 3.1 The estimation of the quasi-dependencystrength under Bayesian framework In Bayesian framework, the task of acquiring the quasi</context>
</contexts>
<marker>[4]</marker>
<rawString>Stolcke A. Bayesian learning of probabilistic language models, Dissertation for Ph.D. Degree, Berkeley, California: University of California, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Solomonoff</author>
</authors>
<title>The mechanization of linguistic learning, In:</title>
<journal>Model G PI Fitness between G and NP Num( NFIG)</journal>
<booktitle>Proceedings of the 2nd International Conference on Cybernetics. Appendix: An</booktitle>
<contexts>
<context position="10784" citStr="[5]" startWordPosition="1655" endWordPosition="1655">set and the model G, Bayesian modeling gives additional consideration to the generality of the model through the prior probability p(G), that is simpler model has higher probability. The central idea of Bayesian modeling is to find a compromise between the goodness of fit and the simplicity of the model. 3.2 Defining the evaluation function of Bayesian modeling using MDL principle The difficulty in Bayesian modeling is the estimation of the prior probability p(G). According to the coding theory, the lower bound of the coding length (bit-string) of an information with probability p is log2 lip [5]. The theorem connects Bayesian modeling with the MDL principle in the coding theory. G = arg max p(NP I G)p(G) = arg min {- log, [p(NP I G)p(G)]) = arg min (In 2 + log2 p(NP I G) p(G)} = arg min {L(NP I G)+ L(G)) Where, L(a) is the optimal coding length of information a. Specially, L(NPIG) is called the data description length and L(G) is called the model description length. Therefore, the problem of estimating the prior probability p(G) and the conditional probability p(NPIG) is converted to the problem of estimating the model description length L(G) and the data description length L(NPIG). </context>
</contexts>
<marker>[5]</marker>
<rawString>Solomonoff R. The mechanization of linguistic learning, In: Proceedings of the 2nd International Conference on Cybernetics. Appendix: An example for quasi-dependency-relation acquisition No. Model G PI Fitness between G and NP Num( NFIG)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>