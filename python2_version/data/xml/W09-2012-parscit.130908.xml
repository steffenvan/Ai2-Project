<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.9960605">
How Creative is Your Writing? A Linguistic Creativity Measure from
Computer Science and Cognitive Psychology Perspectives
</title>
<author confidence="0.99909">
Xiaojin Zhu, Zhiting Xu and Tushar Khot
</author>
<affiliation confidence="0.9977495">
Department of Computer Sciences
University of Wisconsin-Madison
</affiliation>
<address confidence="0.94545">
Madison, WI, USA 53706
</address>
<email confidence="0.997103">
{jerryzhu, zhiting, tushar}@cs.wisc.edu
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885769230769">
We demonstrate that subjective creativity in
sentence-writing can in part be predicted us-
ing computable quantities studied in Com-
puter Science and Cognitive Psychology. We
introduce a task in which a writer is asked to
compose a sentence given a keyword. The
sentence is then assigned a subjective creativ-
ity score by human judges. We build a linear
regression model which, given the keyword
and the sentence, predicts the creativity score.
The model employs features on statistical lan-
guage models from a large corpus, psycholog-
ical word norms, and WordNet.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.91346625">
One definition of creativity is “the ability to tran-
scend traditional ideas, rules, patterns, relationships,
or the like, and to create meaningful new ideas,
forms, methods, interpretations, etc.” Therefore,
any computational measure of creativity needs to ad-
dress two aspects simultaneously:
1. The item to be measured has to be different
from other existing items. If one can model ex-
isting items with a statistical model, the new
item should be an “outlier”.
2. The item has to be meaningful. An item con-
sists of random noise might well be an outlier,
but it is not of interest.
In this paper, we consider the task of measuring hu-
man creativity in composing a single sentence, when
the sentence is constrained by a given keyword. This
</bodyText>
<page confidence="0.987583">
87
</page>
<bodyText confidence="0.999948294117647">
simple task is a first step towards automatically mea-
suring creativity in more complex natural language
text. To further simplify the task, we will focus on
the first aspect of creativity, i.e., quantifying how
novel the sentence is. The second aspect, how mean-
ingful the sentence is, requires the full power of Nat-
ural Language Processing, and is beyond the scope
of this initial work. This, of course, raises the con-
cern that we may regard a nonsense sentence as
highly creative. This is a valid concern. However,
in many applications where a creativity measure is
needed, the input sentences are indeed well-formed.
In such applications, our approach will be useful.
We will leave this issue to future work. The present
paper uses a data set (see the next section) in which
all sentences are well-formed.
A major difficulty in studying creativity is the
lack of an objective definition of creativity. Because
creative writing is highly subjective (“I don’t know
what is creativity, but I recognize it when I see one”),
we circumvent this problem by using human judg-
ment as the ground truth. Our experiment procedure
is the following. First, we give a keyword z to a
human writer, and ask her to compose a sentence
x about z. Then, the sentence x is evaluated by a
group of human judges who assign it a subjective
“creativity score” y. Finally, given a dataset con-
sisting of many such keyword-sentence-score triples
(z, x, y), we develop a statistical predictor f(x, z)
that predicts the score y from the sentence x and
keyword z.
There has been some prior attempts on charac-
terizing creativity from a computational perspec-
tive, for examples see (Ritchie, 2001; Ritchie, 2007;
</bodyText>
<note confidence="0.691739">
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87–93,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.983054714285714">
Pease et al., 2001). The present work distinguishes
itself in the use of a statistical machine learning
framework, the design of candidate features, and its
empirical study.
their general agreement on subjective creativity. Ta-
ble 1 lists the pairwise linear correlation coefficient
between all four judges.
</bodyText>
<sectionHeader confidence="0.89283" genericHeader="method">
2 The Creativity Data Set
</sectionHeader>
<bodyText confidence="0.999890333333333">
We select 105 keywords from the English version of
the Leuven norms dataset (De Deyne and Storms,
2008b; De Deyne and Storms, 2008a). This ensures
that each keyword has their norms feature defined,
see Section 3.2. These are common English words.
The keywords are randomly distributed to 21 writ-
ers, each writer receives 5 keywords. Each writer
composes one sentence per keyword. These 5 key-
words are further randomly split into two groups:
</bodyText>
<listItem confidence="0.5838916">
1. The first group consists of 1 keyword. The
writers are instructed to “write a not-so-creative
sentence” about the keyword. Two examples
are given: “Iguana has legs” for “Iguana”, and
“Anvil can get rusty” for “Anvil.” The purpose
of this group is to establish a non-creative base-
line for the writers, so that they have a sense
what does not count as creative.
2. The second group consists of 4 keywords. The
writers are instructed to “try to write a creative
sentence” about each keyword. They are also
told to write a sentence no matter what, even if
they cannot come up with a creative one. No
example is given to avoid biasing their creative
thinking.
</listItem>
<bodyText confidence="0.9998752">
In the next stage, all sentences are given to four
human judges, who are native English speakers. The
judges are not the writers nor the authors of this
paper. The order of the sentences are randomized.
The judges see the sentences and their correspond-
ing keywords, but not the identity of the writers,
nor which group the keywords are in. The judges
work independently. For each keyword-sentence
pair, each judge assigns a subjective creativity score
between 0 and 10, with 0 being not creative at all
(the judges are given the Iguana and Anvil exam-
ples for this), and 10 the most creative. The judges
are encouraged to use the full scale when scoring.
There is statistically significant (p &lt; 10−8) linear
correlation among the four judges’ scores, showing
</bodyText>
<tableCaption confidence="0.86945625">
Table 1: The pairwise linear correlation coefficient be-
tween four judges’ creativity scores given to the 105 sen-
tences. All correlations are statistically significant with
p &lt; 10−8.
</tableCaption>
<bodyText confidence="0.97470525">
judge 2 judge 3 judge 4
judge 1 0.68 0.61 0.74
judge 2 0.55 0.74
judge 3 0.61
The scores from four judges on each sentence are
then averaged to produce a consensus score y. Ta-
ble 2 shows the top and bottom three sentences as
sorted by y.
As yet another sanity check, note that the judges
have no information which sentences are from group
1 (where the writers are instructed to be non-
creative), and which are from group 2. We would
expect that if both the writers and the judges share
some common notion of creativity, the mean score
of group 1 should be smaller than the mean score of
group 2. Figure 1 shows that this is indeed the case,
with the mean score of group 1 being 1.5 f 0.6, and
that of group 2 being 5.1 f 0.4 (95% confidence in-
terval). A t-test shows that this difference is signifi-
cant (p &lt; 10−11).
</bodyText>
<figure confidence="0.58485">
group
</figure>
<figureCaption confidence="0.823189333333333">
Figure 1: The mean creativity score for group 1 is signif-
icantly smaller than that for group 2. That is, the judges
feel that sentences in group 2 are more creative.
</figureCaption>
<bodyText confidence="0.964544833333333">
To summarize, in the end our dataset con-
sists of 105 keyword, sentence, creativity
score tuples {(zZ, xi, yz)} for i = 1, ... ,105.
The sentence group information is not in-
cluded. This “Wisconsin Creative Writ-
ing” dataset is publicly available at http:
</bodyText>
<figure confidence="0.9925255">
1 2
score
6
4
2
0
</figure>
<page confidence="0.979925">
88
</page>
<tableCaption confidence="0.978283">
Table 2: Example sentences with the largest and smallest consensus creativity scores.
</tableCaption>
<figure confidence="0.939573545454546">
consensus score y keyword z sentence x
9.25 hamster She asked if I had any pets, so I told her I once did until I discovered
that I liked taste of hamster.
9.0 wasp The wasp is a dinosaur in the ant world.
8.5 dove Dove can still bring war by the information it carries.
...
0.25 guitar A Guitar has strings.
0.25 leech Leech lives in the water.
0.25 elephant Elephant is a mammal.
//pages.cs.wisc.edu/∼jerryzhu/pub/
WisconsinCreativeWriting.txt.
</figure>
<sectionHeader confidence="0.6408425" genericHeader="method">
3 Candidate Features for Predicting
Creativity
</sectionHeader>
<bodyText confidence="0.999910625">
In this section, we discuss two families of candi-
date features we use in a statistical model to pre-
dict the creativity of a sentence. One family comes
from a Computer Science perspective, using large-
corpus statistics (how people write). The other fam-
ily comes from a Cognitive Psychology perspective,
specifically the word norms data and WordNet (how
people think).
</bodyText>
<subsectionHeader confidence="0.9997915">
3.1 The Computer Science Perspective:
Language Modeling
</subsectionHeader>
<bodyText confidence="0.999938266666667">
We start from the following hypothesis: if the words
in the sentence x frequently co-occur with the key-
word z, then x is probably not creative. This is of
course an over-simplification, as many creative sen-
tences are about novel usage of common words1.
Nonetheless, this hypothesis inspires some candi-
date features that can be computed from a large cor-
pus.
In this study, we use the Google Web 1T 5-
gram Corpus (Brants et al., 2007). This corpus
was generated from about 1012 word tokens from
Web pages. It consists of counts of N-gram for
N = 1, ... , 5. We denote the words in a sentence
by x = x1, ... , xn, where x1 = (s) and xn = (/s)
are special start- and end-of-sentence symbols. We
</bodyText>
<footnote confidence="0.961398">
1For example, one might argue that Lincoln’s famous sen-
tence on government: “of the people, by the people, for the
people” is creative, even though the keyword “government” fre-
quently co-occurs with all the words in that sentence.
</footnote>
<bodyText confidence="0.893418">
design the following candidate features:
[f1: Zero N-gram Fraction] Let c(xi+N−1
i ) be
the count of the N-gram xi ... xi+N−1 in the corpus.
Let δ(A) be the indicator function with value 1 if
the predicate A is true, and 0 otherwise. A “Zero
N-gram Fraction” feature is the fraction of zero N-
gram counts in the sentence:
</bodyText>
<equation confidence="0.998017">
f1,N(x) = Ei 1 +1 δ(c(xz+N-1) = 0) .(1)
n − N + 1
</equation>
<bodyText confidence="0.950341222222222">
This provided us with 5 features, namely N-gram
zero count fractions for each value of N. These fea-
tures are a crude measure of how surprising the sen-
tence x is. A feature value of 1 indicates that none of
the N-grams in the sentence appeared in the Google
corpus, a rather surprising situation.
[f2: Per-Word Sentence Probability] This fea-
ture is the per-word log likelihood of the sentence,
to normalize for sentence length:
</bodyText>
<equation confidence="0.9883655">
f2(x) = n
1 logp(x). (2)
</equation>
<bodyText confidence="0.9998575">
We use a 5-gram language model to estimate
p(x), with “naive Jelinek-Mercer” smoothing. As
in Jelinek-Mercer smoothing (Jelinek and Mercer,
1980), it is a linear interpolation of N-gram language
models for N = 1... 5. Let the Maximum Likeli-
hood (ML) estimate of a N-gram language model be
</bodyText>
<equation confidence="0.9972464">
pN ML(xi|xi−1
i−N+1) = c(xi
i−1 , (3)
c(xi−N+1)
i−N+1)
</equation>
<bodyText confidence="0.9999535">
which is the familiar frequency estimate of proba-
bility. The denominator is the count of the history
of length N − 1, and the numerator is the count of
the history plus the word to be predicted. A 5-gram
</bodyText>
<page confidence="0.995448">
89
</page>
<bodyText confidence="0.7752545">
Jelinek-Mercer smoothing language model on sen-
tence x is
</bodyText>
<equation confidence="0.998508857142857">
p(xi|xi−1
i−5+1) (4)
5
�
λNPNML(xi|xi−1
i−N+1),(5)
N=1
</equation>
<bodyText confidence="0.978736">
where the linear interpolation weights λ1 + ... +
λ5 = 1. The optimal values of λ’s are a function of
history counts (binned into “buckets”) c(xi−1
i−N+1),
and should be optimized with convex optimiza-
tion from corpus. However, because our corpus is
large, and because we do not require precise lan-
guage modeling, we instead set the λ’s in a heuris-
tic manner. Starting from N=5 to 1, λN is set
to zero until the N where we have enough history
count for reliable estimate. Specifically, we require
c(xi−1
i−N+1) &gt; 1000. The first N that this happens
receives λN = 0.9. The next lower order model
receives 0.9 fraction of the remaining weight, i.e.,
λN−1 = 0.9 × (1 − 0.9), and so on. Finally, λ1 re-
ceives all remaining weight to ensure λ1+...+λ5 =
</bodyText>
<listItem confidence="0.803327">
1. This heuristic captures the essence of Jelinek-
Mercer smoothing and is highly efficient, at the price
of suboptimal interpolation weights.
[f3: Per-Word Context Probability] The previ-
</listItem>
<bodyText confidence="0.999659571428572">
ous feature f2 ignores the fact that our sentence x
is composed around a given keyword z. Given that
the writer was prompted with the keyword z, we are
interested in the novelty of the sentence surround-
ing the keyword. Let xk be the first occurrence of
z in the sentence, and let x−k be the context of the
keyword, i.e., the sentence with the k-th word (the
keyword) removed. This notion of context novelty
can be captured by
where p(x) is estimated from the naive Jelinek-
Mercer 5-gram language model above, and p(z) is
estimated from a unigram language model. Our third
feature is the length-normalized log likelihood of the
context:
</bodyText>
<equation confidence="0.768083">
1
f3(x, z) = n − 1 (log p(x) − log p(z)) . (7)
</equation>
<subsectionHeader confidence="0.930742">
3.2 The Cognitive Psychology Perspective:
Word Norms and WordNet
</subsectionHeader>
<bodyText confidence="0.999750170731707">
A text corpus like the one above captures how peo-
ple write sentences related to a keyword. However,
this can be different from how people think about re-
lated concepts in their head for the same keyword.
In fact, common sense knowledge is often under-
represented in a corpus – for example, why bother
repeating “A duck has a long neck” over and over
again? However, this lack of co-occurrence does not
necessarily make the duck sentence creative.
The way people think about concepts can in part
be captured by word norms experiments in psychol-
ogy. In such experiments, a human subject is pro-
vided with a keyword z, and is asked to write down
the first (or a few) word x that comes to mind.
When aggregated over multiple subjects on the same
keyword, the experiment provides an estimate of
the concept transition probability p(x|z). Given
enough keywords, one can construct a concept net-
work where the nodes are the keywords, and the
edges describe the transitions (Steyvers and Tenen-
baum, 2005). For our purpose, we posit that a sen-
tence x may not be creative with respect to a key-
word z, if many words in x can be readily retrieved
as the norms of keyword z. In a sense, the writer
was thinking the obvious.
[f4: Word Norms Fraction] We use the Leu-
ven dataset, which consists of norms for 1,424 key-
words (De Deyne and Storms, 2008b; De Deyne and
Storms, 2008a). The original Leuven dataset is in
Dutch, we use a version that is translated into En-
glish. For each sentence x, we first exclude the key-
word z from the sentence. We also remove punctu-
ations, and map all words to lower case. We further
remove all stopwords using the Snowball stopword
list (Porter, 2001), and stem all words in the sentence
and the norm word list using NLTK (Loper and Bird,
2002). We then count the number of words xi that
appear in the norm list of the keyword z in the Leu-
ven data. Let this count be cnorm(x, z). The feature
is the fraction of such norm words in the original
sentence:
</bodyText>
<equation confidence="0.964266333333333">
cnorm(x, z)
f4(x, z) =
n
</equation>
<bodyText confidence="0.9405345">
It is worth noting that the Leuven dataset is relatively
small, with less than two thousand keywords. This
</bodyText>
<equation confidence="0.9956304">
n
p(x) =
i=1
p(xi|xi−1
i−5+1) =
p(x−k|xk = z) = p(x−k, xk = z)
p(x)
p(z) , (6)
p(xk = z)
. (8)
</equation>
<page confidence="0.985954">
90
</page>
<bodyText confidence="0.962375206896552">
is a common issue with psychology norms datasets,
as massive number of human subjects are difficult
to obtain. To scale our method up to handle large
vocabulary in the future, one possible method is to
automatically infer the norms of novel keywords us-
ing corpus statistics (e.g., distributional similarity).
[f5 − f13: WordNet Similarity] WordNet is an-
other linguistic resource motivated by cognitive psy-
chology. For each sentence x, we compute Word-
Net 3.0 similarity between the keyword z and each
word xi in the sentence. Specifically, we use the
“path similarity” provided by NLTK (Loper and
Bird, 2002). Path similarity returns a score denot-
ing how similar two word senses are, based on the
shortest path that connects the senses in the hyper-
nym/hyponym taxonomy. The score is in the range
0 to 1, except in those cases where a path cannot be
found, in which case -1 is returned. A score of 1
represents identity, i.e., comparing a sense with it-
self. Let the similarities be s1 ... sn. We experiment
with the following features: The mean, median, and
variance of similarities:
f5(x, z) = mean(s1 ... sn)
f6(x, z) = median(s1 ... sn)
f7(x, z) = var(s1 ... sn).
Features f8, ... , f12 are the top five similarities.
When the length of the sentence is shorter than five,
we fill the remaining features with -1. Finally, fea-
ture f13 is the fraction of positive similarity:
</bodyText>
<equation confidence="0.942997">
�n f13 (x, . (12)
z) =i=1 δ(si &gt; 0)
n
</equation>
<sectionHeader confidence="0.973952" genericHeader="method">
4 Regression Analysis on Creativity
</sectionHeader>
<bodyText confidence="0.999055652173913">
With the candidate features introduced in Section 3,
we construct a linear regression model to predict the
creativity scores given a sentence and its keyword.
The first question one asks in regression analy-
sis is whether the features have a (linear) correlation
with the creativity score y. We compute the correla-
tion coefficient
• The feature f4 (Word Norms Fraction) has the
largest correlation coefficient -0.48 in terms of
magnitude. That is, the more words in the sen-
tence that are also in the norms of the keyword,
the less creative the sentence is.
• The feature f12 (the 5-th WordNet similarity in
the sentence to the keyword) has a large posi-
tive coefficient 0.47. This is rather unexpected.
A closer inspection reveals that f12 equals -1
for about half of the sentences, and is around
0.05 for the other half. Furthermore, the second
half has on average higher creativity scores. Al-
though we hypothesized earlier that more simi-
lar words means lower creativity, this (together
with the positive ρ for f10, f11) suggests the
other way around: more similar words are cor-
related with higher creativity.
• The feature f5 (mean WordNet similarity) has
a negative correlation with creativity. This fea-
ture is related to f12, but in a different direc-
tion. We speculate that this feature measures
the strength of similar words, while f12 indi-
rectly measures the number of similar words.
• The feature f3 (Per-Word Context Probability)
has a negative correlation with creativity. The
more predictable the sentence around the key-
word using a language model, the lower the
creativity.
Next, we build a linear regression model to pre-
dict creativity. We use stepwise regression, which
is a technique for feature selection by iteratively
including / excluding candidate features from the
regression model based on statistical significance
tests (Draper and Smith, 1998). The result is a lin-
ear regression model with a small number of salient
features. For the creativity dataset, the features (and
their regression coefficients) included by stepwise
regression are shown on the second row in Table 3.
The corresponding linear regression model is
</bodyText>
<equation confidence="0.849919">
ρi = Cov(fi, y) (13) ˆy(x, z) = −5.06 x f4 + 1.80 x f12 − 0.76 x f3
σfiσy −3.39 x f5 + 0.92. (14)
</equation>
<bodyText confidence="0.999026">
for each candidate feature fi separately on the first
row in Table 3. Some observations:
A plot comparing yˆ and y is given in Figure 2. The
root mean squared error (RMSE) of this model is
</bodyText>
<page confidence="0.998887">
91
</page>
<tableCaption confidence="0.9931865">
Table 3: p: The linear correlation coefficients between a candidate feature and the creativity score y. β: The selected
features and their regression coefficients in stepwise linear regression.
</tableCaption>
<table confidence="0.950275666666667">
f1,1 f1,2 f1,3 f1,4 f1,5 f2 f3 f4 f5
p 0.09 0.09 0.17 0.06 -0.04 -0.07 -0.32 -0.48 -0.41
Q -0.76 -5.06 -3.39
f6 f7 f8 f9 f10 f11 f12 f13
p -0.19 -0.25 -0.02 0.06 0.23 0.30 0.47 -0.01
Q 1.80
</table>
<figure confidence="0.972461375">
10
8
6
4
2
00 5 10
predicted score
true score
</figure>
<figureCaption confidence="0.999737333333333">
Figure 2: The creativity score yˆ as predicted by the linear
regression model in equation 14, compared to the true
score y. Each dot is a sentence.
</figureCaption>
<bodyText confidence="0.909686375">
1.51. In contrast, the constant predictor would have
RMSE 2.37 (i.e., the standard deviation of y).
We make two comments:
1. It is interesting to note that our intuitive fea-
tures are able to partially predict subjective cre-
ativity scores. On the other hand, we certainly
do not claim that our features or model solved
this difficult problem.
2. All three kinds of knowledge: corpus statistics
(f3), word norms (f4), and WordNet (f5, f12)
are included in the regression model. Coin-
cidentally, these features have the largest cor-
relation coefficients with the creativity score.
The fact that they are all included suggests that
these are not redundant features, and each cap-
tures some aspect of creativity.
</bodyText>
<sectionHeader confidence="0.998743" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999995608695652">
We presented a simplified creativity prediction task,
and showed that features derived from statistical
language modeling, word norms, and WordNet can
partially predict human judges’ subjective creativity
scores.
Our problem setting is artificial, in that the cre-
ativity of the sentences are judged with respect to
their respective keywords, which are assumed to be
known beforehand. This allows us to design features
centered around the keywords. We hope our analysis
can be extended to the setting where the only input is
the sentence, without the keyword. This can poten-
tially be achieved by performing keyword extraction
on the sentence first, and apply our analysis on the
extracted keyword.
As discussed in the introduction, our analysis
is susceptible to nonsense input sentences, which
could be predicted as highly creative. Combining
our analysis with a “sensibility analysis” is an im-
portant future direction.
Finally, our model might be adapted to explain
why a sentence is deemed creative, by analyzing the
contribution of individual features in the model.
</bodyText>
<sectionHeader confidence="0.999655" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999637166666667">
We thank the anonymous reviewers for suggestions
on related work and other helpful comments, and
Chuck Dyer, Andrew Goldberg, Jake Rosin, and
Steve Yazicioglu for assisting the project. This work
is supported in part by the Wisconsin Alumni Re-
search Foundation.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998789777777778">
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP.
S. De Deyne and G Storms. 2008a. Word associations:
Network and semantic properties. Behavior Research
Methods, 40:213–231.
S. De Deyne and G Storms. 2008b. Word associations:
Norms for 1,424 Dutch words in a continuous task.
Behavior Research Methods, 40:198–205.
</reference>
<page confidence="0.936195">
92
</page>
<reference confidence="0.996016607142857">
Norman R. Draper and Harry Smith. 1998. Applied
Regression Analysis (Wiley Series in Probability and
Statistics). John Wiley &amp; Sons Inc, third edition.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters from
sparse data. In Workshop on Pattern Recognition in
Practice.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In The ACL Workshop on Ef-
fective Tools and Methodologies for Teaching Natural
Language Processing and Computational Linguistics,
pages 62–69.
Alison Pease, Daniel Winterstein, and Simon Colton.
2001. Evaluating machine creativity. In Workshop
on Creative Systems, 4th International Conference on
Case Based Reasoning, pages 129–137.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms. Published online.
Graeme Ritchie. 2001. Assessing creativity. In Pro-
ceedings of the AISB01 Symposium on Artificial Intel-
ligence and Creativity in Arts and Science, pages 3–11.
Graeme Ritchie. 2007. Some empirical criteria for at-
tributing creativity to a computer program. Minds and
Machines, 17(1):67–99.
Mark Steyvers and Joshua Tenenbaum. 2005. The large
scale structure of semantic networks: Statistical anal-
yses and a model of semantic growth. Cognitive Sci-
ence, 29(1):41–78.
</reference>
<page confidence="0.999166">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748240">
<title confidence="0.9972525">How Creative is Your Writing? A Linguistic Creativity Measure Computer Science and Cognitive Psychology Perspectives</title>
<author confidence="0.973656">Xiaojin Zhu</author>
<author confidence="0.973656">Zhiting Xu</author>
<author confidence="0.973656">Tushar</author>
<affiliation confidence="0.9998475">Department of Computer University of</affiliation>
<address confidence="0.999817">Madison, WI, USA</address>
<email confidence="0.990683">zhiting,</email>
<abstract confidence="0.982855857142857">We demonstrate that subjective creativity in sentence-writing can in part be predicted using computable quantities studied in Computer Science and Cognitive Psychology. We introduce a task in which a writer is asked to compose a sentence given a keyword. The sentence is then assigned a subjective creativity score by human judges. We build a linear regression model which, given the keyword and the sentence, predicts the creativity score. The model employs features on statistical language models from a large corpus, psychological word norms, and WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8544" citStr="Brants et al., 2007" startWordPosition="1440" endWordPosition="1443">. The other family comes from a Cognitive Psychology perspective, specifically the word norms data and WordNet (how people think). 3.1 The Computer Science Perspective: Language Modeling We start from the following hypothesis: if the words in the sentence x frequently co-occur with the keyword z, then x is probably not creative. This is of course an over-simplification, as many creative sentences are about novel usage of common words1. Nonetheless, this hypothesis inspires some candidate features that can be computed from a large corpus. In this study, we use the Google Web 1T 5- gram Corpus (Brants et al., 2007). This corpus was generated from about 1012 word tokens from Web pages. It consists of counts of N-gram for N = 1, ... , 5. We denote the words in a sentence by x = x1, ... , xn, where x1 = (s) and xn = (/s) are special start- and end-of-sentence symbols. We 1For example, one might argue that Lincoln’s famous sentence on government: “of the people, by the people, for the people” is creative, even though the keyword “government” frequently co-occurs with all the words in that sentence. design the following candidate features: [f1: Zero N-gram Fraction] Let c(xi+N−1 i ) be the count of the N-gra</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S De Deyne</author>
<author>G Storms</author>
</authors>
<title>Word associations: Network and semantic properties.</title>
<date>2008</date>
<journal>Behavior Research Methods,</journal>
<pages>40--213</pages>
<marker>De Deyne, Storms, 2008</marker>
<rawString>S. De Deyne and G Storms. 2008a. Word associations: Network and semantic properties. Behavior Research Methods, 40:213–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S De Deyne</author>
<author>G Storms</author>
</authors>
<title>Word associations: Norms for 1,424 Dutch words in a continuous task. Behavior Research Methods,</title>
<date>2008</date>
<pages>40--198</pages>
<marker>De Deyne, Storms, 2008</marker>
<rawString>S. De Deyne and G Storms. 2008b. Word associations: Norms for 1,424 Dutch words in a continuous task. Behavior Research Methods, 40:198–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman R Draper</author>
<author>Harry Smith</author>
</authors>
<title>Applied Regression Analysis (Wiley Series in Probability and Statistics).</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons Inc,</publisher>
<note>third edition.</note>
<contexts>
<context position="17706" citStr="Draper and Smith, 1998" startWordPosition="3070" endWordPosition="3073">erent direction. We speculate that this feature measures the strength of similar words, while f12 indirectly measures the number of similar words. • The feature f3 (Per-Word Context Probability) has a negative correlation with creativity. The more predictable the sentence around the keyword using a language model, the lower the creativity. Next, we build a linear regression model to predict creativity. We use stepwise regression, which is a technique for feature selection by iteratively including / excluding candidate features from the regression model based on statistical significance tests (Draper and Smith, 1998). The result is a linear regression model with a small number of salient features. For the creativity dataset, the features (and their regression coefficients) included by stepwise regression are shown on the second row in Table 3. The corresponding linear regression model is ρi = Cov(fi, y) (13) ˆy(x, z) = −5.06 x f4 + 1.80 x f12 − 0.76 x f3 σfiσy −3.39 x f5 + 0.92. (14) for each candidate feature fi separately on the first row in Table 3. Some observations: A plot comparing yˆ and y is given in Figure 2. The root mean squared error (RMSE) of this model is 91 Table 3: p: The linear correlatio</context>
</contexts>
<marker>Draper, Smith, 1998</marker>
<rawString>Norman R. Draper and Harry Smith. 1998. Applied Regression Analysis (Wiley Series in Probability and Statistics). John Wiley &amp; Sons Inc, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Workshop on Pattern Recognition in Practice.</booktitle>
<contexts>
<context position="10002" citStr="Jelinek and Mercer, 1980" startWordPosition="1706" endWordPosition="1709">xz+N-1) = 0) .(1) n − N + 1 This provided us with 5 features, namely N-gram zero count fractions for each value of N. These features are a crude measure of how surprising the sentence x is. A feature value of 1 indicates that none of the N-grams in the sentence appeared in the Google corpus, a rather surprising situation. [f2: Per-Word Sentence Probability] This feature is the per-word log likelihood of the sentence, to normalize for sentence length: f2(x) = n 1 logp(x). (2) We use a 5-gram language model to estimate p(x), with “naive Jelinek-Mercer” smoothing. As in Jelinek-Mercer smoothing (Jelinek and Mercer, 1980), it is a linear interpolation of N-gram language models for N = 1... 5. Let the Maximum Likelihood (ML) estimate of a N-gram language model be pN ML(xi|xi−1 i−N+1) = c(xi i−1 , (3) c(xi−N+1) i−N+1) which is the familiar frequency estimate of probability. The denominator is the count of the history of length N − 1, and the numerator is the count of the history plus the word to be predicted. A 5-gram 89 Jelinek-Mercer smoothing language model on sentence x is p(xi|xi−1 i−5+1) (4) 5 � λNPNML(xi|xi−1 i−N+1),(5) N=1 where the linear interpolation weights λ1 + ... + λ5 = 1. The optimal values of λ’</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Workshop on Pattern Recognition in Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In The ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,</booktitle>
<pages>62--69</pages>
<contexts>
<context position="13955" citStr="Loper and Bird, 2002" startWordPosition="2418" endWordPosition="2421">e norms of keyword z. In a sense, the writer was thinking the obvious. [f4: Word Norms Fraction] We use the Leuven dataset, which consists of norms for 1,424 keywords (De Deyne and Storms, 2008b; De Deyne and Storms, 2008a). The original Leuven dataset is in Dutch, we use a version that is translated into English. For each sentence x, we first exclude the keyword z from the sentence. We also remove punctuations, and map all words to lower case. We further remove all stopwords using the Snowball stopword list (Porter, 2001), and stem all words in the sentence and the norm word list using NLTK (Loper and Bird, 2002). We then count the number of words xi that appear in the norm list of the keyword z in the Leuven data. Let this count be cnorm(x, z). The feature is the fraction of such norm words in the original sentence: cnorm(x, z) f4(x, z) = n It is worth noting that the Leuven dataset is relatively small, with less than two thousand keywords. This n p(x) = i=1 p(xi|xi−1 i−5+1) = p(x−k|xk = z) = p(x−k, xk = z) p(x) p(z) , (6) p(xk = z) . (8) 90 is a common issue with psychology norms datasets, as massive number of human subjects are difficult to obtain. To scale our method up to handle large vocabulary </context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In The ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Pease</author>
<author>Daniel Winterstein</author>
<author>Simon Colton</author>
</authors>
<title>Evaluating machine creativity.</title>
<date>2001</date>
<booktitle>In Workshop on Creative Systems, 4th International Conference on Case Based Reasoning,</booktitle>
<pages>129--137</pages>
<contexts>
<context position="3495" citStr="Pease et al., 2001" startWordPosition="564" endWordPosition="567">y a group of human judges who assign it a subjective “creativity score” y. Finally, given a dataset consisting of many such keyword-sentence-score triples (z, x, y), we develop a statistical predictor f(x, z) that predicts the score y from the sentence x and keyword z. There has been some prior attempts on characterizing creativity from a computational perspective, for examples see (Ritchie, 2001; Ritchie, 2007; Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87–93, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Pease et al., 2001). The present work distinguishes itself in the use of a statistical machine learning framework, the design of candidate features, and its empirical study. their general agreement on subjective creativity. Table 1 lists the pairwise linear correlation coefficient between all four judges. 2 The Creativity Data Set We select 105 keywords from the English version of the Leuven norms dataset (De Deyne and Storms, 2008b; De Deyne and Storms, 2008a). This ensures that each keyword has their norms feature defined, see Section 3.2. These are common English words. The keywords are randomly distributed t</context>
</contexts>
<marker>Pease, Winterstein, Colton, 2001</marker>
<rawString>Alison Pease, Daniel Winterstein, and Simon Colton. 2001. Evaluating machine creativity. In Workshop on Creative Systems, 4th International Conference on Case Based Reasoning, pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>Published online.</note>
<contexts>
<context position="13862" citStr="Porter, 2001" startWordPosition="2402" endWordPosition="2403">eative with respect to a keyword z, if many words in x can be readily retrieved as the norms of keyword z. In a sense, the writer was thinking the obvious. [f4: Word Norms Fraction] We use the Leuven dataset, which consists of norms for 1,424 keywords (De Deyne and Storms, 2008b; De Deyne and Storms, 2008a). The original Leuven dataset is in Dutch, we use a version that is translated into English. For each sentence x, we first exclude the keyword z from the sentence. We also remove punctuations, and map all words to lower case. We further remove all stopwords using the Snowball stopword list (Porter, 2001), and stem all words in the sentence and the norm word list using NLTK (Loper and Bird, 2002). We then count the number of words xi that appear in the norm list of the keyword z in the Leuven data. Let this count be cnorm(x, z). The feature is the fraction of such norm words in the original sentence: cnorm(x, z) f4(x, z) = n It is worth noting that the Leuven dataset is relatively small, with less than two thousand keywords. This n p(x) = i=1 p(xi|xi−1 i−5+1) = p(x−k|xk = z) = p(x−k, xk = z) p(x) p(z) , (6) p(xk = z) . (8) 90 is a common issue with psychology norms datasets, as massive number </context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin F. Porter. 2001. Snowball: A language for stemming algorithms. Published online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Ritchie</author>
</authors>
<title>Assessing creativity.</title>
<date>2001</date>
<booktitle>In Proceedings of the AISB01 Symposium on Artificial Intelligence and Creativity in Arts and Science,</booktitle>
<pages>3--11</pages>
<contexts>
<context position="3275" citStr="Ritchie, 2001" startWordPosition="537" endWordPosition="538">by using human judgment as the ground truth. Our experiment procedure is the following. First, we give a keyword z to a human writer, and ask her to compose a sentence x about z. Then, the sentence x is evaluated by a group of human judges who assign it a subjective “creativity score” y. Finally, given a dataset consisting of many such keyword-sentence-score triples (z, x, y), we develop a statistical predictor f(x, z) that predicts the score y from the sentence x and keyword z. There has been some prior attempts on characterizing creativity from a computational perspective, for examples see (Ritchie, 2001; Ritchie, 2007; Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87–93, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Pease et al., 2001). The present work distinguishes itself in the use of a statistical machine learning framework, the design of candidate features, and its empirical study. their general agreement on subjective creativity. Table 1 lists the pairwise linear correlation coefficient between all four judges. 2 The Creativity Data Set We select 105 keywords from the English version of the Leuven norm</context>
</contexts>
<marker>Ritchie, 2001</marker>
<rawString>Graeme Ritchie. 2001. Assessing creativity. In Proceedings of the AISB01 Symposium on Artificial Intelligence and Creativity in Arts and Science, pages 3–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Ritchie</author>
</authors>
<title>Some empirical criteria for attributing creativity to a computer program.</title>
<date>2007</date>
<journal>Minds and Machines,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="3290" citStr="Ritchie, 2007" startWordPosition="539" endWordPosition="540">judgment as the ground truth. Our experiment procedure is the following. First, we give a keyword z to a human writer, and ask her to compose a sentence x about z. Then, the sentence x is evaluated by a group of human judges who assign it a subjective “creativity score” y. Finally, given a dataset consisting of many such keyword-sentence-score triples (z, x, y), we develop a statistical predictor f(x, z) that predicts the score y from the sentence x and keyword z. There has been some prior attempts on characterizing creativity from a computational perspective, for examples see (Ritchie, 2001; Ritchie, 2007; Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 87–93, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Pease et al., 2001). The present work distinguishes itself in the use of a statistical machine learning framework, the design of candidate features, and its empirical study. their general agreement on subjective creativity. Table 1 lists the pairwise linear correlation coefficient between all four judges. 2 The Creativity Data Set We select 105 keywords from the English version of the Leuven norms dataset (De D</context>
</contexts>
<marker>Ritchie, 2007</marker>
<rawString>Graeme Ritchie. 2007. Some empirical criteria for attributing creativity to a computer program. Minds and Machines, 17(1):67–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>The large scale structure of semantic networks: Statistical analyses and a model of semantic growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13190" citStr="Steyvers and Tenenbaum, 2005" startWordPosition="2271" endWordPosition="2275"> this lack of co-occurrence does not necessarily make the duck sentence creative. The way people think about concepts can in part be captured by word norms experiments in psychology. In such experiments, a human subject is provided with a keyword z, and is asked to write down the first (or a few) word x that comes to mind. When aggregated over multiple subjects on the same keyword, the experiment provides an estimate of the concept transition probability p(x|z). Given enough keywords, one can construct a concept network where the nodes are the keywords, and the edges describe the transitions (Steyvers and Tenenbaum, 2005). For our purpose, we posit that a sentence x may not be creative with respect to a keyword z, if many words in x can be readily retrieved as the norms of keyword z. In a sense, the writer was thinking the obvious. [f4: Word Norms Fraction] We use the Leuven dataset, which consists of norms for 1,424 keywords (De Deyne and Storms, 2008b; De Deyne and Storms, 2008a). The original Leuven dataset is in Dutch, we use a version that is translated into English. For each sentence x, we first exclude the keyword z from the sentence. We also remove punctuations, and map all words to lower case. We furt</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Joshua Tenenbaum. 2005. The large scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive Science, 29(1):41–78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>