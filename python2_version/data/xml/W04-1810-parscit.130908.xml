<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000805">
<note confidence="0.962223">
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 75
</note>
<title confidence="0.990473">
Quantitative Portraits of Lexical Elements
</title>
<author confidence="0.889454">
Kyo Kageura
</author>
<affiliation confidence="0.8805405">
Human and Social Information Research Division
National Institute of Informatics,
</affiliation>
<address confidence="0.9832315">
2-1-2 Hitotsubashi, Chiyoda-ku,
Tokyo, 101-8430, Japan
</address>
<email confidence="0.999728">
kyo@nii.ac.jp
</email>
<sectionHeader confidence="0.995655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999236333333333">
This paper clarifies the basic concepts and theoret-
ical perspectives by and from which quantitative
“weighting” of lexical elements are defined, and
then draws, quantitative portraits of a few lexical el-
ements in order to exemplify the relevance of the
concepts and perspectives examined.
</bodyText>
<figure confidence="0.9714481">
Textual sphere / theoretical sphere of discourse
Lexicological sphere / theoretical sphere of lexica
Terms as attributes of concrete set of documents
text
term term erm erm term term temterm
A set of actual texts (targets of IR)
xt
text
text te text
text
</figure>
<sectionHeader confidence="0.954499" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996515">
Since Luhn’s pioneering work (Luhn, 1958) in au-
tomatic term weighting, many methods have been
proposed in the fields of IR (e.g. Spark-Jones, 1973;
Harter, 1975) and NLP (e.g. Church et al., 1990).
Some “standard” methods of term weighting such
as have been established (Aizawa, 2003;
, 1999) and the application range has widened;
term weighting has become a mature technology.
Despite this, what has been technically proposed
has not been examined from a theoretical point
of view, i.e. what kind of weighting scheme re-
flects what kind of lexical nature within what kind
of framework of interpretations in language. We
will clarify this and then illustrate the relevance of
this clarification by drawing quantitative portraits of
some lexical items using the quantitative measures.
</bodyText>
<sectionHeader confidence="0.923516" genericHeader="introduction">
2 Texts and lexica
</sectionHeader>
<bodyText confidence="0.999135785714286">
Automatic term weighting starts from
texts/documents. To what spheres the weights
are attributed can differ. Figure 1 shows the lin-
guistic spheres of lexica and texts (Kageura, 2002);
there are both concrete data spheres and abstract
spheres on both the lexical and textual sides.
Within this scheme, three types of relations be-
tween lexica and texts can be identified: concrete
terms attributed to concrete texts, concrete terms
corresponding to discourse, and abstract lexica cor-
responding to abstract discourse. We will show be-
low that three major types of automatic term weight-
ing methods correspond to these three types of rela-
tions between lexica and texts.
</bodyText>
<figureCaption confidence="0.995752">
Figure 1: Textual sphere and lexicological sphere.
</figureCaption>
<sectionHeader confidence="0.789045" genericHeader="method">
3 Methods of term weighting
</sectionHeader>
<subsectionHeader confidence="0.986585">
3.1 Tfidf
</subsectionHeader>
<bodyText confidence="0.909859">
is defined as:
</bodyText>
<equation confidence="0.816747">
(1)
</equation>
<bodyText confidence="0.997076833333333">
where is the total frequency of a term, is the
total number of the documents, andis the total
number of documents in which the termoccurs.
Aizawa (2003) has shown that this can be derived
from an information theoretic measure. Let and
be random variables defined over events in a set
of documents and a set
of different terms in .
Letdenote the frequency ofin ,the to-
tal frequency of, the total number of running
terms in , and the total number of term tokens
in . The “weight” of a termcan be given by:
Giving probabilities by relative frequencies, and as-
suming that all the documents have equal size and
the frequency ofin the documents that contain
is equal, this measure becomes ; has an
information theoretic meaning within the given set
of documents (Figure 2).
</bodyText>
<subsectionHeader confidence="0.997503">
3.2 Term representativeness
</subsectionHeader>
<bodyText confidence="0.9412896">
Hisamitsu, et al. (2000a) proposed a measure of
“term representativeness”, in order to overcome the
76 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
where denotes the set of all documents; the
distribution of words in ;a focal term; the
set of all documents containing; distribution
of words in ; distribution of words in ran-
domly selected documents whose size equals ;
the distance between two distributions
of wordsand . Log-likelihood ratio was used
to measure the distance.
This measure observes the centripetal force of a
term vis-`a-vis discourse. i.e. it captures the charac-
teristic of terms in the general discourse as repre-
sented by the given set of documents (Figure 3).
</bodyText>
<subsectionHeader confidence="0.966138">
3.3 Lexical productivity
</subsectionHeader>
<bodyText confidence="0.99962317948718">
Nakagawa (2000) incorporates a factor of lexical
productivity of constituent elements of compound
units for complex term extraction. The method ob-
serves in how many different compounds an ele-
mentis used in a given document set (let us de-
note this as where indicates the size of
the overall document set as counted by the number
of word tokens), and used that in the weighting of
compounds containing, by taking weighted aver-
age. By explicitly limiting the syntagmatic range
of observation of cooccurrence to the unit of com-
pounds, he focused on the lexical productivity as
manifasted in texts.
This measure depends on the token occurrence,
but we can also think of the theoretical lexical pro-
ductivity in the lexicological sphere: how many
compoundscan potentially make” (let us denote
this by). For that, it is necessary to remove the
factor of token occurrence. This can be done by:
This has so far been unexplored. Potential lex-
ical productivity of an element can be estimated
from textual data: Letting be the occurrence
probability of in texts, be the token
occurrence ofin texts, andbe the sample
space of the distribution of com-
pounds (and simplex word) that containswith
probability given to each compound , and
assuming the combination of binomial distribution,
we have:
What is given in the data is the empirical value for
, with the empirical distributions of what ac-
tually occur in the document set among.can
be estimated by LNRE methods (Baayen, 2001).
Being a measure representing the potential power
of a lexical elementfor constructing compounds,
indicates the lexical productivity in the lex-
icological sphere which correspond to theoretical
sphere of discourse as represented by the given doc-
ument set (Figure 4).
</bodyText>
<figureCaption confidence="0.99903">
Figure 4: The position of lexical productivity.
</figureCaption>
<figure confidence="0.976578866666667">
Terms as an attribute of autonomous lexicological shpere
Lexicological sphere / theoretical sphere of lexica
A set of actual texts (a ladder to be discarded)
Textual sphere / theoretical sphere of discourse
text
term term erm erm term term temterm
xt
text
text te
text
text
term term erm erm term term temterm
Terms as attributes of concrete set of documents
text
A set of actual texts (targets of IR)
</figure>
<figureCaption confidence="0.964117">
Figure 2: The position of .
</figureCaption>
<figure confidence="0.9021733">
A set of actual texts (a manifestation of discourse)
Terms as attributes of theoretical discourse
represented by the given set of documents
text
term term erm erm term term termterm
xt
text
text te text
text
Textual sphere / theoretical sphere of discourse
</figure>
<figureCaption confidence="0.999977">
Figure 3: The position of term representativeness.
</figureCaption>
<bodyText confidence="0.954286052631579">
excessive sensitivity of weighting measures to to-
ken frequencies. They hypothesised that, for a term
, if the term is representative, (the set of all
documents containing) have some specific char-
acteristic. They define a measure which calculates
the distance between a distributional characteristic
of words aroundand the same distributional char-
acteristic in the whole document set.
In order to remove the factor of data size depen-
dency, Hisamitsu et al. (2000a) defines the “baseline
function,” which indicates the distance between the
distribution of words in the original document set
and the distribution of words in randomly selected
document subsets for each size. The distance be-
tween the distribution of words in the original doc-
ument set and the distribution of words in the doc-
uments which accompany the focal termis nor-
malised by the “baseline function.”
Formally,
</bodyText>
<figure confidence="0.987901625">
(2)
xt
text
text
te
text
text
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 77
</figure>
<sectionHeader confidence="0.581439" genericHeader="method">
4 Portraits of lexical elements
</sectionHeader>
<bodyText confidence="0.9713244">
As the three different measures capture three differ-
ent aspects of lexical elements, they are not com-
petitive 1. We here use these measures to illustrate
characteristics of a few lexical elements.
We used NII morphologically tagged-corpus for
observation (Okada et al., 2001), which consists of
Japanese abstracts in the field of artificial intelli-
gence. Table 1 shows the basic quantitative infor-
mation.
No. of word tokens word types
</bodyText>
<equation confidence="0.4620445">
abstracts (simplex/compound) (simp./comp.)
1816 299846/230708 8764/23243
</equation>
<tableCaption confidence="0.994928">
Table 1: The basic data for NII corpus.
</tableCaption>
<bodyText confidence="0.996105153846154">
We chose the six most frequently occurring nom-
inal element for observation, i.e. (system),
(knowledge), (learning), (problem),
(model), and (information). Intu-
itively, “system”, and “model” are rather general
with respect to the domain of artificial intelligence,
“knowledge” and “learning” are domain specific,
and “information” and “problem” are in between.
Table 2 shows the basic quantitative information for
these six lexical elements.
Figure 5 plots and term representativeness
for the six elements. Table 3 shows the estimated
value of lexical productivity.
</bodyText>
<table confidence="0.956717">
system 0.96 273402688337
knowledge 0.88 689
learning 0.39 2251563675
problem 0.70 1951
model 0.47 3676671255
information 0.84 667
</table>
<tableCaption confidence="0.998114">
Table 3: Lexical productivity for the six elements.
</tableCaption>
<bodyText confidence="0.9287375">
Figure 5 shows “learning” and “knowledge”, in-
tuitively the domain-dependent elements, take high
</bodyText>
<footnote confidence="0.995063666666667">
1It is thus simplistic to evaluate which measures work better
in an application, unless the conceptual status of the applica-
tions is sufficiently clarified.
</footnote>
<bodyText confidence="0.99481962962963">
values, while “information” takes the lowest
value. Term representativeness gives “learning” a
high value but the values of “knowledge” is much
lower, and about the same as “information”. In-
terestingly, the lexical productivity of “knowledge”
and “information” is also very close to each other.
It is possible to infer from these values of term
representativeness and lexical productivity that both
“information” and “knowledge” are, within the dis-
course of artificial intelligence, not with high cen-
tripetal value as both are rather “base” concepts of
the domain. If we observe Table 2, “knowledge” is
more often used as it is, while “information” tends
to occur as compounds. From this we might be
able to hypothesise that “knowledge” is in itself the
“base” concept of artificial intelligence while “in-
formation” becomes the “base” concept in combina-
tion with other lexical items. This fits our intuition,
as “information” in itself is more a “base” concept
of information and computer science, which is a
broader domain of which artificial intelligence is
a subdomain. The low value of “informa-
tion” comes from the low token frequency coupled
with relatively high DF, which shows that “informa-
tion”, as long as it is used, tends to scatter across
documents. This is in accordance with the inter-
pretation that “information” tends to occur in com-
pounds. Still, however, it is difficult to interpret sen-
sibly the fact that the value of “information”
is lower than those of “model” and “system”. Per-
haps it is more sensible to interpret among
elements which take the values of term representa-
tiveness higher than a certain threshold. Then we
can say that “learning” and “knowledge” represent
concepts more “central” to the domain of artificial
intelligence than “information”.
The element “learning”, which takes the highest
values both in and in term representativeness,
is conspicuous in its lexical productivity. Compared
to “knowledge” whose value is also high,
and with the three elements “problem”, “informa-
tion” and “knowledge” whose term representative-
ness values are relatively high, the order of lexical
productivity of “learning” is a million times higher
(and similar to “model” or “system”). Table 2 shows
that “learning” does not occur much as it is, nor does
it occur much as the head of compounds. This in-
dicates that “learning” represents an important con-
cept of the given data and in the discourse of ar-
tificial intelligence, but only “indirectly” in com-
bination with other elements in compounds where
“learning” tend to contribute to as a modifier rather
than a head.
The two “general” lexical elements, i.e. “model”
</bodyText>
<figure confidence="0.998753333333333">
•
•
•
•
•
•
</figure>
<figureCaption confidence="0.997871">
Figure 5: and term representativeness.
</figureCaption>
<table confidence="0.925585166666667">
78 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
TF DF Comp(A) Comp(H) Simp (A) (H)
system 2659 989 1922 1247 737 937 502
knowledge 2183 669 1399 443 784 424 137
learning 1776 462 1513 208 263 375 73
problem 1758 660 1197 558 561 334 152
model 1480 550 1144 687 343 447 263
information 1038 460 656 268 382 207 155
Note: Comp(A) indicates the number of compounds that contains the lexical element; Comp(H) indicates the number
of compounds that contains the lexical element as the head; (A) indicates the number of different compounds
(plus one simplex) that contains the lexical element; (H) indicates the number of different compounds (plus
one simplex) that contains the lexical element as the head.
</table>
<tableCaption confidence="0.999583">
Table 2: The basic data for the six lexical elements.
</tableCaption>
<bodyText confidence="0.998067884615385">
and “system”, take low term representativeness val-
ues2. This is in accordance with our intuition. The
lexical productivity of these two elements are ex-
tremely high (practically infinite). This indicates
that these two elements can be widely used in va-
rieties of discoursal contexts, without in itself con-
tributing much to consolidating the content of dis-
course. This fits nicely to our intuitive interpretation
of the meanings of these two elements, i.e. they are
orthogonal to to such domain-dependent elements
as “knowledge” or “learning”.
This leaves us with the final element “problem”.
The value of term representativeness is high, second
only to “learning” and in between “learning” and
“information”/“knowledge”. The lexical productiv-
ity is much closer to “information” and “knowl-
edge” than to the other three. As such, “prob-
lem” can be interpreted as a kind of “base” concept,
though it retains stronger centripetal force than “in-
formation” and “knowledge”. If we ignore
values of “model” and “system” and only compare
“information”, “problem”, “learning” and “knowl-
edge”, it is also sensible to see that “problem” rep-
resent a concept more central to the domain than
“information” but less central than “learning” and
“knowledge”.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999865">
We have shown that different term weighting mea-
sures have different spheres of interpretation; on
the basis of that we have illustrated that the com-
bination of these can illustrates complex aspects of
lexical nature. Though it can be argued that the
present study does not show ways for applications
nor “empirical” evaluations within applications, we
believe that “empirical” evaluations should be prop-
erly founded by the framework of interpretation in
order for the results to be generalised in a scientific
</bodyText>
<footnote confidence="0.986995">
2This is in accordance with the observation by Hisamitsu et
al. (2000) which says that the measure of term representative-
ness is particularly useful to exclude general elements.
</footnote>
<bodyText confidence="0.950158">
way; history of sciences have shown that often re-
liance on “empirical” evaluations correlates with the
lack of theory or scientific wholesomeness.
</bodyText>
<sectionHeader confidence="0.995482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999537903225807">
Akiko N. Aizawa. 2003. An information-theoretic
perspective of tf-idf measures. Information Pro-
cessing and Management, 39(1): 45–65.
Harald Baayen. 2001. Word Frequency Distribu-
tions. Dordrecht: Kluwer.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information and lexi-
cography. Computational Linguistics, 16(1): 22–
29.
S. P. Harter. 1975. A probabilistic approach to au-
tomatic keyword indexing. Journal of the Ameri-
can Society for Information Science, 26(4): 197–
206.
Toru Hisamitsu, et. al. 2000. A method of mea-
suring term representativeness. COLING 2000,
320–326.
Kyo Kageura. 2002. The Dynamics of Terminology.
Amsterdam: John Benjamins.
Hans P. Luhn. 1958. The automatic creation of lit-
erature abstracts. IBM Journal of Research and
Development, 2(2): 159–165.
Hiroshi Nakagawa. 2000. Automatic term recogni-
tion based on statistics of compound nouns. Ter-
minology, 6(2): 195–210.
Maho Okada, et. al. 2001. Defining principled but
practically manageable lexical units in Japanese
textual corpora. NLPRS’01 Workshop on Lan-
guage Resources in Asia, 47–53.
Karen Sparck-Jones. 1973. Index term weighting.
Information Storage and Retrieval, 9(11): 619–
633.
</reference>
<equation confidence="0.570252333333333">
. 1999.
. :
.
</equation>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304260">
<title confidence="0.7460405">CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 75 Quantitative Portraits of Lexical Elements</title>
<author confidence="0.90384">Kyo Kageura</author>
<affiliation confidence="0.9508555">Human and Social Information Research Division National Institute of Informatics,</affiliation>
<address confidence="0.9936065">2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430, Japan</address>
<email confidence="0.987537">kyo@nii.ac.jp</email>
<abstract confidence="0.9985821875">This paper clarifies the basic concepts and theoretical perspectives by and from which quantitative “weighting” of lexical elements are defined, and then draws, quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined. Textual sphere / theoretical sphere of discourse Lexicological sphere / theoretical sphere of lexica Terms as attributes of concrete set of documents text term term erm erm term term temterm A set of actual texts (targets of IR) xt text tetext</abstract>
<intro confidence="0.72466">text</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akiko N Aizawa</author>
</authors>
<title>An information-theoretic perspective of tf-idf measures.</title>
<date>2003</date>
<journal>Information Processing and Management,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>45--65</pages>
<contexts>
<context position="1148" citStr="Aizawa, 2003" startWordPosition="171" endWordPosition="172">y the relevance of the concepts and perspectives examined. Textual sphere / theoretical sphere of discourse Lexicological sphere / theoretical sphere of lexica Terms as attributes of concrete set of documents text term term erm erm term term temterm A set of actual texts (targets of IR) xt text text te text text 1 Introduction Since Luhn’s pioneering work (Luhn, 1958) in automatic term weighting, many methods have been proposed in the fields of IR (e.g. Spark-Jones, 1973; Harter, 1975) and NLP (e.g. Church et al., 1990). Some “standard” methods of term weighting such as have been established (Aizawa, 2003; , 1999) and the application range has widened; term weighting has become a mature technology. Despite this, what has been technically proposed has not been examined from a theoretical point of view, i.e. what kind of weighting scheme reflects what kind of lexical nature within what kind of framework of interpretations in language. We will clarify this and then illustrate the relevance of this clarification by drawing quantitative portraits of some lexical items using the quantitative measures. 2 Texts and lexica Automatic term weighting starts from texts/documents. To what spheres the weight</context>
<context position="2592" citStr="Aizawa (2003)" startWordPosition="403" endWordPosition="404">s of relations between lexica and texts can be identified: concrete terms attributed to concrete texts, concrete terms corresponding to discourse, and abstract lexica corresponding to abstract discourse. We will show below that three major types of automatic term weighting methods correspond to these three types of relations between lexica and texts. Figure 1: Textual sphere and lexicological sphere. 3 Methods of term weighting 3.1 Tfidf is defined as: (1) where is the total frequency of a term, is the total number of the documents, andis the total number of documents in which the termoccurs. Aizawa (2003) has shown that this can be derived from an information theoretic measure. Let and be random variables defined over events in a set of documents and a set of different terms in . Letdenote the frequency ofin ,the total frequency of, the total number of running terms in , and the total number of term tokens in . The “weight” of a termcan be given by: Giving probabilities by relative frequencies, and assuming that all the documents have equal size and the frequency ofin the documents that contain is equal, this measure becomes ; has an information theoretic meaning within the given set of docume</context>
</contexts>
<marker>Aizawa, 2003</marker>
<rawString>Akiko N. Aizawa. 2003. An information-theoretic perspective of tf-idf measures. Information Processing and Management, 39(1): 45–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>Word Frequency Distributions.</title>
<date>2001</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="5448" citStr="Baayen, 2001" startWordPosition="877" endWordPosition="878"> occurrence. This can be done by: This has so far been unexplored. Potential lexical productivity of an element can be estimated from textual data: Letting be the occurrence probability of in texts, be the token occurrence ofin texts, andbe the sample space of the distribution of compounds (and simplex word) that containswith probability given to each compound , and assuming the combination of binomial distribution, we have: What is given in the data is the empirical value for , with the empirical distributions of what actually occur in the document set among.can be estimated by LNRE methods (Baayen, 2001). Being a measure representing the potential power of a lexical elementfor constructing compounds, indicates the lexical productivity in the lexicological sphere which correspond to theoretical sphere of discourse as represented by the given document set (Figure 4). Figure 4: The position of lexical productivity. Terms as an attribute of autonomous lexicological shpere Lexicological sphere / theoretical sphere of lexica A set of actual texts (a ladder to be discarded) Textual sphere / theoretical sphere of discourse text term term erm erm term term temterm xt text text te text text term term e</context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>Harald Baayen. 2001. Word Frequency Distributions. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>22--29</pages>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1): 22– 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Harter</author>
</authors>
<title>A probabilistic approach to automatic keyword indexing.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>26</volume>
<issue>4</issue>
<pages>197--206</pages>
<contexts>
<context position="1026" citStr="Harter, 1975" startWordPosition="151" endWordPosition="152">ing” of lexical elements are defined, and then draws, quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined. Textual sphere / theoretical sphere of discourse Lexicological sphere / theoretical sphere of lexica Terms as attributes of concrete set of documents text term term erm erm term term temterm A set of actual texts (targets of IR) xt text text te text text 1 Introduction Since Luhn’s pioneering work (Luhn, 1958) in automatic term weighting, many methods have been proposed in the fields of IR (e.g. Spark-Jones, 1973; Harter, 1975) and NLP (e.g. Church et al., 1990). Some “standard” methods of term weighting such as have been established (Aizawa, 2003; , 1999) and the application range has widened; term weighting has become a mature technology. Despite this, what has been technically proposed has not been examined from a theoretical point of view, i.e. what kind of weighting scheme reflects what kind of lexical nature within what kind of framework of interpretations in language. We will clarify this and then illustrate the relevance of this clarification by drawing quantitative portraits of some lexical items using the </context>
</contexts>
<marker>Harter, 1975</marker>
<rawString>S. P. Harter. 1975. A probabilistic approach to automatic keyword indexing. Journal of the American Society for Information Science, 26(4): 197– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Hisamitsu</author>
</authors>
<title>A method of measuring term representativeness. COLING</title>
<date>2000</date>
<pages>320--326</pages>
<marker>Hisamitsu, 2000</marker>
<rawString>Toru Hisamitsu, et. al. 2000. A method of measuring term representativeness. COLING 2000, 320–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
</authors>
<title>The Dynamics of Terminology.</title>
<date>2002</date>
<location>Amsterdam: John Benjamins.</location>
<contexts>
<context position="1850" citStr="Kageura, 2002" startWordPosition="281" endWordPosition="282">nology. Despite this, what has been technically proposed has not been examined from a theoretical point of view, i.e. what kind of weighting scheme reflects what kind of lexical nature within what kind of framework of interpretations in language. We will clarify this and then illustrate the relevance of this clarification by drawing quantitative portraits of some lexical items using the quantitative measures. 2 Texts and lexica Automatic term weighting starts from texts/documents. To what spheres the weights are attributed can differ. Figure 1 shows the linguistic spheres of lexica and texts (Kageura, 2002); there are both concrete data spheres and abstract spheres on both the lexical and textual sides. Within this scheme, three types of relations between lexica and texts can be identified: concrete terms attributed to concrete texts, concrete terms corresponding to discourse, and abstract lexica corresponding to abstract discourse. We will show below that three major types of automatic term weighting methods correspond to these three types of relations between lexica and texts. Figure 1: Textual sphere and lexicological sphere. 3 Methods of term weighting 3.1 Tfidf is defined as: (1) where is t</context>
</contexts>
<marker>Kageura, 2002</marker>
<rawString>Kyo Kageura. 2002. The Dynamics of Terminology. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>159--165</pages>
<contexts>
<context position="906" citStr="Luhn, 1958" startWordPosition="131" endWordPosition="132">p Abstract This paper clarifies the basic concepts and theoretical perspectives by and from which quantitative “weighting” of lexical elements are defined, and then draws, quantitative portraits of a few lexical elements in order to exemplify the relevance of the concepts and perspectives examined. Textual sphere / theoretical sphere of discourse Lexicological sphere / theoretical sphere of lexica Terms as attributes of concrete set of documents text term term erm erm term term temterm A set of actual texts (targets of IR) xt text text te text text 1 Introduction Since Luhn’s pioneering work (Luhn, 1958) in automatic term weighting, many methods have been proposed in the fields of IR (e.g. Spark-Jones, 1973; Harter, 1975) and NLP (e.g. Church et al., 1990). Some “standard” methods of term weighting such as have been established (Aizawa, 2003; , 1999) and the application range has widened; term weighting has become a mature technology. Despite this, what has been technically proposed has not been examined from a theoretical point of view, i.e. what kind of weighting scheme reflects what kind of lexical nature within what kind of framework of interpretations in language. We will clarify this an</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2): 159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Automatic term recognition based on statistics of compound nouns.</title>
<date>2000</date>
<journal>Terminology,</journal>
<volume>6</volume>
<issue>2</issue>
<pages>195--210</pages>
<contexts>
<context position="3999" citStr="Nakagawa (2000)" startWordPosition="635" endWordPosition="636">onal Workshop on Computational Terminology where denotes the set of all documents; the distribution of words in ;a focal term; the set of all documents containing; distribution of words in ; distribution of words in randomly selected documents whose size equals ; the distance between two distributions of wordsand . Log-likelihood ratio was used to measure the distance. This measure observes the centripetal force of a term vis-`a-vis discourse. i.e. it captures the characteristic of terms in the general discourse as represented by the given set of documents (Figure 3). 3.3 Lexical productivity Nakagawa (2000) incorporates a factor of lexical productivity of constituent elements of compound units for complex term extraction. The method observes in how many different compounds an elementis used in a given document set (let us denote this as where indicates the size of the overall document set as counted by the number of word tokens), and used that in the weighting of compounds containing, by taking weighted average. By explicitly limiting the syntagmatic range of observation of cooccurrence to the unit of compounds, he focused on the lexical productivity as manifasted in texts. This measure depends </context>
</contexts>
<marker>Nakagawa, 2000</marker>
<rawString>Hiroshi Nakagawa. 2000. Automatic term recognition based on statistics of compound nouns. Terminology, 6(2): 195–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maho Okada</author>
</authors>
<title>Defining principled but practically manageable lexical units</title>
<date>2001</date>
<booktitle>in Japanese textual corpora. NLPRS’01 Workshop on Language Resources in Asia,</booktitle>
<pages>47--53</pages>
<marker>Okada, 2001</marker>
<rawString>Maho Okada, et. al. 2001. Defining principled but practically manageable lexical units in Japanese textual corpora. NLPRS’01 Workshop on Language Resources in Asia, 47–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
</authors>
<title>Index term weighting.</title>
<date>1973</date>
<journal>Information Storage and Retrieval,</journal>
<volume>9</volume>
<issue>11</issue>
<pages>619--633</pages>
<marker>Sparck-Jones, 1973</marker>
<rawString>Karen Sparck-Jones. 1973. Index term weighting. Information Storage and Retrieval, 9(11): 619– 633.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>