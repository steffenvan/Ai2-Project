<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.274696">
<title confidence="0.993653">
CNTS: Memory-Based Learning of Generating Repeated References
</title>
<author confidence="0.998689">
Iris Hendrickx, Walter Daelemans, Kim Luyckx, Roser Morante, Vincent Van Asch
</author>
<affiliation confidence="0.997135">
CNTS, Department of Linguistics
University of Antwerp
</affiliation>
<address confidence="0.93333">
Prinsstraat 13, 2000, Antwerp, Belgium
</address>
<email confidence="0.997747">
firstname.lastname@ua.ac.be
</email>
<sectionHeader confidence="0.995824" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9930033">
In this paper we describe our machine learning
approach to the generation of referring expres-
sions. As our algorithm we use memory-based
learning. Our results show that in case of pre-
dicting the TYPE of the expression, having one
general classifier gives the best results. On the
contrary, when predicting the full set of prop-
erties of an expression, a combined set of spe-
cialized classifiers for each subdomain gives
the best performance.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996595">
In this paper we describe the systems with which
we participated in the GREC task of the REG 2008
challenge (Belz and Varges, 2007). The GREC task
concerns predicting which expression is appropriate
to refer to a particular discourse referent in a certain
position in a text, given a set of alternative referring
expressions for selection. The organizers provided
the GREC corpus that consists of 2000 texts col-
lected from Wikipedia, from 5 different subdomains
(people, cities, countries, mountains and rivers) .
One of the main goals of the task is to discover
what kind of information is useful in the input to
make the decision between candidate referring ex-
pressions. We experimented with a pool of features
and several machine learning algorithms in order to
achieve this goal.
</bodyText>
<sectionHeader confidence="0.968813" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999911777777778">
We apply a standard machine learning approach
to the task. We train a classifier to predict the
correct label for each mention. As our machine
learning algorithm we use memory-based learn-
ing as implemented in the Timbl package (Daele-
mans et al., 2007). To select the optimal algorith-
mic parameter setting for each classifier we used
a heuristic optimization method called paramsearch
(Van den Bosch, 2004). We also tried several other
machine learning algorithms implemented in the
Weka package (Witten and Frank, 2005), but these
experiments did not lead to better results and are not
further discussed here.
We developed four systems: a system that only
predicts the TYPE of each expression (Type), so it
predicts four class labels; and a system that pre-
dicts the four properties (TYPE, EMPATHIC, HEAD,
CASE) of each expression simultaneously (Prop).
The class labels predicted by this system are con-
catenated strings: ’common no nominal plain’, and
these concatenations lead to 14 classes, which
means that not all combinations appear in the train-
ing set. For both Type an Prop we created two vari-
ants: one general classifer (g) that is trained on all
subdomains, and a set of combined specialized clas-
sifiers (s) that are optimized for each domain sepa-
rately.
</bodyText>
<sectionHeader confidence="0.993367" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.999926285714286">
To build the feature representations, we first prepro-
cessed the texts performing the following actions:
rule-based tokenization, memory-based part-of-
speech tagging, NP-chunking, Named entity recog-
nition, and grammatical relation finding (Daelemans
and van den Bosch, 2005). We create an instance for
each mention, using the following features to repre-
</bodyText>
<page confidence="0.963252">
194
</page>
<equation confidence="0.400762">
sent each instance:
</equation>
<listItem confidence="0.999808466666667">
• Positional features: the sentence number, the
NP number, a boolean feature that indicates if
the mention appears in the first sentence.
• Syntactic and semantic category given of the
entity (SEMCAT, SYNCAT).
• Local context of 3 words and POS tags left and
right of the entity.
• Distance to the previous mention measured in
sentences and in NPs.
• Trigram pattern of the given syntactic cate-
gories of 3 previous mentions.
• Boolean feature indicating if the previous sen-
tence contains another named entity than the
entity in focus.
• the main verb of the sentence.
</listItem>
<bodyText confidence="0.999916545454545">
We do not use any information about the given set
of alternative expressions except for post process-
ing. In a few cases our classifier predicts a label that
is not present in the set of alternatives. For those
cases we choose the most frequent class label (as es-
timated on the training set).
We experimented with predicting all subdomains
with the same classifier and with creating separate
classifiers for each subdomains. We expected that
semantically different domains would have different
preferences for expressions.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999692">
We provide results for the four systems Type-g,
Type-s, Prop-g and Prop-s in Table 1. The evalua-
tion script was provided by the organisers. The vari-
ant Type-g performs best with a score of 76.52% on
the development set.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.996026285714286">
In this paper we described our machine learning ap-
proach to the generation of referring expressions.
We reported results of four memory-based systems.
Predicting all subdomains with the same classifier
is more efficient when predicting the coarse-grained
TYPE class. On the contrary, training specialized
classifiers for each subdomain works better for the
</bodyText>
<table confidence="0.998799071428571">
Data Type-g Type-s
Cities 64.65 60.61
Countries 75.00 71.74
Mountains 75.42 77.07
People 85.37 72.50
Rivers 65.00 80.00
All 76.52 72.26
Data Prop-g Prop-s
Cities 63.64 65.66
Countries 72.83 69.57
Mountains 72.08 74.58
People 79.51 79.51
Rivers 65.00 70.00
All 73.02 73.93
</table>
<tableCaption confidence="0.99995">
Table 1: Accuracy on GREC development set.
</tableCaption>
<bodyText confidence="0.998312333333333">
more fine-grained prediction of all properties simu-
laneously. For the test set we will present results the
two best systems: CNTS-Type-g and CNTS-Prop-s.
</bodyText>
<sectionHeader confidence="0.997491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.985482333333333">
This research is funded by FWO, IWT, GOA BOF UA,
and the STEVIN programme funded by the Dutch and
Flemish Governments.
</bodyText>
<sectionHeader confidence="0.998483" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998943888888889">
A. Belz and S. Varges. 2007. Generation of repeated ref-
erences to discourse entities. In In Proceedings of the
11th European Workshop on Natural Language Gen-
eration (ENLG’07), pages 9–16.
W. Daelemans and A. van den Bosch. 2005. Memory-
based language processing. Cambridge University
Press, Cambridge, UK.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 2007. TiMBL: Tilburg Memory
Based Learner, version 6.1, reference manual. Techni-
cal Report 07-07, ILK, Tilburg University.
A. Van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters.
In Proceedings of the 16th Belgian-Dutch Conference
on Artificial Intelligence, pages 219–226.
I. H. Witten and E. Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques, second
edition. Morgan Kaufmann, San Francisco.
</reference>
<page confidence="0.998942">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765214">
<title confidence="0.999451">CNTS: Memory-Based Learning of Generating Repeated References</title>
<author confidence="0.999302">Iris Hendrickx</author>
<author confidence="0.999302">Walter Daelemans</author>
<author confidence="0.999302">Kim Luyckx</author>
<author confidence="0.999302">Roser Morante</author>
<author confidence="0.999302">Vincent Van</author>
<affiliation confidence="0.9555975">CNTS, Department of University of</affiliation>
<address confidence="0.898803">Prinsstraat 13, 2000, Antwerp,</address>
<email confidence="0.997777">firstname.lastname@ua.ac.be</email>
<abstract confidence="0.992072636363636">In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of prethe the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>S Varges</author>
</authors>
<title>Generation of repeated references to discourse entities. In</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG’07),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="856" citStr="Belz and Varges, 2007" startWordPosition="127" endWordPosition="130">irstname.lastname@ua.ac.be Abstract In this paper we describe our machine learning approach to the generation of referring expressions. As our algorithm we use memory-based learning. Our results show that in case of predicting the TYPE of the expression, having one general classifier gives the best results. On the contrary, when predicting the full set of properties of an expression, a combined set of specialized classifiers for each subdomain gives the best performance. 1 Introduction In this paper we describe the systems with which we participated in the GREC task of the REG 2008 challenge (Belz and Varges, 2007). The GREC task concerns predicting which expression is appropriate to refer to a particular discourse referent in a certain position in a text, given a set of alternative referring expressions for selection. The organizers provided the GREC corpus that consists of 2000 texts collected from Wikipedia, from 5 different subdomains (people, cities, countries, mountains and rivers) . One of the main goals of the task is to discover what kind of information is useful in the input to make the decision between candidate referring expressions. We experimented with a pool of features and several machin</context>
</contexts>
<marker>Belz, Varges, 2007</marker>
<rawString>A. Belz and S. Varges. 2007. Generation of repeated references to discourse entities. In In Proceedings of the 11th European Workshop on Natural Language Generation (ENLG’07), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>Memorybased language processing.</title>
<date>2005</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<marker>Daelemans, van den Bosch, 2005</marker>
<rawString>W. Daelemans and A. van den Bosch. 2005. Memorybased language processing. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>TiMBL: Tilburg Memory Based Learner, version 6.1, reference manual.</title>
<date>2007</date>
<tech>Technical Report 07-07,</tech>
<institution>ILK, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2007</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 2007. TiMBL: Tilburg Memory Based Learner, version 6.1, reference manual. Technical Report 07-07, ILK, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<title>Wrapped progressive sampling search for optimizing learning algorithm parameters.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th Belgian-Dutch Conference on Artificial Intelligence,</booktitle>
<pages>219--226</pages>
<marker>Van den Bosch, 2004</marker>
<rawString>A. Van den Bosch. 2004. Wrapped progressive sampling search for optimizing learning algorithm parameters. In Proceedings of the 16th Belgian-Dutch Conference on Artificial Intelligence, pages 219–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques, second edition.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="2037" citStr="Witten and Frank, 2005" startWordPosition="317" endWordPosition="320">with a pool of features and several machine learning algorithms in order to achieve this goal. 2 Method We apply a standard machine learning approach to the task. We train a classifier to predict the correct label for each mention. As our machine learning algorithm we use memory-based learning as implemented in the Timbl package (Daelemans et al., 2007). To select the optimal algorithmic parameter setting for each classifier we used a heuristic optimization method called paramsearch (Van den Bosch, 2004). We also tried several other machine learning algorithms implemented in the Weka package (Witten and Frank, 2005), but these experiments did not lead to better results and are not further discussed here. We developed four systems: a system that only predicts the TYPE of each expression (Type), so it predicts four class labels; and a system that predicts the four properties (TYPE, EMPATHIC, HEAD, CASE) of each expression simultaneously (Prop). The class labels predicted by this system are concatenated strings: ’common no nominal plain’, and these concatenations lead to 14 classes, which means that not all combinations appear in the training set. For both Type an Prop we created two variants: one general c</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. H. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques, second edition. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>