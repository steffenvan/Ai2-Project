<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001419">
<title confidence="0.791943">
Can You Repeat That?
Using Word Repetition to Improve Spoken Term Detection
</title>
<author confidence="0.876161">
Jonathan Wintrode and Sanjeev Khudanpur
</author>
<affiliation confidence="0.80174">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<email confidence="0.997067">
jcwintr@cs.jhu.edu , khudanpur@jhu.edu
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998647826087">
We aim to improve spoken term detec-
tion performance by incorporating con-
textual information beyond traditional N-
gram language models. Instead of taking a
broad view of topic context in spoken doc-
uments, variability of word co-occurrence
statistics across corpora leads us to fo-
cus instead the on phenomenon of word
repetition within single documents. We
show that given the detection of one in-
stance of a term we are more likely to
find additional instances of that term in the
same document. We leverage this bursti-
ness of keywords by taking the most con-
fident keyword hypothesis in each docu-
ment and interpolating with lower scor-
ing hits. We then develop a principled
approach to select interpolation weights
using only the ASR training data. Us-
ing this re-weighting approach we demon-
strate consistent improvement in the term
detection performance across all five lan-
guages in the BABEL program.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944418181819">
The spoken term detection task arises as a key sub-
task in applying NLP applications to spoken con-
tent. Tasks like topic identification and named-
entity detection require transforming a continu-
ous acoustic signal into a stream of discrete to-
kens which can then be handled by NLP and other
statistical machine learning techniques. Given a
small vocabulary of interest (1000-2000 words or
multi-word terms) the aim of the term detection
task is to enumerate occurrences of the keywords
within a target corpus. Spoken term detection con-
verts the raw acoustics into time-marked keyword
occurrences, which may subsequently be fed (e.g.
as a bag-of-terms) to standard NLP algorithms.
Although spoken term detection does not re-
quire the use of word-based automatic speech
recognition (ASR), it is closely related. If we
had perfectly accurate ASR in the language of
the corpus, term detection is reduced to an exact
string matching task. The word error rate (WER)
and term detection performance are clearly corre-
lated. Given resource constraints, domain, chan-
nel, and vocabulary limitations, particularly for
languages other than English, the errorful token
stream makes term detection a non-trivial task.
In order to improve detection performance, and
restricting ourselves to an existing ASR system
or systems at our disposal, we focus on leverag-
ing broad document context around detection hy-
potheses. ASR systems traditionally use N-gram
language models to incorporate prior knowledge
of word occurrence patterns into prediction of the
next word in the token stream. N-gram mod-
els cannot, however, capture complex linguistic or
topical phenomena that occur outside the typical
3-5 word scope of the model. Yet, though many
language models more sophisticated than N-grams
have been proposed, N-grams are empirically hard
to beat in terms of WER.
We consider term detection rather than the tran-
scription task in considering how to exploit topic
context, because in evaluating the retrieval of cer-
tain key terms we need not focus on improving
the entire word sequence. Confidence scores from
an ASR system (which incorporate N-gram prob-
abilities) are optimized in order to produce the
most likely sequence of words rather than the ac-
curacy of individual word detections. Looking at
broader document context within a more limited
task might allow us to escape the limits of N-gram
performance. We will show that by focusing on
contextual information in the form of word repe-
tition within documents, we obtain consistent im-
provement across five languages in the so called
Base Phase of the IARPA BABEL program.
</bodyText>
<page confidence="0.928619">
1316
</page>
<note confidence="0.864636">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1316–1325,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.98451">
1.1 Task Overview
</subsectionHeader>
<bodyText confidence="0.999977515151515">
We evaluate term detection and word repetition-
based re-scoring on the IARPA BABEL training
and development corpora1 for five languages Can-
tonese, Pashto, Turkish, Tagalog and Vietnamese
(Harper, 2011). The BABEL task is modeled on
the 2006 NIST Spoken Term Detection evaluation
(NIST, 2006) but focuses on limited resource con-
ditions. We focus specifically on the so called no
target audio reuse (NTAR) condition to make our
method broadly applicable.
In order to arrive at our eventual solution, we
take the BABEL Tagalog corpus and analyze word
co-occurrence and repetition statistics in detail.
Our observation of the variability in co-occurrence
statistics between Tagalog training and develop-
ment partitions leads us to narrow the scope of
document context to same word co-occurrences,
i.e. word repetitions.
We then analyze the tendency towards within-
document repetition. The strength of this phe-
nomenon suggests it may be more viable for im-
proving term-detection than, say, topic-sensitive
language models. We validate this by develop-
ing an interpolation formula to boost putative word
repetitions in the search results, and then inves-
tigate a method for setting interpolation weights
without manually tuning on a development set.
We then demonstrate that the method general-
izes well, by applying it to the 2006 English data
and the remaining four 2013 BABEL languages.
We demonstrate consistent improvements in all
languages in both the Full LP (80 hours of ASR
training data) and Limited LP (10 hours) settings.
</bodyText>
<sectionHeader confidence="0.97083" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.9999645">
We seek a workable definition of broad docu-
ment context beyond N-gram models that will im-
prove term detection performance on an arbitrary
set of queries. Given the rise of unsupervised la-
tent topic modeling with Latent Dirchlet Alloca-
tion (Blei et al., 2003) and similar latent variable
approaches for discovering meaningful word co-
occurrence patterns in large text corpora, we ought
to be able to leverage these topic contexts instead
of merely N-grams. Indeed there is work in the
literature that shows that various topic models, la-
tent or otherwise, can be useful for improving lan-
</bodyText>
<footnote confidence="0.957006">
1Language collection releases IARPA-babel101-v0.4c,
IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPA-
babel106-v0.2g and IARPA-babel107b-v0.7 respectively.
</footnote>
<bodyText confidence="0.9868813">
guage model perplexity and word error rate (Khu-
danpur and Wu, 1999; Chen, 2009; Naptali et
al., 2012). However, given the preponderance of
highly frequent non-content words in the compu-
tation of a corpus’ WER, it’s not clear that a 1-2%
improvement in WER would translate into an im-
provement in term detection.
Still, intuition suggests that knowing the topic
context of a detected word ought to be useful
in predicting whether or not a term does belong
in that context. For example, if we determine
the context of the detection hypothesis is about
computers, containing words like ‘monitor,’ ‘in-
ternet’ and ‘mouse,’ then we would be more con-
fident of a term such as ‘keyboard’ and less con-
fident of a term such as ‘cheese board’. The dif-
ficulty in this approach arises from the variabil-
ity in word co-occurrence statistics. Using topic
information will be helpful if ‘monitor,’ ‘key-
board’ and ‘mouse’ consistently predict that ‘key-
board’ is present. Unfortunately, estimates of co-
occurrence from small corpora are not very consis-
tent, and often over- or underestimate concurrence
probabilities needed for term detection.
We illustrate this variability by looking at how
consistent word co-occurrences are between two
separate corpora in the same language: i.e., if we
observe words that frequently co-occur with a key-
word in the training corpus, do they also co-occur
with the keywords in a second held-out corpus?
</bodyText>
<figureCaption confidence="0.9880188">
Figure 1, based on the BABEL Tagalog corpus, sug-
gests this is true only for high frequency keywords.
Figure 1: Correlation between the co-occurrence
counts in the training and held-out sets for a fixed
keyword (term) and all its “context” words.
</figureCaption>
<bodyText confidence="0.809947">
Each point in Figure 1 represents one of 355
</bodyText>
<page confidence="0.982917">
1317
</page>
<figure confidence="0.993097">
(a) High frequency keyword ‘bukas’ (b) Low frequency keyword ‘Davao’
</figure>
<figureCaption confidence="0.909699">
Figure 2: The number of times a fixed keyword k co-occurs with a vocabulary word w in the training
speech collection — T (k, w) — versus the search collection — D(k, w).
</figureCaption>
<bodyText confidence="0.999559138888889">
Tagalog keywords used for system development
by all BABEL participants. For each keyword k,
we count how often it co-occurs in the same con-
versation as a vocabulary word w in the ASR
training data and the development data, and des-
ignate the counts T(k, w) and D(k, w) respec-
tively. The x-coordinate of each point in Figure 1
is the frequency of k in the training data, and the
y-coordinate is the correlation coefficient pk be-
tween T(k, w) and D(k, w). A high pk implies
that words w that co-occur frequently with k in the
training data also do so in the search collection.
To further illustrate how Figure 1 was obtained,
consider the high-frequency keyword bukas (count
= 879) and the low-frequency keyword Davao
(count = 11), and plot T(k, ·) versus D(k, ·),
as done in Figure 2. The correlation coefficients
pbukas and pDavao from the two plots end up as two
points in Figure 1.
Figure 1 suggests that (k, w) co-occurrences are
consistent between the two corpora (pk &gt; 0.8) for
keywords occurring 100 or more times. However,
if the goal is to help a speech retrieval system de-
tect content-rich (and presumably infrequent) key-
words, then using word co-occurrence informa-
tion (i.e. topic context) does not appear to be
too promising, even though intuition suggests that
such information ought to be helpful.
In light of this finding, we will restrict the type
of context we use for term detection to the co-
occurrence of the term itself elsewhere within the
document. As it turns out this ‘burstiness’ of
words within documents, as the term is defined by
Church and Gale in their work on Poisson mix-
tures (1995), provides a more reliable framework
for successfully exploiting document context.
</bodyText>
<sectionHeader confidence="0.93602" genericHeader="related work">
2.1 Related Work
</sectionHeader>
<bodyText confidence="0.999990484848485">
A number of efforts have been made to augment
traditional N-gram models with latent topic infor-
mation (Khudanpur and Wu, 1999; Florian and
Yarowsky, 1999; Liu and Liu, 2008; Hsu and
Glass, 2006; Naptali et al., 2012) including some
of the early work on Probabilistic Latent Semantic
Analysis by Hofmann (2001). In all of these cases
WER gains in the 1-2% range were observed by
interpolating latent topic information with N-gram
models.
The re-scoring approach we present is closely
related to adaptive or cache language models (Je-
linek, 1997; Kuhn and De Mori, 1990; Kneser and
Steinbiss, 1993). The primary difference between
this and previous work on similar language mod-
els is the narrower focus here on the term detec-
tion task, in which we consider each search term in
isolation, rather than all words in the vocabulary.
Most recently, Chiu and Rudnicky (2013) looked
at word bursts in the IARPA BABEL conversational
corpora, and were also able to successfully im-
prove performance by leveraging the burstiness of
language. One advantage of the approach pro-
posed here, relative to their approach, is its sim-
plicity and its not requiring an additional tuning
set to estimate parameters.
In the information retrieval community, cluster-
ing and latent topic models have yielded improve-
ments over traditional vector space models. We
will discuss in detail in the following section re-
lated works by Church and Gale (1995, 1999, and
2000). Work by Wei and Croft (2006) and Chen
(2009) take a language model-based approach to
</bodyText>
<page confidence="0.936584">
1318
</page>
<figure confidence="0.9996">
(a) fw versus IDFw
‘
(b) Obsered versus predicted IDFw
</figure>
<figureCaption confidence="0.999936">
Figure 3: Tagalog corpus frequency statistics, unigrams
</figureCaption>
<bodyText confidence="0.999872181818182">
information retrieval, and again, interpolate latent
topic models with N-grams to improve retrieval
performance. However, in many text retrieval
tasks, queries are often tens or hundreds of words
in length rather than short spoken phrases. In these
efforts, the topic model information was helpful in
boosting retrieval performance above the baseline
vector space or N-gram models.
Clearly topic or context information is relevant
to a retrieval type task, but we need a stable, con-
sistent framework in which to apply it.
</bodyText>
<sectionHeader confidence="0.9045695" genericHeader="method">
3 Term and Document Frequency
Statistics
</sectionHeader>
<bodyText confidence="0.999939789473684">
To this point we have assumed an implicit property
of low-frequency words which Church and Gale
state concisely in their 1999 study of inverse doc-
ument frequency:
Low frequency words tend to be rich
in content, and vice versa. But not
all equally frequent words are equally
meaningful. Church and Gale (1999).
The typical use of Document Frequency (DF) in
information retrieval or text categorization is to
emphasize words that occur in only a few docu-
ments and are thus more “rich in content”. Close
examination of DF statistics by Church and Gale
in their work on Poisson Mixtures (1995) resulted
in an analysis of the burstiness of content words.
In this section we look at DF and burstiness
statistics applying some of the analyses of Church
and Gale (1999) to the BABEL Tagalog corpus.
We observe, in 648 Tagalog conversations, simi-
lar phenomena as observed by Church and Gale on
89,000 AP English newswire articles. We proceed
in this fashion to make a case for why burstiness
ought to help in the term detection task.
For the Tagalog conversations, as with En-
glish newswire, we observe that the document fre-
quency, DFw, of a word w is not a linear function
of word frequency fw in the log domain, as would
be expected under a naive Poisson generative as-
sumption. The implication of deviations from a
Poisson model is that words tend to be concen-
trated in a small number of documents rather than
occurring uniformly across the corpus. This is the
burstiness we leverage to improve term detection.
The first illustration of word burstiness can be
seen by plotting observed inverse document fre-
quency, IDFw, versus fw in the log domain (Fig-
ure 3a). We use the same definition of IDFw as
Church and Gale (1999):
</bodyText>
<equation confidence="0.8715975">
DFw
IDFw = −log2 N ,(1)
</equation>
<bodyText confidence="0.999976307692308">
where N is the number of documents (i.e. conver-
sations) in the corpus.
There is good linear correlation (p = 0.73) be-
tween log fw and IDFw. Yet, visually, the rela-
tionship in Figure 3a is clearly not linear. In con-
trast, the AP English data exhibits a correlation of
p = 0.93 (Church and Gale, 1999). Thus the devi-
ation in the Tagalog corpus is more pronounced,
i.e. words are less uniformly distributed across
documents.
A second perspective on word burstiness that
follows from Church and Gale (1999) is that a
Poisson assumption should lead us to predict:
</bodyText>
<equation confidence="0.9872685">
( �
�IDFw = −log2 1 − e− fwN . (2)
</equation>
<page confidence="0.939525">
1319
</page>
<figureCaption confidence="0.993849">
Figure 4: Difference between observed and pre- Figure 5: Tagalog burstiness.
dicted IDFw for Tagalog unigrams.
</figureCaption>
<bodyText confidence="0.999869304347826">
For the AP newswire, Church and Gale found the
largest deviation between the predicted IDFw and
observed IDFw to occur in the middle of the fre-
quency range. We see a somewhat different pic-
ture for Tagalog speech in Figure 3b. Observed
IDFw values again deviate significantly from their
predictions (2), but all along the frequency range.
There is a noticeable quantization effect occur-
ring in the high IDF range, given that our N is at
least a factor of 100 smaller than the number of
AP articles they studied: 648 vs. 89,000. Figure 4
also shows the difference between and observed
IDFw and Poisson estimate �IDFw and further il-
lustrates the high variance in IDFw for low fre-
quency words.
Two questions arise: what is happening with in-
frequent words, and why does this matter for term
detection? To look at the data from a different
perspective, we consider the random variable k,
which is the number of times a word occurs in a
particular document. In Figure 5 we plot the fol-
lowing ratio, which Church and Gale (1995) define
as burstiness :
</bodyText>
<equation confidence="0.9360485">
Ew[k k &gt; 0] = fw (3)
DFw
</equation>
<bodyText confidence="0.999863035714286">
as a function of fw. We denote this as E[k] and
can interpret burstiness as the expected word count
given we see w at least once.
In Figure 5 we see two classes of words emerge.
A similar phenomenon is observed concerning
adaptive language models (Church, 2000). In
general, we can think of using word repetitions
to re-score term detection as applying a limited
form of adaptive or cache language model (Je-
linek, 1997). Likewise, Katz attempts to capture
these two classes in his G model of word frequen-
cies (1996).
For the first class, burstiness increases slowly
but steadily as w occurs more frequently. Let us
label these Class A words. Since our corpus size
is fixed, we might expect this to occur, as more
word occurrences must be pigeon-holed into the
same number of documents
Looking close to the y-axis in Figure 5, we ob-
serve a second class of exclusively low frequency
words whose burstiness ranges from highly con-
centrated to singletons. We will refer to these as
Class B words. If we take the Class A concentra-
tion trend as typical, we can argue that most Class
B words exhibit a larger than average concentra-
tion. In either case we see evidence that both high
and low frequency words tend towards repeating
within a document.
</bodyText>
<subsectionHeader confidence="0.996099">
3.1 Unigram Probabilities
</subsectionHeader>
<bodyText confidence="0.999855384615385">
In applying the burstiness quantity to term detec-
tion, we recall that the task requires us to locate a
particular instance of a term, not estimate a count,
hence the utility of N-gram language models pre-
dicting words in sequence.
We encounter the burstiness property of words
again by looking at unigram occurrence probabili-
ties. We compare the unconditional unigram prob-
ability (the probability that a given word token is
w) with the conditional unigram probability, given
the term has occurred once in the document. We
compute the conditional probability for w using
frequency information.
</bodyText>
<page confidence="0.982971">
1320
</page>
<figureCaption confidence="0.9682445">
Figure 6: Difference between conditional and un-
conditional unigram probabilities for Tagalog
</figureCaption>
<equation confidence="0.998953">
P (w|k &gt; 0) = fw − DFw (4)
ED:w∈D |D|
</equation>
<bodyText confidence="0.999511444444444">
Figure 6 shows the difference between con-
ditional and unconditional unigram probabilities.
Without any other information, Zipf’s law sug-
gests that most word types do not occur in a partic-
ular document. However, conditioning on one oc-
currence, most word types are more likely to occur
again, due to their burstiness.
Finally we measure the adaptation of a word,
which is defined by Church and Gale (1995) as:
</bodyText>
<equation confidence="0.999428">
Padapt(w) = Pw(k &gt; 1|k &gt; 0) (5)
</equation>
<bodyText confidence="0.9998702">
When we plot adaptation versus fw (Figure 7)
we see that all high-frequency and a significant
number of low-frequency terms have adaptation
greater that 50%. To be precise, 26% of all to-
kens and 25% of low-frequency (fw &lt; 100) have
at least 50% adaptation. Given that adaptation val-
ues are roughly an order of magnitude higher than
the conditional unigram probabilities, in the next
two sections we describe how we use adaptation
to boost term detection scores.
</bodyText>
<figureCaption confidence="0.995545">
Figure 7: Tagalog word adaptation probability
</figureCaption>
<bodyText confidence="0.995166">
For each term t and document d we propose in-
terpolating the ASR confidence score for a partic-
ular detection td with the top scoring hit in d which
we’ll call �td.
</bodyText>
<equation confidence="0.998383">
S(td) = (1 − α)Pasr(td|O) + αPasr(�td|O) (6)
</equation>
<bodyText confidence="0.999838">
We will we develop a principled approach to se-
lecting α using the adaptation property of the cor-
pus. However to verify that this approach is worth
pursuing, we sweep a range of small α values, on
the assumption that we still do want to mostly rely
on the ASR confidence score for term detection.
For the Tagalog data, we let α range from 0 (the
baseline) to 0.4 and re-score each term detection
score according to (6). Table 1 shows the results
of this parameter sweep and yields us 1 to 2% ab-
solute performance gains in a number of term de-
tection metrics.
</bodyText>
<figure confidence="0.985284088235294">
α ATWV P(Miss)
0.00
0.422
0.420
0.418
0.416
0.430
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.417
0.417
0.415
0.413
0.413
0.410
0.470
0.481
0.483
0.484
0.483
0.480
0.477
0.475
0.471
0.465
0.462
</figure>
<sectionHeader confidence="0.964609" genericHeader="method">
4 Term Detection Re-scoring
</sectionHeader>
<bodyText confidence="0.970587142857143">
We summarize our re-scoring of repeated words
with the observation: given a correct detection,
the likelihood of additional terms in the same doc-
uments should increase. When we observe a term
detection score with high confidence, we boost the
other lower-scoring terms in the same document to
reflect this increased likelihood of repeated terms.
</bodyText>
<tableCaption confidence="0.9382495">
Table 1: Term detection scores for swept α values
on Tagalog development data
</tableCaption>
<page confidence="0.993195">
1321
</page>
<bodyText confidence="0.999268833333333">
The primary metric for the BABEL program, Ac-
tual Term Weighted Value (ATWV) is defined by
NIST using a cost function of the false alarm prob-
ability P(FA) and P(Miss), averaged over a set
of queries (NIST, 2006). The manner in which the
components of ATWV are defined:
</bodyText>
<equation confidence="0.999994">
P(Miss) = 1 − Ntrue(term)/fterm (7)
P(FA) = Nfalge/Durationcorpug (8)
</equation>
<bodyText confidence="0.999843285714286">
implies that cost of a miss is inversely proportional
to the frequency of the term in the corpus, but the
cost of a false alarm is fixed. For this reason, we
report both ATWV and the P(Miss) component.
A decrease in P(Miss) reflects the fact that we
are able to boost correct detections of the repeated
terms.
</bodyText>
<subsectionHeader confidence="0.98437">
4.1 Interpolation Weights
</subsectionHeader>
<bodyText confidence="0.99970465625">
We would prefer to use prior knowledge rather
than naive tuning to select an interpolation weight
α. Our analysis of word burstiness suggests that
adaptation, is a reasonable candidate. Adaptation
also has the desirable property that we can esti-
mate it for each word in the training vocabulary
directly from training data and not post-hoc on a
per-query basis. We consider several different es-
timates and we can show that the favorable result
extends across languages.
Intuition suggests that we prefer per-term in-
terpolation weights related to the term’s adapta-
tion. But despite the strong evidence of the adapta-
tion phenomenon in both high and low-frequency
words (Figure 7), we have less confidence in the
adaptation strength of any particular word.
As with word co-occurrence, we consider if es-
timates of Padapt(w) from training data are con-
sistent when estimated on development data. Fig-
ure 8 shows the difference between Padapt(w)
measured on the two corpora (for words occurring
in both).
We see that the adaptation estimates are only
consistent between corpora for high-frequency
words. Using this Padapt(w) estimate directly ac-
tually hurts ATWV performance by 4.7% absolute
on the 355 term development query set (Table 2).
Given the variability in estimating Padapt(w),
an alternative approach would be take Pw as an
upper bound on α, reached as the DFw increases
(cf. Equation 9). We would discount the adapta-
tion factor when DFw is low and we are unsure of
</bodyText>
<figureCaption confidence="0.9961595">
Figure 8: Difference in adaptation estimates be-
tween Tagalog training and development corpora
</figureCaption>
<table confidence="0.999842">
Interpolation Weight ATWV P(Miss)
None 0.470 0.430
Padapt(w) 0.423 0.474
(1 − e−DF,,,)Padapt(w) 0.477 0.415
α� = 0.20 0.483 0.416
</table>
<tableCaption confidence="0.992056">
Table 2: Term detection performance using vari-
</tableCaption>
<bodyText confidence="0.523169">
ous interpolation weight strategies on Tagalog dev
data
the effect.
</bodyText>
<equation confidence="0.972468">
αw = (1 − e−DF,,,) · �Padapt(w) (9)
</equation>
<bodyText confidence="0.9999384">
This approach shows a significant improvement
(0.7% absolute) over the baseline. However, con-
sidering this estimate in light of the two classes of
words in Figure 5, there are clearly words in Class
B with high burstiness that will be ignored by try-
ing to compensate for the high adaptation variabil-
ity in the low-frequency range.
Alternatively, we take a weighted average of
αw’s estimated on training transcripts to obtain a
single α� per language (cf. Equation 10).
</bodyText>
<equation confidence="0.889188">
α� = Avg L(1 − e−DF,,,) · Padapt(w)] (10)
w
</equation>
<bodyText confidence="0.999921714285714">
Using this average as a single interpolation weight
for all terms gives near the best performance as
we observed in our parameter sweep. Table 2
contrasts the results for using the three different
interpolation heuristics on the Tagalog develop-
ment queries. Using the mean α� instead of indi-
vidual αw’s provides an additional 0.5% absolute
</bodyText>
<page confidence="0.769521">
1322
</page>
<equation confidence="0.9626329375">
Language α� ATWV (%±) P(Miss) (%±)
Full LP setting
0.523 (+1.1)
0.418 (+1.3)
0.419 (+1.1)
0.466 (+0.8)
0.396 (-1.9)
0.458 (-1.9)
0.453 (-1.6)
0.430 (-1.3)
Vietnamese
English (Dev06)
0.420 (+0.7)
0.670 (+0.3)
0.445 (-1.0)
0.240 (-0.4)
</equation>
<figure confidence="0.967243454545455">
Tagalog
Cantonese
Pashto
Turkish
0.20
0.23
0.19
0.14
0.30
0.20
Limited LP setting
</figure>
<equation confidence="0.916701272727273">
0.228 (+0.9)
0.205 (+1.0)
0.206 (+0.9)
0.202 (+1.1)
0.692 (-1.7)
0.684 (-1.3)
0.682 (-0.9)
0.700 (-0.8)
Vietnamese
0.227 (+1.0)
0.646 (+0.4)
</equation>
<figure confidence="0.988906777777778">
Tagalog
Cantonese
Pashto
Turkish
0.22
0.26
0.21
0.16
0.34
</figure>
<tableCaption confidence="0.997318">
Table 3: Word-repetition re-scored results for available CTS term detection corpora
</tableCaption>
<bodyText confidence="0.8270435">
improvement, suggesting that we find additional
gains boosting low-frequency words.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999989533333334">
Now that we have tested word repetition-based
re-scoring on a small Tagalog development set
we want to know if our approach, and particu-
larly our α� estimate is sufficiently robust to apply
broadly. At our disposal, we have the five BABEL
languages — Tagalog, Cantonese, Pashto, Turk-
ish and Vietnamese — as well as the development
data from the NIST 2006 English evaluation. The
BABEL evaluation query sets contain roughly 2000
terms each and the 2006 English query set con-
tains roughly 1000 terms.
The procedure we follow for each language
condition is as follows. We first estimate adap-
tation probabilities from the ASR training tran-
scripts. From these we take the weighted aver-
age as described previously to obtain a single in-
terpolation weight α� for each training condition.
We train ASR acoustic and language models from
the training corpus using the Kaldi speech recog-
nition toolkit (Povey et al., 2011) following the
default BABEL training and search recipe which is
described in detail by Chen et al. (2013). Lastly,
we re-score the search output by interpolating the
top term detection score for a document with sub-
sequent hits according to Equation 6 using the α�
estimated for this training condition.
For each of the BABEL languages we consider
both the FullLP (80 hours) and LimitedLP (10
hours) training conditions. For the English sys-
tem, we also train a Kaldi system on the 240 hours
of the Switchboard conversational English cor-
pus. Although Kaldi can produce multiple types
of acoustic models, for simplicity we report results
using discriminatively trained Subspace Gaussian
Mixture Model (SGMM) acoustic output densi-
ties, but we do find that similar results can be ob-
tained with other acoustic model configurations.
Using our final algorithm, we are able to boost
repeated term detections and improve results in all
languages and training conditions. Table 3 lists
complete results and the associated estimates for
a. For the BABEL languages, we observe improve-
ments in ATWV from 0.7% to 1.3% absolute and
reductions in the miss rate of 0.8% to 1.9%. The
only test for which P(Miss) did not improve was
the Vietnamese Limited LP setting, although over-
all ATWV did improve, reflecting a lower P(FA).
In all conditions we also obtain α estimates
which correspond to our expectations for partic-
ular languages. For example, adaptation is low-
est for the agglutinative Turkish language where
longer word tokens should be less likely to re-
peat. For Vietnamese, with shorter, syllable length
word tokens, we observe the lowest adaptation es-
timates.
Lastly, the reductions in P(Miss) suggests that
we are improving the term detection metric, which
is sensitive to threshold changes, by doing what
we set out to do, which is to boost lower confi-
dence repeated words and correctly asserting them
</bodyText>
<page confidence="0.93772">
1323
</page>
<bodyText confidence="0.999370833333333">
as true hits. Moreover, we are able to accomplish
this in a wide variety of languages.
Quantifying the value of pronunciation lexicons for
keyword search in low resource languages. In Inter-
national Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP). IEEE.
</bodyText>
<sectionHeader confidence="0.988115" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999947681818182">
Leveraging the burstiness of content words, we
have developed a simple technique to consis-
tently boost term detection performance across
languages. Using word repetitions, we effectively
use a broad document context outside of the typi-
cal 2-5 N-gram window. Furthermore, we see im-
provements across a broad spectrum of languages:
languages with syllable-based word tokens (Viet-
namese, Cantonese), complex morphology (Turk-
ish), and dialect variability (Pashto).
Secondly, our results are not only effective but
also intuitive, given that the interpolation weight
parameter matches our expectations for the bursti-
ness of the word tokens in the language on which
it is estimated.
We have focused primarily on re-scoring results
for the term detection task. Given the effective-
ness of the technique across multiple languages,
we hope to extend our effort to exploit our hu-
man tendency towards redundancy to decoding or
other aspects of the spoken document processing
pipeline.
</bodyText>
<sectionHeader confidence="0.994587" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999736533333333">
This work was partially supported by the In-
telligence Advanced Research Projects Activity
(IARPA) via Department of Defense U.S. Army
Research Laboratory (DoD / ARL) contract num-
ber W911NF-12-C-0015. The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes notwithstanding any
copyright annotation thereon. Disclaimer: The
views and conclusions contained herein are those
of the authors and should not be interpreted as
necessarily representing the official policies or
endorsements, either expressed or implied, of
IARPA, DoD/ARL, or the U.S. Government.
Insightful discussions with Chiu and Rudnicky
(2013) are also gratefully acknowledged.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999706603773585">
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3:993–1022.
Guoguo Chen, Sanjeev Khudanpur, Daniel Povey, Jan
Trmal, David Yarowsky, and Oguz Yilmaz. 2013.
Berlin Chen. 2009. Latent topic modelling of word
co-occurence information for spoken document re-
trieval. In Proceedings of the International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), pages 3961–3964. IEEE.
Justin Chiu and Alexander Rudnicky. 2013. Using
conversational word bursts in spoken term detection.
In Proceedings of the 14th Annual Conference of
the International Speech Communication Associa-
tion, pages 2247–2251. ISCA.
Kenneth Church and William Gale. 1995. Pois-
son Mixtures. Natural Language Engineering,
1(2):163–190.
Kenneth Church and William Gale. 1999. Inverse Foc-
ument Frequency (IDF): A measure of deviations
from Poisson. In Natural Language Processing Us-
ing Very Large Corpora, pages 283–295. Springer.
Kenneth Church. 2000. Empirical estimates of adap-
tation: the chance of two Noriegas is closer to p/2
than p 2. In Proceedings of the 18th Conference
on Computational Linguistics, volume 1, pages 180–
186. ACL.
Radu Florian and David Yarowsky. 1999. Dynamic
nonlocal language modeling via hierarchical topic-
based adaptation. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics, pages 167–174. ACL.
Mary Harper. 2011. IARPA Solicitation IARPA-
BAA-11-02. http://www.iarpa.gov/
solicitations_babel.html.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42(1):177–196.
Bo-June Paul Hsu and James Glass. 2006. Style &amp;
topic language model adaptation using HMM-LDA.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing. ACL.
Fred Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
Slava Katz. 1996. Distribution of content words and
phrases in text and language modelling. Natural
Language Engineering, 2(1):15–59.
Sanjeev Khudanpur and Jun Wu. 1999. A maxi-
mum entropy language model integrating n-grams
and topic dependencies for conversational speech
recognition. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), volume 1, pages 553–556. IEEE.
</reference>
<page confidence="0.877911">
1324
</page>
<reference confidence="0.997472571428571">
Reinhard Kneser and Volker Steinbiss. 1993. On the
dynamic adaptation of stochastic language models.
In Proceedings of the International Conference on
Acoustics, Speech, and Signal Processing (ICASSP),
volume 2, pages 586–589. IEEE.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. Transactions on Pattern Analysis and Machine
Intelligence, 12(6):570–583.
Yang Liu and Feifan Liu. 2008. Unsupervised lan-
guage model adaptation via topic modeling based
on named entity hypotheses. In Proceedings of the
International Conference on Acoustics, Speech and
Signal Processing, (ICASSP), pages 4921–4924.
IEEE.
Welly Naptali, Masatoshi Tsuchiya, and Seiichi Naka-
gawa. 2012. Topic-dependent-class-based n-gram
language model. Transactions on Audio, Speech,
and Language Processing, 20(5):1513–1525.
NIST. 2006. The Spoken Term Detection (STD)
2006 Evaluation Plan. http://www.itl.
nist.gov/iad/mig/tests/std/2006/
docs/std06-evalplan-v10.pdf. [Online;
accessed 28-Feb-2013].
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The Kaldi speech recogni-
tion toolkit. In Proceedings of the Automatic Speech
Recognition and Understanding Workshop (ASRU).
Xing Wei and W Bruce Croft. 2006. LDA-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 178–185.
ACM.
</reference>
<page confidence="0.992692">
1325
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649881">
<title confidence="0.95863975">Can You Repeat That? Using Word Repetition to Improve Spoken Term Detection Wintrode Center for Language and Speech</title>
<author confidence="0.951589">Johns Hopkins</author>
<email confidence="0.998631">jcwintr@cs.jhu.edu,khudanpur@jhu.edu</email>
<abstract confidence="0.988968458333333">aim to improve term detecby incorporating contextual information beyond traditional Ngram language models. Instead of taking a broad view of topic context in spoken documents, variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents. We show that given the detection of one instance of a term we are more likely to find additional instances of that term in the document. We leverage this burstikeywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits. We then develop a principled approach to select interpolation weights using only the ASR training data. Using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five lanin the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="5757" citStr="Blei et al., 2003" startWordPosition="903" endWordPosition="906">rpolation weights without manually tuning on a development set. We then demonstrate that the method generalizes well, by applying it to the 2006 English data and the remaining four 2013 BABEL languages. We demonstrate consistent improvements in all languages in both the Full LP (80 hours of ASR training data) and Limited LP (10 hours) settings. 2 Motivation We seek a workable definition of broad document context beyond N-gram models that will improve term detection performance on an arbitrary set of queries. Given the rise of unsupervised latent topic modeling with Latent Dirchlet Allocation (Blei et al., 2003) and similar latent variable approaches for discovering meaningful word cooccurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams. Indeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving lan1Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPAbabel106-v0.2g and IARPA-babel107b-v0.7 respectively. guage model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012). However, given</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guoguo Chen</author>
<author>Sanjeev Khudanpur</author>
<author>Daniel Povey</author>
<author>Jan Trmal</author>
<author>David Yarowsky</author>
<author>Oguz Yilmaz</author>
</authors>
<date>2013</date>
<contexts>
<context position="25072" citStr="Chen et al. (2013)" startWordPosition="4158" endWordPosition="4161">ery sets contain roughly 2000 terms each and the 2006 English query set contains roughly 1000 terms. The procedure we follow for each language condition is as follows. We first estimate adaptation probabilities from the ASR training transcripts. From these we take the weighted average as described previously to obtain a single interpolation weight α� for each training condition. We train ASR acoustic and language models from the training corpus using the Kaldi speech recognition toolkit (Povey et al., 2011) following the default BABEL training and search recipe which is described in detail by Chen et al. (2013). Lastly, we re-score the search output by interpolating the top term detection score for a document with subsequent hits according to Equation 6 using the α� estimated for this training condition. For each of the BABEL languages we consider both the FullLP (80 hours) and LimitedLP (10 hours) training conditions. For the English system, we also train a Kaldi system on the 240 hours of the Switchboard conversational English corpus. Although Kaldi can produce multiple types of acoustic models, for simplicity we report results using discriminatively trained Subspace Gaussian Mixture Model (SGMM) </context>
</contexts>
<marker>Chen, Khudanpur, Povey, Trmal, Yarowsky, Yilmaz, 2013</marker>
<rawString>Guoguo Chen, Sanjeev Khudanpur, Daniel Povey, Jan Trmal, David Yarowsky, and Oguz Yilmaz. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berlin Chen</author>
</authors>
<title>Latent topic modelling of word co-occurence information for spoken document retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>3961--3964</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6318" citStr="Chen, 2009" startWordPosition="983" endWordPosition="984">th Latent Dirchlet Allocation (Blei et al., 2003) and similar latent variable approaches for discovering meaningful word cooccurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams. Indeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving lan1Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPAbabel106-v0.2g and IARPA-babel107b-v0.7 respectively. guage model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012). However, given the preponderance of highly frequent non-content words in the computation of a corpus’ WER, it’s not clear that a 1-2% improvement in WER would translate into an improvement in term detection. Still, intuition suggests that knowing the topic context of a detected word ought to be useful in predicting whether or not a term does belong in that context. For example, if we determine the context of the detection hypothesis is about computers, containing words like ‘monitor,’ ‘internet’ and ‘mouse,’ then we would be more confident of a term such as ‘keyboard’ </context>
<context position="11384" citStr="Chen (2009)" startWordPosition="1839" endWordPosition="1840">ed at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring an additional tuning set to estimate parameters. In the information retrieval community, clustering and latent topic models have yielded improvements over traditional vector space models. We will discuss in detail in the following section related works by Church and Gale (1995, 1999, and 2000). Work by Wei and Croft (2006) and Chen (2009) take a language model-based approach to 1318 (a) fw versus IDFw ‘ (b) Obsered versus predicted IDFw Figure 3: Tagalog corpus frequency statistics, unigrams information retrieval, and again, interpolate latent topic models with N-grams to improve retrieval performance. However, in many text retrieval tasks, queries are often tens or hundreds of words in length rather than short spoken phrases. In these efforts, the topic model information was helpful in boosting retrieval performance above the baseline vector space or N-gram models. Clearly topic or context information is relevant to a retriev</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Berlin Chen. 2009. Latent topic modelling of word co-occurence information for spoken document retrieval. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3961–3964. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Chiu</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using conversational word bursts in spoken term detection.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>2247--2251</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="10768" citStr="Chiu and Rudnicky (2013)" startWordPosition="1736" endWordPosition="1739">rly work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary. Most recently, Chiu and Rudnicky (2013) looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring an additional tuning set to estimate parameters. In the information retrieval community, clustering and latent topic models have yielded improvements over traditional vector space models. We will discuss in detail in the following section related works by Church and Gale (1995, 1999, and 2000). Work by Wei and Croft (2006)</context>
</contexts>
<marker>Chiu, Rudnicky, 2013</marker>
<rawString>Justin Chiu and Alexander Rudnicky. 2013. Using conversational word bursts in spoken term detection. In Proceedings of the 14th Annual Conference of the International Speech Communication Association, pages 2247–2251. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Poisson Mixtures.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="11321" citStr="Church and Gale (1995" startWordPosition="1825" endWordPosition="1828">all words in the vocabulary. Most recently, Chiu and Rudnicky (2013) looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring an additional tuning set to estimate parameters. In the information retrieval community, clustering and latent topic models have yielded improvements over traditional vector space models. We will discuss in detail in the following section related works by Church and Gale (1995, 1999, and 2000). Work by Wei and Croft (2006) and Chen (2009) take a language model-based approach to 1318 (a) fw versus IDFw ‘ (b) Obsered versus predicted IDFw Figure 3: Tagalog corpus frequency statistics, unigrams information retrieval, and again, interpolate latent topic models with N-grams to improve retrieval performance. However, in many text retrieval tasks, queries are often tens or hundreds of words in length rather than short spoken phrases. In these efforts, the topic model information was helpful in boosting retrieval performance above the baseline vector space or N-gram models</context>
<context position="15573" citStr="Church and Gale (1995)" startWordPosition="2560" endWordPosition="2563">igh IDF range, given that our N is at least a factor of 100 smaller than the number of AP articles they studied: 648 vs. 89,000. Figure 4 also shows the difference between and observed IDFw and Poisson estimate �IDFw and further illustrates the high variance in IDFw for low frequency words. Two questions arise: what is happening with infrequent words, and why does this matter for term detection? To look at the data from a different perspective, we consider the random variable k, which is the number of times a word occurs in a particular document. In Figure 5 we plot the following ratio, which Church and Gale (1995) define as burstiness : Ew[k k &gt; 0] = fw (3) DFw as a function of fw. We denote this as E[k] and can interpret burstiness as the expected word count given we see w at least once. In Figure 5 we see two classes of words emerge. A similar phenomenon is observed concerning adaptive language models (Church, 2000). In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model (Jelinek, 1997). Likewise, Katz attempts to capture these two classes in his G model of word frequencies (1996). For the first class, burstiness in</context>
<context position="18020" citStr="Church and Gale (1995)" startWordPosition="2980" endWordPosition="2983">ocument. We compute the conditional probability for w using frequency information. 1320 Figure 6: Difference between conditional and unconditional unigram probabilities for Tagalog P (w|k &gt; 0) = fw − DFw (4) ED:w∈D |D| Figure 6 shows the difference between conditional and unconditional unigram probabilities. Without any other information, Zipf’s law suggests that most word types do not occur in a particular document. However, conditioning on one occurrence, most word types are more likely to occur again, due to their burstiness. Finally we measure the adaptation of a word, which is defined by Church and Gale (1995) as: Padapt(w) = Pw(k &gt; 1|k &gt; 0) (5) When we plot adaptation versus fw (Figure 7) we see that all high-frequency and a significant number of low-frequency terms have adaptation greater that 50%. To be precise, 26% of all tokens and 25% of low-frequency (fw &lt; 100) have at least 50% adaptation. Given that adaptation values are roughly an order of magnitude higher than the conditional unigram probabilities, in the next two sections we describe how we use adaptation to boost term detection scores. Figure 7: Tagalog word adaptation probability For each term t and document d we propose interpolating</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Kenneth Church and William Gale. 1995. Poisson Mixtures. Natural Language Engineering, 1(2):163–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Inverse Focument Frequency (IDF): A measure of deviations from Poisson.</title>
<date>1999</date>
<booktitle>In Natural Language Processing Using Very Large Corpora,</booktitle>
<pages>283--295</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12412" citStr="Church and Gale (1999)" startWordPosition="2000" endWordPosition="2003">efforts, the topic model information was helpful in boosting retrieval performance above the baseline vector space or N-gram models. Clearly topic or context information is relevant to a retrieval type task, but we need a stable, consistent framework in which to apply it. 3 Term and Document Frequency Statistics To this point we have assumed an implicit property of low-frequency words which Church and Gale state concisely in their 1999 study of inverse document frequency: Low frequency words tend to be rich in content, and vice versa. But not all equally frequent words are equally meaningful. Church and Gale (1999). The typical use of Document Frequency (DF) in information retrieval or text categorization is to emphasize words that occur in only a few documents and are thus more “rich in content”. Close examination of DF statistics by Church and Gale in their work on Poisson Mixtures (1995) resulted in an analysis of the burstiness of content words. In this section we look at DF and burstiness statistics applying some of the analyses of Church and Gale (1999) to the BABEL Tagalog corpus. We observe, in 648 Tagalog conversations, similar phenomena as observed by Church and Gale on 89,000 AP English newsw</context>
<context position="13817" citStr="Church and Gale (1999)" startWordPosition="2246" endWordPosition="2249">e that the document frequency, DFw, of a word w is not a linear function of word frequency fw in the log domain, as would be expected under a naive Poisson generative assumption. The implication of deviations from a Poisson model is that words tend to be concentrated in a small number of documents rather than occurring uniformly across the corpus. This is the burstiness we leverage to improve term detection. The first illustration of word burstiness can be seen by plotting observed inverse document frequency, IDFw, versus fw in the log domain (Figure 3a). We use the same definition of IDFw as Church and Gale (1999): DFw IDFw = −log2 N ,(1) where N is the number of documents (i.e. conversations) in the corpus. There is good linear correlation (p = 0.73) between log fw and IDFw. Yet, visually, the relationship in Figure 3a is clearly not linear. In contrast, the AP English data exhibits a correlation of p = 0.93 (Church and Gale, 1999). Thus the deviation in the Tagalog corpus is more pronounced, i.e. words are less uniformly distributed across documents. A second perspective on word burstiness that follows from Church and Gale (1999) is that a Poisson assumption should lead us to predict: ( � �IDFw = −lo</context>
</contexts>
<marker>Church, Gale, 1999</marker>
<rawString>Kenneth Church and William Gale. 1999. Inverse Focument Frequency (IDF): A measure of deviations from Poisson. In Natural Language Processing Using Very Large Corpora, pages 283–295. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p 2.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>180--186</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="15883" citStr="Church, 2000" startWordPosition="2621" endWordPosition="2622">s happening with infrequent words, and why does this matter for term detection? To look at the data from a different perspective, we consider the random variable k, which is the number of times a word occurs in a particular document. In Figure 5 we plot the following ratio, which Church and Gale (1995) define as burstiness : Ew[k k &gt; 0] = fw (3) DFw as a function of fw. We denote this as E[k] and can interpret burstiness as the expected word count given we see w at least once. In Figure 5 we see two classes of words emerge. A similar phenomenon is observed concerning adaptive language models (Church, 2000). In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model (Jelinek, 1997). Likewise, Katz attempts to capture these two classes in his G model of word frequencies (1996). For the first class, burstiness increases slowly but steadily as w occurs more frequently. Let us label these Class A words. Since our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents Looking close to the y-axis in Figure 5, we observe a second class of exclus</context>
</contexts>
<marker>Church, 2000</marker>
<rawString>Kenneth Church. 2000. Empirical estimates of adaptation: the chance of two Noriegas is closer to p/2 than p 2. In Proceedings of the 18th Conference on Computational Linguistics, volume 1, pages 180– 186. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>David Yarowsky</author>
</authors>
<title>Dynamic nonlocal language modeling via hierarchical topicbased adaptation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>167--174</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10056" citStr="Florian and Yarowsky, 1999" startWordPosition="1617" endWordPosition="1620">n though intuition suggests that such information ought to be helpful. In light of this finding, we will restrict the type of context we use for term detection to the cooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider</context>
</contexts>
<marker>Florian, Yarowsky, 1999</marker>
<rawString>Radu Florian and David Yarowsky. 1999. Dynamic nonlocal language modeling via hierarchical topicbased adaptation. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics, pages 167–174. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Harper</author>
</authors>
<date>2011</date>
<journal>IARPA Solicitation</journal>
<pages>11--02</pages>
<note>http://www.iarpa.gov/ solicitations_babel.html.</note>
<contexts>
<context position="4165" citStr="Harper, 2011" startWordPosition="651" endWordPosition="652">using on contextual information in the form of word repetition within documents, we obtain consistent improvement across five languages in the so called Base Phase of the IARPA BABEL program. 1316 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1316–1325, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 1.1 Task Overview We evaluate term detection and word repetitionbased re-scoring on the IARPA BABEL training and development corpora1 for five languages Cantonese, Pashto, Turkish, Tagalog and Vietnamese (Harper, 2011). The BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation (NIST, 2006) but focuses on limited resource conditions. We focus specifically on the so called no target audio reuse (NTAR) condition to make our method broadly applicable. In order to arrive at our eventual solution, we take the BABEL Tagalog corpus and analyze word co-occurrence and repetition statistics in detail. Our observation of the variability in co-occurrence statistics between Tagalog training and development partitions leads us to narrow the scope of document context to same word co-occurrences, i.e. word</context>
</contexts>
<marker>Harper, 2011</marker>
<rawString>Mary Harper. 2011. IARPA Solicitation IARPABAA-11-02. http://www.iarpa.gov/ solicitations_babel.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="10212" citStr="Hofmann (2001)" startWordPosition="1645" endWordPosition="1646">ooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary. Most recently, Chiu and Rudnicky (2013) looked at word bursts in the IARPA BABEL co</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Paul Hsu</author>
<author>James Glass</author>
</authors>
<title>Style &amp; topic language model adaptation using HMM-LDA.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="10096" citStr="Hsu and Glass, 2006" startWordPosition="1625" endWordPosition="1628">on ought to be helpful. In light of this finding, we will restrict the type of context we use for term detection to the cooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather t</context>
</contexts>
<marker>Hsu, Glass, 2006</marker>
<rawString>Bo-June Paul Hsu and James Glass. 2006. Style &amp; topic language model adaptation using HMM-LDA. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10445" citStr="Jelinek, 1997" startWordPosition="1682" endWordPosition="1684">amework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary. Most recently, Chiu and Rudnicky (2013) looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring a</context>
<context position="16041" citStr="Jelinek, 1997" startWordPosition="2647" endWordPosition="2649">able k, which is the number of times a word occurs in a particular document. In Figure 5 we plot the following ratio, which Church and Gale (1995) define as burstiness : Ew[k k &gt; 0] = fw (3) DFw as a function of fw. We denote this as E[k] and can interpret burstiness as the expected word count given we see w at least once. In Figure 5 we see two classes of words emerge. A similar phenomenon is observed concerning adaptive language models (Church, 2000). In general, we can think of using word repetitions to re-score term detection as applying a limited form of adaptive or cache language model (Jelinek, 1997). Likewise, Katz attempts to capture these two classes in his G model of word frequencies (1996). For the first class, burstiness increases slowly but steadily as w occurs more frequently. Let us label these Class A words. Since our corpus size is fixed, we might expect this to occur, as more word occurrences must be pigeon-holed into the same number of documents Looking close to the y-axis in Figure 5, we observe a second class of exclusively low frequency words whose burstiness ranges from highly concentrated to singletons. We will refer to these as Class B words. If we take the Class A conc</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Fred Jelinek. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Distribution of content words and phrases in text and language modelling.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<volume>2</volume>
<issue>1</issue>
<marker>Katz, 1996</marker>
<rawString>Slava Katz. 1996. Distribution of content words and phrases in text and language modelling. Natural Language Engineering, 2(1):15–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>A maximum entropy language model integrating n-grams and topic dependencies for conversational speech recognition.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>553--556</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6306" citStr="Khudanpur and Wu, 1999" startWordPosition="978" endWordPosition="982">latent topic modeling with Latent Dirchlet Allocation (Blei et al., 2003) and similar latent variable approaches for discovering meaningful word cooccurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams. Indeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving lan1Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPAbabel106-v0.2g and IARPA-babel107b-v0.7 respectively. guage model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012). However, given the preponderance of highly frequent non-content words in the computation of a corpus’ WER, it’s not clear that a 1-2% improvement in WER would translate into an improvement in term detection. Still, intuition suggests that knowing the topic context of a detected word ought to be useful in predicting whether or not a term does belong in that context. For example, if we determine the context of the detection hypothesis is about computers, containing words like ‘monitor,’ ‘internet’ and ‘mouse,’ then we would be more confident of a term such as</context>
<context position="10028" citStr="Khudanpur and Wu, 1999" startWordPosition="1613" endWordPosition="1616">to be too promising, even though intuition suggests that such information ought to be helpful. In light of this finding, we will restrict the type of context we use for term detection to the cooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detectio</context>
</contexts>
<marker>Khudanpur, Wu, 1999</marker>
<rawString>Sanjeev Khudanpur and Jun Wu. 1999. A maximum entropy language model integrating n-grams and topic dependencies for conversational speech recognition. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 1, pages 553–556. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Volker Steinbiss</author>
</authors>
<title>On the dynamic adaptation of stochastic language models.</title>
<date>1993</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<volume>2</volume>
<pages>586--589</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10498" citStr="Kneser and Steinbiss, 1993" startWordPosition="1690" endWordPosition="1693">ument context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vocabulary. Most recently, Chiu and Rudnicky (2013) looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring an additional tuning set to estimate parameters. In th</context>
</contexts>
<marker>Kneser, Steinbiss, 1993</marker>
<rawString>Reinhard Kneser and Volker Steinbiss. 1993. On the dynamic adaptation of stochastic language models. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 2, pages 586–589. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato De Mori</author>
</authors>
<title>A cachebased natural language model for speech recognition.</title>
<date>1990</date>
<journal>Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>Roland Kuhn and Renato De Mori. 1990. A cachebased natural language model for speech recognition. Transactions on Pattern Analysis and Machine Intelligence, 12(6):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Feifan Liu</author>
</authors>
<title>Unsupervised language model adaptation via topic modeling based on named entity hypotheses.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, (ICASSP),</booktitle>
<pages>4921--4924</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="10075" citStr="Liu and Liu, 2008" startWordPosition="1621" endWordPosition="1624">that such information ought to be helpful. In light of this finding, we will restrict the type of context we use for term detection to the cooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term i</context>
</contexts>
<marker>Liu, Liu, 2008</marker>
<rawString>Yang Liu and Feifan Liu. 2008. Unsupervised language model adaptation via topic modeling based on named entity hypotheses. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, (ICASSP), pages 4921–4924. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Welly Naptali</author>
<author>Masatoshi Tsuchiya</author>
<author>Seiichi Nakagawa</author>
</authors>
<title>Topic-dependent-class-based n-gram language model.</title>
<date>2012</date>
<journal>Transactions on Audio, Speech, and Language Processing,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="6341" citStr="Naptali et al., 2012" startWordPosition="985" endWordPosition="988">rchlet Allocation (Blei et al., 2003) and similar latent variable approaches for discovering meaningful word cooccurrence patterns in large text corpora, we ought to be able to leverage these topic contexts instead of merely N-grams. Indeed there is work in the literature that shows that various topic models, latent or otherwise, can be useful for improving lan1Language collection releases IARPA-babel101-v0.4c, IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPAbabel106-v0.2g and IARPA-babel107b-v0.7 respectively. guage model perplexity and word error rate (Khudanpur and Wu, 1999; Chen, 2009; Naptali et al., 2012). However, given the preponderance of highly frequent non-content words in the computation of a corpus’ WER, it’s not clear that a 1-2% improvement in WER would translate into an improvement in term detection. Still, intuition suggests that knowing the topic context of a detected word ought to be useful in predicting whether or not a term does belong in that context. For example, if we determine the context of the detection hypothesis is about computers, containing words like ‘monitor,’ ‘internet’ and ‘mouse,’ then we would be more confident of a term such as ‘keyboard’ and less confident of a</context>
<context position="10119" citStr="Naptali et al., 2012" startWordPosition="1629" endWordPosition="1632">l. In light of this finding, we will restrict the type of context we use for term detection to the cooccurrence of the term itself elsewhere within the document. As it turns out this ‘burstiness’ of words within documents, as the term is defined by Church and Gale in their work on Poisson mixtures (1995), provides a more reliable framework for successfully exploiting document context. 2.1 Related Work A number of efforts have been made to augment traditional N-gram models with latent topic information (Khudanpur and Wu, 1999; Florian and Yarowsky, 1999; Liu and Liu, 2008; Hsu and Glass, 2006; Naptali et al., 2012) including some of the early work on Probabilistic Latent Semantic Analysis by Hofmann (2001). In all of these cases WER gains in the 1-2% range were observed by interpolating latent topic information with N-gram models. The re-scoring approach we present is closely related to adaptive or cache language models (Jelinek, 1997; Kuhn and De Mori, 1990; Kneser and Steinbiss, 1993). The primary difference between this and previous work on similar language models is the narrower focus here on the term detection task, in which we consider each search term in isolation, rather than all words in the vo</context>
</contexts>
<marker>Naptali, Tsuchiya, Nakagawa, 2012</marker>
<rawString>Welly Naptali, Masatoshi Tsuchiya, and Seiichi Nakagawa. 2012. Topic-dependent-class-based n-gram language model. Transactions on Audio, Speech, and Language Processing, 20(5):1513–1525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The Spoken Term Detection (STD)</title>
<date>2006</date>
<note>Evaluation Plan. http://www.itl. nist.gov/iad/mig/tests/std/2006/ docs/std06-evalplan-v10.pdf. [Online; accessed 28-Feb-2013].</note>
<contexts>
<context position="4255" citStr="NIST, 2006" startWordPosition="666" endWordPosition="667">onsistent improvement across five languages in the so called Base Phase of the IARPA BABEL program. 1316 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1316–1325, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 1.1 Task Overview We evaluate term detection and word repetitionbased re-scoring on the IARPA BABEL training and development corpora1 for five languages Cantonese, Pashto, Turkish, Tagalog and Vietnamese (Harper, 2011). The BABEL task is modeled on the 2006 NIST Spoken Term Detection evaluation (NIST, 2006) but focuses on limited resource conditions. We focus specifically on the so called no target audio reuse (NTAR) condition to make our method broadly applicable. In order to arrive at our eventual solution, we take the BABEL Tagalog corpus and analyze word co-occurrence and repetition statistics in detail. Our observation of the variability in co-occurrence statistics between Tagalog training and development partitions leads us to narrow the scope of document context to same word co-occurrences, i.e. word repetitions. We then analyze the tendency towards withindocument repetition. The strength</context>
<context position="20199" citStr="NIST, 2006" startWordPosition="3366" endWordPosition="3367">of repeated words with the observation: given a correct detection, the likelihood of additional terms in the same documents should increase. When we observe a term detection score with high confidence, we boost the other lower-scoring terms in the same document to reflect this increased likelihood of repeated terms. Table 1: Term detection scores for swept α values on Tagalog development data 1321 The primary metric for the BABEL program, Actual Term Weighted Value (ATWV) is defined by NIST using a cost function of the false alarm probability P(FA) and P(Miss), averaged over a set of queries (NIST, 2006). The manner in which the components of ATWV are defined: P(Miss) = 1 − Ntrue(term)/fterm (7) P(FA) = Nfalge/Durationcorpug (8) implies that cost of a miss is inversely proportional to the frequency of the term in the corpus, but the cost of a false alarm is fixed. For this reason, we report both ATWV and the P(Miss) component. A decrease in P(Miss) reflects the fact that we are able to boost correct detections of the repeated terms. 4.1 Interpolation Weights We would prefer to use prior knowledge rather than naive tuning to select an interpolation weight α. Our analysis of word burstiness sug</context>
<context position="24410" citStr="NIST 2006" startWordPosition="4051" endWordPosition="4052">+1.0) 0.646 (+0.4) Tagalog Cantonese Pashto Turkish 0.22 0.26 0.21 0.16 0.34 Table 3: Word-repetition re-scored results for available CTS term detection corpora improvement, suggesting that we find additional gains boosting low-frequency words. 5 Results Now that we have tested word repetition-based re-scoring on a small Tagalog development set we want to know if our approach, and particularly our α� estimate is sufficiently robust to apply broadly. At our disposal, we have the five BABEL languages — Tagalog, Cantonese, Pashto, Turkish and Vietnamese — as well as the development data from the NIST 2006 English evaluation. The BABEL evaluation query sets contain roughly 2000 terms each and the 2006 English query set contains roughly 1000 terms. The procedure we follow for each language condition is as follows. We first estimate adaptation probabilities from the ASR training transcripts. From these we take the weighted average as described previously to obtain a single interpolation weight α� for each training condition. We train ASR acoustic and language models from the training corpus using the Kaldi speech recognition toolkit (Povey et al., 2011) following the default BABEL training and se</context>
</contexts>
<marker>NIST, 2006</marker>
<rawString>NIST. 2006. The Spoken Term Detection (STD) 2006 Evaluation Plan. http://www.itl. nist.gov/iad/mig/tests/std/2006/ docs/std06-evalplan-v10.pdf. [Online; accessed 28-Feb-2013].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
<author>Arnab Ghoshal</author>
<author>Gilles Boulianne</author>
<author>Lukas Burget</author>
<author>Ondrej Glembek</author>
<author>Nagendra Goel</author>
<author>Mirko Hannemann</author>
<author>Petr Motlicek</author>
<author>Yanmin Qian</author>
<author>Petr Schwarz</author>
</authors>
<title>The Kaldi speech recognition toolkit.</title>
<date>2011</date>
<booktitle>In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU).</booktitle>
<contexts>
<context position="24966" citStr="Povey et al., 2011" startWordPosition="4140" endWordPosition="4143">Vietnamese — as well as the development data from the NIST 2006 English evaluation. The BABEL evaluation query sets contain roughly 2000 terms each and the 2006 English query set contains roughly 1000 terms. The procedure we follow for each language condition is as follows. We first estimate adaptation probabilities from the ASR training transcripts. From these we take the weighted average as described previously to obtain a single interpolation weight α� for each training condition. We train ASR acoustic and language models from the training corpus using the Kaldi speech recognition toolkit (Povey et al., 2011) following the default BABEL training and search recipe which is described in detail by Chen et al. (2013). Lastly, we re-score the search output by interpolating the top term detection score for a document with subsequent hits according to Equation 6 using the α� estimated for this training condition. For each of the BABEL languages we consider both the FullLP (80 hours) and LimitedLP (10 hours) training conditions. For the English system, we also train a Kaldi system on the 240 hours of the Switchboard conversational English corpus. Although Kaldi can produce multiple types of acoustic model</context>
</contexts>
<marker>Povey, Ghoshal, Boulianne, Burget, Glembek, Goel, Hannemann, Motlicek, Qian, Schwarz, 2011</marker>
<rawString>Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011. The Kaldi speech recognition toolkit. In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>LDA-based document models for ad-hoc retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>178--185</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11368" citStr="Wei and Croft (2006)" startWordPosition="1834" endWordPosition="1837"> and Rudnicky (2013) looked at word bursts in the IARPA BABEL conversational corpora, and were also able to successfully improve performance by leveraging the burstiness of language. One advantage of the approach proposed here, relative to their approach, is its simplicity and its not requiring an additional tuning set to estimate parameters. In the information retrieval community, clustering and latent topic models have yielded improvements over traditional vector space models. We will discuss in detail in the following section related works by Church and Gale (1995, 1999, and 2000). Work by Wei and Croft (2006) and Chen (2009) take a language model-based approach to 1318 (a) fw versus IDFw ‘ (b) Obsered versus predicted IDFw Figure 3: Tagalog corpus frequency statistics, unigrams information retrieval, and again, interpolate latent topic models with N-grams to improve retrieval performance. However, in many text retrieval tasks, queries are often tens or hundreds of words in length rather than short spoken phrases. In these efforts, the topic model information was helpful in boosting retrieval performance above the baseline vector space or N-gram models. Clearly topic or context information is relev</context>
</contexts>
<marker>Wei, Croft, 2006</marker>
<rawString>Xing Wei and W Bruce Croft. 2006. LDA-based document models for ad-hoc retrieval. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 178–185. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>