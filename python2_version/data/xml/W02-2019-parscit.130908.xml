<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002705">
<title confidence="0.945929">
Markov models for language-independent named entity recognition
</title>
<author confidence="0.980854">
Robert Malouf
</author>
<affiliation confidence="0.771126">
Alfa-Informatica
Rijksuniversiteit Groningen
</affiliation>
<address confidence="0.556477333333333">
Postbus 716
9700AS Groningen
The Netherlands
</address>
<sectionHeader confidence="0.997722" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996727409090909">
This report describes the application of Markov
models to the problem of language-independent
named entity recognition for the CoNLL-2002
shared task (Tjong Kim Sang, 2002).
We approach the problem of identifying named
entities as a kind of probabilistic tagging: given a
sequence of words w1 wn, we want to find the
corresponding sequence of tags t1 tn, drawn from
a vocabulary of possible tags T, which satisfies:
The possible tags are and , which
mark the beginning and continuation of personal
names; and , which mark names of or-
ganizations; and , which mark names
of locations; and , which mark mis-
cellaneous names; and, which marks non-name
tokens.
We will assume that a sequence of tags can be
modeled by Markov process, and that the probabil-
ity of assigning a tag to a word depends only on a
fixed context window (say, the previous word and
tag). Thus, the sequence probability in (1) can be
restated as the product of tag probabilities:
</bodyText>
<sectionHeader confidence="0.995502" genericHeader="keywords">
2 Models
</sectionHeader>
<bodyText confidence="0.9712385">
In the first model ( ), the tag probabilities de-
pend only on the current word:
</bodyText>
<equation confidence="0.9035615">
Pt1 tnw1 wn H
i1n
</equation>
<bodyText confidence="0.999951111111111">
The effect of this is that each word in the test data
will be assigned the tag which occurred most fre-
quently with that word in the training data.
The next model considered ( ) is a simple
Hidden Markov Model (DeRose, 1988; Charniak,
1993), in which the tag probabilities depend on the
current word and the previous tag. Suppose we as-
sume that the word/tag probabilities and the tag se-
quence probabilities are independent, or:
</bodyText>
<equation confidence="0.927567285714286">
Pwititi1 PwitiPtiti1 (2)
Then by Bayes’ Theorem and the Markov property,
we have:
Pw1 wnt1 tnPt1 tn
Pw1 wn
H i1nPwitiPtiti1
Pw1 wn
</equation>
<bodyText confidence="0.674306666666667">
Since the probability of the word sequence
Pw1 wn is the same for all candidate tag se-
quences, the optimal sequence of tags satisfies:
</bodyText>
<equation confidence="0.9993368">
S argmax
t1tn
Pt1 tnw1 wn (1)
Ptiwi
Pt1 tnw1 wn
</equation>
<bodyText confidence="0.974698142857143">
For each of the models described in the next sec-
tion, the model parameters were estimated based
on the provided training data, with no preprocess-
ing or filtering. Then, the most likely tag sequence
(based on the model) is selected for each sentence
in the test data, and the results are evaluated using
the script.
</bodyText>
<equation confidence="0.671732">
H
i1n
</equation>
<bodyText confidence="0.999151571428571">
The probabilities Pwiti and Ptiti1 can easily
be estimated from training data. Using (3) to calcu-
late the probability of a candidate tag sequence, the
optimal sequence of tags can be found efficiently
using dynamic programming (Viterbi, 1967).
While this kind of HMM is simple and easy to
construct and apply, it has its limitations. For one,
</bodyText>
<equation confidence="0.995881666666667">
S argmax
t1tn
PwitiPtiti1 (3)
Pt1 tnw1 wn H
i1n
Ptiwiti1wi1
</equation>
<bodyText confidence="0.99346975">
(3) depends on the independence assumption in (2).
In the next model ( ), we avoid this by using a
conditional maximum entropy model to estimate tag
probabilities. Maximum entropy models (Jaynes,
1957; Berger et al., 1996; Della Pietra et al., 1997)
are a class of exponential models which require no
unwarranted independence assumptions and have
proven to be very successful in general for integrat-
ing information from disparate and possibly over-
lapping sources. In this model, the optimal tag se-
quence satisfies:
where
</bodyText>
<equation confidence="0.9182545">
exp∑j λj fjti1witi (4)
∑τT exp∑j λjfjti1wiτ
</equation>
<bodyText confidence="0.989460141025641">
The indicator functions fj ‘fire’ for particular
combinations of contexts and tags. For instance,
one such function might indicate the occurrence of
the word Javier with the tag :
(5)
and another might indicate the tag sequence
:
(6)
Each indicator fj function also has an associ-
ated weight λj, which is chosen so that the prob-
abilities (4) minimize the relative entropy between
the empirical distribution P˜ (derived from the train-
ing data) and the model probabilities P, or, equiva-
lently, which maximize the likelihood of the train-
ing data. Unlike the parameters of an HMM, there is
no closed form expression for estimating the param-
eters of a maximum entropy model from the train-
ing data. So, we proceed iteratively, gradually refin-
ing the parameter estimates until the desired level
of precision is reached. For these experiments, the
parameters were fit to the training data using a lim-
ited memory variable metric algorithm (Malouf, in
press).
The basic structure of the model is very similar to
that of Borthwick (1999). However, in the models
described here, no feature selection is performed.
Also note that this formulation of maximum entropy
Markov models differs slightly from that of McCal-
lum et al. (2000). Here we use a single maximum
entropy model, while McCallum, et al. use a sep-
arate model for each source state. Using separate
models increases the sparseness of the training data
and, at least for this task, slightly reduces the accu-
racy of the final tagger.
Using indicator functions of the type in (5) and
(6), the model encodes exactly the same informa-
tion as the HMM in (3), but with much weaker in-
dependence assumptions. This means we can add
information to the model from partially redundant
and overlapping sources. The model adds
two additional types of information that were used
by Borthwick (1999). It includes capitalization fea-
tures, which indicate whether the current word is
capitalized, all upper case, all lower case, mixed
case, or non-alphanumeric, and whether or not the
word is the first word in the sentence. And it also
adds additional context sensitivity, so that the tag
probabilities depend on the previous word, as well
as the previous tag and the current word.
The next model, , adds one additional fea-
ture to that takes advantage of the structure of
the training and test data. Often in newspaper ar-
ticles, the first reference to an individual is by full
name and title, while later references use only the
person’s surname. While an unfamiliar full name
can often be identified as a name by the surround-
ing context, the surname appearing alone is more
difficult to catch. For example, one article begins:
El presidente electo de la Repblica Do-
minicana, Hiplito Meja, del Partido Revolu-
cionario Dominicano (PRD) socialdemcrata,
manifest que mantendr su apoyo a los XIV
Juegos Panamericanos del 2003 en Santo
Domingo. Meja, quien gan los comicios pres-
idenciales en las votaciones del pasado 16 de
mayo, asegur que ni l ni su partido cambiarn
la posicin asumida ante el pueblo dominicano
de respaldar la organizacin de los Juegos.
In the first sentence, the phrase Hiplito Meja can
likely be identified as a personal name even if the
surname is an unknown word, since the phrase con-
sists of two capitalized words (the first a common
first name) set off by commas. In the second sen-
tence, however, Meja is much more difficult to iden-
tify as a name: a sentence-initial capitalized un-
known word is most likely to be tagged as. To
allow the use in the first sentence to provide in-
formation about the second, uses a feature
</bodyText>
<figure confidence="0.9771586">
S argmax
t1tn
∏
i1n
Ptiwiti1
1 if wi Javier &amp; ti
fti1witi 0 otherwise
1 if ti1 &amp; ti
fti1witi 0 otherwise
Ptiwiti1
</figure>
<bodyText confidence="0.999845533333333">
which is true just in case the current word occurred
as part of a personal name previously in the text be-
ing tagged. With this feature, the model can take
advantage of easy instances of names to help with
more difficult instances later in the text.
All of the models described to this point are com-
pletely language independent and use no informa-
tion not contained in the training data. The fi-
nal model, ❩, includes one additional feature
which indicates whether or not the current word ap-
pears in a list of 13,821 first names collected from
a number of multi-lingual sources on the Internet.
While the names are drawn from a wide range of
languages and cultures, the emphasis is on Euro-
pean names, and in particular English and Spanish.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="introduction">
3 Results
</sectionHeader>
<bodyText confidence="0.998973777777778">
Each of the models described in the previous sec-
tion were trained using and evaluated
on . The results are summarized in Ta-
ble 1.
As would be expected, performs substan-
tially better than for every category but lo-
cations, though earlier cross-validation experiments
suggest that this exception is an accident of the par-
ticular split between training and test data.
Perhaps more surprisingly, outperforms
by an even wider margin. In these two mod-
els, the tag probabilities are conditioned on exactly
the same properties of the contexts. The only differ-
ence between the models is that the probabilities in
are estimated in a way which avoids the inde-
pendence assumption in (2). The poor performance
of suggests that this assumption is highly
problematic.
</bodyText>
<table confidence="0.989280875">
Adding additional features, in and
Method Type Precision Recall Fβ1
baseline overall 44.59 43.52 44.05
LOC 52.67 72.18 60.90
MISC 22.27 22.52 22.40
ORG 51.59 45.29 48.23
PER 32.81 25.61 28.77
HMM overall 44.03 42.97 43.50
LOC 31.35 69.04 43.12
MISC 44.09 25.23 32.09
ORG 65.30 46.18 54.10
PER 47.49 23.98 31.87
ME overall 71.50 50.95 59.50
LOC 66.36 72.49 69.29
MISC 58.04 33.33 42.35
ORG 73.67 49.26 59.04
PER 81.80 42.31 55.77
ME+ overall 72.07 67.70 69.82
LOC 63.84 77.26 69.91
MISC 49.85 38.51 43.46
ORG 77.45 59.45 67.27
PER 80.48 82.00 81.23
ME+m overall 74.78 71.07 72.88
LOC 68.28 80.00 73.68
MISC 56.51 37.16 44.84
ORG 78.99 61.94 69.44
PER 80.13 88.79 84.24
ME+mf overall 74.55 70.45 72.44
LOC 63.50 80.20 70.88
MISC 54.63 38.51 45.18
ORG 79.71 61.94 69.71
PER 85.30 85.92 85.61
</table>
<tableCaption confidence="0.999883">
Table 1: Summary of preliminary models
</tableCaption>
<bodyText confidence="0.993699818181818">
, to allow a more direct cross-language
comparison of the performance of .
The results of the final evaluation are given in Ta-
ble 2. The performance of the model is roughly
the same for both test samples of each language,
though the performance differs somewhat between
the two languages. In particular, the performance
on entities is quite a bit better for Dutch than
it is for Spanish, and the performance on en-
tities is quite a bit better for Spanish than it is for
Dutch. These differences are somewhat surprising,
as nothing in the model is language specific. Per-
haps the discrepancy (especially for the class)
reflects differences in the way the training data was
annotated; is a highly heterogenous class, and
the criteria for distinguishing between and
entities is sometimes unclear.
,
offer further gains over the base model. How-
ever, the addition of a database of first names, in
, only slightly improves the performance on
personal names and actually reduces the overall per-
formance. This is likely due to the fact that the list
of names contains many words which can also be
used as locations and organizations. Perhaps the
use of additional databases of geographic and non-
personal names would help counteract this effect.
For the final results, the model which preformed
the best on the evaluation data, , was trained
on and evaluated with and
, and trained on and eval-
uated with and . Before
training, the part of speech tags were removed from
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999974071428572">
The models described here are very simple and ef-
ficient, depend on no preprocessing or (with the ex-
ception of ) external databases, and yet pro-
vide a dramatic improvement over a baseline model.
However, the performance is still quite a bit lower
than results for industrial-strength language-specific
named entity recognition systems.
There are a number of small improvements which
could be made to these models, such as feature
selection (to reduce overtraining) and the use of
whole sentence sequence models, as in Lafferty et
al. (2001) (to avoid the ‘label-bias problem’). These
refinements can be expected to offer a modest boost
to the performance of the best model.
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9837435">
The research of Dr. Malouf has been made possible
by a fellowship of the Royal Netherlands Academy
of Arts and Sciences and by the NWO PIONIER
project Algorithms for Linguistic Processing.
</bodyText>
<sectionHeader confidence="0.995583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969292407407407">
Adam Berger, Stephen Della Pietra, and Vincent
Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Compu-
tational Linguistics, 22.
Andrew Borthwick. 1999. A maximum entropy ap-
proach to named entity recognition. Ph.D. thesis,
New York University.
Eugene Charniak. 1993. Statistical Language
Learning. MIT Press, Cambridge, MA.
Stephen Della Pietra, Vincent Della Pietra, and
John Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 19:380–393.
Steven J. DeRose. 1988. Grammatical category
disambiguation by statistical optimization. Com-
putational Linguistics, 14:31–39.
E.T. Jaynes. 1957. Information theory and statis-
tical mechanics. Physical Review, 106,108:620–
630.
John Lafferty, Fernando Pereira, and Andrew Mc-
Callum. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML).
Robert Malouf. in press. A comparison of algo-
rithms for maximum entropy parameter estima-
tion. In Proceedings of the Sixth Conference
</reference>
<table confidence="0.998623875">
Spanish dev. precision recall FR1
LOC 68.28% 80.00% 73.68
MISC 56.51% 37.16% 44.84
ORG 78.99% 61.94% 69.44
PER 80.13% 88.79% 84.24
overall 74.78% 71.07% 72.88
Spanish test precision recall FR1
LOC 74.71% 70.57% 72.58
MISC 60.43% 40.88% 48.77
ORG 76.51% 74.43% 75.45
PER 72.63% 90.61% 80.63
overall 73.93% 73.39% 73.66
Dutch devel. precision recall FR1
LOC 84.50% 58.40% 69.07
MISC 68.29% 60.32% 64.06
ORG 76.52% 42.71% 54.82
PER 54.55% 81.21% 65.27
overall 65.80% 61.06% 63.34
Dutch test precision recall FR1
LOC 85.81% 68.22% 76.01
MISC 72.43% 59.98% 65.62
ORG 78.87% 47.66% 59.42
PER 61.03% 83.70% 70.59
overall 70.88% 65.50% 68.08
</table>
<tableCaption confidence="0.988545">
Table 2: Results obtained for the development and
</tableCaption>
<reference confidence="0.924305882352941">
the test data sets for the two languages used in this
shared task.
on Computational Language Learning (CoNLL-
2002), Taipei.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation.
In Proceedings of the 17th International Confer-
ence on Machine Learning (ICML 2000), pages
591–598.
Erik Tjong Kim Sang. 2002. CoNLL 2002
shared task.
.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymptotically optimal de-
coding algorithm. IEEE Transactions on Infor-
mation Theory, 13:260–269.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.200032">
<title confidence="0.999636">Markov models for language-independent named entity recognition</title>
<author confidence="0.926917">Robert</author>
<affiliation confidence="0.4320395">Rijksuniversiteit Postbus</affiliation>
<address confidence="0.4768105">9700AS The Netherlands</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="2921" citStr="Berger et al., 1996" startWordPosition="498" endWordPosition="501">robabilities Pwiti and Ptiti1 can easily be estimated from training data. Using (3) to calculate the probability of a candidate tag sequence, the optimal sequence of tags can be found efficiently using dynamic programming (Viterbi, 1967). While this kind of HMM is simple and easy to construct and apply, it has its limitations. For one, S argmax t1tn PwitiPtiti1 (3) Pt1 tnw1 wn H i1n Ptiwiti1wi1 (3) depends on the independence assumption in (2). In the next model ( ), we avoid this by using a conditional maximum entropy model to estimate tag probabilities. Maximum entropy models (Jaynes, 1957; Berger et al., 1996; Della Pietra et al., 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources. In this model, the optimal tag sequence satisfies: where exp∑j λj fjti1witi (4) ∑τT exp∑j λjfjti1wiτ The indicator functions fj ‘fire’ for particular combinations of contexts and tags. For instance, one such function might indicate the occurrence of the word Javier with the tag : (5) and another might indicate the tag sequence : (6) Each indicator fj func</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A maximum entropy approach to named entity recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="4290" citStr="Borthwick (1999)" startWordPosition="726" endWordPosition="727">(derived from the training data) and the model probabilities P, or, equivalently, which maximize the likelihood of the training data. Unlike the parameters of an HMM, there is no closed form expression for estimating the parameters of a maximum entropy model from the training data. So, we proceed iteratively, gradually refining the parameter estimates until the desired level of precision is reached. For these experiments, the parameters were fit to the training data using a limited memory variable metric algorithm (Malouf, in press). The basic structure of the model is very similar to that of Borthwick (1999). However, in the models described here, no feature selection is performed. Also note that this formulation of maximum entropy Markov models differs slightly from that of McCallum et al. (2000). Here we use a single maximum entropy model, while McCallum, et al. use a separate model for each source state. Using separate models increases the sparseness of the training data and, at least for this task, slightly reduces the accuracy of the final tagger. Using indicator functions of the type in (5) and (6), the model encodes exactly the same information as the HMM in (3), but with much weaker indep</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A maximum entropy approach to named entity recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1474" citStr="Charniak, 1993" startWordPosition="249" endWordPosition="250"> sequence of tags can be modeled by Markov process, and that the probability of assigning a tag to a word depends only on a fixed context window (say, the previous word and tag). Thus, the sequence probability in (1) can be restated as the product of tag probabilities: 2 Models In the first model ( ), the tag probabilities depend only on the current word: Pt1 tnw1 wn H i1n The effect of this is that each word in the test data will be assigned the tag which occurred most frequently with that word in the training data. The next model considered ( ) is a simple Hidden Markov Model (DeRose, 1988; Charniak, 1993), in which the tag probabilities depend on the current word and the previous tag. Suppose we assume that the word/tag probabilities and the tag sequence probabilities are independent, or: Pwititi1 PwitiPtiti1 (2) Then by Bayes’ Theorem and the Markov property, we have: Pw1 wnt1 tnPt1 tn Pw1 wn H i1nPwitiPtiti1 Pw1 wn Since the probability of the word sequence Pw1 wn is the same for all candidate tag sequences, the optimal sequence of tags satisfies: S argmax t1tn Pt1 tnw1 wn (1) Ptiwi Pt1 tnw1 wn For each of the models described in the next section, the model parameters were estimated based on</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380</pages>
<contexts>
<context position="2949" citStr="Pietra et al., 1997" startWordPosition="503" endWordPosition="506">i1 can easily be estimated from training data. Using (3) to calculate the probability of a candidate tag sequence, the optimal sequence of tags can be found efficiently using dynamic programming (Viterbi, 1967). While this kind of HMM is simple and easy to construct and apply, it has its limitations. For one, S argmax t1tn PwitiPtiti1 (3) Pt1 tnw1 wn H i1n Ptiwiti1wi1 (3) depends on the independence assumption in (2). In the next model ( ), we avoid this by using a conditional maximum entropy model to estimate tag probabilities. Maximum entropy models (Jaynes, 1957; Berger et al., 1996; Della Pietra et al., 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources. In this model, the optimal tag sequence satisfies: where exp∑j λj fjti1witi (4) ∑τT exp∑j λjfjti1wiτ The indicator functions fj ‘fire’ for particular combinations of contexts and tags. For instance, one such function might indicate the occurrence of the word Javier with the tag : (5) and another might indicate the tag sequence : (6) Each indicator fj function also has an associated </context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J DeRose</author>
</authors>
<title>Grammatical category disambiguation by statistical optimization.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<pages>14--31</pages>
<contexts>
<context position="1457" citStr="DeRose, 1988" startWordPosition="247" endWordPosition="248"> assume that a sequence of tags can be modeled by Markov process, and that the probability of assigning a tag to a word depends only on a fixed context window (say, the previous word and tag). Thus, the sequence probability in (1) can be restated as the product of tag probabilities: 2 Models In the first model ( ), the tag probabilities depend only on the current word: Pt1 tnw1 wn H i1n The effect of this is that each word in the test data will be assigned the tag which occurred most frequently with that word in the training data. The next model considered ( ) is a simple Hidden Markov Model (DeRose, 1988; Charniak, 1993), in which the tag probabilities depend on the current word and the previous tag. Suppose we assume that the word/tag probabilities and the tag sequence probabilities are independent, or: Pwititi1 PwitiPtiti1 (2) Then by Bayes’ Theorem and the Markov property, we have: Pw1 wnt1 tnPt1 tn Pw1 wn H i1nPwitiPtiti1 Pw1 wn Since the probability of the word sequence Pw1 wn is the same for all candidate tag sequences, the optimal sequence of tags satisfies: S argmax t1tn Pt1 tnw1 wn (1) Ptiwi Pt1 tnw1 wn For each of the models described in the next section, the model parameters were e</context>
</contexts>
<marker>DeRose, 1988</marker>
<rawString>Steven J. DeRose. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14:31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Information theory and statistical mechanics.</title>
<date>1957</date>
<journal>Physical Review,</journal>
<volume>106</volume>
<pages>630</pages>
<contexts>
<context position="2900" citStr="Jaynes, 1957" startWordPosition="496" endWordPosition="497">t. H i1n The probabilities Pwiti and Ptiti1 can easily be estimated from training data. Using (3) to calculate the probability of a candidate tag sequence, the optimal sequence of tags can be found efficiently using dynamic programming (Viterbi, 1967). While this kind of HMM is simple and easy to construct and apply, it has its limitations. For one, S argmax t1tn PwitiPtiti1 (3) Pt1 tnw1 wn H i1n Ptiwiti1wi1 (3) depends on the independence assumption in (2). In the next model ( ), we avoid this by using a conditional maximum entropy model to estimate tag probabilities. Maximum entropy models (Jaynes, 1957; Berger et al., 1996; Della Pietra et al., 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources. In this model, the optimal tag sequence satisfies: where exp∑j λj fjti1witi (4) ∑τT exp∑j λjfjti1wiτ The indicator functions fj ‘fire’ for particular combinations of contexts and tags. For instance, one such function might indicate the occurrence of the word Javier with the tag : (5) and another might indicate the tag sequence : (6) E</context>
</contexts>
<marker>Jaynes, 1957</marker>
<rawString>E.T. Jaynes. 1957. Information theory and statistical mechanics. Physical Review, 106,108:620– 630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<marker>Lafferty, Pereira, McCallum, 2001</marker>
<rawString>John Lafferty, Fernando Pereira, and Andrew McCallum. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Malouf</author>
</authors>
<title>in press. A comparison of algorithms for maximum entropy parameter estimation.</title>
<booktitle>In Proceedings of the Sixth Conference the</booktitle>
<marker>Malouf, </marker>
<rawString>Robert Malouf. in press. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference the test data sets for the two languages used in this shared task.</rawString>
</citation>
<citation valid="false">
<booktitle>on Computational Language Learning (CoNLL2002),</booktitle>
<location>Taipei.</location>
<marker></marker>
<rawString>on Computational Language Learning (CoNLL2002), Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning (ICML</booktitle>
<pages>591--598</pages>
<contexts>
<context position="4483" citStr="McCallum et al. (2000)" startWordPosition="754" endWordPosition="758">orm expression for estimating the parameters of a maximum entropy model from the training data. So, we proceed iteratively, gradually refining the parameter estimates until the desired level of precision is reached. For these experiments, the parameters were fit to the training data using a limited memory variable metric algorithm (Malouf, in press). The basic structure of the model is very similar to that of Borthwick (1999). However, in the models described here, no feature selection is performed. Also note that this formulation of maximum entropy Markov models differs slightly from that of McCallum et al. (2000). Here we use a single maximum entropy model, while McCallum, et al. use a separate model for each source state. Using separate models increases the sparseness of the training data and, at least for this task, slightly reduces the accuracy of the final tagger. Using indicator functions of the type in (5) and (6), the model encodes exactly the same information as the HMM in (3), but with much weaker independence assumptions. This means we can add information to the model from partially redundant and overlapping sources. The model adds two additional types of information that were used by Borthw</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
</authors>
<date>2002</date>
<journal>CoNLL</journal>
<note>shared task. .</note>
<marker>Sang, 2002</marker>
<rawString>Erik Tjong Kim Sang. 2002. CoNLL 2002 shared task. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="2539" citStr="Viterbi, 1967" startWordPosition="433" endWordPosition="434">s: S argmax t1tn Pt1 tnw1 wn (1) Ptiwi Pt1 tnw1 wn For each of the models described in the next section, the model parameters were estimated based on the provided training data, with no preprocessing or filtering. Then, the most likely tag sequence (based on the model) is selected for each sentence in the test data, and the results are evaluated using the script. H i1n The probabilities Pwiti and Ptiti1 can easily be estimated from training data. Using (3) to calculate the probability of a candidate tag sequence, the optimal sequence of tags can be found efficiently using dynamic programming (Viterbi, 1967). While this kind of HMM is simple and easy to construct and apply, it has its limitations. For one, S argmax t1tn PwitiPtiti1 (3) Pt1 tnw1 wn H i1n Ptiwiti1wi1 (3) depends on the independence assumption in (2). In the next model ( ), we avoid this by using a conditional maximum entropy model to estimate tag probabilities. Maximum entropy models (Jaynes, 1957; Berger et al., 1996; Della Pietra et al., 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possi</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. IEEE Transactions on Information Theory, 13:260–269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>