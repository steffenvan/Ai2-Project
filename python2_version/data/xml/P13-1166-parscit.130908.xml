<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007611">
<title confidence="0.9967765">
Offspring from Reproduction Problems:
What Replication Failure Teaches Us
</title>
<author confidence="0.986617">
Marten Postma
</author>
<affiliation confidence="0.9637195">
Utrecht University
Utrecht, The Netherlands
</affiliation>
<email confidence="0.920629">
martenp@gmail.com
</email>
<note confidence="0.44660075">
Antske Fokkens and Marieke van Erp
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
</note>
<email confidence="0.965619">
{a.s.fokkens,m.g.j.van.erp}@vu.nl
</email>
<author confidence="0.998234">
Ted Pedersen
</author>
<affiliation confidence="0.999013">
Dept. of Computer Science
University of Minnesota
</affiliation>
<address confidence="0.67205">
Duluth, MN 55812 USA
</address>
<email confidence="0.996212">
tpederse@d.umn.edu
</email>
<author confidence="0.994302">
Piek Vossen
</author>
<affiliation confidence="0.917281333333333">
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
</affiliation>
<email confidence="0.968183">
piek.vossen@vu.nl
</email>
<author confidence="0.88035">
Nuno Freire
</author>
<affiliation confidence="0.7363305">
The European Library
The Hague, The Netherlands
</affiliation>
<email confidence="0.991762">
nfreire@gmail.com
</email>
<sectionHeader confidence="0.998563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925181818182">
Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962375">
Research is a collaborative effort to increase
knowledge. While it includes validating previous
approaches, our experience is that most research
output in our field focuses on presenting new ap-
proaches, and to a somewhat lesser extent building
upon existing work.
In this paper, we argue that the value of research
that attempts to replicate previous approaches goes
beyond simply validating what is already known.
It is also an essential aspect for building upon
existing approaches. Especially when validation
fails or variations in results are found, systematic
testing helps to obtain a clearer picture of both the
approach itself and of the meaning of state-of-the-
art results leading to a better insight into the qual-
ity of new approaches in relation to previous work.
We support our claims by presenting two use
cases that aim to reproduce results of previous
work in two key NLP technologies: measuring
WordNet similarity and Named Entity Recogni-
tion (NER). Besides highlighting the difficulty of
repeating other researchers’ work, new insights
about the approaches emerged that were not pre-
sented in the original papers. This last point shows
that reproducing results is not merely part of good
practice in science, but also an essential part in
gaining a better understanding of the methods we
use. Likewise, the problems we face in reproduc-
ing previous results are not merely frustrating in-
conveniences, but also pointers to research ques-
tions that deserve deeper investigation.
We investigated five aspects that cause exper-
imental variation that are not typically described
in publications: preprocessing (e.g. tokenisa-
tion), experimental setup (e.g. splitting data for
cross-validation), versioning (e.g. which version
of WordNet), system output (e.g. the exact fea-
tures used for individual tokens in NER), and sys-
tem variation (e.g. treatment of ties).
As such, reproduction provides a platform for
systematically testing individual aspects of an ap-
proach that contribute to a given result. What is
the influence of the size of the dataset, for exam-
ple? How does using a different dataset affect the
results? What is a reasonable divergence between
different runs of the same experiment? Finding
answers to these questions enables us to better in-
terpret our state-of-the-art results.
</bodyText>
<page confidence="0.930315">
1691
</page>
<note confidence="0.9130045">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691–1701,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999738214285714">
Moreover, the experiments in this paper show
that even while strictly trying to replicate a pre-
vious experiment, results may vary up to a point
where they lead to different answers to the main
question addressed by the experiment. The Word-
Net similarity experiment use case compares the
performance of different similarity measures. We
will show that the answer as to which measure
works best changes depending on factors such as
the gold standard used, the strategy towards part-
of-speech or the ranking coefficient, all aspects
that are typically not addressed in the literature.
The main contributions of this paper are the
following:
</bodyText>
<listItem confidence="0.9946415">
1) An in-depth analysis of two reproduction use
cases in NLP
2) New insights into the state-of-the-art results
for WordNet similarities and NER, found because
of problems in reproducing prior research
3) A categorisation of aspects influencing
reproduction of experiments and suggestions on
testing their influence systematically
</listItem>
<bodyText confidence="0.999938933333333">
The code, data and experimental setup
for the WordNet experiments are avail-
able at http://github.com/antske/
WordNetSimilarity, and for the NER exper-
iments at http://github.com/Mvanerp/
NER. The experiments presented in this paper
have been repeated by colleagues not involved in
the development of the software using the code
included in these repositories. The remainder of
this paper is structured as follows. In Section 2,
previous work is discussed. Sections 3 and 4
describe our real-world use cases. In Section 5,
we present our observations, followed by a more
general discussion in Section 6. In Section 7, we
present our conclusions.
</bodyText>
<sectionHeader confidence="0.98727" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999959616666667">
This section provides a brief overview of recent
work addressing reproduction and benchmark re-
sults in computer science related studies and dis-
cusses how our research fits in the overall picture.
Most researchers agree that validating results
entails that a method should lead to the same over-
all conclusions rather than producing the exact
same numbers (Drummond, 2009; Dalle, 2012;
Buchert and Nussbaum, 2012, etc.). In other
words, we should strive to reproduce the same an-
swer to a research question by different means,
perhaps by re-implementing an algorithm or eval-
uating it on a new (in domain) data set. Replica-
tion has a somewhat more limited aim, and simply
involves running the exact same system under the
same conditions in order to get the exact same re-
sults as output.
According to Drummond (2009) replication is
not interesting, since it does not lead to new in-
sights. On this point we disagree with Drum-
mond (2009) as replication allows us to: 1) vali-
date prior research, 2) improve on prior research
without having to rebuild software from scratch,
and 3) compare results of reimplementations and
obtain the necessary insights to perform reproduc-
tion experiments. The outcome of our use cases
confirms the statement that deeper insights into an
approach can be obtained when all resources are
available, an observation also made by Ince et al.
(2012).
Even if exact replication is not a goal many
strive for, Ince et al. (2012) argue that insightful
reproduction can be an (almost) impossible un-
dertaking without the source code being available.
Moreover, it is not always clear where replication
stops and reproduction begins. Dalle (2012) dis-
tinguishes levels of reproducing results related to
how close they are to the original work and how
each contributes to research. In general, an in-
creasing awareness of the importance of reproduc-
tion research and open code and data can be ob-
served based on publications in high-profile jour-
nals (e.g. Nature (Ince et al., 2012)) and initiatives
such as myExperiment.1
Howison and Herbsleb (2013) point out that,
even though this is important, often not enough
(academic) credit is gained from making resources
available. What is worse, the same holds for re-
search that investigates existing methods rather
than introducing new ones, as illustrated by the
question that is found on many review forms ‘how
novel is the presented approach?’. On the other
hand, initiatives for journals addressing exactly
this issue (Neylon et al., 2012) and tracks focus-
ing on results verification at conferences such as
VLDB2 show that this opinion is not universal.
A handful of use cases on reproducing or repli-
cating results have been published. Louridas and
Gousios (2012) present a use case revealing that
source code alone is not enough for reproducing
</bodyText>
<footnote confidence="0.999985">
1http://www.myexperiment.org
2http://www.vldb.org/2013/
</footnote>
<page confidence="0.995797">
1692
</page>
<bodyText confidence="0.999984025">
results, a point that is also made by Mende (2010)
who provides an overview of all information re-
quired to replicate results.
The experiments in this paper provide use cases
that confirm the points brought out in the litera-
ture mentioned above. This includes both obser-
vations that a detailed level of information is re-
quired for truly insightful reproduction research as
well as the claim that such research leads to better
understanding of our techniques. Furthermore, the
work in this paper relates to Bikel (2004)’s work.
He provides all information needed in addition to
Collins (1999) to replicate Collins’ benchmark re-
sults. Our work is similar in that we also aim to fill
in the blanks needed to replicate results. It must
be noted, however, that the use cases in this paper
have a significantly smaller scale than Bikel’s.
Our research distinguishes itself from previous
work, because it links the challenges of reproduc-
tion to what they mean for reported results be-
yond validation. Ruml (2010) mentions variations
in outcome as a reason not to emphasise compar-
isons to benchmarks. Vanschoren et al. (2012)
propose to use experimental databases to system-
atically test variations for machine learning, but
neither links the two issues together. Raeder et al.
(2010) come closest to our work in a critical study
on the evaluation of machine learning. They show
that choices in the methodology, such as data sets,
evaluation metrics and type of cross-validation can
influence the conclusions of an experiment, as we
also find in our second use case. However, they
focus on the problem of evaluation and recom-
mendations on how to achieve consistent repro-
ducible results. Our contribution is to investigate
how much results vary. We cannot control how
fellow researchers carry out their evaluation, but
if we have an idea of the variations that typically
occur within a system, we can better compare ap-
proaches for which not all details are known.
</bodyText>
<sectionHeader confidence="0.956755" genericHeader="method">
3 WordNet Similarity Measures
</sectionHeader>
<bodyText confidence="0.9983310625">
Patwardhan and Pedersen (2006) and Pedersen
(2010) present studies where the output of a va-
riety of WordNet similarity and relatedness mea-
sures are compared. They rank Miller and Charles
(1991)’s set (henceforth “mc-set”) of 30 word
pairs according to their semantic relatedness with
several WordNet similarity measures.
Each measure ranks the mc-set of word pairs
and these outputs are compared to Miller and
Charles (1991)’s gold standard based on human
rankings using the Spearman’s Correlation Coeffi-
cient (Spearman, 1904, ρ). Pedersen (2010) also
ranks the original set of 65 word pairs ranked
by humans in an experiment by Rubenstein and
Goodenough (1965) (rg-set) which is a superset of
Miller and Charles’s set.
</bodyText>
<subsectionHeader confidence="0.999172">
3.1 Replication Attempts
</subsectionHeader>
<bodyText confidence="0.999874409090909">
This research emerged from a project run-
ning a similar experiment for Dutch on Cor-
netto (Vossen et al., 2013). First, an attempt
was made to reproduce the results reported in
Patwardhan and Pedersen (2006) and Peder-
sen (2010) on the English WordNet using their
WordNet::Similarity web-interface.3 Results dif-
fered from those reported in the aforementioned
works, even when using the same versions as
the original, WordNet::Similarity-1.02 and Word-
Net 2.1 (Patwardhan and Pedersen, 2006) and
WordNet::Similarity-2.05 and WordNet 3.0 (Ped-
ersen, 2010), respectively.4
The fact that results of similarity measures on
WordNet can differ even while the same software
and same versions are used indicates that proper-
ties which are not addressed in the literature may
influence the output of similarity measures. We
therefore conducted a range of experiments that,
in addition to searching for the right settings to
replicate results of previous research, address the
following questions:
</bodyText>
<listItem confidence="0.990244">
1) Which properties have an impact on the per-
formance of WordNet similarity measures?
2) How much does the performance of individ-
ual measures vary?
3) How do commonly used measures compare
when the variation of their performance are taken
into account?
</listItem>
<subsectionHeader confidence="0.998989">
3.2 Methodology and first observations
</subsectionHeader>
<bodyText confidence="0.999974166666667">
The questions above were addressed in two stages.
In the first stage, Fokkens, who was not involved
in the first replication attempt implemented a
script to calculate similarity measures using Word-
Net::Similarity. This included similarity mea-
sures introduced by Wu and Palmer (1994) (wup),
</bodyText>
<footnote confidence="0.991461">
3Obtained from http://talisker.d.umn.edu/
cgi-bin/similarity/similarity.cgi, Word-
Net::Similarity version 2.05. This web interface has now
moved to http://maraca.d.umn.edu
4WordNet::Similarity were obtained http://
search.cpan.org/dist/WordNet-Similarity/.
</footnote>
<page confidence="0.978927">
1693
</page>
<bodyText confidence="0.992429223880597">
Leacock and Chodorow (1998) (lch), Resnik
(1995) (res), Jiang and Conrath (1997) (jcn),
Lin (1998) (lin), Banerjee and Pedersen (2003)
(lesk), Hirst and St-Onge (1998) (hso) and
Patwardhan and Pedersen (2006) (vector and
vpairs) respectively.
Consequently, settings and properties were
changed systematically and shared with Pedersen
who attempted to produce the new results with his
own implementations. First, we made sure that
the script implemented by Fokkens could produce
the same WordNet similarity scores for each in-
dividual word pair as those used to calculate the
ranking on the mc-set by Pedersen (2010). Finally,
the gold standard and exact implementation of the
Spearman ranking coefficient were compared.
Differences in results turned out to be related
to variations in the experimental setup. First,
we made different assumptions on the restriction
of part-of-speech tags (henceforth “PoS-tag”) con-
sidered in the comparison. Miller and Charles
(1991) do not discuss how they deal with words
with more than one PoS-tag in their study. Ped-
ersen therefore included all senses with any PoS-
tag in his study. The first replication attempt had
restricted PoS-tags to nouns based on the idea
that most items are nouns and subjects would be
primed to primarily think of the noun senses. Both
assumptions are reasonable. Pos-tags were not re-
stricted in the second replication attempt, but be-
cause of a bug in the code only the first identified
PoS-tag (“noun” in all cases) was considered. We
therefore mistakenly assumed that PoS-tag restric-
tions did not matter until we compared individual
scores between Pedersen and the replication at-
tempts.
Second, there are two gold standards for the
Miller and Charles (1991) set: one has the scores
assigned during the original experiment run by
Rubenstein and Goodenough (1965), the other
has the scores assigned during Miller and Charles
(1991)’s own experiment. The ranking correlation
between the two sets is high, but they are not iden-
tical. Again, there is no reason why one gold stan-
dard would be a better choice than the other, but in
order to replicate results, it must be known which
of the two was used. Third, results changed be-
cause of differences in the treatment of ties while
calculating Spearman p. The influence of the ex-
act gold standard and calculation of Spearman p
could only be found because Pedersen could pro-
measure Spearman ρ Kendall τ ranking
min max min max variation
path based similarity
path 0.70 0.78 0.55 0.62 1-8
wup 0.70 0.79 0.53 0.61 1-6
lch 0.70 0.78 0.55 0.62 1-7
path based information content
res 0.65 0.75 0.26 0.57 4-11
lin 0.49 0.73 0.36 0.53 6-10
jcn 0.46 0.73 0.32 0.55 5, 7-11
path based relatedness
hso 0.73 0.80 0.36 0.41 1-3,5-10
dictionary and corpus based relatedness
vpairs 0.40 0.70 0.26 0.50 7-11
vector 0.48 0.92 0.33 0.76 1,2,4,6-11
lesk 0.66 0.83 -0.02 0.61 1-8,11,12
</bodyText>
<tableCaption confidence="0.997271">
Table 1: Variation WordNet measures’ results
</tableCaption>
<bodyText confidence="0.9998194">
vide the output of the similarity measures he used
to calculate the coefficient. It is unlikely we would
have been able to replicate his results at all with-
out the output of this intermediate step. Finally,
results for lch, lesk and wup changed accord-
ing to measure specific configuration settings such
as including a PoS-tag specific root node or turn-
ing on normalisation.
In the second stage of this research, we ran ex-
periments that systematically manipulate the influ-
ential factors described above. In this experiment,
we included both the mc-set and the complete rg-
set. The implementation of Spearman p used in
Pedersen (2010) assigned the lowest number in
ranking to ties rather than the mean, resulting in
an unjustified drop in results for scores that lead
to many ties. We therefore experimented with a
different correlation measure, Kendall tau coeffi-
cient (Kendall, 1938, T) rather than two versions
of Spearman p.
</bodyText>
<subsectionHeader confidence="0.998848">
3.3 Variation per measure
</subsectionHeader>
<bodyText confidence="0.999792214285714">
All measures varied in their performance.
The complete outcome of our experiments
(both the similarity measures assigned to
each pair as well as the output of the ranking
coefficients) are included in the data set pro-
vided at http://github.com/antske/
WordNetSimilarity. Table 1 presents an
overview of the main point we wish to make
through this experiment: the minimal and maxi-
mal results according to both ranking coefficients.
Results for similarity measures varied from 0.06-
0.42 points for Spearman p and from 0.05-0.60
points for Kendall T. The last column indi-
cates the variation of performance of a measure
</bodyText>
<page confidence="0.975453">
1694
</page>
<bodyText confidence="0.99991416">
compared to the other measures, where 1 is the
best performing measure and 12 is the worst.5
For instance, path has been best performing
measure, second best, eighth best and all positions
in between, vector has ranked first, second and
fourth, but also occupied all positions from six to
eleven.
In principle, it is to be expected that num-
bers are not exactly the same while evaluating
against a different data set (the mc-set versus the
rg-set), taking a different set of synsets to evalu-
ate on (changing PoS-tag restrictions) or changing
configuration settings that influence the similarity
score. However, a variation of up to 0.44 points
in Spearman ρ and 0.60 in Kendall τ6 leads to
the question of how indicative these results really
are. A more serious problem is the fact that the
comparative performance of individual measure
changes. Which measure performs best depends
on the evaluation set, ranking coefficient, PoS-tag
restrictions and configuration settings. This means
that the answer to the question of which similarity
measure is best to mimic human similarity scores
depends on aspects that are often not even men-
tioned, let alone systematically compared.
</bodyText>
<subsectionHeader confidence="0.992089">
3.4 Variation per category
</subsectionHeader>
<bodyText confidence="0.999989578947369">
For each influential category of experimental vari-
ation, we compared the variation in Spearman ρ
and Kendall τ, while similarity measure and other
influential categories were kept stable. The cat-
egories we varied include WordNet and Word-
Net::Similarity version, the gold standard used to
evaluate, restrictions on PoS-tags, and measure
specific configurations. Table 2 presents the maxi-
mum variation found across measures for each cat-
egory. The last column indicates how often the
ranking of a specific measure changed as the cat-
egory changed, e.g. did the measure ranking third
using specific configurations, PoS-tag restrictions
and a specific gold standard using WordNet 2.1
still rank third when WordNet 3.0 was used in-
stead? The number in parentheses next to the ‘dif-
ferent ranks’ in the table presents the total num-
ber of scores investigated. Note that this num-
ber changes for each category, because we com-
</bodyText>
<footnote confidence="0.984979333333333">
5Some measures ranked differently as their individual
configuration settings changed. In these cases, the measure
was included in the overall ranking multiple times, which is
why there are more ranking positions than measures.
6Section 3.4 explains why the variation in Kendall is this
extreme and ρ is more appropriate for this task.
</footnote>
<table confidence="0.999985166666667">
Variation Maximum difference Different
Spearman ρ Kendall τ rank (tot)
WN version 0.44 0.42 223 (252)
gold standard 0.24 0.21 359 (504)
PoS-tag 0.09 0.08 208 (504)
configuration 0.08 0.60 37 (90)
</table>
<tableCaption confidence="0.999034">
Table 2: Variations per category
</tableCaption>
<bodyText confidence="0.999950139534884">
pared two WordNet versions (WN version), three
gold standard and PoS-tag restriction variations
and configuration only for the subset of scores
where configuration matters.
There are no definite statements to make as to
which version (Patwardhan and Pedersen (2006)
vs Pedersen (2010)), PoS-tag restriction or con-
figuration gives the best results. Likewise, while
most measures do better on the smaller data set,
some achieve their highest results on the full set.
This is partially due to the fact that ranking coef-
ficients are sensitive to outliers. In several cases
where PoS-tag restrictions led to different results,
only one pair received a different score. For in-
stance, path assigns a relatively high score to
the pair chord-smile when verbs are included, be-
cause the hierarchy of verbs in WordNet is rela-
tively flat. This effect is not observed in wup and
lch which correct for the depth of the hierarchy.
On the other hand, res, lin and jcn score bet-
ter on the same set when verbs are considered, be-
cause they cannot detect any relatedness for the
pair crane-implement when restricted to nouns.
On top of the variations presented above, we no-
tice a discrepancy between the two coefficients.
Kendall τ generally leads to lower coefficiency
scores than Spearman ρ. Moreover, they each
give different relative indications: where lesk
achieves its highest Spearman ρ, it has an ex-
tremely low Kendall τ of 0.01. Spearman ρ uses
the difference in rank as its basis to calculate a cor-
relation, where Kendall τ uses the number of items
with the correct rank. The low Kendall τ for lesk
is the result of three pairs receiving a score that is
too high. Other pairs that get a relatively accurate
score are pushed one place down in rank. Because
only items that receive the exact same rank help to
increase τ, such a shift can result in a drastic drop
in the coefficient. In our opinion, Spearman ρ is
therefore preferable over Kendall τ. We included
τ, because many authors do not mention the rank-
ing coefficient they use (cf. Budanitsky and Hirst
(2006), Resnik (1995)) and both ρ and τ are com-
</bodyText>
<page confidence="0.949915">
1695
</page>
<bodyText confidence="0.99642675">
monly used coefficients.
Except for WordNet, which Budanitsky and
Hirst (2006) hold accountable for minor variations
in a footnote, the influential categories we investi-
gated in this paper, to our knowledge, have not yet
been addressed in the literature. Cramer (2008)
points out that results from WordNet-Human sim-
ilarity correlations lead to scattered results report-
ing variations similar to ours, but she compares
studies using different measures, data and exper-
imental setup. This study shows that even if
the main properties are kept stable, results vary
enough to change the identity of the measure that
yields the best performance. Table 1 reveals a
wide variation in ranking relative to alternative ap-
proaches. Results in Table 2 show that it is com-
mon for the ranking of a score to change due to
variations that are not at the core of the method.
This study shows that it is far from clear how
different WordNet similarity measures relate to
each other. In fact, we do not know how we can
obtain the best results. This is particularly chal-
lenging, because the ‘best results’ may depend on
the intended use of the similarity scores (Meng
et al., 2013). This is also the reason why we
presented the maximum variation observed, rather
than the average or typical variation (mostly be-
low 0.10 points). The experiments presented in
this paper resulted in a vast amount of data. An
elaborate analysis of this data is needed to get a
better understanding of how measures work and
why results vary to such an extent. We leave this
investigation to future work. If there is one take-
home message from this experiment, it is that one
should experiment with parameters such as restric-
tions on PoS-tags or configurations and determine
which score to use depending on what it is used
for, rather than picking something that did best in
a study using different data for a different task and
may have used a different version of WordNet.
</bodyText>
<sectionHeader confidence="0.938883" genericHeader="method">
4 Reproducing a NER method
</sectionHeader>
<bodyText confidence="0.999926714285714">
Freire et al. (2012) describe an approach to clas-
sifying named entities in the cultural heritage do-
main. The approach is based on the assumption
that domain knowledge, encoded in complex fea-
tures, can aid a machine learning algorithm in
NER tasks when only little training data is avail-
able. These features include information about
person and organisation names, locations, as well
as PoS-tags. Additionally, some general features
are used such as a window of three preceding and
two following tokens, token length and capitalisa-
tion information. Experiments are run in a 10-fold
cross-validation setup using an open source ma-
chine learning toolkit (McCallum, 2002).
</bodyText>
<subsectionHeader confidence="0.99179">
4.1 Reproducing NER Experiments
</subsectionHeader>
<bodyText confidence="0.999969714285714">
This experiment can be seen as a real-world case
of the sad tale of the Zigglebottom tagger (Peder-
sen, 2008). The (fictional) Zigglebottom tagger is
a tagger with spectacular results that looks like it
will solve some major problems in your system.
However, the code is not available and a new im-
plementation does not yield the same results. The
original authors cannot provide the necessary de-
tails to reproduce their results, because most of the
work has been done by a PhD student who has fin-
ished and moved on to something else. In the end,
the newly implemented Zigglebottom tagger is not
used, because it does not lead to the promised bet-
ter results and all effort went to waste.
Van Erp was interested in the NER approach
presented in Freire et al. (2012). Unfortunately,
the code could not be made available, so she de-
cided to reimplement the approach. Despite feed-
back from Freire about particular details of the
system, results remained 20 points below those
reported in Freire et al. (2012) in overall F-score
(Van Erp and Van der Meij, 2013).
The reimplementation process involved choices
about seemingly small details such as rounding
to how many decimals, how to tokenise or how
much data cleanup to perform (normalisation of
non-alphanumeric characters for example). Try-
ing different parameter combinations for feature
generation and the algorithm never yielded the ex-
act same results as Freire et al. (2012). The results
of the best run in our first reproduction attempt,
together with the original results from Freire et al.
(2012) are presented in Table 3. Van Erp and Van
der Meij (2013) provide an overview of the imple-
mentation efforts.
</bodyText>
<subsectionHeader confidence="0.998698">
4.2 Following up from reproduction
</subsectionHeader>
<bodyText confidence="0.9997135">
Since the experiments in Van Erp and Van der Meij
(2013) introduce several new research questions
regarding the influence of data cleaning and the
limitations of the dataset, we performed some ad-
ditional experiments.
First, we varied the tokenisation, removing non-
alphanumeric characters from the data set. This
yielded a significantly smaller data set (10,442
</bodyText>
<page confidence="0.976689">
1696
</page>
<table confidence="0.9990615">
(Freire et al., 2012) results Van Erp and Van der Meij’s replication results
Precision Recall Fβ=1 Precision Recall Fβ=1
LOC (388) 92% 55% 69 77.80% 39.18% 52.05
ORG (157) 90% 57% 70 65.75% 30.57% 41.74
PER (614) 91% 56% 69 73.33% 37.62% 49.73
Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45
</table>
<tableCaption confidence="0.8157235">
Table 3: Precision, recall and Fβ=1 scores for the original experiments from Freire et al. 2012 and our
replication of their approach as presented in Van Erp and Van der Meij (2013)
</tableCaption>
<bodyText confidence="0.997720902439025">
tokens vs 12,510), and a 15 point drop in over-
all F-score. Then, we investigated whether vari-
ation in the cross-validation splits made any dif-
ference as we noticed that some NEs were only
present in particular fields in the data, which can
have a significant impact on a small dataset. We
inspected the difference between different cross-
validation folds by computing the standard devi-
ations of the scores and found deviations of up
to 25 points in F-score between the 10 splits. In
the general setup, database records were randomly
distributed over the folds and cut off to balance the
fold sizes. In a different approach to dividing the
data by distributing individual sentences from the
records over the folds, performance increases by
8.57 points in overall F-score to 58.02. This is not
what was done in the original Freire et al. (2012)
paper, but shows that the results obtained with this
dataset are quite fragile.
As we worried about the complexity of the fea-
ture set relative to the size of the data set, we de-
viated somewhat from Freire et al. (2012)’s exper-
iments in that we switched some features on and
off. Removal of complex features pertaining to the
window around the focus token improved our re-
sults by 3.84 points in overall F-score to 53.39.
The complex features based on VIAF,7 GeoN-
ames8 and WordNet do contribute to the classifica-
tion in the Mallet setup as removing them and only
using the focus token, window and generic fea-
tures causes a slight drop in overall F-score from
49.45 to 47.25.
When training the Stanford NER system (Finkel
et al., 2005) on just the tokens from the
Freire data set and the parameters from en-
glish.all.3class.distsim.prop (included in the Stan-
ford NER release, see also Van Erp and Van der
Meij (2013)), our F-scores come very close to
those reported by Freire et al. (2012), but mostly
with a higher recall and lower precision. It is puz-
zling that the Stanford system obtains such high
</bodyText>
<footnote confidence="0.999954">
7http://www.viaf.org
8http://www.geonames.org
</footnote>
<bodyText confidence="0.9951465">
results with only very simple features, whereas
for Mallet the complex features show improve-
ment over simpler features. This leads to ques-
tions about the differences between the CRF im-
plementations and the influence of their parame-
ters, which we hope to investigate in future work.
</bodyText>
<subsectionHeader confidence="0.995011">
4.3 Reproduction difficulties explained
</subsectionHeader>
<bodyText confidence="0.999890857142857">
Several reasons may be the cause of why we fail to
reproduce results. As mentioned, not all resources
and data were available for this experiment, thus
causing us to navigate in the dark as we could not
reverse-engineer intermediate steps, but only com-
pare to the final precision, recall and F-scores.
The experiments follow a general machine
learning setup consisting roughly of four steps:
preprocess data, generate features, train model and
test model. The novelty and replication problems
lie in the first three steps. How the data was pre-
processed is a major factor here. The data set con-
sisted of XML files marked up with inline named
entity tags. In order to generate machine learn-
ing features, this data has to be tokenised, possi-
bly cleaned up and the named entity markup had
to be converted to a token-based scheme. Each of
these steps can be carried out in several ways, and
choices made here can have great influence on the
rest of the pipeline.
Similar choices have to be made for prepro-
cessing external resources. From the descriptions
in the original paper, it is unclear how records
in VIAF and GeoNames were preprocessed, or
even which versions of these resources were used.
Preprocessing and calculating occurrence statis-
tics over VIAF takes 30 hours for each run. It
is thus not feasible to identify the main potential
variations without the original data to verify this
prepatory step.
Numbers had to be rounded when generating
the features, leading to the question of how many
decimals are required to be discriminative with-
out creating an overly sparse dataset. Freire recalls
that encoding features as multi-value discrete fea-
</bodyText>
<page confidence="0.985699">
1697
</page>
<bodyText confidence="0.999829125">
tures versus several boolean features can have sig-
nificant impact. These settings are not mentioned
in the paper, making reproduction very difficult.
As the project in which the original research
was performed has ended, and there is no cen-
tral repository where such information can be re-
trieved, we are left to wonder how to reuse this
approach in order to further domain-specific NER.
</bodyText>
<sectionHeader confidence="0.993383" genericHeader="method">
5 Observations
</sectionHeader>
<bodyText confidence="0.999988709090909">
In this section, we generalise the observations
from our use cases to the main categories that can
influence reproduction.
Despite our efforts to describe our systems as
clearly as possible, details that can make a tremen-
dous difference are often omitted in papers. It will
be no surprise to researchers in the field that pre-
processing of data can make or break an experi-
ment.
The choice of which steps we perform, and how
each of these steps is carried out exactly are part
of our experimental setup. A major difference in
the results for the NER experiments was caused by
variations in the way in which we split the data for
cross-validation.
As we fine-tune our techniques, software gets
updated, data sets are extended or annotation bugs
are fixed. In the WordNet experiment, we found
that there were two different gold standard data
sets. There are also different versions of Word-
Net, and the WordNet::Similarity packages. Sim-
ilarly for the NER experiment, GeoNames, VIAF
and Mallet are updated regularly. It is therefore
critical to pay attention to versioning.
Our experiments often consist of several differ-
ent steps whose outputs may be difficult to retrace.
In order to check the output of a reproduction ex-
periment at every step of the way, system out-
put of experiments, including intermediate steps,
is vital. The WordNet replication was only pos-
sible, because Pedersen could provide the similar-
ity scores of each word pair. This enabled us to
compare the intermediate output and identify the
source of differences in output.
Lastly, there may be inherent system variations
in the techniques used. Machine learning algo-
rithms may for instance use coin flips in case of
a tie. This was not observed in our experiments,
but such variations may be determined by running
an experiment several times and taking the average
over the different runs (cf. Raeder et al. (2010)).
All together, these observations show that shar-
ing data and software play a key role in gaining in-
sight into how our methods work. Vanschoren et
al. (2012) propose a setup that allows researchers
to provide their full experimental setup, which
should include exact steps followed in preprocess-
ing the data, documentation of the experimen-
tal setup, exact versions of the software and re-
sources used and experimental output. Having
access to such a setup allows other researchers
to validate research, but also tweak the approach
to investigate system variation, systematically test
the approach in order to learn its limitations and
strengths and ultimately improve on it.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999510942857143">
Many of the aspects addressed in the previous sec-
tion such as preprocessing are typically only men-
tioned in passing, or not at all. There is often not
enough space to capture all details, and they are
generally not the core of the research described.
Still, our use cases have shown that they can have a
tremendous impact on reproduction, and can even
lead to different conclusions. This leads to serious
questions on how we can interpret our results and
how we can compare the performance of different
methods. Is an improvement of a few per cent re-
ally due to the novelty of the approach if larger
variations are found when the data is split differ-
ently? Is a method that does not quite achieve the
highest reported state-of-the-art result truly less
good? What does a state-of-the-art result mean if
it is only tested on one data set?
If one really wants to know whether a result
is better or worse than the state-of-the-art, the
range of variation within the state-of-the-art must
be known. Systematic experiments such as the
ones we carried out for WordNet similarity and
NER, can help determine this range. For results
that fall within the range, it holds that they can
only be judged by evaluations going beyond com-
paring performance numbers, i.e. an evaluation of
how the approach achieves a given result and how
that relates to alternative approaches.
Naturally, our use cases do not represent the en-
tire gamut of research methodologies and prob-
lems in the NLP community. However, they do
represent two core technologies and our observa-
tions align with previous literature on replication
and reproduction.
Despite the systematic variation we employed
</bodyText>
<page confidence="0.975646">
1698
</page>
<bodyText confidence="0.999983304347826">
in our experiments, they do not answer all ques-
tions that the problems in reproduction evoked.
For the WordNet experiments, deeper analysis is
required to gain full understanding of how indi-
vidual influential aspects interact with each mea-
surement. For the NER experiments, we are yet to
identify the cause of our failure to reproduce.
The considerable time investment required for
such experiments forms a challenge. Due to pres-
sure to publish or other time limitations, they can-
not be carried out for each evaluation. There-
fore, it is important to share our experiments, so
that other researchers (or students) can take this
up. This could be stimulated by instituting repro-
duction tracks in conferences, thus rewarding sys-
tematic investigation of research approaches. It
can also be aided by adopting initiatives that en-
able authors to easily include data, code and/or
workflows with their publications such as the
PLOS/figshare collaboration.9 We already do a
similar thing for our research problems by organ-
ising challenges or shared tasks, why not extend
this to systematic testing of our approaches?
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999937916666667">
We have presented two reproduction use cases for
the NLP domain. We show that repeating other
researchers’ experiments can lead to new research
questions and provide new insights into and better
understanding of the investigated techniques.
Our WordNet experiments show that the perfor-
mance of similarity measures can be influenced by
the PoS-tags considered, measure specific varia-
tions, the rank coefficient and the gold standard
used for comparison. We not only find that such
variations lead to different numbers, but also dif-
ferent rankings of the individual measures, i.e.
these aspects lead to a different answer to the
question as to which measure performs best. We
did not succeed in reproducing the NER results
of Freire et al. (2012), showing the complexity
of what seems a straightforward reproduction case
based on a system description and training data
only. Our analyses show that it is still an open
question whether additional complex features im-
prove domain specific NER and that this may par-
tially depend on the CRF implementation.
Some observations go beyond our use cases. In
particular, the fact that results vary significantly
</bodyText>
<footnote confidence="0.8971985">
9http://blogs.plos.org/plos/2013/01/
easier-access-to-plos-data/
</footnote>
<bodyText confidence="0.99988696875">
because of details that are not made explicit in
our publications. Systematic testing can provide
an indication of this variation. We have classi-
fied relevant aspects in five categories occurring
across subdisciplines of NLP: preprocessing, ex-
perimental setup, versioning, system output,
and system variation.
We believe that knowing the influence of differ-
ent aspects in our experimental workflow can help
increase our understanding of the robustness of
the approach at hand and will help understand the
meaning of the state-of-the-art better. Some tech-
niques are reused so often (the papers introducing
WordNet similarity measures have around 1,000-
2,000 citations each as of February 2013, for ex-
ample) that knowing their strengths and weak-
nesses is essential for optimising their use.
As mentioned many times before, sharing is key
to facilitating reuse, even if the code is imper-
fect and contains hacks and possibly bugs. In the
end, the same holds for software as for documen-
tation: it is like sex: if it is good, it is very good
and if it is bad, it is better than nothing!10 But
most of all: when reproduction fails, regardless of
whether original code or a reimplementation was
used, valuable insights can emerge from investi-
gating the cause of this failure. So don’t let your
failing reimplementations of the Zigglebottom tag-
ger collect dusk on a shelf while others reimple-
ment their own failing Zigglebottoms. As a com-
munity, we need to know where our approaches
fail, as much –if not more– as where they succeed.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999677133333333">
We would like to thank the anonymous review-
ers for their eye to detail and useful comments
to make this a better paper. We furthermore
thank Ruben Izquierdo, Lourens van der Meij,
Christoph Zwirello, Rebecca Dridan and the Se-
mantic Web Group at VU University for their
help and useful feedback. The research leading to
this paper was supported by the European Union’s
7th Framework Programme via the NewsReader
Project (ICT-316404), the Agora project, by NWO
CATCH programme, grant 640.004.801, and the
BiographyNed project, a joint project with Huy-
gens/ING Institute of the Dutch Academy of Sci-
ences funded by the Netherlands eScience Center
(http://esciencecenter.nl/).
</bodyText>
<footnote confidence="0.89755">
10The documentation variant of this quote is attributed to
Dick Brandon.
</footnote>
<page confidence="0.997909">
1699
</page>
<sectionHeader confidence="0.994728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997946423076923">
Stanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805–
810, Acapulco, August.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Tomasz Buchert and Lucas Nussbaum. 2012. Lever-
aging business workflows in distributed systems re-
search for the orchestration of reproducible and scal-
able experiments. In Anne Etien, editor, 9`eme
´edition de la conf´erence MAnifestation des JE-
unes Chercheurs en Sciences et Technologies de
l’Information et de la Communication - MajecSTIC
2012 (2012).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd dissertation,
University of Pennsylvania.
Irene Cramer. 2008. How well do semantic related-
ness measures perform? a meta-study. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1, pages 59–70.
Olivier Dalle. 2012. On reproducibility and trace-
ability of simulations. In WSC-Winter Simulation
Conference-2012.
Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. In Proceedings of
the Twenty-Sixth International Conference on Ma-
chine Learning: Workshop on Evaluation Methods
for Machine Learning IV.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363–370, Ann Arbor, USA.
Nuno Freire, Jos´e Borbinha, and P´avel Calado. 2012.
An approach for named entity recognition in poorly
structured data. In Proceedings of ESWC 2012.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection
and correction of malapropisms. In C. Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305–332. MIT Press.
James Howison and James D. Herbsleb. 2013. Shar-
ing the spoils: incentives and collaboration in sci-
entific software development. In Proceedings of the
2013 conference on Computer Supported Coopera-
tive Work, pages 459–470.
Darrel C. Ince, Leslie Hatton, and John Graham-
Cumming. 2012. The case for open computer pro-
grams. Nature, 482(7386):485–488.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), pages 19–33, Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30(1-2):81–93.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In C. Fellbaum, edi-
tor, WordNet: An electronic lexical database, pages
265–283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296—304, Madison, USA.
Panos Louridas and Georgios Gousios. 2012. A note
on rigour and replicability. SIGSOFT Softw. Eng.
Notes, 37(5):1–4.
Andrew K. McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Thilo Mende. 2010. Replication of defect prediction
studies: problems, pitfalls and recommendations. In
Proceedings of the 6th International Conference on
Predictive Models in Software Engineering. ACM.
Lingling Meng, Runqing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1):1–12.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28.
Cameron Neylon, Jan Aerts, C Titus Brown, Si-
mon J Coles, Les Hatton, Daniel Lemire, K Jar-
rod Millman, Peter Murray-Rust, Fernando Perez,
Neil Saunders, Nigam Shah, Arfon Smith, Ga¨el
Varoquaux, and Egon Willighagen. 2012. Chang-
ing computational research. the challenges ahead.
Source Code for Biology and Medicine, 7(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1–8, Trento, Italy.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465–470.
</reference>
<page confidence="0.720351">
1700
</page>
<reference confidence="0.999441695652174">
Ted Pedersen. 2010. Information content measures
of semantic similarity perform better without sense-
tagged text. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), pages 329–332, Los Angeles, USA.
Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.
2010. Consequences of variability in classifier per-
formance estimates. In Proceedings of ICDM’2010.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448–453,
Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Wheeler Ruml. 2010. The logic of benchmarking: A
case against state-of-the-art performance. In Pro-
ceedings of the Third Annual Symposium on Combi-
natorial Search (SOCS-10).
Charles Spearman. 1904. Proof and measurement of
association between two things. American Journal
of Psychology, 15:72—101.
Marieke Van Erp and Lourens Van der Meij. 2013.
Reusable research? a case study in named entity
recognition. CLTL 2013-01, Computational Lexi-
cology &amp; Terminology Lab, VU University Amster-
dam.
Joaquin Vanschoren, Hendrik Blockeel, Bernhard
Pfahringer, and Geoffrey Holmes. 2012. Experi-
ment databases. Machine Learning, 87(2):127–158.
Piek Vossen, Isa Maks, Roxane Segers, Hennie van der
Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke. 2013. Cor-
netto: a Combinatorial Lexical Semantic Database
for Dutch. In Peter Spyns and Jan Odijk, editors, Es-
sential Speech and Language Technology for Dutch
Results by the STEVIN-programme, number XVII in
Theory and Applications of Natural Language Pro-
cessing, chapter 10. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133—138, Las Cruces,
USA.
</reference>
<page confidence="0.99184">
1701
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.340801">
<title confidence="0.998373">Offspring from Reproduction What Replication Failure Teaches Us</title>
<author confidence="0.997639">Marten</author>
<affiliation confidence="0.905665">Utrecht Utrecht, The</affiliation>
<email confidence="0.996859">martenp@gmail.com</email>
<author confidence="0.992275">Fokkens van_The Network</author>
<affiliation confidence="0.99996">VU University</affiliation>
<address confidence="0.966341">Amsterdam, The</address>
<author confidence="0.999759">Ted Pedersen</author>
<affiliation confidence="0.999979">Dept. of Computer Science University of Minnesota</affiliation>
<address confidence="0.973738">Duluth, MN 55812 USA</address>
<email confidence="0.99967">tpederse@d.umn.edu</email>
<author confidence="0.9385695">Piek The Network</author>
<affiliation confidence="0.999931">VU University</affiliation>
<address confidence="0.967597">Amsterdam, The</address>
<email confidence="0.969238">piek.vossen@vu.nl</email>
<author confidence="0.708629666666667">Nuno The European The Hague</author>
<author confidence="0.708629666666667">The</author>
<email confidence="0.998355">nfreire@gmail.com</email>
<abstract confidence="0.99985147826087">Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<location>Acapulco,</location>
<contexts>
<context position="13158" citStr="Banerjee and Pedersen (2003)" startWordPosition="2036" endWordPosition="2039">first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared. Differences in </context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Stanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 805– 810, Acapulco, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="8990" citStr="Bikel (2004)" startWordPosition="1399" endWordPosition="1400">one is not enough for reproducing 1http://www.myexperiment.org 2http://www.vldb.org/2013/ 1692 results, a point that is also made by Mende (2010) who provides an overview of all information required to replicate results. The experiments in this paper provide use cases that confirm the points brought out in the literature mentioned above. This includes both observations that a detailed level of information is required for truly insightful reproduction research as well as the claim that such research leads to better understanding of our techniques. Furthermore, the work in this paper relates to Bikel (2004)’s work. He provides all information needed in addition to Collins (1999) to replicate Collins’ benchmark results. Our work is similar in that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel’s. Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) p</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Buchert</author>
<author>Lucas Nussbaum</author>
</authors>
<title>Leveraging business workflows in distributed systems research for the orchestration of reproducible and scalable experiments.</title>
<date>2012</date>
<booktitle>In Anne Etien, editor, 9`eme ´edition de la conf´erence MAnifestation des JEunes Chercheurs en Sciences et Technologies de l’Information et de la Communication - MajecSTIC</booktitle>
<contexts>
<context position="6020" citStr="Buchert and Nussbaum, 2012" startWordPosition="912" endWordPosition="915">is discussed. Sections 3 and 4 describe our real-world use cases. In Section 5, we present our observations, followed by a more general discussion in Section 6. In Section 7, we present our conclusions. 2 Background This section provides a brief overview of recent work addressing reproduction and benchmark results in computer science related studies and discusses how our research fits in the overall picture. Most researchers agree that validating results entails that a method should lead to the same overall conclusions rather than producing the exact same numbers (Drummond, 2009; Dalle, 2012; Buchert and Nussbaum, 2012, etc.). In other words, we should strive to reproduce the same answer to a research question by different means, perhaps by re-implementing an algorithm or evaluating it on a new (in domain) data set. Replication has a somewhat more limited aim, and simply involves running the exact same system under the same conditions in order to get the exact same results as output. According to Drummond (2009) replication is not interesting, since it does not lead to new insights. On this point we disagree with Drummond (2009) as replication allows us to: 1) validate prior research, 2) improve on prior re</context>
</contexts>
<marker>Buchert, Nussbaum, 2012</marker>
<rawString>Tomasz Buchert and Lucas Nussbaum. 2012. Leveraging business workflows in distributed systems research for the orchestration of reproducible and scalable experiments. In Anne Etien, editor, 9`eme ´edition de la conf´erence MAnifestation des JEunes Chercheurs en Sciences et Technologies de l’Information et de la Communication - MajecSTIC 2012 (2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="22248" citStr="Budanitsky and Hirst (2006)" startWordPosition="3531" endWordPosition="3534"> uses the difference in rank as its basis to calculate a correlation, where Kendall τ uses the number of items with the correct rank. The low Kendall τ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase τ, such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ. We included τ, because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing. Phd dissertation,</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="9063" citStr="Collins (1999)" startWordPosition="1410" endWordPosition="1411">www.vldb.org/2013/ 1692 results, a point that is also made by Mende (2010) who provides an overview of all information required to replicate results. The experiments in this paper provide use cases that confirm the points brought out in the literature mentioned above. This includes both observations that a detailed level of information is required for truly insightful reproduction research as well as the claim that such research leads to better understanding of our techniques. Furthermore, the work in this paper relates to Bikel (2004)’s work. He provides all information needed in addition to Collins (1999) to replicate Collins’ benchmark results. Our work is similar in that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel’s. Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) propose to use experimental databases to systematically test variations fo</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Phd dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Cramer</author>
</authors>
<title>How well do semantic relatedness measures perform? a meta-study.</title>
<date>2008</date>
<booktitle>In Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>59--70</pages>
<contexts>
<context position="22562" citStr="Cramer (2008)" startWordPosition="3584" endWordPosition="3585"> that receive the exact same rank help to increase τ, such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ. We included τ, because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method. This study s</context>
</contexts>
<marker>Cramer, 2008</marker>
<rawString>Irene Cramer. 2008. How well do semantic relatedness measures perform? a meta-study. In Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Dalle</author>
</authors>
<title>On reproducibility and traceability of simulations.</title>
<date>2012</date>
<booktitle>In WSC-Winter Simulation Conference-2012.</booktitle>
<contexts>
<context position="5992" citStr="Dalle, 2012" startWordPosition="910" endWordPosition="911">revious work is discussed. Sections 3 and 4 describe our real-world use cases. In Section 5, we present our observations, followed by a more general discussion in Section 6. In Section 7, we present our conclusions. 2 Background This section provides a brief overview of recent work addressing reproduction and benchmark results in computer science related studies and discusses how our research fits in the overall picture. Most researchers agree that validating results entails that a method should lead to the same overall conclusions rather than producing the exact same numbers (Drummond, 2009; Dalle, 2012; Buchert and Nussbaum, 2012, etc.). In other words, we should strive to reproduce the same answer to a research question by different means, perhaps by re-implementing an algorithm or evaluating it on a new (in domain) data set. Replication has a somewhat more limited aim, and simply involves running the exact same system under the same conditions in order to get the exact same results as output. According to Drummond (2009) replication is not interesting, since it does not lead to new insights. On this point we disagree with Drummond (2009) as replication allows us to: 1) validate prior rese</context>
<context position="7267" citStr="Dalle (2012)" startWordPosition="1123" endWordPosition="1124">ftware from scratch, and 3) compare results of reimplementations and obtain the necessary insights to perform reproduction experiments. The outcome of our use cases confirms the statement that deeper insights into an approach can be obtained when all resources are available, an observation also made by Ince et al. (2012). Even if exact replication is not a goal many strive for, Ince et al. (2012) argue that insightful reproduction can be an (almost) impossible undertaking without the source code being available. Moreover, it is not always clear where replication stops and reproduction begins. Dalle (2012) distinguishes levels of reproducing results related to how close they are to the original work and how each contributes to research. In general, an increasing awareness of the importance of reproduction research and open code and data can be observed based on publications in high-profile journals (e.g. Nature (Ince et al., 2012)) and initiatives such as myExperiment.1 Howison and Herbsleb (2013) point out that, even though this is important, often not enough (academic) credit is gained from making resources available. What is worse, the same holds for research that investigates existing metho</context>
</contexts>
<marker>Dalle, 2012</marker>
<rawString>Olivier Dalle. 2012. On reproducibility and traceability of simulations. In WSC-Winter Simulation Conference-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Drummond</author>
</authors>
<title>Replicability is not reproducibility: nor is it good science.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Sixth International Conference on Machine Learning: Workshop on Evaluation Methods for Machine Learning IV.</booktitle>
<contexts>
<context position="5979" citStr="Drummond, 2009" startWordPosition="908" endWordPosition="909"> In Section 2, previous work is discussed. Sections 3 and 4 describe our real-world use cases. In Section 5, we present our observations, followed by a more general discussion in Section 6. In Section 7, we present our conclusions. 2 Background This section provides a brief overview of recent work addressing reproduction and benchmark results in computer science related studies and discusses how our research fits in the overall picture. Most researchers agree that validating results entails that a method should lead to the same overall conclusions rather than producing the exact same numbers (Drummond, 2009; Dalle, 2012; Buchert and Nussbaum, 2012, etc.). In other words, we should strive to reproduce the same answer to a research question by different means, perhaps by re-implementing an algorithm or evaluating it on a new (in domain) data set. Replication has a somewhat more limited aim, and simply involves running the exact same system under the same conditions in order to get the exact same results as output. According to Drummond (2009) replication is not interesting, since it does not lead to new insights. On this point we disagree with Drummond (2009) as replication allows us to: 1) valida</context>
</contexts>
<marker>Drummond, 2009</marker>
<rawString>Chris Drummond. 2009. Replicability is not reproducibility: nor is it good science. In Proceedings of the Twenty-Sixth International Conference on Machine Learning: Workshop on Evaluation Methods for Machine Learning IV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="29058" citStr="Finkel et al., 2005" startWordPosition="4685" endWordPosition="4688">ty of the feature set relative to the size of the data set, we deviated somewhat from Freire et al. (2012)’s experiments in that we switched some features on and off. Removal of complex features pertaining to the window around the focus token improved our results by 3.84 points in overall F-score to 53.39. The complex features based on VIAF,7 GeoNames8 and WordNet do contribute to the classification in the Mallet setup as removing them and only using the focus token, window and generic features causes a slight drop in overall F-score from 49.45 to 47.25. When training the Stanford NER system (Finkel et al., 2005) on just the tokens from the Freire data set and the parameters from english.all.3class.distsim.prop (included in the Stanford NER release, see also Van Erp and Van der Meij (2013)), our F-scores come very close to those reported by Freire et al. (2012), but mostly with a higher recall and lower precision. It is puzzling that the Stanford system obtains such high 7http://www.viaf.org 8http://www.geonames.org results with only very simple features, whereas for Mallet the complex features show improvement over simpler features. This leads to questions about the differences between the CRF implem</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363–370, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuno Freire</author>
<author>Jos´e Borbinha</author>
<author>P´avel Calado</author>
</authors>
<title>An approach for named entity recognition in poorly structured data.</title>
<date>2012</date>
<booktitle>In Proceedings of ESWC</booktitle>
<contexts>
<context position="24272" citStr="Freire et al. (2012)" startWordPosition="3880" endWordPosition="3883"> a vast amount of data. An elaborate analysis of this data is needed to get a better understanding of how measures work and why results vary to such an extent. We leave this investigation to future work. If there is one takehome message from this experiment, it is that one should experiment with parameters such as restrictions on PoS-tags or configurations and determine which score to use depending on what it is used for, rather than picking something that did best in a study using different data for a different task and may have used a different version of WordNet. 4 Reproducing a NER method Freire et al. (2012) describe an approach to classifying named entities in the cultural heritage domain. The approach is based on the assumption that domain knowledge, encoded in complex features, can aid a machine learning algorithm in NER tasks when only little training data is available. These features include information about person and organisation names, locations, as well as PoS-tags. Additionally, some general features are used such as a window of three preceding and two following tokens, token length and capitalisation information. Experiments are run in a 10-fold cross-validation setup using an open so</context>
<context position="25714" citStr="Freire et al. (2012)" startWordPosition="4121" endWordPosition="4124">ger is a tagger with spectacular results that looks like it will solve some major problems in your system. However, the code is not available and a new implementation does not yield the same results. The original authors cannot provide the necessary details to reproduce their results, because most of the work has been done by a PhD student who has finished and moved on to something else. In the end, the newly implemented Zigglebottom tagger is not used, because it does not lead to the promised better results and all effort went to waste. Van Erp was interested in the NER approach presented in Freire et al. (2012). Unfortunately, the code could not be made available, so she decided to reimplement the approach. Despite feedback from Freire about particular details of the system, results remained 20 points below those reported in Freire et al. (2012) in overall F-score (Van Erp and Van der Meij, 2013). The reimplementation process involved choices about seemingly small details such as rounding to how many decimals, how to tokenise or how much data cleanup to perform (normalisation of non-alphanumeric characters for example). Trying different parameter combinations for feature generation and the algorithm</context>
<context position="27032" citStr="Freire et al., 2012" startWordPosition="4331" endWordPosition="4334">first reproduction attempt, together with the original results from Freire et al. (2012) are presented in Table 3. Van Erp and Van der Meij (2013) provide an overview of the implementation efforts. 4.2 Following up from reproduction Since the experiments in Van Erp and Van der Meij (2013) introduce several new research questions regarding the influence of data cleaning and the limitations of the dataset, we performed some additional experiments. First, we varied the tokenisation, removing nonalphanumeric characters from the data set. This yielded a significantly smaller data set (10,442 1696 (Freire et al., 2012) results Van Erp and Van der Meij’s replication results Precision Recall Fβ=1 Precision Recall Fβ=1 LOC (388) 92% 55% 69 77.80% 39.18% 52.05 ORG (157) 90% 57% 70 65.75% 30.57% 41.74 PER (614) 91% 56% 69 73.33% 37.62% 49.73 Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45 Table 3: Precision, recall and Fβ=1 scores for the original experiments from Freire et al. 2012 and our replication of their approach as presented in Van Erp and Van der Meij (2013) tokens vs 12,510), and a 15 point drop in overall F-score. Then, we investigated whether variation in the cross-validation splits made any differenc</context>
<context position="28325" citStr="Freire et al. (2012)" startWordPosition="4554" endWordPosition="4557">he data, which can have a significant impact on a small dataset. We inspected the difference between different crossvalidation folds by computing the standard deviations of the scores and found deviations of up to 25 points in F-score between the 10 splits. In the general setup, database records were randomly distributed over the folds and cut off to balance the fold sizes. In a different approach to dividing the data by distributing individual sentences from the records over the folds, performance increases by 8.57 points in overall F-score to 58.02. This is not what was done in the original Freire et al. (2012) paper, but shows that the results obtained with this dataset are quite fragile. As we worried about the complexity of the feature set relative to the size of the data set, we deviated somewhat from Freire et al. (2012)’s experiments in that we switched some features on and off. Removal of complex features pertaining to the window around the focus token improved our results by 3.84 points in overall F-score to 53.39. The complex features based on VIAF,7 GeoNames8 and WordNet do contribute to the classification in the Mallet setup as removing them and only using the focus token, window and gene</context>
<context position="37944" citStr="Freire et al. (2012)" startWordPosition="6148" endWordPosition="6151">ead to new research questions and provide new insights into and better understanding of the investigated techniques. Our WordNet experiments show that the performance of similarity measures can be influenced by the PoS-tags considered, measure specific variations, the rank coefficient and the gold standard used for comparison. We not only find that such variations lead to different numbers, but also different rankings of the individual measures, i.e. these aspects lead to a different answer to the question as to which measure performs best. We did not succeed in reproducing the NER results of Freire et al. (2012), showing the complexity of what seems a straightforward reproduction case based on a system description and training data only. Our analyses show that it is still an open question whether additional complex features improve domain specific NER and that this may partially depend on the CRF implementation. Some observations go beyond our use cases. In particular, the fact that results vary significantly 9http://blogs.plos.org/plos/2013/01/ easier-access-to-plos-data/ because of details that are not made explicit in our publications. Systematic testing can provide an indication of this variation</context>
</contexts>
<marker>Freire, Borbinha, Calado, 2012</marker>
<rawString>Nuno Freire, Jos´e Borbinha, and P´avel Calado. 2012. An approach for named entity recognition in poorly structured data. In Proceedings of ESWC 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>WordNet: An electronic lexical database,</booktitle>
<pages>305--332</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13191" citStr="Hirst and St-Onge (1998)" startWordPosition="2041" endWordPosition="2044">olved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared. Differences in results turned out to be related </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Howison</author>
<author>James D Herbsleb</author>
</authors>
<title>Sharing the spoils: incentives and collaboration in scientific software development.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 conference on Computer Supported Cooperative Work,</booktitle>
<pages>459--470</pages>
<contexts>
<context position="7666" citStr="Howison and Herbsleb (2013)" startWordPosition="1187" endWordPosition="1190">e et al. (2012) argue that insightful reproduction can be an (almost) impossible undertaking without the source code being available. Moreover, it is not always clear where replication stops and reproduction begins. Dalle (2012) distinguishes levels of reproducing results related to how close they are to the original work and how each contributes to research. In general, an increasing awareness of the importance of reproduction research and open code and data can be observed based on publications in high-profile journals (e.g. Nature (Ince et al., 2012)) and initiatives such as myExperiment.1 Howison and Herbsleb (2013) point out that, even though this is important, often not enough (academic) credit is gained from making resources available. What is worse, the same holds for research that investigates existing methods rather than introducing new ones, as illustrated by the question that is found on many review forms ‘how novel is the presented approach?’. On the other hand, initiatives for journals addressing exactly this issue (Neylon et al., 2012) and tracks focusing on results verification at conferences such as VLDB2 show that this opinion is not universal. A handful of use cases on reproducing or repli</context>
</contexts>
<marker>Howison, Herbsleb, 2013</marker>
<rawString>James Howison and James D. Herbsleb. 2013. Sharing the spoils: incentives and collaboration in scientific software development. In Proceedings of the 2013 conference on Computer Supported Cooperative Work, pages 459–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darrel C Ince</author>
<author>Leslie Hatton</author>
<author>John GrahamCumming</author>
</authors>
<title>The case for open computer programs.</title>
<date>2012</date>
<journal>Nature,</journal>
<volume>482</volume>
<issue>7386</issue>
<contexts>
<context position="6977" citStr="Ince et al. (2012)" startWordPosition="1075" endWordPosition="1078">the exact same results as output. According to Drummond (2009) replication is not interesting, since it does not lead to new insights. On this point we disagree with Drummond (2009) as replication allows us to: 1) validate prior research, 2) improve on prior research without having to rebuild software from scratch, and 3) compare results of reimplementations and obtain the necessary insights to perform reproduction experiments. The outcome of our use cases confirms the statement that deeper insights into an approach can be obtained when all resources are available, an observation also made by Ince et al. (2012). Even if exact replication is not a goal many strive for, Ince et al. (2012) argue that insightful reproduction can be an (almost) impossible undertaking without the source code being available. Moreover, it is not always clear where replication stops and reproduction begins. Dalle (2012) distinguishes levels of reproducing results related to how close they are to the original work and how each contributes to research. In general, an increasing awareness of the importance of reproduction research and open code and data can be observed based on publications in high-profile journals (e.g. Natur</context>
</contexts>
<marker>Ince, Hatton, GrahamCumming, 2012</marker>
<rawString>Darrel C. Ince, Leslie Hatton, and John GrahamCumming. 2012. The case for open computer programs. Nature, 482(7386):485–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING X),</booktitle>
<pages>pages</pages>
<contexts>
<context position="13104" citStr="Jiang and Conrath (1997)" startWordPosition="2028" endWordPosition="2031">stions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spear</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING X), pages 19–33, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--1</pages>
<contexts>
<context position="16823" citStr="Kendall, 1938" startWordPosition="2642" endWordPosition="2643">ecific configuration settings such as including a PoS-tag specific root node or turning on normalisation. In the second stage of this research, we ran experiments that systematically manipulate the influential factors described above. In this experiment, we included both the mc-set and the complete rgset. The implementation of Spearman p used in Pedersen (2010) assigned the lowest number in ranking to ties rather than the mean, resulting in an unjustified drop in results for scores that lead to many ties. We therefore experimented with a different correlation measure, Kendall tau coefficient (Kendall, 1938, T) rather than two versions of Spearman p. 3.3 Variation per measure All measures varied in their performance. The complete outcome of our experiments (both the similarity measures assigned to each pair as well as the output of the ranking coefficients) are included in the data set provided at http://github.com/antske/ WordNetSimilarity. Table 1 presents an overview of the main point we wish to make through this experiment: the minimal and maximal results according to both ranking coefficients. Results for similarity measures varied from 0.06- 0.42 points for Spearman p and from 0.05-0.60 po</context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1-2):81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>WordNet: An electronic lexical database,</booktitle>
<pages>265--283</pages>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13051" citStr="Leacock and Chodorow (1998)" startWordPosition="2020" endWordPosition="2023"> account? 3.2 Methodology and first observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, th</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In C. Fellbaum, editor, WordNet: An electronic lexical database, pages 265–283. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<location>Madison, USA.</location>
<contexts>
<context position="13122" citStr="Lin (1998)" startWordPosition="2033" endWordPosition="2034">wo stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coeffi</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, pages 296—304, Madison, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos Louridas</author>
<author>Georgios Gousios</author>
</authors>
<title>A note on rigour and replicability.</title>
<date>2012</date>
<journal>SIGSOFT Softw. Eng. Notes,</journal>
<volume>37</volume>
<issue>5</issue>
<contexts>
<context position="8329" citStr="Louridas and Gousios (2012)" startWordPosition="1294" endWordPosition="1297"> important, often not enough (academic) credit is gained from making resources available. What is worse, the same holds for research that investigates existing methods rather than introducing new ones, as illustrated by the question that is found on many review forms ‘how novel is the presented approach?’. On the other hand, initiatives for journals addressing exactly this issue (Neylon et al., 2012) and tracks focusing on results verification at conferences such as VLDB2 show that this opinion is not universal. A handful of use cases on reproducing or replicating results have been published. Louridas and Gousios (2012) present a use case revealing that source code alone is not enough for reproducing 1http://www.myexperiment.org 2http://www.vldb.org/2013/ 1692 results, a point that is also made by Mende (2010) who provides an overview of all information required to replicate results. The experiments in this paper provide use cases that confirm the points brought out in the literature mentioned above. This includes both observations that a detailed level of information is required for truly insightful reproduction research as well as the claim that such research leads to better understanding of our techniques</context>
</contexts>
<marker>Louridas, Gousios, 2012</marker>
<rawString>Panos Louridas and Georgios Gousios. 2012. A note on rigour and replicability. SIGSOFT Softw. Eng. Notes, 37(5):1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet. cs.umass.edu.</note>
<contexts>
<context position="24918" citStr="McCallum, 2002" startWordPosition="3983" endWordPosition="3984">fying named entities in the cultural heritage domain. The approach is based on the assumption that domain knowledge, encoded in complex features, can aid a machine learning algorithm in NER tasks when only little training data is available. These features include information about person and organisation names, locations, as well as PoS-tags. Additionally, some general features are used such as a window of three preceding and two following tokens, token length and capitalisation information. Experiments are run in a 10-fold cross-validation setup using an open source machine learning toolkit (McCallum, 2002). 4.1 Reproducing NER Experiments This experiment can be seen as a real-world case of the sad tale of the Zigglebottom tagger (Pedersen, 2008). The (fictional) Zigglebottom tagger is a tagger with spectacular results that looks like it will solve some major problems in your system. However, the code is not available and a new implementation does not yield the same results. The original authors cannot provide the necessary details to reproduce their results, because most of the work has been done by a PhD student who has finished and moved on to something else. In the end, the newly implemented</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet. cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thilo Mende</author>
</authors>
<title>Replication of defect prediction studies: problems, pitfalls and recommendations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Conference on Predictive Models in Software Engineering.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="8523" citStr="Mende (2010)" startWordPosition="1323" endWordPosition="1324">strated by the question that is found on many review forms ‘how novel is the presented approach?’. On the other hand, initiatives for journals addressing exactly this issue (Neylon et al., 2012) and tracks focusing on results verification at conferences such as VLDB2 show that this opinion is not universal. A handful of use cases on reproducing or replicating results have been published. Louridas and Gousios (2012) present a use case revealing that source code alone is not enough for reproducing 1http://www.myexperiment.org 2http://www.vldb.org/2013/ 1692 results, a point that is also made by Mende (2010) who provides an overview of all information required to replicate results. The experiments in this paper provide use cases that confirm the points brought out in the literature mentioned above. This includes both observations that a detailed level of information is required for truly insightful reproduction research as well as the claim that such research leads to better understanding of our techniques. Furthermore, the work in this paper relates to Bikel (2004)’s work. He provides all information needed in addition to Collins (1999) to replicate Collins’ benchmark results. Our work is simila</context>
</contexts>
<marker>Mende, 2010</marker>
<rawString>Thilo Mende. 2010. Replication of defect prediction studies: problems, pitfalls and recommendations. In Proceedings of the 6th International Conference on Predictive Models in Software Engineering. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingling Meng</author>
<author>Runqing Huang</author>
<author>Junzhong Gu</author>
</authors>
<title>A review of semantic similarity measures in wordnet.</title>
<date>2013</date>
<journal>International Journal of Hybrid Information Technology,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="23453" citStr="Meng et al., 2013" startWordPosition="3737" endWordPosition="3740">able, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method. This study shows that it is far from clear how different WordNet similarity measures relate to each other. In fact, we do not know how we can obtain the best results. This is particularly challenging, because the ‘best results’ may depend on the intended use of the similarity scores (Meng et al., 2013). This is also the reason why we presented the maximum variation observed, rather than the average or typical variation (mostly below 0.10 points). The experiments presented in this paper resulted in a vast amount of data. An elaborate analysis of this data is needed to get a better understanding of how measures work and why results vary to such an extent. We leave this investigation to future work. If there is one takehome message from this experiment, it is that one should experiment with parameters such as restrictions on PoS-tags or configurations and determine which score to use depending</context>
</contexts>
<marker>Meng, Huang, Gu, 2013</marker>
<rawString>Lingling Meng, Runqing Huang, and Junzhong Gu. 2013. A review of semantic similarity measures in wordnet. International Journal of Hybrid Information Technology, 6(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="10648" citStr="Miller and Charles (1991)" startWordPosition="1669" endWordPosition="1672">case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Goodenough (1965) (rg-set) which is a superset of Miller and Charles’s set. 3.1 Replication Attempts This research emerged from a project running a s</context>
<context position="13989" citStr="Miller and Charles (1991)" startWordPosition="2159" endWordPosition="2162">tempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared. Differences in results turned out to be related to variations in the experimental setup. First, we made different assumptions on the restriction of part-of-speech tags (henceforth “PoS-tag”) considered in the comparison. Miller and Charles (1991) do not discuss how they deal with words with more than one PoS-tag in their study. Pedersen therefore included all senses with any PoStag in his study. The first replication attempt had restricted PoS-tags to nouns based on the idea that most items are nouns and subjects would be primed to primarily think of the noun senses. Both assumptions are reasonable. Pos-tags were not restricted in the second replication attempt, but because of a bug in the code only the first identified PoS-tag (“noun” in all cases) was considered. We therefore mistakenly assumed that PoS-tag restrictions did not matt</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron Neylon</author>
<author>Jan Aerts</author>
<author>C Titus Brown</author>
<author>Simon J Coles</author>
<author>Les Hatton</author>
<author>Daniel Lemire</author>
<author>K Jarrod Millman</author>
<author>Peter Murray-Rust</author>
<author>Fernando Perez</author>
<author>Neil Saunders</author>
<author>Nigam Shah</author>
<author>Arfon Smith</author>
<author>Ga¨el Varoquaux</author>
<author>Egon Willighagen</author>
</authors>
<title>Changing computational research. the challenges ahead. Source Code for Biology and Medicine,</title>
<date>2012</date>
<volume>7</volume>
<issue>2</issue>
<marker>Neylon, Aerts, Brown, Coles, Hatton, Lemire, Millman, Murray-Rust, Perez, Saunders, Shah, Smith, Ga¨el Varoquaux, Willighagen, 2012</marker>
<rawString>Cameron Neylon, Jan Aerts, C Titus Brown, Simon J Coles, Les Hatton, Daniel Lemire, K Jarrod Millman, Peter Murray-Rust, Fernando Perez, Neil Saunders, Nigam Shah, Arfon Smith, Ga¨el Varoquaux, and Egon Willighagen. 2012. Changing computational research. the challenges ahead. Source Code for Biology and Medicine, 7(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using wordnet based context vectors to estimate the semantic relatedness of concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="10485" citStr="Patwardhan and Pedersen (2006)" startWordPosition="1642" endWordPosition="1645">n the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by R</context>
<context position="13232" citStr="Patwardhan and Pedersen (2006)" startWordPosition="2047" endWordPosition="2050">mpt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared. Differences in results turned out to be related to variations in the experimental setup. </context>
<context position="20459" citStr="Patwardhan and Pedersen (2006)" startWordPosition="3219" endWordPosition="3222">ng positions than measures. 6Section 3.4 explains why the variation in Kendall is this extreme and ρ is more appropriate for this task. Variation Maximum difference Different Spearman ρ Kendall τ rank (tot) WN version 0.44 0.42 223 (252) gold standard 0.24 0.21 359 (504) PoS-tag 0.09 0.08 208 (504) configuration 0.08 0.60 37 (90) Table 2: Variations per category pared two WordNet versions (WN version), three gold standard and PoS-tag restriction variations and configuration only for the subset of scores where configuration matters. There are no definite statements to make as to which version (Patwardhan and Pedersen (2006) vs Pedersen (2010)), PoS-tag restriction or configuration gives the best results. Likewise, while most measures do better on the smaller data set, some achieve their highest results on the full set. This is partially due to the fact that ranking coefficients are sensitive to outliers. In several cases where PoS-tag restrictions led to different results, only one pair received a different score. For instance, path assigns a relatively high score to the pair chord-smile when verbs are included, because the hierarchy of verbs in WordNet is relatively flat. This effect is not observed in wup and </context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using wordnet based context vectors to estimate the semantic relatedness of concepts. In Proceedings of the EACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Empiricism is not a matter of faith.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="25060" citStr="Pedersen, 2008" startWordPosition="4006" endWordPosition="4008">es, can aid a machine learning algorithm in NER tasks when only little training data is available. These features include information about person and organisation names, locations, as well as PoS-tags. Additionally, some general features are used such as a window of three preceding and two following tokens, token length and capitalisation information. Experiments are run in a 10-fold cross-validation setup using an open source machine learning toolkit (McCallum, 2002). 4.1 Reproducing NER Experiments This experiment can be seen as a real-world case of the sad tale of the Zigglebottom tagger (Pedersen, 2008). The (fictional) Zigglebottom tagger is a tagger with spectacular results that looks like it will solve some major problems in your system. However, the code is not available and a new implementation does not yield the same results. The original authors cannot provide the necessary details to reproduce their results, because most of the work has been done by a PhD student who has finished and moved on to something else. In the end, the newly implemented Zigglebottom tagger is not used, because it does not lead to the promised better results and all effort went to waste. Van Erp was interested</context>
</contexts>
<marker>Pedersen, 2008</marker>
<rawString>Ted Pedersen. 2008. Empiricism is not a matter of faith. Computational Linguistics, 34(3):465–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Information content measures of semantic similarity perform better without sensetagged text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010),</booktitle>
<pages>329--332</pages>
<location>Los Angeles, USA.</location>
<contexts>
<context position="10505" citStr="Pedersen (2010)" startWordPosition="1647" endWordPosition="1648">s, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Gooden</context>
<context position="11748" citStr="Pedersen, 2010" startWordPosition="1838" endWordPosition="1840">s a superset of Miller and Charles’s set. 3.1 Replication Attempts This research emerged from a project running a similar experiment for Dutch on Cornetto (Vossen et al., 2013). First, an attempt was made to reproduce the results reported in Patwardhan and Pedersen (2006) and Pedersen (2010) on the English WordNet using their WordNet::Similarity web-interface.3 Results differed from those reported in the aforementioned works, even when using the same versions as the original, WordNet::Similarity-1.02 and WordNet 2.1 (Patwardhan and Pedersen, 2006) and WordNet::Similarity-2.05 and WordNet 3.0 (Pedersen, 2010), respectively.4 The fact that results of similarity measures on WordNet can differ even while the same software and same versions are used indicates that properties which are not addressed in the literature may influence the output of similarity measures. We therefore conducted a range of experiments that, in addition to searching for the right settings to replicate results of previous research, address the following questions: 1) Which properties have an impact on the performance of WordNet similarity measures? 2) How much does the performance of individual measures vary? 3) How do commonly </context>
<context position="13638" citStr="Pedersen (2010)" startWordPosition="2111" endWordPosition="2112"> Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spearman ranking coefficient were compared. Differences in results turned out to be related to variations in the experimental setup. First, we made different assumptions on the restriction of part-of-speech tags (henceforth “PoS-tag”) considered in the comparison. Miller and Charles (1991) do not discuss how they deal with words with more than one PoS-tag in their study. Pedersen therefore included all senses with any PoStag in his study. The first replication attempt had restricted PoS-tags to nouns based on the idea that most items</context>
<context position="16573" citStr="Pedersen (2010)" startWordPosition="2602" endWordPosition="2603">of the similarity measures he used to calculate the coefficient. It is unlikely we would have been able to replicate his results at all without the output of this intermediate step. Finally, results for lch, lesk and wup changed according to measure specific configuration settings such as including a PoS-tag specific root node or turning on normalisation. In the second stage of this research, we ran experiments that systematically manipulate the influential factors described above. In this experiment, we included both the mc-set and the complete rgset. The implementation of Spearman p used in Pedersen (2010) assigned the lowest number in ranking to ties rather than the mean, resulting in an unjustified drop in results for scores that lead to many ties. We therefore experimented with a different correlation measure, Kendall tau coefficient (Kendall, 1938, T) rather than two versions of Spearman p. 3.3 Variation per measure All measures varied in their performance. The complete outcome of our experiments (both the similarity measures assigned to each pair as well as the output of the ranking coefficients) are included in the data set provided at http://github.com/antske/ WordNetSimilarity. Table 1 </context>
<context position="20478" citStr="Pedersen (2010)" startWordPosition="3224" endWordPosition="3225">on 3.4 explains why the variation in Kendall is this extreme and ρ is more appropriate for this task. Variation Maximum difference Different Spearman ρ Kendall τ rank (tot) WN version 0.44 0.42 223 (252) gold standard 0.24 0.21 359 (504) PoS-tag 0.09 0.08 208 (504) configuration 0.08 0.60 37 (90) Table 2: Variations per category pared two WordNet versions (WN version), three gold standard and PoS-tag restriction variations and configuration only for the subset of scores where configuration matters. There are no definite statements to make as to which version (Patwardhan and Pedersen (2006) vs Pedersen (2010)), PoS-tag restriction or configuration gives the best results. Likewise, while most measures do better on the smaller data set, some achieve their highest results on the full set. This is partially due to the fact that ranking coefficients are sensitive to outliers. In several cases where PoS-tag restrictions led to different results, only one pair received a different score. For instance, path assigns a relatively high score to the pair chord-smile when verbs are included, because the hierarchy of verbs in WordNet is relatively flat. This effect is not observed in wup and lch which correct f</context>
</contexts>
<marker>Pedersen, 2010</marker>
<rawString>Ted Pedersen. 2010. Information content measures of semantic similarity perform better without sensetagged text. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), pages 329–332, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Troy Raeder</author>
<author>T Ryan Hoens</author>
<author>Nitesh V Chawla</author>
</authors>
<title>Consequences of variability in classifier performance estimates.</title>
<date>2010</date>
<booktitle>In Proceedings of ICDM’2010.</booktitle>
<contexts>
<context position="9746" citStr="Raeder et al. (2010)" startWordPosition="1520" endWordPosition="1523">n that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel’s. Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) propose to use experimental databases to systematically test variations for machine learning, but neither links the two issues together. Raeder et al. (2010) come closest to our work in a critical study on the evaluation of machine learning. They show that choices in the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a</context>
<context position="33727" citStr="Raeder et al. (2010)" startWordPosition="5459" endWordPosition="5462">output of experiments, including intermediate steps, is vital. The WordNet replication was only possible, because Pedersen could provide the similarity scores of each word pair. This enabled us to compare the intermediate output and identify the source of differences in output. Lastly, there may be inherent system variations in the techniques used. Machine learning algorithms may for instance use coin flips in case of a tie. This was not observed in our experiments, but such variations may be determined by running an experiment several times and taking the average over the different runs (cf. Raeder et al. (2010)). All together, these observations show that sharing data and software play a key role in gaining insight into how our methods work. Vanschoren et al. (2012) propose a setup that allows researchers to provide their full experimental setup, which should include exact steps followed in preprocessing the data, documentation of the experimental setup, exact versions of the software and resources used and experimental output. Having access to such a setup allows other researchers to validate research, but also tweak the approach to investigate system variation, systematically test the approach in </context>
</contexts>
<marker>Raeder, Hoens, Chawla, 2010</marker>
<rawString>Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla. 2010. Consequences of variability in classifier performance estimates. In Proceedings of ICDM’2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="13072" citStr="Resnik (1995)" startWordPosition="2025" endWordPosition="2026"> observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and e</context>
<context position="22263" citStr="Resnik (1995)" startWordPosition="3535" endWordPosition="3536">as its basis to calculate a correlation, where Kendall τ uses the number of items with the correct rank. The low Kendall τ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase τ, such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ. We included τ, because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI), pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="11116" citStr="Rubenstein and Goodenough (1965)" startWordPosition="1741" endWordPosition="1744">) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Goodenough (1965) (rg-set) which is a superset of Miller and Charles’s set. 3.1 Replication Attempts This research emerged from a project running a similar experiment for Dutch on Cornetto (Vossen et al., 2013). First, an attempt was made to reproduce the results reported in Patwardhan and Pedersen (2006) and Pedersen (2010) on the English WordNet using their WordNet::Similarity web-interface.3 Results differed from those reported in the aforementioned works, even when using the same versions as the original, WordNet::Similarity-1.02 and WordNet 2.1 (Patwardhan and Pedersen, 2006) and WordNet::Similarity-2.05 </context>
<context position="14849" citStr="Rubenstein and Goodenough (1965)" startWordPosition="2303" endWordPosition="2306"> idea that most items are nouns and subjects would be primed to primarily think of the noun senses. Both assumptions are reasonable. Pos-tags were not restricted in the second replication attempt, but because of a bug in the code only the first identified PoS-tag (“noun” in all cases) was considered. We therefore mistakenly assumed that PoS-tag restrictions did not matter until we compared individual scores between Pedersen and the replication attempts. Second, there are two gold standards for the Miller and Charles (1991) set: one has the scores assigned during the original experiment run by Rubenstein and Goodenough (1965), the other has the scores assigned during Miller and Charles (1991)’s own experiment. The ranking correlation between the two sets is high, but they are not identical. Again, there is no reason why one gold standard would be a better choice than the other, but in order to replicate results, it must be known which of the two was used. Third, results changed because of differences in the treatment of ties while calculating Spearman p. The influence of the exact gold standard and calculation of Spearman p could only be found because Pedersen could promeasure Spearman ρ Kendall τ ranking min max </context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wheeler Ruml</author>
</authors>
<title>The logic of benchmarking: A case against state-of-the-art performance.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Annual Symposium on Combinatorial Search (SOCS-10).</booktitle>
<contexts>
<context position="9476" citStr="Ruml (2010)" startWordPosition="1480" endWordPosition="1481">at such research leads to better understanding of our techniques. Furthermore, the work in this paper relates to Bikel (2004)’s work. He provides all information needed in addition to Collins (1999) to replicate Collins’ benchmark results. Our work is similar in that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel’s. Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) propose to use experimental databases to systematically test variations for machine learning, but neither links the two issues together. Raeder et al. (2010) come closest to our work in a critical study on the evaluation of machine learning. They show that choices in the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluatio</context>
</contexts>
<marker>Ruml, 2010</marker>
<rawString>Wheeler Ruml. 2010. The logic of benchmarking: A case against state-of-the-art performance. In Proceedings of the Third Annual Symposium on Combinatorial Search (SOCS-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Spearman</author>
</authors>
<title>Proof and measurement of association between two things.</title>
<date>1904</date>
<journal>American Journal of Psychology,</journal>
<pages>15--72</pages>
<contexts>
<context position="10980" citStr="Spearman, 1904" startWordPosition="1720" endWordPosition="1721">er compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Goodenough (1965) (rg-set) which is a superset of Miller and Charles’s set. 3.1 Replication Attempts This research emerged from a project running a similar experiment for Dutch on Cornetto (Vossen et al., 2013). First, an attempt was made to reproduce the results reported in Patwardhan and Pedersen (2006) and Pedersen (2010) on the English WordNet using their WordNet::Similarity web-interface.3 Results differed from those reported in the aforementioned works, even when using t</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>Charles Spearman. 1904. Proof and measurement of association between two things. American Journal of Psychology, 15:72—101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marieke Van Erp</author>
<author>Lourens Van der Meij</author>
</authors>
<title>Reusable research? a case study in named entity recognition.</title>
<date>2013</date>
<booktitle>CLTL 2013-01, Computational Lexicology &amp; Terminology Lab,</booktitle>
<publisher>VU University</publisher>
<location>Amsterdam.</location>
<marker>Van Erp, Van der Meij, 2013</marker>
<rawString>Marieke Van Erp and Lourens Van der Meij. 2013. Reusable research? a case study in named entity recognition. CLTL 2013-01, Computational Lexicology &amp; Terminology Lab, VU University Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquin Vanschoren</author>
<author>Hendrik Blockeel</author>
<author>Bernhard Pfahringer</author>
<author>Geoffrey Holmes</author>
</authors>
<title>Experiment databases.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="9588" citStr="Vanschoren et al. (2012)" startWordPosition="1496" endWordPosition="1499">er relates to Bikel (2004)’s work. He provides all information needed in addition to Collins (1999) to replicate Collins’ benchmark results. Our work is similar in that we also aim to fill in the blanks needed to replicate results. It must be noted, however, that the use cases in this paper have a significantly smaller scale than Bikel’s. Our research distinguishes itself from previous work, because it links the challenges of reproduction to what they mean for reported results beyond validation. Ruml (2010) mentions variations in outcome as a reason not to emphasise comparisons to benchmarks. Vanschoren et al. (2012) propose to use experimental databases to systematically test variations for machine learning, but neither links the two issues together. Raeder et al. (2010) come closest to our work in a critical study on the evaluation of machine learning. They show that choices in the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how </context>
<context position="33885" citStr="Vanschoren et al. (2012)" startWordPosition="5487" endWordPosition="5490">ores of each word pair. This enabled us to compare the intermediate output and identify the source of differences in output. Lastly, there may be inherent system variations in the techniques used. Machine learning algorithms may for instance use coin flips in case of a tie. This was not observed in our experiments, but such variations may be determined by running an experiment several times and taking the average over the different runs (cf. Raeder et al. (2010)). All together, these observations show that sharing data and software play a key role in gaining insight into how our methods work. Vanschoren et al. (2012) propose a setup that allows researchers to provide their full experimental setup, which should include exact steps followed in preprocessing the data, documentation of the experimental setup, exact versions of the software and resources used and experimental output. Having access to such a setup allows other researchers to validate research, but also tweak the approach to investigate system variation, systematically test the approach in order to learn its limitations and strengths and ultimately improve on it. 6 Discussion Many of the aspects addressed in the previous section such as preproce</context>
</contexts>
<marker>Vanschoren, Blockeel, Pfahringer, Holmes, 2012</marker>
<rawString>Joaquin Vanschoren, Hendrik Blockeel, Bernhard Pfahringer, and Geoffrey Holmes. 2012. Experiment databases. Machine Learning, 87(2):127–158.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Piek Vossen</author>
<author>Isa Maks</author>
<author>Roxane Segers</author>
</authors>
<title>Hennie van der Vliet, Marie-Francine Moens, Katja Hofmann, Erik Tjong Kim Sang, and Maarten de Rijke.</title>
<date>2013</date>
<booktitle>Essential Speech and Language Technology for Dutch Results by the STEVIN-programme, number XVII in Theory and Applications of Natural Language Processing, chapter 10.</booktitle>
<editor>In Peter Spyns and Jan Odijk, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="11309" citStr="Vossen et al., 2013" startWordPosition="1774" endWordPosition="1777">airs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Goodenough (1965) (rg-set) which is a superset of Miller and Charles’s set. 3.1 Replication Attempts This research emerged from a project running a similar experiment for Dutch on Cornetto (Vossen et al., 2013). First, an attempt was made to reproduce the results reported in Patwardhan and Pedersen (2006) and Pedersen (2010) on the English WordNet using their WordNet::Similarity web-interface.3 Results differed from those reported in the aforementioned works, even when using the same versions as the original, WordNet::Similarity-1.02 and WordNet 2.1 (Patwardhan and Pedersen, 2006) and WordNet::Similarity-2.05 and WordNet 3.0 (Pedersen, 2010), respectively.4 The fact that results of similarity measures on WordNet can differ even while the same software and same versions are used indicates that proper</context>
</contexts>
<marker>Vossen, Maks, Segers, 2013</marker>
<rawString>Piek Vossen, Isa Maks, Roxane Segers, Hennie van der Vliet, Marie-Francine Moens, Katja Hofmann, Erik Tjong Kim Sang, and Maarten de Rijke. 2013. Cornetto: a Combinatorial Lexical Semantic Database for Dutch. In Peter Spyns and Jan Odijk, editors, Essential Speech and Language Technology for Dutch Results by the STEVIN-programme, number XVII in Theory and Applications of Natural Language Processing, chapter 10. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verb semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<location>Las Cruces, USA.</location>
<contexts>
<context position="12755" citStr="Wu and Palmer (1994)" startWordPosition="1993" endWordPosition="1996">us research, address the following questions: 1) Which properties have an impact on the performance of WordNet similarity measures? 2) How much does the performance of individual measures vary? 3) How do commonly used measures compare when the variation of their performance are taken into account? 3.2 Methodology and first observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Peders</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verb semantics and lexical selection. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 133—138, Las Cruces, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>