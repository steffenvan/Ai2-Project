<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.948559">
Document Fusion for Comprehensive Event Description
</title>
<author confidence="0.992397">
Christof Monz
</author>
<affiliation confidence="0.9979665">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.764049">
1018 TV Amsterdam, The Netherlands
</address>
<email confidence="0.885897">
christof@science.uva.nl
www.science.uva.nl/˜christof
</email>
<sectionHeader confidence="0.994869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914416666667">
This paper describes a fully imple-
mented system for fusing related news
stories into a single comprehensive de-
scription of an event. The basic compo-
nents and the underlying algorithm are
explained. The system uses a compu-
tationally feasible and robust notion of
entailment for comparing information
stemming from different documents.
We discuss the issue of evaluating doc-
ument fusion and provide some prelim-
inary results.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993853164557">
Conventional text retrieval systems respond to a
user’s query by providing a (ranked) list of doc-
uments which potentially satisfy the information
need. After having identified a number of docu-
ments which are actually relevant, the user reads
some of those documents to get the information
requested. To be sure to get a comprehensive ac-
count of a particular topic, the list of documents
one has to read may be rather long, including a se-
vere amount of redundancy; i.e., documents par-
tially conveying the same information.
Although this problems basically holds for any
text retrieval situation, where comprehensiveness
is relevant, it becomes particularly evident in the
retrieval of news texts.
News agencies, such as AP, BBC, CNN, or
Reuters, often describe the same event differ-
ently. For instance, they provide different back-
ground information, helping the reader to situate
the story, they interview different people to com-
ment on an event, and they provide additional,
conflicting or more accurate information, depend-
ing on their sources.
To get a description of an event which is as
comprehensive as possible and also as short as
possible, a user has to compile his or her own
description by taking parts of the original news
stories, ignoring duplicate information. Typical
users include journalists and intelligence analysts,
for whom compiling and fusing information is an
integral part of their work (Carbonell et al., 2000).
Obviously, if done manually, this process can be
rather laborious as it involves numerous compar-
isons, depending on the number and length of the
documents.
The aim of this paper is to describe an approach
automatizing this process by fusing information
stemming from different documents to generate
a single comprehensive document, containing the
information of all original documents without re-
peating information which is conveyed by two or
more documents.
The work described in this paper is closely re-
lated to the area of multi-document summariza-
tion (Barzilay et al., 1999; Mani and Bloedorn,
1999; McKeown and Radev, 1995; Radev, 2000),
where related documents are analyzed to use fre-
quently occurring segments for identifying rele-
vant information that has to be included in the
summary. Our work differs from the work on
multi-document summarization as we focus on
document fusion disregarding summarization. On
the contrary, we are not aiming for the shortest
description containing the most relevant informa-
tion, but for the shortest description containing
all information. For instance, even historic back-
ground information is included, as long as it al-
lows the reader to get a more comprehensive de-
scription of an event.
Although the techniques that are used for
multi-document fusion and multi-document sum-
marization are similar, the task of fusion is com-
plementary to the summarization task. They dif-
fer in the way that, roughly speaking, multi-
document summarization is the intersection of
information within a topic, whereas multi-
document fusion is the union of information.
They are similar to the extent that in both cases
nearly equivalent information stemming from dif-
ferent documents within the topic has to be iden-
tified as such.
The remainder of this paper is structured as fol-
lows: Section 2 introduces the main components
and challenges of implementing a document fu-
sion system. Issues of evaluating document fu-
sion and some preliminary evaluation of our sys-
tem are presented in Section 3. In Section 4,
some conclusions and prospects on future work
are given.
</bodyText>
<sectionHeader confidence="0.95538" genericHeader="method">
2 Fusing Documents
</sectionHeader>
<bodyText confidence="0.7892995">
Before developing a document fusion system,
some basic issues have to be considered.
</bodyText>
<listItem confidence="0.874986545454545">
1. On which level of granularity are the docu-
ments fused (i.e., word or phrase level, sen-
tence level, or paragraph level?
2. How to decide whether news fragments from
different sources convey the same informa-
tion?
3. How to ensure readability of the fused docu-
ment? I.e., where should information stem-
ming from different documents be placed in
the fused document, retaining a natural flow
of information.
</listItem>
<bodyText confidence="0.85897">
Each of the these issues is addressed in the fol-
lowing subsections.
</bodyText>
<subsectionHeader confidence="0.985579">
2.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999925666666666">
In the current implementation, we decided to
fuse documents on paragraph level for two rea-
sons: First, paragraphs are less context-dependent
than sentences and are therefore easier to com-
pare. Second, compiling paragraphs yields a bet-
ter readability of the fused document. It should
be noted that paragraphs are rather short in news
stories, rarely being longer than three sentences.
When putting together (fusing) pieces of text
from different sources in a way that was not an-
ticipated by the writers of the news stories, it can
introduce information gaps. For instance, if a
paragraph containing a pronoun is taken out of its
original context and placed in a new context (the
fused document), this can lead to dangling pro-
nouns, which cannot be correctly resolved any-
more. In general, this problem does not only hold
for pronouns but for all kind of anaphoric ex-
pressions such as pronouns, definite noun phrases
(e.g., the negotiations) and anaphoric adverbials
(e.g., later). To cope with this problem simple
segmentation is applied as a pre-processing step
where paragraphs that contain pronouns or simple
definite noun phrases are attached to the preced-
ing paragraph. A more sophisticated approach to
text segmentation is described in (Hearst, 1997).
Obviously, it would be better to use an au-
tomatic anaphora resolution component to cope
with this problem, see, e.g., (Kennedy and Bogu-
raev, 1996; Kameyama, 1997), where anaphoric
expressions are replaced by their antecedents, but
at the moment, the integration of such a compo-
nent remains future work.
</bodyText>
<subsectionHeader confidence="0.964101">
2.2 Informativity
</subsectionHeader>
<bodyText confidence="0.999970384615384">
(Radev, 2000) describes 24 cross-document rela-
tions that can hold between their segments, one of
which is the subsumption (or entailment) relation.
In the context of document fusion, we focus on
the entailment relation and how it can be formally
defined; unfortunately, (Radev, 2000) provides no
formal definition for any of the relations.
Computing the informativity of a segment
compared to another segment is an essential task
during document fusion. Here, we say that the i-
th segment of document d (si,d) is more informa-
tive than the j-th segment of document d&apos; (sj,d&apos;) if
si,d entails sj,d&apos;. In theory, this should be proven
logically, but in practice this is far beyond the cur-
rent state of the art in natural language processing.
Additionally, a binary logical decision might also
be too strict for simulating the human understand-
ing of entailment.
A simple but nevertheless quite effective solu-
tion is based on one of the simpler similarity mea-
sures in information retrieval (IR), where texts are
simply represented as bags of (weighted) words.
The definition of the entailment score (es) is given
in (1). es(si,d, sj,d&apos;) compares the sum of the
weights of terms that appear in both segments to
the total sum weights of sj,d&apos;.
</bodyText>
<equation confidence="0.9952795">
E idf k
es(si,d, sj,d&apos;) = tk∈si,d∩sj,d&apos;
</equation>
<bodyText confidence="0.9999734">
The weight of a term ti is its inverse document
frequency (idf i), as defined in (2), where N is the
number of all segments in the set of related doc-
uments (the topic) and ni is the number of seg-
ments in which the term ti occurs.
</bodyText>
<equation confidence="0.912326">
idf i = log Cni) (2)
</equation>
<bodyText confidence="0.999301375">
Terms which occur in many segments (i.e., for
which ni is rather large), such as the, some, etc.,
receive a lower idf-score than terms that occur
only in a few segments. The underlying intuition
of the idf-score is that terms with a higher idf-
score are better suited for discriminating the con-
tent of a particular segment from the other seg-
ments in the topic, or to put it differently, they are
more content-bearing. Note, that the logarithm in
(2) is only used for dampening the differences.
The entailment score es(si,d, sj,d&apos;) measures
how many of the words of the segment si,d oc-
cur in sj,d&apos;, and how important those words are.
This is obviously a very shallow approach to en-
tailment computation, but nevertheless it proved
to be effective, see (Monz and de Rijke, 2001).
</bodyText>
<subsectionHeader confidence="0.996686">
2.3 Implementation
</subsectionHeader>
<bodyText confidence="0.996395690909091">
In this subsection, we present the general algo-
rithm underlying the implementation, given a set
of documents belonging to topic T. The imple-
mentation has to tackle two basic tasks. First,
identify segments that are entailed by other seg-
ments and use the more informative one. Second,
place the remaining segments at positions with
similar content. The fusion algorithm depicted in
Figure 1 consists of five steps.
1. is basically a pre-processing step as ex-
plained above. 2. computes pairwise the cross-
document entailment scores for all segments in T.
Although the pairwise computation of es and sim
is exponential in the number of documents in T,
it still remains computationally tractable in prac-
tice. For instance, for a topic containing 4 docu-
ments (the average case) it takes 10 CPU seconds
to compute all entailment and similarity relations.
For a topic containing 8 documents (an artificially
constructed extreme case) it takes 66 CPU sec-
onds; both on a 600 MHz Pentium III PC.
In 3., one of the documents is taken as base for
the fusion process. Starting with a ‘real’ docu-
ment improves the readability of the final fused
documents as it imposes some structure on the
fusion process. There are several ways to select
the base document. For instance, take the docu-
ment with the most unique terms, or the document
with the highest document weight (sum of all idf-
scores). In the current implementation we simply
took the longest document within the topic, which
ensures a good base coverage of an event.
4. and 5. are the actual fusion steps. Step 4. re-
places a segment si,dF in the fused document by
a segment sj,d&apos; from another document if sj,d&apos; is
the segment maximally entailing si,dF and if it is
significantly (above the threshold Bes) more infor-
mative than si,dF . Choosing an optimal value for
Bes is essential for the effectiveness of the fusion
system. Section 3 discusses some of our experi-
ments to determine Bes.
Step 5. is kind of complementary to step 4.,
where related but more informative segments are
identified. Step 5. identifies segments that add
new information to dF, where a segment sj,d&apos; is
new if it has low similarity to all segments in dF,
i.e., if the the similarity score is below the thresh-
old Bsim. If a segment sj,d&apos; is new, it is placed
right after the segment in dF to which it is most
similar.
Similarity is implemented as the traditional co-
sine similarity in information retrieval, as defined
in (3). This similarity measure is also known
as the tfc.tfc measure, see (Salton and Buckley,
1988).
</bodyText>
<equation confidence="0.963222384615385">
wk,si,d · wk,sj,d&apos;
(3)
/// wsi,d · wsj, d&apos;
VtkEsi,d tkEsj,d&apos;
Where wk,si,d is the weight associated with the
term tk in segment si,d. In the nominator of (3),
E
tk∈sj,d&apos;
(1)
idf k
sim(Si,d, Sj,d&apos;) =
E
tkEsi,dnsj,d&apos;
</equation>
<listItem confidence="0.887549571428572">
1. segmentize all documents in T
2. for all si,d, sj,d0 s.t. d, d&apos; E T and d =� d&apos;: compute es(si,d, sj,d0)
3. select a document d E T as fusion base document: dF
4. for all si,dF: find sj,d0 s.t. dF =� d&apos; and sj,d0 = arg max : es(sk,d0, si,dF ) &gt; es(si,dF , sk,d0)
sk d0
if es(sj,d0, si,dF) &gt; Bes then replace si,dF by sj,d0 in the fused document
5. for all sj,d0 s.t. sj,d0 E� dF:
</listItem>
<figure confidence="0.87181425">
if for all si,dF: sim(si,dF , sj,d0) &lt; Bsim,
then find the most similar si,dF: si,dF = arg max : sim(sj,d0, sk,dF)
skIdF
and place sj,d0 between si,dF and si+1,dF
</figure>
<figureCaption confidence="0.999985">
Figure 1: Sketch of the document fusion algorithm.
</figureCaption>
<bodyText confidence="0.999710125">
the weights of the terms that occur in si,d and sj,d0
are summed up. The denominator is used for nor-
malization. Otherwise, longer documents tend to
result in a higher similarity score. In the current
implementation wk,szId = idf k for all si,d. The
reader is referred to (Salton and Buckley, 1988;
Zobel and Moffat, 1998) for a broad spectrum of
similarity measures for information retrieval.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="method">
3 Evaluation Issues
</sectionHeader>
<bodyText confidence="0.999907333333333">
The document fusion system is evaluated in two
steps. First, the effectiveness of entailment detec-
tion is evaluated, which is the key component of
our system. Then we present some preliminary
evaluation of the whole system focusing on the
quality of the fused documents.
</bodyText>
<subsectionHeader confidence="0.998408">
3.1 Evaluating Entailment
</subsectionHeader>
<bodyText confidence="0.999943333333333">
Recently, we have started to build a small test col-
lection for evaluating entailment relations. The
reader is referred to (Monz and de Rijke, 2001)
for more details on the results presented in this
subsection.
For each of the 21 topics in our test corpus two
documents in the topic were randomly selected,
and given to a human assessor to determine all
subsumption relations between segments in dif-
ferent documents (within the same topic). Judg-
ments were made on a scale 0–2, according to the
extent to which one segment was found to entail
another.
Out of the 12083 possible subsumption rela-
tions between the text segments, 501 (4.15%) re-
ceived a score of 1, and 89 (0.73%) received a
score of 2.
Let a subsumption pair be an ordered pair of
segments (si,d, sj,d0) that may or may not stand
in the subsumption relation, and let a correct sub-
sumption pair be a subsumption pair (si,d, sj,d0)
for which si,d does indeed entail sj,d0. Further, a
computed subsumption pair is a subsumption pair
for which our subsumption method has produced
a score above the subsumption threshold.
Then, precision is the fraction of computed
subsumption pairs that is correct:
</bodyText>
<equation confidence="0.832502">
Precision =
</equation>
<bodyText confidence="0.9105086">
number of correct subsumption pairs computed
total number of subsumption pairs computed .
And recall is the proportion of the total number of
correct subsumption pairs that were computed:
number of correct subsumption pairs computed
</bodyText>
<equation confidence="0.702533">
Recall = .
</equation>
<subsectionHeader confidence="0.363349">
total number of correct subsumption pairs
</subsectionHeader>
<bodyText confidence="0.999379">
Observe that precision and recall depend on the
subsumption threshold that we use.
We computed average recall and precision at 11
different subsumption thresholds, ranging from 0
to 1, with .1 increments; the average was com-
puted over all topics. The results are summarized
in Figures 2 (a) and (b).
Since precision and recall suggest two differ-
ent optimal subsumption thresholds, we use the F-
Score, or harmonic mean, which has a high value
only when both recall and precision are high.
</bodyText>
<footnote confidence="0.598772">
1+ 1
Recall Precision
</footnote>
<page confidence="0.947996">
2
</page>
<figure confidence="0.97314955882353">
F=
Precision
Recall
Subsumption Threshold
Subsumption Threshold
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Human Judgm. &gt; 0
Human Judgm. &gt; 1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Human Judgm. &gt; 0
Human Judgm. &gt; 1
(a) (b)
</figure>
<figureCaption confidence="0.996804">
Figure 2: (a) Average precision with human judgments &gt; 0 and &gt; 1. (b) Average recall with human
judgments &gt; 0 and &gt; 1.
</figureCaption>
<subsectionHeader confidence="0.488724">
Subsumption Threshold
</subsectionHeader>
<figureCaption confidence="0.9454115">
Figure 3: Average F-scores with human judg-
ments &gt; 0 and &gt; 1.
</figureCaption>
<bodyText confidence="0.9998072">
The optimal subsumption threshold for human
judgments &gt; 0 is around 0.18, while it is approx-
imately 0.4 for human judgments &gt; 1. This con-
firms the intuition that a higher threshold is more
effective when human judgments are stricter.
</bodyText>
<subsectionHeader confidence="0.999113">
3.2 Evaluating Fusion
</subsectionHeader>
<bodyText confidence="0.999939275">
In the introduction, it was pointed out that docu-
ment fusion by hand can be rather laborious, and
the same holds for the evaluation of automatic
document fusion. Similar to automatic summa-
rization, there are no standard document collec-
tions or clear evaluation criteria aiding to autom-
atize the process of evaluation. One approach
could be to focus on news stories which mention
their sources. For instance CNN’s new stories of-
ten say that “AP and Reuters contributed to this
story”. On the other hand one has to be cautious
to take those news stories as gold standard as the
respective contributions of the journalist and his
or her sources are not made explicit.
In the area of multi-document summarization,
there is a distinction between intrinsic and extrin-
sic evaluation, see (Mani et al., 1998). Intrin-
sic evaluation judges the quality directly based on
analysis of the summary. Usually, a human judge
assesses the quality of a summary based on some
standardized evaluation criteria.
In extrinsic evaluation, the usefulness of a sum-
mary is judged based on how it affects the com-
pletion of some other task. A typical task used
for extrinsic evaluation is ad-hoc retrieval, where
the relevance of a retrieved document is assessed
by a human judge based on the document’s sum-
mary. Then, those judgments are compared to
judgments based on original documents, see, e.g.,
(Brandow et al., 1995; Mani and Bloedorn, 1999).
At this stage we have just carried out some pre-
liminary evaluation. The test collection consists
of 69 news stories categorized into 21 topics. Cat-
egorization was done by hand, but it is also pos-
sible to have information filtering, see (Robertson
and Hull, 2001), or topic detection and tracking
(TDT) tools carrying out this task (Allan et al.,
1998). All documents belonging to the same topic
were released on the same day and describe the
same event. Table 1 provides further details on
</bodyText>
<figure confidence="0.959214722222222">
The average F-scores are given in Figure 3.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F−Score
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Human Judgm. &gt; 0
Human Judgm. &gt; 1
the collection.
avg. per topic
</figure>
<bodyText confidence="0.7757878">
no. of docs. 3.3 docs.
length of a doc. 612 words
length of all docs. together 2115 words
length of longest doc. 783 words
length of shortest doc. 444 words
</bodyText>
<tableCaption confidence="0.832993">
Table 1: Test collection (21 topics, 69 docu-
ments).
</tableCaption>
<bodyText confidence="0.999659928571429">
In addition to the aforementioned news agen-
cies, the collection includes texts from the L.A.
Times, Washington Post and Washington Times.
In general, a segment should be included in the
fused document if it did not occur before to avoid
redundancy (False Alarm), and if it adds informa-
tion, so no information is left out (Miss). As in IR
or TDT, Miss and False Alarm tend to be inversely
related; i.e., a decrease of Miss often results in an
increase of False Alarm and vice versa.
Table 2 illustrates the different possibilities
how the system responds as to whether a seg-
ment should be included in the fused document
and how a human reader judges.
</bodyText>
<table confidence="0.41851825">
system reader: reader:
judgement include exclude
include a b
exclude c d
</table>
<tableCaption confidence="0.994188">
Table 2: Contingency table.
</tableCaption>
<bodyText confidence="0.9945965">
Then, Miss and False Alarm can be defined as
in (4) and (5), respectively.
</bodyText>
<equation confidence="0.485078">
ifa+c&gt; 0 (4)
b
False Alarm = b + d if b + d &gt; 0 (5)
</equation>
<bodyText confidence="0.999933">
The fusion impact factor (fif) describes to what
extent the different sources actually contributed to
the fused document. For instance if the fused doc-
ument solely contains segments from one source,
fif equals 0, and if all sources equally contributed
it equals 1. This can be formalized as follows:
</bodyText>
<equation confidence="0.996218">
E � 1/nT − nseg,d/nseg �
fif = 1 − d∈T
</equation>
<bodyText confidence="0.934677875">
Where S is a set of related documents, and nT
is its size. nseg is the number of segments in the
fused document and nseg,d is the number of seg-
ments stemming from document d.
For our test collection, the average fusion im-
pact factor was 0.56. Of course the fif-score de-
pends on the choice of Bes and Bsim, in a way that
a lower value of Bes or a higher value of Bsim in-
creases the fif-score. In this case, Bes = 0.2 and
Bsim = 0.05.
Table 3 shows the length of the fused docu-
ments in average compared to the longest, short-
est, and all documents in a topic, for Bes = 0.2
and Bsim = 0.05.
avg. compression
ratio per topic
</bodyText>
<table confidence="0.795131666666667">
all docs. together 0.55
longest doc. 1.36
shortest doc. 2.55
</table>
<tableCaption confidence="0.997797">
Table 3: Compression ratios.
</tableCaption>
<bodyText confidence="0.99974636">
Measuring Miss intrinsically is extremely labo-
rious; especially comparing the effectiveness of
different values for the thresholds Bes and Bsim is
infeasible in practice. Therefore, we decided to
measure Miss extrinsically. We used ad-hoc re-
trieval as the extrinsic evaluation task. The eval-
uation criterion is stated as follows: Using the
fused document of each topic as a query, what is
the average (non-interpolated) precision?
As baseline, we concatenated all documents
of each topic. This would constitute an event
description that does not miss any information
within the topic. This document is then used to
query a collection of 242,996 documents, con-
taining the 69 documents from our test collection.
Since the baseline is simply the concatenation of
all documents within the topic, one can expect
that all documents from that topic receive a high
rank in the set of retrieved documents. This aver-
age precision forms the optimal performance for
that topic. For instance, if a topic contains three
documents, and the ad-hoc retrieval ranks those
documents as 1, 3, and 6, there are three recall
levels: 33.¯3%, 66.¯6%, and 100%. The precision
at these levels is 1/1, 2/3, and 3/6 respectively,
</bodyText>
<equation confidence="0.95339725">
Miss = c
a + c
2 · (1 − 1/nT)
(6)
</equation>
<bodyText confidence="0.999285714285714">
which averages to 0.72.
The next step is to compare the actually fused
documents to the baseline. It is to be expected that
the performance is worse, because the fused docu-
ments do not contain segments which are entailed
by other segments in the topic. For instance, if
the fused document for the aforementioned topic
is used as a query and the original documents of
the topic are ranked as 2, 4, and 9, the average
precision is (1/2 + 2/4 + 3/9)/3 = 0.¯4.
Compared to 0.7¯2 for the baseline, fusion leads
to a decrease of effectiveness of approximately
38.5%. Figure 4, gives the averaged precision for
the different values for θes.
</bodyText>
<subsectionHeader confidence="0.766589">
Entailment Threshold
</subsectionHeader>
<figureCaption confidence="0.989916">
Figure 4: Average precision for ad-hoc retrieval.
</figureCaption>
<bodyText confidence="0.99994455">
It is not obvious how to interpret the numerical
value of the ad-hoc retrieval precision in terms of
Miss, but the degree of deviation from the base-
line gives a rough estimate of the completeness
of a fused document. At least, this allows for an
ordinally scaled ranking of the different methods
(in our case different values for θes), that are used
for generating the fused documents. Figure 4 il-
lustrates that in the context of the ad-hoc retrieval
evaluation an optimal entailment threshold (θes)
lies around 0.2. Table 4 shows the decrease in re-
trieval effectiveness in percent, compared to the
baseline. The average precision at 0.2 is 0.8614,
which is just ≈ 11.5% below the baseline.
For all ad-hoc retrieval experiments, the Lnu.ltu
weighting scheme, see (Singhal et al., 1996), has
been used, which is one of the best-performing
weighting schemes in ad-hoc retrieval. In addi-
tion to the 69 documents from our collection, the
retrieval collection contains articles from Associ-
</bodyText>
<table confidence="0.999863625">
θes Decrease in θes Decrease in
precision precision
0.0 20.9% 0.6 23.8%
0.1 14.6% 0.7 23.8%
0.2 11.5% 0.8 23.8%
0.3 13.7% 0.9 23.8%
0.4 22.0% 1.0 24.4%
0.5 22.2%
</table>
<tableCaption confidence="0.999171">
Table 4: Differences to baseline retrieval.
</tableCaption>
<bodyText confidence="0.999644461538462">
ated Press 1988–1990 (from the TREC distribu-
tion), which also belong to the newswire or news-
paper domain. Any meta information such as the
name of the journalist or news agency is removed
to avoid matches based on that information.
In the context of multi-document summariza-
tion, (Stein et al., 2000) use topic clustering for
extrinsic evaluation. Although we did not carry
out any evaluation based on topic clustering, it
seems that it could also be applied to multi-
document fusion, given the close relationship be-
tween fusion and summarization on the one hand
and retrieval and clustering on the other hand.
</bodyText>
<sectionHeader confidence="0.99974" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999902909090909">
The document fusion system described is just pro-
totype and there is much more space for improve-
ment. Although detecting redundancies by using
a shallow notion of entailment works reasonably
well, it is still far from perfect.
In the current implementation, text analysis is
very shallow. Pattern matching is used to avoid
dangling anaphora and lemmatization is used to
make the entailment and similarity scores unsus-
ceptible to morphological variations such as num-
ber and tense. A question for future research is to
what extent shallow parsing techniques can im-
prove the entailment scores. In particular, does
considering the relational structure of a sentence
improve computing entailment relations? This
has shown to be successful in inference-based ap-
proaches to question-answering, see (Harabagiu
et al., 2000), and document fusion might also ben-
efit from representations that are a bit deeper than
the one discussed in this paper.
Another open issue at this point is the need for
standards for evaluating the quality of document
</bodyText>
<figure confidence="0.997565555555556">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Avgerage Precision
0.9
0.8
0.7
0.6
1
System
Baseline
</figure>
<bodyText confidence="0.998417625">
fusion. We think that this can be done by using
standard IR measures like Miss and False Alarm.
Although Miss can be approximated extrinsically,
it is unclear whether this also possible for False
Alarm. Obviously, intrinsic evaluation is more re-
liable, but it remains an extremely laborious pro-
cess, where inter-judge disagreement is still an is-
sue, see (Radev et al., 2000).
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999914">
The author would like to thank Maarten de Rijke
for providing the entailment judgments. This
work was supported by the Physical Sci-
ences Council with financial support from the
Netherlands Organization for Scientific Research
(NWO), project 612-13-001.
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952120481928">
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic detection and tracking pilot
study final report. In Proceedings of the Broadcast
News Transcription and Understranding Workshop
(Sponsored by DARPA).
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proceedings of the 37th Annual
Meeting of the Association of Computational Lin-
guistics (ACL’99).
R. Brandow, K. Mitze, and L. Rau. 1995. Automatic
condensation of electronic publications by sentence
selection. Information Processing &amp; Management,
31(5):675–685.
J. Carbonell, D. Harman, E. Hovy, S. Maiorano,
J. Prange, and Sparck-Jones. K. 2000. Vision
statement to guide research in question answering
(Q&amp;A) and text summarization. NIST Draft Publi-
cation.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Ex-
periments with open-domain textual question an-
swering. In Proceedings of COLING-2000.
M. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
M. Kameyama. 1997. Recognizing referential
links: An information extraction perspective. In
R. Mitkov and B. Boguraev, editors, Proceedings
ofACL/EACL-97 Workshop on Operational Factors
in Practical, Robust Anaphora Resolution for Unre-
stricted Texts, pages 46–53.
C. Kennedy and B. Boguraev. 1996. Anaphora for
everyone: Pronominal anaphora resolution without
a parser. In Proceedings of the 16th International
Conference on Computational Linguistics (COL-
ING’96). Association for Computational Linguis-
tics.
I. Mani and E. Bloedorn. 1999. Summarizing sim-
ilarities and differences among related documents.
Information Retrieval, 1(1–2):35–67.
I. Mani, D. House, G. Klein, L. Hirschman, L. Obrst,
T. Firmin, M. Chrzanowski, and B. Sundheim.
1998. The Tipster SUMMAC text summariza-
tion evaluation, final report. Technical Report
98W0000138, Mitre.
K. McKeown and D. Radev. 1995. Generating sum-
maries of multiple news articles. In Proceedings of
the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 74–82.
C. Monz and M. de Rijke. 2001. Light-weight sub-
sumption checking for computational semantics. In
P. Blackburn and M. Kohlhase, editors, Proceedings
of the 3rd Workshop on Inference in Computational
Semantics (ICoS-3).
D. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: Clustering, sentence extraction, and evalu-
ation. In Proceedings of the ANLP/NAACL-2000
Workshop on Summarization.
D. Radev. 2000. A common theory of information
fusion from multiple text sources, step one: Cross-
document structure. In In Proceedings of the 1st
ACL SIGDIAL Workshop on Discourse and Dia-
logue.
S. Robertson and D. Hull. 2001. The TREC-9 fil-
tering track final report. In Proceedings of The 9th
Text Retrieval Conference (TREC-9). NIST Special
Publication.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing &amp; Management, 24(5):513–523.
A. Singhal, G. Salton, M. Mitra, and C. Buckley.
1996. Document length normalization. Informa-
tion Processing &amp; Management, 32(5):619–633.
G. Stein, G. Wise, T. Strzalkowski, and A. Bagga.
2000. Evaluating summaries for multiple docu-
ments in an interactive environment. In Proceed-
ings of the Second International Conference on
Language Resources and Evaluation (LREC’00),
pages 1651–1657.
J. Zobel and A. Moffat. 1998. Exploring the similarity
space. ACM SIGIR Forum, 32(1):18–34.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.673736">
<title confidence="0.997833">Document Fusion for Comprehensive Event Description</title>
<author confidence="0.768035">Christof</author>
<affiliation confidence="0.98814">Institute for Logic, Language and University of</affiliation>
<address confidence="0.893028">1018 TV Amsterdam, The</address>
<abstract confidence="0.998215307692308">This paper describes a fully implemented system for fusing related news stories into a single comprehensive description of an event. The basic components and the underlying algorithm are explained. The system uses a computationally feasible and robust notion of entailment for comparing information stemming from different documents. We discuss the issue of evaluating document fusion and provide some preliminary results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Carbonell</author>
<author>G Doddington</author>
<author>J Yamron</author>
<author>Y Yang</author>
</authors>
<title>Topic detection and tracking pilot study final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the Broadcast News Transcription and Understranding Workshop (Sponsored by DARPA).</booktitle>
<contexts>
<context position="17239" citStr="Allan et al., 1998" startWordPosition="2901" endWordPosition="2904">ion is ad-hoc retrieval, where the relevance of a retrieved document is assessed by a human judge based on the document’s summary. Then, those judgments are compared to judgments based on original documents, see, e.g., (Brandow et al., 1995; Mani and Bloedorn, 1999). At this stage we have just carried out some preliminary evaluation. The test collection consists of 69 news stories categorized into 21 topics. Categorization was done by hand, but it is also possible to have information filtering, see (Robertson and Hull, 2001), or topic detection and tracking (TDT) tools carrying out this task (Allan et al., 1998). All documents belonging to the same topic were released on the same day and describe the same event. Table 1 provides further details on The average F-scores are given in Figure 3. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 F−Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 Human Judgm. &gt; 0 Human Judgm. &gt; 1 the collection. avg. per topic no. of docs. 3.3 docs. length of a doc. 612 words length of all docs. together 2115 words length of longest doc. 783 words length of shortest doc. 444 words Table 1: Test collection (21 topics, 69 documents). In addition to the aforementioned news agencies, the co</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. 1998. Topic detection and tracking pilot study final report. In Proceedings of the Broadcast News Transcription and Understranding Workshop (Sponsored by DARPA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multi-document summarization.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL’99).</booktitle>
<contexts>
<context position="2685" citStr="Barzilay et al., 1999" startWordPosition="410" endWordPosition="413">art of their work (Carbonell et al., 2000). Obviously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents. The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reade</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. McKeown, and M. Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics (ACL’99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<journal>Information Processing &amp; Management,</journal>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="16860" citStr="Brandow et al., 1995" startWordPosition="2837" endWordPosition="2840">on, see (Mani et al., 1998). Intrinsic evaluation judges the quality directly based on analysis of the summary. Usually, a human judge assesses the quality of a summary based on some standardized evaluation criteria. In extrinsic evaluation, the usefulness of a summary is judged based on how it affects the completion of some other task. A typical task used for extrinsic evaluation is ad-hoc retrieval, where the relevance of a retrieved document is assessed by a human judge based on the document’s summary. Then, those judgments are compared to judgments based on original documents, see, e.g., (Brandow et al., 1995; Mani and Bloedorn, 1999). At this stage we have just carried out some preliminary evaluation. The test collection consists of 69 news stories categorized into 21 topics. Categorization was done by hand, but it is also possible to have information filtering, see (Robertson and Hull, 2001), or topic detection and tracking (TDT) tools carrying out this task (Allan et al., 1998). All documents belonging to the same topic were released on the same day and describe the same event. Table 1 provides further details on The average F-scores are given in Figure 3. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 </context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>R. Brandow, K. Mitze, and L. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing &amp; Management, 31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K</author>
</authors>
<title>Vision statement to guide research in question answering (Q&amp;A) and text summarization.</title>
<date>2000</date>
<publisher>NIST Draft Publication.</publisher>
<marker>K, 2000</marker>
<rawString>J. Carbonell, D. Harman, E. Hovy, S. Maiorano, J. Prange, and Sparck-Jones. K. 2000. Vision statement to guide research in question answering (Q&amp;A) and text summarization. NIST Draft Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>S Maiorano</author>
</authors>
<title>Experiments with open-domain textual question answering.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000.</booktitle>
<contexts>
<context position="24281" citStr="Harabagiu et al., 2000" startWordPosition="4125" endWordPosition="4128">ell, it is still far from perfect. In the current implementation, text analysis is very shallow. Pattern matching is used to avoid dangling anaphora and lemmatization is used to make the entailment and similarity scores unsusceptible to morphological variations such as number and tense. A question for future research is to what extent shallow parsing techniques can improve the entailment scores. In particular, does considering the relational structure of a sentence improve computing entailment relations? This has shown to be successful in inference-based approaches to question-answering, see (Harabagiu et al., 2000), and document fusion might also benefit from representations that are a bit deeper than the one discussed in this paper. Another open issue at this point is the need for standards for evaluating the quality of document 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Avgerage Precision 0.9 0.8 0.7 0.6 1 System Baseline fusion. We think that this can be done by using standard IR measures like Miss and False Alarm. Although Miss can be approximated extrinsically, it is unclear whether this also possible for False Alarm. Obviously, intrinsic evaluation is more reliable, but it remains an extremely labori</context>
</contexts>
<marker>Harabagiu, Pasca, Maiorano, 2000</marker>
<rawString>S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Experiments with open-domain textual question answering. In Proceedings of COLING-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="6040" citStr="Hearst, 1997" startWordPosition="954" endWordPosition="955">ext and placed in a new context (the fused document), this can lead to dangling pronouns, which cannot be correctly resolved anymore. In general, this problem does not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev,</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>M. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>Recognizing referential links: An information extraction perspective.</title>
<date>1997</date>
<booktitle>Proceedings ofACL/EACL-97 Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts,</booktitle>
<pages>46--53</pages>
<editor>In R. Mitkov and B. Boguraev, editors,</editor>
<contexts>
<context position="6204" citStr="Kameyama, 1997" startWordPosition="981" endWordPosition="982">s not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev, 2000) provides no formal definition for any of the relations. Computing the informativity of a segment compared to another segment is an essential task during docu</context>
</contexts>
<marker>Kameyama, 1997</marker>
<rawString>M. Kameyama. 1997. Recognizing referential links: An information extraction perspective. In R. Mitkov and B. Boguraev, editors, Proceedings ofACL/EACL-97 Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, pages 46–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kennedy</author>
<author>B Boguraev</author>
</authors>
<title>Anaphora for everyone: Pronominal anaphora resolution without a parser.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6187" citStr="Kennedy and Boguraev, 1996" startWordPosition="976" endWordPosition="980">In general, this problem does not only hold for pronouns but for all kind of anaphoric expressions such as pronouns, definite noun phrases (e.g., the negotiations) and anaphoric adverbials (e.g., later). To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev, 2000) provides no formal definition for any of the relations. Computing the informativity of a segment compared to another segment is an essential</context>
</contexts>
<marker>Kennedy, Boguraev, 1996</marker>
<rawString>C. Kennedy and B. Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. In Proceedings of the 16th International Conference on Computational Linguistics (COLING’96). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Summarizing similarities and differences among related documents. Information Retrieval,</title>
<date>1999</date>
<pages>1--1</pages>
<contexts>
<context position="2710" citStr="Mani and Bloedorn, 1999" startWordPosition="414" endWordPosition="417">onell et al., 2000). Obviously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents. The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reader to get a more comprehen</context>
<context position="16886" citStr="Mani and Bloedorn, 1999" startWordPosition="2841" endWordPosition="2844">1998). Intrinsic evaluation judges the quality directly based on analysis of the summary. Usually, a human judge assesses the quality of a summary based on some standardized evaluation criteria. In extrinsic evaluation, the usefulness of a summary is judged based on how it affects the completion of some other task. A typical task used for extrinsic evaluation is ad-hoc retrieval, where the relevance of a retrieved document is assessed by a human judge based on the document’s summary. Then, those judgments are compared to judgments based on original documents, see, e.g., (Brandow et al., 1995; Mani and Bloedorn, 1999). At this stage we have just carried out some preliminary evaluation. The test collection consists of 69 news stories categorized into 21 topics. Categorization was done by hand, but it is also possible to have information filtering, see (Robertson and Hull, 2001), or topic detection and tracking (TDT) tools carrying out this task (Allan et al., 1998). All documents belonging to the same topic were released on the same day and describe the same event. Table 1 provides further details on The average F-scores are given in Figure 3. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 F−Score 0.9 0.8 0.7 0.6 </context>
</contexts>
<marker>Mani, Bloedorn, 1999</marker>
<rawString>I. Mani and E. Bloedorn. 1999. Summarizing similarities and differences among related documents. Information Retrieval, 1(1–2):35–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirschman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<title>The Tipster SUMMAC text summarization evaluation, final report.</title>
<date>1998</date>
<tech>Technical Report 98W0000138, Mitre.</tech>
<contexts>
<context position="16267" citStr="Mani et al., 1998" startWordPosition="2739" endWordPosition="2742"> summarization, there are no standard document collections or clear evaluation criteria aiding to automatize the process of evaluation. One approach could be to focus on news stories which mention their sources. For instance CNN’s new stories often say that “AP and Reuters contributed to this story”. On the other hand one has to be cautious to take those news stories as gold standard as the respective contributions of the journalist and his or her sources are not made explicit. In the area of multi-document summarization, there is a distinction between intrinsic and extrinsic evaluation, see (Mani et al., 1998). Intrinsic evaluation judges the quality directly based on analysis of the summary. Usually, a human judge assesses the quality of a summary based on some standardized evaluation criteria. In extrinsic evaluation, the usefulness of a summary is judged based on how it affects the completion of some other task. A typical task used for extrinsic evaluation is ad-hoc retrieval, where the relevance of a retrieved document is assessed by a human judge based on the document’s summary. Then, those judgments are compared to judgments based on original documents, see, e.g., (Brandow et al., 1995; Mani </context>
</contexts>
<marker>Mani, House, Klein, Hirschman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>I. Mani, D. House, G. Klein, L. Hirschman, L. Obrst, T. Firmin, M. Chrzanowski, and B. Sundheim. 1998. The Tipster SUMMAC text summarization evaluation, final report. Technical Report 98W0000138, Mitre.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>D Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="2735" citStr="McKeown and Radev, 1995" startWordPosition="418" endWordPosition="421">ously, if done manually, this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents. The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reader to get a more comprehensive description of an ev</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>K. McKeown and D. Radev. 1995. Generating summaries of multiple news articles. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
<author>M de Rijke</author>
</authors>
<title>Light-weight subsumption checking for computational semantics.</title>
<date>2001</date>
<booktitle>Proceedings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3).</booktitle>
<editor>In P. Blackburn and M. Kohlhase, editors,</editor>
<marker>Monz, de Rijke, 2001</marker>
<rawString>C. Monz and M. de Rijke. 2001. Light-weight subsumption checking for computational semantics. In P. Blackburn and M. Kohlhase, editors, Proceedings of the 3rd Workshop on Inference in Computational Semantics (ICoS-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>H Jing</author>
<author>M Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: Clustering, sentence extraction, and evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL-2000 Workshop on Summarization.</booktitle>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>D. Radev, H. Jing, and M. Budzikowska. 2000. Centroid-based summarization of multiple documents: Clustering, sentence extraction, and evaluation. In Proceedings of the ANLP/NAACL-2000 Workshop on Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
</authors>
<title>A common theory of information fusion from multiple text sources, step one: Crossdocument structure. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st ACL SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2749" citStr="Radev, 2000" startWordPosition="422" endWordPosition="423">this process can be rather laborious as it involves numerous comparisons, depending on the number and length of the documents. The aim of this paper is to describe an approach automatizing this process by fusing information stemming from different documents to generate a single comprehensive document, containing the information of all original documents without repeating information which is conveyed by two or more documents. The work described in this paper is closely related to the area of multi-document summarization (Barzilay et al., 1999; Mani and Bloedorn, 1999; McKeown and Radev, 1995; Radev, 2000), where related documents are analyzed to use frequently occurring segments for identifying relevant information that has to be included in the summary. Our work differs from the work on multi-document summarization as we focus on document fusion disregarding summarization. On the contrary, we are not aiming for the shortest description containing the most relevant information, but for the shortest description containing all information. For instance, even historic background information is included, as long as it allows the reader to get a more comprehensive description of an event. Although </context>
<context position="6376" citStr="Radev, 2000" startWordPosition="1007" endWordPosition="1008"> To cope with this problem simple segmentation is applied as a pre-processing step where paragraphs that contain pronouns or simple definite noun phrases are attached to the preceding paragraph. A more sophisticated approach to text segmentation is described in (Hearst, 1997). Obviously, it would be better to use an automatic anaphora resolution component to cope with this problem, see, e.g., (Kennedy and Boguraev, 1996; Kameyama, 1997), where anaphoric expressions are replaced by their antecedents, but at the moment, the integration of such a component remains future work. 2.2 Informativity (Radev, 2000) describes 24 cross-document relations that can hold between their segments, one of which is the subsumption (or entailment) relation. In the context of document fusion, we focus on the entailment relation and how it can be formally defined; unfortunately, (Radev, 2000) provides no formal definition for any of the relations. Computing the informativity of a segment compared to another segment is an essential task during document fusion. Here, we say that the ith segment of document d (si,d) is more informative than the j-th segment of document d&apos; (sj,d&apos;) if si,d entails sj,d&apos;. In theory, this </context>
</contexts>
<marker>Radev, 2000</marker>
<rawString>D. Radev. 2000. A common theory of information fusion from multiple text sources, step one: Crossdocument structure. In In Proceedings of the 1st ACL SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>D Hull</author>
</authors>
<title>The TREC-9 filtering track final report.</title>
<date>2001</date>
<booktitle>In Proceedings of The 9th Text Retrieval Conference (TREC-9). NIST Special Publication.</booktitle>
<contexts>
<context position="17150" citStr="Robertson and Hull, 2001" startWordPosition="2886" endWordPosition="2889"> on how it affects the completion of some other task. A typical task used for extrinsic evaluation is ad-hoc retrieval, where the relevance of a retrieved document is assessed by a human judge based on the document’s summary. Then, those judgments are compared to judgments based on original documents, see, e.g., (Brandow et al., 1995; Mani and Bloedorn, 1999). At this stage we have just carried out some preliminary evaluation. The test collection consists of 69 news stories categorized into 21 topics. Categorization was done by hand, but it is also possible to have information filtering, see (Robertson and Hull, 2001), or topic detection and tracking (TDT) tools carrying out this task (Allan et al., 1998). All documents belonging to the same topic were released on the same day and describe the same event. Table 1 provides further details on The average F-scores are given in Figure 3. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 F−Score 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 Human Judgm. &gt; 0 Human Judgm. &gt; 1 the collection. avg. per topic no. of docs. 3.3 docs. length of a doc. 612 words length of all docs. together 2115 words length of longest doc. 783 words length of shortest doc. 444 words Table 1: Test coll</context>
</contexts>
<marker>Robertson, Hull, 2001</marker>
<rawString>S. Robertson and D. Hull. 2001. The TREC-9 filtering track final report. In Proceedings of The 9th Text Retrieval Conference (TREC-9). NIST Special Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="11218" citStr="Salton and Buckley, 1988" startWordPosition="1846" endWordPosition="1849">ts to determine Bes. Step 5. is kind of complementary to step 4., where related but more informative segments are identified. Step 5. identifies segments that add new information to dF, where a segment sj,d&apos; is new if it has low similarity to all segments in dF, i.e., if the the similarity score is below the threshold Bsim. If a segment sj,d&apos; is new, it is placed right after the segment in dF to which it is most similar. Similarity is implemented as the traditional cosine similarity in information retrieval, as defined in (3). This similarity measure is also known as the tfc.tfc measure, see (Salton and Buckley, 1988). wk,si,d · wk,sj,d&apos; (3) /// wsi,d · wsj, d&apos; VtkEsi,d tkEsj,d&apos; Where wk,si,d is the weight associated with the term tk in segment si,d. In the nominator of (3), E tk∈sj,d&apos; (1) idf k sim(Si,d, Sj,d&apos;) = E tkEsi,dnsj,d&apos; 1. segmentize all documents in T 2. for all si,d, sj,d0 s.t. d, d&apos; E T and d =� d&apos;: compute es(si,d, sj,d0) 3. select a document d E T as fusion base document: dF 4. for all si,dF: find sj,d0 s.t. dF =� d&apos; and sj,d0 = arg max : es(sk,d0, si,dF ) &gt; es(si,dF , sk,d0) sk d0 if es(sj,d0, si,dF) &gt; Bes then replace si,dF by sj,d0 in the fused document 5. for all sj,d0 s.t. sj,d0 E� dF: </context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>G Salton</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<date>1996</date>
<booktitle>Document length normalization. Information Processing &amp; Management,</booktitle>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="22438" citStr="Singhal et al., 1996" startWordPosition="3829" endWordPosition="3832">gh estimate of the completeness of a fused document. At least, this allows for an ordinally scaled ranking of the different methods (in our case different values for θes), that are used for generating the fused documents. Figure 4 illustrates that in the context of the ad-hoc retrieval evaluation an optimal entailment threshold (θes) lies around 0.2. Table 4 shows the decrease in retrieval effectiveness in percent, compared to the baseline. The average precision at 0.2 is 0.8614, which is just ≈ 11.5% below the baseline. For all ad-hoc retrieval experiments, the Lnu.ltu weighting scheme, see (Singhal et al., 1996), has been used, which is one of the best-performing weighting schemes in ad-hoc retrieval. In addition to the 69 documents from our collection, the retrieval collection contains articles from Associθes Decrease in θes Decrease in precision precision 0.0 20.9% 0.6 23.8% 0.1 14.6% 0.7 23.8% 0.2 11.5% 0.8 23.8% 0.3 13.7% 0.9 23.8% 0.4 22.0% 1.0 24.4% 0.5 22.2% Table 4: Differences to baseline retrieval. ated Press 1988–1990 (from the TREC distribution), which also belong to the newswire or newspaper domain. Any meta information such as the name of the journalist or news agency is removed to avoi</context>
</contexts>
<marker>Singhal, Salton, Mitra, Buckley, 1996</marker>
<rawString>A. Singhal, G. Salton, M. Mitra, and C. Buckley. 1996. Document length normalization. Information Processing &amp; Management, 32(5):619–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Stein</author>
<author>G Wise</author>
<author>T Strzalkowski</author>
<author>A Bagga</author>
</authors>
<title>Evaluating summaries for multiple documents in an interactive environment.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00),</booktitle>
<pages>1651--1657</pages>
<contexts>
<context position="23143" citStr="Stein et al., 2000" startWordPosition="3947" endWordPosition="3950">eval. In addition to the 69 documents from our collection, the retrieval collection contains articles from Associθes Decrease in θes Decrease in precision precision 0.0 20.9% 0.6 23.8% 0.1 14.6% 0.7 23.8% 0.2 11.5% 0.8 23.8% 0.3 13.7% 0.9 23.8% 0.4 22.0% 1.0 24.4% 0.5 22.2% Table 4: Differences to baseline retrieval. ated Press 1988–1990 (from the TREC distribution), which also belong to the newswire or newspaper domain. Any meta information such as the name of the journalist or news agency is removed to avoid matches based on that information. In the context of multi-document summarization, (Stein et al., 2000) use topic clustering for extrinsic evaluation. Although we did not carry out any evaluation based on topic clustering, it seems that it could also be applied to multidocument fusion, given the close relationship between fusion and summarization on the one hand and retrieval and clustering on the other hand. 4 Conclusions The document fusion system described is just prototype and there is much more space for improvement. Although detecting redundancies by using a shallow notion of entailment works reasonably well, it is still far from perfect. In the current implementation, text analysis is ve</context>
</contexts>
<marker>Stein, Wise, Strzalkowski, Bagga, 2000</marker>
<rawString>G. Stein, G. Wise, T. Strzalkowski, and A. Bagga. 2000. Evaluating summaries for multiple documents in an interactive environment. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC’00), pages 1651–1657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zobel</author>
<author>A Moffat</author>
</authors>
<title>Exploring the similarity space.</title>
<date>1998</date>
<journal>ACM SIGIR Forum,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="12353" citStr="Zobel and Moffat, 1998" startWordPosition="2061" endWordPosition="2064">es then replace si,dF by sj,d0 in the fused document 5. for all sj,d0 s.t. sj,d0 E� dF: if for all si,dF: sim(si,dF , sj,d0) &lt; Bsim, then find the most similar si,dF: si,dF = arg max : sim(sj,d0, sk,dF) skIdF and place sj,d0 between si,dF and si+1,dF Figure 1: Sketch of the document fusion algorithm. the weights of the terms that occur in si,d and sj,d0 are summed up. The denominator is used for normalization. Otherwise, longer documents tend to result in a higher similarity score. In the current implementation wk,szId = idf k for all si,d. The reader is referred to (Salton and Buckley, 1988; Zobel and Moffat, 1998) for a broad spectrum of similarity measures for information retrieval. 3 Evaluation Issues The document fusion system is evaluated in two steps. First, the effectiveness of entailment detection is evaluated, which is the key component of our system. Then we present some preliminary evaluation of the whole system focusing on the quality of the fused documents. 3.1 Evaluating Entailment Recently, we have started to build a small test collection for evaluating entailment relations. The reader is referred to (Monz and de Rijke, 2001) for more details on the results presented in this subsection. F</context>
</contexts>
<marker>Zobel, Moffat, 1998</marker>
<rawString>J. Zobel and A. Moffat. 1998. Exploring the similarity space. ACM SIGIR Forum, 32(1):18–34.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>