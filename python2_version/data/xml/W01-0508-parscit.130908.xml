<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001270">
<title confidence="0.990832">
Using Bins
to Empirically Estimate Term Weights
for Text Categorization
</title>
<author confidence="0.904016">
Carl Sable
</author>
<affiliation confidence="0.8135515">
450 Computer Science Building
Columbia University
</affiliation>
<address confidence="0.987379">
New York, NY 10027
</address>
<email confidence="0.854551">
sableAcs.columbia.edu
</email>
<note confidence="0.874089">
Kenneth W. Church
AT&amp;T Shannon Laboratory
</note>
<address confidence="0.8820595">
180 Park Avenue
Florham Park, NJ 07932
</address>
<email confidence="0.876879">
kwcAresearch.att.com
</email>
<sectionHeader confidence="0.975372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937">
This paper introduces a term weighting method for
text categorization based on smoothing ideas bor-
rowed from speech recognition. Empirical estimates
of weights (likelihood ratios) become unstable when
counts are small. Instead of estimating weights for
individual words, as Naive Bayes does, words with
similar features are grouped into bins, and a single
weight is estimated for each bin. This weight is then
assigned to all of the words in the bin. The bin-based
method is intended for tasks where there is insuffi-
cient training data to estimate a separate weight for
each word. Experiments show the bin-based method
is highly competitive with other current methods.
In particular, this method is most similar to Naive
Bayes; it generally performs at least as well as Naive
Bayes, and sometimes better.
</bodyText>
<sectionHeader confidence="0.926301" genericHeader="categories and subject descriptors">
1 Introduction and Related Work
</sectionHeader>
<bodyText confidence="0.999856535211268">
In recent years there has been considerable interest
in text categorization techniques which assign doc-
uments to one or more categories. Most of these
methods assume a supervised training setup, where
the system is given some labeled training data: e.g.,
pairs of documents and category assignments. Doc-
uments, and sometimes categories, are often repre-
sented as weighted word vectors. Word weights are
usually computed by combining separate features in
some fashion, for example, by multiplying together
the term frequency (TF) and inverse document fre-
quency (IDF ) of each word (Salton and Buckley,
1988; Salton, 1989).
The Naive Bayes method of text categorization
(Lewis, 1998) empirically estimates term weights
for each individual word that appears in the train-
ing set based on estimated probabilities of seeing
each word in a document of each possible category.
This method is prone to inaccurate term weights for
words that occur infrequently in the training set.
Words that have never been seen in the training set
are ignored since all of the estimated probabilities
are zero, and words that appear in only one category
in the training set might appear to give that category
infinite likelihood if they appear in a document from
the test set. We will show that empirically estimat-
ing term weights for bins instead of individual words
avoids these pitfalls, while at the same time provid-
ing evidence indicating which features of words are
most important for indicating categories.
The Speech Recognition literature has developed
a number of methods for smoothing term frequencies
(e.g., Chapter 15 of (Jelinek, 1998)). These meth-
ods are important when the raw counts are small,
and particularly important when the counts are zero.
Both the Good-Turing method and the Deleted In-
terpolation method estimate r*, an adjusted value
of r, where r is the number of times that the term
t appeared in one corpus, and r* is the number of
times that t is expected to appear in another corpus
of similar size and material.
The Deleted Interpolation method assigns each
term, t, to a bin, h, usually based on the frequency
r, but binning rules can also make use of other vari-
ables. Section 15.6 of (Jelinek, 1998), for example,
discusses so-called &amp;quot;enhanced&amp;quot; binning rules where
bigrams are assigned to bins not only based on their
joint frequencies but also the frequencies of their
parts. In this work, terms will be assigned to bins
based on IDE and other statistics that are commonly
used in the text categorization literature.
The Deleted Interpolation method splits the train-
ing collection into two pieces. The first piece is used
to assign terms to bins, and to compute the num-
ber of terms that have been assigned to each bin,
Nb. The second piece is used for calibrating bins.
G is the number of times that the terms in bin h
are found in the second piece. The final answer is
then r* Cb/Nb. In general, r* tends to be slightly
smaller than r in most cases except when r = 0.
The adjustments are important when r is small, es-
pecially when r = 0. All of the terms in a bin receive
the same adjusted frequency, r*.
(Umemura and Church, 2000) shows how the
Deleted Interpolation approach can be generalized
to estimate likelihood ratios instead of frequencies in
an information retrieval application. In this paper,
we will use a similar approach for text categoriza-
tion. Text categorization is interestingly different
from information retrieval because there tends to be
relatively more supervised training material.
</bodyText>
<sectionHeader confidence="0.926506" genericHeader="method">
2 Data Sets
</sectionHeader>
<bodyText confidence="0.968918158730159">
Our research has focused on the categorization of
news documents and their embedded images, and
we have experimented with categories representing
various levels of abstraction. The first experiment
discussed in this paper involves the categorization of
images based on associated captions as either Indoor
or Outdoor; the second considers the categorization
of entire news documents into the categories Strug-
gle, Politics, Disaster, Crime, or Other. For these
experiments, we compare our system to two com-
peting systems previously implemented at Columbia
University (Sable and Hatzivassiloglou, 2000; Sable,
2000) as well as several systems which comprise
the publicly available Rainbow package (McCallum,
1996). In Section 6, we will discuss a third exper-
iment involving Reuters documents and topic cate-
gories, for which we compare our system to all sys-
tems tested by Yang and Liu in (Yang and Liu,
1999).
The first two experiments use data sets from a cor-
pus previously collected by researchers at Columbia
University (Sable and Hatzivassiloglou, 2000; Sable,
2000). The raw data consists of news postings from
November 1995 through January 1999 from a vari-
ety of Usenet newsgroups. Using one web interface,
1,675 captioned images were each labeled by two hu-
mans as Indoor, Likely Indoor, Ambiguous, Likely
Outdoor, and Outdoor. 1,339 images were assigned
definite labels in the same direction by both humans,
and these images comprise the data set used in our
first experiment. 894 images are used for training
and 445 are used for testing. We use the same break-
down as (Sable, 2000) to allow for direct compari-
son. Using a second web interface, 1,750 news docu-
ments, each consisting of an article, image, and cap-
tion, were each labeled by two humans into one of
the mutually exclusive categories Struggle, Politics,
Disaster, Crime, or Other. We will refer to these as
our Events categories in the remainder of this pa-
per. 1,328 documents were assigned identical labels
by both humans, and these documents comprise the
data set used in our second experiment. 885 doc-
uments are used for training and 443 are used for
testing, again using the same breakdown as (Sable,
2000).
Instructions provided to volunteers who labeled
images as Indoor or Outdoor, including cate-
gory definitions and guidelines, can be viewed
at http://www.cs.columbia.edur sable/research/
readme.html. The corresponding information
provided to volunteers who labeled documents
according to our Events categories can be seen
at http://www.cs.columbia.edur sable/research/
instr.html. Figure 1 and Figure 2 show two sample
images with captions. Most captions include a
first sentence which describes the associated image
and one or two additional sentences which provide
background information about the related story. All
header information, including locations and dates,
is automatically stripped before our experiments.
Articles generally consist of many paragraphs and
are typical in length to what you would expect in a
standard newspaper.
</bodyText>
<sectionHeader confidence="0.889249" genericHeader="method">
3 Bins and Methodology
</sectionHeader>
<bodyText confidence="0.986950684210526">
The general idea behind bins is to group words with
similar features into a common bin. For example,
you will see that in our first two experiments, we
determine category counts, burstiness values, and
quantized IDFs for all words, and then assign each
word to a bin consisting of all words that share the
same values. Once words have been placed into bins,
we can estimate the likelihood of a word from a spe-
cific bin appearing in a document of a specific cate-
gory with a specific occurrence count.
For each of our first two experiments, we create a
separate set of bins for each category. For each cat-
egory, every word is placed into a single bin based
on the three features of the word. The first binning
feature is the word&apos;s quantized IDF, or inverse doc-
ument frequency. The IDF of a word represents a
measure of the rarity of a word, and is calculated
according to the formula:
Total number
</bodyText>
<equation confidence="0.363432">
IDF (word) log of documents
DF (word)
</equation>
<bodyText confidence="0.999984">
where a word&apos;s DF, or document frequency, is the
number of documents that contain the word. It is
expected that words with higher IDFs will be more
indicative of categories. Based on some initial ex-
periments within the training set which were per-
formed to determine how to quantize IDFs (i.e. to
determine IDF boundaries which would be used to
separate bins), we decided to simply truncate these
values.
It is customary to compute the IDFs of words
based on the training set, but another possibility is
to compute these values based on some other, larger
set of documents. On the one hand, using the train-
ing set ensures that the documents will have a sim-
ilar style and format to documents that appear in
the test set, but on the other hand, sometimes more
data is better data. We ran some initial experiments
comparing the use of our training set against the use
of a year&apos;s worth of AP news documents consisting
of approximately one million articles. We found that
using the AP news documents leads to a significant
improvement (i.e. more data is better), so we de-
cided to use this larger corpus, instead of our train-
</bodyText>
<table confidence="0.998522333333333">
Intuition Word Indoor Outdoor IDF burstiness
category category
count count
Clearly Indoor conference 15 0 2.5 0
bed 1 0 4.5 0
Clearly Outdoor airplane 0 2 5.4 1
earthquake 0 4 4.6 1
Unclear Gore 1 1 4.5 1
ceremony 5 6 3.9 0
</table>
<tableCaption confidence="0.999896">
Table 1: Values of binning features are used to assign words to bins.
</tableCaption>
<bodyText confidence="0.999893645569621">
ing set, to compute IDFs for the remainder of our
experiments.
Following the suggestion of (Umemura and
Church, 2000), our second binning feature is bursti-
ness, an idea introduced in (Katz, 1996). Burstiness
takes into account that some words are likely to ap-
pear many times in a document if they appear at
all. The burstiness of a word is either one or zero,
depending on whether or not the average term fre-
quency of the word in documents in which it appears
is greater than some specific cutoff. It is expected
that bursty words (words with a burstiness of one)
will be more indicative than words which are not
bursty.
The third binning feature is the number of doc-
uments in the first half of the training set that be-
long to the particular category being examined and
contain the word. We will refer to this as the cate-
gory count of the word in the specified category. All
category counts above some pre-determined maxi-
mum cut-off are set equal to this maximum. It is
expected that words which appear multiple times in
a category in the training set will be more indica-
tive of that category. So, for example, one bin for
our first experiment consists of all bursty words that
have a truncated IDF of 3 and appear in two Out-
door documents in the first half of the training set
(i.e. have an Outdoor category count of 3). Table 1
shows the values of the binning features used in our
first experiment for six arbitrary words, where the
category counts have been determined based on the
first half of the training set.
Once bins have been determined, the second half
of the training set is used to empirically estimate
term weights for all bins. Each bin is assigned multi-
ple term weights corresponding to different possible
occurrence counts, or term frequencies, of words in
documents (this will be elaborated shortly). Given
a bin related to a specific category, we estimate the
probability that a word belonging to the bin will ap-
pear in an article of the same category with some
specific count. The log of this probability is used as
a term weight for the bin.
For the first experiment, only first sentences
of captions are used to determine bins and term
weights or to predict labels. (The first sentence of
a caption generally describes the associated image,
while the rest gives background information about
the related story.) We therefore only measure prob-
abilities for two specific occurrence counts of words
in bins; namely, zero versus one or more. For the
second experiment, full news articles are used, and
it is often the case that a word occurs more than
once. For this experiment, separate term weights
are used for different possible occurrence counts. For
each bin, the separate possible occurrence counts are
zero, one, two, three, or four or more. It is expected
that, other things being equal, the more a word oc-
curs in a document, the more indicative it is of a
category.
The above two steps are best illustrated by exam-
ple. For our first experiment, the word &amp;quot;airplane&amp;quot;
appears in two Outdoor documents (i.e. captions of
Outdoor images) in the first half of the training set,
but no Indoor documents in the first half of the train-
ing set. The IDF of &amp;quot;airplane&amp;quot; is 5.4 and the word
is bursty. Therefore, &amp;quot;airplane&amp;quot; is placed in one bin
representing all bursty words with a truncated IDF
of 5 that appear in two Outdoor documents, and we
will refer to this as the Outdoor bin of &amp;quot;airplane&amp;quot;.
The same word is placed in another bin representing
all bursty words with a truncated IDF of 5 that ap-
pear in zero Indoor documents, and we will refer to
this as the Indoor bin of &amp;quot;airplane&amp;quot;. When the sec-
ond half of the training set is examined, it is noted
that the estimated probability that a word belonging
to the same Indoor bin as &amp;quot;airplane&amp;quot; appears one or
more times in an Indoor document is 2.11 x 10-4,
and so the term weight for this bin is:
</bodyText>
<equation confidence="0.863928">
Aindoor = log2(2.11 x 10-4) = —12.21
</equation>
<bodyText confidence="0.79576">
It is also noted that the estimated probability that
a word belonging to the same Outdoor bin as &amp;quot;air-
plane&amp;quot; appears one or more times in an Outdoor
document is 2.90 x 10-3, and so the term weight for
this bin is:
</bodyText>
<equation confidence="0.525215">
Ao,,tdoor = log2(2.90 x 10-3) = —8.43
</equation>
<table confidence="0.999877142857143">
Intuition Word Aindoor Aoutdoor
Clearly Indoor conference 5.91
bed 4.58
Clearly Outdoor airplane -3.78
earthquake -4.86
Unclear Gore 0.74
ceremony -0.32
</table>
<tableCaption confidence="0.999196">
Table 2: Term weights for these words fit intuition as to which words indicate which category.
</tableCaption>
<bodyText confidence="0.999938734177215">
The difference between the term weight associated
with a word&apos;s Indoor bin and the term weight as-
sociated with a word&apos;s Outdoor bin is a log like-
lihood ratio comparing the likelihood of seeing the
word (or one with the same features) in one category
versus the other. In this case, Aoutdoor Aindoor
—8.43—(-12.21) = 3.78, which means that if a word
with the same features as &amp;quot;airplane&amp;quot; is seen in a doc-
ument, it is about 23-78 = 13.74 more likely to be an
Outdoor document than an Indoor document.
This example should help to illustrate why the
training set is divided into two halves. The first half
of the training set determines bins, and the second
half is used to calibrate bin probabilities. Let&apos;s say
that &amp;quot;airplane&amp;quot; does not occur in any Indoor docu-
ments in the training set. This does not mean that if
the word is seen in a test document, it is necessarily
an Outdoor document. Since &amp;quot;airplane&amp;quot; is placed in
an Indoor bin with other words that occur in no In-
door documents in the first half of the training set,
and some of these words may appear in some Indoor
documents in the second half of the training set, we
are able to estimate a probability for the bin as a
whole of seeing a word from the bin in an Indoor
document.
Table 2 shows the results of subtracting the Out-
door term weights from the Indoor term weights for
the bins of the the same six words shown in Ta-
ble 1. As expected, words such as &amp;quot;conference&amp;quot; and
&amp;quot;bed&amp;quot; are good Indoor indicators, while words such
as &amp;quot;airplane&amp;quot; and &amp;quot;earthquake&amp;quot; are good Outdoor
indicators. The words &amp;quot;Gore&amp;quot; and &amp;quot;ceremony&amp;quot; each
show a slight preference for one category over the
other, but according to term weights, each is less
than twice as likely in the favored category.
Once term weights for all bins have been calcu-
lated, we loop through the test documents to pre-
dict categories. For each test document, we iterate
through its words, summing the appropriate term
weights for each category. For example, for our sec-
ond experiment, we generate five sums, one for each
category. The first is the sum of the term weights
for the Struggle bins associated with all words in
the document (taking into account the specific oc-
currence counts of the words), the second is the
sum of the term weights for the Politics bins, etc.
Whichever category has the highest sum is consid-
ered the most likely and is therefore predicted. The
likelihood of bins are assumed to be independent.
This is similar to the independence assumption of
words in Naive Bayes, and in fact the two methods
are virtually identical if every word is assigned its
own bin.
Figure 3 shows pseudo-code summarizing the al-
gorithm. In actuality, we use a script consisting of
approximately 650 lines of Perl code. All IDFs and
burstiness values have been precomputed and are
stored in pre-existing files. The first part of the
script (corresponding to lines 1 through 16 of the
pseudo-code) loads the IDFs and burstiness values,
uses the first half of the training set to determine cat-
egory counts of words, maps words to bins, and com-
putes the size of each bin. Next (corresponding to
lines 17 through 44 of the pseudo-code), the second
half of the training set is used to estimate probabili-
ties of all possible (bin, occurrence count) pairs, and
corresponding term weights are computed. Some
extra code is needed to compute counts for all in-
stances of occurrence counts of 0 (see the comment
at line 27 of the pseudo-code), and there are also
checks throughout the code to avoid infinities. Fi-
nally (corresponding to lines 46 thorugh 60 of the
pseudo-code), the test set is examined. For every
test document, every word is mapped to its appro-
priate bins (one for each category), and the score
for each category is incremented by the appropriate
term weight. Each test document is assigned to the
category with the highest score. The script generally
completes a full execution in a matter of minutes.
</bodyText>
<sectionHeader confidence="0.997785" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999506444444445">
For the first experiments, by construction, the cate-
gories are mutually exclusive, so the system outputs
one and only one category label for each test doc-
ument. The main evaluation measure is overall ac-
curacy, which is the percentage of such predictions
that are correct. In addition, we also report the F1
measure (van Rijsbergen, 1979) for each category.
The F1 measure combines precision and recall into
a single measurement according to the following for-
</bodyText>
<table confidence="0.999686545454545">
System Overall Indoor Outdoor
Accuracy % F1 % F1 %
Bins 84.9 73.3 89.5
Naive Bayes (Rainbow) 85.4 73.5 89.9
Rocchio/TF*IDF (Rainbow) 84.5 73.2 89.1
K-Nearest Neighbor (Rainbow) 77.8 65.3 83.6
Probabilistic Indexing (Rainbow) 86.3 78.1 90.0
SVMs (Rainbow) 82.0 66.9 87.7
Maximum Entropy (Rainbow) 84.5 70.9 89.4
Rocchio/TF*IDF (Columbia) 80.8 69.9 85.7
Density Estimation (Columbia) 86.1 73.7 90.5
</table>
<tableCaption confidence="0.9782005">
Table 3: Our bin-based system outperforms five out of eight alternatives for Indoor versus Outdoor catego-
rization.
</tableCaption>
<table confidence="0.999896727272727">
S ystem Overall Struggle Politics Disaster Crime Other
Accuracy % Fi % F1 % F1 % Fi % Fi %
Bins 88.5 87.5 88.0 97.2 89.6 58.5
Naive Bayes (Rainbow) 87.6 86.2 86.3 96.7 89.1 61.5
Rocchio/TF*IDF (Rainbow) 87.4 81.1 85.3 97.7 88.4 68.3
K-Nearest Neighbor (Rainbow) 81.9 80.0 79.7 95.6 75.6 63.2
Probabilistic Indexing (Rainbow) 86.5 83.6 84.8 97.2 89.4 65.0
SVMs (Rainbow) 88.7 88.1 89.2 96.2 87.0 57.9
Maximum Entropy (Rainbow) 88.3 88.1 87.9 95.7 87.9 55.6
Rocchio/TF*IDF (Columbia) 87.1 85.0 88.4 98.8 79.2 60.0
Density Estimation (Columbia) 84.9 83.7 86.0 97.3 80.0 34.3
</table>
<tableCaption confidence="0.999772">
Table 4: Our bin-based system outperforms seven out of eight alternatives for our Events categories.
</tableCaption>
<equation confidence="0.880481">
mula:
2 x Precison x Recall
F1=
Precision ± Recall
</equation>
<bodyText confidence="0.999970583333334">
This formula leads to a value that is closer to the
smaller of precision and recall, and thus requires
good results for both measurements to achieve a high
F1 score. In Section 6, we will consider a third ex-
periment in which the categories are not mutually
exclusive and therefore the system may output mul-
tiple categories for each document. Overall accuracy
is considered a more useful evaluation measure in the
mutually exclusive case, while evaluation measures
based on F1 are considered more useful in the non-
exclusive case (this will be described in more detail
in Section 6).
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999765233333333">
Table 3 shows the results of all systems tested for
our first experiment (involving the categories Indoor
and Outdoor). These include the bin system de-
scribed in this paper, six competing systems which
are part of the publicly available Rainbow package,
and two competing systems previously developed at
Columbia University. All competing systems use
text categorization techniques which have been pre-
viously described in literature. On this set of cat-
egories, our bin system performs somewhere in the
middle, outperforming five of the eight competing
systems tested. For further comparison, a baseline
classifier which picks the larger category every time
(Outdoor) achieves a 71.2% overall accuracy, signif-
icantly lower than all the automatic systems tested.
Humans who were asked to predict labels after being
shown only captions achieved an 87.6% overall accu-
racy, and we consider this a reasonable upper bound
for how well a text categorization system might be
expected to do.
Table 4 shows the performance of all systems
tested for our second experiment (involving our
Events categories). Note that the bin system outper-
forms all but one of the competing systems, based
on overall accuracy, and only marginally loses to
SVMs by 0.2% (which corresponds to one test docu-
ment). A baseline categorizer which picks the largest
category every time (which happens to be Strug-
gle, based on the training set) would achieve only a
30.5% overall accuracy.
</bodyText>
<sectionHeader confidence="0.980305" genericHeader="evaluation">
6 Reuters
</sectionHeader>
<bodyText confidence="0.994316">
We also performed an additional experiment us-
ing the ModApte split of the Reuters-21578
corpus (Lewis, 1997), a common benchmark
for comparing methods of text categorization
</bodyText>
<table confidence="0.999237571428571">
method miR miP miF1 maF1 error
SVM .8120 .9137 .8599 .5251 .00365
kNN .8339 .8807 .8567 .5242 .00385
LSF .8507 .8489 .8498 .5008 .00414
NNet .7842 .8785 .8287 .3765 .00447
NB .7688 .8245 .7956 .3886 .00544
Bins .8053 .7915 .7984 .4561 .00561
</table>
<tableCaption confidence="0.814248333333333">
Table 5: Our bin-based system marginally outperforms Naive Bayes in terms of micro-average F1 (miF1),
and it significantly outperforms both Naive Bayes and Neural Nets in terms of macro-average F1 (maF1).
(Joachims, 1998; Schapire and Singer, 2000;
</tableCaption>
<bodyText confidence="0.999690046296297">
Yang and Liu, 1999). This collection can be
obtained at http://www.research.att.comrlewis/
reuters21578.html. To allow for direct comparison
with Yang and Liu (1999), all categories which did
not contain at least one training document were
eliminated, and then all unlabeled document were
removed. This data set includes 90 categories,
7,770 training documents, and 3,019 test documents.
Reuters-21578 is a binary categorization corpus,
meaning that documents are allowed to have mul-
tiple labels, so a separate YES/NO decision is re-
quired for each (document, category) pair. The av-
erage number of categories assigned to a document
is 1.24, and the most categories assigned to a sin-
gle document is 15. The main purpose of this ex-
periment was to see if our system, using bin-based
smoothing, would outperform the Naive Bayes sys-
tem tested by Yang and Liu. Since Naive Bayes is a
similar method that estimates term weights for in-
dividual words instead of bins, and its performance
was the lowest of the five systems tested by Yang and
Liu (1999), we did not expect our bin-based system
to do as well as the other systems.
The methodology described in Section 3, which
relies on a separate set of bins for each category, is
not appropriate for the Reuters data set. Although
this data set contains significantly more total docu-
ments than those used for our first two experiments,
it includes many small categories which do not have
enough training examples to estimate term weights
for per category bins. Instead, we implemented a
modified methodology using only a single set of bins
based on the entire training corpus. Each word is
placed into a bin solely according to its IDF. The
term weights computed estimate the likelihood that
two documents share the same or similar categories,
given that they share a word from a specific bin.
(Recall that Reuters is a binary categorization cor-
pus, and so it is possible for two documents to share
some but not all of the same categories; However, it
turns out that in most cases, documents share all or
none of the same categories.)
At first, we used the same IDF values for binning
as we did for the first two experiments; namely, those
based on approximately one million AP news doc-
uments. It is expected that the words belonging to
bins representing higher IDFs should be more indica-
tive of shared categories, and that these bins should
therefore have higher term weights. In general, this
trend was seen, but there were some anomalies. A
few bins had significantly lower term weights than
expected. Further examination showed that in ev-
ery such case, the anomaly was caused by one or
two words that, due to stylistic differences between
the two corpora, are much more common in Reuters
than in AP. For example, the abbreviations &amp;quot;mln&amp;quot;
for &amp;quot;million&amp;quot; and &amp;quot;pet&amp;quot; for &amp;quot;percentage&amp;quot; are both
quite common in various Reuters categories, but ex-
tremely uncommon in AP, and therefore are assigned
IDF values which are misleading. We did not want
to switch entirely to IDF values based only on the
Reuters training set, because that is a much smaller
set than the AP corpus, and words which do not
appear in the training set would have to be ignored
or all placed in a single bin. So instead, we decided
to use two IDF values for each word, one based on
the Reuters training set and the other based on the
AP corpus. These represent two different features
of each word. In general, most words are assigned
two similar IDF values, but the anomalies such as
those just mentioned are not, and words like these
are therefore filtered to their own bins, and do not
affect the term weights of other bins.
For this experiment, we use the same metrics that
are used by Yang and Liu (1999) to allow for di-
rect comparison. These measures are micro-average
recall (miR), micro-average precision (miP ), micro-
average F1 (miF1), macro-average F1 (miF1), and
overall error (which is one minus overall accuracy).
The macro-average F1 computes the F1 for all cat-
egories and then averages these numbers together
(thus giving all categories equal weight), while the
micro-average F1 computes the F1 once based on all
binary decisions being made (thus giving all docu-
ments equal weight). These metrics are considered
more important than overall accuracy (or overall er-
ror) for binary categorization, since most documents
have few labels, and a trivial system which predicts
no labels for every document can achieve high overall
accuracy.
Table 5 shows the results of our bin system (bot-
tom row) and all systems tested by Yang and Liu
(1999). The top five rows of the table are a re-
production of Table 1 from (Yang and Liu, 1999),
showing the results of five state-of-the-art systems
using methods which are commonly employed for
text categorization. The five systems represented in
these rows, from top to bottom, are Support Vector
Machines, K-Nearest Neighbors, Least Squares Fit,
Neural Nets, and Naive Bayes, all of which are de-
scribed in (Yang and Liu, 1999). The two most im-
portant metrics shown are the micro-average F1 and
macro-average F1. The bin system described in this
paper has a micro-average F1 which is marginally
higher than the Naive Bayes system tested by Yang
and Liu, while the macro-average F1 is significantly
better than two of the systems tested by Yang and
Liu including Naive Bayes.
</bodyText>
<sectionHeader confidence="0.996262" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981205882353">
This paper describes how to use bins to empirically
estimate term weights for the purpose of text cate-
gorization. Using bins allows our system to estimate
term weights for words which appear infrequently, or
even not at all, in the training set. This smoothing
effect can improve performance by avoiding inaccu-
rate term weights for infrequent words. We have
performed three complete experiments on very dif-
ferent data sets and compared our bin system to
many competing systems using a variety of methods
which have proved successful for text categorization
in the past. Our results show that our bin-based sys-
tem is highly competitive with other systems, and in
particular, that it is usually at least as good as Naive
Bayes.
In the future, we hope to test more binning fea-
tures. Even without conducting full experiments,
examination of term weights can determine which
features are important. We hope to use query ex-
pansion, analogous to the way it was used with bin-
ning to improve information retrieval in (Umemura
and Church, 2000), to boost results further. Also,
we believe that it might be better to compute term
weights for individual words when possible, and
to back off to the bin only when necessary. In
other words, when there is sufficient evidence to
compute term weights for words, we should do so,
but in other cases, we should back off and use
bins. These changes may improve our system, which
is already faring well against competitors. Addi-
tional information about this work can be found at
http://www.cs.columbia.edursable/bins.html, and
we will continue to expand this page as our research
continues.
</bodyText>
<sectionHeader confidence="0.993108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97735125">
F. Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
T. Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many rel-
evant features. In Proceedings of the European
Conference on Machine Learning.
S. Katz. 1996. Distribution of content words and
phrases in text and language modelling.
D. Lewis. 1997. Reuters-21578 text categorization
test collection, readme file (version 1.2).
D. Lewis. 1998. Naive (Bayes) at forty: The inde-
pendence assumption in information retrieval. In
Proceedings of the European Conference on Ma-
chine Learning.
A. McCallum. 1996. Bow: A toolkit for
statistical language modeling, text re-
trieval, classification, and clustering.
http://www.cs.cmu.edurmccallum/bow.
C. Sable and V. Hatzivassiloglou. 2000. Text-
based approaches for non-topical image catego-
rization. International Journal of Digital Li-
braries, 3(3):261-275.
C. Sable. 2000. Categorizing multimedia docu-
ments using associated text (thesis proposal).
http://www.cs.columbia.edur sable/proposal.
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information
Processing and Management, 24(5):513-523.
G. Salton. 1989. Automatic &apos;Text Processing: The
Transformation, Analysis, and Retrieval of Infor-
mation by Computer. Addison-Wesley, Reading,
Massachusetts.
R. Schapire and Y. Singer. 2000. Boostexter: A
boosting-based system for text categorization.
Machine Learning, 39(2):135-168.
K. Umemura and K. W. Church. 2000. Empirical
term weighting and expansion frequency. In Joint
SIGDAT Conference on Empircal Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, London, 2nd edition.
Y. Yang and X. Liu. 1999. A re-examination of
text categorization methods. In Proceedings of the
22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval (SIGIR-99).
PONTA DELGADA, PORTUGAL, 1-NOV-1997: Rescue workers remove the body of a man from
mud, October 31, following a landslide in Ribeira Quente, Azores. Ten people died and dozens
are still missing inside their destroyed houses after a mass of rocks fell on a group of homes. The
disaster may have been caused by heavy rain which has battered the Azores. [Photo by AFP]
</reference>
<figureCaption confidence="0.9788675">
Figure 1: This image was manually labeled as Outdoor, and the associated document was labeled as a
Disaster document.
</figureCaption>
<bodyText confidence="0.889951">
TIRANA, ALBANIA, 29-JUN-97: Albanian President Sali Berisha casts his vote at a central Tirana
polling station on, June 29. Albania holds its general elections three months after the collapsed
pyramid investment schemes drove the country into armed turmoil. [Photo by Petr Josek, Reuters]
</bodyText>
<figureCaption confidence="0.9941525">
Figure 2: This image was manually labeled as Indoor, and the associated document was labeled as a Politics
document.
</figureCaption>
<figure confidence="0.992990983333333">
1 # Determine category counts of words in corpus.
2 for every document in first half of training set {
3 current_categary &lt;-- categary[document];
4 for every word in document {
5 count [word, current_categary] += 1;
6
7}
8 # Determine bins for all known words (not just those in corpus).
9 for every known word {
10 for every category {
11 categary_count &lt;-- count [word, category];
12 current_bin &lt;-- bin(IDF[word], burstiness[word], categary_count);
13 # Increment size of current bin.
14 size[current_bin] += 1;
15
16 1
17 # Determine count of every (bin, occurrence count) pair.
18 for every document in second half of training set {
19 current_categary &lt;-- categary[document];
20 for every word in document {
21 categary_count &lt;-- count [word, current_categary];
22 current_bin &lt;-- bin(IDF[word], burstiness[word], categary_count);
23 accurrence_count &lt;-- term_frequency[word, document];
24 # Increment count for this word&apos;s (current_bin, occurrence count) pair.
25 count [current_bin, accurrence_count] += 1;
26
27 # Fix counts for occurrence counts of 0 (uses bin sizes) here.
28 1
29 # Estimate probability of every (bin, occurrence count) pair.
30 for every bin {
31 total = 0;
32 for every possible accurrence_count (0 to MAX) {
33 total += count [bin, accurrence_count];
34
35 for every possible accurrence_count (0 to MAX) {
36 probability[bin, accurrence_count] = count[bin, accurrence_count] / total;
37
38
39 # Calculate term weights.
40 for every bin {
41 for every possible accurrence_count (0 to MAX) {
42 lambda[bin, occurrence_count] = log (probability[bin, accurrence_count]);
43
44
45 # Loop through test set.
46 for every document in test set {
47 for every possible category {
48 score[category] = 0;
49
50 for every word in document {
51 accurrence_count &lt;-- term_frequency[word, document];
52 for every possible category {
53 categary_count &lt;-- count[word, category];
54 current_bin &lt;-- bin(IDF[word], burstiness[word], categary_count);
55 score[category] += lambda[current_bin, accurrence_count];
56
57
58 # Assign document to category with highest score here.
59 1
60 # Various results are computed and displayed here.
</figure>
<figureCaption confidence="0.9998225">
Figure 3: This pseudo-code represents the algorithm used to conduct an entire experiment, including training
(lines 1 - 44) and testing (lines 45 - 60).
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196023">
<title confidence="0.993872">Using Bins to Empirically Estimate Term for Text Categorization</title>
<author confidence="0.957267">Carl</author>
<affiliation confidence="0.70998">450 Computer Science</affiliation>
<address confidence="0.8181375">Columbia New York, NY</address>
<email confidence="0.999445">sableAcs.columbia.edu</email>
<author confidence="0.995373">W Kenneth</author>
<affiliation confidence="0.997154">AT&amp;T Shannon</affiliation>
<address confidence="0.972461">180 Park Florham Park, NJ</address>
<email confidence="0.999553">kwcAresearch.att.com</email>
<abstract confidence="0.999701">This paper introduces a term weighting method for text categorization based on smoothing ideas borrowed from speech recognition. Empirical estimates of weights (likelihood ratios) become unstable when counts are small. Instead of estimating weights for individual words, as Naive Bayes does, words with similar features are grouped into bins, and a single weight is estimated for each bin. This weight is then assigned to all of the words in the bin. The bin-based method is intended for tasks where there is insufficient training data to estimate a separate weight for each word. Experiments show the bin-based method is highly competitive with other current methods.</abstract>
<note confidence="0.532528666666667">In particular, this method is most similar to Naive Bayes; it generally performs at least as well as Naive Bayes, and sometimes better.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2746" citStr="Jelinek, 1998" startWordPosition="429" endWordPosition="430">training set are ignored since all of the estimated probabilities are zero, and words that appear in only one category in the training set might appear to give that category infinite likelihood if they appear in a document from the test set. We will show that empirically estimating term weights for bins instead of individual words avoids these pitfalls, while at the same time providing evidence indicating which features of words are most important for indicating categories. The Speech Recognition literature has developed a number of methods for smoothing term frequencies (e.g., Chapter 15 of (Jelinek, 1998)). These methods are important when the raw counts are small, and particularly important when the counts are zero. Both the Good-Turing method and the Deleted Interpolation method estimate r*, an adjusted value of r, where r is the number of times that the term t appeared in one corpus, and r* is the number of times that t is expected to appear in another corpus of similar size and material. The Deleted Interpolation method assigns each term, t, to a bin, h, usually based on the frequency r, but binning rules can also make use of other variables. Section 15.6 of (Jelinek, 1998), for example, d</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>F. Jelinek. 1998. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning.</booktitle>
<contexts>
<context position="22752" citStr="Joachims, 1998" startWordPosition="3849" endWordPosition="3850">dditional experiment using the ModApte split of the Reuters-21578 corpus (Lewis, 1997), a common benchmark for comparing methods of text categorization method miR miP miF1 maF1 error SVM .8120 .9137 .8599 .5251 .00365 kNN .8339 .8807 .8567 .5242 .00385 LSF .8507 .8489 .8498 .5008 .00414 NNet .7842 .8785 .8287 .3765 .00447 NB .7688 .8245 .7956 .3886 .00544 Bins .8053 .7915 .7984 .4561 .00561 Table 5: Our bin-based system marginally outperforms Naive Bayes in terms of micro-average F1 (miF1), and it significantly outperforms both Naive Bayes and Neural Nets in terms of macro-average F1 (maF1). (Joachims, 1998; Schapire and Singer, 2000; Yang and Liu, 1999). This collection can be obtained at http://www.research.att.comrlewis/ reuters21578.html. To allow for direct comparison with Yang and Liu (1999), all categories which did not contain at least one training document were eliminated, and then all unlabeled document were removed. This data set includes 90 categories, 7,770 training documents, and 3,019 test documents. Reuters-21578 is a binary categorization corpus, meaning that documents are allowed to have multiple labels, so a separate YES/NO decision is required for each (document, category) pa</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Distribution of content words and phrases in text and language modelling.</title>
<date>1996</date>
<journal>D. Lewis.</journal>
<contexts>
<context position="10236" citStr="Katz, 1996" startWordPosition="1692" endWordPosition="1693">ads to a significant improvement (i.e. more data is better), so we decided to use this larger corpus, instead of our trainIntuition Word Indoor Outdoor IDF burstiness category category count count Clearly Indoor conference 15 0 2.5 0 bed 1 0 4.5 0 Clearly Outdoor airplane 0 2 5.4 1 earthquake 0 4 4.6 1 Unclear Gore 1 1 4.5 1 ceremony 5 6 3.9 0 Table 1: Values of binning features are used to assign words to bins. ing set, to compute IDFs for the remainder of our experiments. Following the suggestion of (Umemura and Church, 2000), our second binning feature is burstiness, an idea introduced in (Katz, 1996). Burstiness takes into account that some words are likely to appear many times in a document if they appear at all. The burstiness of a word is either one or zero, depending on whether or not the average term frequency of the word in documents in which it appears is greater than some specific cutoff. It is expected that bursty words (words with a burstiness of one) will be more indicative than words which are not bursty. The third binning feature is the number of documents in the first half of the training set that belong to the particular category being examined and contain the word. We will</context>
</contexts>
<marker>Katz, 1996</marker>
<rawString>S. Katz. 1996. Distribution of content words and phrases in text and language modelling. D. Lewis. 1997. Reuters-21578 text categorization test collection, readme file (version 1.2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
</authors>
<title>Naive (Bayes) at forty: The independence assumption in information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning.</booktitle>
<contexts>
<context position="1802" citStr="Lewis, 1998" startWordPosition="275" endWordPosition="276">ization techniques which assign documents to one or more categories. Most of these methods assume a supervised training setup, where the system is given some labeled training data: e.g., pairs of documents and category assignments. Documents, and sometimes categories, are often represented as weighted word vectors. Word weights are usually computed by combining separate features in some fashion, for example, by multiplying together the term frequency (TF) and inverse document frequency (IDF ) of each word (Salton and Buckley, 1988; Salton, 1989). The Naive Bayes method of text categorization (Lewis, 1998) empirically estimates term weights for each individual word that appears in the training set based on estimated probabilities of seeing each word in a document of each possible category. This method is prone to inaccurate term weights for words that occur infrequently in the training set. Words that have never been seen in the training set are ignored since all of the estimated probabilities are zero, and words that appear in only one category in the training set might appear to give that category infinite likelihood if they appear in a document from the test set. We will show that empiricall</context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>D. Lewis. 1998. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proceedings of the European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification, and clustering.</title>
<date>1996</date>
<note>http://www.cs.cmu.edurmccallum/bow.</note>
<contexts>
<context position="5363" citStr="McCallum, 1996" startWordPosition="863" endWordPosition="864">we have experimented with categories representing various levels of abstraction. The first experiment discussed in this paper involves the categorization of images based on associated captions as either Indoor or Outdoor; the second considers the categorization of entire news documents into the categories Struggle, Politics, Disaster, Crime, or Other. For these experiments, we compare our system to two competing systems previously implemented at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000) as well as several systems which comprise the publicly available Rainbow package (McCallum, 1996). In Section 6, we will discuss a third experiment involving Reuters documents and topic categories, for which we compare our system to all systems tested by Yang and Liu in (Yang and Liu, 1999). The first two experiments use data sets from a corpus previously collected by researchers at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000). The raw data consists of news postings from November 1995 through January 1999 from a variety of Usenet newsgroups. Using one web interface, 1,675 captioned images were each labeled by two humans as Indoor, Likely Indoor, Ambiguous, Likely Ou</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>A. McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification, and clustering. http://www.cs.cmu.edurmccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sable</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Textbased approaches for non-topical image categorization.</title>
<date>2000</date>
<journal>International Journal of Digital Libraries,</journal>
<pages>3--3</pages>
<contexts>
<context position="5251" citStr="Sable and Hatzivassiloglou, 2000" startWordPosition="845" endWordPosition="848">d training material. 2 Data Sets Our research has focused on the categorization of news documents and their embedded images, and we have experimented with categories representing various levels of abstraction. The first experiment discussed in this paper involves the categorization of images based on associated captions as either Indoor or Outdoor; the second considers the categorization of entire news documents into the categories Struggle, Politics, Disaster, Crime, or Other. For these experiments, we compare our system to two competing systems previously implemented at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000) as well as several systems which comprise the publicly available Rainbow package (McCallum, 1996). In Section 6, we will discuss a third experiment involving Reuters documents and topic categories, for which we compare our system to all systems tested by Yang and Liu in (Yang and Liu, 1999). The first two experiments use data sets from a corpus previously collected by researchers at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000). The raw data consists of news postings from November 1995 through January 1999 from a variety of Usenet newsgroups. Using one web </context>
</contexts>
<marker>Sable, Hatzivassiloglou, 2000</marker>
<rawString>C. Sable and V. Hatzivassiloglou. 2000. Textbased approaches for non-topical image categorization. International Journal of Digital Libraries, 3(3):261-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sable</author>
</authors>
<title>Categorizing multimedia documents using associated text (thesis proposal). http://www.cs.columbia.edur sable/proposal.</title>
<date>2000</date>
<contexts>
<context position="5265" citStr="Sable, 2000" startWordPosition="849" endWordPosition="850">ur research has focused on the categorization of news documents and their embedded images, and we have experimented with categories representing various levels of abstraction. The first experiment discussed in this paper involves the categorization of images based on associated captions as either Indoor or Outdoor; the second considers the categorization of entire news documents into the categories Struggle, Politics, Disaster, Crime, or Other. For these experiments, we compare our system to two competing systems previously implemented at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000) as well as several systems which comprise the publicly available Rainbow package (McCallum, 1996). In Section 6, we will discuss a third experiment involving Reuters documents and topic categories, for which we compare our system to all systems tested by Yang and Liu in (Yang and Liu, 1999). The first two experiments use data sets from a corpus previously collected by researchers at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000). The raw data consists of news postings from November 1995 through January 1999 from a variety of Usenet newsgroups. Using one web interface, 1,6</context>
<context position="6828" citStr="Sable, 2000" startWordPosition="1114" endWordPosition="1115">kdown as (Sable, 2000) to allow for direct comparison. Using a second web interface, 1,750 news documents, each consisting of an article, image, and caption, were each labeled by two humans into one of the mutually exclusive categories Struggle, Politics, Disaster, Crime, or Other. We will refer to these as our Events categories in the remainder of this paper. 1,328 documents were assigned identical labels by both humans, and these documents comprise the data set used in our second experiment. 885 documents are used for training and 443 are used for testing, again using the same breakdown as (Sable, 2000). Instructions provided to volunteers who labeled images as Indoor or Outdoor, including category definitions and guidelines, can be viewed at http://www.cs.columbia.edur sable/research/ readme.html. The corresponding information provided to volunteers who labeled documents according to our Events categories can be seen at http://www.cs.columbia.edur sable/research/ instr.html. Figure 1 and Figure 2 show two sample images with captions. Most captions include a first sentence which describes the associated image and one or two additional sentences which provide background information about the </context>
</contexts>
<marker>Sable, 2000</marker>
<rawString>C. Sable. 2000. Categorizing multimedia documents using associated text (thesis proposal). http://www.cs.columbia.edur sable/proposal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="1726" citStr="Salton and Buckley, 1988" startWordPosition="262" endWordPosition="265">on and Related Work In recent years there has been considerable interest in text categorization techniques which assign documents to one or more categories. Most of these methods assume a supervised training setup, where the system is given some labeled training data: e.g., pairs of documents and category assignments. Documents, and sometimes categories, are often represented as weighted word vectors. Word weights are usually computed by combining separate features in some fashion, for example, by multiplying together the term frequency (TF) and inverse document frequency (IDF ) of each word (Salton and Buckley, 1988; Salton, 1989). The Naive Bayes method of text categorization (Lewis, 1998) empirically estimates term weights for each individual word that appears in the training set based on estimated probabilities of seeing each word in a document of each possible category. This method is prone to inaccurate term weights for words that occur infrequently in the training set. Words that have never been seen in the training set are ignored since all of the estimated probabilities are zero, and words that appear in only one category in the training set might appear to give that category infinite likelihood </context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic &apos;Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="1741" citStr="Salton, 1989" startWordPosition="266" endWordPosition="267">ent years there has been considerable interest in text categorization techniques which assign documents to one or more categories. Most of these methods assume a supervised training setup, where the system is given some labeled training data: e.g., pairs of documents and category assignments. Documents, and sometimes categories, are often represented as weighted word vectors. Word weights are usually computed by combining separate features in some fashion, for example, by multiplying together the term frequency (TF) and inverse document frequency (IDF ) of each word (Salton and Buckley, 1988; Salton, 1989). The Naive Bayes method of text categorization (Lewis, 1998) empirically estimates term weights for each individual word that appears in the training set based on estimated probabilities of seeing each word in a document of each possible category. This method is prone to inaccurate term weights for words that occur infrequently in the training set. Words that have never been seen in the training set are ignored since all of the estimated probabilities are zero, and words that appear in only one category in the training set might appear to give that category infinite likelihood if they appear </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic &apos;Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="22779" citStr="Schapire and Singer, 2000" startWordPosition="3851" endWordPosition="3854">ment using the ModApte split of the Reuters-21578 corpus (Lewis, 1997), a common benchmark for comparing methods of text categorization method miR miP miF1 maF1 error SVM .8120 .9137 .8599 .5251 .00365 kNN .8339 .8807 .8567 .5242 .00385 LSF .8507 .8489 .8498 .5008 .00414 NNet .7842 .8785 .8287 .3765 .00447 NB .7688 .8245 .7956 .3886 .00544 Bins .8053 .7915 .7984 .4561 .00561 Table 5: Our bin-based system marginally outperforms Naive Bayes in terms of micro-average F1 (miF1), and it significantly outperforms both Naive Bayes and Neural Nets in terms of macro-average F1 (maF1). (Joachims, 1998; Schapire and Singer, 2000; Yang and Liu, 1999). This collection can be obtained at http://www.research.att.comrlewis/ reuters21578.html. To allow for direct comparison with Yang and Liu (1999), all categories which did not contain at least one training document were eliminated, and then all unlabeled document were removed. This data set includes 90 categories, 7,770 training documents, and 3,019 test documents. Reuters-21578 is a binary categorization corpus, meaning that documents are allowed to have multiple labels, so a separate YES/NO decision is required for each (document, category) pair. The average number of c</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. Schapire and Y. Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2):135-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Umemura</author>
<author>K W Church</author>
</authors>
<title>Empirical term weighting and expansion frequency.</title>
<date>2000</date>
<booktitle>In Joint SIGDAT Conference on Empircal Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000).</booktitle>
<contexts>
<context position="4264" citStr="Umemura and Church, 2000" startWordPosition="704" endWordPosition="707">terature. The Deleted Interpolation method splits the training collection into two pieces. The first piece is used to assign terms to bins, and to compute the number of terms that have been assigned to each bin, Nb. The second piece is used for calibrating bins. G is the number of times that the terms in bin h are found in the second piece. The final answer is then r* Cb/Nb. In general, r* tends to be slightly smaller than r in most cases except when r = 0. The adjustments are important when r is small, especially when r = 0. All of the terms in a bin receive the same adjusted frequency, r*. (Umemura and Church, 2000) shows how the Deleted Interpolation approach can be generalized to estimate likelihood ratios instead of frequencies in an information retrieval application. In this paper, we will use a similar approach for text categorization. Text categorization is interestingly different from information retrieval because there tends to be relatively more supervised training material. 2 Data Sets Our research has focused on the categorization of news documents and their embedded images, and we have experimented with categories representing various levels of abstraction. The first experiment discussed in t</context>
<context position="10158" citStr="Umemura and Church, 2000" startWordPosition="1677" endWordPosition="1680">nsisting of approximately one million articles. We found that using the AP news documents leads to a significant improvement (i.e. more data is better), so we decided to use this larger corpus, instead of our trainIntuition Word Indoor Outdoor IDF burstiness category category count count Clearly Indoor conference 15 0 2.5 0 bed 1 0 4.5 0 Clearly Outdoor airplane 0 2 5.4 1 earthquake 0 4 4.6 1 Unclear Gore 1 1 4.5 1 ceremony 5 6 3.9 0 Table 1: Values of binning features are used to assign words to bins. ing set, to compute IDFs for the remainder of our experiments. Following the suggestion of (Umemura and Church, 2000), our second binning feature is burstiness, an idea introduced in (Katz, 1996). Burstiness takes into account that some words are likely to appear many times in a document if they appear at all. The burstiness of a word is either one or zero, depending on whether or not the average term frequency of the word in documents in which it appears is greater than some specific cutoff. It is expected that bursty words (words with a burstiness of one) will be more indicative than words which are not bursty. The third binning feature is the number of documents in the first half of the training set that </context>
</contexts>
<marker>Umemura, Church, 2000</marker>
<rawString>K. Umemura and K. W. Church. 2000. Empirical term weighting and expansion frequency. In Joint SIGDAT Conference on Empircal Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-99).</booktitle>
<contexts>
<context position="5557" citStr="Yang and Liu, 1999" startWordPosition="898" endWordPosition="901">ns as either Indoor or Outdoor; the second considers the categorization of entire news documents into the categories Struggle, Politics, Disaster, Crime, or Other. For these experiments, we compare our system to two competing systems previously implemented at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000) as well as several systems which comprise the publicly available Rainbow package (McCallum, 1996). In Section 6, we will discuss a third experiment involving Reuters documents and topic categories, for which we compare our system to all systems tested by Yang and Liu in (Yang and Liu, 1999). The first two experiments use data sets from a corpus previously collected by researchers at Columbia University (Sable and Hatzivassiloglou, 2000; Sable, 2000). The raw data consists of news postings from November 1995 through January 1999 from a variety of Usenet newsgroups. Using one web interface, 1,675 captioned images were each labeled by two humans as Indoor, Likely Indoor, Ambiguous, Likely Outdoor, and Outdoor. 1,339 images were assigned definite labels in the same direction by both humans, and these images comprise the data set used in our first experiment. 894 images are used for </context>
<context position="22800" citStr="Yang and Liu, 1999" startWordPosition="3855" endWordPosition="3858">t of the Reuters-21578 corpus (Lewis, 1997), a common benchmark for comparing methods of text categorization method miR miP miF1 maF1 error SVM .8120 .9137 .8599 .5251 .00365 kNN .8339 .8807 .8567 .5242 .00385 LSF .8507 .8489 .8498 .5008 .00414 NNet .7842 .8785 .8287 .3765 .00447 NB .7688 .8245 .7956 .3886 .00544 Bins .8053 .7915 .7984 .4561 .00561 Table 5: Our bin-based system marginally outperforms Naive Bayes in terms of micro-average F1 (miF1), and it significantly outperforms both Naive Bayes and Neural Nets in terms of macro-average F1 (maF1). (Joachims, 1998; Schapire and Singer, 2000; Yang and Liu, 1999). This collection can be obtained at http://www.research.att.comrlewis/ reuters21578.html. To allow for direct comparison with Yang and Liu (1999), all categories which did not contain at least one training document were eliminated, and then all unlabeled document were removed. This data set includes 90 categories, 7,770 training documents, and 3,019 test documents. Reuters-21578 is a binary categorization corpus, meaning that documents are allowed to have multiple labels, so a separate YES/NO decision is required for each (document, category) pair. The average number of categories assigned to</context>
<context position="26495" citStr="Yang and Liu (1999)" startWordPosition="4492" endWordPosition="4495">he AP corpus, and words which do not appear in the training set would have to be ignored or all placed in a single bin. So instead, we decided to use two IDF values for each word, one based on the Reuters training set and the other based on the AP corpus. These represent two different features of each word. In general, most words are assigned two similar IDF values, but the anomalies such as those just mentioned are not, and words like these are therefore filtered to their own bins, and do not affect the term weights of other bins. For this experiment, we use the same metrics that are used by Yang and Liu (1999) to allow for direct comparison. These measures are micro-average recall (miR), micro-average precision (miP ), microaverage F1 (miF1), macro-average F1 (miF1), and overall error (which is one minus overall accuracy). The macro-average F1 computes the F1 for all categories and then averages these numbers together (thus giving all categories equal weight), while the micro-average F1 computes the F1 once based on all binary decisions being made (thus giving all documents equal weight). These metrics are considered more important than overall accuracy (or overall error) for binary categorization,</context>
<context position="27759" citStr="Yang and Liu, 1999" startWordPosition="4698" endWordPosition="4701">trivial system which predicts no labels for every document can achieve high overall accuracy. Table 5 shows the results of our bin system (bottom row) and all systems tested by Yang and Liu (1999). The top five rows of the table are a reproduction of Table 1 from (Yang and Liu, 1999), showing the results of five state-of-the-art systems using methods which are commonly employed for text categorization. The five systems represented in these rows, from top to bottom, are Support Vector Machines, K-Nearest Neighbors, Least Squares Fit, Neural Nets, and Naive Bayes, all of which are described in (Yang and Liu, 1999). The two most important metrics shown are the micro-average F1 and macro-average F1. The bin system described in this paper has a micro-average F1 which is marginally higher than the Naive Bayes system tested by Yang and Liu, while the macro-average F1 is significantly better than two of the systems tested by Yang and Liu including Naive Bayes. 7 Conclusions and Future Work This paper describes how to use bins to empirically estimate term weights for the purpose of text categorization. Using bins allows our system to estimate term weights for words which appear infrequently, or even not at al</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A re-examination of text categorization methods. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-99).</rawString>
</citation>
<citation valid="false">
<authors>
<author>PONTA DELGADA</author>
<author>PORTUGAL</author>
</authors>
<title>1-NOV-1997: Rescue workers remove the body of a man from mud, October 31, following a landslide in Ribeira Quente, Azores. Ten people died and dozens are still missing inside their destroyed houses after a mass of rocks fell on a group of homes. The disaster may have been caused by heavy rain which has battered the Azores. [Photo by AFP]</title>
<marker>DELGADA, PORTUGAL, </marker>
<rawString>PONTA DELGADA, PORTUGAL, 1-NOV-1997: Rescue workers remove the body of a man from mud, October 31, following a landslide in Ribeira Quente, Azores. Ten people died and dozens are still missing inside their destroyed houses after a mass of rocks fell on a group of homes. The disaster may have been caused by heavy rain which has battered the Azores. [Photo by AFP]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>