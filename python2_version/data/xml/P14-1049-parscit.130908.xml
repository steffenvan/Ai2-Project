<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994948">
Negation Focus Identification with Contextual Discourse Information
</title>
<author confidence="0.892738">
Bowei Zou Qiaoming Zhu Guodong Zhou*
</author>
<affiliation confidence="0.857971">
Natural Language Processing Lab, School of Computer Science and Technology
</affiliation>
<address confidence="0.878194">
Soochow University, Suzhou, 215006, China
</address>
<email confidence="0.998086">
zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994184375">
Negative expressions are common in natural
language text and play a critical role in in-
formation extraction. However, the perfor-
mances of current systems are far from satis-
faction, largely due to its focus on intra-
sentence information and its failure to con-
sider inter-sentence information. In this paper,
we propose a graph model to enrich intra-
sentence features with inter-sentence features
from both lexical and topic perspectives.
Evaluation on the *SEM 2012 shared task
corpus indicates the usefulness of contextual
discourse information in negation focus iden-
tification and justifies the effectiveness of our
graph model in capturing such global infor-
mation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999355666666667">
Negation is a grammatical category which com-
prises various kinds of devices to reverse the
truth value of a proposition (Morante and
Sporleder, 2012). For example, sentence (1)
could be interpreted as it is not the case that he
stopped.
</bodyText>
<listItem confidence="0.668235">
(1) He didn&apos;t stop.
</listItem>
<bodyText confidence="0.999874333333333">
Negation expressions are common in natural
language text. According to the statistics on bio-
medical literature genre (Vincze et al., 2008),
19.44% of sentences contain negative expres-
sions. The percentage rises to 22.5% on Conan
Doyle stories (Morante and Daelemans, 2012). It
is interesting that a negative sentence may have
both negative and positive meanings. For exam-
ple, sentence (2) could be interpreted as He
stopped, but not until he got to Jackson Hole
with positive part he stopped and negative part
until he got to Jackson Hole. Moreover, a nega-
</bodyText>
<note confidence="0.476051">
* Corresponding author
</note>
<bodyText confidence="0.998765375">
tive expression normally interacts with some
special part in the sentence, referred as negation
focus in linguistics. Formally, negation focus is
defined as the special part in the sentence, which
is most prominently or explicitly negated by a
negative expression. Hereafter, we denote nega-
tive expression in boldface and negation focus
underlined.
</bodyText>
<listItem confidence="0.710643">
(2) He didn&apos;t stop until he got to Jackson Hole.
</listItem>
<bodyText confidence="0.999726117647059">
While people tend to employ stress or intona-
tion in speech to emphasize negation focus and
thus it is easy to identify negation focus in
speech corpora, such stress or intonation infor-
mation often misses in the dominating text cor-
pora. This poses serious challenges on negation
focus identification. Current studies (e.g., Blanco
and Moldovan, 2011; Rosenberg and Bergler,
2012) sort to various kinds of intra-sentence in-
formation, such as lexical features, syntactic fea-
tures, semantic role features and so on, ignoring
less-obvious inter-sentence information. This
largely defers the performance of negation focus
identification and its wide applications, since
such contextual discourse information plays a
critical role on negation focus identification.
Take following sentence as an example.
</bodyText>
<listItem confidence="0.874217">
(3) Helen didn’t allow her youngest son to
play the violin.
</listItem>
<bodyText confidence="0.982604">
In sentence (3), there are several scenarios on
identification of negation focus, with regard to
negation expression n’t, given different contexts:
Scenario A: Given sentence But her husband did
as next sentence, the negation focus should be
Helen, yielding interpretation the person who
didn’t allow the youngest son to play the violin is
Helen but not her husband.
Scenario B: Given sentence She thought that he
didn’t have the artistic talent like her eldest son
as next sentence, the negation focus should be
the youngest son, yielding interpretation Helen
</bodyText>
<page confidence="0.950503">
522
</page>
<note confidence="0.8324105">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522–530,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997449642857143">
thought that her eldest son had the talent to play
the violin, but the youngest son didn’t.
Scenario C: Given sentence Because of her
neighbors’ protests as previous sentence, the ne-
gation focus should be play the violin, yielding
interpretation Helen didn’t allow her youngest
son to play the violin, but it didn’t show whether
he was allowed to do other things.
In this paper, to well accommodate such con-
textual discourse information in negation focus
identification, we propose a graph model to en-
rich normal intra-sentence features with various
kinds of inter-sentence features from both lexical
and topic perspectives. Besides, the standard
PageRank algorithm is employed to optimize the
graph model. Evaluation on the *SEM 2012
shared task corpus (Morante and Blanco, 2012)
justifies our approach over several strong base-
lines.
The rest of this paper is organized as follows.
Section 2 overviews the related work. Section 3
presents several strong baselines on negation fo-
cus identification with only intra-sentence fea-
tures. Section 4 introduces our topic-driven
word-based graph model with contextual dis-
course information. Section 5 reports the exper-
imental results and analysis. Finally, we con-
clude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997634375">
Earlier studies of negation were almost in lin-
guistics (e.g. Horn, 1989; van der Wouden,
1997), and there were only a few in natural lan-
guage processing with focus on negation recog-
nition in the biomedical domain. For example,
Chapman et al. (2001) developed a rule-based
negation recognition system, NegEx, to deter-
mine whether a finding mentioned within narra-
tive medical reports is present or absent. Since
the release of the BioScope corpus (Vincze et al.,
2008), a freely available resource consisting of
medical and biological texts, machine learning
approaches begin to dominate the research on
negation recognition (e.g. Morante et al., 2008;
Li et al., 2010).
Generally, negation recognition includes three
subtasks: cue detection, which detects and identi-
fies possible negative expressions in a sentence,
scope resolution, which determines the grammat-
ical scope in a sentence affected by a negative
expression, and focus identification, which iden-
tifies the constituent in a sentence most promi-
nently or explicitly negated by a negative expres-
sion. This paper concentrates on the third subtask,
negation focus identification.
Due to the increasing demand on deep under-
standing of natural language text, negation
recognition has been drawing more and more
attention in recent years, with a series of shared
tasks and workshops, however, with focus on cue
detection and scope resolution, such as the Bi-
oNLP 2009 shared task for negative event detec-
tion (Kim et al., 2009) and the ACL 2010 Work-
shop for scope resolution of negation and specu-
lation (Morante and Sporleder, 2010), followed
by a special issue of Computational Linguistics
(Morante and Sporleder, 2012) for modality and
negation.
The research on negation focus identification
was pioneered by Blanco and Moldovan (2011),
who investigated the negation phenomenon in
semantic relations and proposed a supervised
learning approach to identify the focus of a nega-
tion expression. However, although Morante and
Blanco (2012) proposed negation focus identifi-
cation as one of the *SEM’2012 shared tasks,
only one team (Rosenberg and Bergler, 2012) 1
participated in this task. They identified negation
focus using three kinds of heuristics and
achieved 58.40 in F1-measure. This indicates
great expectation in negation focus identification.
The key problem in current research on nega-
tion focus identification is its focus on intra-
sentence information and large ignorance of in-
ter-sentence information, which plays a critical
role in the success of negation focus identifica-
tion. For example, Ding (2011) made a qualita-
tive analysis on implied negations in conversa-
tion and attempted to determine whether a sen-
tence was negated by context information, from
the linguistic perspective. Moreover, a negation
focus is always associated with authors’ intention
in article. This indicates the great challenges in
negation focus identification.
</bodyText>
<sectionHeader confidence="0.99032" genericHeader="method">
3 Baselines
</sectionHeader>
<bodyText confidence="0.99971975">
Negation focus identification in *SEM’2012
shared tasks is restricted to verbal negations an-
notated with MNEG in PropBank, with only the
constituent belonging to a semantic role selected
as negation focus. Normally, a verbal negation
expression (not or n’t) is grammatically associat-
ed with its corresponding verb (e.g., He didn’t
stop). For details on annotation guidelines and
</bodyText>
<footnote confidence="0.9839565">
1 In *SEM’2013, the shared task is changed with focus on
&amp;quot;Semantic Textual Similarity&amp;quot;.
</footnote>
<page confidence="0.998643">
523
</page>
<bodyText confidence="0.9835108125">
examples for verbal negations, please refer to
Blanco and Moldovan (2011).
For comparison, we choose the state-of-the-art
system described in Blanco and Moldovan
(2011), which employed various kinds of syntac-
tic features and semantic role features, as one of
our baselines. Since this system adopted C4.5 for
training, we name it as BaselineC4.5. In order to
provide a stronger baseline, besides those fea-
tures adopted in BaselineC4.5, we added more re-
fined intra-sentence features and adopted ranking
Support Vector Machine (SVM) model for train-
ing. We name it as BaselineSVM.
Following is a list of features adopted in the
two baselines, for both BaselineC4.5 and Base-
lineSVM,
</bodyText>
<listItem confidence="0.943831882352941">
➢ Basic features: first token and its part-of-
speech (POS) tag of the focus candidate; the
number of tokens in the focus candidate;
relative position of the focus candidate
among all the roles present in the sentence;
negated verb and its POS tag of the negative
expression;
➢ Syntactic features: the sequence of words
from the beginning of the governing VP to
the negated verb; the sequence of POS tags
from the beginning of the governing VP to
the negated verb; whether the governing VP
contains a CC; whether the governing VP
contains a RB.
➢ Semantic features: the syntactic label of se-
mantic role A1; whether A1 contains POS
tag DT, JJ, PRP, CD, RB, VB, and WP, as
</listItem>
<bodyText confidence="0.951394272727273">
defined in Blanco and Moldovan (2011);
whether A1 contains token any, anybody, an-
ymore, anyone, anything, anytime, anywhere,
certain, enough, full, many, much, other,
some, specifics, too, and until, as defined in
Blanco and Moldovan (2011); the syntactic
label of the first semantic role in the sentence;
the semantic label of the last semantic role in
the sentence; the thematic role for
A0/A1/A2/A3/A4 of the negated predicate.
and for BaselineSVM only,
</bodyText>
<listItem confidence="0.647676333333333">
➢ Basic features: the named entity and its type
in the focus candidate; relative position of the
focus candidate to the negative expression
(before or after).
➢ Syntactic features: the dependency path and
its depth from the focus candidate to the neg-
ative expression; the constituent path and its
depth from the focus candidate to the nega-
tive expression;
</listItem>
<sectionHeader confidence="0.745084" genericHeader="method">
4 Exploring Contextual Discourse In-
</sectionHeader>
<bodyText confidence="0.97497775">
formation for Negation Focus Identi-
fication
While some of negation focuses could be identi-
fied by only intra-sentence information, others
must be identified by contextual discourse in-
formation. Section 1 illustrates the necessity of
such contextual discourse information in nega-
tion focus identification by giving three scenarios
of different discourse contexts for negation ex-
pression n’t in sentence (3).
For better illustration of the importance of
contextual discourse information, Table 1 shows
the statistics of intra- and inter-sentence infor-
mation necessary for manual negation focus
identification with 100 instances randomly ex-
tracted from the held-out dataset of *SEM&apos;2012
shared task corpus. It shows that only 17 instanc-
es can be identified by intra-sentence information.
It is surprising that inter-sentence information is
indispensable in 77 instances, among which 42
instances need only inter-sentence information
and 35 instances need both intra- and inter-
sentence information. This indicates the great
importance of contextual discourse information
on negation focus identification. It is also inter-
esting to note 6 instances are hard to determine
even given both intra- and inter-sentence infor-
mation.
</bodyText>
<table confidence="0.666261">
Info Number
#Intra-Sentence Only 17
#Inter-Sentence Only 42
#Both 35
#Hard to Identify 6
</table>
<tableCaption confidence="0.6776926">
(Note: &amp;quot;Hard to Identify&amp;quot; means that it is hard for a
human being to identify the negation focus even
given both intra- and inter-sentence information.)
Table 1. Statistics of intra- and inter-sentence
information on negation focus identification.
</tableCaption>
<bodyText confidence="0.999892461538462">
Statistically, we find that negation focus is al-
ways related with what authors repeatedly states
in discourse context. This explains why contex-
tual discourse information could help identify
negation focus. While inter-sentence information
provides the global characteristics from the dis-
course context perspective and intra-sentence
information provides the local features from lex-
ical, syntactic and semantic perspectives, both
have their own contributions on negation focus
identification.
In this paper, we first propose a graph model
to gauge the importance of contextual discourse
</bodyText>
<page confidence="0.988739">
524
</page>
<bodyText confidence="0.99992575">
information. Then, we incorporate both intra-
and inter-sentence features into a machine learn-
ing-based framework for negation focus identifi-
cation.
</bodyText>
<subsectionHeader confidence="0.971407">
4.1 Graph Model
</subsectionHeader>
<bodyText confidence="0.9999867">
Graph models have been proven successful in
many NLP applications, especially in represent-
ing the link relationships between words or sen-
tences (Wan and Yang, 2008; Li et al., 2009).
Generally, such models could construct a graph
to compute the relevance between document
theme and words.
In this paper, we propose a graph model to
represent the contextual discourse information
from both lexical and topic perspectives. In par-
ticular, a word-based graph model is proposed to
represent the explicit relatedness among words in
a discourse from the lexical perspective, while a
topic-driven word-based model is proposed to
enrich the implicit relatedness between words, by
adding one more layer to the word-based graph
model in representing the global topic distribu-
tion of the whole dataset. Besides, the PageRank
algorithm (Page et al., 1998) is adopted to opti-
mize the graph model.
</bodyText>
<sectionHeader confidence="0.447938" genericHeader="method">
Word-based Graph Model:
</sectionHeader>
<bodyText confidence="0.9403215">
A word-based graph model can be defined as
Gword (W, E), where W={wi} is the set of words in
one document and E={eij|wi, wj ∈W} is the set of
directed edges between these words, as shown in
</bodyText>
<figureCaption confidence="0.9855755">
Figure 1.
Figure 1. Word-based graph model.
</figureCaption>
<bodyText confidence="0.999407266666667">
In the word-based graph model, word node wi
is weighted to represent the correlation of the
word with authors’ intention. Since such correla-
tion is more from the semantic perspective than
the grammatical perspective, only content words
are considered in our graph model, ignoring
functional words (e.g., the, to,...). Especially, the
content words limited to those with part-of-
speech tags of JJ, NN, PRP, and VB. For sim-
plicity, the weight of word node wi is initialized
to 1.
In addition, directed edge eij is weighted to
represent the relatedness between word wi and
word wj in a document with transition probability
P(j|i) from i to j, which is normalized as follows:
</bodyText>
<equation confidence="0.990272666666667">
ௌ௜௠ሺ௪೔,௪ೕሻ
ܲሺ݆|݅ሻ ൌ
∑ೖௌ௜௠ሺ௪೔,௪ೖሻ (1)
</equation>
<bodyText confidence="0.999951076923077">
where k represents the nodes in discourse, and
Sim(wi,wj) denotes the similarity between wi and
wj. In this paper, two kinds of information are
used to calculate the similarity between words.
One is word co-occurrence (if word wi and word
wj occur in the same sentence or in the adjacent
sentences, Sim(wi,wj) increases 1), and the other
is WordNet (Miller, 1995) based similarity.
Please note that Sim(wi,wi) = 0 to avoid self-
transition, and Sim(wi,wj) and Sim(wj,wi) may not
be equal.
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows:
</bodyText>
<equation confidence="0.997724333333333">
ܵܿ݋ݎ݁ሺ଴ሻሺݓ௜ሻ ൌ 1
ܵܿ݋ݎ݁ሺ௡ାଵሻሺݓ௜ሻ ൌ ݀ ∑௝ஷ௜ ܵܿ݋ݎ݁ሺ௡ሻ൫ݓ௝൯ ൈܲሺ݆|݅ሻ ൅
ሺ1 െ ݀ሻ (2)
</equation>
<bodyText confidence="0.9977675">
where d is the damping factor as in the PageRank
algorithm.
</bodyText>
<subsectionHeader confidence="0.574844">
Topic-driven Word-based Graph Model
</subsectionHeader>
<bodyText confidence="0.999430333333333">
While the above word-based graph model can
well capture the relatedness between content
words, it can only partially model the focus of a
negation expression since negation focus is more
directly related with topic than content. In order
to reduce the gap, we propose a topic-driven
word-based model by adding one more layer to
refine the word-based graph model over the
global topic distribution, as shown in Figure 2.
</bodyText>
<figureCaption confidence="0.994243">
Figure 2. Topic-driven word-based graph model.
</figureCaption>
<page confidence="0.989523">
525
</page>
<bodyText confidence="0.999777428571429">
Here, the topics are extracted from all the doc-
uments in the *SEM 2012 shared task using the
LDA Gibbs Sampling algorithm (Griffiths, 2002).
In the topic-driven word-based graph model, the
first layer denotes the relatedness among content
words as captured in the above word-based graph
model, and the second layer denotes the topic
distribution, with the dashed lines between these
two layers indicating the word-topic model re-
turn by LDA.
Formally, the topic-driven word-based two-
layer graph is defined as Gtopic (W, T, Ew, Et),
where W={wi} is the set of words in one docu-
ment and T={ti} is the set of topics in all docu-
ments; Ew={ewij|wi, wj EW} is the set of directed
edges between words and Et ={etij|wiEW, tj ET}
is the set of undirected edges between words and
topics; transition probability Pw(j|i) of ewij is de-
fined as the same as P(j|i) of the word-based
graph model. Besides, transition probability Pt
(i,m) of etij in the word-topic model is defined as:
</bodyText>
<equation confidence="0.995851">
Pt(i,M) = Rel(11&apos;i,tm.) (3)
Ek Rel(wi,tk)
</equation>
<bodyText confidence="0.9969834">
where Rel(wi, tm) is the weight of word wi in top-
ic tm calculated by the LDA Gibbs Sampling al-
gorithm. On the basis, the transition probability
Pw (j|i) of ewij is updated by calculating as fol-
lowing:
</bodyText>
<equation confidence="0.996243666666667">
P′w(�10 = 0 . PW(�10 + (1 — 0) . Pt(i,M)XPt(i,M)
Ek Pt(i,k)XPt(l,k)
(4)
</equation>
<bodyText confidence="0.999955">
where k represents all topics linked to both word
wi and word wj, and θE[0,1] is the coefficient
controlling the relative contributions from the
lexical information in current document and the
topic information in all documents.
Finally, the weights of word nodes are calcu-
lated using the PageRank algorithm as follows:
</bodyText>
<equation confidence="0.999948">
Score(0)(wi) = 1
Score(n+i)(wi) = d Ej*i Score(n)(wi) X P′w(lji) +
(1 — d) (5)
</equation>
<bodyText confidence="0.9986615">
where d is the damping factor as in the PageRank
algorithm.
</bodyText>
<subsectionHeader confidence="0.995203">
4.2 Negation Focus Identification via
Graph Model
</subsectionHeader>
<bodyText confidence="0.999867285714286">
Given the graph models and the PageRank opti-
mization algorithm discussed above, four kinds
of contextual discourse information are extracted
as inter-sentence features (Table 2).
In particular, the total weight and the max
weight of words in the focus candidate are calcu-
lated as follows:
</bodyText>
<equation confidence="0.9998955">
Weighttotal = EiScore(final)(wi) (6)
Weightm,, = maxi Score(final)(Wi) (7)
</equation>
<bodyText confidence="0.848280882352941">
where i represents the content words in the focus
candidate. These two kinds of weights focus on
different aspects about the focus candidate with
the former on the contribution of content words,
which is more beneficial for a long focus candi-
date, and the latter biased towards the focus can-
didate which contains some critical word in a
discourse.
No Feature
1 Total weight of words in the focus candi-
date using the co-occurrence similarity.
2 Max weight of words in the focus candi-
date using the co-occurrence similarity.
3 Total weight of words in the focus candi-
date using the WordNet similarity.
4 Max weight of words in the focus candi-
date using the WordNet similarity.
</bodyText>
<tableCaption confidence="0.699281">
Table 2. Inter-sentence features extracted from
graph model.
</tableCaption>
<bodyText confidence="0.999880833333333">
For evaluating the contribution of contextual
discourse information on negation focus identifi-
cation directly, we incorporate the four inter-
sentence features from the topic-driven word-
based graph model into a negation focus identifi-
er.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.9998202">
In this section, we describe experimental settings
and systematically evaluate our negation focus
identification approach with focus on exploring
the effectiveness of contextual discourse infor-
mation.
</bodyText>
<subsectionHeader confidence="0.96948">
5.1 Experimental Settings
Dataset
</subsectionHeader>
<bodyText confidence="0.9998315">
In all our experiments, we employ the
*SEM&apos;2012 shared task corpus (Morante and
Blanco, 2012)2. As a freely downloadable re-
source, the *SEM shared task corpus is annotated
on top of PropBank, which uses the WSJ section
of the Penn TreeBank. In particular, negation
focus annotation on this corpus is restricted to
verbal negations (with corresponding mark
</bodyText>
<footnote confidence="0.906638">
2 http://www.clips.ua.ac.be/sem2012-st-neg/
</footnote>
<page confidence="0.997883">
526
</page>
<bodyText confidence="0.999697294117647">
MNEG in PropBank). On 50% of the corpus an-
notated by two annotators, the inter-annotator
agreement was 0.72 (Blanco and Moldovan,
2011). Along with negation focus annotation,
this corpus also contains other annotations, such
as POS tag, named entity, chunk, constituent tree,
dependency tree, and semantic role.
In total, this corpus provides 3,544 instances
of negation focus annotations. For fair compari-
son, we adopt the same partition as *SEM’2012
shared task in all our experiments, i.e., with
2,302 for training, 530 for development, and 712
for testing. Although for each instance, the cor-
pus only provides the current sentence, the pre-
vious and next sentences as its context, we sort to
the Penn TreeBank3 to obtain the corresponding
document as its discourse context.
</bodyText>
<subsectionHeader confidence="0.612722">
Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.978781727272727">
Same as the *SEM&apos;2012 shared task, the evalua-
tion is made using precision, recall, and F1-score.
Especially, a true positive (TP) requires an exact
match for the negation focus, a false positive (FP)
occurs when a system predicts a non-existing
negation focus, and a false negative (FN) occurs
when the gold annotations specify a negation
focus but the system makes no prediction. For
each instance, the predicted focus is considered
correct if it is a complete match with a gold an-
notation.
Beside, to show whether an improvement is
significant, we conducted significance testing
using z-test, as described in Blanco and Moldo-
van (2011).
Toolkits
In our experiments, we report not only the de-
fault performance with gold additional annotated
features provided by the *SEM&apos;2012 shared task
corpus and the Penn TreeBank, but also the per-
formance with various kinds of features extracted
automatically, using following toolkits:
</bodyText>
<listItem confidence="0.967787285714286">
➢ Syntactic Parser: We employ the Stanford
Parser4 (Klein and Manning, 2003; De Marn-
effe et al., 2006) for tokenization, constituent
and dependency parsing.
➢ Named Entity Recognizer: We employ the
Stanford NER5 (Finkel et al., 2005) to obtain
named entities.
</listItem>
<footnote confidence="0.998544666666667">
3 http://www.cis.upenn.edu/~treebank/
4 http://nlp.stanford.edu/software/lex-parser.shtml
5 http://nlp.stanford.edu/ner/
</footnote>
<bodyText confidence="0.76900225">
➢ Semantic Role Labeler: We employ the se-
mantic role labeler, as described in Punyaka-
nok et al (2008).
➢ Topic Modeler: For estimating transition
probability Pt(i,m), we employ
GibbsLDA++6, an LDA model using Gibbs
Sampling technique for parameter estimation
and inference.
</bodyText>
<listItem confidence="0.708895">
➢ Classifier: We employ SVMLight 7 with default
parameters as our classifier.
</listItem>
<subsectionHeader confidence="0.9558205">
5.2 Experimental Results
With Only Intra-sentence Information
</subsectionHeader>
<bodyText confidence="0.99898175">
Table 3 shows the performance of the two base-
lines, the decision tree-based classifier as in
Blanco and Moldovan (2011) and our ranking
SVM-based classifier. It shows that our ranking
SVM-based baseline slightly improves the F1-
measure by 2.52% over the decision tree-based
baseline, largely due to the incorporation of more
refined features.
</bodyText>
<table confidence="0.989692">
System P(%) R(%) F1
BaselineC4.5 66.73 49.93 57.12
BaselineSVM 60.22 59.07 59.64
</table>
<tableCaption confidence="0.9635645">
Table 3. Performance of baselines with only
intra-sentence information.
</tableCaption>
<bodyText confidence="0.9999684">
Error analysis of the ranking SVM-based
baseline on development data shows that 72% of
them are caused by the ignorance of inter-
sentence information. For example, among the
42 instances listed in the category of “#Inter-
Sentence Only” in Table 1, only 7 instances can
be identified correctly by the ranking SVM-
based classifier. With about 4 focus candidates in
one sentence on average, this percentage is even
lower than random.
</bodyText>
<subsectionHeader confidence="0.795749">
With Only Inter-sentence Information
</subsectionHeader>
<bodyText confidence="0.999840636363636">
For exploring the usefulness of pure contextual
discourse information in negation focus identifi-
cation, we only employ inter-sentence features
into ranking SVM-based classifier. First of all,
we estimate two parameters for our topic-driven
word-based graph model: topic number T for
topic model and coefficient θ between Pw(j|i) and
Pt (i,m) in Formula 4.
Given the LDA Gibbs Sampling model with
parameters α = 50/T and β = 0.1, we vary T from
20 to 100 with an interval of 10 to find the opti-
</bodyText>
<footnote confidence="0.998512">
6 http://gibbslda.sourceforge.net/
7 http://svmlight.joachims.org
</footnote>
<page confidence="0.995001">
527
</page>
<bodyText confidence="0.9997624">
mal T. Figure 3 shows the experiment results of
varying T (with 0 = 0.5) on development data. It
shows that the best performance is achieved
when T = 50 with 51.11 in F1). Therefore, we set
T as 50 in our following experiments.
</bodyText>
<figureCaption confidence="0.941287">
Figure 3. Performance with varying T.
</figureCaption>
<bodyText confidence="0.99993525">
For parameter 0, a trade-off between the tran-
sition probability Pw(j|i) (word to word) and the
transition probability Pt (i,m) (word and topic) to
update P’w(j|i), we vary it from 0 to 1 with an
interval of 0.1. Figure 4 shows the experiment
results of varying 0 (with T=50) on development
data. It shows that the best performance is
achieved when 0 = 0.6, which are adopted here-
after in all our experiments. This indicates that
direct lexical information in current document
contributes more than indirect topic information
in all documents on negation focus identification.
It also shows that direct lexical information in
current document and indirect topic information
in all documents are much complementary on
negation focus identification.
</bodyText>
<figureCaption confidence="0.952257">
Figure 4. Performance with varying 0.
</figureCaption>
<table confidence="0.6887284">
System P(%) R(%) F1
using word-based graph 45.62 42.02 43.75
model
using topic-driven word- 54.59 50.76 52.61
based graph model
</table>
<tableCaption confidence="0.9007105">
Table 4. Performance with only inter-sentence
information.
</tableCaption>
<bodyText confidence="0.995929875">
Table 4 shows the performance of negation
focus identification with only inter-sentence fea-
tures. It also shows that the system with inter-
sentence features from the topic-driven word-
based graph model significantly improves the
F1-measure by 8.86 over the system with inter-
sentence features from the word-based graph
model, largely due to the usefulness of topic in-
formation.
In comparison with Table 3, it shows that the
system with only intra-sentence features achieves
better performance than the one with only inter-
sentence features (59.64 vs. 52.61 in F1-
measure).
With both Intra- and Inter-sentence In-
formation
Table 5 shows that enriching intra-sentence fea-
tures with inter-sentence features significantly
(p&lt;0.01) improve the performance by 9.85 in F1-
measure than the better baseline. This indicates
the usefulness of such contextual discourse in-
formation and the effectiveness of our topic-
driven word-based graph model in negation fo-
cus identification.
</bodyText>
<table confidence="0.710525181818182">
System P(%) R(%) F1
BaselineC4.5 with intra 66.73 49.93 57.12
feat. only
BaselineSVM with intra 60.22 59.07 59.64
feat. only
Ours with Both feat. 64.93 62.47 63.68
using word-based GM
Ours with Both feat. 71.67 67.43 69.49
using topic-driven
word-based GM
(Note: “feat.” denotes features; “GM” denotes graph model.)
</table>
<tableCaption confidence="0.925844">
Table 5. Performance comparison of systems on
negation focus identification.
</tableCaption>
<table confidence="0.991424181818182">
System P(%) R(%) F1
BaselineC4.5 with intra 60.94 44.62 51.52
feat. only (auto)
BaselineSVM with intra 53.81 51.67 52.72
feat. Only (auto)
Ours with Both feat. 58.77 57.19 57.97
using word-based GM
(auto)
Ours with Both feat. 66.74 64.53 65.62
using topic-driven
word-based GM (auto)
</table>
<tableCaption confidence="0.983338666666667">
Table 6. Performance comparison of systems on
negation focus identification with automatically
extracted features.
</tableCaption>
<page confidence="0.996044">
528
</page>
<bodyText confidence="0.999954928571429">
Besides, Table 6 shows the performance of
our best system with all features automatically
extracted using the toolkits as described in Sec-
tion 5.1. Compared with our best system employ-
ing gold additional annotated features (the last
line in Table 5), the homologous system with
automatically extracted features (the last line in
Table 6) only decrease of less than 4 in F1-
measure. This demonstrates the achievability of
our approach.
In comparison with the best-reported perfor-
mance on the *SEM’2012 shared task (Rosen-
berg and Bergler, 2012), our system performs
better by about 11 in F-measure.
</bodyText>
<subsectionHeader confidence="0.853964">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999865296296296">
While this paper verifies the usefulness of con-
textual discourse information on negation focus
identification, the performance with only inter-
sentence features is still weaker than that with
only intra-sentence features. There are two main
reasons. On the one hand, the former employs an
unsupervised approach without prior knowledge
for training. On the other hand, the usefulness of
inter-sentence features depends on the assump-
tion that a negation focus relates to the meaning
of which is most relevant to authors’ intention in
a discourse. If there lacks relevant information in
a discourse context, negation focus will become
difficult to be identified only by inter-sentence
features.
Error analysis also shows that some of the ne-
gation focuses are very difficult to be identified,
even for a human being. Consider the sentence (3)
in Section 1, if given sentence because of her
neighbors&apos; protests, but her husband doesn’t
think so as its following context, both Helen and
to play the violin can become the negation focus.
Moreover, the inter-annotator agreement in the
first round of negation focus annotation can only
reach 0.72 (Blanco and Moldovan, 2011). This
indicates inherent difficulty in negation focus
identification.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999565625">
In this paper, we propose a graph model to enrich
intra-sentence features with inter-sentence fea-
tures from both lexical and topic perspectives. In
this graph model, the relatedness between words
is calculated by word co-occurrence, WordNet-
based similarity, and topic-driven similarity.
Evaluation on the *SEM 2012 shared task corpus
indicates the usefulness of contextual discourse
information on negation focus identification and
our graph model in capturing such global infor-
mation.
In future work, we will focus on exploring
more contextual discourse information via the
graph model and better ways of integrating intra-
and inter-sentence information on negation focus
identification.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999227153846154">
This research is supported by the National Natu-
ral Science Foundation of China, No.61272260,
No.61331011, No.61273320, the Natural Science
Foundation of Jiangsu Province, No. BK2011282,
the Major Project of College Natural Science
Foundation of Jiangsu Province,
No.11KIJ520003, and the Graduates Project of
Science and Innovation, No. CXZZ12_0818. The
authors would like to thank the anonymous re-
viewers for their insightful comments and sug-
gestions. Our sincere thanks are also extended to
Dr. Zhongqing Wang for his valuable discus-
sions during this study.
</bodyText>
<sectionHeader confidence="0.983186" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.997810928571428">
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
581-589, Portland, Oregon, June 19-24, 2011.
Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated find-
ings and diseases in discharge summaries. Journal
of Biomedical Informatics, 34:301-310.
Marie-Catherine De Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In Proceedings of LREC’2006.
Yun Ding. 2011. Implied Negation in Discourse.
Journal of Theory and Practice in Language Stud-
ies, 1(1): 44-51, Jan 2011.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local infor-
mation into information extraction systems by
gibbs sampling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 363-370, Stroudsburg, PA, USA.
Tom Griffiths. 2002. Gibbs sampling in the generative
model of Latent Dirichlet Allocation. Tech. rep.,
Stanford University.
Laurence R Horn. 1989. A Natural History of Nega-
tion. Chicago University Press, Chicago, IL.
</reference>
<page confidence="0.984744">
529
</page>
<reference confidence="0.999801855263158">
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan
Zhu. 2009. Answering Opinion Questions with
Random Walks on Graphs. In Proceedings of the
47th Annual Meeting of the ACL and the 4th
IJCNLP of the AFNLP, pages 737-745, Suntec,
Singapore, 2-7 Aug 2009.
Junhui Li, Guodong Zhou, Hongling Wang, and Qi-
aoming Zhu. 2010. Learning the Scope of Negation
via Shallow Semantic Parsing. In Proceedings of
the 23rd International Conference on Computa-
tional Linguistics. Stroudsburg, PA, USA: Associa-
tion for Computational Linguistics, 671-679.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo,
Yoshinobu Kano, and Jun&apos;ichi Tsujii. 2009. Over-
view of BioNLP&apos;09 Shared Task on Event Extrac-
tion. In Proceedings of the BioNLP&apos;2009 Workshop
Companion Volume for Shared Task. Stroudsburg,
PA, USA: Association for Computational Linguis-
tics, 1-9.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423-430.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39-41.
Roser Morante, Anthony Liekens and Walter Daele-
mans. 2008. Learning the Scope of Negation in Bi-
omedical Texts. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 715-724, Honolulu, Oc-
tober 2008.
Roser Morante and Caroline Sporleder, editors. 2010.
In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing. Uni-
versity of Antwerp, Uppsala, Sweden.
Roser Morante and Eduardo Blanco. 2012. *SEM
2012 Shared Task: Resolving the Scope and Focus
of Negation. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics
(*SEM), pages 265-274, Montreal, Canada, June 7-
8, 2012.
Roser Morante and Caroline Sporleder. 2012. Modali-
ty and Negation: An Introduction to the Special Is-
sue. Computational Linguistics, 2012, 38(2): 223-
260.
Roser Morante and Walter Daelemans. 2012. Conan
Doyle-neg: Annotation of negation cues and their
scope in Conan Doyle stories. In Proceedings of
LREC 2012, Istambul.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference
in semantic role labeling. Computational Linguis-
tics, 34(2):257-287, June.
Sabine Rosenberg and Sabine Bergler. 2012. UCon-
cordia: CLaC Negation Focus Detection at *Sem
2012. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 294-300, Montreal, Canada, June 7-8, 2012.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity, and Multiple Negation.
Routledge, London.
Veronika Vincze, György Szarvas, Richárd Farkas,
György Móra, and János Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty,negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9.
Xiaojun Wan and Jianwu Yang. 2008. Multi-
document summarization using cluster-based link
analysis. In Proceedings of the 31st annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 299-
306.
</reference>
<page confidence="0.997097">
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.590969">
<title confidence="0.999664">Negation Focus Identification with Contextual Discourse Information</title>
<author confidence="0.994468">Zou Qiaoming Zhu Guodong</author>
<affiliation confidence="0.991492">Natural Language Processing Lab, School of Computer Science and</affiliation>
<address confidence="0.770527">Soochow University, Suzhou, 215006, China</address>
<email confidence="0.988898">zoubowei@gmail.com,{qmzhu,gdzhou}@suda.edu.cn</email>
<abstract confidence="0.986705823529412">Negative expressions are common in natural language text and play a critical role in information extraction. However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eduardo Blanco</author>
<author>Dan Moldovan</author>
</authors>
<title>Semantic Representation of Negation Using Focus Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>581--589</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="2541" citStr="Blanco and Moldovan, 2011" startWordPosition="388" endWordPosition="391">mally, negation focus is defined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with rega</context>
<context position="6849" citStr="Blanco and Moldovan (2011)" startWordPosition="1052" endWordPosition="1055">d on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in negation focus identification. The key problem in current research on negation focus identification is its focus on </context>
<context position="8518" citStr="Blanco and Moldovan (2011)" startWordPosition="1303" endWordPosition="1306">This indicates the great challenges in negation focus identification. 3 Baselines Negation focus identification in *SEM’2012 shared tasks is restricted to verbal negations annotated with MNEG in PropBank, with only the constituent belonging to a semantic role selected as negation focus. Normally, a verbal negation expression (not or n’t) is grammatically associated with its corresponding verb (e.g., He didn’t stop). For details on annotation guidelines and 1 In *SEM’2013, the shared task is changed with focus on &amp;quot;Semantic Textual Similarity&amp;quot;. 523 examples for verbal negations, please refer to Blanco and Moldovan (2011). For comparison, we choose the state-of-the-art system described in Blanco and Moldovan (2011), which employed various kinds of syntactic features and semantic role features, as one of our baselines. Since this system adopted C4.5 for training, we name it as BaselineC4.5. In order to provide a stronger baseline, besides those features adopted in BaselineC4.5, we added more refined intra-sentence features and adopted ranking Support Vector Machine (SVM) model for training. We name it as BaselineSVM. Following is a list of features adopted in the two baselines, for both BaselineC4.5 and Baselin</context>
<context position="9830" citStr="Blanco and Moldovan (2011)" startWordPosition="1525" endWordPosition="1528">date; the number of tokens in the focus candidate; relative position of the focus candidate among all the roles present in the sentence; negated verb and its POS tag of the negative expression; ➢ Syntactic features: the sequence of words from the beginning of the governing VP to the negated verb; the sequence of POS tags from the beginning of the governing VP to the negated verb; whether the governing VP contains a CC; whether the governing VP contains a RB. ➢ Semantic features: the syntactic label of semantic role A1; whether A1 contains POS tag DT, JJ, PRP, CD, RB, VB, and WP, as defined in Blanco and Moldovan (2011); whether A1 contains token any, anybody, anymore, anyone, anything, anytime, anywhere, certain, enough, full, many, much, other, some, specifics, too, and until, as defined in Blanco and Moldovan (2011); the syntactic label of the first semantic role in the sentence; the semantic label of the last semantic role in the sentence; the thematic role for A0/A1/A2/A3/A4 of the negated predicate. and for BaselineSVM only, ➢ Basic features: the named entity and its type in the focus candidate; relative position of the focus candidate to the negative expression (before or after). ➢ Syntactic features:</context>
<context position="19942" citStr="Blanco and Moldovan, 2011" startWordPosition="3128" endWordPosition="3131">e effectiveness of contextual discourse information. 5.1 Experimental Settings Dataset In all our experiments, we employ the *SEM&apos;2012 shared task corpus (Morante and Blanco, 2012)2. As a freely downloadable resource, the *SEM shared task corpus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank. In particular, negation focus annotation on this corpus is restricted to verbal negations (with corresponding mark 2 http://www.clips.ua.ac.be/sem2012-st-neg/ 526 MNEG in PropBank). On 50% of the corpus annotated by two annotators, the inter-annotator agreement was 0.72 (Blanco and Moldovan, 2011). Along with negation focus annotation, this corpus also contains other annotations, such as POS tag, named entity, chunk, constituent tree, dependency tree, and semantic role. In total, this corpus provides 3,544 instances of negation focus annotations. For fair comparison, we adopt the same partition as *SEM’2012 shared task in all our experiments, i.e., with 2,302 for training, 530 for development, and 712 for testing. Although for each instance, the corpus only provides the current sentence, the previous and next sentences as its context, we sort to the Penn TreeBank3 to obtain the corresp</context>
<context position="21240" citStr="Blanco and Moldovan (2011)" startWordPosition="3334" endWordPosition="3338"> *SEM&apos;2012 shared task, the evaluation is made using precision, recall, and F1-score. Especially, a true positive (TP) requires an exact match for the negation focus, a false positive (FP) occurs when a system predicts a non-existing negation focus, and a false negative (FN) occurs when the gold annotations specify a negation focus but the system makes no prediction. For each instance, the predicted focus is considered correct if it is a complete match with a gold annotation. Beside, to show whether an improvement is significant, we conducted significance testing using z-test, as described in Blanco and Moldovan (2011). Toolkits In our experiments, we report not only the default performance with gold additional annotated features provided by the *SEM&apos;2012 shared task corpus and the Penn TreeBank, but also the performance with various kinds of features extracted automatically, using following toolkits: ➢ Syntactic Parser: We employ the Stanford Parser4 (Klein and Manning, 2003; De Marneffe et al., 2006) for tokenization, constituent and dependency parsing. ➢ Named Entity Recognizer: We employ the Stanford NER5 (Finkel et al., 2005) to obtain named entities. 3 http://www.cis.upenn.edu/~treebank/ 4 http://nlp.</context>
<context position="28626" citStr="Blanco and Moldovan, 2011" startWordPosition="4473" endWordPosition="4476">rse. If there lacks relevant information in a discourse context, negation focus will become difficult to be identified only by inter-sentence features. Error analysis also shows that some of the negation focuses are very difficult to be identified, even for a human being. Consider the sentence (3) in Section 1, if given sentence because of her neighbors&apos; protests, but her husband doesn’t think so as its following context, both Helen and to play the violin can become the negation focus. Moreover, the inter-annotator agreement in the first round of negation focus annotation can only reach 0.72 (Blanco and Moldovan, 2011). This indicates inherent difficulty in negation focus identification. 6 Conclusion In this paper, we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives. In this graph model, the relatedness between words is calculated by word co-occurrence, WordNetbased similarity, and topic-driven similarity. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information on negation focus identification and our graph model in capturing such global information. In future work, we will focus </context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>Eduardo Blanco and Dan Moldovan. 2011. Semantic Representation of Negation Using Focus Detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 581-589, Portland, Oregon, June 19-24, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>34--301</pages>
<contexts>
<context position="5308" citStr="Chapman et al. (2001)" startWordPosition="815" endWordPosition="818">llows. Section 2 overviews the related work. Section 3 presents several strong baselines on negation focus identification with only intra-sentence features. Section 4 introduces our topic-driven word-based graph model with contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determine</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 34:301-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’2006.</booktitle>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of LREC’2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Ding</author>
</authors>
<title>Implied Negation in Discourse.</title>
<date>2011</date>
<journal>Journal of Theory and Practice in Language Studies,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>44--51</pages>
<contexts>
<context position="7627" citStr="Ding (2011)" startWordPosition="1171" endWordPosition="1172">lthough Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in negation focus identification. The key problem in current research on negation focus identification is its focus on intrasentence information and large ignorance of inter-sentence information, which plays a critical role in the success of negation focus identification. For example, Ding (2011) made a qualitative analysis on implied negations in conversation and attempted to determine whether a sentence was negated by context information, from the linguistic perspective. Moreover, a negation focus is always associated with authors’ intention in article. This indicates the great challenges in negation focus identification. 3 Baselines Negation focus identification in *SEM’2012 shared tasks is restricted to verbal negations annotated with MNEG in PropBank, with only the constituent belonging to a semantic role selected as negation focus. Normally, a verbal negation expression (not or </context>
</contexts>
<marker>Ding, 2011</marker>
<rawString>Yun Ding. 2011. Implied Negation in Discourse. Journal of Theory and Practice in Language Studies, 1(1): 44-51, Jan 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21762" citStr="Finkel et al., 2005" startWordPosition="3415" endWordPosition="3418">ficant, we conducted significance testing using z-test, as described in Blanco and Moldovan (2011). Toolkits In our experiments, we report not only the default performance with gold additional annotated features provided by the *SEM&apos;2012 shared task corpus and the Penn TreeBank, but also the performance with various kinds of features extracted automatically, using following toolkits: ➢ Syntactic Parser: We employ the Stanford Parser4 (Klein and Manning, 2003; De Marneffe et al., 2006) for tokenization, constituent and dependency parsing. ➢ Named Entity Recognizer: We employ the Stanford NER5 (Finkel et al., 2005) to obtain named entities. 3 http://www.cis.upenn.edu/~treebank/ 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://nlp.stanford.edu/ner/ ➢ Semantic Role Labeler: We employ the semantic role labeler, as described in Punyakanok et al (2008). ➢ Topic Modeler: For estimating transition probability Pt(i,m), we employ GibbsLDA++6, an LDA model using Gibbs Sampling technique for parameter estimation and inference. ➢ Classifier: We employ SVMLight 7 with default parameters as our classifier. 5.2 Experimental Results With Only Intra-sentence Information Table 3 shows the performance of the t</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363-370, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
</authors>
<title>Gibbs sampling in the generative model of Latent Dirichlet Allocation.</title>
<date>2002</date>
<tech>Tech. rep.,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="16161" citStr="Griffiths, 2002" startWordPosition="2519" endWordPosition="2520">Model While the above word-based graph model can well capture the relatedness between content words, it can only partially model the focus of a negation expression since negation focus is more directly related with topic than content. In order to reduce the gap, we propose a topic-driven word-based model by adding one more layer to refine the word-based graph model over the global topic distribution, as shown in Figure 2. Figure 2. Topic-driven word-based graph model. 525 Here, the topics are extracted from all the documents in the *SEM 2012 shared task using the LDA Gibbs Sampling algorithm (Griffiths, 2002). In the topic-driven word-based graph model, the first layer denotes the relatedness among content words as captured in the above word-based graph model, and the second layer denotes the topic distribution, with the dashed lines between these two layers indicating the word-topic model return by LDA. Formally, the topic-driven word-based twolayer graph is defined as Gtopic (W, T, Ew, Et), where W={wi} is the set of words in one document and T={ti} is the set of topics in all documents; Ew={ewij|wi, wj EW} is the set of directed edges between words and Et ={etij|wiEW, tj ET} is the set of undir</context>
</contexts>
<marker>Griffiths, 2002</marker>
<rawString>Tom Griffiths. 2002. Gibbs sampling in the generative model of Latent Dirichlet Allocation. Tech. rep., Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence R Horn</author>
</authors>
<title>A Natural History of Negation.</title>
<date>1989</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="5131" citStr="Horn, 1989" startWordPosition="786" endWordPosition="787">luation on the *SEM 2012 shared task corpus (Morante and Blanco, 2012) justifies our approach over several strong baselines. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 presents several strong baselines on negation focus identification with only intra-sentence features. Section 4 introduces our topic-driven word-based graph model with contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Ge</context>
</contexts>
<marker>Horn, 1989</marker>
<rawString>Laurence R Horn. 1989. A Natural History of Negation. Chicago University Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Yang Tang</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Answering Opinion Questions with Random Walks on Graphs.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>737--745</pages>
<location>Suntec,</location>
<contexts>
<context position="13133" citStr="Li et al., 2009" startWordPosition="2018" endWordPosition="2021">intra-sentence information provides the local features from lexical, syntactic and semantic perspectives, both have their own contributions on negation focus identification. In this paper, we first propose a graph model to gauge the importance of contextual discourse 524 information. Then, we incorporate both intraand inter-sentence features into a machine learning-based framework for negation focus identification. 4.1 Graph Model Graph models have been proven successful in many NLP applications, especially in representing the link relationships between words or sentences (Wan and Yang, 2008; Li et al., 2009). Generally, such models could construct a graph to compute the relevance between document theme and words. In this paper, we propose a graph model to represent the contextual discourse information from both lexical and topic perspectives. In particular, a word-based graph model is proposed to represent the explicit relatedness among words in a discourse from the lexical perspective, while a topic-driven word-based model is proposed to enrich the implicit relatedness between words, by adding one more layer to the word-based graph model in representing the global topic distribution of the whole</context>
</contexts>
<marker>Li, Tang, Huang, Zhu, 2009</marker>
<rawString>Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu. 2009. Answering Opinion Questions with Random Walks on Graphs. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737-745, Suntec, Singapore, 2-7 Aug 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhui Li</author>
<author>Guodong Zhou</author>
<author>Hongling Wang</author>
<author>Qiaoming Zhu</author>
</authors>
<title>Learning the Scope of Negation via Shallow Semantic Parsing.</title>
<date>2010</date>
<journal>Association for Computational Linguistics,</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<pages>671--679</pages>
<location>Stroudsburg, PA, USA:</location>
<contexts>
<context position="5727" citStr="Li et al., 2010" startWordPosition="879" endWordPosition="882">ics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more at</context>
</contexts>
<marker>Li, Zhou, Wang, Zhu, 2010</marker>
<rawString>Junhui Li, Guodong Zhou, Hongling Wang, and Qiaoming Zhu. 2010. Learning the Scope of Negation via Shallow Semantic Parsing. In Proceedings of the 23rd International Conference on Computational Linguistics. Stroudsburg, PA, USA: Association for Computational Linguistics, 671-679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>Overview of BioNLP&apos;09 Shared Task on Event Extraction.</title>
<date>2009</date>
<journal>Association for Computational Linguistics,</journal>
<booktitle>In Proceedings of the BioNLP&apos;2009 Workshop Companion Volume for Shared Task.</booktitle>
<pages>1--9</pages>
<location>Stroudsburg, PA, USA:</location>
<contexts>
<context position="6539" citStr="Kim et al., 2009" startWordPosition="1005" endWordPosition="1008">cal scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one tea</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun&apos;ichi Tsujii. 2009. Overview of BioNLP&apos;09 Shared Task on Event Extraction. In Proceedings of the BioNLP&apos;2009 Workshop Companion Volume for Shared Task. Stroudsburg, PA, USA: Association for Computational Linguistics, 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="21604" citStr="Klein and Manning, 2003" startWordPosition="3390" endWordPosition="3393">n. For each instance, the predicted focus is considered correct if it is a complete match with a gold annotation. Beside, to show whether an improvement is significant, we conducted significance testing using z-test, as described in Blanco and Moldovan (2011). Toolkits In our experiments, we report not only the default performance with gold additional annotated features provided by the *SEM&apos;2012 shared task corpus and the Penn TreeBank, but also the performance with various kinds of features extracted automatically, using following toolkits: ➢ Syntactic Parser: We employ the Stanford Parser4 (Klein and Manning, 2003; De Marneffe et al., 2006) for tokenization, constituent and dependency parsing. ➢ Named Entity Recognizer: We employ the Stanford NER5 (Finkel et al., 2005) to obtain named entities. 3 http://www.cis.upenn.edu/~treebank/ 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://nlp.stanford.edu/ner/ ➢ Semantic Role Labeler: We employ the semantic role labeler, as described in Punyakanok et al (2008). ➢ Topic Modeler: For estimating transition probability Pt(i,m), we employ GibbsLDA++6, an LDA model using Gibbs Sampling technique for parameter estimation and inference. ➢ Classifier: We emp</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<pages>38--11</pages>
<contexts>
<context position="15164" citStr="Miller, 1995" startWordPosition="2355" endWordPosition="2356"> initialized to 1. In addition, directed edge eij is weighted to represent the relatedness between word wi and word wj in a document with transition probability P(j|i) from i to j, which is normalized as follows: ௌሺ௪,௪ೕሻ ܲሺ݆|݅ሻ ൌ ∑ೖௌሺ௪,௪ೖሻ (1) where k represents the nodes in discourse, and Sim(wi,wj) denotes the similarity between wi and wj. In this paper, two kinds of information are used to calculate the similarity between words. One is word co-occurrence (if word wi and word wj occur in the same sentence or in the adjacent sentences, Sim(wi,wj) increases 1), and the other is WordNet (Miller, 1995) based similarity. Please note that Sim(wi,wi) = 0 to avoid selftransition, and Sim(wi,wj) and Sim(wj,wi) may not be equal. Finally, the weights of word nodes are calculated using the PageRank algorithm as follows: ܵܿݎ݁ሺሻሺݓሻ ൌ 1 ܵܿݎ݁ሺାଵሻሺݓሻ ൌ ݀ ∑ஷ ܵܿݎ݁ሺሻ൫ݓ൯ ൈܲሺ݆|݅ሻ  ሺ1 െ ݀ሻ (2) where d is the damping factor as in the PageRank algorithm. Topic-driven Word-based Graph Model While the above word-based graph model can well capture the relatedness between content words, it can only partially model the focus of a negation expression since negation focus is more directly related with topi</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Commun. ACM, 38(11):39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Anthony Liekens</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the Scope of Negation in Biomedical Texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>715--724</pages>
<location>Honolulu,</location>
<contexts>
<context position="5709" citStr="Morante et al., 2008" startWordPosition="875" endWordPosition="878">ere almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawin</context>
</contexts>
<marker>Morante, Liekens, Daelemans, 2008</marker>
<rawString>Roser Morante, Anthony Liekens and Walter Daelemans. 2008. Learning the Scope of Negation in Biomedical Texts. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 715-724, Honolulu, October 2008.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing. University of Antwerp,</booktitle>
<editor>Roser Morante and Caroline Sporleder, editors.</editor>
<location>Uppsala, Sweden.</location>
<marker>2010</marker>
<rawString>Roser Morante and Caroline Sporleder, editors. 2010. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing. University of Antwerp, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>SEM 2012 Shared Task: Resolving the Scope and Focus of Negation.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>265--274</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="4591" citStr="Morante and Blanco, 2012" startWordPosition="699" endWordPosition="702">s previous sentence, the negation focus should be play the violin, yielding interpretation Helen didn’t allow her youngest son to play the violin, but it didn’t show whether he was allowed to do other things. In this paper, to well accommodate such contextual discourse information in negation focus identification, we propose a graph model to enrich normal intra-sentence features with various kinds of inter-sentence features from both lexical and topic perspectives. Besides, the standard PageRank algorithm is employed to optimize the graph model. Evaluation on the *SEM 2012 shared task corpus (Morante and Blanco, 2012) justifies our approach over several strong baselines. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 presents several strong baselines on negation focus identification with only intra-sentence features. Section 4 introduces our topic-driven word-based graph model with contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natura</context>
<context position="7049" citStr="Morante and Blanco (2012)" startWordPosition="1081" endWordPosition="1084">detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in negation focus identification. The key problem in current research on negation focus identification is its focus on intrasentence information and large ignorance of inter-sentence information, which plays a critical role in the success of negation focus identification. For example, Ding (2011) made a qualitative an</context>
<context position="19496" citStr="Morante and Blanco, 2012" startWordPosition="3061" endWordPosition="3064">tence features extracted from graph model. For evaluating the contribution of contextual discourse information on negation focus identification directly, we incorporate the four intersentence features from the topic-driven wordbased graph model into a negation focus identifier. 5 Experimentation In this section, we describe experimental settings and systematically evaluate our negation focus identification approach with focus on exploring the effectiveness of contextual discourse information. 5.1 Experimental Settings Dataset In all our experiments, we employ the *SEM&apos;2012 shared task corpus (Morante and Blanco, 2012)2. As a freely downloadable resource, the *SEM shared task corpus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank. In particular, negation focus annotation on this corpus is restricted to verbal negations (with corresponding mark 2 http://www.clips.ua.ac.be/sem2012-st-neg/ 526 MNEG in PropBank). On 50% of the corpus annotated by two annotators, the inter-annotator agreement was 0.72 (Blanco and Moldovan, 2011). Along with negation focus annotation, this corpus also contains other annotations, such as POS tag, named entity, chunk, constituent tree, dependency tr</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. *SEM 2012 Shared Task: Resolving the Scope and Focus of Negation. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 265-274, Montreal, Canada, June 7-8, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Caroline Sporleder</author>
</authors>
<title>Modality and Negation: An Introduction to the Special Issue. Computational Linguistics,</title>
<date>2012</date>
<volume>38</volume>
<issue>2</issue>
<pages>223--260</pages>
<contexts>
<context position="1108" citStr="Morante and Sporleder, 2012" startWordPosition="156" endWordPosition="159">ocus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information. 1 Introduction Negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition (Morante and Sporleder, 2012). For example, sentence (1) could be interpreted as it is not the case that he stopped. (1) He didn&apos;t stop. Negation expressions are common in natural language text. According to the statistics on biomedical literature genre (Vincze et al., 2008), 19.44% of sentences contain negative expressions. The percentage rises to 22.5% on Conan Doyle stories (Morante and Daelemans, 2012). It is interesting that a negative sentence may have both negative and positive meanings. For example, sentence (2) could be interpreted as He stopped, but not until he got to Jackson Hole with positive part he stopped </context>
<context position="6732" citStr="Morante and Sporleder, 2012" startWordPosition="1035" endWordPosition="1038">ve expression. This paper concentrates on the third subtask, negation focus identification. Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al., 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in</context>
</contexts>
<marker>Morante, Sporleder, 2012</marker>
<rawString>Roser Morante and Caroline Sporleder. 2012. Modality and Negation: An Introduction to the Special Issue. Computational Linguistics, 2012, 38(2): 223-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Conan Doyle-neg: Annotation of negation cues and their scope in Conan Doyle stories.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC 2012, Istambul.</booktitle>
<contexts>
<context position="1488" citStr="Morante and Daelemans, 2012" startWordPosition="217" endWordPosition="220">ustifies the effectiveness of our graph model in capturing such global information. 1 Introduction Negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition (Morante and Sporleder, 2012). For example, sentence (1) could be interpreted as it is not the case that he stopped. (1) He didn&apos;t stop. Negation expressions are common in natural language text. According to the statistics on biomedical literature genre (Vincze et al., 2008), 19.44% of sentences contain negative expressions. The percentage rises to 22.5% on Conan Doyle stories (Morante and Daelemans, 2012). It is interesting that a negative sentence may have both negative and positive meanings. For example, sentence (2) could be interpreted as He stopped, but not until he got to Jackson Hole with positive part he stopped and negative part until he got to Jackson Hole. Moreover, a nega* Corresponding author tive expression normally interacts with some special part in the sentence, referred as negation focus in linguistics. Formally, negation focus is defined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negativ</context>
</contexts>
<marker>Morante, Daelemans, 2012</marker>
<rawString>Roser Morante and Walter Daelemans. 2012. Conan Doyle-neg: Annotation of negation cues and their scope in Conan Doyle stories. In Proceedings of LREC 2012, Istambul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="13794" citStr="Page et al., 1998" startWordPosition="2120" endWordPosition="2123">graph to compute the relevance between document theme and words. In this paper, we propose a graph model to represent the contextual discourse information from both lexical and topic perspectives. In particular, a word-based graph model is proposed to represent the explicit relatedness among words in a discourse from the lexical perspective, while a topic-driven word-based model is proposed to enrich the implicit relatedness between words, by adding one more layer to the word-based graph model in representing the global topic distribution of the whole dataset. Besides, the PageRank algorithm (Page et al., 1998) is adopted to optimize the graph model. Word-based Graph Model: A word-based graph model can be defined as Gword (W, E), where W={wi} is the set of words in one document and E={eij|wi, wj ∈W} is the set of directed edges between these words, as shown in Figure 1. Figure 1. Word-based graph model. In the word-based graph model, word node wi is weighted to represent the correlation of the word with authors’ intention. Since such correlation is more from the semantic perspective than the grammatical perspective, only content words are considered in our graph model, ignoring functional words (e.g</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<pages>34--2</pages>
<contexts>
<context position="22011" citStr="Punyakanok et al (2008)" startWordPosition="3443" endWordPosition="3447">ask corpus and the Penn TreeBank, but also the performance with various kinds of features extracted automatically, using following toolkits: ➢ Syntactic Parser: We employ the Stanford Parser4 (Klein and Manning, 2003; De Marneffe et al., 2006) for tokenization, constituent and dependency parsing. ➢ Named Entity Recognizer: We employ the Stanford NER5 (Finkel et al., 2005) to obtain named entities. 3 http://www.cis.upenn.edu/~treebank/ 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 http://nlp.stanford.edu/ner/ ➢ Semantic Role Labeler: We employ the semantic role labeler, as described in Punyakanok et al (2008). ➢ Topic Modeler: For estimating transition probability Pt(i,m), we employ GibbsLDA++6, an LDA model using Gibbs Sampling technique for parameter estimation and inference. ➢ Classifier: We employ SVMLight 7 with default parameters as our classifier. 5.2 Experimental Results With Only Intra-sentence Information Table 3 shows the performance of the two baselines, the decision tree-based classifier as in Blanco and Moldovan (2011) and our ranking SVM-based classifier. It shows that our ranking SVM-based baseline slightly improves the F1- measure by 2.52% over the decision tree-based baseline, la</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257-287, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Rosenberg</author>
<author>Sabine Bergler</author>
</authors>
<title>UConcordia: CLaC Negation Focus Detection at *Sem</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>294--300</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="2571" citStr="Rosenberg and Bergler, 2012" startWordPosition="392" endWordPosition="395">fined as the special part in the sentence, which is most prominently or explicitly negated by a negative expression. Hereafter, we denote negative expression in boldface and negation focus underlined. (2) He didn&apos;t stop until he got to Jackson Hole. While people tend to employ stress or intonation in speech to emphasize negation focus and thus it is easy to identify negation focus in speech corpora, such stress or intonation information often misses in the dominating text corpora. This poses serious challenges on negation focus identification. Current studies (e.g., Blanco and Moldovan, 2011; Rosenberg and Bergler, 2012) sort to various kinds of intra-sentence information, such as lexical features, syntactic features, semantic role features and so on, ignoring less-obvious inter-sentence information. This largely defers the performance of negation focus identification and its wide applications, since such contextual discourse information plays a critical role on negation focus identification. Take following sentence as an example. (3) Helen didn’t allow her youngest son to play the violin. In sentence (3), there are several scenarios on identification of negation focus, with regard to negation expression n’t,</context>
<context position="7170" citStr="Rosenberg and Bergler, 2012" startWordPosition="1100" endWordPosition="1103">d the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. The research on negation focus identification was pioneered by Blanco and Moldovan (2011), who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) 1 participated in this task. They identified negation focus using three kinds of heuristics and achieved 58.40 in F1-measure. This indicates great expectation in negation focus identification. The key problem in current research on negation focus identification is its focus on intrasentence information and large ignorance of inter-sentence information, which plays a critical role in the success of negation focus identification. For example, Ding (2011) made a qualitative analysis on implied negations in conversation and attempted to determine whether a sentence was negated by context informat</context>
<context position="27391" citStr="Rosenberg and Bergler, 2012" startWordPosition="4277" endWordPosition="4281"> systems on negation focus identification with automatically extracted features. 528 Besides, Table 6 shows the performance of our best system with all features automatically extracted using the toolkits as described in Section 5.1. Compared with our best system employing gold additional annotated features (the last line in Table 5), the homologous system with automatically extracted features (the last line in Table 6) only decrease of less than 4 in F1- measure. This demonstrates the achievability of our approach. In comparison with the best-reported performance on the *SEM’2012 shared task (Rosenberg and Bergler, 2012), our system performs better by about 11 in F-measure. 5.3 Discussion While this paper verifies the usefulness of contextual discourse information on negation focus identification, the performance with only intersentence features is still weaker than that with only intra-sentence features. There are two main reasons. On the one hand, the former employs an unsupervised approach without prior knowledge for training. On the other hand, the usefulness of inter-sentence features depends on the assumption that a negation focus relates to the meaning of which is most relevant to authors’ intention in</context>
</contexts>
<marker>Rosenberg, Bergler, 2012</marker>
<rawString>Sabine Rosenberg and Sabine Bergler. 2012. UConcordia: CLaC Negation Focus Detection at *Sem 2012. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pages 294-300, Montreal, Canada, June 7-8, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ton van der Wouden</author>
</authors>
<title>Negative Contexts: Collocation, Polarity, and Multiple Negation.</title>
<date>1997</date>
<location>Routledge, London.</location>
<marker>van der Wouden, 1997</marker>
<rawString>Ton van der Wouden. 1997. Negative Contexts: Collocation, Polarity, and Multiple Negation. Routledge, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>György Szarvas</author>
<author>Richárd Farkas</author>
<author>György Móra</author>
<author>János Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty,negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<contexts>
<context position="1354" citStr="Vincze et al., 2008" startWordPosition="197" endWordPosition="200">EM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information. 1 Introduction Negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition (Morante and Sporleder, 2012). For example, sentence (1) could be interpreted as it is not the case that he stopped. (1) He didn&apos;t stop. Negation expressions are common in natural language text. According to the statistics on biomedical literature genre (Vincze et al., 2008), 19.44% of sentences contain negative expressions. The percentage rises to 22.5% on Conan Doyle stories (Morante and Daelemans, 2012). It is interesting that a negative sentence may have both negative and positive meanings. For example, sentence (2) could be interpreted as He stopped, but not until he got to Jackson Hole with positive part he stopped and negative part until he got to Jackson Hole. Moreover, a nega* Corresponding author tive expression normally interacts with some special part in the sentence, referred as negation focus in linguistics. Formally, negation focus is defined as th</context>
<context position="5526" citStr="Vincze et al., 2008" startWordPosition="849" endWordPosition="852">ith contextual discourse information. Section 5 reports the experimental results and analysis. Finally, we conclude our work in Section 6. 2 Related Work Earlier studies of negation were almost in linguistics (e.g. Horn, 1989; van der Wouden, 1997), and there were only a few in natural language processing with focus on negation recognition in the biomedical domain. For example, Chapman et al. (2001) developed a rule-based negation recognition system, NegEx, to determine whether a finding mentioned within narrative medical reports is present or absent. Since the release of the BioScope corpus (Vincze et al., 2008), a freely available resource consisting of medical and biological texts, machine learning approaches begin to dominate the research on negation recognition (e.g. Morante et al., 2008; Li et al., 2010). Generally, negation recognition includes three subtasks: cue detection, which detects and identifies possible negative expressions in a sentence, scope resolution, which determines the grammatical scope in a sentence affected by a negative expression, and focus identification, which identifies the constituent in a sentence most prominently or explicitly negated by a negative expression. This pa</context>
</contexts>
<marker>Vincze, Szarvas, Farkas, Móra, Csirik, 2008</marker>
<rawString>Veronika Vincze, György Szarvas, Richárd Farkas, György Móra, and János Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty,negation and their scopes. BMC Bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multidocument summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="13115" citStr="Wan and Yang, 2008" startWordPosition="2014" endWordPosition="2017">ext perspective and intra-sentence information provides the local features from lexical, syntactic and semantic perspectives, both have their own contributions on negation focus identification. In this paper, we first propose a graph model to gauge the importance of contextual discourse 524 information. Then, we incorporate both intraand inter-sentence features into a machine learning-based framework for negation focus identification. 4.1 Graph Model Graph models have been proven successful in many NLP applications, especially in representing the link relationships between words or sentences (Wan and Yang, 2008; Li et al., 2009). Generally, such models could construct a graph to compute the relevance between document theme and words. In this paper, we propose a graph model to represent the contextual discourse information from both lexical and topic perspectives. In particular, a word-based graph model is proposed to represent the explicit relatedness among words in a discourse from the lexical perspective, while a topic-driven word-based model is proposed to enrich the implicit relatedness between words, by adding one more layer to the word-based graph model in representing the global topic distrib</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multidocument summarization using cluster-based link analysis. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 299-306.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>