<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010286">
<title confidence="0.947352">
Indian Institute of Technology-Patna: Sentiment Analysis in Twitter
</title>
<author confidence="0.992513">
Vikram Singh, Arif Md. Khan and Asif Ekbal
</author>
<affiliation confidence="0.882207">
Indian Institute of Technology Patna
Patna, India
</affiliation>
<email confidence="0.983105">
(vikram.mtcs13,arif.mtmc13,asif)@iitp.ac.in
</email>
<sectionHeader confidence="0.993511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999553666666667">
This paper is an overview of the system
submitted to the SemEval-2014 shared
task on sentiment analysis in twitter. For
the very first time we participated in both
the tasks, viz contextual polarity disam-
biguation and message polarity classifi-
cation. Our approach is supervised in
nature and we use sequential minimal
optimization classifier. We implement
the features for sentiment analysis with-
out using deep domain-specific resources
and/or tools. Experiments within the
benchmark setup of SemEval-14 shows
the F-scores of 77.99%, 75.99%, 76.54
%, 76.43% and 71.43% for LiveJour-
nal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respec-
tively for Subtask A. For Subtask B we
obtain the F-scores of 60.39%, 51.96%,
52.58%, 57.25%, 41.33% for five different
test sets, respectively.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.961621103448276">
In current era microblogging is an efficient way
of communication where people can communicate
without physical presence of receiver(s). Twitter
is the medium where people post real time mes-
sages to discuss on the different topics, and ex-
press their sentiments. The texts used in twit-
ter are generally informal and unstructured in na-
ture. Tweets and SMS messages are very short
in length, usually a sentence or a headline rather
than a document. These texts are very informal
in nature and contains creative spellings and punc-
tuation symbols. Text also contains lots of mis-
spellings, slang, out-of-vocabulary words, URLs,
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
and genre-specific terminology and abbreviations,
e.g., RT for re-Tweet and #hashtags. Such kinds of
structures introduce difficulties in building various
lexical and syntactic resources and/or tools, which
are required for efficient processing of texts. Find-
ing relevant information from these posts poses
big challenges to the researchers compared to the
traditional text genres such as newswire.
In recent times, there has been a huge inter-
est to mine and understand the opinions and sen-
timents that people are communicating in social
media (Barbosa and Feng, 2010; Bifet et al.,
; Pak and Paroubek, 2010; Kouloumpis et al.,
2011). There is a tremendous interest in sentiment
analysis of Tweets across a variety of domains
such as commerce (Jansen et al., 2009), health
(Chew and Eysenbach, 2010; Salathe and Khan-
delwal, 2011) and disaster management (Verma
et al., 2011; Mandel et al., 2012). Agarwal et
al.(Agarwal et al., 2011) used tree kernel decision
tree that made use of the features such as Part-
of-Speech (PoS) information, lexicon-based fea-
tures and several other features. They acquired
11,875 manually annotated Twitter data (Tweets)
from a commercial source, and reported an accu-
racy of 75.39%. Semantics has also been used as
the feature to improve the performance of senti-
ment analysis (Saif et al., 2012). For each ex-
tracted entity (e.g. iPhone) from Tweets, they
added its semantic concept (e.g. Apple product)
as an additional feature. Thereafter they devised
a method to measure the correlation of the rep-
resentative concept with negative/positive senti-
ment, and applied this approach to predict sen-
timent for three different Twitter datasets. They
showed that semantic features produce better re-
call and F-score when classifying negative senti-
ment, and better precision with lower recall and
F-score in positive sentiment classification. The
benchmark corpus were made available with the
SemEval-2013 shared task (Nakov et al., 2013) on
</bodyText>
<page confidence="0.984625">
341
</page>
<note confidence="0.7300425">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 341–345,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999917">
sentiment analysis in twitter. The datasets used are
from the domains of Tweets and SMS messages.
The datasets were labelled with contextual phrase-
level polarity and overall message-level polarity.
Among the 44 submissions, the support vector ma-
chine based system proposed in (Mohammad et
al., 2013) achieved the highest F-scores of 69.02%
for Task A, i.e. the message-level polarity and and
88.93% for Task B, i.e. term-level polarity.
The issues addressed in SemEval-13 are further
extended in SemEval-14 shared task 1. The same
two tasks, viz. Subtask A and Subtask B denot-
ing contextual polarity disambiguation and mes-
sage polarity classification. The goal of Subtask
A is to determine, for a given message containing
a marked instance of a word or phrase, whether
that instance is positive, negative or neutral in that
context. Given a message, the task is to classify
it with its entirety whether it is positive, negative,
or neutral sentiment. For messages that convey
both positive and negative sentiments, the stronger
one should be chosen. In this paper we report
on our submitted systems for both the tasks. Our
evaluation for the first task shows the F-scores of
77.99%, 75.99%, 76.54%, 76.43% and 71.43% for
LiveJournal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respectively for
Subtask A. For Subtask B we obtain the F-scores
of 60.39%, 51.96%, 52.58%, 57.25%, 41.33% for
five different test sets, respectively.
</bodyText>
<sectionHeader confidence="0.988494" genericHeader="method">
2 Methods
</sectionHeader>
<bodyText confidence="0.999688">
In this section we describe preprocessing steps,
features and our methods for sentiment classifica-
tion
</bodyText>
<subsectionHeader confidence="0.999916">
2.1 Preprocessing of Data
</subsectionHeader>
<bodyText confidence="0.999932">
The data has to be pre-processed before being used
for actual machine learning training. Each Tweet
is processed to extract only those relevant parts
that are useful for sentiment classification. For
example, stop words are removed; symbols and
punctuation markers are filtered out; URLs are
replaced by the word URL etc. Each Tweet is
then passed through the ARK tagger developed by
CMU 2 for tokenization and Part-of-Speech (PoS)
tagging.
</bodyText>
<footnote confidence="0.9999235">
1http://alt.qcri.org/semeval2014/task9/
2http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<subsectionHeader confidence="0.996397">
2.2 Approach
</subsectionHeader>
<bodyText confidence="0.999978818181818">
Our approach is based on supervised machine
learning. We explored different models such as
naive Bayes, decision tree and support vector ma-
chine. Based on the results obtained on the de-
velopment sets we finally select SVM for both
the tasks. We also carried out a number of ex-
periments with the various feature combinations.
Once the model is fixed with certain feature com-
binations, these are finally used for blind evalua-
tion on the test sets for both the tasks. We sub-
mit two runs, one for each task. Both of our sub-
missions were constrained in nature, i.e. we did
not make use of any additional resources and/or
tools to build our systems. We adapt a supervised
machine learning algorithm, namely Support Vec-
tor Machine (Joachims, 1999; Vapnik, 1995). We
use its sequential minimal optimization version for
faster training3. We use the same set of features
for both the tasks. Development sets are used to
identify the best feature combinations for both the
tasks. Default parameters as implemented in Weka
are used for the SVM experiments.
</bodyText>
<subsectionHeader confidence="0.968169">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.999931333333333">
Like any other classification algorithm, features
play an important role for sentiment classifica-
tion. For the very first time we participated in
this kind of task, and therefore had to spend quite
long time in conceptualization and implementa-
tion of the features. We focused on implementing
the features without using any domain-dependent
resources and/or tools. Brief descriptions of the
features that we use are presented below:
</bodyText>
<listItem confidence="0.986590230769231">
• Bag-of-words: Bag-of-words in the expres-
sion or in the entire Tweet is used as the fea-
ture(s).
• SentiWordNet feature: This feature is de-
fined based on the scores assigned to each
word of a Tweet using the SentiWordNet4. A
feature vector of length three is defined. The
scores of all words of the phrase or Tweet is
summed over and normalized in the scale of
3. We define the following three thresholds:
if the score is less than 0.5 then it is treated to
be a negative polarity; for the score above 0.8,
it is assumed to contain positive sentiment;
</listItem>
<footnote confidence="0.996563">
3http://research.microsoft.com/en-
us/um/people/jplatt/smo-book.pdf
4sentiwordnet.isti.cnr.it/
</footnote>
<page confidence="0.993492">
342
</page>
<bodyText confidence="0.997045">
and the polarity is considered to be neutral
for all the other words. Depending upon the
score the corresponding bit of the feature vec-
tor is set.
</bodyText>
<listItem confidence="0.9947304">
• Stop word: If a Tweet/phrase is having more
number of stop words then it most likely con-
tains neutral sentiment. We obtain the stop
words from the Wikipedia5. We assume that
a particular Tweet or phrase most likely bears
a neutral sentiment if 20% of its words be-
long to the category of stop words.
• All Cap Words: This feature is defined to
count the number of capitalized words in an
entire Tweet/phrase. More the number of
capitalized words, more the chances of being
positive or negative sentiment bearing units.
While counting, the words preceded by # are
not considered. We include this with the as-
sumption that the texts written in capitalized
letters express the sentiment strongly.
• Init Cap: The words starting with capital-
ized letter contribute more towards classify-
ing it.
• Percent Cap: This feature is based on the
percentage of capitalized characters in a
Tweet/phrase. If this is more than 75%, then
most likely it is not of neutral type.
• Psmiley (+ve Smiley): Generally people use
smileys to represent their emotions. A smiley
present in a Tweet/phrase directly represents
its sentiment. A feature is defined that takes
the value equal to the number of positive smi-
leys. We make use of the list available at this
page6.
• Nsmiley (-ve Smiley): The value of this fea-
ture is set to the number of negative smileys
present in the Tweet. This list was also ob-
tained from the web7.
• NumberPostive words: This feature takes
the value equal to the number of positive
words present in the Tweet/phrase. We search
the adjective words present in the Tweet in
the SentiWordNet to determine whether it
bears positive sentiment.
</listItem>
<footnote confidence="0.999931">
5http://en.wikipedia.org/wiki/Stop words
6http://en.wikipedia.org/wiki/List of emoticons
7http://en.wikipedia.org/wiki/List of emoticons
</footnote>
<listItem confidence="0.995631391304348">
• NumberNegative words: This feature takes
the value equal to the number of negative
words present in the Tweet/phrase. The
words are again looked at the SentiWordNet
to determine its polarity.
• NumberNeutral words: This feature deter-
mines the number of neutral words present in
the Tweet or phrase. This information is ob-
tained by looking the adjective words in the
SentiWordNet.
• Repeating char: It has been seen that peo-
ple express strong emotion by typing a char-
acter many times in a Tweet. For exam-
ple, happppppppy, hurrrrrey etc. This feature
checks whether the word(s) have at least three
consecutive repeated characters.
• LenTweet: Length of the Tweet is used as
the feature. The value of this feature is set
equal to the number of words present in the
Tweet/phrase.
• Numhash: The value of this feature is set
equal to the number of hashtags present in the
Tweet.
</listItem>
<sectionHeader confidence="0.982496" genericHeader="evaluation">
3 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.983667916666667">
SemEval-2014 shared task is a continuation of the
SemEval-2013 shared task. In 2014 shared task,
datasets from different domains were incorporated
with a wide range of topics, including a mixture of
entities, products and events. Messages relevant to
the topics are selected based on the keywords and
twitter hashtags.
The training set of Task-A has 4,914 positive,
2,592 negative and 384 neutral class instances.
The Task-B training set contains 3,057 positive,
1,200 negative and 3,941 neutral sentiments. De-
velopments sets contain 555, 45 and 365 positive,
negative and neutral sentiments, respectively for
the first task; and 493, 288 and 632 positive,
negative and neutral sentiments, respectively for
the second task. The selected test sets were taken
mainly from the following domains:
LiveJournal2014: 2000 sentences from Live-
Journal blogs;
SMS2013: SMS test from last year-used as a
progress test for comparison;
Twitter2013: Twitter test data from last year-used
as a progress test for comparison;
Twitter2014: A new Twitter test data of 2000
</bodyText>
<page confidence="0.998009">
343
</page>
<table confidence="0.9196718">
Model Avg. F-score
Model-1 75.75
Model-2 72.69
Model-3 75.45
Model-4 75.77
</table>
<tableCaption confidence="0.9585295">
Table 1: Results for Task-A on development set(in
%).
</tableCaption>
<bodyText confidence="0.8655828">
Tweets;
Twitter2014Sarcasm: 100 Tweets that are known
to contain sarcasm.
We build different models by varying the fea-
tures as follows:
</bodyText>
<listItem confidence="0.99770825">
1. Model-1: This model is constructed by
considering the features, ”Repeating char”,
”Numhash”, ”LenTweet”, ”Percent Cap”,
”Init Cap”, ”All Cap”, ”Bag-of-words”,
”Nsmiley”, ”Psmiley”, ”SentiWordNet” and
”Stop Words”.
2. Model-2: This model is constructed by the
features ”Repeating char”, ”Percent Cap”,
</listItem>
<bodyText confidence="0.947994">
”Numhash”, ”LenTweet”, ”Init Cap”,
”All Cap”, ”Bag-of-words”, ”SentiWord-
Net” and ”Stop Words”.
closer investigation to the evaluation results re-
veals that most of the errors are due to the con-
fusions between positive vs. neutral and negative
vs. neutral classes.
Comparisons with the best system(s) submitted
in this shared task show that we are behind ap-
proximately in the range of 6-14% F-score mea-
sures for all the domains for Task-A. Results that
we obtain in Task-B need more attention as these
fall much shorter compared to the best one (in the
the range of 14-18%).
</bodyText>
<table confidence="0.999522117647059">
Features used Classifier Result(Task A) Result(Task B)
SWN +ve LiveJournal2014 LiveJournal2014
SWN -ve 77.99 60.39
SWN neutral SMS2013 SMS2013
#Stop Words 75.99 51.96
#All Cap Words Twitter2013 Twitter2013
#Numhash 76.54 52.58
Len Tweet Twitter2014 Twitter2014
#Init Cap Words 76.43 57.25
% Init Cap SVM T2014S T2014S
#+ve Smiley 71.43 41.33
#-ve Smiley
#+ve Words
#-ve Words
#Neutral Words
#Bag of words
Rep character
</table>
<tableCaption confidence="0.998393">
Table 2: Result on test sets for Task-A and Task-B.
</tableCaption>
<listItem confidence="0.996161222222222">
3. Model-3: This model is built by consid-
ering the features ”Repeating char”, ”Bag-
of-words”, ”SentiWordNet”, ”Nsmiley” and
”Psmiley”.
4. Model-4: The model incorporates the fea-
tures ”Repeating char”, ”Bag-of-words”,
”SentiWordNet”, ”Nsmiley”, ”Psmiley”,
”Stop Words”, ”Numhash”, ”LenTweet”,
”Init Cap” and ”All Cap”.
</listItem>
<bodyText confidence="0.999">
Results on the development set for Task-A are
reported in Table 1 that shows the highest perfor-
mance in Model-4 with the average F-score value
of 75.77%. Thereafter we use this particular fea-
ture combination for training SVM, and to report
the results. Detailed results are reported in Table
2 for both the tasks. It shows 77.99%, 75.99%,
76.54 %, 76.43% and 71.43% F-scores for the
LiveJournal2014, SMS2013, Twitter2013, Twit-
ter2014 and Twitter2014Sarcasm, respectively for
Subtask A. For Subtask B we obtain the F-scores
of 60.39%, 51.96%, 52.58%, 57.25% and 41.33%
for the five different test sets, respectively. A
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999679904761905">
In this paper we report our works as part of our
participation to the SemEval-14 shared task on
sentiment analysis for twitter data. Our systems
were based on supervised classification, where we
fixed SVM to report the test results after conduct-
ing several experiments with different classifiers
on the development data. We implement a set of
features that are applied for both the tasks. Our
runs are constrained in nature, i.e. we did not make
use of any external resources and/or tools. Our re-
sults are quite promising that need further inves-
tigation. A closer analysis to the results suggest
that most of the errors are due to the confusions
between positive vs. neutral and negative vs. neu-
tral classes.
This is our first participation, and within the
short period of time we developed the systems
with reasonable accuracies. There are still many
ways to improve the performance. Possible im-
mediate future extension will be to investigate and
implement more features, specific to the task.
</bodyText>
<page confidence="0.996083">
344
</page>
<bodyText confidence="0.972688166666667">
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based System: Using Twitter for Disambiguat-
ing Sentiment Ambiguous Adjectives. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, SemEval 10, pages 436–439, Los Ange-
les,USA.
</bodyText>
<table confidence="0.6939708">
References
Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and
Rebecca Passonneau. 2011. Sentiment Analysis of
Twitter Data. ACL Workshop on Languages in So-
cial Media LSM-2011, pages 30–38.
</table>
<reference confidence="0.999139294117647">
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics (COLING),
Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald‘a. Detecting Sentiment Change
in Twitter Streaming Data. Journal of Machine
Learning Research - Proceedings Track, 17.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis
of Tweets during the 2009 H1N1 Outbreak. PLoS
ONE, 5(11):e14118+.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter Power: Tweets as
Electronic Word of Mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169–2188.
Thorsten Joachims, 1999. Making Large Scale SVM
Learning Practical, pages 169–184. MIT Press,
Cambridge, MA, USA.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter Sentiment Analysis: The
Good the Bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM, pages 538–541, Barcelona,
Spain.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A Demographic Analysis of Online Senti-
ment during Hurricane Irene. In Proceedings of
the Second Workshop on Language in Social Media,
LSM 12, Stroudsburg.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-art in Sentiment Analysis of Tweets. In Pro-
ceedings of Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 321–327, Atlanta, Geor-
gia.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 Task 2: Sentiment Analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312–
320, Atlanta, Georgia, USA.
Hassan Saif, Yulan He, and Harith Alani. 2012. Se-
mantic Sentiment Analysis of Twitter. In ISWC’12
Proceedings of the 11th International Conference on
the Semantic Web - Volume Part I, pages 508–524.
Marcel Salathe and Shashank Khandelwal. 2011. As-
sessing Vaccination Sentiments with Online Social
Media: Implications for Infectious Disease Dynam-
ics and Control. PLoS Computational Biology,
7(10):e14118+.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Sudha Verma, Sarah Vieweg, William Corvey, Leysia
Palen, James Martin, Martha Palmer, Aaron Schram,
and Kenneth Anderson. 2011. Natural Language
Processing to the Rescue? Extracting Situational
Awareness Tweets during Mass Emergency. In Pro-
ceedings of the AAAI Conference on Weblogs and
Social Media, Velingrad.
</reference>
<page confidence="0.999137">
345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.533764">
<title confidence="0.993992">Indian Institute of Technology-Patna: Sentiment Analysis in Twitter</title>
<author confidence="0.901118">Khan</author>
<author confidence="0.901118">Asif</author>
<affiliation confidence="0.999838">Indian Institute of Technology</affiliation>
<address confidence="0.763231">Patna,</address>
<email confidence="0.949599">(vikram.mtcs13,arif.mtmc13,asif)@iitp.ac.in</email>
<abstract confidence="0.979067954545455">This paper is an overview of the system submitted to the SemEval-2014 shared task on sentiment analysis in twitter. For the very first time we participated in both tasks, contextual polarity disampolarity classifi- Our approach is supervised in nature and we use sequential minimal optimization classifier. We implement the features for sentiment analysis without using deep domain-specific resources and/or tools. Experiments within the benchmark setup of SemEval-14 shows the F-scores of 77.99%, 75.99%, 76.54 %, 76.43% and 71.43% for LiveJournal2014, SMS2013, Twitter2013, Twitter2014 and Twitter2014Sarcasm, respectively for Subtask A. For Subtask B we obtain the F-scores of 60.39%, 51.96%, 52.58%, 57.25%, 41.33% for five different test sets, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust Sentiment Detection on Twitter from Biased and Noisy Data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2429" citStr="Barbosa and Feng, 2010" startWordPosition="355" endWordPosition="358">e details: http://creativecommons.org/licenses/by/4.0/ and genre-specific terminology and abbreviations, e.g., RT for re-Tweet and #hashtags. Such kinds of structures introduce difficulties in building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial </context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust Sentiment Detection on Twitter from Biased and Noisy Data. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), Beijing, China.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Albert Bifet</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Ricard Gavald‘a</author>
</authors>
<title>Detecting Sentiment Change in Twitter Streaming Data.</title>
<journal>Journal of Machine Learning Research - Proceedings Track,</journal>
<volume>17</volume>
<marker>Bifet, Holmes, Pfahringer, Gavald‘a, </marker>
<rawString>Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and Ricard Gavald‘a. Detecting Sentiment Change in Twitter Streaming Data. Journal of Machine Learning Research - Proceedings Track, 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Chew</author>
<author>Gunther Eysenbach</author>
</authors>
<title>Pandemics in the Age of Twitter: Content Analysis</title>
<date>2010</date>
<booktitle>of Tweets during the 2009 H1N1 Outbreak. PLoS ONE,</booktitle>
<pages>5--11</pages>
<contexts>
<context position="2660" citStr="Chew and Eysenbach, 2010" startWordPosition="394" endWordPosition="397">ic resources and/or tools, which are required for efficient processing of texts. Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone) from Tweets, they added its seman</context>
</contexts>
<marker>Chew, Eysenbach, 2010</marker>
<rawString>Cynthia Chew and Gunther Eysenbach. 2010. Pandemics in the Age of Twitter: Content Analysis of Tweets during the 2009 H1N1 Outbreak. PLoS ONE, 5(11):e14118+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter Power: Tweets as Electronic Word of Mouth.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="2626" citStr="Jansen et al., 2009" startWordPosition="389" endWordPosition="392">ng various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone)</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter Power: Tweets as Electronic Word of Mouth. Journal of the American Society for Information Science and Technology, 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large Scale SVM Learning Practical,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="6819" citStr="Joachims, 1999" startWordPosition="1048" endWordPosition="1049">port vector machine. Based on the results obtained on the development sets we finally select SVM for both the tasks. We also carried out a number of experiments with the various feature combinations. Once the model is fixed with certain feature combinations, these are finally used for blind evaluation on the test sets for both the tasks. We submit two runs, one for each task. Both of our submissions were constrained in nature, i.e. we did not make use of any additional resources and/or tools to build our systems. We adapt a supervised machine learning algorithm, namely Support Vector Machine (Joachims, 1999; Vapnik, 1995). We use its sequential minimal optimization version for faster training3. We use the same set of features for both the tasks. Development sets are used to identify the best feature combinations for both the tasks. Default parameters as implemented in Weka are used for the SVM experiments. 2.3 Features Like any other classification algorithm, features play an important role for sentiment classification. For the very first time we participated in this kind of task, and therefore had to spend quite long time in conceptualization and implementation of the features. We focused on im</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims, 1999. Making Large Scale SVM Learning Practical, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter Sentiment Analysis: The Good the Bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM,</booktitle>
<pages>538--541</pages>
<location>Barcelona,</location>
<contexts>
<context position="2495" citStr="Kouloumpis et al., 2011" startWordPosition="367" endWordPosition="370">-specific terminology and abbreviations, e.g., RT for re-Tweet and #hashtags. Such kinds of structures introduce difficulties in building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also bee</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter Sentiment Analysis: The Good the Bad and the OMG! In Proceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM, pages 538–541, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Mandel</author>
<author>Aron Culotta</author>
<author>John Boulahanis</author>
<author>Danielle Stark</author>
<author>Bonnie Lewis</author>
<author>Jeremy Rodrigue</author>
</authors>
<title>A Demographic Analysis of Online Sentiment during Hurricane Irene.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM 12,</booktitle>
<location>Stroudsburg.</location>
<contexts>
<context position="2757" citStr="Mandel et al., 2012" startWordPosition="410" endWordPosition="413">ormation from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone) from Tweets, they added its semantic concept (e.g. Apple product) as an additional feature. Thereafter they devised a method to me</context>
</contexts>
<marker>Mandel, Culotta, Boulahanis, Stark, Lewis, Rodrigue, 2012</marker>
<rawString>Benjamin Mandel, Aron Culotta, John Boulahanis, Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue. 2012. A Demographic Analysis of Online Sentiment during Hurricane Irene. In Proceedings of the Second Workshop on Language in Social Media, LSM 12, Stroudsburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the Stateof-the-art in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>321--327</pages>
<location>Atlanta,</location>
<contexts>
<context position="4262" citStr="Mohammad et al., 2013" startWordPosition="640" endWordPosition="643">tter precision with lower recall and F-score in positive sentiment classification. The benchmark corpus were made available with the SemEval-2013 shared task (Nakov et al., 2013) on 341 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 341–345, Dublin, Ireland, August 23-24, 2014. sentiment analysis in twitter. The datasets used are from the domains of Tweets and SMS messages. The datasets were labelled with contextual phraselevel polarity and overall message-level polarity. Among the 44 submissions, the support vector machine based system proposed in (Mohammad et al., 2013) achieved the highest F-scores of 69.02% for Task A, i.e. the message-level polarity and and 88.93% for Task B, i.e. term-level polarity. The issues addressed in SemEval-13 are further extended in SemEval-14 shared task 1. The same two tasks, viz. Subtask A and Subtask B denoting contextual polarity disambiguation and message polarity classification. The goal of Subtask A is to determine, for a given message containing a marked instance of a word or phrase, whether that instance is positive, negative or neutral in that context. Given a message, the task is to classify it with its entirety whet</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the Stateof-the-art in Sentiment Analysis of Tweets. In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 321–327, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3818" citStr="Nakov et al., 2013" startWordPosition="574" endWordPosition="577">racted entity (e.g. iPhone) from Tweets, they added its semantic concept (e.g. Apple product) as an additional feature. Thereafter they devised a method to measure the correlation of the representative concept with negative/positive sentiment, and applied this approach to predict sentiment for three different Twitter datasets. They showed that semantic features produce better recall and F-score when classifying negative sentiment, and better precision with lower recall and F-score in positive sentiment classification. The benchmark corpus were made available with the SemEval-2013 shared task (Nakov et al., 2013) on 341 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 341–345, Dublin, Ireland, August 23-24, 2014. sentiment analysis in twitter. The datasets used are from the domains of Tweets and SMS messages. The datasets were labelled with contextual phraselevel polarity and overall message-level polarity. Among the 44 submissions, the support vector machine based system proposed in (Mohammad et al., 2013) achieved the highest F-scores of 69.02% for Task A, i.e. the message-level polarity and and 88.93% for Task B, i.e. term-level polarity. The issues address</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 Task 2: Sentiment Analysis in Twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312– 320, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
<author>Yulan He</author>
<author>Harith Alani</author>
</authors>
<title>Semantic Sentiment Analysis of Twitter. In</title>
<date>2012</date>
<booktitle>ISWC’12 Proceedings of the 11th International Conference on the Semantic Web - Volume Part I,</booktitle>
<pages>508--524</pages>
<contexts>
<context position="3185" citStr="Saif et al., 2012" startWordPosition="479" endWordPosition="482">a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone) from Tweets, they added its semantic concept (e.g. Apple product) as an additional feature. Thereafter they devised a method to measure the correlation of the representative concept with negative/positive sentiment, and applied this approach to predict sentiment for three different Twitter datasets. They showed that semantic features produce better recall and F-score when classifying negative sentiment, and better precision with lower recall and F-score in positive sentiment classification. The benchmark corpus were made available with the SemEval-2013</context>
</contexts>
<marker>Saif, He, Alani, 2012</marker>
<rawString>Hassan Saif, Yulan He, and Harith Alani. 2012. Semantic Sentiment Analysis of Twitter. In ISWC’12 Proceedings of the 11th International Conference on the Semantic Web - Volume Part I, pages 508–524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Salathe</author>
<author>Shashank Khandelwal</author>
</authors>
<title>Assessing Vaccination Sentiments with Online Social Media: Implications for Infectious Disease Dynamics and Control.</title>
<date>2011</date>
<journal>PLoS Computational Biology,</journal>
<volume>7</volume>
<issue>10</issue>
<contexts>
<context position="2691" citStr="Salathe and Khandelwal, 2011" startWordPosition="398" endWordPosition="402"> which are required for efficient processing of texts. Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone) from Tweets, they added its semantic concept (e.g. Apple product</context>
</contexts>
<marker>Salathe, Khandelwal, 2011</marker>
<rawString>Marcel Salathe and Shashank Khandelwal. 2011. Assessing Vaccination Sentiments with Online Social Media: Implications for Infectious Disease Dynamics and Control. PLoS Computational Biology, 7(10):e14118+.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="6834" citStr="Vapnik, 1995" startWordPosition="1050" endWordPosition="1051">ine. Based on the results obtained on the development sets we finally select SVM for both the tasks. We also carried out a number of experiments with the various feature combinations. Once the model is fixed with certain feature combinations, these are finally used for blind evaluation on the test sets for both the tasks. We submit two runs, one for each task. Both of our submissions were constrained in nature, i.e. we did not make use of any additional resources and/or tools to build our systems. We adapt a supervised machine learning algorithm, namely Support Vector Machine (Joachims, 1999; Vapnik, 1995). We use its sequential minimal optimization version for faster training3. We use the same set of features for both the tasks. Development sets are used to identify the best feature combinations for both the tasks. Default parameters as implemented in Weka are used for the SVM experiments. 2.3 Features Like any other classification algorithm, features play an important role for sentiment classification. For the very first time we participated in this kind of task, and therefore had to spend quite long time in conceptualization and implementation of the features. We focused on implementing the </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudha Verma</author>
<author>Sarah Vieweg</author>
<author>William Corvey</author>
<author>Leysia Palen</author>
<author>James Martin</author>
<author>Martha Palmer</author>
<author>Aaron Schram</author>
<author>Kenneth Anderson</author>
</authors>
<title>Natural Language Processing to the Rescue? Extracting Situational Awareness Tweets during Mass Emergency.</title>
<date>2011</date>
<booktitle>In Proceedings of the AAAI Conference on Weblogs and Social</booktitle>
<location>Media, Velingrad.</location>
<contexts>
<context position="2735" citStr="Verma et al., 2011" startWordPosition="406" endWordPosition="409">Finding relevant information from these posts poses big challenges to the researchers compared to the traditional text genres such as newswire. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., ; Pak and Paroubek, 2010; Kouloumpis et al., 2011). There is a tremendous interest in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Verma et al., 2011; Mandel et al., 2012). Agarwal et al.(Agarwal et al., 2011) used tree kernel decision tree that made use of the features such as Partof-Speech (PoS) information, lexicon-based features and several other features. They acquired 11,875 manually annotated Twitter data (Tweets) from a commercial source, and reported an accuracy of 75.39%. Semantics has also been used as the feature to improve the performance of sentiment analysis (Saif et al., 2012). For each extracted entity (e.g. iPhone) from Tweets, they added its semantic concept (e.g. Apple product) as an additional feature. Thereafter they </context>
</contexts>
<marker>Verma, Vieweg, Corvey, Palen, Martin, Palmer, Schram, Anderson, 2011</marker>
<rawString>Sudha Verma, Sarah Vieweg, William Corvey, Leysia Palen, James Martin, Martha Palmer, Aaron Schram, and Kenneth Anderson. 2011. Natural Language Processing to the Rescue? Extracting Situational Awareness Tweets during Mass Emergency. In Proceedings of the AAAI Conference on Weblogs and Social Media, Velingrad.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>