<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001263">
<title confidence="0.985356">
The Effects of Lexical Resource Quality on Preference Violation Detection
</title>
<author confidence="0.997371">
Jesse Dunietz
</author>
<affiliation confidence="0.891083">
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.994869">
jdunietz@cs.cmu.edu
</email>
<author confidence="0.978725">
Lori Levin and Jaime Carbonell
</author>
<affiliation confidence="0.822196">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.999606">
{lsl,jgc}@cs.cmu.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871153846154">
Lexical resources such as WordNet and
VerbNet are widely used in a multitude
of NLP tasks, as are annotated corpora
such as treebanks. Often, the resources
are used as-is, without question or exam-
ination. This practice risks missing sig-
nificant performance gains and even entire
techniques.
This paper addresses the importance of
resource quality through the lens of a
challenging NLP task: detecting selec-
tional preference violations. We present
DAVID, a simple, lexical resource-based
preference violation detector. With as-
is lexical resources, DAVID achieves an
Fl-measure of just 28.27%. When the
resource entries and parser outputs for
a small sample are corrected, however,
the Fi-measure on that sample jumps
from 40% to 61.54%, and performance
on other examples rises, suggesting that
the algorithm becomes practical given re-
fined resources. More broadly, this pa-
per shows that resource quality matters
tremendously, sometimes even more than
algorithmic improvements.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929658536586">
A variety of NLP tasks have been addressed
using selectional preferences or restrictions, in-
cluding word sense disambiguation (see Navigli
(2009)), semantic parsing (e.g., Shi and Mihalcea
(2005)), and metaphor processing (see Shutova
(2010)). These semantic problems are quite chal-
lenging; metaphor analysis, for instance, has long
been recognized as requiring considerable seman-
tic knowledge (Wilks, 1978; Carbonell, 1980).
The advent of extensive lexical resources, an-
notated corpora, and a spectrum of NLP tools
presents an opportunity to revisit such challenges
from the perspective of selectional preference vio-
lations. Detecting these violations, however, con-
stitutes a severe stress-test for resources designed
for other tasks. As such, it can highlight shortcom-
ings and allow quantifying the potential benefits of
improving resources such as WordNet (Fellbaum,
1998) and VerbNet (Schuler, 2005).
In this paper, we present DAVID (Detector of
Arguments of Verbs with Incompatible Denota-
tions), a resource-based system for detecting pref-
erence violations. DAVID is one component of
METAL (Metaphor Extraction via Targeted Anal-
ysis of Language), a new system for identifying,
interpreting, and cataloguing metaphors. One pur-
pose of DAVID was to explore how far lexical
resource-based techniques can take us. Though
our initial results suggested that the answer is “not
very,” further analysis revealed that the problem
lies less in the technique than in the state of exist-
ing resources and tools.
Often, it is assumed that the frontier of perfor-
mance on NLP tasks is shaped entirely by algo-
rithms. Manning (2011) showed that this may not
hold for POS tagging – that further improvements
may require resource cleanup. In the same spirit,
we argue that for some semantic tasks, exemplified
by preference violation detection, resource qual-
ity may be at least as essential as algorithmic en-
hancements.
</bodyText>
<sectionHeader confidence="0.965943" genericHeader="introduction">
2 The Preference Violation Detection
Task
</sectionHeader>
<bodyText confidence="0.999559375">
DAVID builds on the insight of Wilks (1978) that
the strongest indicator of metaphoricity is the vi-
olation of selectional preferences. For example,
only plants can literally be pruned. If laws is
the object of pruned, the verb is likely metaphori-
cal. Flagging such semantic mismatches between
verbs and arguments is the task of preference vio-
lation detection.
</bodyText>
<page confidence="0.972796">
765
</page>
<bodyText confidence="0.918061764705882">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765–770,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
We base our definition of preferences on the
Pragglejaz guidelines (Pragglejaz Group, 2007)
for identifying the most basic sense of a word as
the most concrete, embodied, or precise one. Sim-
ilarly, we define selectional preferences as the se-
mantic constraints imposed by a verb’s most basic
sense. Dictionaries may list figurative senses of
prune, but we take the basic sense to be cutting
plant growth.
Several types of verbs were excluded from the
task because they have very lax preferences. These
include verbs of becoming or seeming (e.g., trans-
form, appear), light verbs, auxiliaries, and aspec-
tual verbs. For the sake of simplifying implemen-
tation, phrasal verbs were also ignored.
</bodyText>
<sectionHeader confidence="0.995591" genericHeader="method">
3 Algorithm Design
</sectionHeader>
<bodyText confidence="0.9997268">
To identify violations, DAVID employs a simple
algorithm based on several existing tools and re-
sources: SENNA (Collobert et al., 2011), a seman-
tic role labeling (SRL) system; VerbNet, a com-
putational verb lexicon; SemLink (Loper et al.,
2007), which includes mappings between Prop-
Bank (Palmer et al., 2005) and VerbNet; and
WordNet. As one metaphor detection component
of METAL’s several, DAVID is designed to favor
precision over recall. The algorithm is as follows:
</bodyText>
<listItem confidence="0.998040260869565">
1. Run the Stanford CoreNLP POS tagger
(Toutanova et al., 2003) and the TurboParser
dependency parser (Martins et al., 2011).
2. Run SENNA to identify the semantic argu-
ments of each verb in the sentence using the
PropBank argument annotation scheme (Arg0,
Arg1, etc.). See Table 1 for example output.
3. For each verb V , find all VerbNet entries for
V . Using SemLink, map each PropBank argu-
ment name to the corresponding VerbNet the-
matic roles in these entries (Agent, Patient,
etc.). For example, the VerbNet class for prune
is carve-21.2-2. SemLink maps Arg0 to
the Agent of carve-21.2-2 and Arg1 to
the Patient.
4. Retrieve from VerbNet the selectional restric-
tions of each thematic role. In our running
example, VerbNet specifies +int control
and +concrete for the Agent and Patient of
carve-21.2-2, respectively.
5. If the head of any argument cannot be inter-
preted to meet V ’s preferences, flag V as a vi-
olation.
</listItem>
<bodyText confidence="0.918209333333333">
“The politician pruned laws regulating plastic
bags, and created new fees for inspecting dairy
farms.”
</bodyText>
<subsectionHeader confidence="0.685884">
Verb Arg0 Arg1
</subsectionHeader>
<bodyText confidence="0.9791055">
pruned The politician laws ... bags
regulating laws plastic bags
created The politician new fees
inspecting - - dairy farms
</bodyText>
<tableCaption confidence="0.489438">
Table 1: SENNA’s SRL output for the example
sentence above. Though this example demon-
strates only two arguments, SENNA is capable of
labeling up to six.
</tableCaption>
<figure confidence="0.878252111111111">
Restriction WordNet Synsets
animate animate being.n.01
people.n.01
person.n.01
concrete physical object.n.01
matter.n.01
substance.n.01
organization social group.n.01
district.n.01
</figure>
<tableCaption confidence="0.482709">
Table 2: DAVID’s mappings between some
</tableCaption>
<bodyText confidence="0.985371642857143">
common VerbNet restriction types and WordNet
synsets.
Each VerbNet restriction is interpreted as man-
dating or forbidding a set of WordNet hypernyms,
defined by a custom mapping (see Table 2).
For example, VerbNet requires both the Patient
of a verb in carve-21.2-2 and the Theme
of a verb in wipe manner-10.4.1-1 to
be concrete. By empirical inspection, concrete
nouns are hyponyms of the WordNet synsets
physical object.n.01, matter.n.03,
or substance.n.04. Laws (the Patient of
prune) is a hyponym of none of these, so prune
would be flagged as a violation.
</bodyText>
<sectionHeader confidence="0.987271" genericHeader="method">
4 Corpus Annotation
</sectionHeader>
<bodyText confidence="0.999942625">
To evaluate our system, we assembled a corpus
of 715 sentences from the METAL project’s cor-
pus of sentences with and without metaphors. The
corpus was annotated by two annotators follow-
ing an annotation manual. Each verb was marked
for whether its arguments violated the selectional
preferences of the most basic, literal meaning of
the verb. The annotators resolved conflicts by dis-
</bodyText>
<page confidence="0.993987">
766
</page>
<table confidence="0.999819416666667">
Error source Frequency
Bad/missing VN entries 4.5 (14.1%)
Bad/missing VN restrictions 6 (18.8%)
Bad/missing SL mappings 2 (6.3%)
Parsing/head-finding errors 3.5 (10.9%)
SRL errors 8.5 (26.6%)
VN restriction system too weak 4 (12.5%)
Confounding WordNet senses 3.5 (10.9%)
Endemic errors: 7.5 (23.4%)
Resource errors: 12.5 (39.1%)
Tool errors: 12 (37.5%)
Total: 32 (100%)
</table>
<tableCaption confidence="0.990985">
Table 3: Sources of error in 90 randomly selected
</tableCaption>
<bodyText confidence="0.940964">
sentences. For errors that were due to a combi-
nation of sources, 1/2 point was awarded to each
source. (VN stands for VerbNet and SL for Sem-
Link.)
cussing until consensus.
</bodyText>
<sectionHeader confidence="0.998629" genericHeader="method">
5 Initial Results
</sectionHeader>
<bodyText confidence="0.999994916666667">
As the first row of Table 4 shows, our initial eval-
uation left little hope for the technique. With
such low precision and F1, it seemed a lexical
resource-based preference violation detector was
out. When we analyzed the errors in 90 randomly
selected sentences, however, we found that most
were not due to systemic problems with the ap-
proach; rather, they stemmed from SRL and pars-
ing errors and missing or incorrect resource entries
(see Table 3). Armed with this information, we de-
cided to explore how viable our algorithm would
be absent these problems.
</bodyText>
<sectionHeader confidence="0.989374" genericHeader="method">
6 Refining The Data
</sectionHeader>
<bodyText confidence="0.999627857142857">
To evaluate the effects of correcting DAVID’s in-
puts, we manually corrected the tool outputs and
resource entries that affected the aforementioned
90 sentences. SRL output was corrected for ev-
ery sentence, while SemLink and VerbNet entries
were corrected only for each verb that produced an
error.
</bodyText>
<subsectionHeader confidence="0.998078">
6.1 Corrections to Tool Output (Parser/SRL)
</subsectionHeader>
<bodyText confidence="0.999917142857143">
Guided by the PropBank database and annotation
guidelines, we corrected all errors in core role
assignments from SENNA. These corrections in-
cluded relabeling arguments, adding missed argu-
ments, fixing argument spans, and deleting anno-
tations for non-verbs. The only parser-related er-
ror we corrected was a mislabeled noun.
</bodyText>
<subsectionHeader confidence="0.999845">
6.2 Correcting Corrupted Data in VerbNet
</subsectionHeader>
<bodyText confidence="0.999988071428571">
The VerbNet download is missing several sub-
classes that are referred to by SemLink or that
have been updated on the VerbNet website. Some
roles also have not been updated to the latest ver-
sion, and some subclasses are listed with incor-
rect IDs. These problems, which caused SemLink
mappings to fail, were corrected before reviewing
errors from the corpus.
Six subclasses needed to be fixed, all of which
were easily detected by a simple script that did not
depend on the 90-sentence subcorpus. We there-
fore expect that few further changes of this type
would be needed for a more complete resource re-
finement effort.
</bodyText>
<subsectionHeader confidence="0.97896">
6.3 Corpus-Based Updates to SemLink
</subsectionHeader>
<bodyText confidence="0.9999629375">
Our modifications to SemLink’s mappings in-
cluded adding missing verbs, adding missing roles
to mappings, and correcting mappings to more ap-
propriate classes or roles. We also added null map-
pings in cases where a PropBank argument had no
corresponding role in VerbNet. This makes the
system’s strategy for ruling out mappings more re-
liable.
No corrections were made purely based on the
sample. Any time a verb’s mappings were edited,
VerbNet was scoured for plausible mappings for
every verb sense in PropBank, and any nonsensi-
cal mappings were deleted. For example, when
the phrase go dormant caused an error, we in-
spected the mappings for go. Arguments of all but
2 of the 7 available mappings were edited, either
to add missing arguments or to correct nonsensi-
cal ones. These changes actually had a net neg-
ative impact on test set performance because the
bad mappings had masked parsing and selectional
preference problems.
Based on the 90-sentence subcorpus, we mod-
ified 20 of the existing verb entries in SemLink.
These changes included correcting 8 role map-
pings, adding 13 missing role mappings to existing
senses, deleting 2 incorrect senses, adding 11 verb
senses, correcting 2 senses, deleting 1 superfluous
role mapping, and adding 46 null role mappings.
(Note that although null mappings represented the
largest set of changes, they also had the least im-
pact on system behavior.) One entirely new verb
was added, as well.
</bodyText>
<page confidence="0.990038">
767
</page>
<subsectionHeader confidence="0.935007">
6.4 Corpus-Based Updates to VerbNet
</subsectionHeader>
<bodyText confidence="0.999974620689655">
Nineteen VerbNet classes were modified, and one
class had to be added. The modifications gener-
ally involved adding, correcting, or deleting se-
lectional restrictions, often by introducing or re-
arranging subclasses. Other changes amounted to
fixing clerical errors, such as incorrect role names
or restrictions that had been ANDed instead of
ORed.
An especially difficult problem was an inconsis-
tency in the semantics of VerbNet’s subclass sys-
tem. In some cases, the restrictions specified on
a verb in a subclass did not apply to subcatego-
rization frames inherited from a superclass, but in
other cases the restrictions clearly applied to all
frames. The conflict was resolved by duplicating
subclassed verbs in the top-level class whenever
different selectional restrictions were needed for
the two sets of frames.
As with SemLink, samples determined only
which classes were modified, not what modifica-
tions were made. Any non-obvious changes to
selectional restrictions were verified by examin-
ing dozens of verb instances from SketchEngine’s
(Kilgarriff et al., 2004) corpus. For example, the
Agent of seek was restricted to +animate, but
the corpus confirmed that organizations are com-
monly described non-metaphorically as seeking,
so the restriction was updated to +animate |
+organization.
</bodyText>
<sectionHeader confidence="0.999353" genericHeader="method">
7 Results After Resource Refinement
</sectionHeader>
<bodyText confidence="0.998856204081633">
After making corrections for each set of 10 sen-
tences, we incrementally recomputed F1 and pre-
cision, both on the subcorpus corrected so far and
on a test set of all 625 sentences that were never
corrected. (The manual nature of the correction ef-
fort made testing k-fold subsets impractical.) The
results for 30-sentence increments are shown in
Table 4.
The most striking feature of these figures is how
much performance improves on corrected sen-
tences: for the full 90 sentences, F1 rose from
30.43% to 61.54%, and precision rose even more
dramatically from 31.82% to 80.00%. Interest-
ingly, resource corrections alone generally made a
larger difference than tool corrections alone, sug-
gesting that resources may be the dominant fac-
tor in resource-intensive tasks such as this one.
Even more compellingly, the improvement from
correcting both the tools and the resources was
nearly double the sum of the improvements from
each alone: tool and resource improvements inter-
act synergistically.
The effects on the test corpus are harder to
interpret. Due to a combination of SRL prob-
lems and the small number of sentences cor-
rected, the scores on the test set improved little
with resource correction; in fact, they even dipped
slightly between the 30- and 60-sentence incre-
ments. Nonetheless, we contend that our results
testify to the generality of our corrections: after
each iteration, every altered result was either an
error fixed or an error that should have appeared
before but had been masked by another. Note also
that all results on the test set are without corrected
tool output; presumably, these sentences would
also have improved synergistically with more ac-
curate SRL. How long corrections would continue
to improve performance is a question that we did
not have the resources to answer, but our results
suggest that there is plenty of room to go.
Some errors, of course, are endemic to the ap-
proach and cannot be fixed either by improved re-
sources or by better tools. For example, we con-
sider every WordNet sense to be plausible, which
produces false negatives. Additionally, the selec-
tional restrictions specified by VerbNet are fairly
loose; a more refined set of categories might cap-
ture the range of verbs’ restrictions more accu-
rately.
</bodyText>
<sectionHeader confidence="0.926126" genericHeader="method">
8 Implications for Future Refinement
Efforts
</sectionHeader>
<bodyText confidence="0.999477333333333">
Although improving resources is infamously
labor-intensive, we believe that similarly refining
the remainder of VerbNet and SemLink would be
doable. In our study, it took about 25-35 person-
hours to examine about 150 verbs and to mod-
ify 20 VerbNet classes and 25 SemLink verb en-
tries (excluding time for SENNA corrections, fix-
ing corrupt VerbNet data, and analysis of DAVID’s
errors). Extrapolating from our experience, we es-
timate that it would take roughly 6-8 person-weeks
to systematically fix this particular set of issues
with VerbNet.
Improving SemLink could be more complex,
as its mappings are automatically generated from
VerbNet annotations on top of the PropBank cor-
pus. One possibility is to correct the generated
mappings directly, as we did in our study, which
we estimate would take about two person-months.
</bodyText>
<page confidence="0.99359">
768
</page>
<bodyText confidence="0.999983857142857">
With the addition of some metadata from the gen-
eration process, it would then be possible to follow
the corrected mappings back to annotations from
which they were generated and fix those annota-
tions. One downside of this approach is that if the
mappings were ever regenerated from the anno-
tated corpus, any mappings not encountered in the
corpus would have to be added back afterwards.
Null role mappings would be particularly thorny
to implement. To add a null mapping, we must
know that a role definitely does not belong, and
is not just incidentally missing from an exam-
ple. For instance, VerbNet’s defend-85 class
truly has no equivalent to Arg2 in PropBank’s
defend.01, but Arg0 or Arg1 may be missing
for other reasons (e.g., in a passive). It may be best
to simply omit null mappings, as is currently done.
Alternatively, full parses from the Penn Treebank,
on which PropBank is based, might allow distin-
guishing phenomena such as passives where argu-
ments are predictably omitted.
The maintainers of VerbNet and PropBank are
aware of many of the issues we have raised, and
we have been in contact with them about possi-
ble approaches to fixing them. They are particu-
larly aware of the inconsistent semantics of selec-
tional restrictions on VerbNet subclasses, and they
hope to fix this issue within a larger attempt at re-
tooling VerbNet’s selectional restrictions. In the
meantime, we are sharing our VerbNet modifica-
tions with them for them to verify and incorporate.
We are also sharing our SemLink changes so that
they can, if they choose, continue manual correc-
tion efforts or trace SemLink problems back to the
annotated corpus.
</bodyText>
<sectionHeader confidence="0.99681" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9999767">
Our results argue for investing effort in developing
and fixing resources, in addition to developing bet-
ter NLP tools. Resource and tool improvements
interact synergistically: better resources multiply
the effect of algorithm enhancements. Gains from
fixing resources may sometimes even exceed what
the best possible algorithmic improvements can
provide. We hope the NLP community will take
up the challenge of investing in its resources to the
extent that its tools demand.
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.783935">
Thanks to Eric Nyberg for suggesting building a
system like DAVID, to Spencer Onuffer for his an-
</bodyText>
<table confidence="0.999403210526316">
Sent. Tools Rsrcs P F1
715 0 0 27.14% 28.27%
625 0 0 26.55% 27.98%
625 0 corr. 26.37% 28.15%
30 0 0 50.00% 40.00%
30 30 0 66.67% 44.44%
30 0 corr.+30 62.50% 50.00%
30 30 corr.+30 87.50% 70.00%
625 0 corr.+30 27.07% 28.82%
60 0 0 35.71% 31.25%
60 60 0 54.55% 31.38%
60 0 corr.+60 53.85% 45.16%
60 60 corr.+60 90.91% 68.97%
625 0 corr.+60 26.92% 28.74%
90 0 0 31.82% 30.43%
90 90 0 44.44% 38.10%
90 0 corr.+90 47.37% 41.86%
90 90 corr.+90 80.00% 61.54%
625 0 corr.+90 27.37% 28.99%
</table>
<tableCaption confidence="0.990236">
Table 4: Performance on preference violation de-
</tableCaption>
<bodyText confidence="0.993668789473684">
tection task. Column 1 shows the sentence count.
Columns 2 and 3 show how many sentences’
SRL/parsing and resource errors, respectively, had
been fixed (“corr.” indicates corrupted files).
notation efforts, and to Davida Fromm for curating
METAL’s corpus of Engish sentences.
This work was supported by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0020.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998573571428572">
Jaime G. Carbonell. 1980. Metaphor: a key to ex-
tensible semantic analysis. In Proceedings of the
18th annual meeting on Association for Computa-
tional Linguistics, ACL ’80, pages 17–21, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
</reference>
<page confidence="0.979153">
769
</page>
<reference confidence="0.999918163934426">
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford Books.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of EURALEX.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
PropBank and VerbNet. In Proceedings of the 7th
International Workshop on Computational Linguis-
tics, Tilburg, the Netherlands.
Christopher D Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: is it time for some linguis-
tics? In Computational Linguistics and Intelligent
Text Processing, pages 171–189. Springer.
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M´ario A. T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 238–249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys (CSUR), 41(2):10.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Pragglejaz Group. 2007. MIP: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and Symbol, 22(1):1–39.
Karin K. Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. the-
sis, University of Pennsylvania, Philadelphia, PA.
AAI3179808.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Alexander
Gelbukh, editor, Computational Linguistics and In-
telligent Text Processing, volume 3406 of Lec-
ture Notes in Computer Science, pages 100–111.
Springer Berlin Heidelberg.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 688–697, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ’03, pages 173–180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11:197–223.
</reference>
<page confidence="0.997112">
770
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.660156">
<title confidence="0.999881">The Effects of Lexical Resource Quality on Preference Violation Detection</title>
<author confidence="0.999877">Jesse Dunietz</author>
<affiliation confidence="0.999947">Computer Science Department Carnegie Mellon University</affiliation>
<address confidence="0.999673">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.99985">jdunietz@cs.cmu.edu</email>
<affiliation confidence="0.865347333333333">Levin Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.998536">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.999342296296297">Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
</authors>
<title>Metaphor: a key to extensible semantic analysis.</title>
<date>1980</date>
<booktitle>In Proceedings of the 18th annual meeting on Association for Computational Linguistics, ACL ’80,</booktitle>
<pages>17--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1752" citStr="Carbonell, 1980" startWordPosition="245" endWordPosition="246">gorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, it can highlight shortcomings and allow quantifying the potential benefits of improving resources such as WordNet (Fellbaum, 1998) and VerbNet (Schuler, 2005). In this paper, we present DAVID (Detector of Arguments of Verbs with Incompatible Denotations), a resource-based system</context>
</contexts>
<marker>Carbonell, 1980</marker>
<rawString>Jaime G. Carbonell. 1980. Metaphor: a key to extensible semantic analysis. In Proceedings of the 18th annual meeting on Association for Computational Linguistics, ACL ’80, pages 17–21, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="4666" citStr="Collobert et al., 2011" startWordPosition="698" endWordPosition="701"> preferences as the semantic constraints imposed by a verb’s most basic sense. Dictionaries may list figurative senses of prune, but we take the basic sense to be cutting plant growth. Several types of verbs were excluded from the task because they have very lax preferences. These include verbs of becoming or seeming (e.g., transform, appear), light verbs, auxiliaries, and aspectual verbs. For the sake of simplifying implementation, phrasal verbs were also ignored. 3 Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). 2. Run SENNA to identify the semantic arguments of each verb in the sentence using the PropBank argument annotation scheme (Arg0, Arg1, etc.</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>Bradford Books.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychly</author>
<author>Pavel Smrz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of EURALEX.</booktitle>
<contexts>
<context position="12592" citStr="Kilgarriff et al., 2004" startWordPosition="1971" endWordPosition="1974">. In some cases, the restrictions specified on a verb in a subclass did not apply to subcategorization frames inherited from a superclass, but in other cases the restrictions clearly applied to all frames. The conflict was resolved by duplicating subclassed verbs in the top-level class whenever different selectional restrictions were needed for the two sets of frames. As with SemLink, samples determined only which classes were modified, not what modifications were made. Any non-obvious changes to selectional restrictions were verified by examining dozens of verb instances from SketchEngine’s (Kilgarriff et al., 2004) corpus. For example, the Agent of seek was restricted to +animate, but the corpus confirmed that organizations are commonly described non-metaphorically as seeking, so the restriction was updated to +animate | +organization. 7 Results After Resource Refinement After making corrections for each set of 10 sentences, we incrementally recomputed F1 and precision, both on the subcorpus corrected so far and on a test set of all 625 sentences that were never corrected. (The manual nature of the correction effort made testing k-fold subsets impractical.) The results for 30-sentence increments are sho</context>
</contexts>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceedings of EURALEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Combining lexical resources: Mapping between PropBank and VerbNet.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Linguistics,</booktitle>
<location>Tilburg, the Netherlands.</location>
<contexts>
<context position="4774" citStr="Loper et al., 2007" startWordPosition="716" endWordPosition="719">senses of prune, but we take the basic sense to be cutting plant growth. Several types of verbs were excluded from the task because they have very lax preferences. These include verbs of becoming or seeming (e.g., transform, appear), light verbs, auxiliaries, and aspectual verbs. For the sake of simplifying implementation, phrasal verbs were also ignored. 3 Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). 2. Run SENNA to identify the semantic arguments of each verb in the sentence using the PropBank argument annotation scheme (Arg0, Arg1, etc.). See Table 1 for example output. 3. For each verb V , find all VerbNet entries for V . Using SemLink, map </context>
</contexts>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu-ting Yi, and Martha Palmer. 2007. Combining lexical resources: Mapping between PropBank and VerbNet. In Proceedings of the 7th International Workshop on Computational Linguistics, Tilburg, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>171--189</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2945" citStr="Manning (2011)" startWordPosition="430" endWordPosition="431">resource-based system for detecting preference violations. DAVID is one component of METAL (Metaphor Extraction via Targeted Analysis of Language), a new system for identifying, interpreting, and cataloguing metaphors. One purpose of DAVID was to explore how far lexical resource-based techniques can take us. Though our initial results suggested that the answer is “not very,” further analysis revealed that the problem lies less in the technique than in the state of existing resources and tools. Often, it is assumed that the frontier of performance on NLP tasks is shaped entirely by algorithms. Manning (2011) showed that this may not hold for POS tagging – that further improvements may require resource cleanup. In the same spirit, we argue that for some semantic tasks, exemplified by preference violation detection, resource quality may be at least as essential as algorithmic enhancements. 2 The Preference Violation Detection Task DAVID builds on the insight of Wilks (1978) that the strongest indicator of metaphoricity is the violation of selectional preferences. For example, only plants can literally be pruned. If laws is the object of pruned, the verb is likely metaphorical. Flagging such semanti</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D Manning. 2011. Part-of-speech tagging from 97% to 100%: is it time for some linguistics? In Computational Linguistics and Intelligent Text Processing, pages 171–189. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>238--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5124" citStr="Martins et al., 2011" startWordPosition="771" endWordPosition="774"> ignored. 3 Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). 2. Run SENNA to identify the semantic arguments of each verb in the sentence using the PropBank argument annotation scheme (Arg0, Arg1, etc.). See Table 1 for example output. 3. For each verb V , find all VerbNet entries for V . Using SemLink, map each PropBank argument name to the corresponding VerbNet thematic roles in these entries (Agent, Patient, etc.). For example, the VerbNet class for prune is carve-21.2-2. SemLink maps Arg0 to the Agent of carve-21.2-2 and Arg1 to the Patient. 4. Retrieve from VerbNet the selectional restrictions of each thematic role. In our running example, VerbNe</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 238–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1473" citStr="Navigli (2009)" startWordPosition="207" endWordPosition="208"> asis lexical resources, DAVID achieves an Fl-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the Fi-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR), 41(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4838" citStr="Palmer et al., 2005" startWordPosition="726" endWordPosition="729">t growth. Several types of verbs were excluded from the task because they have very lax preferences. These include verbs of becoming or seeming (e.g., transform, appear), light verbs, auxiliaries, and aspectual verbs. For the sake of simplifying implementation, phrasal verbs were also ignored. 3 Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). 2. Run SENNA to identify the semantic arguments of each verb in the sentence using the PropBank argument annotation scheme (Arg0, Arg1, etc.). See Table 1 for example output. 3. For each verb V , find all VerbNet entries for V . Using SemLink, map each PropBank argument name to the corresponding VerbNet themati</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pragglejaz Group</author>
</authors>
<title>MIP: A method for identifying metaphorically used words in discourse.</title>
<date>2007</date>
<journal>Metaphor and Symbol,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3915" citStr="Group, 2007" startWordPosition="578" endWordPosition="579"> Wilks (1978) that the strongest indicator of metaphoricity is the violation of selectional preferences. For example, only plants can literally be pruned. If laws is the object of pruned, the verb is likely metaphorical. Flagging such semantic mismatches between verbs and arguments is the task of preference violation detection. 765 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765–770, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We base our definition of preferences on the Pragglejaz guidelines (Pragglejaz Group, 2007) for identifying the most basic sense of a word as the most concrete, embodied, or precise one. Similarly, we define selectional preferences as the semantic constraints imposed by a verb’s most basic sense. Dictionaries may list figurative senses of prune, but we take the basic sense to be cutting plant growth. Several types of verbs were excluded from the task because they have very lax preferences. These include verbs of becoming or seeming (e.g., transform, appear), light verbs, auxiliaries, and aspectual verbs. For the sake of simplifying implementation, phrasal verbs were also ignored. 3 </context>
</contexts>
<marker>Group, 2007</marker>
<rawString>Pragglejaz Group. 2007. MIP: A method for identifying metaphorically used words in discourse. Metaphor and Symbol, 22(1):1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin K Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2231" citStr="Schuler, 2005" startWordPosition="315" endWordPosition="316">; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, it can highlight shortcomings and allow quantifying the potential benefits of improving resources such as WordNet (Fellbaum, 1998) and VerbNet (Schuler, 2005). In this paper, we present DAVID (Detector of Arguments of Verbs with Incompatible Denotations), a resource-based system for detecting preference violations. DAVID is one component of METAL (Metaphor Extraction via Targeted Analysis of Language), a new system for identifying, interpreting, and cataloguing metaphors. One purpose of DAVID was to explore how far lexical resource-based techniques can take us. Though our initial results suggested that the answer is “not very,” further analysis revealed that the problem lies less in the technique than in the state of existing resources and tools. O</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>Karin K. Schuler. 2005. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA. AAI3179808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>3406</volume>
<pages>100--111</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1523" citStr="Shi and Mihalcea (2005)" startWordPosition="212" endWordPosition="215">n Fl-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the Fi-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, it can highlight shortcomings and allow quantifyin</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>Lei Shi and Rada Mihalcea. 2005. Putting pieces together: Combining FrameNet, VerbNet and WordNet for robust semantic parsing. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 3406 of Lecture Notes in Computer Science, pages 100–111. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Models of metaphor in NLP.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>688--697</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1569" citStr="Shutova (2010)" startWordPosition="220" endWordPosition="221">and parser outputs for a small sample are corrected, however, the Fi-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, it can highlight shortcomings and allow quantifying the potential benefits of improving resource</context>
</contexts>
<marker>Shutova, 2010</marker>
<rawString>Ekaterina Shutova. 2010. Models of metaphor in NLP. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 688–697, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North</booktitle>
<contexts>
<context position="5063" citStr="Toutanova et al., 2003" startWordPosition="762" endWordPosition="765">the sake of simplifying implementation, phrasal verbs were also ignored. 3 Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). 2. Run SENNA to identify the semantic arguments of each verb in the sentence using the PropBank argument annotation scheme (Arg0, Arg1, etc.). See Table 1 for example output. 3. For each verb V , find all VerbNet entries for V . Using SemLink, map each PropBank argument name to the corresponding VerbNet thematic roles in these entries (Agent, Patient, etc.). For example, the VerbNet class for prune is carve-21.2-2. SemLink maps Arg0 to the Agent of carve-21.2-2 and Arg1 to the Patient. 4. Retrieve from VerbNet the selectional restr</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North</rawString>
</citation>
<citation valid="false">
<title>American Chapter of the Association for Computational Linguistics on Human Language Technology -</title>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Making preferences more active.</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<pages>11--197</pages>
<contexts>
<context position="1734" citStr="Wilks, 1978" startWordPosition="243" endWordPosition="244">g that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools presents an opportunity to revisit such challenges from the perspective of selectional preference violations. Detecting these violations, however, constitutes a severe stress-test for resources designed for other tasks. As such, it can highlight shortcomings and allow quantifying the potential benefits of improving resources such as WordNet (Fellbaum, 1998) and VerbNet (Schuler, 2005). In this paper, we present DAVID (Detector of Arguments of Verbs with Incompatible Denotations), a res</context>
<context position="3316" citStr="Wilks (1978)" startWordPosition="490" endWordPosition="491">ry,” further analysis revealed that the problem lies less in the technique than in the state of existing resources and tools. Often, it is assumed that the frontier of performance on NLP tasks is shaped entirely by algorithms. Manning (2011) showed that this may not hold for POS tagging – that further improvements may require resource cleanup. In the same spirit, we argue that for some semantic tasks, exemplified by preference violation detection, resource quality may be at least as essential as algorithmic enhancements. 2 The Preference Violation Detection Task DAVID builds on the insight of Wilks (1978) that the strongest indicator of metaphoricity is the violation of selectional preferences. For example, only plants can literally be pruned. If laws is the object of pruned, the verb is likely metaphorical. Flagging such semantic mismatches between verbs and arguments is the task of preference violation detection. 765 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765–770, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics We base our definition of preferences on the Pragglejaz guidelines (Pragglejaz Group, 2007) </context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence, 11:197–223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>