<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<note confidence="0.983892">
Proceeings of the Seventh CoNLL conference
held at HLT-NAACL 2003 , pp. 25-32
Edmonton, May-June 2003
</note>
<title confidence="0.99794">
Learning Subjective Nouns using Extraction Pattern Bootstrapping*
</title>
<author confidence="0.998923">
Ellen Riloff
</author>
<affiliation confidence="0.909246666666667">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.997508">
riloff@cs.utah.edu
</email>
<author confidence="0.996068">
Janyce Wiebe
</author>
<affiliation confidence="0.998642">
Department of Computer Science
University of Pittsburgh
</affiliation>
<address confidence="0.58242">
Pittsburgh, PA 15260
</address>
<email confidence="0.998116">
wiebe@cs.pitt.edu
</email>
<author confidence="0.98647">
Theresa Wilson
</author>
<affiliation confidence="0.9718385">
Intelligent Systems Program
University of Pittsburgh
</affiliation>
<address confidence="0.582123">
Pittsburgh, PA 15260
</address>
<email confidence="0.998714">
twilson@cs.pitt.edu
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821133333333">
We explore the idea of creating a subjectiv-
ity classifier that uses lists of subjective nouns
learned by bootstrapping algorithms. The goal
of our research is to develop a system that
can distinguish subjective sentences from ob-
jective sentences. First, we use two bootstrap-
ping algorithms that exploit extraction patterns
to learn sets of subjective nouns. Then we
train a Naive Bayes classifier using the subjec-
tive nouns, discourse features, and subjectivity
clues identified in prior research. The boot-
strapping algorithms learned over 1000 subjec-
tive nouns, and the subjectivity classifier per-
formed well, achieving 77% recall with 81%
precision.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973769230769">
Many natural language processing applications could
benefit from being able to distinguish between factual
and subjective information. Subjective remarks come
in a variety of forms, including opinions, rants, allega-
tions, accusations, suspicions, and speculation. Ideally,
information extraction systems should be able to distin-
guish between factual information (which should be ex-
tracted) and non-factual information (which should be
discarded or labeled as uncertain). Question answering
systems should distinguish between factual and specula-
tive answers. Multi-perspective question answering aims
to present multiple answers to the user based upon specu-
lation or opinions derived from different sources. Multi-
</bodyText>
<footnote confidence="0.6112295">
* This work was supported in part by the National Sci-
ence Foundation under grants IIS-0208798 and IRI-9704240.
The data preparation was performed in support of the North-
east Regional Reseach Center (NRRC) which is sponsored by
the Advanced Research and Development Activity (ARDA), a
U.S. Government entity which sponsors and promotes research
of import to the Intelligence Community which includes but is
not limited to the CIA, DIA, NSA, NIMA, and NRO.
</footnote>
<bodyText confidence="0.99986709375">
document summarization systems need to summarize dif-
ferent opinions and perspectives. Spam filtering systems
must recognize rants and emotional tirades, among other
things. In general, nearly any system that seeks to iden-
tify information could benefit from being able to separate
factual and subjective information.
Subjective language has been previously studied in
fields such as linguistics, literary theory, psychology, and
content analysis. Some manually-developed knowledge
resources exist, but there is no comprehensive dictionary
of subjective language.
Meta-Bootstrapping (Riloff and Jones, 1999) and
Basilisk (Thelen and Riloff, 2002) are bootstrapping al-
gorithms that use automatically generated extraction pat-
terns to identify words belonging to a semantic cate-
gory. We hypothesized that extraction patterns could
also identify subjective words. For example, the pat-
tern “expressed &lt;direct object&gt;” often extracts subjec-
tive nouns, such as “concern”, “hope”, and “support”.
Furthermore, these bootstrapping algorithms require only
a handful of seed words and unannotated texts for train-
ing; no annotated data is needed at all.
In this paper, we use the Meta-Bootstrapping and
Basilisk algorithms to learn lists of subjective nouns from
a large collection of unannotated texts. Then we train
a subjectivity classifier on a small set of annotated data,
using the subjective nouns as features along with some
other previously identified subjectivity features. Our ex-
perimental results show that the subjectivity classifier
performs well (77% recall with 81% precision) and that
the learned nouns improve upon previous state-of-the-art
subjectivity results (Wiebe et al., 1999).
</bodyText>
<sectionHeader confidence="0.989765" genericHeader="method">
2 Subjectivity Data
</sectionHeader>
<subsectionHeader confidence="0.974496">
2.1 The Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.995783833333333">
In 2002, an annotation scheme was developed
for a U.S. government-sponsored project with a
team of 10 researchers (the annotation instruc-
tions and project reports are available on the Web
at http://www.cs.pitt.edu/˜wiebe/pubs/ardasummer02/).
The scheme was inspired by work in linguistics and
literary theory on subjectivity, which focuses on how
opinions, emotions, etc. are expressed linguistically in
context (Banfield, 1982). The scheme is more detailed
and comprehensive than previous ones. We mention only
those aspects of the annotation scheme relevant to this
paper.
The goal of the annotation scheme is to identify and
characterize expressions of private states in a sentence.
Private state is a general covering term for opinions, eval-
uations, emotions, and speculations (Quirk et al., 1985).
For example, in sentence (1) the writer is expressing a
negative evaluation.
</bodyText>
<listItem confidence="0.9738045">
(1) “The time has come, gentlemen, for Sharon, the as-
sassin, to realize that injustice cannot last long.”
</listItem>
<bodyText confidence="0.94452075">
Sentence (2) reflects the private state of Western coun-
tries. Mugabe’s use of “overwhelmingly” also reflects a
private state, his positive reaction to and characterization
of his victory.
</bodyText>
<listItem confidence="0.989201333333333">
(2) “Western countries were left frustrated and impotent
after Robert Mugabe formally declared that he had over-
whelmingly won Zimbabwe’s presidential election.”
Annotators are also asked to judge the strength of each
private state. A private state can have low, medium, high
or extreme strength.
</listItem>
<subsectionHeader confidence="0.994068">
2.2 Corpus and Agreement Results
</subsectionHeader>
<bodyText confidence="0.9993903">
Our data consists of English-language versions of foreign
news documents from FBIS, the U.S. Foreign Broadcast
Information Service. The data is from a variety of publi-
cations and countries. The annotated corpus used to train
and test our subjectivity classifiers (the experiment cor-
pus) consists of 109 documents with a total of 2197 sen-
tences. We used a separate, annotated tuning corpus of
33 documents with a total of 698 sentences to establish
some experimental parameters.&apos;
Each document was annotated by one or both of two
annotators, A and T. To allow us to measure interanno-
tator agreement, the annotators independently annotated
the same 12 documents with a total of 178 sentences. We
began with a strict measure of agreement at the sentence
level by first considering whether the annotator marked
any private-state expression, of any strength, anywhere
in the sentence. If so, the sentence should be subjective.
Otherwise, it is objective. Table 1 shows the contingency
table. The percentage agreement is 88%, and the rc value
is 0.71.
</bodyText>
<footnote confidence="0.828885333333333">
&apos;The annotated data will be available to U.S. government
contractors this summer. We are working to resolve copyright
issues to make it available to the wider research community.
</footnote>
<table confidence="0.5498185">
Tagger T
Subj Obj
Tagger A Subj nyy = 112 nyn = 16
Obj nny = 6 nnn = 44
</table>
<tableCaption confidence="0.510239666666667">
Table 1: Agreement for sentence-level annotations
Tagger T
Subj Obj
</tableCaption>
<table confidence="0.570777">
nyy = 106 nyn = 9
nny = 0 nnn = 44
</table>
<tableCaption confidence="0.9119985">
Table 2: Agreement for sentence-level annotations, low-
strength cases removed
</tableCaption>
<bodyText confidence="0.999792590909091">
One would expect that there are clear cases of objec-
tive sentences, clear cases of subjective sentences, and
borderline sentences in between. The agreement study
supports this. In terms of our annotations, we define
a sentence as borderline if it has at least one private-
state expression identified by at least one annotator, and
all strength ratings of private-state expressions are low.
Table 2 shows the agreement results when such border-
line sentences are removed (19 sentences, or 11% of the
agreement test corpus). The percentage agreement in-
creases to 94% and the rc value increases to 0.87.
As expected, the majority of disagreement cases in-
volve low-strength subjectivity. The annotators consis-
tently agree about which are the clear cases of subjective
sentences. This leads us to define the gold-standard that
we use in our experiments. A sentence is subjective if it
contains at least one private-state expression of medium
or higher strength. The second class, which we call ob-
jective, consists of everything else. Thus, sentences with
only mild traces of subjectivity are tossed into the objec-
tive category, making the system’s goal to find the clearly
subjective sentences.
</bodyText>
<sectionHeader confidence="0.5679535" genericHeader="method">
3 Using Extraction Patterns to Learn
Subjective Nouns
</sectionHeader>
<bodyText confidence="0.97595603125">
In the last few years, two bootstrapping algorithms have
been developed to create semantic dictionaries by ex-
ploiting extraction patterns: Meta-Bootstrapping (Riloff
and Jones, 1999) and Basilisk (Thelen and Riloff, 2002).
Extraction patterns were originally developed for infor-
mation extraction tasks (Cardie, 1997). They represent
lexico-syntactic expressions that typically rely on shal-
low parsing and syntactic role assignment. For example,
the pattern “&lt;subject&gt; was hired” would apply to sen-
tences that contain the verb “hired” in the passive voice.
The subject would be extracted as the hiree.
Meta-Bootstrapping and Basilisk were designed to
learn words that belong to a semantic category (e.g.,
Tagger A Subj
Obj
“truck” is a VEHICLE and “seashore” is a LOCATION).
Both algorithms begin with unannotated texts and seed
words that represent a semantic category. A bootstrap-
ping process looks for words that appear in the same ex-
traction patterns as the seeds and hypothesizes that those
words belong to the same semantic class. The principle
behind this approach is that words of the same semantic
class appear in similar pattern contexts. For example, the
phrases “lived in” and “traveled to” will co-occur with
many noun phrases that represent LOCATIONS.
In our research, we want to automatically identify
words that are subjective. Subjective terms have many
different semantic meanings, but we believe that the same
contextual principle applies to subjectivity. In this sec-
tion, we briefly overview these bootstrapping algorithms
and explain how we used them to generate lists of subjec-
tive nouns.
</bodyText>
<subsectionHeader confidence="0.999812">
3.1 Meta-Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999971703703704">
The Meta-Bootstrapping (“MetaBoot”) process (Riloff
and Jones, 1999) begins with a small set of seed words
that represent a targeted semantic category (e.g., 10
words that represent LOCATIONS) and an unannotated
corpus. First, MetaBoot automatically creates a set of ex-
traction patterns for the corpus by applying and instanti-
ating syntactic templates. This process literally produces
thousands of extraction patterns that, collectively, will ex-
tract every noun phrase in the corpus. Next, MetaBoot
computes a score for each pattern based upon the num-
ber of seed words among its extractions. The best pat-
tern is saved and all of its extracted noun phrases are
automatically labeled as the targeted semantic category.2
MetaBoot then re-scores the extraction patterns, using the
original seed words as well as the newly labeled words,
and the process repeats. This procedure is called mutual
bootstrapping.
A second level of bootstrapping (the “meta-” boot-
strapping part) makes the algorithm more robust. When
the mutual bootstrapping process is finished, all nouns
that were put into the semantic dictionary are re-
evaluated. Each noun is assigned a score based on how
many different patterns extracted it. Only the five best
nouns are allowed to remain in the dictionary. The other
entries are discarded, and the mutual bootstrapping pro-
cess starts over again using the revised semantic dictio-
nary.
</bodyText>
<subsectionHeader confidence="0.997861">
3.2 Basilisk
</subsectionHeader>
<bodyText confidence="0.9986065">
Basilisk (Thelen and Riloff, 2002) is a more recent boot-
strapping algorithm that also utilizes extraction patterns
to create a semantic dictionary. Similarly, Basilisk be-
gins with an unannotated text corpus and a small set of
</bodyText>
<footnote confidence="0.598299">
2Our implementation of Meta-Bootstrapping learns individ-
ual nouns (vs. noun phrases) and discards capitalized words.
</footnote>
<bodyText confidence="0.999980321428571">
seed words for a semantic category. The bootstrapping
process involves three steps. (1) Basilisk automatically
generates a set of extraction patterns for the corpus and
scores each pattern based upon the number of seed words
among its extractions. This step is identical to the first
step of Meta-Bootstrapping. Basilisk then puts the best
patterns into a Pattern Pool. (2) All nouns3 extracted by a
pattern in the Pattern Pool are put into a Candidate Word
Pool. Basilisk scores each noun based upon the set of
patterns that extracted it and their collective association
with the seed words. (3) The top 10 nouns are labeled as
the targeted semantic class and are added to the dictio-
nary. The bootstrapping process then repeats, using the
original seeds and the newly labeled words.
The main difference between Basilisk and Meta-
Bootstrapping is that Basilisk scores each noun based
on collective information gathered from all patterns that
extracted it. In contrast, Meta-Bootstrapping identifies
a single best pattern and assumes that everything it ex-
tracted belongs to the same semantic class. The second
level of bootstrapping smoothes over some of the prob-
lems caused by this assumption. In comparative experi-
ments (Thelen and Riloff, 2002), Basilisk outperformed
Meta-Bootstrapping. But since our goal of learning sub-
jective nouns is different from the original intent of the
algorithms, we tried them both. We also suspected they
might learn different words, in which case using both al-
gorithms could be worthwhile.
</bodyText>
<subsectionHeader confidence="0.993793">
3.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99964905">
The Meta-Bootstrapping and Basilisk algorithms need
seed words and an unannotated text corpus as input.
Since we did not need annotated texts, we created a much
larger training corpus, the bootstrapping corpus, by gath-
ering 950 new texts from the FBIS source mentioned
in Section 2.2. To find candidate seed words, we auto-
matically identified 850 nouns that were positively corre-
lated with subjective sentences in another data set. How-
ever, it is crucial that the seed words occur frequently
in our FBIS texts or the bootstrapping process will not
get off the ground. So we searched for each of the 850
nouns in the bootstrapping corpus, sorted them by fre-
quency, and manually selected 20 high-frequency words
that we judged to be strongly subjective. Table 3 shows
the 20 seed words used for both Meta-Bootstrapping and
Basilisk.
We ran each bootstrapping algorithm for 400 itera-
tions, generating 5 words per iteration. Basilisk gener-
ated 2000 nouns and Meta-Bootstrapping generated 1996
nouns.4 Table 4 shows some examples of extraction pat-
</bodyText>
<footnote confidence="0.929599333333333">
3Technically, each head noun of an extracted noun phrase.
4Meta-Bootstrapping will sometimes produce fewer than 5
words per iteration if it has low confidence in its judgements.
</footnote>
<table confidence="0.9998508">
cowardice embarrassment hatred outrage
crap fool hell slander
delight gloom hypocrisy sigh
disdain grievance love twit
dismay happiness nonsense virtue
</table>
<tableCaption confidence="0.89914">
Table 3: Subjective Seed Words
</tableCaption>
<table confidence="0.717177277777778">
B M BnM BUM
StrongSubj 372 192 110 454
WeakSubj 453 330 185 598
Total 825 522 295 1052
Table 6: Subjective Word Lexicons after Manual Review
(B=Basilisk, M=MetaBootstrapping)
Extraction Patterns Examples of Extracted Nouns
expressed &lt;dobj&gt; condolences, hope, grief,
views, worries, recognition
indicative of &lt;np&gt; compromise, desire, thinking
inject &lt;dobj&gt; vitality, hatred
reaffirmed &lt;dobj&gt; resolve, position, commitment
voiced &lt;dobj&gt; outrage, support, skepticism,
disagreement, opposition,
concerns, gratitude, indignation
show of &lt;np&gt; support, strength, goodwill,
solidarity, feeling
&lt;subject&gt; was shared anxiety, view, niceties, feeling
</table>
<tableCaption confidence="0.999643">
Table 4: Extraction Pattern Examples
</tableCaption>
<bodyText confidence="0.998989636363636">
terns that were discovered to be associated with subjec-
tive nouns.
Meta-Bootstrapping and Basilisk are semi-automatic
lexicon generation tools because, although the bootstrap-
ping process is 100% automatic, the resulting lexicons
need to be reviewed by a human.5 So we manually re-
viewed the 3996 words proposed by the algorithms. This
process is very fast; it takes only a few seconds to classify
each word. The entire review process took approximately
3-4 hours. One author did this labeling; this person did
not look at or run tests on the experiment corpus.
</bodyText>
<table confidence="0.999897777777778">
Strong Subjective Weak Subjective
tyranny scum aberration plague
smokescreen bully allusion risk
apologist devil apprehensions drama
barbarian liar beneficiary trick
belligerence pariah resistant promise
condemnation venom credence intrigue
sanctimonious diatribe distortion unity
exaggeration mockery eyebrows failures
repudiation anguish inclination tolerance
insinuation fallacies liability persistent
antagonism evil assault trust
atrocities genius benefit success
denunciation goodwill blood spirit
exploitation injustice controversy slump
humiliation innuendo likelihood sincerity
ill-treatment revenge peaceful eternity
sympathy rogue pressure rejection
</table>
<tableCaption confidence="0.999373">
Table 5: Examples of Learned Subjective Nouns
</tableCaption>
<footnote confidence="0.820567">
5This is because NLP systems expect dictionaries to have
high integrity. Even if the algorithms could achieve 90% ac-
curacy, a dictionary in which 1 of every 10 words is defined
incorrectly would probably not be desirable.
</footnote>
<figure confidence="0.7991905">
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Words Generated
</figure>
<figureCaption confidence="0.999967">
Figure 1: Accuracy during Bootstrapping
</figureCaption>
<bodyText confidence="0.9999763">
We classified the words as StrongSubjective, WeakSub-
jective, or Objective. Objective terms are not subjective at
all (e.g., “chair” or “city”). StrongSubjective terms have
strong, unambiguously subjective connotations, such as
“bully” or “barbarian”. WeakSubjective was used for
three situations: (1) words that have weak subjective con-
notations, such as “aberration” which implies something
out of the ordinary but does not evoke a strong sense of
judgement, (2) words that have multiple senses or uses,
where one is subjective but the other is not. For example,
the word “plague” can refer to a disease (objective) or an
onslaught of something negative (subjective), (3) words
that are objective by themselves but appear in idiomatic
expressions that are subjective. For example, the word
“eyebrows” was labeled WeakSubjective because the ex-
pression “raised eyebrows” probably occurs more often
in our corpus than literal references to “eyebrows”. Ta-
ble 5 shows examples of learned words that were classi-
fied as StrongSubjective or WeakSubjective.
Once the words had been manually classified, we could
go back and measure the effectiveness of the algorithms.
The graph in Figure 1 tracks their accuracy as the boot-
strapping progressed. The X-axis shows the number of
words generated so far. The Y-axis shows the percent-
age of those words that were manually classified as sub-
jective. As is typical of bootstrapping algorithms, ac-
curacy was high during the initial iterations but tapered
off as the bootstrapping continued. After 20 words,
both algorithms were 95% accurate. After 100 words
Basilisk was 75% accurate and MetaBoot was 81% accu-
</bodyText>
<figure confidence="0.994782083333333">
% of Words Subjective
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
’Basilisk’
’MetaBoot’
</figure>
<bodyText confidence="0.993494315789474">
rate. After 1000 words, accuracy dropped to about 28%
for MetaBoot, but Basilisk was still performing reason-
ably well at 53%. Although 53% accuracy is not high for
a fully automatic process, Basilisk depends on a human
to review the words so 53% accuracy means that the hu-
man is accepting every other word, on average. Thus, the
reviewer’s time was still being spent productively even
after 1000 words had been hypothesized.
Table 6 shows the size of the final lexicons created
by the bootstrapping algorithms. The first two columns
show the number of subjective terms learned by Basilisk
and Meta-Bootstrapping. Basilisk was more prolific, gen-
erating 825 subjective terms compared to 522 for Meta-
Bootstrapping. The third column shows the intersection
between their word lists. There was substantial overlap,
but both algorithms produced many words that the other
did not. The last column shows the results of merging
their lists. In total, the bootstrapping algorithms produced
1052 subjective nouns.
</bodyText>
<sectionHeader confidence="0.965617" genericHeader="method">
4 Creating Subjectivity Classifiers
</sectionHeader>
<bodyText confidence="0.999993">
To evaluate the subjective nouns, we trained a Naive
Bayes classifier using the nouns as features. We also in-
corporated previously established subjectivity clues, and
added some new discourse features. In this section, we
describe all the feature sets and present performance re-
sults for subjectivity classifiers trained on different com-
binations of these features. The threshold values and fea-
ture representations used in this section are the ones that
produced the best results on our separate tuning corpus.
</bodyText>
<subsectionHeader confidence="0.978346">
4.1 Subjective Noun Features
</subsectionHeader>
<bodyText confidence="0.9993340625">
We defined four features to represent the sets of subjec-
tive nouns produced by the bootstrapping algorithms.
BA-Strong: the set of StrongSubjective nouns generated
by Basilisk
BA-Weak: the set of WeakSubjective nouns generated
by Basilisk
MB-Strong: the set of StrongSubjective nouns generated
by Meta-Bootstrapping
MB-Weak: the set of WeakSubjective nouns generated
by Meta-Bootstrapping
For each set, we created a three-valued feature based on
the presence of 0, 1, or &gt; 2 words from that set. We used
the nouns as feature sets, rather than define a separate
feature for each word, so the classifier could generalize
over the set to minimize sparse data problems. We will
refer to these as the SubjNoun features.
</bodyText>
<subsectionHeader confidence="0.994097">
4.2 Previously Established Features
</subsectionHeader>
<bodyText confidence="0.999985032258064">
Wiebe, Bruce, &amp; O’Hara (1999) developed a machine
learning system to classify subjective sentences. We ex-
perimented with the features that they used, both to com-
pare their results to ours and to see if we could benefit
from their features. We will refer to these as the WBO
features.
WBO includes a set of stems positively correlated with
the subjective training examples (subjStems) and a set
of stems positively correlated with the objective training
examples (objStems). We defined a three-valued feature
for the presence of 0, 1, or &gt; 2 members of subjStems
in a sentence, and likewise for objStems. For our exper-
iments, subjStems includes stems that appear &gt; 7 times
in the training set, and for which the precision is 1.25
times the baseline word precision for that training set.
objStems contains the stems that appear &gt; 7 times and
for which at least 50% of their occurrences in the training
set are in objective sentences. WBO also includes a bi-
nary feature for each of the following: the presence in the
sentence of a pronoun, an adjective, a cardinal number, a
modal other than will, and an adverb other than not.
We also added manually-developed features found by
other researchers. We created 14 feature sets represent-
ing some classes from (Levin, 1993; Ballmer and Bren-
nenstuhl, 1981), some Framenet lemmas with frame ele-
ment experiencer (Baker et al., 1998), adjectives manu-
ally annotated for polarity (Hatzivassiloglou and McKe-
own, 1997), and some subjectivity clues listed in (Wiebe,
1990). We represented each set as a three-valued feature
based on the presence of 0, 1, or &gt; 2 members of the set.
We will refer to these as the manual features.
</bodyText>
<subsectionHeader confidence="0.99939">
4.3 Discourse Features
</subsectionHeader>
<bodyText confidence="0.999956428571429">
We created discourse features to capture the density of
clues in the text surrounding a sentence. First, we com-
puted the average number of subjective clues and objec-
tive clues per sentence, normalized by sentence length.
The subjective clues, subjClues, are all sets for which
3-valued features were defined above (except objStems).
The objective clues consist only of objStems. For sen-
</bodyText>
<equation confidence="0.91640625">
tence S, let ClueRatesubj(S) = |subjClues in S |and
|S|
ClueRateobj(S) = |objStems in S |.Then we define
|S|
</equation>
<bodyText confidence="0.988137">
AvgClueRatesubj to be the average of ClueRate(S)
over all sentences S and similarly for AvgClueRateobj.
Next, we characterize the number of subjective and
objective clues in the previous and next sentences as:
higher-than-expected (high), lower-than-expected (low),
or expected (medium). The value for ClueRatesubj(S)
is high if ClueRatesubj(S) &gt; AvgClueRatesubj * 1.3;
low if ClueRatesubj(S) &lt;_ AvgClueRatesubj/1.3; oth-
erwise it is medium. The values for ClueRateobj(S) are
defined similarly.
Using these definitions we created four features:
ClueRatesubj for the previous and following sen-
tences, and ClueRateobj for the previous and follow-
ing sentences. We also defined a feature for sentence
length. Let AvgSentLen be the average sentence length.
SentLen(S) is high if length(S) &gt; AvgSentLen*1.3;
low if length(S) &lt; AvgSentLen/1.3; and medium oth-
erwise.
</bodyText>
<subsectionHeader confidence="0.999549">
4.4 Classification Results
</subsectionHeader>
<bodyText confidence="0.999939727272727">
We conducted experiments to evaluate the performance
of the feature sets, both individually and in various com-
binations. Unless otherwise noted, all experiments in-
volved training a Naive Bayes classifier using a particu-
lar set of features. We evaluated each classifier using 25-
fold cross validation on the experiment corpus and used
paired t-tests to measure significance at the 95% confi-
dence level. As our evaluation metrics, we computed ac-
curacy (Acc) as the percentage of the system’s classifica-
tions that match the gold-standard, and precision (Prec)
and recall (Rec) with respect to subjective sentences.
</bodyText>
<table confidence="0.99476475">
Acc Prec Rec
(1) Bag-Of-Words 73.3 81.7 70.9
(2) WBO 72.1 76.0 77.4
(3) Most-Frequent 59.0 59.0 100.0
</table>
<tableCaption confidence="0.836796">
Table 7: Baselines for Comparison
Table 7 shows three baseline experiments. Row (3)
</tableCaption>
<bodyText confidence="0.997621115384615">
represents the common baseline of assigning every sen-
tence to the most frequent class. The Most-Frequent
baseline achieves 59% accuracy because 59% of the sen-
tences in the gold-standard are subjective. Row (2) is
a Naive Bayes classifier that uses the WBO features,
which performed well in prior research on sentence-level
subjectivity classification (Wiebe et al., 1999). Row (1)
shows a Naive Bayes classifier that uses unigram bag-of-
words features, with one binary feature for the absence
or presence in the sentence of each word that appeared
during training. Pang et al. (2002) reported that a similar
experiment produced their best results on a related clas-
sification task. The difference in accuracy between Rows
(1) and (2) is not statistically significant (Bag-of-Word’s
higher precision is balanced by WBO’s higher recall).
Next, we trained a Naive Bayes classifier using only
the SubjNoun features. This classifier achieved good
precision (77%) but only moderate recall (64%). Upon
further inspection, we discovered that the subjective
nouns are good subjectivity indicators when they appear,
but not every subjective sentence contains one of them.
And, relatively few sentences contain more than one,
making it difficult to recognize contextual effects (i.e.,
multiple clues in a region). We concluded that the ap-
propriate way to benefit from the subjective nouns is to
use them in tandem with other subjectivity clues.
</bodyText>
<table confidence="0.9986846">
Acc Prec Rec
76.1 81.3 77.4 WBO+SubjNoun+
manual+discourse
74.3 78.6 77.8 WBO+SubjNoun
72.1 76.0 77.4 WBO
</table>
<tableCaption confidence="0.999959">
Table 8: Results with New Features
</tableCaption>
<bodyText confidence="0.968195916666666">
Table 8 shows the results of Naive Bayes classifiers
trained with different combinations of features. The ac-
curacy differences between all pairs of experiments in
Table 8 are statistically significant. Row (3) uses only
the WBO features (also shown in Table 7 as a baseline).
Row (2) uses the WBO features as well as the SubjNoun
features. There is a synergy between these feature sets:
using both types of features achieves better performance
than either one alone. The difference is mainly precision,
presumably because the classifier found more and better
combinations of features. In Row (1), we also added the
manual and discourse features. The discourse features
explicitly identify contexts in which multiple clues are
found. This classifier produced even better performance,
achieving 81.3% precision with 77.4% recall. The 76.1%
accuracy result is significantly higher than the accuracy
results for all of the other classifiers (in both Table 8 and
Table 7).
Finally, higher precision classification can be obtained
by simply classifying a sentence as subjective if it con-
tains any of the StrongSubjective nouns. On our data, this
method produces 87% precision with 26% recall. This
approach could support applications for which precision
is paramount.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999954627906977">
Several types of research have involved document-level
subjectivity classification. Some work identifies inflam-
matory texts (e.g., (Spertus, 1997)) or classifies reviews
as positive or negative ((Turney, 2002; Pang et al., 2002)).
Tong’s system (Tong, 2001) generates sentiment time-
lines, tracking online discussions and creating graphs of
positive and negative opinion messages over time. Re-
search in genre classification may include recognition of
subjective genres such as editorials (e.g., (Karlgren and
Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)).
In contrast, our work classifies individual sentences, as
does the research in (Wiebe et al., 1999). Sentence-level
subjectivity classification is useful because most docu-
ments contain a mix of subjective and objective sen-
tences. For example, newspaper articles are typically
thought to be relatively objective, but (Wiebe et al., 2001)
reported that, in their corpus, 44% of sentences (in arti-
cles that are not editorials or reviews) were subjective.
Some previous work has focused explicitly on learn-
ing subjective words and phrases. (Hatzivassiloglou and
McKeown, 1997) describes a method for identifying the
semantic orientation of words, for example that beauti-
ful expresses positive sentiments. Researchers have fo-
cused on learning adjectives or adjectival phrases (Tur-
ney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe,
2000) and verbs (Wiebe et al., 2001), but no previous
work has focused on learning nouns. A unique aspect
of our work is the use of bootstrapping methods that ex-
ploit extraction patterns. (Turney, 2002) used patterns
representing part-of-speech sequences, (Hatzivassiloglou
and McKeown, 1997) recognized adjectival phrases, and
(Wiebe et al., 2001) learned N-grams. The extraction
patterns used in our research are linguistically richer pat-
terns, requiring shallow parsing and syntactic role assign-
ment.
In recent years several techniques have been developed
for semantic lexicon creation (e.g., (Hearst, 1992; Riloff
and Shepherd, 1997; Roark and Charniak, 1998; Cara-
ballo, 1999)). Semantic word learning is different from
subjective word learning, but we have shown that Meta-
Bootstrapping and Basilisk could be successfully applied
to subjectivity learning. Perhaps some of these other
methods could also be used to learn subjective words.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999972368421053">
This research produced interesting insights as well as per-
formance results. First, we demonstrated that weakly
supervised bootstrapping techniques can learn subjec-
tive terms from unannotated texts. Subjective features
learned from unannotated documents can augment or en-
hance features learned from annotated training data us-
ing more traditional supervised learning techniques. Sec-
ond, Basilisk and Meta-Bootstrapping proved to be use-
ful for a different task than they were originally intended.
By seeding the algorithms with subjective words, the ex-
traction patterns identified expressions that are associated
with subjective nouns. This suggests that the bootstrap-
ping algorithms should be able to learn not only general
semantic categories, but any category for which words
appear in similar linguistic phrases. Third, our best sub-
jectivity classifier used a wide variety of features. Sub-
jectivity is a complex linguistic phenomenon and our evi-
dence suggests that reliable subjectivity classification re-
quires a broad array of features.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679813333334">
C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley
framenet project. In Proceedings ofthe COLING-ACL.
T. Ballmer and W. Brennenstuhl. 1981. Speech Act Clas-
sification: A Study in the Lexical Analysis of English
Speech Activity Verbs. Springer-Verlag.
A. Banfield. 1982. Unspeakable Sentences. Routledge
and Kegan Paul, Boston.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 120–126.
C. Cardie. 1997. Empirical Methods in Information Ex-
traction. AI Magazine, 18(4):65–79.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In ACL-EACL
1997.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th Interna-
tional Conference on Computational Linguistics.
J. Karlgren and D. Cutting. 1994. Recognizing text gen-
res with simple metrics using discriminant analysis. In
COLING-94.
B. Kessler, G. Nunberg, and H. Schutze. 1997. Auto-
matic detection of text genre. In Proc. ACL-EACL-97.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press, Chicago.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? sentiment classification using machine learning
techniques. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the 16th National Conference on Ar-
tificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117–124.
B. Roark and E. Charniak. 1998. Noun-phrase
Co-occurrence Statistics for Semi-automatic Seman-
tic Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110–1116.
E. Spertus. 1997. Smokey: Automatic recognition of
hostile messages. In Proc. IAAI.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing.
R. Tong. 2001. An operational system for detecting and
tracking opinions in on-line discussion. In SIGIR 2001
Workshop on Operational Text Classification.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Orientation Applied to Unsupervised Classification
of Reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics.
J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development
and use of a gold standard data set for subjectivity clas-
sifications. In Proc. 37th Annual Meeting of the Assoc.
for Computational Linguistics (ACL-99).
J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying col-
locations for recognizing opinions. In Proc. ACL-01
Workshop on Collocation: Computational Extraction,
Analysis, and Exploitation, July.
J. Wiebe. 1990. Recognizing Subjective Sentences: A
Computational Investigation of Narrative Text. Ph.D.
thesis, State University of New York at Buffalo.
J. Wiebe. 2000. Learning subjective adjectives from cor-
pora. In 17th National Conference on Artificial Intelli-
gence.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.130491">
<note confidence="0.941818333333333">Proceeings of the Seventh CoNLL conference held at HLT-NAACL 2003 , pp. 25-32 Edmonton, May-June 2003</note>
<title confidence="0.960784">Subjective Nouns using Extraction Pattern</title>
<author confidence="0.995444">Ellen</author>
<affiliation confidence="0.9983835">School of University of</affiliation>
<address confidence="0.461738">Salt Lake City, UT</address>
<email confidence="0.998711">riloff@cs.utah.edu</email>
<author confidence="0.770045">Janyce</author>
<affiliation confidence="0.9998965">Department of Computer University of</affiliation>
<address confidence="0.776108">Pittsburgh, PA</address>
<email confidence="0.997971">wiebe@cs.pitt.edu</email>
<author confidence="0.961799">Theresa</author>
<affiliation confidence="0.998455">Intelligent Systems University of</affiliation>
<address confidence="0.78601">Pittsburgh, PA</address>
<email confidence="0.999441">twilson@cs.pitt.edu</email>
<abstract confidence="0.983884875">We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms. The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences. First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns. Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research. The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe COLING-ACL.</booktitle>
<contexts>
<context position="22318" citStr="Baker et al., 1998" startWordPosition="3423" endWordPosition="3426">precision for that training set. objStems contains the stems that appear &gt; 7 times and for which at least 50% of their occurrences in the training set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al., 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990). We represented each set as a three-valued feature based on the presence of 0, 1, or &gt; 2 members of the set. We will refer to these as the manual features. 4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence. First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence length. The subjective clues, subjClues, are all sets for which 3-valued </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley framenet project. In Proceedings ofthe COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ballmer</author>
<author>W Brennenstuhl</author>
</authors>
<title>Speech Act Classification: A Study in the Lexical Analysis of English Speech Activity Verbs.</title>
<date>1981</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="22244" citStr="Ballmer and Brennenstuhl, 1981" startWordPosition="3410" endWordPosition="3414">imes in the training set, and for which the precision is 1.25 times the baseline word precision for that training set. objStems contains the stems that appear &gt; 7 times and for which at least 50% of their occurrences in the training set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al., 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990). We represented each set as a three-valued feature based on the presence of 0, 1, or &gt; 2 members of the set. We will refer to these as the manual features. 4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence. First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence</context>
</contexts>
<marker>Ballmer, Brennenstuhl, 1981</marker>
<rawString>T. Ballmer and W. Brennenstuhl. 1981. Speech Act Classification: A Study in the Lexical Analysis of English Speech Activity Verbs. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Banfield</author>
</authors>
<title>Unspeakable Sentences. Routledge and Kegan Paul,</title>
<date>1982</date>
<location>Boston.</location>
<contexts>
<context position="4485" citStr="Banfield, 1982" startWordPosition="639" endWordPosition="640">(77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al., 1999). 2 Subjectivity Data 2.1 The Annotation Scheme In 2002, an annotation scheme was developed for a U.S. government-sponsored project with a team of 10 researchers (the annotation instructions and project reports are available on the Web at http://www.cs.pitt.edu/˜wiebe/pubs/ardasummer02/). The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982). The scheme is more detailed and comprehensive than previous ones. We mention only those aspects of the annotation scheme relevant to this paper. The goal of the annotation scheme is to identify and characterize expressions of private states in a sentence. Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al., 1985). For example, in sentence (1) the writer is expressing a negative evaluation. (1) “The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long.” Sentence (2) reflects the private state of</context>
</contexts>
<marker>Banfield, 1982</marker>
<rawString>A. Banfield. 1982. Unspeakable Sentences. Routledge and Kegan Paul, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<title>Automatic Acquisition of a Hypernym-Labeled Noun Hierarchy from Text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="29737" citStr="Caraballo, 1999" startWordPosition="4554" endWordPosition="4556">ocused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999)). Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning. Perhaps some of these other methods could also be used to learn subjective words. 6 Conclusions This research produced interesting insights as well as performance results. First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts. Subjective features learned from unannotated documents can augment or enhance features learned from annotated training data us</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>S. Caraballo. 1999. Automatic Acquisition of a Hypernym-Labeled Noun Hierarchy from Text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>Empirical Methods in Information Extraction.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8595" citStr="Cardie, 1997" startWordPosition="1296" endWordPosition="1297">strength. The second class, which we call objective, consists of everything else. Thus, sentences with only mild traces of subjectivity are tossed into the objective category, making the system’s goal to find the clearly subjective sentences. 3 Using Extraction Patterns to Learn Subjective Nouns In the last few years, two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns: Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002). Extraction patterns were originally developed for information extraction tasks (Cardie, 1997). They represent lexico-syntactic expressions that typically rely on shallow parsing and syntactic role assignment. For example, the pattern “&lt;subject&gt; was hired” would apply to sentences that contain the verb “hired” in the passive voice. The subject would be extracted as the hiree. Meta-Bootstrapping and Basilisk were designed to learn words that belong to a semantic category (e.g., Tagger A Subj Obj “truck” is a VEHICLE and “seashore” is a LOCATION). Both algorithms begin with unannotated texts and seed words that represent a semantic category. A bootstrapping process looks for words that a</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>C. Cardie. 1997. Empirical Methods in Information Extraction. AI Magazine, 18(4):65–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In ACL-EACL</booktitle>
<contexts>
<context position="22399" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="3433" endWordPosition="3437"> appear &gt; 7 times and for which at least 50% of their occurrences in the training set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al., 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990). We represented each set as a three-valued feature based on the presence of 0, 1, or &gt; 2 members of the set. We will refer to these as the manual features. 4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence. First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence length. The subjective clues, subjClues, are all sets for which 3-valued features were defined above (except objStems). The objective clues consist only o</context>
<context position="28800" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="4417" endWordPosition="4420">and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou and McKeown, 1997) describes a method for identifying the semantic orientation of words, for example that beautiful expresses positive sentiments. Researchers have focused on learning adjectives or adjectival phrases (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000) and verbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K. McKeown. 1997. Predicting the semantic orientation of adjectives. In ACL-EACL 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="29666" citStr="Hearst, 1992" startWordPosition="4544" endWordPosition="4545">e, 2000) and verbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999)). Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning. Perhaps some of these other methods could also be used to learn subjective words. 6 Conclusions This research produced interesting insights as well as performance results. First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts. Subjective features learned from unannotated documents </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Karlgren</author>
<author>D Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In COLING-94.</booktitle>
<contexts>
<context position="28182" citStr="Karlgren and Cutting, 1994" startWordPosition="4323" endWordPosition="4326">th 26% recall. This approach could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou </context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>J. Karlgren and D. Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In COLING-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kessler</author>
<author>G Nunberg</author>
<author>H Schutze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proc. ACL-EACL-97.</booktitle>
<contexts>
<context position="28204" citStr="Kessler et al., 1997" startWordPosition="4327" endWordPosition="4330"> could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou and McKeown, 1997) des</context>
</contexts>
<marker>Kessler, Nunberg, Schutze, 1997</marker>
<rawString>B. Kessler, G. Nunberg, and H. Schutze. 1997. Automatic detection of text genre. In Proc. ACL-EACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="22211" citStr="Levin, 1993" startWordPosition="3408" endWordPosition="3409"> appear &gt; 7 times in the training set, and for which the precision is 1.25 times the baseline word precision for that training set. objStems contains the stems that appear &gt; 7 times and for which at least 50% of their occurrences in the training set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al., 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990). We represented each set as a three-valued feature based on the presence of 0, 1, or &gt; 2 members of the set. We will refer to these as the manual features. 4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence. First, we computed the average number of subjective clues and objective clues per</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="25393" citStr="Pang et al. (2002)" startWordPosition="3901" endWordPosition="3904">three baseline experiments. Row (3) represents the common baseline of assigning every sentence to the most frequent class. The Most-Frequent baseline achieves 59% accuracy because 59% of the sentences in the gold-standard are subjective. Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al., 1999). Row (1) shows a Naive Bayes classifier that uses unigram bag-ofwords features, with one binary feature for the absence or presence in the sentence of each word that appeared during training. Pang et al. (2002) reported that a similar experiment produced their best results on a related classification task. The difference in accuracy between Rows (1) and (2) is not statistically significant (Bag-of-Word’s higher precision is balanced by WBO’s higher recall). Next, we trained a Naive Bayes classifier using only the SubjNoun features. This classifier achieved good precision (77%) but only moderate recall (64%). Upon further inspection, we discovered that the subjective nouns are good subjectivity indicators when they appear, but not every subjective sentence contains one of them. And, relatively few se</context>
<context position="27889" citStr="Pang et al., 2002" startWordPosition="4282" endWordPosition="4285">accuracy results for all of the other classifiers (in both Table 8 and Table 7). Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns. On our data, this method produces 87% precision with 26% recall. This approach could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="4858" citStr="Quirk et al., 1985" startWordPosition="696" endWordPosition="699">e Web at http://www.cs.pitt.edu/˜wiebe/pubs/ardasummer02/). The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982). The scheme is more detailed and comprehensive than previous ones. We mention only those aspects of the annotation scheme relevant to this paper. The goal of the annotation scheme is to identify and characterize expressions of private states in a sentence. Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al., 1985). For example, in sentence (1) the writer is expressing a negative evaluation. (1) “The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long.” Sentence (2) reflects the private state of Western countries. Mugabe’s use of “overwhelmingly” also reflects a private state, his positive reaction to and characterization of his victory. (2) “Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwe’s presidential election.” Annotators are also asked to judge the strength of each private st</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2929" citStr="Riloff and Jones, 1999" startWordPosition="411" endWordPosition="414">A, NIMA, and NRO. document summarization systems need to summarize different opinions and perspectives. Spam filtering systems must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information. Subjective language has been previously studied in fields such as linguistics, literary theory, psychology, and content analysis. Some manually-developed knowledge resources exist, but there is no comprehensive dictionary of subjective language. Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002) are bootstrapping algorithms that use automatically generated extraction patterns to identify words belonging to a semantic category. We hypothesized that extraction patterns could also identify subjective words. For example, the pattern “expressed &lt;direct object&gt;” often extracts subjective nouns, such as “concern”, “hope”, and “support”. Furthermore, these bootstrapping algorithms require only a handful of seed words and unannotated texts for training; no annotated data is needed at all. In this paper, we use the Meta-Bootstrapping and Basilisk algorith</context>
<context position="8461" citStr="Riloff and Jones, 1999" startWordPosition="1276" endWordPosition="1279">gold-standard that we use in our experiments. A sentence is subjective if it contains at least one private-state expression of medium or higher strength. The second class, which we call objective, consists of everything else. Thus, sentences with only mild traces of subjectivity are tossed into the objective category, making the system’s goal to find the clearly subjective sentences. 3 Using Extraction Patterns to Learn Subjective Nouns In the last few years, two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns: Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002). Extraction patterns were originally developed for information extraction tasks (Cardie, 1997). They represent lexico-syntactic expressions that typically rely on shallow parsing and syntactic role assignment. For example, the pattern “&lt;subject&gt; was hired” would apply to sentences that contain the verb “hired” in the passive voice. The subject would be extracted as the hiree. Meta-Bootstrapping and Basilisk were designed to learn words that belong to a semantic category (e.g., Tagger A Subj Obj “truck” is a VEHICLE and “seashore” is a LOCATION). Both alg</context>
<context position="9980" citStr="Riloff and Jones, 1999" startWordPosition="1508" endWordPosition="1511"> words of the same semantic class appear in similar pattern contexts. For example, the phrases “lived in” and “traveled to” will co-occur with many noun phrases that represent LOCATIONS. In our research, we want to automatically identify words that are subjective. Subjective terms have many different semantic meanings, but we believe that the same contextual principle applies to subjectivity. In this section, we briefly overview these bootstrapping algorithms and explain how we used them to generate lists of subjective nouns. 3.1 Meta-Bootstrapping The Meta-Bootstrapping (“MetaBoot”) process (Riloff and Jones, 1999) begins with a small set of seed words that represent a targeted semantic category (e.g., 10 words that represent LOCATIONS) and an unannotated corpus. First, MetaBoot automatically creates a set of extraction patterns for the corpus by applying and instantiating syntactic templates. This process literally produces thousands of extraction patterns that, collectively, will extract every noun phrase in the corpus. Next, MetaBoot computes a score for each pattern based upon the number of seed words among its extractions. The best pattern is saved and all of its extracted noun phrases are automati</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the 16th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="29693" citStr="Riloff and Shepherd, 1997" startWordPosition="4546" endWordPosition="4549">erbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999)). Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning. Perhaps some of these other methods could also be used to learn subjective words. 6 Conclusions This research produced interesting insights as well as performance results. First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts. Subjective features learned from unannotated documents can augment or enhance feat</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase Co-occurrence Statistics for Semi-automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1110--1116</pages>
<contexts>
<context position="29719" citStr="Roark and Charniak, 1998" startWordPosition="4550" endWordPosition="4553">but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999)). Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning. Perhaps some of these other methods could also be used to learn subjective words. 6 Conclusions This research produced interesting insights as well as performance results. First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts. Subjective features learned from unannotated documents can augment or enhance features learned from annotate</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase Co-occurrence Statistics for Semi-automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1110–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Spertus</author>
</authors>
<title>Smokey: Automatic recognition of hostile messages.</title>
<date>1997</date>
<booktitle>In Proc. IAAI.</booktitle>
<contexts>
<context position="27807" citStr="Spertus, 1997" startWordPosition="4271" endWordPosition="4272">with 77.4% recall. The 76.1% accuracy result is significantly higher than the accuracy results for all of the other classifiers (in both Table 8 and Table 7). Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns. On our data, this method produces 87% precision with 26% recall. This approach could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents co</context>
</contexts>
<marker>Spertus, 1997</marker>
<rawString>E. Spertus. 1997. Smokey: Automatic recognition of hostile messages. In Proc. IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2968" citStr="Thelen and Riloff, 2002" startWordPosition="417" endWordPosition="420">on systems need to summarize different opinions and perspectives. Spam filtering systems must recognize rants and emotional tirades, among other things. In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information. Subjective language has been previously studied in fields such as linguistics, literary theory, psychology, and content analysis. Some manually-developed knowledge resources exist, but there is no comprehensive dictionary of subjective language. Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002) are bootstrapping algorithms that use automatically generated extraction patterns to identify words belonging to a semantic category. We hypothesized that extraction patterns could also identify subjective words. For example, the pattern “expressed &lt;direct object&gt;” often extracts subjective nouns, such as “concern”, “hope”, and “support”. Furthermore, these bootstrapping algorithms require only a handful of seed words and unannotated texts for training; no annotated data is needed at all. In this paper, we use the Meta-Bootstrapping and Basilisk algorithms to learn lists of subjective nouns f</context>
<context position="8500" citStr="Thelen and Riloff, 2002" startWordPosition="1282" endWordPosition="1285">iments. A sentence is subjective if it contains at least one private-state expression of medium or higher strength. The second class, which we call objective, consists of everything else. Thus, sentences with only mild traces of subjectivity are tossed into the objective category, making the system’s goal to find the clearly subjective sentences. 3 Using Extraction Patterns to Learn Subjective Nouns In the last few years, two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns: Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002). Extraction patterns were originally developed for information extraction tasks (Cardie, 1997). They represent lexico-syntactic expressions that typically rely on shallow parsing and syntactic role assignment. For example, the pattern “&lt;subject&gt; was hired” would apply to sentences that contain the verb “hired” in the passive voice. The subject would be extracted as the hiree. Meta-Bootstrapping and Basilisk were designed to learn words that belong to a semantic category (e.g., Tagger A Subj Obj “truck” is a VEHICLE and “seashore” is a LOCATION). Both algorithms begin with unannotated texts an</context>
<context position="11357" citStr="Thelen and Riloff, 2002" startWordPosition="1726" endWordPosition="1729">eled words, and the process repeats. This procedure is called mutual bootstrapping. A second level of bootstrapping (the “meta-” bootstrapping part) makes the algorithm more robust. When the mutual bootstrapping process is finished, all nouns that were put into the semantic dictionary are reevaluated. Each noun is assigned a score based on how many different patterns extracted it. Only the five best nouns are allowed to remain in the dictionary. The other entries are discarded, and the mutual bootstrapping process starts over again using the revised semantic dictionary. 3.2 Basilisk Basilisk (Thelen and Riloff, 2002) is a more recent bootstrapping algorithm that also utilizes extraction patterns to create a semantic dictionary. Similarly, Basilisk begins with an unannotated text corpus and a small set of 2Our implementation of Meta-Bootstrapping learns individual nouns (vs. noun phrases) and discards capitalized words. seed words for a semantic category. The bootstrapping process involves three steps. (1) Basilisk automatically generates a set of extraction patterns for the corpus and scores each pattern based upon the number of seed words among its extractions. This step is identical to the first step of</context>
<context position="12911" citStr="Thelen and Riloff, 2002" startWordPosition="1972" endWordPosition="1975">eled as the targeted semantic class and are added to the dictionary. The bootstrapping process then repeats, using the original seeds and the newly labeled words. The main difference between Basilisk and MetaBootstrapping is that Basilisk scores each noun based on collective information gathered from all patterns that extracted it. In contrast, Meta-Bootstrapping identifies a single best pattern and assumes that everything it extracted belongs to the same semantic class. The second level of bootstrapping smoothes over some of the problems caused by this assumption. In comparative experiments (Thelen and Riloff, 2002), Basilisk outperformed Meta-Bootstrapping. But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both. We also suspected they might learn different words, in which case using both algorithms could be worthwhile. 3.3 Experimental Results The Meta-Bootstrapping and Basilisk algorithms need seed words and an unannotated text corpus as input. Since we did not need annotated texts, we created a much larger training corpus, the bootstrapping corpus, by gathering 950 new texts from the FBIS source mentioned in Section 2.2. To find cand</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tong</author>
</authors>
<title>An operational system for detecting and tracking opinions in on-line discussion.</title>
<date>2001</date>
<booktitle>In SIGIR 2001 Workshop on Operational Text Classification.</booktitle>
<contexts>
<context position="27918" citStr="Tong, 2001" startWordPosition="4288" endWordPosition="4289">r classifiers (in both Table 8 and Table 7). Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns. On our data, this method produces 87% precision with 26% recall. This approach could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be </context>
</contexts>
<marker>Tong, 2001</marker>
<rawString>R. Tong. 2001. An operational system for detecting and tracking opinions in on-line discussion. In SIGIR 2001 Workshop on Operational Text Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27869" citStr="Turney, 2002" startWordPosition="4280" endWordPosition="4281">gher than the accuracy results for all of the other classifiers (in both Table 8 and Table 7). Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns. On our data, this method produces 87% precision with 26% recall. This approach could support applications for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example</context>
<context position="29259" citStr="Turney, 2002" startWordPosition="4490" endWordPosition="4491">ials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou and McKeown, 1997) describes a method for identifying the semantic orientation of words, for example that beautiful expresses positive sentiments. Researchers have focused on learning adjectives or adjectival phrases (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000) and verbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999)). Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilis</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O’Hara</author>
</authors>
<title>Development and use of a gold standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99).</booktitle>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>J. Wiebe, R. Bruce, and T. O’Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. In Proc. 37th Annual Meeting of the Assoc. for Computational Linguistics (ACL-99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>M Bell</author>
</authors>
<title>Identifying collocations for recognizing opinions.</title>
<date>2001</date>
<booktitle>In Proc. ACL-01 Workshop on Collocation: Computational Extraction, Analysis, and Exploitation,</booktitle>
<contexts>
<context position="28225" citStr="Wiebe et al., 2001" startWordPosition="4331" endWordPosition="4334">tions for which precision is paramount. 5 Related Work Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al., 2002)). Tong’s system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001)). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999). Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou and McKeown, 1997) describes a method for i</context>
</contexts>
<marker>Wiebe, Wilson, Bell, 2001</marker>
<rawString>J. Wiebe, T. Wilson, and M. Bell. 2001. Identifying collocations for recognizing opinions. In Proc. ACL-01 Workshop on Collocation: Computational Extraction, Analysis, and Exploitation, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Recognizing Subjective Sentences: A Computational Investigation of Narrative Text.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>State University of New York at Buffalo.</institution>
<contexts>
<context position="22452" citStr="Wiebe, 1990" startWordPosition="3444" endWordPosition="3445">ining set are in objective sentences. WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not. We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al., 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990). We represented each set as a three-valued feature based on the presence of 0, 1, or &gt; 2 members of the set. We will refer to these as the manual features. 4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence. First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence length. The subjective clues, subjClues, are all sets for which 3-valued features were defined above (except objStems). The objective clues consist only of objStems. For sentence S, let ClueRatesubj(S) = |su</context>
</contexts>
<marker>Wiebe, 1990</marker>
<rawString>J. Wiebe. 1990. Recognizing Subjective Sentences: A Computational Investigation of Narrative Text. Ph.D. thesis, State University of New York at Buffalo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In 17th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="29062" citStr="Wiebe, 2000" startWordPosition="4456" endWordPosition="4457">tences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective. Some previous work has focused explicitly on learning subjective words and phrases. (Hatzivassiloglou and McKeown, 1997) describes a method for identifying the semantic orientation of words, for example that beautiful expresses positive sentiments. Researchers have focused on learning adjectives or adjectival phrases (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000) and verbs (Wiebe et al., 2001), but no previous work has focused on learning nouns. A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns. (Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al., 2001) learned N-grams. The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment. In recent years several techniques have been developed for semantic lexicon creation (e.g., (Hearst, </context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>J. Wiebe. 2000. Learning subjective adjectives from corpora. In 17th National Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>