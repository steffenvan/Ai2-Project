<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000334">
<title confidence="0.9818">
Combining Heterogeneous Classifiers for Word-Sense
Disambiguation
</title>
<author confidence="0.7927565">
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning and Kristina Toutanova
</author>
<affiliation confidence="0.866767333333333">
Computer Science Department
Stanford University
Stanford, CA 94305-9040, USA
</affiliation>
<sectionHeader confidence="0.908842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999608692307692">
The Stanford-CS224N system is an ensemble of sim-
ple classifiers. The first-tier systems are heteroge-
neous, consisting primarily of naive-Bayes variants,
but also including vector space, memory-based, and
other classifier types. These simple classifiers are
combined by a second-tier classifier, which variously
uses majority voting, weighted voting, or a maxi-
mum entropy model. Results from SENSEVAL-2 lex-
ical sample tasks indicate that, while the individual
classifiers perform at a level comparable to middle-
scoring team&apos;s systems, the combination achieves
high performance. In this paper, we discuss both
our system and lessons learned from its behavior.
</bodyText>
<sectionHeader confidence="0.99559" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984833333333">
The problem of supervised word sense disam-
biguation (wsp) has been approached using
many different classification algorithms, includ-
ing naive Bayes, decision trees, decision lists,
and memory-based learners. While it is un-
questionable that certain algorithms are better
suited to the WSD problem than others (for a
comparison, see Mooney (1996)), it seems to be
the case that, given similar features as input,
various algorithms do not behave dramatically
differently. This was seen in the SENSEVAL-2 re-
sults where a large fraction of the systems had
scores clustered in a fairly narrow region.
We began building our system with 23 su-
pervised WSD systems, each submitted by a
student taking the natural language processing
course (CS224N) at Stanford University. Stu-
dents were free to implement whatever WSD
</bodyText>
<footnote confidence="0.897001666666667">
This paper is based on work supported in part by the
National Science Foundation under Grants IIS-0085896
and IIS-9982226, by an NSF Graduate Fellowship, and
by the Research Collaboration between NTT Communi-
cation Science Laboratories, Nippon Telegraph and Tele-
phone Corporation and CSLI, Stanford University.
</footnote>
<figure confidence="0.7969255">
Combination Classifier
First—tier Classifiers
</figure>
<figureCaption confidence="0.999532">
Figure I: Organization of the system.
</figureCaption>
<bodyText confidence="0.9996775625">
method they chose. While most implemented
variants of naive Bayes, some implemented a
range of other methods, including n-gram mod-
els, vector space models, and even memory-
based learners. Although none of these systems
alone would have produced more than middle-
level performance on the SENSEVAL-2 task, we
decided to investigate how they would behave
in combination.
In section 2, we discuss the first-tier classifiers
in greater depth and describe our methods of
combination. Section 3 discusses performance,
analyzing what benefit was found from combi-
nation, and when. We also discuss aspects of
the component systems which substantially in-
fluenced overall performance.
</bodyText>
<sectionHeader confidence="0.909936" genericHeader="method">
2 The System
</sectionHeader>
<bodyText confidence="0.999214111111111">
Figure 1 shows the high-level organization of
our system. First, each of the 23 classifiers is
run with 5-fold cross-validation on the train-
ing data. Classifiers are ranked, for each word,
based on their held-out accuracy. In any given
run of the system, for some k, the top k clas-
sifiers are kept, while lower-ranking classifiers
are discarded. These remaining classifiers are
combined by one of three methods.
</bodyText>
<listItem confidence="0.931096333333333">
• Majority voting: The sense output by the most
classifiers is chosen. Ties are broken in favor of
the highest-ranked classifier.
</listItem>
<page confidence="0.945421">
87
</page>
<listItem confidence="0.999081428571429">
• Weighted voting: Each classifier is assigned a vot-
ing weight (see below) and adds that weight to the
sense it outputs. The sense receiving the greatest
total weight is chosen.
• Maximum entropy: A maximum entropy classifier
is trained (see below) and run on the (classifier,
vote) outputs from the first tier.
</listItem>
<bodyText confidence="0.9998240625">
We consider k in the range {5, 7, 9, 11,13,15},
and so, once the ranking of the first-tier clas-
sifiers is set, there are 18 possible second-tier
classifiers.
We train and test each (k, method) pair
on the training data, again with 5-fold cross-
validation. The classifier type and k-value
which perform best on the held-out data are
chosen. Once the (k, method) pair is chosen,
all first-tier classifiers, as well as the parameters
for the second-tier combinator, are retrained on
the entire training corpus. Each target word
is considered an entirely separate task, and dif-
ferent first- and second-tier choices can be, and
are, made for each word. Table 1 shows what
second-tier choices were made for each word.
</bodyText>
<subsectionHeader confidence="0.755997">
2.1 Combination Methods
</subsectionHeader>
<bodyText confidence="0.985144166666667">
Our second-tier classifier takes training in-
stances of the form (s, sl, ,$) where s is
the correct sense and each s, is the sense chosen
by classifier i. We initially planned to combine
students&apos; classifiers using only a maximum en-
tropy model. Such a model has a set of features
fx(,) where each feature fx is true over a sub-
set of vectors g. A conditional maximum en-
tropy model with such features assigns, for any
given choices si, a distribution over the possible
senses s. This distribution is of the form:
exp Ex A, fx(s, si, , sk)
</bodyText>
<equation confidence="0.960028">
Et exp Ex f
-sxJx\-1 si, • • SO
</equation>
<bodyText confidence="0.996026875">
The intent was to design the features to recog-
nize and exploit &amp;quot;sense expertise&amp;quot; in the individ-
ual classifiers. For example, one classifier might
be trustworthy when reporting a certain sense
but less so for other senses. However, there was
nowhere near enough data to accurately esti-
mate parameters for such models.1
In fact, we noticed that, for certain words,
simple majority voting performed better than
1The number of features was not large, only one for
each (classifier, chosen sense, correct sense) triple. How-
ever, most senses are rarely chosen and rarely correct,
and so most features had zero or singleton support.
the maximum entropy model. It also turned
out that the most complex features we could
get value from were features of the form:
</bodyText>
<equation confidence="0.890712">
fz(S,S17 • &gt;SO = 1 &gt; s —s
-
</equation>
<bodyText confidence="0.997881333333333">
However, with only these features, the maxi-
mum entropy approach reduces to a weighted
vote; the s which maximizes the posterior prob-
</bodyText>
<equation confidence="0.926486">
ability P(sjsi, , sk) also maximizes the vote:
v(s) = Ei Ai6(si = s)
</equation>
<bodyText confidence="0.999389346153846">
The indicators 6 are true for exactly one sense,
and correspond to the simple L defined above.2
The sense with the highest vote value of v(s) will
be the sense with the highest posterior proba-
bility P(sIsi, sk) and will be chosen.
All three of our combination schemes can be
seen as ways of estimating the weights Ai. For
majority voting, we skip any attempt at statis-
tical estimation and simply assign each Ai to be
1/k. For the maximum entropy classifier, we
estimate the weights by maximizing the likeli-
hood of a held-out set, using the standard ITS
algorithm (Berger et al., 1996).
In weighted voting, we do something in be-
tween. We treat the 6 functions as probabilities,
treat v(s) as a mixture model, and do a single
round of EM to update the Ai starting from uni-
form weights. As we move from majority voting
to weighted voting to maximum entropy, the es-
timation becomes more sophisticated, but also
more prone to overfitting. Since solving overfit-
ting is hard, while choosing between classifiers
based on held-out data is relatively easy, this
spectrum gives us a way to gracefully handle
the range of sparsities in the training corpora
for different words.
</bodyText>
<subsectionHeader confidence="0.992348">
2.2 Individual Classifiers
</subsectionHeader>
<bodyText confidence="0.9998713">
While our first-tier classifiers implemented a va-
riety of classification algorithms, the differences
in their individual accuracies did not primarily
stem from the algorithm chosen. Rather,
implementation details led to the largest
differences. Naive-Bayes classifiers which chose
sensible window sizes, or dynamically chose
between window sizes tended to outperform
those which chose poor sizes. Generally, the
optimal windows were either of size one (which
</bodyText>
<footnote confidence="0.5752825">
21f the nth classifier en returns s as the sense, then
6(sn = s) is 1, otherwise it is zero.
</footnote>
<listItem confidence="0.348565">
• , sk) =
</listItem>
<page confidence="0.986747">
88
</page>
<bodyText confidence="0.999936413793104">
detected syntactic or collocation.al cues) or of
very large size (which detected more topical
cues). Programs with hard-wired window sizes
of, say, 5, performed poorly. Ironically, such
middle-size windows were commonly chosen by
students, but never useful; either extreme was
a better design.
Another implementation choice dramatically
affecting performance, also for naive-Bayes, was
the amount and type of smoothing. Heavy
smoothing and smoothing which backed off con-
ditional distributions to the relevant marginal
distributions gave good results, while insuf-
ficient smoothing or backing off to uniform
marginals gave substantially degraded results.3
There is one significant way in which our first-
tier classifiers were likely different from other
teams&apos; systems. In the original class project,
students were guaranteed that the ambiguous
word would only appear in a single orthographic
form. Since this was not true of the SENSEVAL-2
data, we mapped the ambiguous words (but not
their context words) down to a citation form.
We suspect that this lost quite a bit of informa-
tion, since there is considerable correlation be-
tween form and sense, especially for verbs, but
we made no attempt to re-engineer the student
systems, and have not thoroughly investigated
how big a difference this stemming made.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.915249523809524">
Table 1 shows the results per word, and table 2
shows results by part-of-speech. A wide range
of models are chosen, and the chosen model usu-
ally beats the best single classifier for that word,
on average by 1.9%. The improvement over the
globally best single classifier is even greater.
Notably, if we use the test data as an oracle
to chose the best combination method, rather
than relying on held-out data, accuracy jumps
by an average of 3.6%. This gap is dramati-
cally larger than the gap between the top scor-
ing systems for this SENSEVAL-2 task. While
the knowledge of actual best performance is ob-
viously not available, one might suspect that a
more sophisticated or better-tuned method of
31n particular, there is a defective behavior with naive
Bayes where, when one smoothes far too little, the cho-
sen sense is the one which has occurred with the most
words in the context window. For skewed-prior data
like the SENSEVAL-2 sets, this is invariably the common
sense, regardless of what the context words are.
</bodyText>
<figure confidence="0.953474571428571">
62
61
a 60
59
58
3 5 7 9 11 13 15
Number of Classifiers
</figure>
<figureCaption confidence="0.90692">
Figure 2: The accuracy of the various combina-
tion methods as the number of component systems
changes. The best single classifier is chosen per word
from held-out data and averaged. Chosen combina-
tion is also selected per word and averaged.
</figureCaption>
<bodyText confidence="0.998425394736842">
choosing a final combination model might lead
to significant improvement.
Figure 2 shows how the three combination
methods&apos; average scores varied with the num-
ber of component classifiers used. A critical as-
pect of our system is that the first-tier classi-
fiers are very diverse, not only in implementa-
tion but also in performance. Initially, accuracy
increases as added classifiers bring value to the
ensemble. However, as lower-quality classifiers
are added in, the better classifiers are steadily
drowned out. The weighted vote and maxi-
mum entropy combinations are less affected by
low-quality classifiers than the majority vote,
being able to suppress them with low weights.
Still, majority vote was a good method to have
around for words where weights could not be
usefully set by the other methods.
When combining heterogeneous classifiers,
one would like to know when and how the
combination will outperform the individuals.
One factor is how complementary the mistakes
of the individual classifiers are. We can mea-
sure this complementarity by averaging, over
all pairs of classifiers, the fraction of errors
that pair has in common. This gives average
pairwise error independence. Another factor is
the difficulty of the word being disambiguated.
A high most-frequent sense baseline means
that there is little room for improvement by
combining classifiers. Figure 3 shows, for the
overall top 7 first-tier classifiers, the absolute
gain between their average accuracy and the
accuracy of their majority. The x-axis is the dif-
ference between the pairwise independence and
the baseline accuracy. The pattern is loose, but
clear. The gain increases with complementarity
and decreases with the baseline.
</bodyText>
<figure confidence="0.6264482">
Chown
Combination
--M--Mauirturn
Entropy
--et-- Weighted
Vote
Majony
Vote
— — — Best Single
ClawTher
</figure>
<page confidence="0.955319">
89
</page>
<table confidence="0.997771283950618">
Single Combination Oracle Chosen
word base sngl vo t 7 wei 7 roe? best any used model
art-n 41.8 58.2 0 0 01 CO 01 0 )0 0 )0 0 co I&apos;m m co co m )6 0 0 07 0 0 0 0 CO. 0) )0 00 00 )1) 0, 01) C0 70 0 0 )0 0 0, 0, 00 0 CO 6 0 00 . DO c0, h CO GO 0 0 07 0 58.2 74.5 58.2 wei5
6i 6 6 6 m m 4 6 6 6 6 6 6 6 6 ocnc 4 4 4 .4 6 o6 oi 6 6 c6 6 6 6 6 6 6 6 6 6 n 6 6 6 6 6 6 6 6 6 n 6 6 6 6 6 6 6 4 6 4 .4 no&apos; o: to&apos; 6 6 6 co&apos;
. n co co n 6 oo 6 cn to 4 co co co m co 6 6 6 N 70 0 0. CO to 6 6 m co 6 to m r- 6 ro 6, 6 co . co to co m co 6 co. co . n
o- m 06 0)C N 1)-- 0 0— 0 N. CO 0 07 CO )1) 01 0 c—c 0 0 0 07 N. 0, 0) 0 N. CO CO 0 0— 0 0, 0 0) Cl 0 0 0 0 N 0 6 0 0 0 0 0, 0 0.. 0 0 0 0 0 0 0 .0) 0 0 0. 0
4 6 4 6 6 tri 6 6 M. 6 4 6 .4 6 6 0 6 01) 4 4 6 kri oi 6 6 6 6 6 6 6 6 to. ot 6 6 6 6 6 6 6 4 4 6 6 6 6 r4 6 6 6 6 6 6 6 co 6 6 4. 6 6 4. 6 4 6 6 6 oi 6 6 4 cO
CC N- 0 0 oo Cr, 00 CO . . CC) 0 07 00 01 CO 0, CO CO 0— 6 U) 0 0 0 0 0— cp CO 0 0 0 0- .0 0 N 0) 0 0 N C ce0 0 07 0 0— 00 0 00 0 0, 0— .0 N ).0 07 0 CC 0 tr, C 1,—
0. CO CO 0) CV 0 0.. 0 0. 0— 0— 07) 0 0 0 0 0, 0 0. 6 .0 0, 0 )0 .0 0 0 6 0 6 6 0 0 0 00 0 6 0 c,
c&apos;Fc ■C&apos;n c0 co .0 ,oc .0 cc- co co on c0 ‘,r c..c, to cc, co co ctn co ■tc co t- t&apos;s cto nos. 01 )0 &apos;0 0, 0 )0 C7) 1)— oocoon on co co o co,
authority-n 33.7 70.7 76.1 92.9 72.8 wei5
bar-n 39.7 72.2 71.5 86.8 65.6 me9
begin-v 58.6 81.4 86.1 95.0 84.3 rnel5
blind-a 83.6 76.9 87.3 94.5 87.3 wei7
bum-n 75.6 55.6 75.6 91.1 64.4 mel5
call-v 25.8 25.8 33.3 65.2 25.8 me5
carry-v 22.7 24.2 37.9 72.7 21.2 mel 5
chair-n 79.7 82_6 82.6 84.1 82.6 me5
channel-n 27.4 60.3 67.1 86.3 60.3 wei 7
child-n 54.7 79.7 78.1 89.1 75.0 mel5
church-n 53.1 73.4 76.6 90.6 75.0 me5
circuit-n 27.1 78.8 78.8 89.9 78.8 rne5
collaborate-v 90.0 90.0 90.0 90.0 90.0 weil5
colorless-a 65.7 62.9 68.6 85.7 62.9 vot7
cool-a 46.2 53.8 59.6 84.6 98.1 me5
day-n 59.3 62.1 69.0 84.8 67.6 me5
detention-n 65.6 84.4 84.4 90.6 84.4 wei5
develop-v 29.0 29.0 42.0 69.6 33.3 vot13
draw-v 9.8 24.4 31.7 43.9 24.4 me5
dress-v 42.4 49.2 49.2 72.9 49.2 wei9
drift-v 25.0 28.1 34.4 75.0 25.0 vot7
drive- v 28.6 26.2 45.2 69.0 45.2 weil5
dyke-n 89.3 92.9 92.9 96.4 92.9 vot5
face-v 83.9 67.7 86.0 88.2 83.9 wei 15
facility-n 48.3 67.2 74.1 91.9 65.5 weil5
faithful-a 78.3 78.3 78.3 100 78.3 wei 15
fatigue-n 76.7 90.7 93.0 93.0 90.7 wei 7
feeling-s 56.9 99.0 60.8 88.2 56.9 wei 9
find-v 14.7 29.4 30.9 55.9 29.4 vot 13
fine-a 38.6 51.4 61.4 80.0 55.7 me5
fit-a 51.7 82.8 89.7 96.6 89.7 wei 9
free-a 39.0 53.7 61.0 75.6 61.0 me9
graceful-a 75.9 79.3 79.3 89.7 79.3 vot9
green-a 78.7 83.0 85.1 92.6 89.0 me15
grip-n 54.9 74.5 70.6 84.3 66.7 melt
hearth-n 75.0 62.5 75.0 87.5 75.0 vot15
holiday-n 83.9 83.9 83.9 96.8 83.9 mel5
keep-v 37.3 47.8 52.2 68.7 47.8 me5
lady-n 69.8 77.4 79.2 83.0 79.2 wei 7
leave-v 31.8 40.9 45.5 75.8 93.9 vot15
live- v 50.7 62.7 67.2 79.1 58.2 mel5
local-a 57.9 68.9 73.7 92.1 68.4 vot 15
match-v 35.7 47.6 5-4.8 83.3 92.9 mel5
materiabn 42.0 46.4 60.9 88.9 58.0 well 1
mouth-n 45.0 50.0 58.3 90.0 51.7 vot9
nation-n 70.3 73.0 73.0 83.8 73.0 mel5
natural-a 27.2 55.3 55.3 79.6 52.4 wei13
nature-n 45.7 45.7 58.7 84.8 45.7 vot5
oblique-a 69.0 75.9 79.3 93.1 79.3 wei9
play- v 19.7 37.9 45.5 68.2 90.9 wei7
post-n 31.6 67.1 68.4 79.7 64.6 mel3
pull-v 21.7 25.0 35.0 71.7 33.3 rnel I
replace-v 53.3 53.3 55.6 88.9 53.3 vot 7
restraint-n 31.1 64.4 73.3 84.4 66.7 wei 1 I
see-v 31.9 37.7 43.5 60.9 90.6 vot15
sense-n 22.6 52.8 69.2 83.0 60.9 votll
serve-v 29.4 54.9 66.7 76.5 56.9 vot 15
simple-a 51.5 54.5 54.5 83.3 53.0 me5
solemn-a 96.0 96.0 96.0 96.0 96.0 weil 5
spade-n 63.6 63.6 81.8 81.8 75.8 wei I 5
stress-n 46.2 98.7 51.3 89.7 51.3 me9
strike-v 16.7 22.2 38.9 66.7 35.2 wei 1 5
train-v 30.2 54.0 60.3 84.1 55.6 weill
treat-v 38.6 47.7 59.1 95.5 54.5 vot 7
turn-v 14.9 23.9 39.3 58.2 31.3 wei 11
use-v 65.8 64.5 68.4 81.6 65.8 me9
vital-a 92.1 92.1 92.1 92.1 92.1 weil5
wander-v 80.0 80.0 82.0 82.0 80.0 mel5
wash-v 25.0 66.7 58.3 83.3 25.0 vot 15
work-v 26.7 50.0 45.0 76.7 91.7 wei 13
yew-n 78.6 78.6 78.6 82A 78.6 mel5
</table>
<tableCaption confidence="0.888425923076923">
Table 1: Results by word. Single classifiers: base
= most-frequent-sense baseline, sngl = best single
first-tier classifier as chosen on held-out data for that
word. Fixed combinations: vot = majority vote, wei
= weighted vote, me = maximum entropy combina-
tion; all are shown for the top seven classifiers only.
Oracle bounds: best = best combination system as
measured on the test data, any = test cases where
at least one first-tier classifier produced the correct
answer. Actually chosen: model shows which model
performed best according to held-out data, and used
shows its performance, which were our results for
the SENSEVAL-2 English lexical sample task.
</tableCaption>
<table confidence="0.9995955">
Single Combination Oracle Chosen
base sngl vot7 wei 7 me7 best any used
noun 50.5 67.0 65.8 66.4 67.7 71.7 86.6 68.3
adjective 57.8 67.1 68.0 68.9 67.8 71.1 86.7 68.6
verb 40.2 49-8 52.8 53.0 52.1 56.8 76.9 52.3
average 47.5 59.8 60.8 61.1 61.2 65.4 82.6 61.7
</table>
<tableCaption confidence="0.999201">
Table 2: Results by part-of-speech, and overall.
</tableCaption>
<figure confidence="0.981532363636364">
• •
. •
• • ... •
• •. ti •
• .4
• • • • r***
..--- • • . s • • •
i. - • •
• •
-100 -80 -60 -40 -20 0 20 40
Error Independence minus Baseline (percent)
</figure>
<figureCaption confidence="0.970794666666667">
Figure 3: Gain in accuracy of majority vote over
the average component performance as (pair-
wise independence - baseline accuracy) grows.
</figureCaption>
<sectionHeader confidence="0.998009" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999979444444444">
We have demonstrated that the combination of
a number of heterogeneous classifiers can lead
to a substantial performance increase over the
individual classifiers. Our system is robust to
both the wide range of accuracy of the first-tier
classifiers and to sparsity of training data when
building the second-tier classifier. The system&apos;s
overall accuracy is high, despite the medium
level of accuracy of the component systems.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.5991395">
We wish to thank the following people for
contributing their classifiers to the Stanford-
</bodyText>
<reference confidence="0.997980333333333">
CS224N system: Zoe Abrams, Jenny Berglund,
Dmitri Bobrovnikoff, Chris Callison-Burch,
Marcos Chavira, Shipra Dingare, Elizabeth
Douglas, Sarah Harris, Ido Milstein, Jyotir-
moy Paul, Soumya Raychaudhuri, Paul Ruhlen,
Magnus Sandberg, Adil Sherwani, Philip Shi-
lane, Joshua Solomin, Patrick Sutphin, Yuliya
Tarnikova, Ben Taskar, Kristina Toutanova,
Christopher Unkel, and Vincent Vanhoucke.
</reference>
<sectionHeader confidence="0.932246" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.690401625">
A. L. Berger, S. A. Della Pietra, and V. J.
Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Com-
putational Linguistics, 22:39-71.
R. J. Mooney. 1996. Comparative experiments
on disambiguating word senses: An illustra-
tion of the role of bias in machine learning.
In EMNLP I, pages 82-91.
</reference>
<figure confidence="0.991212555555555">
25
i 20
&amp;quot;FE 15
0
10
5
&apos;T6
0
5
</figure>
<page confidence="0.944098">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480460">
<title confidence="0.9998315">Combining Heterogeneous Classifiers for Disambiguation</title>
<author confidence="0.8592675">H Tolga Ilhan</author>
<author confidence="0.8592675">Sepandar D Kamvar</author>
<author confidence="0.8592675">Dan Christopher D Manning</author>
<author confidence="0.8592675">Kristina</author>
<affiliation confidence="0.834288">Computer Science Stanford</affiliation>
<address confidence="0.993543">Stanford, CA 94305-9040, USA</address>
<abstract confidence="0.999635428571429">The Stanford-CS224N system is an ensemble of simple classifiers. The first-tier systems are heterogeneous, consisting primarily of naive-Bayes variants, but also including vector space, memory-based, and other classifier types. These simple classifiers are combined by a second-tier classifier, which variously uses majority voting, weighted voting, or a maxientropy model. Results from lexical sample tasks indicate that, while the individual classifiers perform at a level comparable to middlescoring team&apos;s systems, the combination achieves high performance. In this paper, we discuss both our system and lessons learned from its behavior.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>CS224N system Zoe Abrams</author>
<author>Jenny Berglund</author>
<author>Dmitri Bobrovnikoff</author>
<author>Chris Callison-Burch</author>
<author>Marcos Chavira</author>
</authors>
<title>Shipra Dingare, Elizabeth Douglas, Sarah Harris, Ido Milstein, Jyotirmoy Paul, Soumya Raychaudhuri,</title>
<institution>Paul Ruhlen, Magnus Sandberg, Adil Sherwani, Philip Shilane, Joshua Solomin, Patrick Sutphin, Yuliya Tarnikova, Ben Taskar, Kristina Toutanova, Christopher Unkel, and Vincent Vanhoucke.</institution>
<marker>Abrams, Berglund, Bobrovnikoff, Callison-Burch, Chavira, </marker>
<rawString>CS224N system: Zoe Abrams, Jenny Berglund, Dmitri Bobrovnikoff, Chris Callison-Burch, Marcos Chavira, Shipra Dingare, Elizabeth Douglas, Sarah Harris, Ido Milstein, Jyotirmoy Paul, Soumya Raychaudhuri, Paul Ruhlen, Magnus Sandberg, Adil Sherwani, Philip Shilane, Joshua Solomin, Patrick Sutphin, Yuliya Tarnikova, Ben Taskar, Kristina Toutanova, Christopher Unkel, and Vincent Vanhoucke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="6531" citStr="Berger et al., 1996" startWordPosition="1059" endWordPosition="1062">zes the vote: v(s) = Ei Ai6(si = s) The indicators 6 are true for exactly one sense, and correspond to the simple L defined above.2 The sense with the highest vote value of v(s) will be the sense with the highest posterior probability P(sIsi, sk) and will be chosen. All three of our combination schemes can be seen as ways of estimating the weights Ai. For majority voting, we skip any attempt at statistical estimation and simply assign each Ai to be 1/k. For the maximum entropy classifier, we estimate the weights by maximizing the likelihood of a held-out set, using the standard ITS algorithm (Berger et al., 1996). In weighted voting, we do something in between. We treat the 6 functions as probabilities, treat v(s) as a mixture model, and do a single round of EM to update the Ai starting from uniform weights. As we move from majority voting to weighted voting to maximum entropy, the estimation becomes more sophisticated, but also more prone to overfitting. Since solving overfitting is hard, while choosing between classifiers based on held-out data is relatively easy, this spectrum gives us a way to gracefully handle the range of sparsities in the training corpora for different words. 2.2 Individual Cla</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In EMNLP I,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="1262" citStr="Mooney (1996)" startWordPosition="176" endWordPosition="177">xical sample tasks indicate that, while the individual classifiers perform at a level comparable to middlescoring team&apos;s systems, the combination achieves high performance. In this paper, we discuss both our system and lessons learned from its behavior. 1 Introduction The problem of supervised word sense disambiguation (wsp) has been approached using many different classification algorithms, including naive Bayes, decision trees, decision lists, and memory-based learners. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems to be the case that, given similar features as input, various algorithms do not behave dramatically differently. This was seen in the SENSEVAL-2 results where a large fraction of the systems had scores clustered in a fairly narrow region. We began building our system with 23 supervised WSD systems, each submitted by a student taking the natural language processing course (CS224N) at Stanford University. Students were free to implement whatever WSD This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS-9982226, by an NSF Gra</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. J. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In EMNLP I, pages 82-91.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>