<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021206">
<title confidence="0.99218">
Chinese Morphological Analysis with Character-level POS Tagging
</title>
<author confidence="0.996072">
Mo Shen†, Hongxiao Liu$, Daisuke Kawahara†, and Sadao Kurohashi††Graduate School of Informatics, Kyoto University, Japan
</author>
<affiliation confidence="0.994321">
$School of Computer Science, Fudan University, China
</affiliation>
<email confidence="0.9878835">
shen@nlp.ist.i.kyoto-u.ac.jp {dk,kuro}@i.kyoto-u.ac.jp
12210240027@fudan.edu.cn
</email>
<sectionHeader confidence="0.995527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823588235294">
The focus of recent studies on Chinese word
segmentation, part-of-speech (POS) tagging
and parsing has been shifting from words to
characters. However, existing methods have
not yet fully utilized the potentials of Chinese
characters. In this paper, we investigate the
usefulness of character-level part-of-speech
in the task of Chinese morphological analysis.
We propose the first tagset designed for the
task of character-level POS tagging. We pro-
pose a method that performs character-level
POS tagging jointly with word segmentation
and word-level POS tagging. Through exper-
iments, we demonstrate that by introducing
character-level POS information, the perfor-
mance of a baseline morphological analyzer
can be significantly improved.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997411101694915">
In recent years, the focus of research on Chinese
word segmentation, part-of-speech (POS) tag-
ging and parsing has been shifting from words
toward characters. Character-based methods
have shown superior performance in these tasks
compared to traditional word-based methods (Ng
and Low, 2004; Nakagawa, 2004; Zhao et al.,
2006; Kruengkrai et al., 2009; Xue, 2003; Sun,
2010). Studies investigating the morphological-
level and character-level internal structures of
words, which treat character as the true atom of
morphological and syntactic processing, have
demonstrated encouraging results (Li, 2011; Li
and Zhou, 2012; Zhang et al., 2013). This line of
research has provided great insight in revealing
the roles of characters in word formation and
syntax of Chinese language.
However, existing methods have not yet fully
utilized the potentials of Chinese characters.
While Li (2011) pointed out that some characters
Character-level Examples of Verb
Part-of-Speech
verb + noun Mf (invest : throw + wealth)
noun + verb bA_ (feel sorry : heart + hurt)
verb + adjective i`� (realize : recognize +
clear)
adjective + verb �N, (hate : pain + hate)
verb + verb *1 (inspect : examine + re-
view)
Table 1. Character-level POS sequence as a
more specified version of word-level POS: an
example of verb.
can productively form new words by attaching to
existing words, these characters consist only a
portion of all Chinese characters and appear in
35% of the words in Chinese Treebank 5.0
(CTB5) (Xue et al., 2005). Zhang (2013) took
one step further by investigating the character-
level structures of words; however, the machine
learning of inferring these internal structures re-
lies on the character forms, which still suffers
from data sparseness.
In our view, since each Chinese character is in
fact created as a word in origin with complete
and independent meaning, it should be treated as
the actual minimal morphological unit in Chinese
language, and therefore should carry specific
part-of-speech. For example, the character “TT”
(beat) is a verb and the character “TA” (broken) is
an adjective. A word on the other hand, is either
single-character, or a compound formed by sin-
gle-character words. For example, the verb “TT
TA” (break) can be seen as a compound formed
by the two single-character words with the con-
struction “verb + adjective”.
Under this treatment, we observe that words
with the same construction in terms of character-
level POS tend to also have similar syntactic
roles. For example, the words having the con-
</bodyText>
<page confidence="0.975891">
253
</page>
<bodyText confidence="0.936906333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 253–258,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
struction “verb + adjective” are typically verbs,
and those having the construction “adjective +
noun” are typically nouns, as shown in the fol-
lowing examples:
</bodyText>
<equation confidence="0.985142875">
(a) verb : verb + adjective
“打破”(break) : “打”(beat) + “破”(broken)
“更新”(update) : “更”(replace) + “新”(new)
“漂白”(bleach) : “漂”(wash) + “白”(white)
(b) noun : adjective + noun
“主题”(theme) : “主”(main) + “题”(topic)
“新人”(newcomer) : “新”(new) + “人”(person)
“快车”(express) : “快”(fast) + “车”(car)
</equation>
<bodyText confidence="0.99977875">
This suggests that character-level POS can be
used as cues in predicting the part-of-speech of
unknown words.
Another advantage of character-level POS is
that, the sequence of character-level POS in a
word can be seen as a more fine-grained version
of word-level POS. An example is shown in Ta-
ble 1. The five words in this table are very likely
to be tagged with the same word-level POS as
verb in any available annotated corpora, while it
can be commonly agreed among native speakers
of Chinese that the syntactic behaviors of these
words are different from each other, due to their
distinctions in word constructions. For example,
verbs having the construction “verb + noun” (e.g.
投资) or “verb + verb” (e.g. 审查) can also be
nouns in some context, while others cannot; And
verbs having the constructions “verb + adjective”
(e.g. 认清) require exact one object argument,
while others generally do not. Therefore, com-
pared to word-level POS, the character-level
POS can produce information for more expres-
sive features during the learning process of a
morphological analyzer.
In this paper, we investigate the usefulness of
character-level POS in the task of Chinese mor-
phological analysis. We propose the first tagset
designed for the task of character-level POS tag-
ging, based on which we manually annotate the
entire CTB5. We propose a method that performs
character-level POS tagging jointly with word
segmentation and word-level POS tagging.
Through experiments, we demonstrate that by
introducing character-level POS information, the
performance of a baseline morphological analyz-
er can be significantly improved.
</bodyText>
<table confidence="0.9990211">
Tag Part-of-Speech Example
n noun 法案/NN (bill)
v verb 发布/VV (publish)
j adj./adv. 广阔/VA (vast)
t numerical 三点一四/CD (3.14)
m quantifier 一/CD 件/M (a piece of)
d date 九五年/NT (1995)
k proper noun 中美/NR (sino-US)
b prefix 副市长/NN (vice mayor)
e suffix 建筑业/NN (construction
inductry)
r transliteration 阿尔帕德/NR (Árpád)
u punctuation 查尔斯 · 狄更斯 /NR
(Charles Dickens)
f foreign chars X 射线/NN (X-ray)
o onomatopoeia 隆隆/AD (rumble)
s surname 王新民/NR (Wang
Xinmin)
p pronoun 他们/PN (they)
c other functional 用于/VV (be used for)
</table>
<tableCaption confidence="0.887526">
Table 2. Tagset for character-level part-of-
</tableCaption>
<bodyText confidence="0.78809875">
speech tagging. The underlined characters in
the examples correspond to the tags on the
left-most column. The CTB-style word-level
POS are also shown for the examples.
</bodyText>
<sectionHeader confidence="0.921084" genericHeader="method">
2 Character-level POS Tagset
</sectionHeader>
<bodyText confidence="0.999967269230769">
We propose a tagset for the task of character-
level POS tagging. This tagset contains 16 tags,
as illustrated in Table 2. The tagset is designed
by treating each Chinese character as a single-
character word, and each (multi-character) word
as a phrase of single-character words. Some of
these tags are directly derived from the common-
ly accepted word-level part-of-speech, such as
noun, verb, adjective and adverb. It should be
noted that, for single-character words, the differ-
ence between adjective and adverb can almost be
ignored, because for any of such words that can
be used as an adjective, it usually can also be
used as an adverb. Therefore, we have merged
these two tags into one.
On the other hand, some other tags are de-
signed specifically for characters, such as trans-
literation, surname, prefix and suffix. Unlike
some Asian languages such as Japanese, there is
no explicit character set in Chinese that are used
exclusively for expressing names of foreign per-
sons, places or organizations. However, some
characters are used much more frequently than
others in these situations. For example, in the
person’s name “阿尔帕德” (Árpád), all the four
characters can be frequently observed in words
</bodyText>
<page confidence="0.996853">
254
</page>
<figureCaption confidence="0.9964845">
Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by blue
bold lines.
</figureCaption>
<table confidence="0.9975195">
Word Length 1 2 3 4 5 6 7 or more
Tags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...ME
</table>
<tableCaption confidence="0.99739">
Table 3. Word representation with a 6-tag tagset: S, B, B2, B3, M, E
</tableCaption>
<bodyText confidence="0.999684736842105">
of transliterations. Similarly, surnames in Chi-
nese are also drawn from a set of limited number
of characters. We therefore assign specific tags
for this kind of character sets. The tags for pre-
fixes and suffixes are motivated by the previous
studies (Li, 2011; Li and Zhou, 2012).
We have annotated character-level POS for all
words in CTB5 1 . Fortunately, character-level
POS in most words are independent of context,
which means it is sufficient to annotate word
forms unless there is an ambiguity. The annota-
tion was conducted by two persons, where each
one of them was responsible for about 70% of
the documents in the corpus. The redundancy
was set for the purposes of style unification and
quality control, on which we find that the inter-
annotator agreement is 96.2%. Although the an-
notation also includes the test set, we blind this
portion in all the experiments.
</bodyText>
<footnote confidence="0.936355">
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN
</footnote>
<sectionHeader confidence="0.998373" genericHeader="method">
3 Chinese Morphological Analysis with
Character-level POS
</sectionHeader>
<subsectionHeader confidence="0.999188">
3.1 System Description
</subsectionHeader>
<bodyText confidence="0.999468363636364">
Previous studies have shown that jointly pro-
cessing word segmentation and POS tagging is
preferable to pipeline processing, which can
propagate errors (Nakagawa and Uchimoto, 2007;
Kruengkrai et al., 2009). Based on these studies,
we propose a word-character hybrid model
which can also utilize the character-level POS
information. This hybrid model constructs a lat-
tice that consists of word-level and character-
level nodes from a given input sentence. Word-
level nodes correspond to words found in the
system’s lexicon, which has been compiled from
training data. Character-level nodes have special
tags called position-of-character (POC) that indi-
cate the word-internal position (Asahara, 2003;
Nakagawa, 2004). We have adopted the 6-tag
tagset, which (Zhao et al., 2006) reported to be
optimal. This tagset is illustrated in Table 3.
Figure 2 shows an example of a lattice for the
Chinese sentence: “陈德铭答记者问” (Chen
Deming answers to journalists’ questions). The
correct path is marked with blue bold lines. The
</bodyText>
<page confidence="0.984978">
255
</page>
<table confidence="0.9997181">
Category Template Condition
Baseline-unigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉
〈 〉
〈 〉 〈 〉 〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉 〈 〉 〈 〉
Baseline-bigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉 〈 〉
〈 〉 〈 〉 〈 〉
〈 〉 Otherwise
Proposed-unigram 〈 〉
Proposed-bigram 〈 〉 〈 〉
〈 〉 〈 〉
〈 〉 〈 〉
〈 〉 〈 〉
〈 〉 〈 〉
〈 〉 〈 〉
〈 〉 〈 〉 〈 〉
</table>
<tableCaption confidence="0.999256">
Table 4. Feature templates. The “Condition” column describes when to apply the templates:
</tableCaption>
<bodyText confidence="0.999511533333333">
and denote the previous and the current word-level node; and denote the previous and
the current character-level node; and denote the previous and the current node of any
types. Word-level nodes represent known words that can be found in the system’s lexicon.
upper part of the lattice (word-level nodes) rep-
resents known words, where each node carries
information such as character form, character-
level POS , and word-level POS. A word that
contains multiple characters is represented by a
sub-lattice (the dashed rectangle in the figure),
where a path stands for a possible sequence of
character-level POS for this word. For example,
the word “记者” (journalist) has two possible
paths of character-level POS: “verb + suffix” and
“noun + suffix”. Nodes that are inside a sub-
lattice cannot be linked to nodes that are outside,
except from the boundaries. The lower part of
the lattice (character-level nodes) represents un-
known words, where each node carries a posi-
tion-of-character tag, in addition to other types of
information that can also be found on a word-
level node. A sequence of character-level nodes
are considered as an unknown word if and only if
the sequence of POC tags forms one of the cases
listed in Table 3. This table also illustrates the
permitted transitions between adjacent character-
level nodes. We use the standard dynamic pro-
gramming technique to search for the best path in
the lattice. We use the averaged perceptron (Col-
lins, 2002), an efficient online learning algorithm,
to train the model.
</bodyText>
<subsectionHeader confidence="0.958392">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.981653727272727">
We show the feature templates of our model in
Table 4. The features consist of two categories:
baseline features, which are modified from the
templates proposed in (Kruengkrai et al., 2009);
and proposed features, which encode character-
level POS information.
Baseline features: For word-level nodes that
represent known words, we use the symbols ,
and to denote the word form, POS tag and
length of the word, respectively. The functions
and return the first and last
character of . If has only one character, we
omit the templates that contain or
. We use the subscript indices 0 and -1 to
indicate the current node and the previous node
during a Viterbi search, respectively. For charac-
ter-level nodes, denotes the surface character,
and denotes the combination of POS and POC
(position-of-character) tags.
Proposed features: For word-level nodes, the
function returns the pair of the char-
acter-level POS tags of the first and last charac-
ters of , and returns the sequence of
character-level POS tags of . If either the pair
or the sequence of character-level POS is ambig-
uous, which means there are multiple paths in the
sub-lattice of the word-level node, then the val-
ues on the current best path (with local context)
during the Viterbi search will be returned. If
has only one character, we omit the templates
that contain . For character-level nodes,
the function returns its character-level
POS. The subscript indices 0 and -1 as well as
</bodyText>
<page confidence="0.994823">
256
</page>
<bodyText confidence="0.989475">
other symbols stand for the same meaning as
they are in the baseline features.
</bodyText>
<sectionHeader confidence="0.998591" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999396">
4.1 Settings
</subsectionHeader>
<bodyText confidence="0.999980366666667">
To evaluate our proposed method, we have con-
ducted two sets of experiments on CTB5: word
segmentation, and joint word segmentation and
word-level POS tagging. We have adopted the
same data division as in (Jiang et al., 2008a;
Jiang et al., 2008b; Kruengkrai et al., 2009;
Zhang and Clark, 2010; Sun, 2011): the training
set, dev set and test set have 18,089, 350 and 348
sentences, respectively. The models applied on
all test sets are those that result in the best per-
formance on the CTB5 dev set.
We have annotated character-level POS in-
formation for all 508,768 word tokens in CTB5.
As mentioned in section 2, we blind the annota-
tion in the test set in all the experiments. To learn
the characteristics of unknown words, we built
the system’s lexicon using only the words in the
training data that appear at least 3 times. We ap-
plied a similar strategy in building the lexicon for
character-level POS, where the threshold we
choose is 2. These thresholds were tuned using
the development data.
We have used precision, recall and the F-score
to measure the performance of the systems. Pre-
cision (P) is defined as the percentage of output
tokens that are consistent with the gold standard
test data, and recall (R) is the percentage of to-
kens in the gold standard test data that are recog-
nized in the output. The balanced F-score ( F) is
defined as ? P R.
</bodyText>
<subsectionHeader confidence="0.995275">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999854375">
We compare the performance between a baseline
model and our proposed approach. The results of
the word segmentation experiment and the joint
experiment of segmentation and POS tagging are
shown in Table 5(a) and Table 5(b), respectively.
Each row in these tables shows the performance
of the corresponding system. “CharPos” stands
for our proposed model which has been de-
scribed in section 3. “Baseline” stands for the
same model except it only enables features from
the baseline templates.
The results show that, while the differences
between the baseline model and the proposed
model in word segmentation accuracies are small,
the proposed model achieves significant im-
provement in the experiment of joint segmentati-
</bodyText>
<table confidence="0.94325875">
(a) Word Segmentation Results
System P R F
Baseline 97.48 98.44 97.96
CharPOS 97.55 98.51 98.03
(b) Joint Segmentation and POS Tagging Results
System P R F
Baseline 93.01 93.95 93.48
CharPOS 93.42 94.18 93.80
</table>
<tableCaption confidence="0.974487">
Table 5. Experimental results on CTB5.
</tableCaption>
<table confidence="0.999707625">
System Segmentation Joint
Baseline 97.96 93.48
CharPOS 98.03 93.80
Jiang2008a 97.85 93.41
Jiang2008b 97.74 93.37
Kruengkrai2009 97.87 93.67
Zhang2010 97.78 93.67
Sun2011 98.17 94.02
</table>
<tableCaption confidence="0.938271">
Table 6. Comparison with previous studies on
CTB5.
</tableCaption>
<bodyText confidence="0.999540692307692">
on and POS tagging2. This suggests that our pro-
posed method is particularly effective in predict-
ing the word-level POS, which is consistent with
our observations mentioned in section 1.
In Table 6 we compare our approach with
morphological analyzers in previous studies. The
accuracies of the systems in previous work are
directly taken from the original paper. As the
results show, despite the fact that the perfor-
mance of our baseline model is relatively weak
in the joint segmentation and POS tagging task,
our proposed model achieves the second-best
performance in both segmentation and joint tasks.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.949288">
We believe that by treating characters as the true
atoms of Chinese morphological and syntactic
analysis, it is possible to address the out-of-
vocabulary problem that word-based methods
have been long suffered from. In our error analy-
sis, we believe that by exploring the character-
level POS and the internal word structure (Zhang
et al., 2013) at the same time, it is possible to
further improve the performance of morphologi-
cal analysis and parsing. We will address these
issues in our future work.
2 p &lt; 0.05 in McNemar’s test.
</bodyText>
<page confidence="0.992222">
257
</page>
<sectionHeader confidence="0.855378" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998202705882353">
Masayuki Asahara. 2003. Corpus-based Japanese
Morphological Analysis. Nara Institute of Science
and Technology, Doctor’s Thesis.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of EMNLP, pages 1–8.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lü.
2008a. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-speech Tagging.
In Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
Lattice Reranking for Chinese Word Segmentation
and Part-of-speech Tagging. In Proceedings of COL-
ING.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, YiouWang, Kentaro Torisawa, and Hi-
toshi Isahara. 2009. An Error-Driven Word-
Character Hybird Model for Joint Chinese Word
Segmentation and POS Tagging. In Proceedings of
ACL-IJCNLP, pages 513-521.
Zhongguo Li. 2011. Parsing the Internal Structure of
Words: A New Paradigm for Chinese Word Seg-
mentation. In Proceedings of ACL-HLT, pages
1405–1414.
Zhongguo Li and Guodong Zhou. 2012. Unified De-
pendency Parsing of Chinese Morphological and
Syntactic Structures. In Proceedings of EMNLP,
pages 1445–1454.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-speech Tagging: One-at-a-time or All-at-once?
Word-based or Character-based? In Proceedings of
EMNLP, pages 277–284.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466–472.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007.
Hybrid Approach to Word Segmentation and Pos
Tagging. In Proceedings of ACL Demo and Poster
Sessions, pages 217-220.
Weiwei Sun. 2010. Word-based and Character-based
Word Segmentation Models: Comparison and
Combination. In Proceedings of COLING Poster
Sessions, pages 1211–1219.
Weiwei Sun. 2011. A Stacked Sub-word Model for
Joint Chinese Word Segmentation and Part-of-
speech Tagging. In Proceedings of ACL-HLT,
pages 1385–1394.
Nianwen Xue. 2003. Chinese Word Segmentation as
Character Tagging. In International Journal of
Computational Linguistics and Chinese Language
Processing.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
Structure Annotation of a Large Corpus. Natural
Language Engineering, 11(2):207–238.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective Tag Set Selection in Chinese
Word Segmentation via Conditional Random Field
Modeling. In Proceedings of PACLIC, pages 87-94.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese Parsing Exploiting Characters.
In Proceedings of ACL, page 125-134.
Yue Zhang and Stephen Clark. 2010. A Fast Decoder
for Joint Word Segmentation and POS-tagging Us-
ing a Single Discriminative Model. In Proceedings
of EMNLP, pages 843–852.
</reference>
<page confidence="0.996133">
258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280248">
<title confidence="0.957259">Chinese Morphological Analysis with Character-level POS Tagging</title>
<affiliation confidence="0.859614">Hongxiao Daisuke School of Informatics, Kyoto University, of Computer Science, Fudan University,</affiliation>
<email confidence="0.86924">shen@nlp.ist.i.kyoto-u.ac.jp</email>
<note confidence="0.464315">12210240027@fudan.edu.cn</note>
<abstract confidence="0.999544222222222">The focus of recent studies on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words to characters. However, existing methods have not yet fully utilized the potentials of Chinese characters. In this paper, we investigate the usefulness of character-level part-of-speech in the task of Chinese morphological analysis. We propose the first tagset designed for the task of character-level POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
</authors>
<title>Corpus-based Japanese Morphological Analysis.</title>
<date>2003</date>
<institution>Nara Institute of Science and Technology, Doctor’s Thesis.</institution>
<contexts>
<context position="9808" citStr="Asahara, 2003" startWordPosition="1529" endWordPosition="1530">ging is preferable to pipeline processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). Based on these studies, we propose a word-character hybrid model which can also utilize the character-level POS information. This hybrid model constructs a lattice that consists of word-level and characterlevel nodes from a given input sentence. Wordlevel nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Character-level nodes have special tags called position-of-character (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 3. Figure 2 shows an example of a lattice for the Chinese sentence: “陈德铭答记者问” (Chen Deming answers to journalists’ questions). The correct path is marked with blue bold lines. The 255 Category Template Condition Baseline-unigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Baseline-bigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Otherwise Proposed-unigram 〈 〉 Proposed-bigr</context>
</contexts>
<marker>Asahara, 2003</marker>
<rawString>Masayuki Asahara. 2003. Corpus-based Japanese Morphological Analysis. Nara Institute of Science and Technology, Doctor’s Thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12019" citStr="Collins, 2002" startWordPosition="1963" endWordPosition="1965">undaries. The lower part of the lattice (character-level nodes) represents unknown words, where each node carries a position-of-character tag, in addition to other types of information that can also be found on a wordlevel node. A sequence of character-level nodes are considered as an unknown word if and only if the sequence of POC tags forms one of the cases listed in Table 3. This table also illustrates the permitted transitions between adjacent characterlevel nodes. We use the standard dynamic programming technique to search for the best path in the lattice. We use the averaged perceptron (Collins, 2002), an efficient online learning algorithm, to train the model. 3.2 Features We show the feature templates of our model in Table 4. The features consist of two categories: baseline features, which are modified from the templates proposed in (Kruengkrai et al., 2009); and proposed features, which encode characterlevel POS information. Baseline features: For word-level nodes that represent known words, we use the symbols , and to denote the word form, POS tag and length of the word, respectively. The functions and return the first and last character of . If has only one character, we omit the temp</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lü</author>
</authors>
<title>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13870" citStr="Jiang et al., 2008" startWordPosition="2272" endWordPosition="2275"> then the values on the current best path (with local context) during the Viterbi search will be returned. If has only one character, we omit the templates that contain . For character-level nodes, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 other symbols stand for the same meaning as they are in the baseline features. 4 Evaluation 4.1 Settings To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time</context>
</contexts>
<marker>Jiang, Huang, Liu, Lü, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lü. 2008a. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-speech Tagging. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word Lattice Reranking for Chinese Word Segmentation and Part-of-speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="13870" citStr="Jiang et al., 2008" startWordPosition="2272" endWordPosition="2275"> then the values on the current best path (with local context) during the Viterbi search will be returned. If has only one character, we omit the templates that contain . For character-level nodes, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 other symbols stand for the same meaning as they are in the baseline features. 4 Evaluation 4.1 Settings To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 time</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word Lattice Reranking for Chinese Word Segmentation and Part-of-speech Tagging. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa YiouWang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An Error-Driven WordCharacter Hybird Model for Joint Chinese Word Segmentation and POS Tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>513--521</pages>
<contexts>
<context position="1428" citStr="Kruengkrai et al., 2009" startWordPosition="188" endWordPosition="191">haracter-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb</context>
<context position="9319" citStr="Kruengkrai et al., 2009" startWordPosition="1455" endWordPosition="1458">bout 70% of the documents in the corpus. The redundancy was set for the purposes of style unification and quality control, on which we find that the interannotator agreement is 96.2%. Although the annotation also includes the test set, we blind this portion in all the experiments. 1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN 3 Chinese Morphological Analysis with Character-level POS 3.1 System Description Previous studies have shown that jointly processing word segmentation and POS tagging is preferable to pipeline processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). Based on these studies, we propose a word-character hybrid model which can also utilize the character-level POS information. This hybrid model constructs a lattice that consists of word-level and characterlevel nodes from a given input sentence. Wordlevel nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Character-level nodes have special tags called position-of-character (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tag</context>
<context position="12283" citStr="Kruengkrai et al., 2009" startWordPosition="2004" endWordPosition="2007">evel nodes are considered as an unknown word if and only if the sequence of POC tags forms one of the cases listed in Table 3. This table also illustrates the permitted transitions between adjacent characterlevel nodes. We use the standard dynamic programming technique to search for the best path in the lattice. We use the averaged perceptron (Collins, 2002), an efficient online learning algorithm, to train the model. 3.2 Features We show the feature templates of our model in Table 4. The features consist of two categories: baseline features, which are modified from the templates proposed in (Kruengkrai et al., 2009); and proposed features, which encode characterlevel POS information. Baseline features: For word-level nodes that represent known words, we use the symbols , and to denote the word form, POS tag and length of the word, respectively. The functions and return the first and last character of . If has only one character, we omit the templates that contain or . We use the subscript indices 0 and -1 to indicate the current node and the previous node during a Viterbi search, respectively. For character-level nodes, denotes the surface character, and denotes the combination of POS and POC (position-o</context>
<context position="13917" citStr="Kruengkrai et al., 2009" startWordPosition="2280" endWordPosition="2283">(with local context) during the Viterbi search will be returned. If has only one character, we omit the templates that contain . For character-level nodes, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 other symbols stand for the same meaning as they are in the baseline features. 4 Evaluation 4.1 Settings To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building th</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, YiouWang, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, YiouWang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An Error-Driven WordCharacter Hybird Model for Joint Chinese Word Segmentation and POS Tagging. In Proceedings of ACL-IJCNLP, pages 513-521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
</authors>
<title>Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>1405--1414</pages>
<contexts>
<context position="1677" citStr="Li, 2011" startWordPosition="223" endWordPosition="224">roduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb Part-of-Speech verb + noun Mf (invest : throw + wealth) noun + verb bA_ (feel sorry : heart + hurt) verb + adjective i`� (realize : recognize + clear) adjective + verb �N, (hate : pain + hate) verb + verb *1 (inspect : examine + review) Table 1. Ch</context>
<context position="8363" citStr="Li, 2011" startWordPosition="1310" endWordPosition="1311"> “阿尔帕德” (Árpád), all the four characters can be frequently observed in words 254 Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by blue bold lines. Word Length 1 2 3 4 5 6 7 or more Tags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...ME Table 3. Word representation with a 6-tag tagset: S, B, B2, B3, M, E of transliterations. Similarly, surnames in Chinese are also drawn from a set of limited number of characters. We therefore assign specific tags for this kind of character sets. The tags for prefixes and suffixes are motivated by the previous studies (Li, 2011; Li and Zhou, 2012). We have annotated character-level POS for all words in CTB5 1 . Fortunately, character-level POS in most words are independent of context, which means it is sufficient to annotate word forms unless there is an ambiguity. The annotation was conducted by two persons, where each one of them was responsible for about 70% of the documents in the corpus. The redundancy was set for the purposes of style unification and quality control, on which we find that the interannotator agreement is 96.2%. Although the annotation also includes the test set, we blind this portion in all the</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Zhongguo Li. 2011. Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation. In Proceedings of ACL-HLT, pages 1405–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Guodong Zhou</author>
</authors>
<title>Unified Dependency Parsing of Chinese Morphological and Syntactic Structures.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1445--1454</pages>
<contexts>
<context position="1696" citStr="Li and Zhou, 2012" startWordPosition="225" endWordPosition="228">In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb Part-of-Speech verb + noun Mf (invest : throw + wealth) noun + verb bA_ (feel sorry : heart + hurt) verb + adjective i`� (realize : recognize + clear) adjective + verb �N, (hate : pain + hate) verb + verb *1 (inspect : examine + review) Table 1. Character-level POS s</context>
<context position="8383" citStr="Li and Zhou, 2012" startWordPosition="1312" endWordPosition="1315">rpád), all the four characters can be frequently observed in words 254 Figure 1. A Word-character hybrid lattice of a Chinese sentence. Correct path is represented by blue bold lines. Word Length 1 2 3 4 5 6 7 or more Tags S BE BB2E BB2B3E BB2B3ME BB2B3MME BB2B3M...ME Table 3. Word representation with a 6-tag tagset: S, B, B2, B3, M, E of transliterations. Similarly, surnames in Chinese are also drawn from a set of limited number of characters. We therefore assign specific tags for this kind of character sets. The tags for prefixes and suffixes are motivated by the previous studies (Li, 2011; Li and Zhou, 2012). We have annotated character-level POS for all words in CTB5 1 . Fortunately, character-level POS in most words are independent of context, which means it is sufficient to annotate word forms unless there is an ambiguity. The annotation was conducted by two persons, where each one of them was responsible for about 70% of the documents in the corpus. The redundancy was set for the purposes of style unification and quality control, on which we find that the interannotator agreement is 96.2%. Although the annotation also includes the test set, we blind this portion in all the experiments. 1 http</context>
</contexts>
<marker>Li, Zhou, 2012</marker>
<rawString>Zhongguo Li and Guodong Zhou. 2012. Unified Dependency Parsing of Chinese Morphological and Syntactic Structures. In Proceedings of EMNLP, pages 1445–1454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese Partof-speech Tagging: One-at-a-time or All-at-once? Word-based or Character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>277--284</pages>
<contexts>
<context position="1368" citStr="Ng and Low, 2004" startWordPosition="178" endWordPosition="181">evel POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) point</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Partof-speech Tagging: One-at-a-time or All-at-once? Word-based or Character-based? In Proceedings of EMNLP, pages 277–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Chinese and japanese word segmentation using word-level and character-level information.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>466--472</pages>
<contexts>
<context position="1384" citStr="Nakagawa, 2004" startWordPosition="182" endWordPosition="183">We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some</context>
<context position="9825" citStr="Nakagawa, 2004" startWordPosition="1531" endWordPosition="1532">ble to pipeline processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). Based on these studies, we propose a word-character hybrid model which can also utilize the character-level POS information. This hybrid model constructs a lattice that consists of word-level and characterlevel nodes from a given input sentence. Wordlevel nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Character-level nodes have special tags called position-of-character (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 3. Figure 2 shows an example of a lattice for the Chinese sentence: “陈德铭答记者问” (Chen Deming answers to journalists’ questions). The correct path is marked with blue bold lines. The 255 Category Template Condition Baseline-unigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Baseline-bigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Otherwise Proposed-unigram 〈 〉 Proposed-bigram 〈 〉 〈 〉 〈 〉 〈 </context>
</contexts>
<marker>Nakagawa, 2004</marker>
<rawString>Tetsuji Nakagawa. 2004. Chinese and japanese word segmentation using word-level and character-level information. In Proceedings of COLING, pages 466–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Hybrid Approach to Word Segmentation and Pos Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL Demo and Poster Sessions,</booktitle>
<pages>217--220</pages>
<contexts>
<context position="9293" citStr="Nakagawa and Uchimoto, 2007" startWordPosition="1451" endWordPosition="1454">of them was responsible for about 70% of the documents in the corpus. The redundancy was set for the purposes of style unification and quality control, on which we find that the interannotator agreement is 96.2%. Although the annotation also includes the test set, we blind this portion in all the experiments. 1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?CharPosCN 3 Chinese Morphological Analysis with Character-level POS 3.1 System Description Previous studies have shown that jointly processing word segmentation and POS tagging is preferable to pipeline processing, which can propagate errors (Nakagawa and Uchimoto, 2007; Kruengkrai et al., 2009). Based on these studies, we propose a word-character hybrid model which can also utilize the character-level POS information. This hybrid model constructs a lattice that consists of word-level and characterlevel nodes from a given input sentence. Wordlevel nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Character-level nodes have special tags called position-of-character (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) report</context>
</contexts>
<marker>Nakagawa, Uchimoto, 2007</marker>
<rawString>Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. Hybrid Approach to Word Segmentation and Pos Tagging. In Proceedings of ACL Demo and Poster Sessions, pages 217-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and Character-based Word Segmentation Models: Comparison and Combination.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING Poster Sessions,</booktitle>
<pages>1211--1219</pages>
<contexts>
<context position="1451" citStr="Sun, 2010" startWordPosition="194" endWordPosition="195">ith word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb Part-of-Speech verb + </context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and Character-based Word Segmentation Models: Comparison and Combination. In Proceedings of COLING Poster Sessions, pages 1211–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A Stacked Sub-word Model for Joint Chinese Word Segmentation and Part-ofspeech Tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>1385--1394</pages>
<contexts>
<context position="13952" citStr="Sun, 2011" startWordPosition="2288" endWordPosition="2289">ill be returned. If has only one character, we omit the templates that contain . For character-level nodes, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 other symbols stand for the same meaning as they are in the baseline features. 4 Evaluation 4.1 Settings To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character-level POS, </context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A Stacked Sub-word Model for Joint Chinese Word Segmentation and Part-ofspeech Tagging. In Proceedings of ACL-HLT, pages 1385–1394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>In International Journal of Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="1439" citStr="Xue, 2003" startWordPosition="192" endWordPosition="193">g jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb Part-of-Sp</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese Word Segmentation as Character Tagging. In International Journal of Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2581" citStr="Xue et al., 2005" startWordPosition="377" endWordPosition="380">ointed out that some characters Character-level Examples of Verb Part-of-Speech verb + noun Mf (invest : throw + wealth) noun + verb bA_ (feel sorry : heart + hurt) verb + adjective i`� (realize : recognize + clear) adjective + verb �N, (hate : pain + hate) verb + verb *1 (inspect : examine + review) Table 1. Character-level POS sequence as a more specified version of word-level POS: an example of verb. can productively form new words by attaching to existing words, these characters consist only a portion of all Chinese characters and appear in 35% of the words in Chinese Treebank 5.0 (CTB5) (Xue et al., 2005). Zhang (2013) took one step further by investigating the characterlevel structures of words; however, the machine learning of inferring these internal structures relies on the character forms, which still suffers from data sparseness. In our view, since each Chinese character is in fact created as a word in origin with complete and independent meaning, it should be treated as the actual minimal morphological unit in Chinese language, and therefore should carry specific part-of-speech. For example, the character “TT” (beat) is a verb and the character “TA” (broken) is an adjective. A word on t</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese Treebank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<pages>87--94</pages>
<contexts>
<context position="1403" citStr="Zhao et al., 2006" startWordPosition="184" endWordPosition="187">hod that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved. 1 Introduction In recent years, the focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Charact</context>
<context position="9886" citStr="Zhao et al., 2006" startWordPosition="1540" endWordPosition="1543">kagawa and Uchimoto, 2007; Kruengkrai et al., 2009). Based on these studies, we propose a word-character hybrid model which can also utilize the character-level POS information. This hybrid model constructs a lattice that consists of word-level and characterlevel nodes from a given input sentence. Wordlevel nodes correspond to words found in the system’s lexicon, which has been compiled from training data. Character-level nodes have special tags called position-of-character (POC) that indicate the word-internal position (Asahara, 2003; Nakagawa, 2004). We have adopted the 6-tag tagset, which (Zhao et al., 2006) reported to be optimal. This tagset is illustrated in Table 3. Figure 2 shows an example of a lattice for the Chinese sentence: “陈德铭答记者问” (Chen Deming answers to journalists’ questions). The correct path is marked with blue bold lines. The 255 Category Template Condition Baseline-unigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Baseline-bigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Otherwise Proposed-unigram 〈 〉 Proposed-bigram 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 〈 〉 Table 4. Featur</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective Tag Set Selection in Chinese Word Segmentation via Conditional Random Field Modeling. In Proceedings of PACLIC, pages 87-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Chinese Parsing Exploiting Characters.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>125--134</pages>
<contexts>
<context position="1717" citStr="Zhang et al., 2013" startWordPosition="229" endWordPosition="232">e focus of research on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words toward characters. Character-based methods have shown superior performance in these tasks compared to traditional word-based methods (Ng and Low, 2004; Nakagawa, 2004; Zhao et al., 2006; Kruengkrai et al., 2009; Xue, 2003; Sun, 2010). Studies investigating the morphologicallevel and character-level internal structures of words, which treat character as the true atom of morphological and syntactic processing, have demonstrated encouraging results (Li, 2011; Li and Zhou, 2012; Zhang et al., 2013). This line of research has provided great insight in revealing the roles of characters in word formation and syntax of Chinese language. However, existing methods have not yet fully utilized the potentials of Chinese characters. While Li (2011) pointed out that some characters Character-level Examples of Verb Part-of-Speech verb + noun Mf (invest : throw + wealth) noun + verb bA_ (feel sorry : heart + hurt) verb + adjective i`� (realize : recognize + clear) adjective + verb �N, (hate : pain + hate) verb + verb *1 (inspect : examine + review) Table 1. Character-level POS sequence as a more spe</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese Parsing Exploiting Characters. In Proceedings of ACL, page 125-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A Fast Decoder for Joint Word Segmentation and POS-tagging Using a Single Discriminative Model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>843--852</pages>
<contexts>
<context position="13940" citStr="Zhang and Clark, 2010" startWordPosition="2284" endWordPosition="2287">ng the Viterbi search will be returned. If has only one character, we omit the templates that contain . For character-level nodes, the function returns its character-level POS. The subscript indices 0 and -1 as well as 256 other symbols stand for the same meaning as they are in the baseline features. 4 Evaluation 4.1 Settings To evaluate our proposed method, we have conducted two sets of experiments on CTB5: word segmentation, and joint word segmentation and word-level POS tagging. We have adopted the same data division as in (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011): the training set, dev set and test set have 18,089, 350 and 348 sentences, respectively. The models applied on all test sets are those that result in the best performance on the CTB5 dev set. We have annotated character-level POS information for all 508,768 word tokens in CTB5. As mentioned in section 2, we blind the annotation in the test set in all the experiments. To learn the characteristics of unknown words, we built the system’s lexicon using only the words in the training data that appear at least 3 times. We applied a similar strategy in building the lexicon for character</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A Fast Decoder for Joint Word Segmentation and POS-tagging Using a Single Discriminative Model. In Proceedings of EMNLP, pages 843–852.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>