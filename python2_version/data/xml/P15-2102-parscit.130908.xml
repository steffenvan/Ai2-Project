<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002330">
<title confidence="0.9992455">
A Hierarchical Knowledge Representation for Expert Finding
on Social Media
</title>
<author confidence="0.983563">
Yanran Li&apos;, Wenjie Li&apos;, and Sujian Li2
&apos;Computing Department, Hong Kong Polytechnic University, Hong Kong
</author>
<affiliation confidence="0.960522">
2Key Laboratory of Computational Linguistics, Peking University, MOE, China
</affiliation>
<email confidence="0.9649535">
{csyli, cswjli}@comp.polyu.edu.hk
lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999461153846154">
Expert finding on social media benefits
both individuals and commercial services.
In this paper, we exploit a 5-level tree rep-
resentation to model the posts on social
media and cast the expert finding prob-
lem to the matching problem between the
learned user tree and domain tree. We
enhance the traditional approximate tree
matching algorithm and incorporate word
embeddings to improve the matching re-
sult. The experiments conducted on Sina
Microblog demonstrate the effectiveness
of our work.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962560606061">
Expert finding has been arousing great interests
among social media researchers after its success-
ful applications on traditional media like academic
publications. As already observed, social media
users tend to follow others for professional inter-
ests and knowledge (Ramage et al, 2010). This
builds the basis for mining expertise and find-
ing experts on social media, which facilitates the
services of user recommendation and question-
answering, etc.
Despite the demand to access expertise, the
challenges of identifying domain experts on social
media exist. Social media often contains plenty of
noises such as the tags with which users describe
themselves. Noises impose the inherent drawback
on the feature-based learning methods (Krishna-
murthy et al, 2008). Data imbalance and sparse-
ness also limits the performance of the promis-
ing latent semantic analysis methods such as the
LDA-like topic models (Blei et al, 2003; Ram-
age et al, 2009). When some topics co-occur
more frequently than others, the strict assump-
tion of these topic models cannot be met and con-
sequently many nonsensical topics will be gen-
erated (Zhao and Jiang, 2011; Pal et al, 2011;
Quercia et al, 2012). Furthermore, not as simple
as celebrities, the definition of experts introduces
additional difficulties. Experts cannot be simply
judged by the number of followers. The knowl-
edge conveyed in what they say is essential. This
leads to the failures of the network-based meth-
ods (Java et al, 2007; Weng et al, 2010; Pal et al,
2011).
The challenges mentioned above inherently
come from insufficient representations. They mo-
tivate us to propose a more flexible domain expert
finding framework to explore effective representa-
tions that are able to tackle the complexity lies in
the social media data. The basic idea is as follows.
Experts talk about the professional knowledge in
their posts and these posts are supposed to contain
more domain knowledge than the posts from the
other ordinary users. We determine whether or not
users are experts on specific domains by matching
their professional knowledge and domain knowl-
edge. The key is how to capture such information
for both users and domains with the appropriate
representation, which is, in our view, the reason
why most of previous work fails.
To go beyond the feature-based classification
methods and the vector representation inference in
expert finding, a potential solution is to incorpo-
rate the semantic information for knowledge mod-
eling. We achieve this goal by representing user
posts using a hierarchical tree structure to capture
correlations among words and topics. To tackle
the data sparseness problem, we apply word em-
beddings to tree-nodes to further enhance seman-
tic representation and to support semantic match-
ing. Expert finding is then cast to the problem of
determining the edit distance between the user tree
and the domain tree, which is computed with an
approximate tree matching algorithm.
The main contribution of this work is to inte-
grate the hierarchical tree representation and struc-
ture matching together to profile users’ and do-
</bodyText>
<page confidence="0.928105">
616
</page>
<bodyText confidence="0.869687363636364">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 616–622,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
=
mains’ knowledge. Using such trees allows us to
flexibly incorporate more information into the data
representation, such as the relations between la-
tent topics and the semantic similarities between
words. The experiments conducted on Sina Mi-
croblog demonstrate the effectiveness of the pro-
posed framework and the corresponding methods.
</bodyText>
<sectionHeader confidence="0.9045465" genericHeader="method">
2 Knowledge Representation
with Hierarchical Tree
</sectionHeader>
<bodyText confidence="0.99796135">
To capture correlations between topics, Pachinko
Allocation Model (PAM) (Li and McCallum,
2006) uses a directed acyclic graph (DAG) with
leaves representing individual words in the vocab-
ulary and each interior node representing a corre-
lation among its children. In particular, multi-level
PAM is capable of revealing interconnection be-
tween sub-level nodes by inferencing correspond-
ing super-level nodes. It is a desired property that
enables us to capture hierarchical relations among
both inner-level and inter-level nodes and thereby
enhance the representation of users’ posts. More
important, the inter-level hierarchy benefits to dis-
tribute words from super-level generic topics to
sub-level specific topics.
In this work, we exploit a 5-level PAM to learn
the hierarchical knowledge representation for each
individual user and domain. As shown in Figure 1,
the 5-level hierarchy consists of one root topic r, I
topics at the second level X = {x1, x2, ... , xI},
</bodyText>
<equation confidence="0.77732">
J topics at the third level Y = {y1, y2, ... , yJ},
K topics at the fourth level Z = {z1, z2, ... , zK}
</equation>
<bodyText confidence="0.901123">
and words at the bottom. The whole hierarchy is
fully connected.
</bodyText>
<figureCaption confidence="0.998979">
Figure 1: 5-level PAM
</figureCaption>
<bodyText confidence="0.998598857142857">
Each topic in 5-level PAM is associated with
a distribution g(·) over its children. In general,
g(·) can be any distribution over discrete vari-
ables. Here, we use a set of Dirichlet com-
pound multinomial distributions associated with
the root, the second-level and the third-level top-
ics. These distributions are {gr(α)}, {gi(γi)}Ii=1
and {gi(δj)}Jj=1. They are used to sample the
multinomial distributions θx, θy and θz over the
corresponding sub-level topics. As to the fourth-
level topics, we use a fixed multinomial distribu-
tion {ϕzk}Kk=1 sampled once for the whole data
from a single Dirichlet distribution g(β). Figure 2
illustrates the plate notation of this 5-level PAM.
</bodyText>
<figureCaption confidence="0.992184">
Figure 2: Plate Notation of 5-level PAM
</figureCaption>
<bodyText confidence="0.99867225">
By integrating out the sampled multinomial dis-
tributions θx, θy, θz, ϕ and summing over x, y, z,
we obtain the Gibbs sampling distribution for
word w = wm in document d as:
</bodyText>
<equation confidence="0.999221181818182">
P (xw=xi, yw=yj, zw=zk|D, X−w, Y−w, Z−w, α, γ, δ, β)
∝P (w, xw, yw, zw|D−w, X−w, Y−w, Z−w, α, γ, δ, β)
P(D, X, Y, Z|α, γ, δ, β)
P(D−w, X−w, Y−w, Z−w|α, γ, δ, β)
ni + αi
(d)
= (d)K ×
nr + Ei′=1 αi′
njk + δjk
(d)
× ×
</equation>
<bodyText confidence="0.901139785714286">
n,(d
+ Ek′=1 δjk′
where n(d)
r is the number of occurrences of the
root r in document d, which is equivalent to the
number of tokens in the document. n(d)
i , n(d)
ij and
njk are respectively the number of occurrences of
(d)
xi, yj and zk sampled from their upper-level top-
ics. nk is the number of occurrences of the fourth-
level topics zk in the whole dataset and nkm is the
number of occurrences of word wm in zk. −w
</bodyText>
<figure confidence="0.993300096774193">
N
K
β
ϕ
|V |
θx
α
x y
I
θy
γ
J
θz
w
z
δ
root
x-topic
y-topic
z-topic
. . . word
· · ·
· · ·
. . .
nij + γij
(d)
(d) [� L
ni + L�j,=1 γij′
nkm + βm
(d)
nk +En m′=1 βm′
</figure>
<page confidence="0.97582">
617
</page>
<bodyText confidence="0.998243461538461">
indicates all observations or topic assignments ex-
cept word w.
With the fixed Dirichlet parameter α for the root
and Q as the prior, what’s left is to estimate (learn
from data) -y and 6 to capture the different corre-
lations among topics. To avoid the use of iterative
methods which are often computationally exten-
sive, instead we approximate these two Dirichlet
parameters using the moment matching algorithm,
the same as (Minka, 2000; Casella and Berger,
2001; Shafiei and Milios, 2006). With smoothing
techniques, in each iteration of Gibbs sampling we
update:
</bodyText>
<equation confidence="0.956929307692308">
(d)
nij
× (E(
(d) − meanij)2
d ni
�
1
+ (L − meanij)2
meanij × (1 − meanij) 1
varij
meanij
∑j log(mij)
exp L−1
</equation>
<bodyText confidence="0.999911666666667">
where Ni is the number of documents with non-
zero counts of super-level topic xi. Parameter es-
timation of 6 is the same as &apos;y.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="method">
3 Expert Finding
</sectionHeader>
<subsectionHeader confidence="0.577092">
with Approximate Tree Matching
</subsectionHeader>
<bodyText confidence="0.998821377358491">
Once the hierarchical representations of users and
domains have been generated, we can determine
whether or not a user is an expert on a domain
based on their matching degree, which is a prob-
lem analogous to tree-to-tree correction using edit
distance (Selkow, 1977; Shasha and Zhang, 1990;
Wagner, 1975; Wagner and Fischer, 1974; Zhang
and Shasha, 1989). Given two trees T1 and T2,
a typical edit distance-based correction approach
is to transform T1 to T2 with a sequence of edit-
ing operations 5 =&lt; s1, s2, ... , sk &gt; such that
sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation
is assigned a cost u(si) that represents the diffi-
culty of making that operation. By summing up
the costs of all necessary operations, the total cost
Q(5) = ∑ki=1 u(si) defines the matching degree
of T1 and T2.
We assume that an expert could only master a
part of professional domain knowledge rather than
the whole and thereby revise a traditional approxi-
mate tree matching algorithm (Zhang and Shasha,
1989) to calculate the matching degree. This as-
sumption especially makes sense when the domain
we are concerned with is quite general. Let Td and
Tu denote the learned domain knowledge tree and
the user knowledge tree, we match Td to the re-
maining trees resulting from cutting all possible
sets of disjoint sub-trees of Tu. We specifically
penalize no cost if some sub-trees are missing in
matching process. We define two types of oper-
ations. The substitution operations edit the dis-
similar words on tree-nodes, while the insertion
and deletion operations perform on tree-structures.
Expert finding is then to calculate the minimum
matching cost on Td and Tu. If the cost is smaller
than an empirically defined threshold Ad, we iden-
tify user u as an expert on domain d.
To alleviate the sparseness problem caused by
direct letter-to-letter matching in tree-node map-
ping, we embed word embeddings (Bengio et al,
2003) into the substitution operation. We apply
the word2vec skip-gram model (Mikolov et al,
2013(a); Mikolov et al, 2013(b)) to encode each
word in our vocabulary with a probability vec-
tor and directly use the similarity generated by
word2vec as the tree-node similarity. The costs
of insertion and deletion operations will be ex-
plained in Section 4. Actually all these three costs
can be defined in accordance with applicant needs.
In brief, by combining both hierarchical represen-
tation of tree-structure and word embeddings of
tree-nodes, we achieve our goal to enhance seman-
tics.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.984684733333333">
The experiments are conducted on 5 domains (i.e.,
Beauty Blogger, Beauty Doctor, Parenting, E-
Commerce, and Data Science) in Sina Microblog,
a Twitter-like microblog in China. To learn PAM,
we manually select 40 users in each domain
from the official expert lists released by Sina Mi-
croblog1, and crawl all of their posts. In average,
there are 113,924 posts in each domain. Notice
that the expert lists are not of high quality. We
have to do manual verification to filter out noises.
For evaluation, we select another 80 users in each
domain from the expert list, with 40 verified as ex-
perts and the other 40 as non-experts.
Since there is no state-of-art Chinese word em-
beddings publicly available, we use another Sina
</bodyText>
<footnote confidence="0.892225">
1http://d.weibo.com/1087030002-558-3-
2014#
</footnote>
<equation confidence="0.998522545454545">
meanij =
1 C�Ni + 1 × I\ d
(d) 1
n(d) + L /
i
n
1
varij =
Ni + 1
mij =
7ij =
</equation>
<page confidence="0.998711">
618
</page>
<tableCaption confidence="0.999298">
Table 1: Classification Results
</tableCaption>
<table confidence="0.999405142857143">
Approach Precision Recall F-Score
Macro Micro Macro Micro Macro Micro
unigram 0.380 0.484 0.615 0.380 0.469 0.432
bigram 0.435 0.537 0.615 0.435 0.507 0.486
LDA 0.430 0.473 0.540 0.430 0.474 0.451
Twitter-LDA 0.675 0.763 0.680 0.430 0.675 0.451
PAM 0.720 0.818 0.720 0.720 0.714 0.769
</table>
<bodyText confidence="0.985220944444444">
Microblog dataset provided by pennyliang2,
which contains 25 million posts and nearly 100
million tokens in total, to learn the word embed-
dings of 50-dimension. We pre-process the data
with the Rwordseg segmentation package3 and
discard nonsensical words with the pullword
package4.
When learning 5-level PAM, we set fixed pa-
rameters a = 0.25, Q = 0.25 and from top to down,
I = 10, J = 20, K = 20 for the number of second,
third and fourth levels of topics, respectively. And
we initialize -y and 6 with 0.25. For tree match-
ing, we define the cost of tree-node substitution
operation between word a and b as Eq (1). The
costs of insertion and deletion operations for tree-
structure matching are MAX VALUE. Here we set
MAX VALUE as 100 experimentally. The thresh-
old Ad used to determine the expert is set to be 12
times of MAX VALUE.
a(a—*b)= { 0, a = b (1)
sim (a, b) , sim(a, b) &gt;0.55
MAX VALUE, otherwise
We compare PAM with n-gram (unigram and
bigram), LDA (Blei et al, 2003) and Twitter-
LDA (Zhao and Jiang, 2011). We set Q in LDA
and Twitter-LDA to 0.01, -y in Twiitter-LDA to 20.
For a, we adopt the commonly used 50/T heuris-
tics where the number of topics T = 50. To be fair,
we all use the tokens after pullword preprocessing
as the input to extract features for classification.
Following Zhao and Jiang (2011), we train four
E2-regularized logistic regression classifiers using
the LIBLINEAR package (Fan et al, 2008) on the
top 200 unigrams and bigrams ranked according to
Chi-squared and 100-dimensional topic vectors in-
duced by LDA and Twitter-LDA, respectively. We
</bodyText>
<footnote confidence="0.999916333333333">
2http://chuansong.me/account/pennyjob
3http://jliblog.com/app/rwordseg
4http://pullword.com/
</footnote>
<bodyText confidence="0.999982097560976">
also compare our model with/without word em-
beddings to demonstrate the effectiveness of this
semantic enhancement. The results are presented
in Table 1.
In general, LDA, Twitter-LDA and PAM
outperform unigram and bigram, showing the
strength of latent semantic modeling. Within the
first two models, Twitter-LDA yields better preci-
sions than LDA because of its ability to overcome
the difficulty of modeling short posts on social me-
dia. It designs an additional background word dis-
tribution to remove the noisy words and assumes
that a single post can belong to several topics.
Our 5-level PAM gains observed improvement
over Twitter-LDA. We attribute this to the ad-
vantages of tree representations over vector fea-
ture representations, the effective approximate tree
matching algorithm and the complementary word
embeddings. As mentioned in Section 1, LDA
and other topic models like Twitter-LDA share the
same assumption that each topic should be inde-
pendent with each other. This assumption however
is too strict for the real world data. Our tree-like 5-
level PAM relaxes such assumption with two addi-
tional layers of super-topics modeled with Dirich-
let compound multinomial distributions, which is
the key to capture topic correlations. Furthermore,
by allowing partial matching and incorporating
word embeddings, we successfully overcome the
sparseness problem.
While macro-averages give equal weight to
each domain, micro-averages give equal weight
to each user. The significant difference between
the macro- and micro- scores in Table 1 is caused
by the different nature of 5 domains. In fact, the
posts of experts on the domain E-Commerce are
to some extent noisy and contain lots of words
irrelevant to the domain knowledge. Meanwhile,
the posts of experts on the domain Data Science
are less distinguishable. The higher micro-recalls
of PAM demonstrate its generalization ability over
</bodyText>
<page confidence="0.996695">
619
</page>
<note confidence="0.789292">
LDA and Twitter-LDA. Allan M. Collins and M. Ross. Quillian. 1969. Re-
</note>
<figure confidence="0.392489">
trieval time from semantic memory. Journal of Ver-
5 Conclusion bal Learning and Verbal Behaviour, 8: 240247.
</figure>
<bodyText confidence="0.999822875">
In this paper, we formulate the expert finding task
as a tree matching problem with the hierarchical
knowledge representation. The experimental re-
sults demonstrate the advantage of using 5-level
PAM and semantic enhancement against n-gram
models and LDA-like models. To further improve
the work, we will incorporate more information to
enrich the hierarchical representation in the future.
</bodyText>
<sectionHeader confidence="0.969304" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996375">
The work described in this paper was supported
by the grants from the Research Grants Coun-
cil of Hong Kong (PolyU 5202/12E and PolyU
152094/14E) and the grants from the National
Natural Science Foundation of China (61272291
and 61273278).
</bodyText>
<sectionHeader confidence="0.990585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999337893333333">
Eugene Agichtein, Carlos Castillo, Debora Donato, et
al. 2008. Finding high-quality content in social me-
dia. In Proc. of WSDM.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, et al.
2003. A neural probabilistic language model. The
Journal of Machine Learning Research, 3: 1137-
1155.
Marc Bernard, Laurent Boyer, et al. 2008. Learning
probabilistic models of tree edit distance. Pattern
Recognition, 41(8): 2611-2629.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. The Journal of
Machine Learning Research, 3: 993-1022.
Mohamed Bouguessa, Benot Dumoulin, and Shen-
grui Wang. 2008. Identifying authoritative actors in
question-answering forums: the case of yahoo! an-
swers. In Proc. of SIGKDD.
George Casella and Roger L. Berger. 2001. Statistical
Inference. Duxbury Press.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP, pages 740750.
Fei Cheng, Kevin Duh, Yuji Matsumoto. 2014. Pars-
ing Chinese Synthetic Words with a Character-based
Dependency Model. LREC.
Ronan Collobert, Jason Weston, Leon Bottou, et al.
2011. Natural language processing (almost) from
scratch. JMLR, 12.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199207.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al.
2008. LIBLINEAR: A library for large linear clas-
sification. The Journal of Machine Learning Re-
search, 9: 1871-1874.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proc. of ACL.
Akshay Java, Pranam Kolari, Tim Finin, et al. 2006.
Modeling the spread of influence on the blogo-
sphere. In Proc. of WWW.
Akshay Java, Xiaodan Song, Tim Finin, and Belle
Tseng. 2007. Why we Twitter: Understanding
Microblogging Usage and Communities. In Proc.
WebKDD-SNA-KDD.
Jeffrey Pennington, Richard Socher, and Christopher
D. Manning. Glove: Global Vectors for Word Rep-
resentation. In Proc. of EMNLP.
Pawel Jurczyk and Eugene Agichtein. 2007. Discover-
ing authorities in question answer communities by
using link analysis. In Proc. of CIKM.
David Kempe, Jon Kleinberg, and Eva Tardos. 2003.
Maximizing the spread of influence through a social
network. In Proc. of SIGKDD.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, et al. 2014. A dependency parser for
tweets. In Proc. of EMNLP, pages 10011012, Doha,
Qatar, October.
Balachander Krishnamurthy, Phillipa Gill, and Martin
Arlitt. 2008. A few chirps about Twitter. In Proc. of
the first workshop on Online social networks. ACM,
pages 19-24.
Remi Lebret, Jo el Legrand, and Ronan Collobert.
2013. Is deep learning really necessary for word em-
beddings? In Proc. of NIPS.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. of ACL.
Omer Levy, Yoav Goldberg, And Ido Dagan. 2015.
Improving Distributional Similarity with Lessons
Learned from Word Embeddings. In Proc. of TACL.
</reference>
<page confidence="0.989017">
620
</page>
<reference confidence="0.993464281553398">
Wei Li and Andrew McCallum. 2006. Pachinko al-
location: DAG-structured mixture models of topic
correla-tions. In Proc. of the 23rd international con-
ference on Machine learning. ACM, pages 577-584.
Wei Li and Andrew McCallum. 2008. Pachinko alloca-
tion: Scalable mixture models of topic correlations.
Journal of Machine Learning Research.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A
recursive recurrent neural network for statistical ma-
chine translation. In Proc. ofACL, pages 1491 1500.
Minh-Thang Luong, Richard Socher, and Christopher
D. Manning. 2013. Better Word Representations
with Recursive Neural Networks for Morphology. In
Proc. of CoNLL.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the ACM,
38(11):3941.
Thomas P. Minka. 2000. Estimating a Dirichlet distri-
bution. Technical report, MIT.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013(a). Efficient estimation of word repre-
sentations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
S. Corrado, and Jeffrey Dean. 2013(b). Distributed
representations of words and phrases and their
composition-ality. In Advances in Neural Informa-
tion Processing Systems. pages 3111-3119.
Aditya Pal and Joseph A. Konstan. 2010. Expert Iden-
tification in Community Question Answering: Ex-
ploring Question Selection Bias. In Proc. of the 19th
ACM international conference on Information and
knowledge management. ACM, pages 1505-1508.
Aditya Pal and Scott Counts. 2011. Identifying topi-
cal authorities in microblogs. In Proc. of the fourth
ACM international conference on Web search and
data mining. ACM, pages 45-54.
Siyu Qiu, Qing Cui, Jiang Bian, and et al. 2014. Co-
learning of Word Representations and Morpheme
Representations. In Proc. of COLING.
Daniele Quercia, Harry Askham, and Jon Crowcroft.
2012. TweetLDA: supervised topic classification
and link prediction in Twitter. In Proc. of the 4th
Annual ACM Web Science Conference. ACM, pages
247-250.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009. Labeled LDA: A su-
pervised topic model for credit attribution in multi-
label corpora. In Proc. of EMNLP.
Daniel Ramage Susan Dumais, and Dan Liebling.
2010. Characterizing Microblogs with Topic Mod-
els. In ICWSM, 5(4): 130-137.
Ana Raposo, Mafalda Mendes, and J. Frederico Mar-
ques. 2012. The hierarchical organization of seman-
tic memory: Executive function in the processing of
superordinate concepts. NeuroImage, 59: 18701878.
Stanley M. Selkow. 1977. The tree-to-tree editing prob-
lem. Information processing letters, 6(6): 184-186.
Mahdi M. Shafiei and Evangelos E. Milios. 2006. La-
tent Dirichlet coclustering. In Proc. of International
Conference on Data Mining, pages 542-551.
Dennis Shasha and Kaizhong Zhang. 1990. Fast al-
gorithms for the unit cost editing distance between
trees. Journal of algorithms, 11(4): 581-621.
Yaming Sun, Lei Lin, Duyu Tang, and et al.
2014. Radical-enhanced chinese character embed-
ding. arXiv preprint arXiv:1404.4714.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 31043112.
Jie Tang, Jing Zhang, Limin Yao, et al. 2008. Arnet-
miner: Extraction and mining of academic social
networks. In Proc. of SIGKDD.
Duyu Tang, Furu Wei, Nan Yang, et al. 2014. Learning
sentiment-specific word embedding for twitter sen-
timent classification. In Proc. of ACL.
Robert A. Wagner. 1975. On the complexity of the ex-
tended string-to-string correction problem. In Proc.
of seventh annual ACM symposium on Theory of
computing. pages 218-223. ACM.
Robert A. Wagner and Michael J. Fischer. 1974. The
string-to-string correction problem. Journal of the
ACM(JACM), 21(1), 168-173.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proc. of NAACL, Denver,
CO.
Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He.
2010. Twitterrank: finding topic-sensitive influential
twitterers. In Proc. of WSDM.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proc. of Computation and Lan-
guage.
Yi Yang and Jacob Eisenstein. 2015. Unsupervised
multi-domain adaptation with feature embeddings.
In Proc. of NAACL-HIT.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proc. ofACL.
Jun Zhang, Mark S. Ackerman, and Lada Adamic.
2007. Expertise networks in online communities:
structure and algorithms. In Proc. of WWW.
</reference>
<page confidence="0.978327">
621
</page>
<reference confidence="0.998002">
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees
and related problems. SIAM journal on computing,
18(6): 1245-1262.
Meishan Zhang, Yue Zhang, Wan Xiang Che, and et
al. 2013. Chinese parsing exploiting characters. In
Proc. of ACL.
Xin Zhao and Jing Jiang. 2011. An empirical compari-
son of topics in twitter and traditional media. Singa-
pore Management University School of Information
Systems Technical paper series. Retrieved Novem-
ber, 10: 2011.
</reference>
<page confidence="0.997978">
622
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592531">
<title confidence="0.838224">A Hierarchical Knowledge Representation for Expert on Social Media Wenjie</title>
<author confidence="0.795373">Hong Kong Polytechnic University Department</author>
<author confidence="0.795373">Hong</author>
<affiliation confidence="0.999229">Laboratory of Computational Linguistics, Peking University, MOE,</affiliation>
<email confidence="0.963222">lisujian@pku.edu.cn</email>
<abstract confidence="0.999066214285714">Expert finding on social media benefits both individuals and commercial services. In this paper, we exploit a 5-level tree representation to model the posts on social media and cast the expert finding problem to the matching problem between the learned user tree and domain tree. We enhance the traditional approximate tree matching algorithm and incorporate word embeddings to improve the matching result. The experiments conducted on Sina Microblog demonstrate the effectiveness of our work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proc. of WSDM.</booktitle>
<marker>Agichtein, Castillo, Donato, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, et al. 2008. Finding high-quality content in social media. In Proc. of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rjean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="10188" citStr="Bengio et al, 2003" startWordPosition="1705" endWordPosition="1708"> disjoint sub-trees of Tu. We specifically penalize no cost if some sub-trees are missing in matching process. We define two types of operations. The substitution operations edit the dissimilar words on tree-nodes, while the insertion and deletion operations perform on tree-structures. Expert finding is then to calculate the minimum matching cost on Td and Tu. If the cost is smaller than an empirically defined threshold Ad, we identify user u as an expert on domain d. To alleviate the sparseness problem caused by direct letter-to-letter matching in tree-node mapping, we embed word embeddings (Bengio et al, 2003) into the substitution operation. We apply the word2vec skip-gram model (Mikolov et al, 2013(a); Mikolov et al, 2013(b)) to encode each word in our vocabulary with a probability vector and directly use the similarity generated by word2vec as the tree-node similarity. The costs of insertion and deletion operations will be explained in Section 4. Actually all these three costs can be defined in accordance with applicant needs. In brief, by combining both hierarchical representation of tree-structure and word embeddings of tree-nodes, we achieve our goal to enhance semantics. 4 Experiments The ex</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Rjean Ducharme, Pascal Vincent, et al. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3: 1137-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Bernard</author>
<author>Laurent Boyer</author>
</authors>
<title>Learning probabilistic models of tree edit distance.</title>
<date>2008</date>
<journal>Pattern Recognition,</journal>
<volume>41</volume>
<issue>8</issue>
<pages>2611--2629</pages>
<marker>Bernard, Boyer, 2008</marker>
<rawString>Marc Bernard, Laurent Boyer, et al. 2008. Learning probabilistic models of tree edit distance. Pattern Recognition, 41(8): 2611-2629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="1749" citStr="Blei et al, 2003" startWordPosition="254" endWordPosition="257">basis for mining expertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc. Despite the demand to access expertise, the challenges of identifying domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenge</context>
<context position="12922" citStr="Blei et al, 2003" startWordPosition="2182" endWordPosition="2185"> top to down, I = 10, J = 20, K = 20 for the number of second, third and fourth levels of topics, respectively. And we initialize -y and 6 with 0.25. For tree matching, we define the cost of tree-node substitution operation between word a and b as Eq (1). The costs of insertion and deletion operations for treestructure matching are MAX VALUE. Here we set MAX VALUE as 100 experimentally. The threshold Ad used to determine the expert is set to be 12 times of MAX VALUE. a(a—*b)= { 0, a = b (1) sim (a, b) , sim(a, b) &gt;0.55 MAX VALUE, otherwise We compare PAM with n-gram (unigram and bigram), LDA (Blei et al, 2003) and TwitterLDA (Zhao and Jiang, 2011). We set Q in LDA and Twitter-LDA to 0.01, -y in Twiitter-LDA to 20. For a, we adopt the commonly used 50/T heuristics where the number of topics T = 50. To be fair, we all use the tokens after pullword preprocessing as the input to extract features for classification. Following Zhao and Jiang (2011), we train four E2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al, 2008) on the top 200 unigrams and bigrams ranked according to Chi-squared and 100-dimensional topic vectors induced by LDA and Twitter-LDA, respectively. We 2</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. The Journal of Machine Learning Research, 3: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Bouguessa</author>
<author>Benot Dumoulin</author>
<author>Shengrui Wang</author>
</authors>
<title>Identifying authoritative actors in question-answering forums: the case of yahoo! answers.</title>
<date>2008</date>
<booktitle>In Proc. of SIGKDD.</booktitle>
<marker>Bouguessa, Dumoulin, Wang, 2008</marker>
<rawString>Mohamed Bouguessa, Benot Dumoulin, and Shengrui Wang. 2008. Identifying authoritative actors in question-answering forums: the case of yahoo! answers. In Proc. of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Casella</author>
<author>Roger L Berger</author>
</authors>
<title>Statistical Inference.</title>
<date>2001</date>
<publisher>Duxbury Press.</publisher>
<contexts>
<context position="7899" citStr="Casella and Berger, 2001" startWordPosition="1306" endWordPosition="1309">|V | θx α x y I θy γ J θz w z δ root x-topic y-topic z-topic . . . word · · · · · · . . . nij + γij (d) (d) [� L ni + L�j,=1 γij′ nkm + βm (d) nk +En m′=1 βm′ 617 indicates all observations or topic assignments except word w. With the fixed Dirichlet parameter α for the root and Q as the prior, what’s left is to estimate (learn from data) -y and 6 to capture the different correlations among topics. To avoid the use of iterative methods which are often computationally extensive, instead we approximate these two Dirichlet parameters using the moment matching algorithm, the same as (Minka, 2000; Casella and Berger, 2001; Shafiei and Milios, 2006). With smoothing techniques, in each iteration of Gibbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree</context>
</contexts>
<marker>Casella, Berger, 2001</marker>
<rawString>George Casella and Roger L. Berger. 2001. Statistical Inference. Duxbury Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>740750</pages>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. of EMNLP, pages 740750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Cheng</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Parsing Chinese Synthetic Words with a Character-based Dependency Model.</title>
<date>2014</date>
<publisher>LREC.</publisher>
<marker>Cheng, Duh, Matsumoto, 2014</marker>
<rawString>Fei Cheng, Kevin Duh, Yuji Matsumoto. 2014. Parsing Chinese Synthetic Words with a Character-based Dependency Model. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>JMLR,</journal>
<volume>12</volume>
<marker>Collobert, Weston, Bottou, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Leon Bottou, et al. 2011. Natural language processing (almost) from scratch. JMLR, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>199207</pages>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In Advances in Neural Information Processing Systems, pages 199207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="13369" citStr="Fan et al, 2008" startWordPosition="2260" endWordPosition="2263"> 12 times of MAX VALUE. a(a—*b)= { 0, a = b (1) sim (a, b) , sim(a, b) &gt;0.55 MAX VALUE, otherwise We compare PAM with n-gram (unigram and bigram), LDA (Blei et al, 2003) and TwitterLDA (Zhao and Jiang, 2011). We set Q in LDA and Twitter-LDA to 0.01, -y in Twiitter-LDA to 20. For a, we adopt the commonly used 50/T heuristics where the number of topics T = 50. To be fair, we all use the tokens after pullword preprocessing as the input to extract features for classification. Following Zhao and Jiang (2011), we train four E2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al, 2008) on the top 200 unigrams and bigrams ranked according to Chi-squared and 100-dimensional topic vectors induced by LDA and Twitter-LDA, respectively. We 2http://chuansong.me/account/pennyjob 3http://jliblog.com/app/rwordseg 4http://pullword.com/ also compare our model with/without word embeddings to demonstrate the effectiveness of this semantic enhancement. The results are presented in Table 1. In general, LDA, Twitter-LDA and PAM outperform unigram and bigram, showing the strength of latent semantic modeling. Within the first two models, Twitter-LDA yields better precisions than LDA because o</context>
</contexts>
<marker>Fan, Chang, Hsieh, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9: 1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Pranam Kolari</author>
<author>Tim Finin</author>
</authors>
<title>Modeling the spread of influence on the blogosphere.</title>
<date>2006</date>
<booktitle>In Proc. of WWW.</booktitle>
<marker>Java, Kolari, Finin, 2006</marker>
<rawString>Akshay Java, Pranam Kolari, Tim Finin, et al. 2006. Modeling the spread of influence on the blogosphere. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshay Java</author>
<author>Xiaodan Song</author>
<author>Tim Finin</author>
<author>Belle Tseng</author>
</authors>
<title>Why we Twitter: Understanding Microblogging Usage and Communities. In</title>
<date>2007</date>
<booktitle>Proc. WebKDD-SNA-KDD.</booktitle>
<contexts>
<context position="2298" citStr="Java et al, 2007" startWordPosition="347" endWordPosition="350">nalysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenges mentioned above inherently come from insufficient representations. They motivate us to propose a more flexible domain expert finding framework to explore effective representations that are able to tackle the complexity lies in the social media data. The basic idea is as follows. Experts talk about the professional knowledge in their posts and these posts are supposed to contain more domain knowledge than the posts from the other ordinary users. We determine whether or not users are experts on specific domains by matching their professional k</context>
</contexts>
<marker>Java, Song, Finin, Tseng, 2007</marker>
<rawString>Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng. 2007. Why we Twitter: Understanding Microblogging Usage and Communities. In Proc. WebKDD-SNA-KDD.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global Vectors for Word Representation.</title>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Pennington, Socher, Manning, </marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global Vectors for Word Representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pawel Jurczyk</author>
<author>Eugene Agichtein</author>
</authors>
<title>Discovering authorities in question answer communities by using link analysis.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM.</booktitle>
<marker>Jurczyk, Agichtein, 2007</marker>
<rawString>Pawel Jurczyk and Eugene Agichtein. 2007. Discovering authorities in question answer communities by using link analysis. In Proc. of CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kempe</author>
<author>Jon Kleinberg</author>
<author>Eva Tardos</author>
</authors>
<title>Maximizing the spread of influence through a social network. In</title>
<date>2003</date>
<booktitle>Proc. of SIGKDD.</booktitle>
<marker>Kempe, Kleinberg, Tardos, 2003</marker>
<rawString>David Kempe, Jon Kleinberg, and Eva Tardos. 2003. Maximizing the spread of influence through a social network. In Proc. of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>10011012</pages>
<location>Doha, Qatar,</location>
<marker>Kong, Schneider, Swayamdipta, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, et al. 2014. A dependency parser for tweets. In Proc. of EMNLP, pages 10011012, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balachander Krishnamurthy</author>
<author>Phillipa Gill</author>
<author>Martin Arlitt</author>
</authors>
<title>A few chirps about Twitter.</title>
<date>2008</date>
<booktitle>In Proc. of the first workshop on Online social networks. ACM,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1588" citStr="Krishnamurthy et al, 2008" startWordPosition="226" endWordPosition="230">a like academic publications. As already observed, social media users tend to follow others for professional interests and knowledge (Ramage et al, 2010). This builds the basis for mining expertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc. Despite the demand to access expertise, the challenges of identifying domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge con</context>
</contexts>
<marker>Krishnamurthy, Gill, Arlitt, 2008</marker>
<rawString>Balachander Krishnamurthy, Phillipa Gill, and Martin Arlitt. 2008. A few chirps about Twitter. In Proc. of the first workshop on Online social networks. ACM, pages 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remi Lebret</author>
<author>Jo el Legrand</author>
<author>Ronan Collobert</author>
</authors>
<title>Is deep learning really necessary for word embeddings?</title>
<date>2013</date>
<booktitle>In Proc. of NIPS.</booktitle>
<marker>Lebret, el Legrand, Collobert, 2013</marker>
<rawString>Remi Lebret, Jo el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings? In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving Distributional Similarity with Lessons Learned from Word Embeddings.</title>
<date>2015</date>
<booktitle>In Proc. of TACL.</booktitle>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, And Ido Dagan. 2015. Improving Distributional Similarity with Lessons Learned from Word Embeddings. In Proc. of TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: DAG-structured mixture models of topic correla-tions.</title>
<date>2006</date>
<booktitle>In Proc. of the 23rd international conference on Machine learning. ACM,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="4680" citStr="Li and McCallum, 2006" startWordPosition="712" endWordPosition="715">ural Language Processing (Short Papers), pages 616–622, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics = mains’ knowledge. Using such trees allows us to flexibly incorporate more information into the data representation, such as the relations between latent topics and the semantic similarities between words. The experiments conducted on Sina Microblog demonstrate the effectiveness of the proposed framework and the corresponding methods. 2 Knowledge Representation with Hierarchical Tree To capture correlations between topics, Pachinko Allocation Model (PAM) (Li and McCallum, 2006) uses a directed acyclic graph (DAG) with leaves representing individual words in the vocabulary and each interior node representing a correlation among its children. In particular, multi-level PAM is capable of revealing interconnection between sub-level nodes by inferencing corresponding super-level nodes. It is a desired property that enables us to capture hierarchical relations among both inner-level and inter-level nodes and thereby enhance the representation of users’ posts. More important, the inter-level hierarchy benefits to distribute words from super-level generic topics to sub-leve</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured mixture models of topic correla-tions. In Proc. of the 23rd international conference on Machine learning. ACM, pages 577-584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Pachinko allocation: Scalable mixture models of topic correlations.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research.</journal>
<marker>Li, McCallum, 2008</marker>
<rawString>Wei Li and Andrew McCallum. 2008. Pachinko allocation: Scalable mixture models of topic correlations. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>1491--1500</pages>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proc. ofACL, pages 1491 1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better Word Representations with Recursive Neural Networks for Morphology.</title>
<date>2013</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better Word Representations with Recursive Neural Networks for Morphology. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38(11):3941.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas P Minka</author>
</authors>
<title>Estimating a Dirichlet distribution.</title>
<date>2000</date>
<tech>Technical report, MIT.</tech>
<contexts>
<context position="7873" citStr="Minka, 2000" startWordPosition="1304" endWordPosition="1305">. −w N K β ϕ |V | θx α x y I θy γ J θz w z δ root x-topic y-topic z-topic . . . word · · · · · · . . . nij + γij (d) (d) [� L ni + L�j,=1 γij′ nkm + βm (d) nk +En m′=1 βm′ 617 indicates all observations or topic assignments except word w. With the fixed Dirichlet parameter α for the root and Q as the prior, what’s left is to estimate (learn from data) -y and 6 to capture the different correlations among topics. To avoid the use of iterative methods which are often computationally extensive, instead we approximate these two Dirichlet parameters using the moment matching algorithm, the same as (Minka, 2000; Casella and Berger, 2001; Shafiei and Milios, 2006). With smoothing techniques, in each iteration of Gibbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a</context>
</contexts>
<marker>Minka, 2000</marker>
<rawString>Thomas P. Minka. 2000. Estimating a Dirichlet distribution. Technical report, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="10280" citStr="Mikolov et al, 2013" startWordPosition="1719" endWordPosition="1722"> matching process. We define two types of operations. The substitution operations edit the dissimilar words on tree-nodes, while the insertion and deletion operations perform on tree-structures. Expert finding is then to calculate the minimum matching cost on Td and Tu. If the cost is smaller than an empirically defined threshold Ad, we identify user u as an expert on domain d. To alleviate the sparseness problem caused by direct letter-to-letter matching in tree-node mapping, we embed word embeddings (Bengio et al, 2003) into the substitution operation. We apply the word2vec skip-gram model (Mikolov et al, 2013(a); Mikolov et al, 2013(b)) to encode each word in our vocabulary with a probability vector and directly use the similarity generated by word2vec as the tree-node similarity. The costs of insertion and deletion operations will be explained in Section 4. Actually all these three costs can be defined in accordance with applicant needs. In brief, by combining both hierarchical representation of tree-structure and word embeddings of tree-nodes, we achieve our goal to enhance semantics. 4 Experiments The experiments are conducted on 5 domains (i.e., Beauty Blogger, Beauty Doctor, Parenting, EComme</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013(a). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their composition-ality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="10280" citStr="Mikolov et al, 2013" startWordPosition="1719" endWordPosition="1722"> matching process. We define two types of operations. The substitution operations edit the dissimilar words on tree-nodes, while the insertion and deletion operations perform on tree-structures. Expert finding is then to calculate the minimum matching cost on Td and Tu. If the cost is smaller than an empirically defined threshold Ad, we identify user u as an expert on domain d. To alleviate the sparseness problem caused by direct letter-to-letter matching in tree-node mapping, we embed word embeddings (Bengio et al, 2003) into the substitution operation. We apply the word2vec skip-gram model (Mikolov et al, 2013(a); Mikolov et al, 2013(b)) to encode each word in our vocabulary with a probability vector and directly use the similarity generated by word2vec as the tree-node similarity. The costs of insertion and deletion operations will be explained in Section 4. Actually all these three costs can be defined in accordance with applicant needs. In brief, by combining both hierarchical representation of tree-structure and word embeddings of tree-nodes, we achieve our goal to enhance semantics. 4 Experiments The experiments are conducted on 5 domains (i.e., Beauty Blogger, Beauty Doctor, Parenting, EComme</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013(b). Distributed representations of words and phrases and their composition-ality. In Advances in Neural Information Processing Systems. pages 3111-3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Pal</author>
<author>Joseph A Konstan</author>
</authors>
<title>Expert Identification in Community Question Answering: Exploring Question Selection Bias.</title>
<date>2010</date>
<booktitle>In Proc. of the 19th ACM international conference on Information and knowledge management. ACM,</booktitle>
<pages>1505--1508</pages>
<marker>Pal, Konstan, 2010</marker>
<rawString>Aditya Pal and Joseph A. Konstan. 2010. Expert Identification in Community Question Answering: Exploring Question Selection Bias. In Proc. of the 19th ACM international conference on Information and knowledge management. ACM, pages 1505-1508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Pal</author>
<author>Scott Counts</author>
</authors>
<title>Identifying topical authorities in microblogs.</title>
<date>2011</date>
<booktitle>In Proc. of the fourth ACM international conference on Web search and data mining. ACM,</booktitle>
<pages>45--54</pages>
<marker>Pal, Counts, 2011</marker>
<rawString>Aditya Pal and Scott Counts. 2011. Identifying topical authorities in microblogs. In Proc. of the fourth ACM international conference on Web search and data mining. ACM, pages 45-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siyu Qiu</author>
<author>Qing Cui</author>
<author>Jiang Bian</author>
</authors>
<title>Colearning of Word Representations and Morpheme Representations.</title>
<date>2014</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Qiu, Cui, Bian, 2014</marker>
<rawString>Siyu Qiu, Qing Cui, Jiang Bian, and et al. 2014. Colearning of Word Representations and Morpheme Representations. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Quercia</author>
<author>Harry Askham</author>
<author>Jon Crowcroft</author>
</authors>
<title>TweetLDA: supervised topic classification and link prediction in Twitter.</title>
<date>2012</date>
<booktitle>In Proc. of the 4th Annual ACM Web Science Conference. ACM,</booktitle>
<pages>247--250</pages>
<contexts>
<context position="2004" citStr="Quercia et al, 2012" startWordPosition="300" endWordPosition="303">t. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenges mentioned above inherently come from insufficient representations. They motivate us to propose a more flexible domain expert finding framework to explore effective representations that are able to tackle the complexity lies in the social media data. The</context>
</contexts>
<marker>Quercia, Askham, Crowcroft, 2012</marker>
<rawString>Daniele Quercia, Harry Askham, and Jon Crowcroft. 2012. TweetLDA: supervised topic classification and link prediction in Twitter. In Proc. of the 4th Annual ACM Web Science Conference. ACM, pages 247-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multilabel corpora.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1770" citStr="Ramage et al, 2009" startWordPosition="258" endWordPosition="262">xpertise and finding experts on social media, which facilitates the services of user recommendation and questionanswering, etc. Despite the demand to access expertise, the challenges of identifying domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenges mentioned above inh</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multilabel corpora. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage Susan Dumais</author>
<author>Dan Liebling</author>
</authors>
<title>Characterizing Microblogs with Topic Models.</title>
<date>2010</date>
<booktitle>In ICWSM,</booktitle>
<volume>5</volume>
<issue>4</issue>
<pages>130--137</pages>
<marker>Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage Susan Dumais, and Dan Liebling. 2010. Characterizing Microblogs with Topic Models. In ICWSM, 5(4): 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Raposo</author>
<author>Mafalda Mendes</author>
<author>J Frederico Marques</author>
</authors>
<title>The hierarchical organization of semantic memory: Executive function in the processing of superordinate concepts.</title>
<date>2012</date>
<journal>NeuroImage,</journal>
<volume>59</volume>
<pages>18701878</pages>
<marker>Raposo, Mendes, Marques, 2012</marker>
<rawString>Ana Raposo, Mafalda Mendes, and J. Frederico Marques. 2012. The hierarchical organization of semantic memory: Executive function in the processing of superordinate concepts. NeuroImage, 59: 18701878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley M Selkow</author>
</authors>
<title>The tree-to-tree editing problem.</title>
<date>1977</date>
<journal>Information processing letters,</journal>
<volume>6</volume>
<issue>6</issue>
<pages>184--186</pages>
<contexts>
<context position="8552" citStr="Selkow, 1977" startWordPosition="1426" endWordPosition="1427">ng techniques, in each iteration of Gibbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using edit distance (Selkow, 1977; Shasha and Zhang, 1990; Wagner, 1975; Wagner and Fischer, 1974; Zhang and Shasha, 1989). Given two trees T1 and T2, a typical edit distance-based correction approach is to transform T1 to T2 with a sequence of editing operations 5 =&lt; s1, s2, ... , sk &gt; such that sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation is assigned a cost u(si) that represents the difficulty of making that operation. By summing up the costs of all necessary operations, the total cost Q(5) = ∑ki=1 u(si) defines the matching degree of T1 and T2. We assume that an expert could only master a part of professional domain </context>
</contexts>
<marker>Selkow, 1977</marker>
<rawString>Stanley M. Selkow. 1977. The tree-to-tree editing problem. Information processing letters, 6(6): 184-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahdi M Shafiei</author>
<author>Evangelos E Milios</author>
</authors>
<title>Latent Dirichlet coclustering.</title>
<date>2006</date>
<booktitle>In Proc. of International Conference on Data Mining,</booktitle>
<pages>542--551</pages>
<contexts>
<context position="7926" citStr="Shafiei and Milios, 2006" startWordPosition="1310" endWordPosition="1313">w z δ root x-topic y-topic z-topic . . . word · · · · · · . . . nij + γij (d) (d) [� L ni + L�j,=1 γij′ nkm + βm (d) nk +En m′=1 βm′ 617 indicates all observations or topic assignments except word w. With the fixed Dirichlet parameter α for the root and Q as the prior, what’s left is to estimate (learn from data) -y and 6 to capture the different correlations among topics. To avoid the use of iterative methods which are often computationally extensive, instead we approximate these two Dirichlet parameters using the moment matching algorithm, the same as (Minka, 2000; Casella and Berger, 2001; Shafiei and Milios, 2006). With smoothing techniques, in each iteration of Gibbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using e</context>
</contexts>
<marker>Shafiei, Milios, 2006</marker>
<rawString>Mahdi M. Shafiei and Evangelos E. Milios. 2006. Latent Dirichlet coclustering. In Proc. of International Conference on Data Mining, pages 542-551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Shasha</author>
<author>Kaizhong Zhang</author>
</authors>
<title>Fast algorithms for the unit cost editing distance between trees.</title>
<date>1990</date>
<journal>Journal of algorithms,</journal>
<volume>11</volume>
<issue>4</issue>
<pages>581--621</pages>
<contexts>
<context position="8576" citStr="Shasha and Zhang, 1990" startWordPosition="1428" endWordPosition="1431"> in each iteration of Gibbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using edit distance (Selkow, 1977; Shasha and Zhang, 1990; Wagner, 1975; Wagner and Fischer, 1974; Zhang and Shasha, 1989). Given two trees T1 and T2, a typical edit distance-based correction approach is to transform T1 to T2 with a sequence of editing operations 5 =&lt; s1, s2, ... , sk &gt; such that sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation is assigned a cost u(si) that represents the difficulty of making that operation. By summing up the costs of all necessary operations, the total cost Q(5) = ∑ki=1 u(si) defines the matching degree of T1 and T2. We assume that an expert could only master a part of professional domain knowledge rather than th</context>
</contexts>
<marker>Shasha, Zhang, 1990</marker>
<rawString>Dennis Shasha and Kaizhong Zhang. 1990. Fast algorithms for the unit cost editing distance between trees. Journal of algorithms, 11(4): 581-621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaming Sun</author>
<author>Lei Lin</author>
<author>Duyu Tang</author>
</authors>
<title>Radical-enhanced chinese character embedding. arXiv preprint arXiv:1404.4714.</title>
<date>2014</date>
<marker>Sun, Lin, Tang, 2014</marker>
<rawString>Yaming Sun, Lei Lin, Duyu Tang, and et al. 2014. Radical-enhanced chinese character embedding. arXiv preprint arXiv:1404.4714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>31043112</pages>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 31043112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Tang</author>
<author>Jing Zhang</author>
<author>Limin Yao</author>
</authors>
<title>Arnetminer: Extraction and mining of academic social networks.</title>
<date>2008</date>
<booktitle>In Proc. of SIGKDD.</booktitle>
<marker>Tang, Zhang, Yao, 2008</marker>
<rawString>Jie Tang, Jing Zhang, Limin Yao, et al. 2008. Arnetminer: Extraction and mining of academic social networks. In Proc. of SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Tang, Wei, Yang, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, et al. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
</authors>
<title>On the complexity of the extended string-to-string correction problem.</title>
<date>1975</date>
<booktitle>In Proc. of seventh annual ACM symposium on Theory of computing.</booktitle>
<pages>218--223</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8590" citStr="Wagner, 1975" startWordPosition="1432" endWordPosition="1433">bbs sampling we update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using edit distance (Selkow, 1977; Shasha and Zhang, 1990; Wagner, 1975; Wagner and Fischer, 1974; Zhang and Shasha, 1989). Given two trees T1 and T2, a typical edit distance-based correction approach is to transform T1 to T2 with a sequence of editing operations 5 =&lt; s1, s2, ... , sk &gt; such that sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation is assigned a cost u(si) that represents the difficulty of making that operation. By summing up the costs of all necessary operations, the total cost Q(5) = ∑ki=1 u(si) defines the matching degree of T1 and T2. We assume that an expert could only master a part of professional domain knowledge rather than the whole and th</context>
</contexts>
<marker>Wagner, 1975</marker>
<rawString>Robert A. Wagner. 1975. On the complexity of the extended string-to-string correction problem. In Proc. of seventh annual ACM symposium on Theory of computing. pages 218-223. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The string-to-string correction problem.</title>
<date>1974</date>
<journal>Journal of the ACM(JACM),</journal>
<volume>21</volume>
<issue>1</issue>
<pages>168--173</pages>
<contexts>
<context position="8616" citStr="Wagner and Fischer, 1974" startWordPosition="1434" endWordPosition="1437">e update: (d) nij × (E( (d) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using edit distance (Selkow, 1977; Shasha and Zhang, 1990; Wagner, 1975; Wagner and Fischer, 1974; Zhang and Shasha, 1989). Given two trees T1 and T2, a typical edit distance-based correction approach is to transform T1 to T2 with a sequence of editing operations 5 =&lt; s1, s2, ... , sk &gt; such that sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation is assigned a cost u(si) that represents the difficulty of making that operation. By summing up the costs of all necessary operations, the total cost Q(5) = ∑ki=1 u(si) defines the matching degree of T1 and T2. We assume that an expert could only master a part of professional domain knowledge rather than the whole and thereby revise a traditional</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1974. The string-to-string correction problem. Journal of the ACM(JACM), 21(1), 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL,</booktitle>
<location>Denver, CO.</location>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proc. of NAACL, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianshu Weng</author>
<author>Ee Peng Lim</author>
<author>Jing Jiang</author>
<author>Qi He</author>
</authors>
<title>Twitterrank: finding topic-sensitive influential twitterers.</title>
<date>2010</date>
<booktitle>In Proc. of WSDM.</booktitle>
<contexts>
<context position="2316" citStr="Weng et al, 2010" startWordPosition="351" endWordPosition="354">ch as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenges mentioned above inherently come from insufficient representations. They motivate us to propose a more flexible domain expert finding framework to explore effective representations that are able to tackle the complexity lies in the social media data. The basic idea is as follows. Experts talk about the professional knowledge in their posts and these posts are supposed to contain more domain knowledge than the posts from the other ordinary users. We determine whether or not users are experts on specific domains by matching their professional knowledge and domai</context>
</contexts>
<marker>Weng, Lim, Jiang, He, 2010</marker>
<rawString>Jianshu Weng, Ee Peng Lim, Jing Jiang and Qi He. 2010. Twitterrank: finding topic-sensitive influential twitterers. In Proc. of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proc. of Computation and Language.</booktitle>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proc. of Computation and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Unsupervised multi-domain adaptation with feature embeddings.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL-HIT.</booktitle>
<marker>Yang, Eisenstein, 2015</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2015. Unsupervised multi-domain adaptation with feature embeddings. In Proc. of NAACL-HIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhang</author>
<author>Mark S Ackerman</author>
<author>Lada Adamic</author>
</authors>
<title>Expertise networks in online communities: structure and algorithms.</title>
<date>2007</date>
<booktitle>In Proc. of WWW.</booktitle>
<marker>Zhang, Ackerman, Adamic, 2007</marker>
<rawString>Jun Zhang, Mark S. Ackerman, and Lada Adamic. 2007. Expertise networks in online communities: structure and algorithms. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAM journal on computing,</journal>
<volume>18</volume>
<issue>6</issue>
<pages>1245--1262</pages>
<contexts>
<context position="8641" citStr="Zhang and Shasha, 1989" startWordPosition="1438" endWordPosition="1441">) − meanij)2 d ni � 1 + (L − meanij)2 meanij × (1 − meanij) 1 varij meanij ∑j log(mij) exp L−1 where Ni is the number of documents with nonzero counts of super-level topic xi. Parameter estimation of 6 is the same as &apos;y. 3 Expert Finding with Approximate Tree Matching Once the hierarchical representations of users and domains have been generated, we can determine whether or not a user is an expert on a domain based on their matching degree, which is a problem analogous to tree-to-tree correction using edit distance (Selkow, 1977; Shasha and Zhang, 1990; Wagner, 1975; Wagner and Fischer, 1974; Zhang and Shasha, 1989). Given two trees T1 and T2, a typical edit distance-based correction approach is to transform T1 to T2 with a sequence of editing operations 5 =&lt; s1, s2, ... , sk &gt; such that sk (sk−1 (... (s1 (T1)) ...)) = T2. Each operation is assigned a cost u(si) that represents the difficulty of making that operation. By summing up the costs of all necessary operations, the total cost Q(5) = ∑ki=1 u(si) defines the matching degree of T1 and T2. We assume that an expert could only master a part of professional domain knowledge rather than the whole and thereby revise a traditional approximate tree matchin</context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM journal on computing, 18(6): 1245-1262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wan Xiang Che</author>
</authors>
<title>Chinese parsing exploiting characters.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Zhang, Zhang, Che, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wan Xiang Che, and et al. 2013. Chinese parsing exploiting characters. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Zhao</author>
<author>Jing Jiang</author>
</authors>
<title>An empirical comparison of topics in twitter and traditional media.</title>
<date>2011</date>
<volume>10</volume>
<institution>Singapore Management University School of Information Systems</institution>
<note>Technical paper series. Retrieved</note>
<contexts>
<context position="1965" citStr="Zhao and Jiang, 2011" startWordPosition="292" endWordPosition="295">ing domain experts on social media exist. Social media often contains plenty of noises such as the tags with which users describe themselves. Noises impose the inherent drawback on the feature-based learning methods (Krishnamurthy et al, 2008). Data imbalance and sparseness also limits the performance of the promising latent semantic analysis methods such as the LDA-like topic models (Blei et al, 2003; Ramage et al, 2009). When some topics co-occur more frequently than others, the strict assumption of these topic models cannot be met and consequently many nonsensical topics will be generated (Zhao and Jiang, 2011; Pal et al, 2011; Quercia et al, 2012). Furthermore, not as simple as celebrities, the definition of experts introduces additional difficulties. Experts cannot be simply judged by the number of followers. The knowledge conveyed in what they say is essential. This leads to the failures of the network-based methods (Java et al, 2007; Weng et al, 2010; Pal et al, 2011). The challenges mentioned above inherently come from insufficient representations. They motivate us to propose a more flexible domain expert finding framework to explore effective representations that are able to tackle the comple</context>
<context position="12960" citStr="Zhao and Jiang, 2011" startWordPosition="2189" endWordPosition="2192">20 for the number of second, third and fourth levels of topics, respectively. And we initialize -y and 6 with 0.25. For tree matching, we define the cost of tree-node substitution operation between word a and b as Eq (1). The costs of insertion and deletion operations for treestructure matching are MAX VALUE. Here we set MAX VALUE as 100 experimentally. The threshold Ad used to determine the expert is set to be 12 times of MAX VALUE. a(a—*b)= { 0, a = b (1) sim (a, b) , sim(a, b) &gt;0.55 MAX VALUE, otherwise We compare PAM with n-gram (unigram and bigram), LDA (Blei et al, 2003) and TwitterLDA (Zhao and Jiang, 2011). We set Q in LDA and Twitter-LDA to 0.01, -y in Twiitter-LDA to 20. For a, we adopt the commonly used 50/T heuristics where the number of topics T = 50. To be fair, we all use the tokens after pullword preprocessing as the input to extract features for classification. Following Zhao and Jiang (2011), we train four E2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al, 2008) on the top 200 unigrams and bigrams ranked according to Chi-squared and 100-dimensional topic vectors induced by LDA and Twitter-LDA, respectively. We 2http://chuansong.me/account/pennyjob 3</context>
</contexts>
<marker>Zhao, Jiang, 2011</marker>
<rawString>Xin Zhao and Jing Jiang. 2011. An empirical comparison of topics in twitter and traditional media. Singapore Management University School of Information Systems Technical paper series. Retrieved November, 10: 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>