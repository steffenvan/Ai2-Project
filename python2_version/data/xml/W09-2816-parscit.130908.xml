<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000415">
<title confidence="0.9939015">
The GREC Main Subject Reference Generation Challenge 2009:
Overview and Evaluation Results
</title>
<author confidence="0.979951">
Anja Belz Eric Kow
</author>
<affiliation confidence="0.9834245">
NLT Group
University of Brighton
</affiliation>
<address confidence="0.995824">
Brighton BN2 4GJ, UK
</address>
<email confidence="0.999403">
{asb,eykk10}@bton.ac.uk
</email>
<author confidence="0.980269">
Jette Viethen
</author>
<affiliation confidence="0.98914">
Centre for LT
Macquarie University
</affiliation>
<address confidence="0.703654">
Sydney NSW 2109
</address>
<email confidence="0.992758">
jviethen@ics.mq.edu.au
</email>
<author confidence="0.978177">
Albert Gatt
</author>
<affiliation confidence="0.991283">
Computing Science
University of Aberdeen
</affiliation>
<address confidence="0.958829">
Aberdeen AB24 3UE, UK
</address>
<email confidence="0.998313">
a.gatt@abdn.ac.uk
</email>
<sectionHeader confidence="0.997378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986555555556">
The GREC-MSR Task at Generation Chal-
lenges 2009 required participating systems
to select coreference chains to the main
subject of short encyclopaedic texts col-
lected from Wikipedia. Three teams sub-
mitted one system each, and we addition-
ally created four baseline systems. Sys-
tems were tested automatically using ex-
isting intrinsic metrics. We also evaluated
systems extrinsically by applying corefer-
ence resolution tools to the outputs and
measuring the success of the tools. In ad-
dition, systems were tested in an intrinsic
evaluation involving human judges. This
report describes the GREC-MSR Task and
the evaluation methods applied, gives brief
descriptions of the participating systems,
and presents the evaluation results.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999063696969697">
The GREC-MSR Task is about how to generate ap-
propriate references to an entity in the context of a
piece of discourse longer than a sentence. Rather
than requiring participants to generate referring
expressions from scratch, the GREC-MSR data pro-
vides sets of possible referring expressions for se-
lection. This was the second time we ran a shared
task using the GREC-MSR data (following a first
run in 2008). The task definition was again kept
fairly simple, but in the 2009 round the main aim
for participating systems was to select an appro-
priate word string to serve as a referring expres-
sion, whereas in 2008 it was to select an appropri-
ate type of referring expression (name, common
noun, pronoun, or empty reference).
The immediate motivating application context
for the GREC-MSR Task is the improvement of ref-
erential clarity and coherence in extractive sum-
maries by regenerating referring expressions in
them. There has recently been a small flurry
of work in this area (Steinberger et al., 2007;
Nenkova, 2008). In the longer term, the GREC-
MSR Task is intended to be a step in the direction
of the more general task of generating referential
expressions in discourse context.
The GREC-MSR data is an extension of the
GREC 1.0 Corpus which had about 1,000 texts in
the subdomains of cities, countries, rivers and peo-
ple (Belz and Varges, 2007a). For the purpose of
the GREC-MSR shared task, an additional 1,000
texts in the new subdomain of mountain texts were
obtained and a new XML annotation scheme (Sec-
tion 2.2) was developed.
</bodyText>
<table confidence="0.98731475">
Team System Name
University of Delaware UDel
ICSI, Berkeley ICSI-CRF
Jadavpur University JUNLG
</table>
<tableCaption confidence="0.999793">
Table 1: GREC-MSR’09 participating teams.
</tableCaption>
<bodyText confidence="0.999789625">
Nine teams from seven countries registered for
GREC-MSR’09, of which three teams (Table 1)
submitted one system each.1 Participants had to
submit their system reports before downloading
test data inputs, and had to submit test data out-
puts within 48 hours of downloading the test data
inputs. In addition to the participants’ systems,
we also used the corpus texts themselves as ‘sys-
tem’ outputs, and created 4 baseline systems; we
evaluated the resulting 8 systems using a range of
intrinsic and extrinsic evaluation methods (for de-
tails see Sections 5 and 6). This report presents the
results of all evaluations (Section 6), along with
descriptions of GREC-MSR data and task (Sec-
tion 2), test sets (Section 3), evaluation methods
(Section 4), and participating systems (Section 5).
</bodyText>
<sectionHeader confidence="0.977952" genericHeader="introduction">
2 Data and Task
</sectionHeader>
<bodyText confidence="0.978922">
The GREC Corpus (version 2.0) consists of about
2,000 texts in total, all collected from introduc-
</bodyText>
<footnote confidence="0.9668555">
1One team submitted by the original deadline (Jan. 2009),
one by the revised deadline (1 June 2009), one slightly later.
</footnote>
<page confidence="0.973596">
79
</page>
<note confidence="0.9989965">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 79–87,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997226666666667">
tory sections in Wikipedia articles, in five different
subdomains (cities, countries, rivers, people and
mountains). In each text, three broad categories
of Main Subject Reference (MSR)2 have been an-
notated, resulting in a total of about 13,000 anno-
tated REs. The GREC-MSR shared task version of
the corpus was randomly divided into 90% train-
ing data (of which 10% were randomly selected as
development data) and 10% test data. Participants
used the training data in developing their systems,
and (as a minimum requirement) reported results
on the development data.
</bodyText>
<subsectionHeader confidence="0.999062">
2.1 Types of referential expression annotated
</subsectionHeader>
<bodyText confidence="0.999972631578947">
Three broad categories of main subject referring
expressions (MSREs) are annotated in the GREC
corpus3 — subject NPs, object NPs, and geni-
tive NPs and pronouns which function as subject-
determiners within their matrix NP. These cate-
gories of referring expressions (RE) are relatively
straightforward to identify and to achieve high
inter-annotator agreement on (complete agree-
ment among four annotators in 86% of MSRs), and
account for most cases of overt main subject refer-
ence in the GREC texts. The annotators were asked
to identify subject, object and genitive subject-
determiners and decide whether or not they refer
to the main subject of the text. More detail is pro-
vided in Belz and Varges (2007b).
In addition to the above, relative pronouns in
supplementary relative clauses (as opposed to in-
tegrated relative clauses, Huddleston and Pullum,
2002, p. 1058) were annotated, e.g.:
</bodyText>
<equation confidence="0.331036">
(1) Stoichkov is a football manager and former striker
</equation>
<bodyText confidence="0.8138336">
who was a member of the Bulgaria national team that
finishedfourth at the 1994 FIFA World Cup.
We also annotated ‘non-realised’ subject MSREs
in those cases of VP coordination where an MSRE
is the subject of the coordinated VPs, e.g.:
</bodyText>
<listItem confidence="0.985254333333333">
(2) He stated the first version of the Law of conservation
of mass, introduced the Metric system, and
helped to reform chemical nomenclature.
</listItem>
<bodyText confidence="0.999286833333333">
The motivation for annotating the approximate
place where the subject NP would be if it were
realised (the gap-like underscores above) is that
from a generation perspective there is a choice to
be made about whether to realise the subject NP in
the second and third coordinates or not.
</bodyText>
<footnote confidence="0.9912732">
2The main subject of a Wikipedia article is simply taken to
be given by its title, e.g. in the cities domain the main subject
(and title) of one text is London.
3In terminology and view of grammar the annotations rely
heavily on Huddleston and Pullum (2002).
</footnote>
<subsectionHeader confidence="0.981777">
2.2 XML format
</subsectionHeader>
<bodyText confidence="0.9994635">
Figure 1 is one of the texts distributed in the
GREC-MSR training/development data set. The
REF element indicates a reference, in the sense of
‘an instance of referring’ (which could, in princi-
ple, be realised by gesture or graphically, as well
as by a string of words, or a combination of these).
REFs have three attributes: ID, a unique refer-
ence identifier; SEMCAT, the semantic category of
the referent, ranging over city, country, river,
person, mountain; and SYNCAT, the syntactic cate-
gory required of referential expressions for the ref-
erent in this discourse context (np-obj, np-subj,
subj-det). A REF is composed of one REFEX ele-
ment (the ‘selected’ referential expression for the
given reference; in the training/development data
texts it is simply the referential expression found
in the corpus) and one ALT-REFEX element which
in turn is a list of REFEXs which are possible alter-
native referential expressions (see following sec-
tion).
REFEX elements have four attributes. The
HEAD attribute has the possible values nominal,
pronoun, and rel-pron; the CASE attribute has
the possible values nominative, accusative and
genitive for pronouns, and plain and genitive
for nominals. The binary-valued EMPHATIC at-
tribute indicates whether the RE is emphatic; in
the GREC-MSR corpus, the only type of RE that
has EMPHATIC=yes is one which incorporates a re-
flexive pronoun used emphatically (e.g. India it-
self). The REG08-TYPE attribute indicates basic RE
type. The choice of types is motivated by the hy-
pothesis that one of the most basic decisions to be
taken in RE selection for named entities is whether
to use an RE that includes a name, such as Mod-
ern India (the corresponding REG08-TYPE value
is name); whether to go for a common-noun RE,
i.e. with a category noun like country as the head
(common); whether to use a pronoun (pronoun); or
whether it can be left unrealised (empty).
</bodyText>
<subsectionHeader confidence="0.998615">
2.3 The GREC-MSR Task
</subsectionHeader>
<bodyText confidence="0.9999435">
The task for participating systems was to develop
a method for selecting one of the REFEXs in the
ALT-REFEX list, for each REF in each TEXT in the
test sets. The test data inputs were identical to
the training/development data, except that REF el-
ements contained only an ALT-REFEX list, not the
preceding ‘selected’ REFEX. ALT-REFEX lists are
generated for each text by an automatic method
</bodyText>
<page confidence="0.996254">
80
</page>
<table confidence="0.9611274">
&lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&gt;
&lt;!DOCTYPE TEXT SYSTEM &amp;quot;reg08-grec.dtd&amp;quot;&gt;
&lt;TEXT ID=&amp;quot;36&amp;quot;&gt;
&lt;TITLE&gt;Jean Baudrillard&lt;/TITLE&gt;
&lt;PARAGRAPH&gt;
&lt;REF ID=&amp;quot;36.1&amp;quot; SEMCAT=&amp;quot;person&amp;quot; SYNCAT=&amp;quot;np-subj&amp;quot;&gt;
&lt;REFEX REG08-TYPE=&amp;quot;name&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;nominal&amp;quot; CASE=&amp;quot;plain&amp;quot;&gt;Jean Baudrillard&lt;/REFEX&gt;
&lt;ALT-REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;name&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;nominal&amp;quot; CASE=&amp;quot;plain&amp;quot;&gt;Jean Baudrillard&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;name&amp;quot; EMPHATIC=&amp;quot;yes&amp;quot; HEAD=&amp;quot;nominal&amp;quot; CASE=&amp;quot;plain&amp;quot;&gt;Jean Baudrillard himself&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;empty&amp;quot;&gt;_&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;pronoun&amp;quot; CASE=&amp;quot;nominative&amp;quot;&gt;he&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;yes&amp;quot; HEAD=&amp;quot;pronoun&amp;quot; CASE=&amp;quot;nominative&amp;quot;&gt;he himself&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;rel-pron&amp;quot; CASE=&amp;quot;nominative&amp;quot;&gt;who&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;yes&amp;quot; HEAD=&amp;quot;rel-pron&amp;quot; CASE=&amp;quot;nominative&amp;quot;&gt;who himself&lt;/REFEX&gt;
&lt;/ALT-REFEX&gt;
&lt;/REF&gt;
(born June 20, 1929) is a cultural theorist, philosopher, political commentator,
sociologist, and photographer.
&lt;REF ID=&amp;quot;36.2&amp;quot; SEMCAT=&amp;quot;person&amp;quot; SYNCAT=&amp;quot;subj-det&amp;quot;&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;pronoun&amp;quot; CASE=&amp;quot;genitive&amp;quot;&gt;His&lt;/REFEX&gt;
&lt;ALT-REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;name&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;nominal&amp;quot; CASE=&amp;quot;genitive&amp;quot;&gt;Jean Baudrillard’s&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;pronoun&amp;quot; CASE=&amp;quot;genitive&amp;quot;&gt;his&lt;/REFEX&gt;
&lt;REFEX REG08-TYPE=&amp;quot;pronoun&amp;quot; EMPHATIC=&amp;quot;no&amp;quot; HEAD=&amp;quot;rel-pron&amp;quot; CASE=&amp;quot;genitive&amp;quot;&gt;whose&lt;/REFEX&gt;
</table>
<figure confidence="0.9644604">
&lt;/ALT-REFEX&gt;
&lt;/REF&gt;
work is frequently associated with postmodernism and post-structuralism.
&lt;/PARAGRAPH&gt;
&lt;/TEXT&gt;
</figure>
<figureCaption confidence="0.999985">
Figure 1: Example text from the GREC-MSR Training Data.
</figureCaption>
<bodyText confidence="0.99991725">
which collects all the (manually annotated) MSREs
in a text including the title, and adds several de-
faults: pronouns and reflexive pronouns in all sub-
domains; and category nouns (e.g. the river), in
all subdomains except people. The main objec-
tive in the 2009 GREC-MSR Task was to get the
word strings contained in REFEXs right (whereas
in REG’08 it was the REG08-TYPE attributes).
</bodyText>
<sectionHeader confidence="0.783705" genericHeader="method">
3 Test Data
</sectionHeader>
<listItem confidence="0.988907666666667">
1. Test Set C-1: a randomly selected 10% sub-
set (183 texts) of the GREC corpus (with the same
proportions of texts in the 5 subdomains as in the
training/testing data).
2. Test Set C-2: the same subset of texts as in C-
1; however, for C-2 we did not use the MSREs in
</listItem>
<bodyText confidence="0.992071769230769">
the corpus, but replaced them with human-selected
alternatives. These were obtained in an online ex-
periment as described in Belz &amp; Varges (2007a)
where subjects selected MSREs in a setting that du-
plicated the conditions in which the participating
systems in the GREC-MSR Task make selections.4
We obtained three versions of each text, where in
each version all MSREs were selected by the same
person. The motivation for this version of Test Set
C was that having several human-produced chains
of MSREs to compare the outputs of participating
(‘peer’) systems against is more reliable than hav-
ing one only; and that Wikipedia texts are edited
</bodyText>
<footnote confidence="0.9247955">
4The experiment can be tried out here: http://www.nltg.
brighton.ac.uk/home/Anja.Belz/TESTDRIVE/
</footnote>
<bodyText confidence="0.75081875">
by multiple authors which sometimes adversely
affects MSR chains; we wanted to have additional
reference texts where all references are selected by
a single author.
</bodyText>
<listItem confidence="0.998812">
3. Test Set L: 74 Wikipedia introductory texts
from the subdomain of lakes (there were no lake
texts in the training/development set).
4. Test Set P: 31 short encyclopaedic texts in
</listItem>
<bodyText confidence="0.975102954545455">
the same 5 subdomains as in the GREC corpus,
in approximately the same proportions as in the
training/testing data, but of different origin. We
transcribed these texts from printed encyclopae-
dias published in the 1980s which are not avail-
able in electronic form. The texts in this set are
much shorter and more homogeneous than the
Wikipedia texts, and the sequences of MSRs fol-
low very similar patterns. It seems likely that it is
these properties that have resulted in better scores
overall for Test Set P than for the other test sets
in both the 2008 and 2009 runs of the GREC-MSR
task (for the latter, see Section 6).
Each test set was designed to test peer systems
for generalisation to different kinds of unseen data.
Test Set C tests for generalisation to unseen ma-
terial from the same corpus and the same subdo-
mains as the training set; Test Set L tests for gen-
eralisation to unseen material from the same cor-
pus but different subdomain; and Test Set P for
generalisation to a different corpus but the same
subdomains.
</bodyText>
<page confidence="0.997313">
81
</page>
<sectionHeader confidence="0.999953" genericHeader="method">
4 Evaluation methods
</sectionHeader>
<subsectionHeader confidence="0.997642">
4.1 Automatic intrinsic evaluations5
</subsectionHeader>
<bodyText confidence="0.984806">
Accuracy of REFEX word strings: when com-
puted against test sets (C-1, L and P), Word String
Accuracy is simply the proportion of REFEX word
strings selected by a participating system that are
identical to the one in the corpus. When computed
against test set C-2, which has three versions of
each text, Word String Accuracy is computed as
follows: first the number of correct REFEX word
strings is computed at the text level for each of the
three versions of a text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total number
of REFs in all the texts, which gives the global
Word String Accuracy score. The rationale be-
hind computing the Word String Accuracy scores
in this way for multiple-RE test sets (maximising
scores on RE chains rather than individual REs) is
that an RE is not good or bad in its own right, but
depends on other MSREs in the same text.
Accuracy of REG08-Type: similarly to Word
String Accuracy above, when computed against
test sets C-1, L and P, REG08-Type Accuracy is the
proportion of REFEXs selected by a participating
system that have a REG08-TYPE value identical to
the one in the corpus. When computed against test
set C-2, first the number of correct REG08-Types is
computed at the text level for each of the three ver-
sions of a corpus text and the maximum of these
is determined; then the maximum text-level num-
bers are summed and divided by the total num-
ber of REFs in all the texts, which gives the global
REG08-Type Accuracy score.
String-edit distance metrics: String-edit dis-
tance (SE) is straightforward Levenshtein distance
with a substitution cost of 2 and insertion/deletion
cost of 1. We also used a length-normalised ver-
sion of string-edit distance (denoted ‘norm. SE’ in
results tables below). For test sets C-1, L and P,
the global score is simply the mean of all RE-level
scores. For Test Set C-2, the global score is the
mean of the mean of the three text-level scores.
Other metrics: BLEU is a precision metric
from machine translation that assesses peer trans-
lations in terms of the proportion of word n-grams
5For GREC-MSR’09 we updated the tool that computes all
automatic intrinsic scores and in the course of this eliminated
a character encoding issue; as a result the results for baseline
systems and corpus texts reported here are on the whole very
slightly higher than those reported for GREC-MSR’08.
(n ≤ 4 is standard) they share with several ref-
erence translations. We used BLEU-3 rather than
the more standard BLEU-4 because most REs in
the corpus are less than 4 tokens long. We also
used the NIST version of BLEU which weights in
favour of less frequent n-grams. In both cases,
we assessed just the MSREs selected by peer sys-
tems (leaving out the surrounding text), and com-
puted scores globally (rather than averaging over
RE-level scores), as this is standard for these met-
rics. BLEU, and NIST are designed to work with
one or multiple reference texts, so we did not need
to use a different method for Test Set C-2.
</bodyText>
<subsectionHeader confidence="0.9819">
4.2 Automatic extrinsic evaluation
</subsectionHeader>
<bodyText confidence="0.999988615384615">
As in GREC-MSR’08, we used an automatic ex-
trinsic evaluation method based on coreference
resolution performance.6 The basic idea is that it
seems likely that badly chosen reference chains af-
fect the ability to resolve REs in automatic coref-
erence resolution tools which will tend to perform
worse with poorly selected MSR reference chains.
To counteract the possibility of results being a
function of a specific coreference resolution algo-
rithm or tool, we used two different resolvers—
those included in LingPipe7 and OpenNLP (Mor-
ton, 2005)—and averaged results.
There does not appear to be a single standard
evaluation metric in the coreference resolution
community, so we opted to use three: MUC-6
(Vilain et al., 1995), CEAF (Luo, 2005), and B-
CUBED (Bagga and Baldwin, 1998), which seem
to be the most widely accepted metrics. All three
metrics compute Recall, Precision and F-Scores
on aligned gold-standard and resolver-tool coref-
erence chains. They differ in how the alignment
is obtained and what components of coreference
chains are counted for calculating scores. Results
for the automatic extrinsic evaluations are reported
below in terms of the F-Scores from these three
metrics, as well as in terms of their mean.
</bodyText>
<subsectionHeader confidence="0.991918">
4.3 Human intrinsic evaluation
</subsectionHeader>
<bodyText confidence="0.999985333333333">
The intrinsic human evaluation involved 24 ran-
domly selected items from Test Set C and outputs
for these produced by peer and basline systems as
</bodyText>
<footnote confidence="0.57912">
6However, for GREC’09 we overhauled the tool; the cur-
rent version no longer uses JavaRAP, and uses the most recent
versions of the other resolvers; the GREC-MSR’08 and GREC-
MSR’09 results for this method are not entirely comparable
for this reason.
7http://alias-i.com/lingpipe/
</footnote>
<page confidence="0.996315">
82
</page>
<figureCaption confidence="0.999831">
Figure 2: Example of text presented in human intrinsic evaluation of GREC-MSR systems.
</figureCaption>
<bodyText confidence="0.999625">
well as those found in the original corpus texts
(8 systems in total). We used a Repeated Latin
Squares design which ensures that each subject
sees the same number of outputs from each sys-
tem and for each test set item. There were three
8x8 squares, and a total of 576 individual judg-
ments in this evaluation (72 per system: 3 criteria
x 3 articles x 8 evaluators).
We recruited 8 native speakers of English from
among post-graduate students currently doing a
linguistics-related degree at University College
London (UCL) and University of Sussex.
Following detailed instructions, subjects did
two practice examples, followed by the 24 texts
to be evaluated, in random order. Subjects carried
out the evaluation over the internet, at a time and
place of their choosing. They were allowed to in-
terrupt and resume the experiment (though discor-
ouged from doing so). According to self-reported
timings, subjects took between 25 and 45 minutes
to complete the evaluation (not counting breaks).
Figure 2 shows what subjects saw during the
evaluation of an individual text. All references to
the MS are highlighted in yellow, and the task is to
evaluate the quality of the REs in terms of three cri-
teria which were explained in the introduction as
follows (the wording of the explanations of Crite-
ria 1 and 3 were taken from the DUC evaluations):
</bodyText>
<listItem confidence="0.852662">
1. Referential Clarity: It should be easy to identify who
</listItem>
<bodyText confidence="0.8372602">
or what the referring expressions in the text are refer-
ring to. If a person or other entity is mentioned, it
should be clear what their role in the story is. So, a ref-
erence would be unclear if an entity is referenced, but
their identity or relation to the story remains unclear.
</bodyText>
<listItem confidence="0.998710166666667">
2. Fluency: A referring expression should ‘read well’, i.e.
it should be written in good, clear English, and the use
of titles and names etc. should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
3. Structure and Coherence: The text should be well
structured and well organised. The text should not just
be a heap of related information, but should build from
sentence to sentence to a coherent body of information
about a topic. This criterion too is independent of the
others.
</listItem>
<bodyText confidence="0.99833625">
Subjects selected evaluation scores by moving
sliders (see Figure 2) along scales ranging from 1
to 5. Slider pointers started out in the middle of
the scale (3). These were continuous scales and
we recorded scores with one decimal place (e.g.
3.2). The meaning of the numbers was explained
in terms of integer scores (1=very poor, 2=poor,
3=neither poor nor good, 4=good, 5=very good).
</bodyText>
<sectionHeader confidence="0.998055" genericHeader="method">
5 Systems
</sectionHeader>
<bodyText confidence="0.986505333333333">
Base-rand, Base-freq, Base-1st, Base-name:
Baseline system Base-rand selects one of the
REFEXs at random. Base-freq selects the REFEX
that is the overall most frequent given the SYNCAT
and SEMCAT of the reference. Base-1st al-
ways selects the REFEX which appears first in
the ALT-REFEX list; and Base-name selects the
shortest REFEX with attributes REG08-TYPE=name,
HEAD=nominal and EMPHATIC=no.8
</bodyText>
<footnote confidence="0.779611666666667">
8Attributes are considered in this order. If for one at-
tribute, the right value is not found, the process ignores that
attribute and moves on the next one.
</footnote>
<page confidence="0.999053">
83
</page>
<bodyText confidence="0.9999546">
UDel: The UDel system consists of a prepro-
cessing component performing sentence segmen-
tation and identification of non-referring occur-
rences of main subject (MS) names, an RE type
selection component (two C5.0 decision trees, one
optimised for people and mountains, the other for
the other subdomains), and a word string selec-
tion component. The RE type selection decision
trees use the following features: is the MS the sub-
ject of the current, preceding and preceding but
one sentence; was the last MSR in subject position;
are there interfering references to other entities be-
tween the current and the previous MSR; distance
to preceding non-referring occurrences of an MS
name; sentence and reference IDs; other features
indicating whether the reference occurred before
and after certain words and punctuation marks.
Given a selected RE type, the word-string selec-
tion component selects the longest non-emphatic
name for the first named reference in an article,
and the shortest for subsequent named references;
for other types, the first matching word-string is
used, backing off to pronoun or name.
ICSI-CRF: The ICSI-CRF system construes the
GREC-MSR task as a sequence labelling task and
determines the most likely current label given pre-
ceding labels using a Conditional Random Field
model trained using the follow features for the cur-
rent, preceding and preceding but one MSR: pre-
ceding and following word unigram and bigram;
suffix of preceding and following word; preceding
and following punctuation; reference ID; is this is
the beginning of a paragraph. If more than one la-
bel remains, the last in the list of possible REs in
the GREC-MSR data is selected.
JUNLG: The JUNLG system is based on co-
occurrence statistics between REF feature sets and
REFEX feature sets as found in the GREC-MSR data.
REF feature sets were augmented by a paragraph
counter and a within-paragraph REF counter. For
each given set of REF features, the system selects
the most frequent REFEX feature set (as determined
from co-occurrence counts in the training data). If
the current set of possible REFEXs does not include
a REFEX with the selected feature set, then the sec-
ond most likely feature set is selected. Several
hand-coded default rules override the frequency-
based selections, e.g. if the preceding word is a
conjunction, and the current SYNCAT is np-subj,
then the REG08-Type is empty.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.998454333333333">
This section presents the results of all evalua-
tion methods described in Section 4. We start
with Word String Accuracy, the intrinsic auto-
matic metric which participating teams were told
was going to be the chief evaluation method, fol-
lowed by REG08-Type Accuracy and other intrin-
sic automatic metrics (Section 6.2), the intrinsic
human evaluation (Section 6.3) and the extrinsic
automatic evaluation (Section 6.4).
</bodyText>
<table confidence="0.99676575">
System Word String Acc. REG08-Type Acc. Norm. Edit Dist.
ICSI-CRF 0.67 0.75 0.28
UDel 0.6357 0.7027 0.3383
JUNLG 0.532 0.62 0.421
</table>
<tableCaption confidence="0.993378">
Table 2: Self-reported evaluation scores for devel-
opment set.
</tableCaption>
<subsectionHeader confidence="0.997789">
6.1 Word String Accuracy
</subsectionHeader>
<bodyText confidence="0.999837322580645">
Participants computed Word String Accuracy for
the development set (97 texts) themselves, using
an evaluation tool provided by us. These scores
are shown in column 2 of Table 2, and are also
included in the participants’ reports in this vol-
ume. Corresponding results for test set C-1 are
shown in column 2 of Table 3. Surprisingly, Word
String Accuracy results on the test data are better
(than on the development data) for the UDel and
JUNLG systems. Also included in this table are re-
sults for the four baseline systems, and it is clear
that selecting the most frequent word string given
SEMCAT and SYNCAT (as done by the Base-freq sys-
tem) provides a strong baseline.
The other two parts of Table 3 contain results for
test sets L and P. As expected, results for Test Set L
are lower than for Test Set C-1, because in addition
to consisting of unseen texts (like C-1), Test Set L
is also from an unseen subdomain (unlike C-1).
The Word String Accuracy results for Test Set P
are higher than for any other set, probably for the
reasons discussed at the end of Section 3.
For each test set in Table 3 we carried out a
univariate ANOVA with System as the fixed factor,
‘Number of REFEXs in a text’ as a random factor,
and Word String Accuracy as the dependent vari-
able. We found significant main effects of Sys-
tem on Word String Accuracy at p &lt; .001 in the
case of all three test sets (C-1: F(7,1272) = 90.058;
L: F(7,440) = 44.139; P: F(7,168) = 21.991).9
The columns containing capital letters in Table 3
</bodyText>
<footnote confidence="0.9971865">
9We included the corpus texts themselves in the analysis,
hence 7 degrees of freedom (8 systems).
</footnote>
<page confidence="0.996192">
84
</page>
<table confidence="0.999737">
Test Set C-1 Test Set L Test Set P
UDel 67.68 A UDel 52.89 A UDel 77.16 A
ICSI-CRF 62.98 A JUNLG 50.80 A ICSI-CRF 72.22 A
JUNLG 61.94 A ICSI-CRF 49.20 A JUNLG 71.60 A
Base-freq 47.05 B Base-name 21.06 B Base-freq 53.09 B
Base-name 28.74 C Base-freq 20.74 B Base-name 27.78 C
Base-1st 28.26 C Base-1st 20.74 B Base-1st 27.16 C
Base-rand 18.95 D Base-rand 15.11 B Base-rand 18.52 C
</table>
<tableCaption confidence="0.746274">
Table 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,
alpha = .05) for each test set (systems that do not share a letter are significantly different).
</tableCaption>
<table confidence="0.9997983">
System Word String Accuracy for multiple-RE Test Set C-2
All Cities Countries Rivers People Mountains
Corpus 71.58 A 65.25 69.11 76.47 80.40 66.87
UDel 70.22 A B 68.09 71.20 76.47 76.63 64.84
JUNLG 64.57 B C 54.61 51.83 73.53 71.86 65.85
ICSI-CRF 63.69 C 58.87 56.54 64.71 72.11 60.98
Base-freq 57.01 D 51.06 57.07 58.82 63.82 53.05
Base-name 40.21 E 51.06 46.07 29.41 29.90 43.90
Base-1st 39.65 E 47.52 41.88 38.24 25.63 47.97
Base-rand 26.99 F 28.37 29.32 23.53 21.61 30.28
</table>
<tableCaption confidence="0.892665333333333">
Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo-
geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter are
significantly different).
</tableCaption>
<bodyText confidence="0.999902785714286">
show the homogeneous subsets of systems as de-
termined by post-hoc Tukey HSD comparisons of
means. Systems whose Word String Accuracy
scores are not significantly different (at the .05
level) share a letter.
The results for Word String Accuracy com-
puted against Test Set C-2 are shown in Table 4.
These should be considered the chief results of the
GREC-MSR’09 Task evaluations, as stated in the
participants’ guidelines. Here too we performed
a univariate ANOVA with System as the fixed fac-
tor, Number of REFEXs as the random factor and
Word String Accuracy as the dependent variable.
There was a significant main effect of System
(F(7,1272) = 74.892, p &lt; .001). We compared the
mean scores with Tukey’s HSD. As can be seen
from the resulting homogeneous subsets, there is
no significant difference between the corpus texts
(C-1) and the UDel system, but also there is no
significant difference between the latter and the
JUNLG system. In this analysis, all peer systems
outperform all baselines; the Base-freq baseline
outperforms all other baselines; and Base-name
and Base-1st outperform the random baseline.
Overall, there is a marked improvement in Word
String Accuracy compared to GREC-MSR’08
where peer systems’ scores ranged from 50.72 to
65.61.
</bodyText>
<subsectionHeader confidence="0.999556">
6.2 Other automatic intrinsic metrics
</subsectionHeader>
<bodyText confidence="0.999642952380953">
In addition to the chief evaluation measure re-
ported on in the preceding section, we computed
REG08-Type Accuracy and the string similarity
metrics described in Section 4.1. The resulting
scores for Test Set C-2 are shown in Table 5 (re-
call that in Test Set C-2 corpus texts are evalu-
ated against 3 texts with human-selected alterna-
tive REs). The corpus texts again receive the best
scores across the board. Ranks for peer systems
are very similar to those reported in the last sec-
tion.
We performed a univariate ANOVA with Sys-
tem as the fixed factor, Number of REFEXs as the
random factor, and REG08-Type Accuracy as the
dependent variable. The main effect of System
was F(7,1272) = 75.040, p &lt; .001; the homoge-
neous subsets resulting from the Tukey HSD post-
hoc analysis are shown in columns 3–5 of Table 5.
The differences between the scores of the peer sys-
tems and the corpus texts were not found to be sig-
nificant.
</bodyText>
<subsectionHeader confidence="0.994696">
6.3 Human-assessed intrinsic measures
</subsectionHeader>
<bodyText confidence="0.999512727272727">
Table 6 shows the results of the human intrinsic
evaluation. In each of the three parts of the ta-
ble (showing the results for Fluency, Clarity and
Coherence, respectively) systems are ordered in
terms of their mean scores (shown in the second
column of each part of the table). We first es-
tablished that the main effect of Evaluator was
weak (F between 2.1 and 2.6) on Fluency, Clar-
ity and Coherence, and only of borderline signifi-
cance (just below .05); and that the interaction be-
tween System and Evaluator was very weak and
</bodyText>
<page confidence="0.998998">
85
</page>
<table confidence="0.9991399">
System Other similarity measures for Triple-RE Test Set C-2
REG08-Type BLEU-3 NIST SE norm. SE
Corpus 79.30 A 0.77 5.60 1.04 0.34
UDel 77.71 A 0.74 5.32 1.11 0.37
JUNLG 75.40 A 0.53 4.69 1.34 0.40
ICSI-CRF 75.16 A 0.54 4.68 1.32 0.41
Base-freq 62.50 B 0.54 4.30 1.93 0.50
Base-name 51.04 C 0.46 4.76 1.80 0.63
Base-1st 50.32 C 0.39 4.42 1.93 0.63
Base-rand 48.09 C 0.26 3.02 2.30 0.72
</table>
<tableCaption confidence="0.989401333333333">
Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systems
in order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-Type
Accuracy only (systems that do not share a letter are significantly different).
</tableCaption>
<table confidence="0.999887777777778">
Fluency Clarity Coherence
Corpus 4.43 A Base-name 4.62 A Corpus 4.40 A
UDel 4.27 A Corpus 4.56 A JUNLG 4.33 A
JUNLG 4.26 A JUNLG 4.50 A UDel 4.27 A B
ICSI-CRF 4.15 A B ICSI-CRF 4.45 A ICSI-CRF 4.02 A B
Base-freq 3.33 B C UDel 4.35 A Base-freq 3.96 A B
Base-name 2.84 C D Base-1st 4.27 A Base-name 3.85 A B
Base-1st 2.76 C D Base-freq 4.10 A Base-1st 3.7 A B
Base-rand 2.15 D Base-rand 3.18 B Base-rand 3.46 B
</table>
<tableCaption confidence="0.998134">
Table 6: Clarity, Fluency and Coherence scores (with homogeneous subsets) for all systems.
</tableCaption>
<bodyText confidence="0.999857617021277">
not significant in the case of Clarity and Coher-
ence, and borderline significant in the case of Flu-
ency. We then ran a (non-factorial) multivariate
ANOVA, with Fluency, Coherence and Clarity as
the dependent variables, and (just) System as the
fixed factor. The main effect of System was as
follows: Fluency: F(7,128) = 20.444,p &lt; 0.001;
Clarity: F(7,128) = 5.248,p &lt; 0.001; Coherence:
F(7,128) = 2.680,p &lt; 0.012. The homogeneous
subsets resulting from a post-hoc Tukey analysis
are shown in the letter columns in Table 6.
The effect of System was strongest on Fluency;
here, the system ranks are also the same as for
Word String Accuracy and REG08-Type Accuracy
for Test Set C-2. This, together with the fair
amount of significant differences found, indicates
that the evaluators were able to make sense of the
Fluency criterion and that there were interesting
differences between systems under this criterion.
However, differences between the three peer sys-
tems were not significant.
For Clarity, there were no significant differ-
ences among the peer systems and non-random
baseline systems; all of these were significantly
better than the random baseline. Base-name had
the highest mean Clarity score, possibly because
always chosing the name of an entity when refer-
ring to it ensures high referential clarity.
The Coherence results are perhaps the most dif-
ficult to interpret. Both the main effect of System
on Coherence and its significance were weaker
than for Fluency and Clarity. Only two signifi-
cant pairwise differences were found: Corpus and
JUNLG were better than the random baseline. The
system ranks are roughly the same as for Fluency,
but the mean scores cover a smaller range (from
3.46 to 4.4) than in the case of either of the other
two criteria. Overall, the Coherence results proba-
bly indicate that the evaluators found it somewhat
difficult to make sense of the Coherence criterion.
Computing Pearson’s r for the three criteria
on individual (text-level) scores showed that there
were only moderate correlations between them (all
around r = 0.5) which were all significant at
α = 0.05. This gives some indication that the
evaluators were able to assess the three criteria in-
dependently from each other.
</bodyText>
<subsectionHeader confidence="0.98863">
6.4 Automatic extrinsic measures
</subsectionHeader>
<bodyText confidence="0.999989705882353">
We fed the outputs of all eight systems through
the two coreference resolvers, and computed mean
MUC, CEAF and B-CUBED F-Scores as described
in Section 4.2. The second column in Table 7
shows the mean of these three F-Scores, to give
a single overall result for this evaluation method.
A univariate ANOVA with mean F-Score as the de-
pendent variable and System as the fixed factor
revealed a significant main effect of System on
mean F-Score (F(7,1456) = 73.061,p &lt; .001).
A post-hoc comparison of the means (Tukey HSD,
alpha = .05) found the significant differences in-
dicated by the homogeneous subsets in columns
3–4 (Table 7). The numbers shown in the last
three columns are the separate MUC, CEAF and B-
CUBED F-Scores for each system, averaged over
the two resolver tools. ANOVAs revealed the fol-
</bodyText>
<page confidence="0.986556">
86
</page>
<bodyText confidence="0.976097">
lowing effects of System on the separate scoring
methods: on CEAF F(7,1456) = 43.471,p &lt; .001;
on MUC: F(7,1456) =,p &lt; .001; on B-CUBED:
F(7,1456) = 38.574,p &lt; .001. All three scor-
ing methods separately and their mean yielded the
same significant differences (as shown in columns
3–4 of Table 7).
The three F-Score measures (MUC, CEAF and B-
CUBED) are all significantly correlated (p &lt; .001,
2-tailed). However it is not a strong correlation,
with Pearson’s correlation coefficient around 0.5.
</bodyText>
<table confidence="0.992569">
System (MUC+CEAF+B3)/3 MUC CEAF B3
Base-name 65.19 A 62.35 63.14 70.06
Base-1st 63.77 A 59.95 62.08 69.28
Base-freq 63.14 A 59.08 62.04 68.3
UDel 46.19 B 34.85 46.86 56.86
ICSI-CRF 44.47 B 31.61 45.58 56.21
JUNLG 44.19 B 31.27 45.21 56.10
Base-rand 42.99 B 30.24 43.04 55.7
Corpus 42.52 B 29.53 43.57 54.47
</table>
<tableCaption confidence="0.671779666666667">
Table 7: MUC, CEAF and B-CUBED F-Scores for
all systems; homogeneous subsets (Tukey HSD),
alpha = .05, for mean of F-Scores.
</tableCaption>
<subsectionHeader confidence="0.912385">
6.5 Correlations
</subsectionHeader>
<bodyText confidence="0.999934214285714">
When assessed on the system-level scores and us-
ing Pearson’s r, all evaluation methods above were
strongly and significantly correlated with each
other (at the 0.01 level, 2-tailed), with the fol-
lowing exceptions. Clarity was not significantly
correlated with any of the other methods except
NIST (r = .902,p &lt; .01); apart from this, NIST
was only correlated with Word String Accuracy on
test set C-2, with non-normalised string-edit dis-
tance, Fluency and Coherence, moreover all at the
weaker 0.05 level. Finally, the extrinsic method
was not correlated with any of the intrinsic meth-
ods (and in fact showed signs of being negatively
correlated with all of them except Clarity).
</bodyText>
<sectionHeader confidence="0.987878" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999944833333334">
The GREC-MSR Task is still a relatively new task
not only for an NLG shared-task challenge, but also
as a research task in general (post-processing ex-
tractive summaries in order to improve their qual-
ity seems to be just taking off as a research sub-
field). There was substantial interest in the GREC-
MSR Task this year (as indicated by the nine teams
that originally registered). However, only three
teams were ultimately able to participate.
We continued the traditions of previous NLG
shared tasks in that we used a wide range of eval-
uation metrics to obtain a well-rounded view of
the quality of the participating systems. This in-
cluded intrinsic human evaluations for the first
time. However, we decided against an extrinsic
human evaluation this year, given time constraints
as well as the fact that this evaluation type yielded
barely any significant results last year.
Overall, there was an improvement in system
performance compared to last year, to the point
where the performance of the top system was
barely distinguishable from the human topline.
We are not currently planning to run the GREC-
MSR task again next year.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999704">
Many thanks to the UCL and Sussex students who
participated in the intrinsic evaluation experiment.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599">
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC’98, pages
563–566.
A. Belz and S. Varges. 2007a. Generation of repeated
references to discourse entities. In Proceedings of
ENLG’07, pages 9–16.
A. Belz and S. Varges. 2007b. The GREC corpus:
Main subject reference in context. Technical Report
NLTG-07-01, University of Brighton.
R. Huddleston and G. Pullum. 2002. The Cambridge
Grammar ofthe English Language. Cambridge Uni-
versity Press.
X. Luo. 2005. On coreference resolution performance
metrics. Proc. ofHLT-EMNLP, pages 25–32.
T. Morton. 2005. Using Semantic Relations to Improve
Information Retrieval. Ph.D. thesis, University of
Pensylvania.
A. Nenkova. 2008. Entity-driven rewrite for multi-
document summarization. In Proceedings of IJC-
NLP’08.
L. Qiu, M. Kan, and T.-S. Chua. 2004. A public refer-
ence implementation of the rap anaphora resolution
algorithm. In Proceedings of LREC’04, pages 291–
294.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management:
Special issue on Summarization, 43(6):1663–1680.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. Proceedings ofMUC-6, pages
45–52.
</reference>
<page confidence="0.999472">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.168262">
<title confidence="0.9991395">The GREC Main Subject Reference Generation Challenge Overview and Evaluation Results</title>
<author confidence="0.999311">Anja Belz Eric Kow</author>
<affiliation confidence="0.913032">NLT University of Brighton BN2 4GJ, Jette Centre for</affiliation>
<title confidence="0.908832">Macquarie</title>
<author confidence="0.996193">Sydney NSW</author>
<email confidence="0.983166">jviethen@ics.mq.edu.au</email>
<author confidence="0.541925">Albert</author>
<affiliation confidence="0.985461">Computing University of</affiliation>
<address confidence="0.47677">Aberdeen AB24 3UE,</address>
<email confidence="0.983359">a.gatt@abdn.ac.uk</email>
<abstract confidence="0.999815631578947">at Generation Challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted one system each, and we additionally created four baseline systems. Systems were tested automatically using existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This describes the and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at LREC’98,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="17068" citStr="Bagga and Baldwin, 1998" startWordPosition="2678" endWordPosition="2681">hat badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (Vilain et al., 1995), CEAF (Luo, 2005), and BCUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected items from Test Set C and outputs for these produced by pee</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at LREC’98, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>S Varges</author>
</authors>
<title>Generation of repeated references to discourse entities.</title>
<date>2007</date>
<booktitle>In Proceedings of ENLG’07,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2472" citStr="Belz and Varges, 2007" startWordPosition="386" endWordPosition="389">mmediate motivating application context for the GREC-MSR Task is the improvement of referential clarity and coherence in extractive summaries by regenerating referring expressions in them. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GRECMSR Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC-MSR data is an extension of the GREC 1.0 Corpus which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). For the purpose of the GREC-MSR shared task, an additional 1,000 texts in the new subdomain of mountain texts were obtained and a new XML annotation scheme (Section 2.2) was developed. Team System Name University of Delaware UDel ICSI, Berkeley ICSI-CRF Jadavpur University JUNLG Table 1: GREC-MSR’09 participating teams. Nine teams from seven countries registered for GREC-MSR’09, of which three teams (Table 1) submitted one system each.1 Participants had to submit their system reports before downloading test data inputs, and had to submit test data outputs within 48 hours of downloading the</context>
<context position="5292" citStr="Belz and Varges (2007" startWordPosition="836" endWordPosition="839">ed in the GREC corpus3 — subject NPs, object NPs, and genitive NPs and pronouns which function as subjectdeterminers within their matrix NP. These categories of referring expressions (RE) are relatively straightforward to identify and to achieve high inter-annotator agreement on (complete agreement among four annotators in 86% of MSRs), and account for most cases of overt main subject reference in the GREC texts. The annotators were asked to identify subject, object and genitive subjectdeterminers and decide whether or not they refer to the main subject of the text. More detail is provided in Belz and Varges (2007b). In addition to the above, relative pronouns in supplementary relative clauses (as opposed to integrated relative clauses, Huddleston and Pullum, 2002, p. 1058) were annotated, e.g.: (1) Stoichkov is a football manager and former striker who was a member of the Bulgaria national team that finishedfourth at the 1994 FIFA World Cup. We also annotated ‘non-realised’ subject MSREs in those cases of VP coordination where an MSRE is the subject of the coordinated VPs, e.g.: (2) He stated the first version of the Law of conservation of mass, introduced the Metric system, and helped to reform chemi</context>
<context position="11185" citStr="Belz &amp; Varges (2007" startWordPosition="1681" endWordPosition="1684">the river), in all subdomains except people. The main objective in the 2009 GREC-MSR Task was to get the word strings contained in REFEXs right (whereas in REG’08 it was the REG08-TYPE attributes). 3 Test Data 1. Test Set C-1: a randomly selected 10% subset (183 texts) of the GREC corpus (with the same proportions of texts in the 5 subdomains as in the training/testing data). 2. Test Set C-2: the same subset of texts as in C1; however, for C-2 we did not use the MSREs in the corpus, but replaced them with human-selected alternatives. These were obtained in an online experiment as described in Belz &amp; Varges (2007a) where subjects selected MSREs in a setting that duplicated the conditions in which the participating systems in the GREC-MSR Task make selections.4 We obtained three versions of each text, where in each version all MSREs were selected by the same person. The motivation for this version of Test Set C was that having several human-produced chains of MSREs to compare the outputs of participating (‘peer’) systems against is more reliable than having one only; and that Wikipedia texts are edited 4The experiment can be tried out here: http://www.nltg. brighton.ac.uk/home/Anja.Belz/TESTDRIVE/ by m</context>
</contexts>
<marker>Belz, Varges, 2007</marker>
<rawString>A. Belz and S. Varges. 2007a. Generation of repeated references to discourse entities. In Proceedings of ENLG’07, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>S Varges</author>
</authors>
<title>The GREC corpus: Main subject reference in context.</title>
<date>2007</date>
<tech>Technical Report NLTG-07-01,</tech>
<institution>University of Brighton.</institution>
<contexts>
<context position="2472" citStr="Belz and Varges, 2007" startWordPosition="386" endWordPosition="389">mmediate motivating application context for the GREC-MSR Task is the improvement of referential clarity and coherence in extractive summaries by regenerating referring expressions in them. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GRECMSR Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC-MSR data is an extension of the GREC 1.0 Corpus which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). For the purpose of the GREC-MSR shared task, an additional 1,000 texts in the new subdomain of mountain texts were obtained and a new XML annotation scheme (Section 2.2) was developed. Team System Name University of Delaware UDel ICSI, Berkeley ICSI-CRF Jadavpur University JUNLG Table 1: GREC-MSR’09 participating teams. Nine teams from seven countries registered for GREC-MSR’09, of which three teams (Table 1) submitted one system each.1 Participants had to submit their system reports before downloading test data inputs, and had to submit test data outputs within 48 hours of downloading the</context>
<context position="5292" citStr="Belz and Varges (2007" startWordPosition="836" endWordPosition="839">ed in the GREC corpus3 — subject NPs, object NPs, and genitive NPs and pronouns which function as subjectdeterminers within their matrix NP. These categories of referring expressions (RE) are relatively straightforward to identify and to achieve high inter-annotator agreement on (complete agreement among four annotators in 86% of MSRs), and account for most cases of overt main subject reference in the GREC texts. The annotators were asked to identify subject, object and genitive subjectdeterminers and decide whether or not they refer to the main subject of the text. More detail is provided in Belz and Varges (2007b). In addition to the above, relative pronouns in supplementary relative clauses (as opposed to integrated relative clauses, Huddleston and Pullum, 2002, p. 1058) were annotated, e.g.: (1) Stoichkov is a football manager and former striker who was a member of the Bulgaria national team that finishedfourth at the 1994 FIFA World Cup. We also annotated ‘non-realised’ subject MSREs in those cases of VP coordination where an MSRE is the subject of the coordinated VPs, e.g.: (2) He stated the first version of the Law of conservation of mass, introduced the Metric system, and helped to reform chemi</context>
<context position="11185" citStr="Belz &amp; Varges (2007" startWordPosition="1681" endWordPosition="1684">the river), in all subdomains except people. The main objective in the 2009 GREC-MSR Task was to get the word strings contained in REFEXs right (whereas in REG’08 it was the REG08-TYPE attributes). 3 Test Data 1. Test Set C-1: a randomly selected 10% subset (183 texts) of the GREC corpus (with the same proportions of texts in the 5 subdomains as in the training/testing data). 2. Test Set C-2: the same subset of texts as in C1; however, for C-2 we did not use the MSREs in the corpus, but replaced them with human-selected alternatives. These were obtained in an online experiment as described in Belz &amp; Varges (2007a) where subjects selected MSREs in a setting that duplicated the conditions in which the participating systems in the GREC-MSR Task make selections.4 We obtained three versions of each text, where in each version all MSREs were selected by the same person. The motivation for this version of Test Set C was that having several human-produced chains of MSREs to compare the outputs of participating (‘peer’) systems against is more reliable than having one only; and that Wikipedia texts are edited 4The experiment can be tried out here: http://www.nltg. brighton.ac.uk/home/Anja.Belz/TESTDRIVE/ by m</context>
</contexts>
<marker>Belz, Varges, 2007</marker>
<rawString>A. Belz and S. Varges. 2007b. The GREC corpus: Main subject reference in context. Technical Report NLTG-07-01, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huddleston</author>
<author>G Pullum</author>
</authors>
<title>The Cambridge Grammar ofthe English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5445" citStr="Huddleston and Pullum, 2002" startWordPosition="858" endWordPosition="861">categories of referring expressions (RE) are relatively straightforward to identify and to achieve high inter-annotator agreement on (complete agreement among four annotators in 86% of MSRs), and account for most cases of overt main subject reference in the GREC texts. The annotators were asked to identify subject, object and genitive subjectdeterminers and decide whether or not they refer to the main subject of the text. More detail is provided in Belz and Varges (2007b). In addition to the above, relative pronouns in supplementary relative clauses (as opposed to integrated relative clauses, Huddleston and Pullum, 2002, p. 1058) were annotated, e.g.: (1) Stoichkov is a football manager and former striker who was a member of the Bulgaria national team that finishedfourth at the 1994 FIFA World Cup. We also annotated ‘non-realised’ subject MSREs in those cases of VP coordination where an MSRE is the subject of the coordinated VPs, e.g.: (2) He stated the first version of the Law of conservation of mass, introduced the Metric system, and helped to reform chemical nomenclature. The motivation for annotating the approximate place where the subject NP would be if it were realised (the gap-like underscores above) </context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>R. Huddleston and G. Pullum. 2002. The Cambridge Grammar ofthe English Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>Proc. ofHLT-EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="17030" citStr="Luo, 2005" startWordPosition="2673" endWordPosition="2674">s that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (Vilain et al., 1995), CEAF (Luo, 2005), and BCUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected items from Test Set C</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. Proc. ofHLT-EMNLP, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Morton</author>
</authors>
<title>Using Semantic Relations to Improve Information Retrieval.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pensylvania.</institution>
<contexts>
<context position="16831" citStr="Morton, 2005" startWordPosition="2640" endWordPosition="2642">a different method for Test Set C-2. 4.2 Automatic extrinsic evaluation As in GREC-MSR’08, we used an automatic extrinsic evaluation method based on coreference resolution performance.6 The basic idea is that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (Vilain et al., 1995), CEAF (Luo, 2005), and BCUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below </context>
</contexts>
<marker>Morton, 2005</marker>
<rawString>T. Morton. 2005. Using Semantic Relations to Improve Information Retrieval. Ph.D. thesis, University of Pensylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
</authors>
<title>Entity-driven rewrite for multidocument summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP’08.</booktitle>
<contexts>
<context position="2141" citStr="Nenkova, 2008" startWordPosition="329" endWordPosition="330">8). The task definition was again kept fairly simple, but in the 2009 round the main aim for participating systems was to select an appropriate word string to serve as a referring expression, whereas in 2008 it was to select an appropriate type of referring expression (name, common noun, pronoun, or empty reference). The immediate motivating application context for the GREC-MSR Task is the improvement of referential clarity and coherence in extractive summaries by regenerating referring expressions in them. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GRECMSR Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC-MSR data is an extension of the GREC 1.0 Corpus which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). For the purpose of the GREC-MSR shared task, an additional 1,000 texts in the new subdomain of mountain texts were obtained and a new XML annotation scheme (Section 2.2) was developed. Team System Name University of Delaware UDel ICSI, Berkeley ICSI-CRF Jadavpur Un</context>
</contexts>
<marker>Nenkova, 2008</marker>
<rawString>A. Nenkova. 2008. Entity-driven rewrite for multidocument summarization. In Proceedings of IJCNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M Kan</author>
<author>T-S Chua</author>
</authors>
<title>A public reference implementation of the rap anaphora resolution algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC’04,</booktitle>
<pages>291--294</pages>
<marker>Qiu, Kan, Chua, 2004</marker>
<rawString>L. Qiu, M. Kan, and T.-S. Chua. 2004. A public reference implementation of the rap anaphora resolution algorithm. In Proceedings of LREC’04, pages 291– 294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Steinberger</author>
<author>M Poesio</author>
<author>M Kabadjov</author>
<author>K Jezek</author>
</authors>
<title>Two uses of anaphora resolution in summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management: Special issue on Summarization,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="2125" citStr="Steinberger et al., 2007" startWordPosition="325" endWordPosition="328">llowing a first run in 2008). The task definition was again kept fairly simple, but in the 2009 round the main aim for participating systems was to select an appropriate word string to serve as a referring expression, whereas in 2008 it was to select an appropriate type of referring expression (name, common noun, pronoun, or empty reference). The immediate motivating application context for the GREC-MSR Task is the improvement of referential clarity and coherence in extractive summaries by regenerating referring expressions in them. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GRECMSR Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC-MSR data is an extension of the GREC 1.0 Corpus which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). For the purpose of the GREC-MSR shared task, an additional 1,000 texts in the new subdomain of mountain texts were obtained and a new XML annotation scheme (Section 2.2) was developed. Team System Name University of Delaware UDel ICSI, Berkeley ICSI</context>
</contexts>
<marker>Steinberger, Poesio, Kabadjov, Jezek, 2007</marker>
<rawString>J. Steinberger, M. Poesio, M. Kabadjov, and K. Jezek. 2007. Two uses of anaphora resolution in summarization. Information Processing and Management: Special issue on Summarization, 43(6):1663–1680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>Proceedings ofMUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="17012" citStr="Vilain et al., 1995" startWordPosition="2668" endWordPosition="2671">rformance.6 The basic idea is that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC-6 (Vilain et al., 1995), CEAF (Luo, 2005), and BCUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected ite</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. Proceedings ofMUC-6, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>