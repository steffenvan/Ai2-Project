<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012062">
<title confidence="0.9592985">
Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation
</title>
<author confidence="0.976806">
Ahmed El Kholy, Nizar Habash
</author>
<affiliation confidence="0.967113">
Center for Computational Learning Systems, Columbia University
</affiliation>
<email confidence="0.996298">
{akholy,habash}@ccls.columbia.edu
</email>
<author confidence="0.991337">
Gregor Leusch, Evgeny Matusov
</author>
<affiliation confidence="0.958572">
Science Applications International Corporation
</affiliation>
<email confidence="0.997686">
{gregor.leusch,evgeny.matusov}@saic.com
</email>
<author confidence="0.964046">
Hassan Sawaf
</author>
<affiliation confidence="0.925768">
eBay Inc.
</affiliation>
<email confidence="0.996162">
hsawaf@ebay.com
</email>
<sectionHeader confidence="0.996627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999578944444445">
An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966137931035">
One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of
this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.
Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999897764705882">
Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al., 2008; Bertoldi et al.,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajiˇc et al., 2000) as well as un-
related languages (Koehn et al., 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.
The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language
</bodyText>
<page confidence="0.969345">
412
</page>
<note confidence="0.4877735">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–418,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.334148">
(Khalilov et al., 2008).
</figureCaption>
<bodyText confidence="0.999320806451613">
The second strategy is based on phrase pivot-
ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.
The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al., 2008).
In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.
Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.
To our knowledge, there hasn’t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and K¨opr¨u, 2010).
</bodyText>
<sectionHeader confidence="0.899219" genericHeader="method">
3 Pivoting Strategies
</sectionHeader>
<bodyText confidence="0.999843">
In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.
</bodyText>
<subsectionHeader confidence="0.999913">
3.1 Sentence Pivoting
</subsectionHeader>
<bodyText confidence="0.999582333333333">
In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.
</bodyText>
<subsectionHeader confidence="0.999765">
3.2 Phrase Pivoting
</subsectionHeader>
<bodyText confidence="0.999956785714286">
In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-
to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.
Since we build our models on top of Moses
phrase-based SMT (Koehn et al., 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (φ) and the
phrase probabilities (pw)
</bodyText>
<equation confidence="0.99847525">
φ(f|a) _ E φ(f|e)φ(e|a)
e
φ(a|f) _ E φ(a|e)φ(e|f)
e
pw(f|a) _ E pw(f|e)pw(e|a)
e
pw(a|f) _ E pw(a|e)pw(e|f)
e
</equation>
<bodyText confidence="0.999967947368421">
where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.
We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al. (2010).
As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2
We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.
</bodyText>
<subsectionHeader confidence="0.998068">
3.3 Filtering for Phrase Pivoting
</subsectionHeader>
<bodyText confidence="0.999993">
The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted
</bodyText>
<footnote confidence="0.9904696">
1Four different phrase translation scores are computed in
Moses’ phrase tables: two lexical weighting scores and two
phrase translation probabilities.
2The size of the induced phrase table size is computed but
not created.
</footnote>
<page confidence="0.994702">
413
</page>
<table confidence="0.999827833333333">
Translation Model Training Corpora Phrase Table
Size
# Phrase Pairs Size
Persian-English ≈4M words 96,04,103 1.1GB
English-Arabic ≈60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ≈2.5TB
</table>
<tableCaption confidence="0.999916">
Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.
</tableCaption>
<bodyText confidence="0.999912875">
Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.
We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.
</bodyText>
<sectionHeader confidence="0.99653" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.99986835483871">
One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.
To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.
Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ≤ i ≤ S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ≤ j ≤ T}
is the set of the equivalent target words. The
word alignment between S and T is defined as
</bodyText>
<equation confidence="0.9884658">
A = {(i,j) : i ∈ S and j ∈ T}.
SCS= |A |(1)
|S|
TCS = |A |(2)
|T |
</equation>
<bodyText confidence="0.999972583333333">
We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.
In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.
Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999853">
In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.
</bodyText>
<footnote confidence="0.938293666666667">
3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al., 2007) in the figures with extensions for
Persian as suggested by Habash (2010).
</footnote>
<page confidence="0.994908">
414
</page>
<figure confidence="0.9670868">
Persian: AStmAd myAn dw kšwr ‘ر,-.ود ن &amp;quot;() د&amp;quot;#$%ا’
‘trust between the two countries’
English: trust between the two countries
Arabic: AlθqFi byn Aldwltyn ‘3$2و52ا 34 /012 ا’
‘the trust between the two countries’
</figure>
<figureCaption confidence="0.9984885">
Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.
</figureCaption>
<figure confidence="0.8236298">
Persian: AyjAd cnd šrkt mštrk ‘ ک+./0 )*+, &amp;&apos;( د&amp;quot;#$ا ’
‘Establish few joint companies’
English: joint ventures
Arabic: bςD šrkAt AlmqAwlAt fy Albld ‘ &amp;&lt;=&gt; ا:; ت6و&amp;quot;89ا ت&amp;quot;5+, 123’
‘Some construcBon companies in the country’
</figure>
<figureCaption confidence="0.983337">
Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.
</figureCaption>
<subsectionHeader confidence="0.949128">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999975476190477">
In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (≈60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.
Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).
All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al., 2007).
We use MERT (Och, 2003) for decoding weight
</bodyText>
<footnote confidence="0.916927428571429">
4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.
5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.
</footnote>
<bodyText confidence="0.999105">
optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).
We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al., 2002) and METEOR
(Lavie and Agarwal, 2007).
</bodyText>
<subsectionHeader confidence="0.998454">
5.2 Linguistic Preprocessing
</subsectionHeader>
<bodyText confidence="0.999309916666667">
In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.
One aspect of Arabic’s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). We follow El
Kholy and Habash (2010a) and use the PATB to-
kenization scheme (Maamouri et al., 2004) in our
</bodyText>
<page confidence="0.998316">
415
</page>
<bodyText confidence="0.999966444444444">
experiments. We use MADA v3.1 (Habash and
Rambow, 2005; Habash et al., 2009) to tokenize
the Arabic text. We only evaluate on detokenized
and orthographically correct (enriched) output fol-
lowing the work of El Kholy and Habash (2010b).
Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix lm+ +hA and sometimes J+ +An,
or one of the Arabic plural markers. Verbal mor-
phology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al., 2013). We use Perstem
(Jadidinejad et al., 2010) for segmenting Persian
text.
English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-
rating punctuation and splitting off “’s”.
</bodyText>
<subsectionHeader confidence="0.991234">
5.3 Baseline Evaluation
</subsectionHeader>
<bodyText confidence="0.999772">
We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.
</bodyText>
<table confidence="0.9996976">
Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6
</table>
<tableCaption confidence="0.786351">
Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).
</tableCaption>
<bodyText confidence="0.99836">
We use the best performing setup across the rest
of the experiments.
</bodyText>
<subsectionHeader confidence="0.96888">
5.4 Connectivity Strength Features
Evaluation
</subsectionHeader>
<bodyText confidence="0.998301666666667">
In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
</bodyText>
<table confidence="0.9472254">
(Phrase Pivot F1K).
Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9
</table>
<tableCaption confidence="0.948449">
Table 3: Connectivity strength features experi-
ment result.
</tableCaption>
<bodyText confidence="0.99900325">
The results in Table 3 show that we get a
nice improvement of ≈0.6/0.5 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).
</bodyText>
<sectionHeader confidence="0.977661" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999980285714286">
We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,
to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.
</bodyText>
<sectionHeader confidence="0.979846" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999752333333333">
The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.
</bodyText>
<page confidence="0.998694">
416
</page>
<sectionHeader confidence="0.881303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985049863247863">
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143–149.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montr´eal, Canada.
Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.
Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69–77, Athens, Greece, March.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-
glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173–181, Athens, Greece, March.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05), pages 573–
580, Ann Arbor, Michigan.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49–52, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool Publish-
ers.
Jan Hajiˇc, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP’2000), pages 7–12, Seat-
tle.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187–197, Edinburgh, UK.
Carlos Henriquez, Rafael E. Banchs, and Jos´e B.
Mari˜no. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.
Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98–101.
Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731–2734,
Brisbane, Australia.
M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp &amp; i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116–123.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177–180, Prague, Czech Re-
public.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65–72.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP’04), Barcelona, Spain.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231, Prague, Czech Repub-
lic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
</reference>
<page confidence="0.980766">
417
</page>
<reference confidence="0.997887236363636">
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102–109, Cairo, Egypt.
Evgeny Matusov and Selc¸uk K¨opr¨u. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–52.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, PA.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901–904, Denver,
CO.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484–491, Rochester, New York, April. Association
for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154–162, Suntec, Singapore, August.
Association for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454–
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.99305">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659139">
<title confidence="0.9997705">Language Independent Connectivity Strength for Phrase Pivot Statistical Machine Translation</title>
<author confidence="0.998694">Ahmed El_Kholy</author>
<author confidence="0.998694">Nizar Habash</author>
<affiliation confidence="0.988029">Center for Computational Learning Systems, Columbia</affiliation>
<author confidence="0.890307">Gregor Leusch</author>
<author confidence="0.890307">Evgeny</author>
<affiliation confidence="0.892586333333333">Science Applications International Corporation Hassan eBay Inc.</affiliation>
<email confidence="0.999007">hsawaf@ebay.com</email>
<abstract confidence="0.998921526315789">An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Madalina Barbaiani</author>
<author>Marcello Federico</author>
<author>Roldano Cattoni</author>
</authors>
<title>Phrase-based statistical machine translation with pivot languages. Proceeding of IWSLT,</title>
<date>2008</date>
<pages>143--149</pages>
<contexts>
<context position="2716" citStr="Bertoldi et al., 2008" startWordPosition="409" endWordPosition="412">determine the quality of the pivot phrase pairs between source and target. We show positive results (0.6 BLEU points) on Persian-Arabic SMT. Next, we briefly discuss some related work. We then review two common pivoting strategies and how we use them in Section 3. This is followed by our approach to using connectivity strength features in Section 4. We present our experimental results in Section 5. 2 Related Work Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009). The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. Pivoting has been explored for closely related languages (Hajiˇc et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to </context>
<context position="4096" citStr="Bertoldi et al., 2008" startWordPosition="626" endWordPosition="629">13. c�2013 Association for Computational Linguistics (Khalilov et al., 2008). The second strategy is based on phrase pivoting (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). In phrase pivoting, a new source-target phrase table (translation model) is induced from source-pivot and pivottarget phrase tables. Lexical weights and translation probabilities are computed from the two translation models. The third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological pro</context>
</contexts>
<marker>Bertoldi, Barbaiani, Federico, Cattoni, 2008</marker>
<rawString>Nicola Bertoldi, Madalina Barbaiani, Marcello Federico, and Roldano Cattoni. 2008. Phrase-based statistical machine translation with pivot languages. Proceeding of IWSLT, pages 143–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>728</pages>
<contexts>
<context position="3649" citStr="Cohn and Lapata, 2007" startWordPosition="555" endWordPosition="559">rent pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–418, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Khalilov et al., 2008). The second strategy is based on phrase pivoting (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). In phrase pivoting, a new source-target phrase table (translation model) is induced from source-pivot and pivottarget phrase tables. Lexical weights and translation probabilities are computed from the two translation models. The third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We e</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Orthographic and Morphological Processing for EnglishArabic Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Traitement Automatique du Langage Naturel (TALN-10).</booktitle>
<location>Montr´eal, Canada.</location>
<marker>El Kholy, Habash, 2010</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2010a. Orthographic and Morphological Processing for EnglishArabic Statistical Machine Translation. In Proceedings of Traitement Automatique du Langage Naturel (TALN-10). Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed El Kholy</author>
<author>Nizar Habash</author>
</authors>
<title>Techniques for Arabic Morphological Detokenization and Orthographic Denormalization.</title>
<date>2010</date>
<booktitle>In Proceedings of the seventh International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta,</location>
<marker>El Kholy, Habash, 2010</marker>
<rawString>Ahmed El Kholy and Nizar Habash. 2010b. Techniques for Arabic Morphological Detokenization and Orthographic Denormalization. In Proceedings of the seventh International Conference on Language Resources and Evaluation (LREC), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Nizar Habash</author>
</authors>
<title>Syntactic Reordering for English-Arabic Phrase-Based Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>69--77</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="4549" citStr="Elming and Habash, 2009" startWordPosition="701" endWordPosition="704">e a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. To our knowledge, there hasn’t been a lot of work on Persian and Arabic as a language pair. The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K¨opr¨u, 2010). 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. We also discuss how we overcome the large expansion of source-totarget phrase pai</context>
</contexts>
<marker>Elming, Habash, 2009</marker>
<rawString>Jakob Elming and Nizar Habash. 2009. Syntactic Reordering for English-Arabic Phrase-Based Machine Translation. In Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 69–77, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2007</date>
<booktitle>Arabic Gigaword 3, LDC Catalog No.: LDC2003T40. Linguistic</booktitle>
<institution>Data Consortium, University of Pennsylvania.</institution>
<contexts>
<context position="12818" citStr="Graff, 2007" startWordPosition="2071" endWordPosition="2072">ly one Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2. 5.1 Experimental Setup In our pivoting experiments, we build two SMT models. One model to translate from Persian to English and another model to translate from English to Arabic. The English-Arabic parallel corpus is about 2.8M sentences (≈60M words) available from LDC4 and GALE5 constrained data. We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E06, LDC2007E101, LDC2007E103, LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56, LDC2008G05, LDC200</context>
</contexts>
<marker>Graff, 2007</marker>
<rawString>David Graff. 2007. Arabic Gigaword 3, LDC Catalog No.: LDC2003T40. Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Jun Hu</author>
</authors>
<title>Improving ArabicChinese Statistical Machine Translation using English as Pivot Language.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>173--181</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="2738" citStr="Habash and Hu, 2009" startWordPosition="413" endWordPosition="416">f the pivot phrase pairs between source and target. We show positive results (0.6 BLEU points) on Persian-Arabic SMT. Next, we briefly discuss some related work. We then review two common pivoting strategies and how we use them in Section 3. This is followed by our approach to using connectivity strength features in Section 4. We present our experimental results in Section 5. 2 Related Work Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009). The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. Pivoting has been explored for closely related languages (Hajiˇc et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language 41</context>
</contexts>
<marker>Habash, Hu, 2009</marker>
<rawString>Nizar Habash and Jun Hu. 2009. Improving ArabicChinese Statistical Machine Translation using English as Pivot Language. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 173–181, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>573--580</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="14748" citStr="Habash and Rambow, 2005" startWordPosition="2361" endWordPosition="2364">and Agarwal, 2007). 5.2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments. We use MADA v3.1 (Habash and Rambow, 2005; Habash et al., 2009) to tokenize the Arabic text. We only evaluate on detokenized and orthographically correct (enriched) output following the work of El Kholy and Habash (2010b). Persian on the other hand has a relatively simple nominal system. There is no case system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the Arabic plural markers. Verbal morphology is very complex in Persi</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 573– 580, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>49--52</pages>
<location>New York City, USA.</location>
<contexts>
<context position="4601" citStr="Habash and Sadat, 2006" startWordPosition="710" endWordPosition="713">pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. To our knowledge, there hasn’t been a lot of work on Persian and Arabic as a language pair. The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K¨opr¨u, 2010). 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creating a pivot phrase table. </context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat. 2006. Arabic Preprocessing Schemes for Statistical Machine Translation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 49–52, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Abdelhadi Soudi</author>
<author>Tim Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="11436" citStr="Habash et al., 2007" startWordPosition="1845" endWordPosition="1848">an word in aligned to one Arabic word in the equivalent phrase and the two phrases conveys two different meanings. The English phrase is not a good translation for either, which leads to this bad pairing. This is reflected in the SCS and TCS scores. 5 Experiments In this section, we present a set of baseline experiments including a simple filtering technique to overcome the huge expansion of the pivot phrase table. Then we present our results in using connectivity strength features to improve Persian-Arabic pivot translation quality. 3We use the Habash-Soudi-Buckwalter Arabic transliteration (Habash et al., 2007) in the figures with extensions for Persian as suggested by Habash (2010). 414 Persian: AStmAd myAn dw kšwr ‘ر,-.ود ن &amp;quot;() د&amp;quot;#$%ا’ ‘trust between the two countries’ English: trust between the two countries Arabic: AlθqFi byn Aldwltyn ‘3$2و52ا 34 /012 ا’ ‘the trust between the two countries’ Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0. Persian: AyjAd cnd šrkt mštrk ‘ ک+./0 )*+, &amp;&apos;( د&amp;quot;#$ا ’ ‘Establish few joint companies’ English: joint ventures Arabic: bςD šrkAt AlmqAwlAt f</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter. 2007. On Arabic Transliteration. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools. The MEDAR Consortium,</booktitle>
<contexts>
<context position="14770" citStr="Habash et al., 2009" startWordPosition="2365" endWordPosition="2368">inguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments. We use MADA v3.1 (Habash and Rambow, 2005; Habash et al., 2009) to tokenize the Arabic text. We only evaluate on detokenized and orthographically correct (enriched) output following the work of El Kholy and Habash (2010b). Persian on the other hand has a relatively simple nominal system. There is no case system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the Arabic plural markers. Verbal morphology is very complex in Persian. Each verb has a pa</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools. The MEDAR Consortium, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="4576" citStr="Habash, 2010" startWordPosition="708" endWordPosition="709">ranslating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. To our knowledge, there hasn’t been a lot of work on Persian and Arabic as a language pair. The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K¨opr¨u, 2010). 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creati</context>
<context position="11509" citStr="Habash (2010)" startWordPosition="1859" endWordPosition="1860">es conveys two different meanings. The English phrase is not a good translation for either, which leads to this bad pairing. This is reflected in the SCS and TCS scores. 5 Experiments In this section, we present a set of baseline experiments including a simple filtering technique to overcome the huge expansion of the pivot phrase table. Then we present our results in using connectivity strength features to improve Persian-Arabic pivot translation quality. 3We use the Habash-Soudi-Buckwalter Arabic transliteration (Habash et al., 2007) in the figures with extensions for Persian as suggested by Habash (2010). 414 Persian: AStmAd myAn dw kšwr ‘ر,-.ود ن &amp;quot;() د&amp;quot;#$%ا’ ‘trust between the two countries’ English: trust between the two countries Arabic: AlθqFi byn Aldwltyn ‘3$2و52ا 34 /012 ا’ ‘the trust between the two countries’ Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0. Persian: AyjAd cnd šrkt mštrk ‘ ک+./0 )*+, &amp;&apos;( د&amp;quot;#$ا ’ ‘Establish few joint companies’ English: joint ventures Arabic: bςD šrkAt AlmqAwlAt fy Albld ‘ &amp;&lt;=&gt; ا:; ت6و&amp;quot;89ا ت&amp;quot;5+, 123’ ‘Some construcBon companies in the </context>
<context position="14582" citStr="Habash, 2010" startWordPosition="2333" endWordPosition="2334">ults on an inhouse Persian-Arabic evaluation set of 536 sentences with three references. We evaluate using BLEU-4 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). 5.2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments. We use MADA v3.1 (Habash and Rambow, 2005; Habash et al., 2009) to tokenize the Arabic text. We only evaluate on detokenized and orthographically correct (enriched) output following the work of El Kholy and Habash (2010b). Persian on the other hand has a relatively simple nominal system. There is no case system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural </context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jan Hric</author>
<author>Vladislav Kubon</author>
</authors>
<title>Machine Translation of Very Close Languages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP’2000),</booktitle>
<pages>7--12</pages>
<location>Seattle.</location>
<marker>Hajiˇc, Hric, Kubon, 2000</marker>
<rawString>Jan Hajiˇc, Jan Hric, and Vladislav Kubon. 2000. Machine Translation of Very Close Languages. In Proceedings of the 6th Applied Natural Language Processing Conference (ANLP’2000), pages 7–12, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="13086" citStr="Heafield, 2011" startWordPosition="2117" endWordPosition="2118">Arabic parallel corpus is about 2.8M sentences (≈60M words) available from LDC4 and GALE5 constrained data. We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E06, LDC2007E101, LDC2007E103, LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56, LDC2008G05, LDC2009E16, LDC2009G01. 5Global Autonomous Language Exploitation, or GALE, is a DARPA-funded research project. optimization. For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-A</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Henriquez</author>
<author>Rafael E Banchs</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Learning reordering models for statistical machine translation with a pivot language.</title>
<date>2010</date>
<marker>Henriquez, Banchs, Mari˜no, 2010</marker>
<rawString>Carlos Henriquez, Rafael E. Banchs, and Jos´e B. Mari˜no. 2010. Learning reordering models for statistical machine translation with a pivot language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Hossein Jadidinejad</author>
<author>Fariborz Mahmoudi</author>
<author>Jon Dehdari</author>
</authors>
<title>Evaluation of PerStem: a simple and efficient stemming algorithm for Persian. In Multilingual Information Access Evaluation I. Text Retrieval Experiments,</title>
<date>2010</date>
<pages>98--101</pages>
<contexts>
<context position="15635" citStr="Jadidinejad et al., 2010" startWordPosition="2515" endWordPosition="2518">se system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the Arabic plural markers. Verbal morphology is very complex in Persian. Each verb has a past and present root and many verbs have attached prefix that is regarded part of the root. A verb in Persian inflects for 14 different tense, mood, aspect, person, number and voice combination values (Rasooli et al., 2013). We use Perstem (Jadidinejad et al., 2010) for segmenting Persian text. English, our pivot language, is quite different from both Arabic and Persian. English is poor in morphology and barely inflects for number and tense, and for person in a limited context. English preprocessing simply includes down-casing, separating punctuation and splitting off “’s”. 5.3 Baseline Evaluation We compare the performance of sentence pivoting against phrase pivoting with different filtering thresholds. The results are presented in Table 2. In general, the phrase pivoting outperforms the sentence pivoting even when we use a small filtering threshold of </context>
</contexts>
<marker>Jadidinejad, Mahmoudi, Dehdari, 2010</marker>
<rawString>Amir Hossein Jadidinejad, Fariborz Mahmoudi, and Jon Dehdari. 2010. Evaluation of PerStem: a simple and efficient stemming algorithm for Persian. In Multilingual Information Access Evaluation I. Text Retrieval Experiments, pages 98–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Kathol</author>
<author>Jing Zheng</author>
</authors>
<title>Strategies for building a Farsi-English smt system from limited resources.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Annual Conference of the International Speech Communication Association (INTERSPEECH2008),</booktitle>
<pages>2731--2734</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="4626" citStr="Kathol and Zheng, 2008" startWordPosition="714" endWordPosition="717">ot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. To our knowledge, there hasn’t been a lot of work on Persian and Arabic as a language pair. The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K¨opr¨u, 2010). 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creating a pivot phrase table. 3.1 Sentence Pivoting In </context>
</contexts>
<marker>Kathol, Zheng, 2008</marker>
<rawString>Andreas Kathol and Jing Zheng. 2008. Strategies for building a Farsi-English smt system from limited resources. In Proceedings of the 9th Annual Conference of the International Speech Communication Association (INTERSPEECH2008), pages 2731–2734, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Khalilov</author>
<author>Marta R Costa-juss</author>
<author>Jos A R Fonollosa</author>
<author>Rafael E Banchs</author>
<author>B Chen</author>
<author>M Zhang</author>
<author>A Aw</author>
<author>H Li</author>
<author>Jos B Mario</author>
<author>Adolfo Hernndez</author>
<author>Carlos A Henrquez Q</author>
</authors>
<title>The talp &amp; i2r smt systems for iwslt</title>
<date>2008</date>
<booktitle>In International Workshop on Spoken Language Translation. IWSLT</booktitle>
<pages>116--123</pages>
<contexts>
<context position="2693" citStr="Khalilov et al., 2008" startWordPosition="405" endWordPosition="408">ndependent features to determine the quality of the pivot phrase pairs between source and target. We show positive results (0.6 BLEU points) on Persian-Arabic SMT. Next, we briefly discuss some related work. We then review two common pivoting strategies and how we use them in Section 3. This is followed by our approach to using connectivity strength features in Section 4. We present our experimental results in Section 5. 2 Related Work Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009). The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. Pivoting has been explored for closely related languages (Hajiˇc et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivo</context>
</contexts>
<marker>Khalilov, Costa-juss, Fonollosa, Banchs, Chen, Zhang, Aw, Li, Mario, Hernndez, Q, 2008</marker>
<rawString>M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa, Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li, Jos B. Mario, Adolfo Hernndez, and Carlos A. Henrquez Q. 2008. The talp &amp; i2r smt systems for iwslt 2008. In International Workshop on Spoken Language Translation. IWSLT 2008, pg. 116–123.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Christopher Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5895" citStr="Koehn et al., 2007" startWordPosition="922" endWordPosition="925"> between two separate phrase-based MT systems; Persian-English direct system and EnglishArabic direct system. Given a Persian sentence, we first translate the Persian sentence from Persian to English, and then from English to Arabic. 3.2 Phrase Pivoting In phrase pivoting (sometimes called triangulation or phrase table multiplication), we train a Persianto-Arabic and an English-Arabic translation models, such as those used in the sentence pivoting technique. Based on these two models, we induce a new Persian-Arabic translation model. Since we build our models on top of Moses phrase-based SMT (Koehn et al., 2007), we need to provide the same set of phrase translation probability distributions.1 We follow Utiyama and Isahara (2007) in computing the probability distributions. The following are the set of equations used to compute the lexical probabilities (φ) and the phrase probabilities (pw) φ(f|a) _ E φ(f|e)φ(e|a) e φ(a|f) _ E φ(a|e)φ(e|f) e pw(f|a) _ E pw(f|e)pw(e|a) e pw(a|f) _ E pw(a|e)pw(e|f) e where f is the Persian source phrase. e is the English pivot phrase that is common in both Persian-English translation model and EnglishArabic translation model. a is the Arabic target phrase. We also build</context>
<context position="13178" citStr="Koehn et al., 2007" startWordPosition="2129" endWordPosition="2132">5 constrained data. We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E06, LDC2007E101, LDC2007E103, LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56, LDC2008G05, LDC2009E16, LDC2009G01. 5Global Autonomous Language Exploitation, or GALE, is a DARPA-funded research project. optimization. For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIS</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Alexandra Birch</author>
<author>Ralf Steinberger</author>
</authors>
<title>462 machine translation systems for europe.</title>
<date>2009</date>
<booktitle>Proceedings of MT Summit XII,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2994" citStr="Koehn et al., 2009" startWordPosition="456" endWordPosition="459"> by our approach to using connectivity strength features in Section 4. We present our experimental results in Section 5. 2 Related Work Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009). The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. Pivoting has been explored for closely related languages (Hajiˇc et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–418, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Khalilov et al., 2008). The second strategy is based on phrase piv</context>
</contexts>
<marker>Koehn, Birch, Steinberger, 2009</marker>
<rawString>Philipp Koehn, Alexandra Birch, and Ralf Steinberger. 2009. 462 machine translation systems for europe. Proceedings of MT Summit XII, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests formachine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing Conference (EMNLP’04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="17360" citStr="Koehn, 2004" startWordPosition="2780" endWordPosition="2781">onnectivity strength features (+Conn) to the best performing phrase pivoting model (Phrase Pivot F1K). Model BLEU METEOR Sentence Pivoting 19.2 36.4 Phrase Pivot F1K 20.5 38.6 Phrase Pivot F1K+Conn 21.1 38.9 Table 3: Connectivity strength features experiment result. The results in Table 3 show that we get a nice improvement of ≈0.6/0.5 (BLEU/METEOR) points by adding the connectivity strength features. The differences in BLEU scores between this setup and all other systems are statistically significant above the 95% level. Statistical significance is computed using paired bootstrap resampling (Koehn, 2004). 6 Conclusion and Future Work We presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based SMT. We showed that these features help improving the overall translation quality. In the future, we plan to explore other features, e.g., the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases. We also plan to explore language specific features which could be extracted from some seed parallel data, e.g., s</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests formachine translation evaluation. In Proceedings of the Empirical Methods in Natural Language Processing Conference (EMNLP’04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14143" citStr="Lavie and Agarwal, 2007" startWordPosition="2268" endWordPosition="2271">ct. optimization. For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set (MT04). The optimized weights are used for ranking and filtering (discussed in Section 3.3). We use a maximum phrase length of size 8 across all models. We report results on an inhouse Persian-Arabic evaluation set of 536 sentences with three references. We evaluate using BLEU-4 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). 5.2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments. We use MADA v3.1 (Habash and Rambow,</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus.</title>
<date>2004</date>
<contexts>
<context position="14682" citStr="Maamouri et al., 2004" startWordPosition="2349" endWordPosition="2352"> evaluate using BLEU-4 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). 5.2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments. We use MADA v3.1 (Habash and Rambow, 2005; Habash et al., 2009) to tokenize the Arabic text. We only evaluate on detokenized and orthographically correct (enriched) output following the work of El Kholy and Habash (2010b). Persian on the other hand has a relatively simple nominal system. There is no case system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus.</rawString>
</citation>
<citation valid="false">
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<pages>102--109</pages>
<location>Cairo, Egypt.</location>
<marker></marker>
<rawString>In NEMLAR Conference on Arabic Language Resources and Tools, pages 102–109, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Selc¸uk K¨opr¨u</author>
</authors>
<title>Improving reordering in statistical machine translation from farsi.</title>
<date>2010</date>
<booktitle>In AMTA The Ninth Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Denver, Colorado, USA.</location>
<marker>Matusov, K¨opr¨u, 2010</marker>
<rawString>Evgeny Matusov and Selc¸uk K¨opr¨u. 2010. Improving reordering in statistical machine translation from farsi. In AMTA The Ninth Conference of the Association for Machine Translation in the Americas, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12723" citStr="Och and Ney, 2003" startWordPosition="2052" endWordPosition="2055">the country’ Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2. 5.1 Experimental Setup In our pivoting experiments, we build two SMT models. One model to translate from Persian to English and another model to translate from English to Arabic. The English-Arabic parallel corpus is about 2.8M sentences (≈60M words) available from LDC4 and GALE5 constrained data. We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E0</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13203" citStr="Och, 2003" startWordPosition="2136" endWordPosition="2137">ouse Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E06, LDC2007E101, LDC2007E103, LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56, LDC2008G05, LDC2009E16, LDC2009G01. 5Global Autonomous Language Exploitation, or GALE, is a DARPA-funded research project. optimization. For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="14106" citStr="Papineni et al., 2002" startWordPosition="2262" endWordPosition="2265">E, is a DARPA-funded research project. optimization. For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set (MT04). The optimized weights are used for ranking and filtering (discussed in Section 3.3). We use a maximum phrase length of size 8 across all models. We report results on an inhouse Persian-Arabic evaluation set of 536 sentences with three references. We evaluate using BLEU-4 (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). 5.2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. Both Arabic and Persian are morphologically complex languages but they belong to two different language families. They both express richness and linguistic complexities in different ways. One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al., 2004) in our 415 experiments.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Sadegh Rasooli</author>
<author>Manouchehr Kouhestani</author>
<author>Amirsaeid Moloodi</author>
</authors>
<title>Development of a Persian syntactic dependency treebank.</title>
<date>2013</date>
<booktitle>In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT),</booktitle>
<location>Atlanta, USA.</location>
<contexts>
<context position="15592" citStr="Rasooli et al., 2013" startWordPosition="2508" endWordPosition="2511">y simple nominal system. There is no case system and words do not inflect with gender except for a few animate Arabic loanwords. Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix lm+ +hA and sometimes J+ +An, or one of the Arabic plural markers. Verbal morphology is very complex in Persian. Each verb has a past and present root and many verbs have attached prefix that is regarded part of the root. A verb in Persian inflects for 14 different tense, mood, aspect, person, number and voice combination values (Rasooli et al., 2013). We use Perstem (Jadidinejad et al., 2010) for segmenting Persian text. English, our pivot language, is quite different from both Arabic and Persian. English is poor in morphology and barely inflects for number and tense, and for person in a limited context. English preprocessing simply includes down-casing, separating punctuation and splitting off “’s”. 5.3 Baseline Evaluation We compare the performance of sentence pivoting against phrase pivoting with different filtering thresholds. The results are presented in Table 2. In general, the phrase pivoting outperforms the sentence pivoting even </context>
</contexts>
<marker>Rasooli, Kouhestani, Moloodi, 2013</marker>
<rawString>Mohammad Sadegh Rasooli, Manouchehr Kouhestani, and Amirsaeid Moloodi. 2013. Development of a Persian syntactic dependency treebank. In The 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="12967" citStr="Stolcke, 2002" startWordPosition="2097" endWordPosition="2098">ls. One model to translate from Persian to English and another model to translate from English to Arabic. The English-Arabic parallel corpus is about 2.8M sentences (≈60M words) available from LDC4 and GALE5 constrained data. We use an in-house Persian-English parallel corpus of about 170K sentences and 4M words. Word alignment is done using GIZA++ (Och and Ney, 2003). For Arabic language modeling, we use 200M words from the Arabic Gigaword Corpus (Graff, 2007) together with the Arabic side of our training data. We use 5-grams for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). All experiments are conducted using the Moses phrase-based SMT system (Koehn et al., 2007). We use MERT (Och, 2003) for decoding weight 4LDC Catalog IDs: LDC2005E83, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05, LDC2007E06, LDC2007E101, LDC2007E103, LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56, LDC2008G05, LDC2009E16, LDC2009G01. 5Global Autonomous Language Exploitation, or GALE, is a DARPA-funded research project. optimization. For Persian-English translatio</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A comparison of pivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>484--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="1671" citStr="Utiyama and Isahara, 2007" startWordPosition="234" endWordPosition="237">ults (0.6 BLEU points) on Persian-Arabic SMT as a case study. 1 Introduction One of the main issues in statistical machine translation (SMT) is the scarcity of parallel data for many language pairs especially when the source and target languages are morphologically rich. A common SMT solution to the lack of parallel data is to pivot the translation through a third language (called pivot or bridge language) for which there exist abundant parallel corpora with the source and target languages. The literature covers many pivoting techniques. One of the best performing techniques, phrase pivoting (Utiyama and Isahara, 2007), builds an induced new phrase table between the source and target. One of the main issues of this technique is that the size of the newly created pivot phrase table is very large (Utiyama and Isahara, 2007). Moreover, many of the produced phrase pairs are of low quality which affects the translation choices during decoding and the overall translation quality. In this paper, we introduce language independent features to determine the quality of the pivot phrase pairs between source and target. We show positive results (0.6 BLEU points) on Persian-Arabic SMT. Next, we briefly discuss some relat</context>
<context position="3626" citStr="Utiyama and Isahara, 2007" startWordPosition="551" endWordPosition="554">h and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–418, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Khalilov et al., 2008). The second strategy is based on phrase pivoting (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). In phrase pivoting, a new source-target phrase table (translation model) is induced from source-pivot and pivottarget phrase tables. Lexical weights and translation probabilities are computed from the two translation models. The third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama a</context>
<context position="6015" citStr="Utiyama and Isahara (2007)" startWordPosition="941" endWordPosition="945">en a Persian sentence, we first translate the Persian sentence from Persian to English, and then from English to Arabic. 3.2 Phrase Pivoting In phrase pivoting (sometimes called triangulation or phrase table multiplication), we train a Persianto-Arabic and an English-Arabic translation models, such as those used in the sentence pivoting technique. Based on these two models, we induce a new Persian-Arabic translation model. Since we build our models on top of Moses phrase-based SMT (Koehn et al., 2007), we need to provide the same set of phrase translation probability distributions.1 We follow Utiyama and Isahara (2007) in computing the probability distributions. The following are the set of equations used to compute the lexical probabilities (φ) and the phrase probabilities (pw) φ(f|a) _ E φ(f|e)φ(e|a) e φ(a|f) _ E φ(a|e)φ(e|f) e pw(f|a) _ E pw(f|e)pw(e|a) e pw(a|f) _ E pw(a|e)pw(e|f) e where f is the Persian source phrase. e is the English pivot phrase that is common in both Persian-English translation model and EnglishArabic translation model. a is the Arabic target phrase. We also build a Persian-Arabic reordering table using the same technique but we compute the reordering weights in a similar manner to</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 484–491, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Revisiting pivot language approach for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>154--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2670" citStr="Wu and Wang, 2009" startWordPosition="401" endWordPosition="404">ntroduce language independent features to determine the quality of the pivot phrase pairs between source and target. We show positive results (0.6 BLEU points) on Persian-Arabic SMT. Next, we briefly discuss some related work. We then review two common pivoting strategies and how we use them in Section 3. This is followed by our approach to using connectivity strength features in Section 4. We present our experimental results in Section 5. 2 Related Work Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al., 2008; Bertoldi et al., 2008; Habash and Hu, 2009). The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual corpora. Pivoting has been explored for closely related languages (Hajiˇc et al., 2000) as well as unrelated languages (Koehn et al., 2009; Habash and Hu, 2009). Many different pivot strategies have been presented in the literature. The following three are perhaps the most common. The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and </context>
</contexts>
<marker>Wu, Wang, 2009</marker>
<rawString>Hua Wu and Haifeng Wang. 2009. Revisiting pivot language approach for machine translation. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 154–162, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reyyan Yeniterzi</author>
<author>Kemal Oflazer</author>
</authors>
<title>Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>454--464</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4524" citStr="Yeniterzi and Oflazer, 2010" startWordPosition="697" endWordPosition="700">he third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al., 2008). In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). We extend phrase table scores with two other features that are language independent. Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. To our knowledge, there hasn’t been a lot of work on Persian and Arabic as a language pair. The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K¨opr¨u, 2010). 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. We also discuss how we overcome the large expansion of s</context>
</contexts>
<marker>Yeniterzi, Oflazer, 2010</marker>
<rawString>Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 454– 464, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>