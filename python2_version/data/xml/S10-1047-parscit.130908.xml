<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001354">
<title confidence="0.947076">
FBK-IRST: Semantic Relation Extraction using Cyc
</title>
<author confidence="0.846237">
Kateryna Tymoshenko and Claudio Giuliano
</author>
<affiliation confidence="0.765826">
FBK-IRST
</affiliation>
<address confidence="0.511013">
I-38050, Povo (TN), Italy
</address>
<email confidence="0.986523">
tymoshenko@fbk.eu, giuliano@fbk.eu
</email>
<sectionHeader confidence="0.997266" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999815">
We present an approach for semantic re-
lation extraction between nominals that
combines semantic information with shal-
low syntactic processing. We propose to
use the ResearchCyc knowledge base as
a source of semantic information about
nominals. Each source of information
is represented by a specific kernel func-
tion. The experiments were carried out
using support vector machines as a clas-
sifier. The system achieves an overall F1
of 77.62% on the “Multi-Way Classifica-
tion of Semantic Relations Between Pairs
of Nominals” task at SemEval-2010.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966379310345">
The SemEval-2010 Task 8 “Multi-Way Classifi-
cation of Semantic Relations Between Pairs of
Nominals” consists in identifying which seman-
tic relation holds between two nominals in a sen-
tence (Hendrickx et al., 2010). The set of rela-
tions is composed of nine mutually exclusive se-
mantic relations and the Other relation. Specifi-
cally, the task requires to return the most informa-
tive relation between the specified pair of nomi-
nals e1 and e2 taking into account their order. An-
notation guidelines show that semantic knowledge
about e1 and e2 plays a very important role in dis-
tinguishing among different relations. For exam-
ple, relations Cause-Effect and Product-Producer
are closely related. One of the restrictions which
might help to distinguish between them is that
products must be concrete physical entities, while
effects must not.
Recently, there has emerged a large number of
freely available large-scale knowledge bases. The
ground idea of our research is to use them as
source of semantic information. Among such re-
sources there are DBpedia,1 YAGO,2 and Open-
Cyc.3 On the one hand, DBpedia and YAGO have
been automatically extracted from Wikipedia.
They have a good coverage of named entities, but
their coverage of common nouns is poorer. They
seem to be more suitable for relation extraction be-
tween named entities. On the other hand, Cyc is
a manually designed knowledge base, which de-
scribes actions and entities both in common life
and in specific domains (Lenat, 1995). Cyc has
a good coverage of common nouns, making it in-
teresting for our task. The full version of Cyc is
freely available to the research community as Re-
searchCyc.4
We approached the task using the system intro-
duced by Giuliano et al. (2007) as a basis. They
exploited two information sources: the whole sen-
tence where the relation appears, and WordNet
synonymy and hyperonymy information. In this
paper, we (i) investigate usage of Cyc as a source
of semantic knowledge and (ii) linguistic infor-
mation, which give useful clues to semantic re-
lation extraction. From Cyc, we obtain informa-
tion about super-classes (in the Cyc terminology
generalizations) of the classes which correspond
to nominals in a sentence. The sentence itself
provides linguistic information, such as local con-
texts of entities, bag of verbs and distance between
nominals in the context.
The different sources of information are rep-
resented by kernel functions. The final system
is based on four kernels (i.e., local context ker-
nel, distance kernel, verbs kernel and generaliza-
tion kernel). The experiments were carried out us-
ing support vector machines (Vapnik, 1998) as a
classifier. The system achieves an overall F1 of
</bodyText>
<footnote confidence="0.9995032">
1http://dbpedia.org/
2http://www.mpi-inf.mpg.de/yago-naga/
yago/
3http://www.cyc.com/opencyc
4http://research.cyc.com/
</footnote>
<page confidence="0.966603">
214
</page>
<bodyText confidence="0.578331666666667">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214–217,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
77.62%.
</bodyText>
<sectionHeader confidence="0.97524" genericHeader="method">
2 Kernel Methods for Relation
Extraction
</sectionHeader>
<bodyText confidence="0.99838925">
In order to implement the approach based on shal-
low syntactic and semantic information, we em-
ployed a linear combination of kernels, using the
support vector machines as a classifier. We de-
veloped two types of basic kernels: syntactic and
semantic kernels. They were combined by exploit-
ing the closure properties of kernels. We define the
composite kernel KC(x1, x2) as follows.
</bodyText>
<equation confidence="0.9998905">
�n / Ki(x1, x2) (1)
i=1 � Ki(x1, x1)Ki(x2, x2) .
</equation>
<bodyText confidence="0.845125666666667">
Each basic kernel Ki is normalized.
All the basic kernels are explicitly calculated as
follows
</bodyText>
<equation confidence="0.989292">
Ki(x1, x2) = (ϕ(x1), ϕ(x2)) , (2)
</equation>
<bodyText confidence="0.9999152">
where ϕ(·) is the embedding vector. The resulting
feature space has high dimensionality. However,
Equation 2 can be efficiently computed explicitly
because the representations of input are extremely
sparse.
</bodyText>
<subsectionHeader confidence="0.996547">
2.1 Local context kernel
</subsectionHeader>
<bodyText confidence="0.993753166666667">
Local context is represented by terms, lemmata,
PoS tags, and orthographic features extracted
from a window around the nominals considering
the token order. Formally, given a relation ex-
ample R, we represent a local context LC =
t−w, ...,t−1, t0, t+1, ..., t+w as a row vector
</bodyText>
<equation confidence="0.9980975">
ψLC(R) = (tf1(LC), tf2(LC), ..., tfm(LC) ) E {0, 1}m,
(3)
</equation>
<bodyText confidence="0.90469">
where tfi is a feature function which returns 1
if the feature is active in the specified position
of LC; 0 otherwise. The local context kernel
KLC (R1, R2) is defined as
</bodyText>
<equation confidence="0.99053">
KLC e1(R1, R2) + KLC e2(R1, R2), (4)
</equation>
<bodyText confidence="0.999901333333333">
where KLC e1 and KLC e2 are defined by substi-
tuting the embedding of the local contexts of e1
and e2 into Equation 2, respectively.
</bodyText>
<subsectionHeader confidence="0.999113">
2.2 Verb kernel
</subsectionHeader>
<bodyText confidence="0.9970125">
The verb kernel operates on the verbs present in
the sentence,5 representing it as a bag-of-verbs.
</bodyText>
<footnote confidence="0.511079">
5On average there are 2.65 verbs per sentence
</footnote>
<bodyText confidence="0.9990295">
More formally, given a relation example R, we
represent the verbs from it as a row vector
</bodyText>
<equation confidence="0.997581">
ψV (R) = (vf(v1, R), ..., vf(vl, R)) E {0, 1}l, (5)
</equation>
<bodyText confidence="0.9959175">
where the binary function vf(vi, R) shows if a
particular verb is used in R. By substituting
ψV (R) into Equation 2 we obtain the bag-of-verbs
kernel KV .
</bodyText>
<subsectionHeader confidence="0.999562">
2.3 Distance kernel
</subsectionHeader>
<bodyText confidence="0.999036666666667">
Given a relation example R(e1, e2), we repre-
sent the distance between the nominals as a one-
dimensional vector
</bodyText>
<equation confidence="0.999147333333333">
1
ψD(R) = 1, (6)
dist(e1, e2) E �t
</equation>
<bodyText confidence="0.9998315">
where dist(e1, e2) is number of tokens between
the nominals e1 and e2 in a sentence. By substitut-
ing ψD(R) into Equation 2 we obtain the distance
kernel KD.
</bodyText>
<subsectionHeader confidence="0.979066">
2.4 Cyc-based kernel
</subsectionHeader>
<bodyText confidence="0.99974575">
Cyc is a comprehensive, manually-build knowl-
edge base developed since 1984 by CycCorp. Ac-
cording to Lenat (1995) it can be considered as
an expert system with domain spanning all ev-
eryday actions and entities, like Fish live in wa-
ter. The open-source version of Cyc named Open-
Cyc, which contains the full Cyc ontology and re-
stricted number of assertions, is freely available
on the web. Also the full power of Cyc has been
made available to the research community via Re-
searchCyc. Cyc knowledge base contains more
than 500,000 concepts and more than 5 million as-
sertions about them. They may refer both to com-
mon human knowledge like food or drinks and to
specialized knowledge in domains like physics or
chemistry. The knowledge base has been formu-
lated using CycL language. A Cyc constant repre-
sents a thing or a concept in the world. It may be
an individual, e.g. BarackObama, or a collection,
e.g. Gun, Screaming.
</bodyText>
<subsubsectionHeader confidence="0.50024">
2.4.1 Generalization kernel
</subsubsectionHeader>
<bodyText confidence="0.999864571428572">
Given a nominal e, we map it to a set of Cyc
constants EC = {cil, using the Cyc function
denotation-mapper. Nominals in Cyc usually de-
note constants-collections. Notice that we do not
perform word sense disambiguation. For each ci E
EC, we query Cyc for collections which general-
ize it. In Cyc collection X generalizes collection
</bodyText>
<page confidence="0.992255">
215
</page>
<bodyText confidence="0.91119375">
F1
Y if each element of Y is also an element of col-
lection X. For instance, collection Gun is general-
ized by Weapon, ConventionalWeapon, Mechani-
calDevice and others.
The semantic kernel incorporates the data from
Cyc described above. More formally, given a rela-
tion example R each nominal e is represented as
</bodyText>
<equation confidence="0.997557">
ψEC(R) = (fc(c1, e), ..., fc(ck, e)) ∈ {0, 1}k, (7)
</equation>
<bodyText confidence="0.9991995">
where the binary function fc(ci, e) shows if a par-
ticular Cyc collection ci is a generalization of e.
</bodyText>
<equation confidence="0.891669333333333">
The bag-of-generalizations kernel
Kgenls (R1, R2) is defined as
Kgenls e1 (R1, R2) + Kgenls e2 (R1, R2) , (8)
</equation>
<bodyText confidence="0.999787">
where Kgenls e1 and Kgenls e2 are defined by sub-
stituting the embedding of generalizations e1 and
e2 into Equation 2 respectively.
</bodyText>
<sectionHeader confidence="0.990936" genericHeader="method">
3 Experimental setup and Results
</sectionHeader>
<bodyText confidence="0.998718185185186">
Sentences have been tokenized, lemmatized and
PoS tagged with TextPro.6 Information for gener-
alization kernel has been obtained from Research-
Cyc. All the experiments were performed using
jSRE customized to embed our kernels.7 jSRE
uses the SVM package LIBSVM (Chang and Lin,
2001). The task is casted as multi-class classifica-
tion problem with 19 classes (2 classes for each
relation to encode the directionality and 1 class
to encode Other). The multiple classification task
is handled with One-Against-One technique. The
SVM parameters have been set as follows. The
cost-factor Wi for a given class i is set to be the
ratio between the number of negative and positive
examples. We used two values of regularization
parameter C: (i) Cdef = r 1K(x,x) where x are
all examples from the training set, (ii) optimized
Cg,id value obtained by brute-force grid search
method. The default value is used for the other
parameters.
Table 1 shows the performance of different ker-
nel combinations, trained on 8000 training exam-
ples, on the test set. The system achieves the
best overall macro-average F1 of 77.62% using
KLC + KV + KD + Kgenls. Figure 1 shows the
learning curves on the test set. Our experimen-
tal study has shown that the size of the training
</bodyText>
<footnote confidence="0.94694325">
6http://textpro.fbk.eu/
7jSRE is a Java tool for relation extraction avail-
able at http://tcc.itc.it/research/textec/
tools-resources/jsre.html.
</footnote>
<figure confidence="0.9882635">
1000 2000 4000 8000
Number of training examples
</figure>
<figureCaption confidence="0.958954">
Figure 1: Learning curves on the test set per rela-
tion
</figureCaption>
<table confidence="0.991338142857143">
Kernels P R F1
KLC + KV + KD + Kgenls 74.98 80.69 77.62
KLC + KV + KD + Kgenls* 78.51 76.03 77.11
KLC + KD + Kgenls* 78.14 75.93 76.91
KLC + Kgenls* 78.19 75.70 76.81
KLC + KD + Kgenls 72.98 80.28 76.39
KLC + Kgenls 73.05 79.98 76.28
</table>
<tableCaption confidence="0.873954666666667">
Table 1: Performance on the test set. Combina-
tions marked with * were run with Cg,id, others
with Cdef.
</tableCaption>
<bodyText confidence="0.99938125">
set influences the performance of the system. We
observe that when the system is trained on 8000
examples the overall F1 increases for 14.01% as
compared to the case of 1000 examples.
</bodyText>
<sectionHeader confidence="0.993009" genericHeader="evaluation">
4 Discussion and error analysis
</sectionHeader>
<bodyText confidence="0.999190333333333">
The experiments have shown that KLC is the core
kernel of our approach. It has good performance
on its own. For instance, it achieves precision of
66.16%, recall 72.67% and F1 of 69.13% evalu-
ated using 10-fold cross-validation on the training
set.
</bodyText>
<table confidence="0.9996153">
Relation KLC KLC + Kgenls ΔF1
Cause-Effect 74.29 76.41 2.12
Component-Whole 61.24 66.13 4.89
Content-Container 76.36 79.12 2.76
Entity-Destination 82.85 83.95 1.10
Entity-Origin 72.09 74.13 2.04
Instrument-Agency 57.71 65.51 7.80
Member-Collection 81.30 83.40 2.10
Message-Topic 60.41 69.09 8.68
Product-Producer 55.95 63.52 7.57
</table>
<tableCaption confidence="0.995898">
Table 2: The contribution of Cyc evaluated on the
training set.
</tableCaption>
<figure confidence="0.999569619047619">
0.90
0.85
0.80
0.75
0.70
0.65
0.60
0.55
0.50
0.45
0.40
Cause-Effect
Component-Whole
Content-Container
Entity-Destination
Entity-Origin
Instrument-Agency
Member-Collection
Message-Topic
Product-Producer
All
</figure>
<page confidence="0.998839">
216
</page>
<bodyText confidence="0.999886469387755">
Generalization kernel combined with local con-
text kernel gives precision of 70.38%, recall of
76.96%, and F1 73.47% with the same exper-
imental setting. The increase of F1 per re-
lation is shown in the Table 2 in the col-
umn ΔF1. The largest F1 increase is ob-
served for Instrument-Agency (+7.80%), Message-
Topic (+8.68%) and Product-Producer (+7.57%).
Kgenls reduces the number of misclassifications
between the two directions of the same rela-
tion, like Product-Producer(artist,design). It
also captures the differences among relations,
specified in the annotation guidelines. For in-
stance, the system based only on KLC misclass-
fied “The &lt;e1&gt;species&lt;/e1&gt; makes a squelching
&lt;e2&gt;noise&lt;/e2&gt;” as Product-Producer(e2,e1).
Generalizations for &lt;e2&gt;noise&lt;/e2&gt; provided
by Cyc include Event, MovementEvent, Sound.
According to the annotation guidelines a product
must not be an event. A system based on the com-
bination of KLC and Kgenls correctly labels this
example as Cause-Effect(e1,e2).
Kgenls improves the performance in general.
However, in some cases using Cyc as a source of
semantic information is a source of errors. Firstly,
sometimes the set of constants for a given nom-
inal is empty (e.g., disassembler, babel) or does
not include the correct one (noun surge is mapped
to the constant IncreaseEvent). In other cases,
an ambiguous nominal is mapped to many con-
stants at once. For instance, notes is mapped
to a set of constants, which includes Musical-
Note, Note-Document and InformationRecording-
Process. Word sense disambiguation should help
to solve this problem. Other knowledge bases like
DBpedia and FreeBase8 can be used to overcome
the problem of lack of coverage.
Bag-of-word kernel with all words from the
sentence did not impact the final result.9 However,
the information about verbs present in the sentence
represented by KV helped to improve the perfor-
mance. A preliminary error analysis shows that a
deeper syntactic analysis could help to further im-
prove the performance.
For comparison purposes, we also exploited
WordNet information by means of the supersense
kernel KSS (Giuliano et al., 2007). In all exper-
iments, KSS was outperformed by Kgenls. For
instance, KLC + KSS gives overall F1 measure
</bodyText>
<footnote confidence="0.994786">
8http://www.freebase.com/
9This kernel has been evaluated only on the training data.
</footnote>
<bodyText confidence="0.9963915">
of 70.29% with the same experimental setting as
described in the beginning of this section.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999866">
The paper describes a system for semantic rela-
tions extraction, based on the usage of semantic
information provided by ResearchCyc and shal-
low syntactic features. The experiments have
shown that the external knowledge, encoded as
super-class information from ResearchCyc with-
out any word sense disambiguation, significantly
contributes to improve overall performance of the
system. The problem of the lack of coverage may
be overcome by the usage of other large-scale
knowledge bases, such as DBpedia. For future
work, we will try to use the Cyc inference en-
gine to obtain implicit information about nominals
in addition to the information about their super-
classes and perform word sense disambiguation.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.971505125">
The research leading to these results has received funding
from the ITCH project (http://itch.fbk.eu), spon-
sored by the Italian Ministry of University and Research and
by the Autonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint Research
Project under Future Internet - Internet of Content program
of the Information Technology Center, Fondazione Bruno
Kessler.
</bodyText>
<sectionHeader confidence="0.998725" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9823745">
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
˜cjlin/libsvm.
Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and
Lorenza Romano. 2007. Fbk-irst: Kernel methods
for semantic relation extraction. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010. Semeval-2010 task 8: Multi-way classification of
semantic relations between pairs of nominals. In Proceed-
ings of the 5th SIGLEX Workshop on Semantic Evaluation,
Uppsala, Sweden.
Douglas B. Lenat. 1995. CYC: A large-scale investment in
knowledge infrastructure. Communications of the ACM,
38(11):33–38.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
</reference>
<page confidence="0.998401">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451436">
<title confidence="0.99975">FBK-IRST: Semantic Relation Extraction using Cyc</title>
<author confidence="0.997717">Tymoshenko Giuliano</author>
<affiliation confidence="0.985761">FBK-IRST</affiliation>
<address confidence="0.988766">I-38050, Povo (TN), Italy</address>
<email confidence="0.998464">tymoshenko@fbk.eu,giuliano@fbk.eu</email>
<abstract confidence="0.985323357142857">We present an approach for semantic relation extraction between nominals that combines semantic information with shallow syntactic processing. We propose to use the ResearchCyc knowledge base as a source of semantic information about nominals. Each source of information is represented by a specific kernel function. The experiments were carried out using support vector machines as a clas- The system achieves an overall of 77.62% on the “Multi-Way Classification of Semantic Relations Between Pairs</abstract>
<note confidence="0.62118">of Nominals” task at SemEval-2010.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm.</title>
<date>2001</date>
<contexts>
<context position="8318" citStr="Chang and Lin, 2001" startWordPosition="1366" endWordPosition="1369">shows if a particular Cyc collection ci is a generalization of e. The bag-of-generalizations kernel Kgenls (R1, R2) is defined as Kgenls e1 (R1, R2) + Kgenls e2 (R1, R2) , (8) where Kgenls e1 and Kgenls e2 are defined by substituting the embedding of generalizations e1 and e2 into Equation 2 respectively. 3 Experimental setup and Results Sentences have been tokenized, lemmatized and PoS tagged with TextPro.6 Information for generalization kernel has been obtained from ResearchCyc. All the experiments were performed using jSRE customized to embed our kernels.7 jSRE uses the SVM package LIBSVM (Chang and Lin, 2001). The task is casted as multi-class classification problem with 19 classes (2 classes for each relation to encode the directionality and 1 class to encode Other). The multiple classification task is handled with One-Against-One technique. The SVM parameters have been set as follows. The cost-factor Wi for a given class i is set to be the ratio between the number of negative and positive examples. We used two values of regularization parameter C: (i) Cdef = r 1K(x,x) where x are all examples from the training set, (ii) optimized Cg,id value obtained by brute-force grid search method. The defaul</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ ˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Daniele Pighin</author>
<author>Lorenza Romano</author>
</authors>
<title>Fbk-irst: Kernel methods for semantic relation extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2451" citStr="Giuliano et al. (2007)" startWordPosition="386" endWordPosition="389">YAGO have been automatically extracted from Wikipedia. They have a good coverage of named entities, but their coverage of common nouns is poorer. They seem to be more suitable for relation extraction between named entities. On the other hand, Cyc is a manually designed knowledge base, which describes actions and entities both in common life and in specific domains (Lenat, 1995). Cyc has a good coverage of common nouns, making it interesting for our task. The full version of Cyc is freely available to the research community as ResearchCyc.4 We approached the task using the system introduced by Giuliano et al. (2007) as a basis. They exploited two information sources: the whole sentence where the relation appears, and WordNet synonymy and hyperonymy information. In this paper, we (i) investigate usage of Cyc as a source of semantic knowledge and (ii) linguistic information, which give useful clues to semantic relation extraction. From Cyc, we obtain information about super-classes (in the Cyc terminology generalizations) of the classes which correspond to nominals in a sentence. The sentence itself provides linguistic information, such as local contexts of entities, bag of verbs and distance between nomin</context>
<context position="13048" citStr="Giuliano et al., 2007" startWordPosition="2118" endWordPosition="2121">cordingProcess. Word sense disambiguation should help to solve this problem. Other knowledge bases like DBpedia and FreeBase8 can be used to overcome the problem of lack of coverage. Bag-of-word kernel with all words from the sentence did not impact the final result.9 However, the information about verbs present in the sentence represented by KV helped to improve the performance. A preliminary error analysis shows that a deeper syntactic analysis could help to further improve the performance. For comparison purposes, we also exploited WordNet information by means of the supersense kernel KSS (Giuliano et al., 2007). In all experiments, KSS was outperformed by Kgenls. For instance, KLC + KSS gives overall F1 measure 8http://www.freebase.com/ 9This kernel has been evaluated only on the training data. of 70.29% with the same experimental setting as described in the beginning of this section. 5 Conclusion The paper describes a system for semantic relations extraction, based on the usage of semantic information provided by ResearchCyc and shallow syntactic features. The experiments have shown that the external knowledge, encoded as super-class information from ResearchCyc without any word sense disambiguatio</context>
</contexts>
<marker>Giuliano, Lavelli, Pighin, Romano, 2007</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and Lorenza Romano. 2007. Fbk-irst: Kernel methods for semantic relation extraction. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation,</booktitle>
<location>Uppsala,</location>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>CYC: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2209" citStr="Lenat, 1995" startWordPosition="344" endWordPosition="345">ge number of freely available large-scale knowledge bases. The ground idea of our research is to use them as source of semantic information. Among such resources there are DBpedia,1 YAGO,2 and OpenCyc.3 On the one hand, DBpedia and YAGO have been automatically extracted from Wikipedia. They have a good coverage of named entities, but their coverage of common nouns is poorer. They seem to be more suitable for relation extraction between named entities. On the other hand, Cyc is a manually designed knowledge base, which describes actions and entities both in common life and in specific domains (Lenat, 1995). Cyc has a good coverage of common nouns, making it interesting for our task. The full version of Cyc is freely available to the research community as ResearchCyc.4 We approached the task using the system introduced by Giuliano et al. (2007) as a basis. They exploited two information sources: the whole sentence where the relation appears, and WordNet synonymy and hyperonymy information. In this paper, we (i) investigate usage of Cyc as a source of semantic knowledge and (ii) linguistic information, which give useful clues to semantic relation extraction. From Cyc, we obtain information about </context>
<context position="6133" citStr="Lenat (1995)" startWordPosition="992" endWordPosition="993">) where the binary function vf(vi, R) shows if a particular verb is used in R. By substituting ψV (R) into Equation 2 we obtain the bag-of-verbs kernel KV . 2.3 Distance kernel Given a relation example R(e1, e2), we represent the distance between the nominals as a onedimensional vector 1 ψD(R) = 1, (6) dist(e1, e2) E �t where dist(e1, e2) is number of tokens between the nominals e1 and e2 in a sentence. By substituting ψD(R) into Equation 2 we obtain the distance kernel KD. 2.4 Cyc-based kernel Cyc is a comprehensive, manually-build knowledge base developed since 1984 by CycCorp. According to Lenat (1995) it can be considered as an expert system with domain spanning all everyday actions and entities, like Fish live in water. The open-source version of Cyc named OpenCyc, which contains the full Cyc ontology and restricted number of assertions, is freely available on the web. Also the full power of Cyc has been made available to the research community via ResearchCyc. Cyc knowledge base contains more than 500,000 concepts and more than 5 million assertions about them. They may refer both to common human knowledge like food or drinks and to specialized knowledge in domains like physics or chemist</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B. Lenat. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<location>Wiley-Interscience,</location>
<contexts>
<context position="3351" citStr="Vapnik, 1998" startWordPosition="529" endWordPosition="530">to semantic relation extraction. From Cyc, we obtain information about super-classes (in the Cyc terminology generalizations) of the classes which correspond to nominals in a sentence. The sentence itself provides linguistic information, such as local contexts of entities, bag of verbs and distance between nominals in the context. The different sources of information are represented by kernel functions. The final system is based on four kernels (i.e., local context kernel, distance kernel, verbs kernel and generalization kernel). The experiments were carried out using support vector machines (Vapnik, 1998) as a classifier. The system achieves an overall F1 of 1http://dbpedia.org/ 2http://www.mpi-inf.mpg.de/yago-naga/ yago/ 3http://www.cyc.com/opencyc 4http://research.cyc.com/ 214 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214–217, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics 77.62%. 2 Kernel Methods for Relation Extraction In order to implement the approach based on shallow syntactic and semantic information, we employed a linear combination of kernels, using the support vector machines as a classifier. We develope</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>