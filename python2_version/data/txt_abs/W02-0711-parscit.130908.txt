<newSection> 3 Tongues System Design and Implementation The architecture and user interface of the Tongues system were based in large measure on the Diplomat system (Frederking et al., 2000); the main change was changing the synthesis system to be the open-source Festival (Black et al., 1998) system.
The speech recognition system used was the open-source Sphinx II (Huang et al., 1992), and the translation system was an EBMT/MEMT (Example-Based MT/MultiEngine MT) system (Brown, 1996; Frederking and Nirenburg, 1994; Brown and Frederking, 1995) very similar to that in Diplomat.
We have provided a more detailed description of the development of the Tongues system elsewhere (Black et al., 2002b).
In order to provide in-domain conversational data, we arranged at the start of the project to record a number of chaplains in role-playing conversations of the type they expected the device to encounter.
Fortunately, the chaplains were familiar with role-playing exercises, and all had relevant field experiences to re-enact.
Both sides of the conversations were spoken in English.
These were digitally recorded with head-mounted microphones at 16KHz in stereo (one speaker on each channel), as this was closest to the intended audio channel characteristics of the eventual system.
In all, we recorded 46 conversations, ranging from a few minutes to 20 minutes length.
This provided a total of 4.25 hours of actual English speech.
The recorded conversations were hand-transcribed at the word level, and translated into Croatian by native Croatian speakers.
Since the speech components of the system were previously-developed, open-source systems, we will only describe here the development of their training data, emphasizing the small amount of data used.
The development of the Tongues speech components is described in greater detail elsewhere (Black et al., 2002a).
English speech components.
The English recordings of the role-playing exercises were used directly for training the English acoustic models.
That is, we took only these 4.25 hours of chaplain speech and directly trained semi-continous HMM models for Sphinx2 (Huang et al., 1992).
For the English language model, we required a larger collection of in-domain text.
We used the dialog transcriptions themselves, but also augmented that with text from chaplain handbooks that were made available to us.
Although we knew we could provide better recognition accuracy by using more resources, we were interested in limiting what resources were necessary for this work, and also (see below) we found the trained models from this data adequate for the task.
Croatian speech components.
Building Croatian acoustic models was harder.
As we were aware that our resource of Croatian speakers was limited, and they had less skill in carrying out full word transcription of conversational speech, we wished to find a simpler, less resource-intensive method to build Croatian acoustic models.
From the the translated chaplain transcripts, we wished to select example utterances that when recorded would give sufficient acoustic coverage to allow reasonable acoustic models to be trained.
To do this, we used a technique originally developed for selecting text to record for speech synthesis (Black and Lenzo, 2001).
From a list of several thousand utterances, we used this technique to select groups of 250 utterances that were phonetically rich.
These sets were then read by a number of native Croatian speakers.
Using read speech avoided the process of hand-transcription of the speech, though it does make it less like the intended conversational speech.
Due to the relative scarcity of native Croatian speakers, we recorded only 15 different speakers, of which 13 were female and 2 were male.
This resulted in a gender imbalance, which concerned us greatly, but was not observed to affect the system's performance greatly in the field.
In all, a total of 4.0 hours of Croatian speech was collected.
This data alone was then used to train new acoustic models for Croatian.
The Croatian language model was built from the Croatian side of the translation data (see below), which included the Croatian translations of the role-playing exercises.
We will now describe our MT component.
Due to the project requirements described above, a translation technology was needed that was well-suited to the problems of rapid, low-cost deployment and wide-domain coverage.
While more traditional Knowledge-Based MT (KBMT) can provide high-quality MT in a narrow domain, KBMT systems typically require over a year to bring online, and an order of magnitude more professional staff than we could afford.
Statistical MT systems typically require very large amounts of domain data that are not usually available for speech translation applications.
In contrast, the EBMT/MEMT translation approach that we had used in the Diplomat system seemed appropriate for this application.
The Multi-Engine MT (MEMT) system that was used in Diplomat (Frederking et al., 2000) combines a &quot;shallow&quot; EBMT system with any available glossaries or dictionaries.
There are two primary differences between our &quot;shallow&quot; EBMT system and other systems that employ translation by analogy.
First, our basic system relies primarily on matching sequences of words (surface strings) in a simple parallel corpus of corresponding sentence pairs in the two languages, rather than matching more complex representations (such as parse trees).
Our rationale is that EBMT is being used as the main initial engine in a rapid-deployment MT system; if we will have time to develop deep analysis for a language, we will use the deep analysis as part of a KBMT engine instead of incorporating it into EBMT.
Second, our system returns all matching candidates with a minimum level of goodness, rather than trying to determine the optimal match.
The optimal matching problem can require rather extensive computational resources (Horiguchi and Franz, 1997); fortunately we can avoid it within our EBMT, because the MEMT system handles the selection process externally, using a trigram model of the target language, as described elsewhere (Frederking et al., 2000).
(This selection process is essentially identical to the stack decoder used in many speech recognizers to combine acoustic hypothesis scores with trigram language model probabilities.)
While the use of bilingual glossaries and dictionaries is a low-quality technique, its simplicity allows us to quickly and semi-automatically develop large databases using native speakers with no special training, allowing an initial rapid-deployment of an MT system even when parallel corpora are unavailable.
They are also useful for &quot;filling the cracks&quot; when gaps are discovered in our parallel corpus during system testing.
The training corpus for the EBMT engine consisted of the transscripts of the chaplain dialogs and their translations plus pre-existing parallel text from the Diplomat project (Frederking et al., 2000) and newly-acquired parallel text from the web.
The dictionary/glossary engine used both statistically-extracted translations and manually-created entries.
The English trigram model already existed, and had been generated from newswire and broadcast news transcripts.
Finally, the Croatian trigram model was built from the Croatian half of the EBMT corpus, some Croatian text found on the web, and the full text of some sixty novels and other Croatian literary works (in total, approximately six million words).
There appears to be an interesting match between the properties of spoken input and the properties of a rapid-deployment EBMT/MEMT system.
Compared to text translation, the input to speech translation is of much lower quality, due both to the word-errorrate of state-of-the-art real-time continuous speech recognition and to the disfiuencies present in spontaneous speech.
That is, spontaneous speakers often do not utter the complete, grammatical sentences that linguistic analysis typically expects.
As noted above, KBMT systems do produce better quality translation than the EBMT and glossary/dictionary engines employed in rapid-deployment EBMT/MEMT.
But the degraded quality of the input means that the quality difference between KBMT and rapid-deployment EBMT/MEMT is less important; given a string of words containing word errors and structural anomalies, it appears to us that a rapid-deployment EBMT/MEMT system can do about as well as a (much more costly) KBMT system.
This claim of course would require serious testing before it could be asserted as fact.
However, at least two other major spoken language translation systems, JANUS (Waibel, 1996; Levin et al., 2000) and SRI Cambridge's SLT (Rayner and Carter, 1997), have adopted some form of Multi-Engine MT.
Simply stringing together a recognizer, translator, and synthesizer does not make a very useful speech-to-speech translation system.
A good interface is necessary to make the parts work together in such a way that a user can actually derive benefit from it.
Using our experience from the earlier Diplomat system, we designed the Tongues interface to be asymmetric, with the Croatian side being as simple as possible, and any necessary complexity handled on the English side, since the chaplain would be trained and practiced in using the system.
Note that even the English side was not terribly complex.
We included a back-translation capability, to allow a user with no knowledge of the target language to better assess the quality of the translation.
(We could not use the approach of generating paraphrases from meaning representations, since the system does not use any meaning representations.)
We also included several user-requested features, such as built-in pre-recorded instructions and explanations for the Croatian (since the Croatian speaker is completely naive regarding the device and the chaplain's intentions), emergency key phrases (such as &quot;Don't mover ), and enhancements such as being able to modify the translation lexicon in the field, so that the system could be tuned to more specific tasks.
The final system ran on a Windows-based Toshiba Libretto, running at 200MHz with 192MB of memory.
At the time of the project (2000) this was the best combination of speed and size that was readily available.
The system was equipped with a custom touchscreen, so that the Croatian-speaker would not need to type or use a mouse at all.
Aware that the system might be used in situations where the non-English participant would be unfamiliar with computer technology, we included a microphone/speaker handset that looks like a conventional telephone handset.
This has the advantage of provided a close-talking microphone, thus making speech recognition easier, while coming in a form factor that will be familiar to most people.
Our design provides abundant opportunities for user error correction, in an effort to enable cooperative users to communicate well enough to accomplish significant tasks that they could not accomplish without the system (or a bilingual human interpreter), despite the error-prone nature of current speech recognition, broad-coverage rapid-development machine translation, and speech synthesis.
Determining whether we have met such a goal requires task-based evaluation; while error rates of components are useful information, the real system-level issue is whether communication is achieved, and at what level of effort.