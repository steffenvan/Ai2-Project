<newSection> Abstract Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel.
We present a new model of the translation process: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a QG defines a monolingual grammar that generates translations of T1.
The trees T2 allowed by this monolingual grammar are inspired by pieces of substructure in T1 and aligned to T1 at those points.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
As with other monolingual language models, we evaluate the cross-entropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence.
When evaluated on a word alignment task, QG matches standard baselines.