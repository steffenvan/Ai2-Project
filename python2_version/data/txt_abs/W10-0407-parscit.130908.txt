<newSection> 2 Questions of Validity Analysis of the DSP process that had been in place from 2000 to 2008 (under both sets of questions) showed that it had little validity (Gere et al., forthcoming).
It lacked substantive validity because of the time gap between completion of the survey and course selection; it lacked structural validity because the survey questions bore little relation to the construct of writing central in the FYW courses; it lacked validity of generalizability because only a small percentage of students followed the recommendation generated by the DSP survey; it lacked external validity because there was a low correlation between students’ scores on other measures and on the DSP survey; and it lacked consequential validity because the construct of writing operating in the Writing 100 course bore little relation to that emphasized in the DSP survey.
This analysis, combined with the fact that students’ perception of the importance of writing was influenced by the contrast between answering seven multiple choice questions and completing substantive tests in math, chemistry and foreign languages, led to a reconfiguring of the DSP process to include writing an essay and answering questions about that process as well as about literacy practices more generally.
Beginning in the fall of 2009, entering first-year students at our university write an evidence-based argument in response to a 3500 word publication.
Essays of entering students are submitted electronically and are delivered to individual instructors of students’ first writing class so that they become incorporated into instruction.
The Writing 100 course has been redesigned so that it is aligned with the construct of writing in the DSP process and in the FYW course.
In other words, we now have a large and growing corpus of student writing, and it would be helpful to think through how we might make best use of it, with regard to questions of validity as well as other issues.
The DSP essay corpus currently includes over 3500 student essays comprising over three million words, and by the end of August 2010, these numbers will double as a new cohort of students enters the University.
All the texts in the initial corpus were written by incoming first-year students in response to a prompt for an evidence-based argument about a Malcom Gladwell essay that discusses the difficulty of predicting which candidates will become good teachers—or quarterbacks or financial advisors.
Instructions included a recommendation to consider these features: focus or development around a clear central thesis or argument; structure or organization that elaborates on and supports the central argument; and evidence or well-chosen examples from the text to support claims.
In addition to this corpus, we have the potential to create a smaller corpus of student writing produced in first writing classes, both Writing 100 and courses that satisfy the First Year Writing Requirement, as well as personal narratives written as part of each student’s admission portfolio.
In coming years, we could also collect samples of writing across the entire undergraduate experience of a subset of students.
One of the questions we would like to discuss, then, centers on what decisions we should make about structuring additional corpora so as to take best advantage of the texts and materials available to us.
One clear direction for our work is to continue the investigation of validity to determine the extent to which the modifications in the DSP process and in Writing 100 enhance the validity of the placement process now in place.
In particular, it would be useful to learn more about the consequential validity of the DSP process since its main result or consequence is enrollment in either Writing 100 or a course that meets the First Year Writing Requirement.
Among the possible questions to investigate are these: The evidence-based argument is central in both contexts of first writing courses, and the construct of writing that operates in the DSP and in Writing 100 includes features of formal, purposeful, coherent, complex, audience-aware, and evidence-based writing.
A variety of rhetorical choices in academic writing help writers achieve these features; for example, we know from the work of Hyland (2005) that effective writers use textual signals to pull readers along their line of argument, so one approach in our research would be to compare the writing of students who elect Writing 100 and those who do not in terms of their use of textual signs that make the terms of their arguments clear.
Given student data and surveys we have access to, we also have the capacity to create subcorpora based on student grades and scores, English nativeness, student high school types, or students’ reported attributes such as confidence or writing experience.
Understanding how writers in various subgroups construct arguments will help answer key questions about the validity of the current form of DSP, and we welcome discussion of how quantitative linguistics can aid in that process.