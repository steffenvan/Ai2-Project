<newSection> Abstract A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper.
Lexical features and syntactic constraints are tightly integrated into a uniform linguistic structure called a SuperARV that is associated with a word in the lexicon.
The SuperARV language model reduces perplexity and word error rate compared to trigram, part-of-speech-based, and parser-based language models.
The relative contributions of the various knowledge sources to the strength of our model are also investigated by using constraint relaxation at the level of the knowledge sources.
We have found that although each knowledge source contributes to language model quality, lexical features are an outstanding contributor when they are tightly integrated with word identity and syntactic constraints.
Our investigation also suggests possible reasons for the reported poor performance of several probabilistic dependency grammar models in the literature.
these classes to compute n-gram probabilities.
Partof-Speech (POS) tags were initially used as classes by Jelinek (1990) in a conditional probabilistic model (which predicts the tag sequence for a word sequence first and then uses it to predict the word sequence): However, Jelinek’s POS LM is less effective at predicting word candidates than an n-gram word-based LM because it deletes important lexical information for predicting the next word.
Heeman’s (1998) POS LM achieves a perplexity reduction compared to a trigram LM by instead redefining the speech recognition problem as determining: The purpose of a language model (LM) is to determine the a priori probability of a word sequence w1, ..., wn, P(w1, ... , wn).
Language modeling is essential in a wide variety of applications; we focus on speech recognition in our research.
Although wordbased LMs (with bigram and trigram being the most common) remain the mainstay in many continuous speech recognition systems, recent efforts have explored a variety of ways to improve LM performance (Niesler and Woodland, 1996; Chelba et al., 1997; Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosenfeld, 2000; Goodman, 2001; Roark, 2001; Charniak, 2001).
Class-based LMs attempt to deal with data sparseness and generalize better to unseen word sequences by first grouping words into classes and then using where T is the POS sequence tN 1 associated with the word sequence W = wN1 given the speech utterance A.
The LM P(W, T) is a joint probabilistic model that accounts for both the sequence of words wN 1 and their tag assignments tN 1 by estimating the joint probabilities of words and tags: Johnson (2001) and Lafferty et al.
(2001) provide insight into why a joint model is superior to a conditional model.
Recently, there has been good progress in developing structured models (Chelba, 2000; Charniak, 2001; Roark, 2001) that incorporate syntactic information.
These LMs capture the hierarchical characteristics of a language rather than specific information about words and their lexical features (e.g., case, number).
In an attempt to incorporate even more knowledge into a structured LM, Goodman (1997) has developed a probabilistic feature grammar (PFG) that conditions not only on structure but also on a small set of grammatical features (e.g., number) and has achieved parse accuracy improvement.
Goodman’s work suggests that integrating lexical features with word identity and syntax would benefit LM predictiveness.
PFG uses only a small set of lexical features because it integrates those features at the level of the production rules, causing a significant increase in grammar size and a concomitant data sparsity problem that preclude the addition of richer features.
This sparseness problem can be addressed by associating lexical features directly with words.
We hypothesize that high levels of word prediction capability can be achieved by tightly integrating structural constraints and lexical features at the word level.
Hence, we develop a new dependency-grammar almost-parsing LM, SuperARV LM, which uses enriched tags called SuperARVs.
In Section 2, we introduce our SuperARV LM.
Section 3 compares the performance of the SuperARV LM to other LMs.
Section 4 investigates the knowledge source contributions by constraint relaxation.
Conclusions appear in Section 5.