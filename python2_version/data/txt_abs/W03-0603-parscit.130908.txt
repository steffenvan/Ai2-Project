<newSection> Abstract We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes.
We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework.
The implemented system selects the correct referents in response to a broad range of referring expressions for a large percentage of test cases.
In an analysis of the systemâ€™s successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.