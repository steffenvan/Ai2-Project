<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000068">
<title confidence="0.9998525">
A Pattern Matching Method for Finding Noun and Proper Noun
Translations from Noisy Parallel Corpora
</title>
<author confidence="0.997117">
Pascale Fung
</author>
<affiliation confidence="0.9958195">
Computer Science Department
Columbia University
</affiliation>
<address confidence="0.996043">
New York, NY 10027
</address>
<email confidence="0.999624">
pascale@cs.columbia.edu
</email>
<sectionHeader confidence="0.994818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993715">
We present a pattern matching method for
compiling a bilingual lexicon of nouns and
proper nouns from unaligned, noisy paral-
lel texts of Asian/Indo-European language
pairs. Tagging information of one lan-
guage is used. Word frequency and posi-
tion information for high and low frequency
words are represented in two different vec-
tor forms for pattern matching. New an-
chor point finding and noise elimination
techniques are introduced. We obtained
a 73.1% precision. We also show how the
results can be used in the compilation of
domain-specific noun phrases.
</bodyText>
<sectionHeader confidence="0.9356595" genericHeader="keywords">
1 Bilingual lexicon compilation
without sentence alignment
</sectionHeader>
<bodyText confidence="0.999926461538462">
Automatically compiling a bilingual lexicon of nouns
and proper nouns can contribute significantly to
breaking the bottleneck in machine translation and
machine-aided translation systems. Domain-specific
terms are hard to translate because they often do
not appear in dictionaries. Since most of these terms
are nouns, proper nouns or noun phrases, compiling
a bilingual lexicon of these word groups is an impor-
tant first step.
We have been studying robust lexicon compilation
methods which do not rely on sentence alignment.
Existing lexicon compilation methods (Kupiec 1993;
Smadja &amp; McKeown 1994; Kumano &amp; Hirakawa
1994; Dagan et al. 1993; Wu &amp; Xia 1994) all attempt
to extract pairs of words or compounds that are
translations of each other from previously sentence-
aligned, parallel texts. However, sentence align-
ment (Brown et al. 1991; Kay &amp; Roscheisen 1993;
Gale &amp; Church 1993; Church 1993; Chen 1993;
Wu 1994) is not always practical when corpora have
unclear sentence boundaries or with noisy text seg-
ments present in only one language.
Our proposed algorithm for bilingual lexicon ac-
quisition bootstraps off of corpus alignment proce-
dures we developed earlier (Fung &amp; Church 1994;
Fung &amp; McKeown 1994). Those procedures at-
tempted to align texts by finding matching word
pairs and have demonstrated their effectiveness for
Chinese/English and Japanese/English. The main
focus then was accurate alignment, but the proce-
dure produced a small number of word translations
as a by-product. In contrast, our new algorithm per-
forms a minimal alignment, to facilitate compiling a
much larger bilingual lexicon.
The paradigm for Fung &amp; Church (1994); Fung
&amp; McKeown (1994) is based on two main steps -
find a small bilingual primary lexicon, use the text
segments which contain some of the word pairs in
the lexicon as anchor points for alignment, align the
text, and compute a better secondary lexicon from
these partially aligned texts. This paradigm can be
seen as analogous to the Estimation-Maximization
step in Brown et al. (1991); Dagan et al. (1993); Wu
&amp; Xia (1994).
For a noisy corpus without sentence boundaries,
the primary lexicon accuracy depends on the robust-
ness of the algorithm for finding word translations
given no a priori information. The reliability of the
anchor points will determine the accuracy of the sec-
ondary lexicon. We also want an algorithm that
bypasses a long, tedious sentence or text alignment
step.
</bodyText>
<sectionHeader confidence="0.946244" genericHeader="method">
2 Algorithm overview
</sectionHeader>
<bodyText confidence="0.999767">
We treat the bilingual lexicon compilation problem
as a pattern matching problem - each word shares
some common features with its counterpart in the
translated text. We try to find the best repre-
sentations of these features and the best ways to
match them. We ran the algorithm on a small Chi-
nese/English parallel corpus of approximately 5760
unique English words.
The outline of the algorithm is as follows:
</bodyText>
<sectionHeader confidence="0.400376" genericHeader="method">
1. Tag the English half of the parallel text.
</sectionHeader>
<bodyText confidence="0.95169525">
In the first stage of the algorithm, only En-
glish words which are tagged as nouns or proper
nouns are used to match words in the Chinese
text.
</bodyText>
<page confidence="0.993808">
236
</page>
<listItem confidence="0.958139166666667">
2. Compute the positional difference vector
of each word. Each of these nouns or proper
nouns is converted from their positions in the
text into a vector.
3. Match pairs of positional difference vec-
tors, giving scores. All vectors from English
and Chinese are matched against each other by
Dynamic Time Warping (DTW).
4. Select a primary lexicon using the scores.
A threshold is applied to the DTW score of each
pair, selecting the most correlated pairs as the
first bilingual lexicon.
5. Find anchor points using the primary lex-
icon. The algorithm reconstructs the DTW
paths of these positional vector pairs, giving us
a set of word position points which are filtered
to yield anchor points. These anchor points are
used for compiling a secondary lexicon.
6. Compute a position binary vector for
each word using the anchor points. The re-
maining nouns and proper nouns in English and
all words in Chinese are represented in a non-
linear segment binary vector form from their po-
sitions in the text.
7. Match binary vectors to yield a secondary
lexicon. These vectors are matched against
each other by mutual information. A confidence
score is used to threshold these pairs. We ob-
tain the secondary bilingual lexicon from this
stage.
</listItem>
<bodyText confidence="0.9987518">
In Section 3, we describe the first four stages in
our algorithm, cumulating in a primary lexicon. Sec-
tion 4 describes the next anchor point finding stage.
Section 5 contains the procedure for compiling the
secondary lexicon.
</bodyText>
<sectionHeader confidence="0.8570455" genericHeader="method">
3 Finding high frequency bilingual
word pairs
</sectionHeader>
<bodyText confidence="0.975158769230769">
When the sentence alignments for the corpus are un-
known, standard techniques for extracting bilingual
lexicons cannot apply. To make matters worse, the
corpus might contain chunks of texts which appear
in one language but not in its translation&apos;, suggest-
ing a discontinuous mapping between some parallel
texts.
We have previously shown that using a vector rep-
resentation of the frequency and positional informa-
tion of a high frequency word was an effective way to
match it to its translation (Fung Sz McKeown 1994).
Dynamic Time Warping, a pattern recognition tech-
nique, was proposed as a good way to match these
I This was found to be the case in the Japanese trans-
lation of the AWK manual (Church et a/. 1993). The
Japanese AWK was also found to contain different pro-
gramming examples from the English version.
vectors. In our new algorithm, we use a similar po-
sitional difference vector representation and DTW
matching techniques. However, we improve on the
matching efficiency by installing tagging and statis-
tical filters. In addition, we not only obtain a score
from the DTW matching between pairs of words,
but we also reconstruct the DTW paths to get the
points of the best paths as anchor points for use in
later stages.
</bodyText>
<subsectionHeader confidence="0.998772">
3.1 Tagging to identify nouns
</subsectionHeader>
<bodyText confidence="0.9999814">
Since the positional difference vector representation
relies on the fact that words which are similar in
meaning appear fairly consistently in a parallel text,
this representation is best for nouns or proper nouns
because these are the kind of words which have con-
sistent translations over the entire text.
As ultimately we will be interested in finding
domain-specific terms, we can concentrate our ef-
fort on those words which are nouns or proper nouns
first. For this purpose, we tagged the English part of
the corpus by a modified POS tagger, and apply our
algorithm to find the translations for words which
are tagged as nouns, plural nouns or proper nouns
only. This produced a more useful list of lexicon and
again improved the speed of our program.
</bodyText>
<subsectionHeader confidence="0.998441">
3.2 Positional difference vectors
</subsectionHeader>
<bodyText confidence="0.999992210526316">
According to our previous findings (Fung &amp; McK-
eown 1994), a word and its translated counterpart
usually have some correspondence in their frequency
and positions although this correspondence might
not be linear. Given the position vector of a word
Ai] where the values of this vector are the positions
at which this word occurs in the corpus, one can
compute a positional difference vector V[i— 1} where
V[i — = p[ii — p[i — 1]. dim(V) is the dimension
of the vector which corresponds to the occurrence
count of the word.
For example, if positional difference vectors for the
word Governor and its translation in Chinese itt§
are plotted against their positions in the text, they
give characteristic signals such as shown in Figure 1.
The two vectors have different dimensions because
they occur with different frequencies. Note that the
two signals are shifted and warped versions of each
other with some minor noise.
</bodyText>
<subsectionHeader confidence="0.999346">
3.3 Matching positional difference vectors
</subsectionHeader>
<bodyText confidence="0.9999289">
The positional vectors have different lengths which
complicates the matching process. Dynamic Time
Warping was found to be a good way to match word
vectors of shifted or warped forms (Fung &amp; McK-
eown 1994). However, our previous algorithm only
used the DTW score for finding the most correlated
word pairs. Our new algorithm takes it one step fur-
ther by backtracking to reconstruct the DTW paths
and then automatically choosing the best points on
these DTW paths as anchor points.
</bodyText>
<page confidence="0.954415">
237
</page>
<figure confidence="0.991560357142857">
16000
14000
12000
10000
8000
16000
*govemocen.vec.ditr —
14000
12000
10000 I
8000
&amp;ALLA
100 150 260 SO 100 150 200 250
word position in text word position in text
</figure>
<figureCaption confidence="0.999966">
Figure 1: Positional difference signals showing similarity between Governor in English and Chinese
</figureCaption>
<bodyText confidence="0.999548">
For a given pair of vectors V1, V2, we attempt
to discover which point in V1 corresponds to which
point in V2 . If the two were not scaled, then po-
sition i in VI would correspond to position j in V2
where j/i is a constant. If we plot V1 against V2,
we can get a diagonal line with slope j/i. If they
occurred the same number of times, then every po-
sition i in V1 would correspond to one and only one
position j in V2. For non-identical vectors, DTW
traces the correspondences between all points in V1
and V2 (with no penalty for deletions or insertions).
Our DTW algorithm with path reconstruction is as
follows:
</bodyText>
<listItem confidence="0.999385090909091">
• Initialization
• ((1,1)
• (i,1) + co(i -1,1])
• ((1, j) co(1,j -1)
• minimum cost of moving
from a to b
• 11(1[cl - V 2[4
= 1,2, . . . ,N
= 1,2, . . . ,M
• dim(V1)
• dim(V2)
</listItem>
<equation confidence="0.890509090909091">
min [((1, m) + 1)]
1&lt;/&lt;3
argminK(/, m) + yon(i,1)]
1&lt;l&lt;3
1,2,... , N — 2
I, 2, . . ,M
• Termination
min [((/, in) + PN-1(2, 1)]
1&lt;l&lt;3
argminK(/, in) ± soN -1(i, j)]
1&lt;/&lt;3
</equation>
<listItem confidence="0.836437">
• Path reconstruction
</listItem>
<bodyText confidence="0.9995175">
In our algorithm, we reconstruct the DTW path
and obtain the points on the path for later use.
The DTW path for Governor/ iM is as shown
in Figure 2.
optimal path = i2) • • • im-2):7)
where in =
with iN = n = N — 1,N— 2,... ,1
We thresholded the bilingual word pairs obtained
from above stages in the algorithm and stored the
more reliable pairs as our primary bilingual lexicon.
</bodyText>
<subsectionHeader confidence="0.991743">
3.4 Statistical filters
</subsectionHeader>
<bodyText confidence="0.9978486">
If we have to exhaustively match all nouns and
proper nouns against all Chinese words, the match-
ing will be very expensive since it involves comput-
ing all possible paths between two vectors, and then
backtracking to find the optimal path, and doing this
for all English/Chinese word pairs in the texts. The
complexity of DTW is e(NM) and the complexity
of the matching is e(IJNM) where I is the number
of nouns and proper nouns in the English text, J is
the number of unique words in the Chinese text, N
is the occurrence count of one English word and M
the occurrence count of one Chinese word.
We previously used some frequency difference con-
straints and starting point constraints (Fung 86
McKeown 1994). Those constraints limited the
</bodyText>
<figure confidence="0.995165777777778">
((c,d)
for i
• Recursion
Tri)
61+1(M)
for n
and m
j)
i‘r(j)
</figure>
<page confidence="0.667012">
238
</page>
<figureCaption confidence="0.999666">
Figure 2: Dynamic Time Warping path for Governor in English and Chinese
</figureCaption>
<bodyText confidence="0.999920333333333">
number of the pairs of vectors to be compared by
DTW. For example, low frequency words are not
considered since their positional difference vectors
would not contain much information. We also ap-
ply these constraints in our experiments. However,
there is still many pairs of words left to be compared.
To improve the computation speed, we constrain
the vector pairs further by looking at the Euclidean
distance e of their means and standard deviations:
</bodyText>
<equation confidence="0.993864">
E = Omi — m2)2 + (al — 0&apos;2)2
</equation>
<bodyText confidence="0.99988875">
If their Euclidean distance is higher than a cer-
tain threshold, we filter the pair out and do not use
DTW matching on them. This process eliminated
most word pairs. Note that this Euclidean distance
function helps to filter out word pairs which are very
different from each other, but it is not discriminative
enough to pick out the best translation of a word.
So for word pairs whose Euclidean distance is below
the threshold, we still need to use DTW matching
to find the best translation. However, this Euclidean
distance filtering greatly improved the speed of this
stage of bilingual lexicon compilation.
</bodyText>
<sectionHeader confidence="0.8887455" genericHeader="method">
4 Finding anchor points and
eliminating noise
</sectionHeader>
<bodyText confidence="0.99998275">
Since the primary lexicon after thresholding is rela-
tively small, we would like to compute a secondary
lexicon including some words which were not found
by DTW. At stage 5 of our algorithm, we try to
find anchor points on the DTW paths which divide
the texts into multiple aligned segments for compil-
ing the secondary lexicon. We believe these anchor
points are more reliable than those obtained by trac-
ing all the words in the texts.
For every word pair from this lexicon, we had ob-
tained a DTW score and a DTW path. If we plot the
points on the DTW paths of all word pairs from the
lexicon, we get a graph as in the left hand side of Fig-
ure 3. Each point (i, j) on this graph is on the DTW
path(v1, v2) where vi is from English words in the
lexicon and v2 is from the Chinese words in the lexi-
con. The union effect of all these DTW paths shows
a salient line approximating the diagonal. This line
can be thought of the text alignment path. Its de-
parture from the diagonal illustrates that the texts
of this corpus are not identical nor linearly aligned.
Since the lexicon we computed was not perfect,
we get some noise in this graph. Previous align-
ment methods we used such as Church (1993); Fung
&amp; Church (1994); Fung &amp; McKeown (1994) would
bin the anchor points into continuous blocks for a
rough alignment. This would have a smoothing ef-
fect. However, we later found that these blocks of
anchor points are not precise enough for our Chi-
nese/English corpus. We found that it is more ad-
vantageous to increase the overall reliability of an-
chor points by keeping the highly reliable points and
discarding the rest.
From all the points on the union of the DTW
paths, we filter out the points by the following con-
ditions: If the point (i, j) satisfies
</bodyText>
<equation confidence="0.97050225">
(slope constraint) j/i &gt; 600* N[0]
(window size constraint) i &gt;= 25 + iprevious
(continuity constraint) iprevious
(offset constraint) jjprevious &gt; 500
</equation>
<bodyText confidence="0.9993121">
then the point (i, j) is noise and is discarded.
After filtering, we get points such as shown in the
right hand side of Figure 3. There are 388 highly re-
liable anchor points. They divide the texts into 388
segments. The total length of the texts is around
100000, so each segment has an average window size
of 257 words which is considerably longer than a sen-
tence length; thus this is a much rougher alignment
than sentence alignment, but nonetheless we still get
a bilingual lexicon out of it.
</bodyText>
<page confidence="0.990354">
239
</page>
<figure confidence="0.99960968">
100000
90000
80000
70000
60000
50000
40000
30000
20000
10000
100000
90000
80000
70000
60000
50000
40000
30000
20000
10000
0
0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000
liltered.dtve.pee •..
rk°
20000 40000 60000 80000 100000 120000
</figure>
<figureCaption confidence="0.999947">
Figure 3: DTW path reconstruction output and the anchor points obtained after filtering
</figureCaption>
<bodyText confidence="0.999759578947369">
The constants in the above conditions are cho-
sen roughly in proportion to the corpus size so that
the filtered picture looks close to a clean, diagonal
line. This ensures that our development stage is still
unsupervised. We would like to emphasize that if
they were chosen by looking at the lexicon output
as would be in a supervised training scenario, then
One should evaluate the output on an independent
test corpus.
Note that if one chunk of noisy data appeared in
textl but not in text2, this part would be segmented
between two anchor points (i, j) and (u, v). We know
point i is matched to point j, and point u to point
v, the texts between these two points are matched
but we do not make any assumption about how this
segment of texts are matched. In the extreme case
where i = to, we know that the text between j and
v is noise. We have at this point a segment-aligned
parallel corpus with noise elimination.
</bodyText>
<sectionHeader confidence="0.797658" genericHeader="method">
5 Finding low frequency bilingual
word pairs
</sectionHeader>
<bodyText confidence="0.9995568">
Many nouns and proper nouns were not translated in
the previous stages of our algorithm. They were not
in the first lexicon because their frequencies were too
low to be well represented by positional difference
vectors.
</bodyText>
<subsectionHeader confidence="0.81953">
5.1 Non-linear segment binary vectors
</subsectionHeader>
<bodyText confidence="0.999944724137931">
In stage 6, we represent the positional and frequency
information of low frequency words by a binary vec-
tor for fast matching.
The 388 anchor points (95, 10), (139, 131), ,
(98809, 93251) divide the two texts into 388 non-
linear segments. Textl is segmented by the points
(95, 139,... , 98586, 98809) and text2 is segmented
by the points (10, 131, ... ,90957, 93251).
For the nouns we are interested in finding the
translations for, we again look at the position
vectors. For example, the word prosperity oc-
curred seven times in the English text. Its posi-
tion vector is (2178, 5322, ... , 86521, 95341) . We
convert this position vector into a binary vector
V1 of 388 dimensions where V1[i] = 1 if pros-
perity occured within the ith segment, V1[i] =
0 otherwise. For prosperity, V1{1 = 1 where
i = 20, 27, 41, 47, 193, 321, 360. The Chinese trans-
lation for prosperity is /tV . Its position vec-
tor is (1955, 5050, ... ,88048). Its binary vector is
V2[i] = 1 where i= 14, 29, 41, 47, 193, 275, 321, 360.
We can see that these two vectors share five segments
in common.
We compute the segment vector for all English
nouns and proper nouns not found in the first lex-
icon and whose frequency is above two. Words oc-
curring only once are extremely hard to translate
although our algorithm was able to find some pairs
which occurred only once.
</bodyText>
<subsectionHeader confidence="0.998858">
5.2 &apos;Binary vector correlation measure
</subsectionHeader>
<bodyText confidence="0.997691">
To match these binary vectors V1 with their coun-
terparts in Chinese V2, we use a mutual information
score m.
</bodyText>
<equation confidence="0.979551222222222">
Pr(V1)
Pr(V2)
Pr(V1, V2)
where L = dim(V1) = dim(V2)
Pr(V1, V2)
°g2 Pr(V1) Pr(V2)
freq(Vl[i] = 1)
freq(V2ri] = 1)
freq(Vl[i] = V 2[i] = 1)
</equation>
<page confidence="0.986298">
240
</page>
<bodyText confidence="0.99970225">
If prosperity and Mgk occurred in the same eight
segments, their mutual information score would be
5.6. If they never occur in the same segments, their
m would be negative infinity. Here, for prosperity/2
m = 5.077 which shows that these two words are
indeed highly correlated.
The t-score was used as a confidence measure. We
keep pairs of words if their t &gt; 1.65 where
</bodyText>
<equation confidence="0.999026666666667">
Pr(V1, V2) — Pr(V1) Pr(V2)
t
Pr(V1, V2)
</equation>
<bodyText confidence="0.928859">
For prosperity/, t = 2.33 which shows that
their correlation is reliable.
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999239216216216">
The English half of the corpus has 5760 unique words
containing 2779 nouns and proper nouns. Most
of these words occurred only once. We carried
out two sets of evaluations, first counting only the
best matched pairs, then counting top three Chinese
translations for an English word. The top N candi-
date evaluation is useful because in a machine-aided
translation system, we could propose a list of up to,
say, ten candidate translations to help the transla-
tor. We obtained the evaluations of three human
judges (E1-E3). Evaluator El is a native Cantonese
speaker, E2 a Mandarin speaker, and E3 a speaker of
both languages. The results are shown in Figure 6.
The average accuracy for all evaluators for both
sets is 73.1%. This is a considerable improvement
from our previous algorithm (Fung &amp; McKeown
1994) which found only 32 pairs of single word trans-
lation. Our program also runs much faster than
other lexicon-based alignment methods.
We found that many of the mistaken transla-
tions resulted from insufficient data suggesting that
we should use a larger size corpus in our future
work. Tagging errors also caused some translation
mistakes. English words with multiple senses also
tend to be wrongly translated at least in part (e.g.,
means). There is no difference between capital let-
ters and small letters in Chinese, and no difference
between singular and plural forms of the same term.
This also led to some error in the vector represen-
tation. The evaluators&apos; knowledge of the language
and familiarity with the domain also influenced the
results.
Apart from single word to single word transla-
tion such as Governor/ eV and prosperity/,
we also found many single word translations which
show potential towards being translated as com-
pound domain-specific terms such as follows:
</bodyText>
<listItem confidence="0.827841">
• finding Chinese words: Chinese texts do not
</listItem>
<bodyText confidence="0.93145752">
have word boundaries such as space in English,
therefore our text was tokenized into words by a
statistical Chinese tokenizer (Fung &amp; Wu 1994).
Tokenizer error caused some Chinese characters
to be not grouped together as one word. Our
program located some of these words. For ex-
ample, Green was aligned to 0, and which
suggests that 4tr, could be a single Chinese
word. It indeed is the name for Green Paper —
a government document.
• compound noun translations: carbon could
be translated as kX, and monoxide as —*IL. If
carbon monoxide were translated separately, we
would get V( . However, our algorithm
found both carbon and monoxide to be most
likely translated to the single Chinese word --&amp;quot;a
ILTIX which is the correct translation for carbon
monoxide.
The words Legislative and Council were both
matched to MA and similarly we can de-
duce that Legislative Council is a compound
noun/collocation. The interesting fact here is,
Council is also matched toin. So we can deduce
that MAR should be a single Chinese word cor-
responding to Legislative Council.
</bodyText>
<listItem confidence="0.659671">
• slang: Some word pairs seem unlikely to be
translations of each other, such as collusion and
its first three candidates fi(it pull), M( cal),
(tail). Actually pulling the cat&apos;s tail is Can-
tonese slang for collusion.
</listItem>
<bodyText confidence="0.999846083333333">
The word gweilo is not a conventional English
word and cannot be found in any dictionary
but it appeared eleven times in the text. It
was matched to the Cantonese characters 0,
1, and it which separately mean vulgar/folk,
name/title, ghost and male. onilygg means
the colloquial term gweilo. Gweilo in Cantonese
is actually an idiom referring to a male west-
erner that originally had pejorative implica-
tions. This word reflects a certain cultural con-
text and cannot be simply replaced by a word
to word translation.
</bodyText>
<listItem confidence="0.573888">
• collocations: Some word pairs such as projects
and ME (houses) are not direct translations.
</listItem>
<bodyText confidence="0.887008647058823">
However, they are found to be constituent
words of collocations — the Housing Projects (by
the Hong Kong Government).Both Cross and
Harbour are translated to a(sea bottom), and
then to 2(tunnel), not a very literal transla-
tion. Yet, the correct translation for oatia
is indeed the Cross Harbor Tunnel and not the
Sea Bottom Tunnel.
The words Hong and Kong are both translated
into 4A, indicating Hong Kong is a compound
name.
Basic and Law are both matched to M2KiA, so
we know the correct translation for a2pA is
Basic Law which is a compound noun.
• proper names In Hong Kong, there is .a
specific system for the transliteration of Chi-
nese family names into English. Our algo-
</bodyText>
<page confidence="0.98964">
241
</page>
<table confidence="0.997488625">
lexicons total word pairs correct pairs accuracy
El E2 E3 El E2 E3
primary(1) 128 101 107 90 78.9% 83.6% 70.3%
secondary(1) 533 352 388 382 66.0% 72.8% 71.7%
total(1) 661 453 495 472 68.5% 74.9% 71.4%
primary(3) 128 112 101 99 87.5% 78.9% 77.3%
secondary(3) 533 401 368 398 75.2% 69.0% 74.7%
total(3) 661 513 469 497 77.6% 71.0% 75.2%
</table>
<figureCaption confidence="0.961828">
Figure 4: Bilingual lexicon compilation results
</figureCaption>
<bodyText confidence="0.619996">
rithm found a handful of these such as Fung/gc,
WonglA, Poonla, Lam/4, Tam/, etc.
</bodyText>
<sectionHeader confidence="0.991455" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969571428571">
Our algorithm bypasses the sentence alignment step
to find a bilingual lexicon of nouns and proper nouns.
Its output shows promise for compilation of domain-
specific, technical and regional compounds terms. It
has shown effectiveness in computing such a lexicon
from texts with no sentence boundary information
and with noise; fine-grain sentence alignment is not
necessary for lexicon compilation as long as we have
highly reliable anchor points. Compared to other
word alignment algorithms, it does not need a pri-
ori information. Since EM-based word alignment
algorithms using random initialization can fall into
local maxima, our output can also be used to pro-
vide a better initializing basis for EM methods. It
has also shown promise for finding noun phrases in
English and Chinese, as well as finding new Chinese
words which were not tokenized by a Chinese word
tokenizer. We are currently working on identifying
full noun phrases and compound words from noisy
parallel corpora with statistical and linguistic infor-
mation.
</bodyText>
<sectionHeader confidence="0.998354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999460857142857">
BROWN, P., J. LAI, &amp; R. MERCER. 1991. Aligning
sentences in parallel corpora. In Proceedings of
the 29th Annual Conference of the Association
for Computational Linguistics.
CHEN, STANLEY. 1993. Aligning sentences in bilin-
gual corpora using lexical information. In Pro-
ceedings of the 31st Annual Conference of the
Association for Computational Linguistics, 9-
16, Columbus, Ohio.
CHURCH, K., I. DAGAN, W. GALE, P. FUNG,
J. HELFMAN, &amp; B. SATISH. 1993. Aligning par-
allel texts: Do methods developed for English-
French generalize to Asian languages? In Pro-
ceedings of Pacific Asia Conference on Formal
and Computational Linguistics.
CHURCH, KENNETH. 1993. Char_align: A program
for aligning parallel texts at the character level.
In Proceedings of the 31st Annual Conference of
the Association for Computational Linguistics,
1-8, Columbus, Ohio.
DAGAN, IDO, KENNETH W. CHURCH, &amp;
WILLIAM A. GALE. 1993. Robust bilingual
word alignment for machine aided translation.
In Proceedings of the Workshop on Very Large
Corpora: Academic and Industrial Perspectives,
1-8, Columbus, Ohio.
FUNG, PASCALE &amp; KENNETH CHURCH. 1994. Kvec:
A new approach for aligning parallel texts. In
Proceedings of COLING 94, 1096-1102, Kyoto,
Japan.
FUNG, PASCALE &amp; KATHLEEN MCKEOWN. 1994.
Aligning noisy parallel corpora across language
groups: Word pair feature matching by dy-
namic time warping. In Proceedings of the
First Conference of the Association for Machine
Translation in the Americas, 81-88, Columbia,
Maryland.
FUNG, PASCALE 8.1 DEKAI Wu. 1994. Statistical
augmentation of a Chinese machine-readable
dictionary. In Proceedings of the 2nd Annual
Workshop on Very Large Corpora, 69-85, Ky-
oto, Japan.
GALE, WILLIAM A. &amp; KENNETH W. CHURCH.
1993. A program for aligning sentences in
bilingual corpora. Computational Linguistics,
19(1):75-102.
KAY, MARTIN &amp; MARTIN RoSCHEISEN. 1993. Text-
Translation alignment. Computational Linguis-
tics, 19(1):121-142.
KUMANO, AKIRA &amp; HIDEKI HIRAKAWA. 1994.
Building an mt dictionary from parallel texts
based on linguistic and statistical information.
In Proceedings of the 15th International Con-
ference on Computational Linguistics COLING
94, 76-81, Kyoto, Japan.
KUPIEC, JULIAN. 1993. An algorithm for finding
noun phrase correspondences in bilingual cor-
pora. In Proceedings of the 31st Annual Confer-
ence of the Association for Computational Lin-
guistics, 17-22, Columbus, Ohio.
SMADJA, FRANK &amp; KATHLEEN MCKEOWN. 1994.
Translating collocations for use in bilingual lex-
icons. In Proceedings of the ARPA Human
</reference>
<page confidence="0.966276">
242
</page>
<reference confidence="0.941273416666667">
Language Technology Workshop 94, Plainsboro,
New Jersey.
Wu, DEKAI. 1994. Aligning a parallel English-
Chinese corpus statistically with lexical criteria.
In Proceedings of the 32nd Annual Conference
of the Association for Computational Linguis-
tics, 80-87, Las Cruces, New Mexico.
Wu, DEKAI &amp; XUANYIN XIA. 1994. Learning
an English-Chinese lexicon from a parallel cor-
pus. In Proceedings of the First Conference of
the Association for Machine Translation in the
Americas, 206-213, Columbia, Maryland.
</reference>
<page confidence="0.999046">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989474">A Pattern Matching Method for Finding Noun and Proper Noun Translations from Noisy Parallel Corpora</title>
<author confidence="0.999674">Pascale Fung</author>
<affiliation confidence="0.999927">Computer Science Department Columbia University</affiliation>
<address confidence="0.999392">New York, NY 10027</address>
<email confidence="0.999209">pascale@cs.columbia.edu</email>
<abstract confidence="0.998264517948718">We present a pattern matching method for compiling a bilingual lexicon of nouns and proper nouns from unaligned, noisy parallel texts of Asian/Indo-European language pairs. Tagging information of one language is used. Word frequency and position information for high and low frequency words are represented in two different vector forms for pattern matching. New anchor point finding and noise elimination techniques are introduced. We obtained a 73.1% precision. We also show how the results can be used in the compilation of domain-specific noun phrases. 1 Bilingual lexicon compilation without sentence alignment Automatically compiling a bilingual lexicon of nouns and proper nouns can contribute significantly to breaking the bottleneck in machine translation and machine-aided translation systems. Domain-specific terms are hard to translate because they often do not appear in dictionaries. Since most of these terms are nouns, proper nouns or noun phrases, compiling a bilingual lexicon of these word groups is an important first step. We have been studying robust lexicon compilation methods which do not rely on sentence alignment. Existing lexicon compilation methods (Kupiec 1993; Smadja &amp; McKeown 1994; Kumano &amp; Hirakawa Dagan al. Wu &amp; Xia 1994) all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts. However, sentence align- (Brown al. Kay &amp; Roscheisen 1993; Gale &amp; Church 1993; Church 1993; Chen 1993; Wu 1994) is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language. Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier (Fung &amp; Church 1994; Fung &amp; McKeown 1994). Those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for Chinese/English and Japanese/English. The main focus then was accurate alignment, but the procedure produced a small number of word translations as a by-product. In contrast, our new algorithm performs a minimal alignment, to facilitate compiling a much larger bilingual lexicon. The paradigm for Fung &amp; Church (1994); Fung &amp; McKeown (1994) is based on two main steps a small bilingual lexicon, the text segments which contain some of the word pairs in the lexicon as anchor points for alignment, align the and compute a better lexicon these partially aligned texts. This paradigm can be seen as analogous to the Estimation-Maximization in Brown al. Dagan al. Wu &amp; Xia (1994). For a noisy corpus without sentence boundaries, the primary lexicon accuracy depends on the robustness of the algorithm for finding word translations no priori The reliability of the anchor points will determine the accuracy of the secondary lexicon. We also want an algorithm that bypasses a long, tedious sentence or text alignment step. 2 Algorithm overview We treat the bilingual lexicon compilation problem as a pattern matching problem each word shares some common features with its counterpart in the translated text. We try to find the best representations of these features and the best ways to match them. We ran the algorithm on a small Chinese/English parallel corpus of approximately 5760 unique English words. The outline of the algorithm is as follows: the English half of the parallel text. In the first stage of the algorithm, only English words which are tagged as nouns or proper nouns are used to match words in the Chinese text. 236 2. Compute the positional difference vector each word. of these nouns or proper nouns is converted from their positions in the text into a vector. 3. Match pairs of positional difference vecgiving scores. vectors from English and Chinese are matched against each other by Dynamic Time Warping (DTW). 4. Select a primary lexicon using the scores. A threshold is applied to the DTW score of each pair, selecting the most correlated pairs as the first bilingual lexicon. 5. Find anchor points using the primary lexalgorithm reconstructs the DTW paths of these positional vector pairs, giving us a set of word position points which are filtered to yield anchor points. These anchor points are used for compiling a secondary lexicon. 6. Compute a position binary vector for word using the anchor points. remaining nouns and proper nouns in English and all words in Chinese are represented in a nonlinear segment binary vector form from their positions in the text. 7. Match binary vectors to yield a secondary vectors are matched against each other by mutual information. A confidence score is used to threshold these pairs. We obtain the secondary bilingual lexicon from this stage. In Section 3, we describe the first four stages in our algorithm, cumulating in a primary lexicon. Section 4 describes the next anchor point finding stage. Section 5 contains the procedure for compiling the secondary lexicon. 3 Finding high frequency bilingual word pairs When the sentence alignments for the corpus are unknown, standard techniques for extracting bilingual lexicons cannot apply. To make matters worse, the corpus might contain chunks of texts which appear in one language but not in its translation&apos;, suggesting a discontinuous mapping between some parallel texts. We have previously shown that using a vector representation of the frequency and positional information of a high frequency word was an effective way to match it to its translation (Fung Sz McKeown 1994). Dynamic Time Warping, a pattern recognition technique, was proposed as a good way to match these IThis was found to be the case in the Japanese transof the AWK manual (Church 1993). The Japanese AWK was also found to contain different programming examples from the English version. vectors. In our new algorithm, we use a similar positional difference vector representation and DTW matching techniques. However, we improve on the matching efficiency by installing tagging and statistical filters. In addition, we not only obtain a score from the DTW matching between pairs of words, but we also reconstruct the DTW paths to get the points of the best paths as anchor points for use in later stages. 3.1 Tagging to identify nouns Since the positional difference vector representation relies on the fact that words which are similar in meaning appear fairly consistently in a parallel text, this representation is best for nouns or proper nouns because these are the kind of words which have consistent translations over the entire text. As ultimately we will be interested in finding domain-specific terms, we can concentrate our effort on those words which are nouns or proper nouns first. For this purpose, we tagged the English part of the corpus by a modified POS tagger, and apply our algorithm to find the translations for words which are tagged as nouns, plural nouns or proper nouns only. This produced a more useful list of lexicon and again improved the speed of our program. 3.2 Positional difference vectors According to our previous findings (Fung &amp; McKeown 1994), a word and its translated counterpart usually have some correspondence in their frequency and positions although this correspondence might not be linear. Given the position vector of a word the values of this vector are the positions at which this word occurs in the corpus, one can a positional difference vector where — = 1]. dim(V) is the dimension of the vector which corresponds to the occurrence count of the word. For example, if positional difference vectors for the its translation in Chinese itt§ are plotted against their positions in the text, they give characteristic signals such as shown in Figure 1. The two vectors have different dimensions because they occur with different frequencies. Note that the two signals are shifted and warped versions of each other with some minor noise. 3.3 Matching positional difference vectors The positional vectors have different lengths which complicates the matching process. Dynamic Time Warping was found to be a good way to match word vectors of shifted or warped forms (Fung &amp; McKeown 1994). However, our previous algorithm only the for finding the most correlated word pairs. Our new algorithm takes it one step further by backtracking to reconstruct the DTW paths and then automatically choosing the best points on these DTW paths as anchor points.</abstract>
<address confidence="0.877861909090909">237 16000 14000 12000 10000 8000 16000 *govemocen.vec.ditr — 14000 12000 8000</address>
<phone confidence="0.690079">100 150 260 SO 100 150 200 250</phone>
<abstract confidence="0.99557918367347">word position in text word position in text 1: Positional difference signals showing similarity between English and Chinese For a given pair of vectors V1, V2, we attempt to discover which point in V1 corresponds to which point in V2 . If the two were not scaled, then po- VI would correspond to position V2 a constant. If we plot V1 against V2, can get a diagonal line with slope they occurred the same number of times, then every po- V1 would correspond to one and only one V2. For non-identical vectors, DTW traces the correspondences between all points in V1 and V2 (with no penalty for deletions or insertions). Our DTW algorithm with path reconstruction is as follows: • Initialization • ((1,1) • (i,1) + co(i -1,1]) • ((1, j) co(1,j -1) • minimum cost of moving a to • 11(1[cl - V 2[4 = 1,2, . . . ,N = 1,2, . . . ,M • dim(V1) • dim(V2) + 1&lt;/&lt;3 m) + 1&lt;l&lt;3 , 2, . . • Termination [((/, in) + PN-1(2,1)] 1&lt;l&lt;3 j)] 1&lt;/&lt;3 • Path reconstruction In our algorithm, we reconstruct the DTW path and obtain the points on the path for later use. DTW path for iM as shown in Figure 2. optimal path = • • • = = = 2,... ,1 We thresholded the bilingual word pairs obtained from above stages in the algorithm and stored the more reliable pairs as our primary bilingual lexicon. 3.4 Statistical filters If we have to exhaustively match all nouns and proper nouns against all Chinese words, the matching will be very expensive since it involves computing all possible paths between two vectors, and then backtracking to find the optimal path, and doing this for all English/Chinese word pairs in the texts. The of DTW is the complexity the matching is the number nouns and proper nouns in the English text, number of unique words in the Chinese text, the occurrence count of one English word and the occurrence count of one Chinese word. We previously used some frequency difference constraints and starting point constraints (Fung 86 McKeown 1994). Those constraints limited the ((c,d) • Recursion Tri) 61+1(M) for n and m j) 238 2: Dynamic Time Warping path for English and Chinese number of the pairs of vectors to be compared by DTW. For example, low frequency words are not considered since their positional difference vectors would not contain much information. We also apply these constraints in our experiments. However, there is still many pairs of words left to be compared. To improve the computation speed, we constrain the vector pairs further by looking at the Euclidean distance e of their means and standard deviations: = Omi — + (al — If their Euclidean distance is higher than a certain threshold, we filter the pair out and do not use DTW matching on them. This process eliminated most word pairs. Note that this Euclidean distance function helps to filter out word pairs which are very different from each other, but it is not discriminative enough to pick out the best translation of a word. So for word pairs whose Euclidean distance is below the threshold, we still need to use DTW matching to find the best translation. However, this Euclidean distance filtering greatly improved the speed of this stage of bilingual lexicon compilation. 4 Finding anchor points and eliminating noise Since the primary lexicon after thresholding is relatively small, we would like to compute a secondary lexicon including some words which were not found by DTW. At stage 5 of our algorithm, we try to find anchor points on the DTW paths which divide the texts into multiple aligned segments for compiling the secondary lexicon. We believe these anchor points are more reliable than those obtained by tracing all the words in the texts. For every word pair from this lexicon, we had obtained a DTW score and a DTW path. If we plot the points on the DTW paths of all word pairs from the lexicon, we get a graph as in the left hand side of Fig- 3. Each point j) this graph is on the DTW where vi is from English words in the lexicon and v2 is from the Chinese words in the lexicon. The union effect of all these DTW paths shows a salient line approximating the diagonal. This line can be thought of the text alignment path. Its departure from the diagonal illustrates that the texts of this corpus are not identical nor linearly aligned. Since the lexicon we computed was not perfect, we get some noise in this graph. Previous alignment methods we used such as Church (1993); Fung &amp; Church (1994); Fung &amp; McKeown (1994) would bin the anchor points into continuous blocks for a rough alignment. This would have a smoothing effect. However, we later found that these blocks of anchor points are not precise enough for our Chinese/English corpus. We found that it is more advantageous to increase the overall reliability of anchor points by keeping the highly reliable points and discarding the rest. From all the points on the union of the DTW paths, we filter out the points by the following con- If the point (i, constraint) j/i 600* N[0] size constraint) &gt;= 25 + constraint) constraint) the point j) noise and is discarded. After filtering, we get points such as shown in the right hand side of Figure 3. There are 388 highly reliable anchor points. They divide the texts into 388 segments. The total length of the texts is around 100000, so each segment has an average window size of 257 words which is considerably longer than a sentence length; thus this is a much rougher alignment than sentence alignment, but nonetheless we still get a bilingual lexicon out of it.</abstract>
<address confidence="0.952603136363636">239 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 100000 90000 80000 70000 60000 50000 40000 30000 20000 10000 0</address>
<phone confidence="0.682485">0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000</phone>
<abstract confidence="0.990243593749999">liltered.dtve.pee •.. rk° 20000 40000 60000 80000 100000 120000 Figure 3: DTW path reconstruction output and the anchor points obtained after filtering The constants in the above conditions are chosen roughly in proportion to the corpus size so that the filtered picture looks close to a clean, diagonal line. This ensures that our development stage is still unsupervised. We would like to emphasize that if they were chosen by looking at the lexicon output as would be in a supervised training scenario, then One should evaluate the output on an independent test corpus. Note that if one chunk of noisy data appeared in textl but not in text2, this part would be segmented two anchor points j) (u, v). We know matched to point point u to point v, the texts between these two points are matched but we do not make any assumption about how this segment of texts are matched. In the extreme case where i = to, we know that the text between j and v is noise. We have at this point a segment-aligned parallel corpus with noise elimination. 5 Finding low frequency bilingual word pairs Many nouns and proper nouns were not translated in the previous stages of our algorithm. They were not in the first lexicon because their frequencies were too low to be well represented by positional difference vectors. 5.1 Non-linear segment binary vectors In stage 6, we represent the positional and frequency information of low frequency words by a binary vector for fast matching. The 388 anchor points (95, 10), (139, 131), , (98809, 93251) divide the two texts into 388 nonlinear segments. Textl is segmented by the points (95, 139,... , 98586, 98809) and text2 is segmented by the points (10, 131, ... ,90957, 93251). For the nouns we are interested in finding the translations for, we again look at the position For example, the word occurred seven times in the English text. Its position vector is (2178, 5322, ... , 86521, 95341) . We convert this position vector into a binary vector of 388 dimensions where V1[i] = 1 if proswithin the otherwise. For V1{1 = where = 27, 41, 47, 193, 321, 360. The Chinese translation for prosperity is /tV . Its position vector is (1955, 5050, ... ,88048). Its binary vector is = 1 where 29, 41, 47, 193, 275, 321, 360. We can see that these two vectors share five segments in common. We compute the segment vector for all English nouns and proper nouns not found in the first lexicon and whose frequency is above two. Words occurring only once are extremely hard to translate although our algorithm was able to find some pairs which occurred only once. 5.2 &apos;Binary vector correlation measure To match these binary vectors V1 with their counterparts in Chinese V2, we use a mutual information score m.</abstract>
<note confidence="0.826143">Pr(V1) Pr(V2) Pr(V1, V2) = = dim(V2) Pr(V1, V2) °g2Pr(V1) Pr(V2) freq(Vl[i] = 1) freq(V2ri] = 1) = 2[i] = 1) 240</note>
<abstract confidence="0.993897209677419">Mgk occurred in the same eight segments, their mutual information score would be 5.6. If they never occur in the same segments, their would be negative infinity. Here, for = which shows that these two words are indeed highly correlated. The t-score was used as a confidence measure. We pairs of words if their &gt; where Pr(V1, V2) — Pr(V1) Pr(V2) t Pr(V1, V2) t = which shows that their correlation is reliable. 6 Results The English half of the corpus has 5760 unique words containing 2779 nouns and proper nouns. Most of these words occurred only once. We carried out two sets of evaluations, first counting only the best matched pairs, then counting top three Chinese for an English word. The top candidate evaluation is useful because in a machine-aided translation system, we could propose a list of up to, say, ten candidate translations to help the translator. We obtained the evaluations of three human judges (E1-E3). Evaluator El is a native Cantonese speaker, E2 a Mandarin speaker, and E3 a speaker of both languages. The results are shown in Figure 6. The average accuracy for all evaluators for both sets is 73.1%. This is a considerable improvement from our previous algorithm (Fung &amp; McKeown 1994) which found only 32 pairs of single word translation. Our program also runs much faster than other lexicon-based alignment methods. We found that many of the mistaken translations resulted from insufficient data suggesting that we should use a larger size corpus in our future work. Tagging errors also caused some translation mistakes. English words with multiple senses also tend to be wrongly translated at least in part (e.g., is no difference between capital letters and small letters in Chinese, and no difference between singular and plural forms of the same term. This also led to some error in the vector representation. The evaluators&apos; knowledge of the language and familiarity with the domain also influenced the results. Apart from single word to single word translasuch Governor/ eV we also found many single word translations which show potential towards being translated as compound domain-specific terms such as follows: finding Chinese words: texts do not have word boundaries such as space in English, therefore our text was tokenized into words by a statistical Chinese tokenizer (Fung &amp; Wu 1994). Tokenizer error caused some Chinese characters to be not grouped together as one word. Our program located some of these words. For exaligned to 0, and which suggests that 4tr, could be a single Chinese word. It indeed is the name for Green Paper — a government document. compound noun translations: translated as monoxide translated separately, we get . our algorithm both be most likely translated to the single Chinese word --&amp;quot;a is the correct translation for monoxide. words both to similarly we can deduce that Legislative Council is a compound The interesting fact here also matched we can deduce be a single Chinese word corto Council. slang: word pairs seem unlikely to be of each other, such as first three candidates fi(it pull), M( the cat&apos;s tail Canslang for word not a conventional English word and cannot be found in any dictionary but it appeared eleven times in the text. It was matched to the Cantonese characters 0, separately mean ghost colloquial term gweilo. Gweilo Cantonese is actually an idiom referring to a male westerner that originally had pejorative implications. This word reflects a certain cultural context and cannot be simply replaced by a word to word translation. collocations: word pairs such as (houses) not direct translations. However, they are found to be constituent of collocations — the Projects Hong Kong Government).Both translated to to a very literal transla- Yet, the correct translation for indeed Cross Harbor Tunnel not Sea Bottom Tunnel. words both translated Kong a compound name. both matched to know the correct translation for Law is a compound noun. proper names Hong Kong, there is .a specific system for the transliteration of Chifamily names into English. Our algo- 241 lexicons total word pairs correct pairs accuracy El E2 E3 El E2 E3 primary(1) 128 101 107 90 78.9% 83.6% 70.3% secondary(1) 533 352 388 382 66.0% 72.8% 71.7% total(1) 661 453 495 472 68.5% 74.9% 71.4% primary(3) 128 112 101 99 87.5% 78.9% 77.3% secondary(3) 533 401 368 398 75.2% 69.0% 74.7% total(3) 661 513 469 497 77.6% 71.0% 75.2% Figure 4: Bilingual lexicon compilation results rithm found a handful of these such as Fung/gc,</abstract>
<address confidence="0.499071">Poonla, Lam/4, Tam/, 7 Conclusion</address>
<abstract confidence="0.998778142857143">Our algorithm bypasses the sentence alignment step to find a bilingual lexicon of nouns and proper nouns. Its output shows promise for compilation of domainspecific, technical and regional compounds terms. It has shown effectiveness in computing such a lexicon from texts with no sentence boundary information and with noise; fine-grain sentence alignment is not necessary for lexicon compilation as long as we have highly reliable anchor points. Compared to other alignment algorithms, it does not need pri- Since EM-based word alignment algorithms using random initialization can fall into local maxima, our output can also be used to provide a better initializing basis for EM methods. It has also shown promise for finding noun phrases in English and Chinese, as well as finding new Chinese words which were not tokenized by a Chinese word tokenizer. We are currently working on identifying full noun phrases and compound words from noisy parallel corpora with statistical and linguistic information.</abstract>
<title confidence="0.36868">References</title>
<author confidence="0.450312">Aligning</author>
<note confidence="0.826469">in parallel corpora. In of the 29th Annual Conference of the Association for Computational Linguistics. CHEN, STANLEY. 1993. Aligning sentences in bilincorpora using lexical information. In Proceedings of the 31st Annual Conference of the for Computational Linguistics, 9- 16, Columbus, Ohio. CHURCH, K., I. DAGAN, W. GALE, P. FUNG, J. HELFMAN, &amp; B. SATISH. 1993. Aligning parallel texts: Do methods developed for Englishgeneralize to Asian languages? In Proceedings of Pacific Asia Conference on Formal and Computational Linguistics. CHURCH, KENNETH. 1993. Char_align: A program for aligning parallel texts at the character level. of the 31st Annual Conference of</note>
<affiliation confidence="0.626074">the Association for Computational Linguistics,</affiliation>
<address confidence="0.908298">1-8, Columbus, Ohio.</address>
<email confidence="0.535013">DAGAN,IDO,KENNETHW.CHURCH,&</email>
<author confidence="0.488068">Robust bilingual</author>
<abstract confidence="0.7646385">word alignment for machine aided translation. of the Workshop on Very Large</abstract>
<affiliation confidence="0.816235">Corpora: Academic and Industrial Perspectives,</affiliation>
<address confidence="0.840331">1-8, Columbus, Ohio.</address>
<note confidence="0.8208218">FUNG, PASCALE &amp; KENNETH CHURCH. 1994. Kvec: A new approach for aligning parallel texts. In of COLING 94, Kyoto, Japan. PASCALE MCKEOWN. 1994.</note>
<abstract confidence="0.8687134">Aligning noisy parallel corpora across language groups: Word pair feature matching by dytime warping. In of the First Conference of the Association for Machine in the Americas, Columbia, Maryland. PASCALE Wu. 1994. Statistical augmentation of a Chinese machine-readable In of the 2nd on Very Large Corpora, Kyoto, Japan. WILLIAM A. W. CHURCH. 1993. A program for aligning sentences in corpora. Linguistics, 19(1):75-102. MARTIN RoSCHEISEN. 1993. Textalignment. Linguis- AKIRA HIRAKAWA. 1994. Building an mt dictionary from parallel texts based on linguistic and statistical information. of the 15th International Conference on Computational Linguistics COLING Kyoto, Japan. KUPIEC, JULIAN. 1993. An algorithm for finding noun phrase correspondences in bilingual cor-</abstract>
<affiliation confidence="0.702695">In of the 31st Annual Conference of the Association for Computational Lin-</affiliation>
<address confidence="0.966834">Columbus, Ohio.</address>
<note confidence="0.608192941176471">FRANK MCKEOWN. 1994. Translating collocations for use in bilingual lex- In of the ARPA Human 242 Technology Workshop 94, New Jersey. Wu, DEKAI. 1994. Aligning a parallel English- Chinese corpus statistically with lexical criteria. of the 32nd Annual Conference of the Association for Computational Linguis- Las Cruces, New Mexico. Wu, DEKAI &amp; XUANYIN XIA. 1994. Learning an English-Chinese lexicon from a parallel cor- In of the First Conference of the Association for Machine Translation in the Columbia, Maryland. 243</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P BROWN</author>
<author>J LAI</author>
<author>R MERCER</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics.</booktitle>
<marker>BROWN, LAI, MERCER, 1991</marker>
<rawString>BROWN, P., J. LAI, &amp; R. MERCER. 1991. Aligning sentences in parallel corpora. In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>STANLEY CHEN</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<location>Columbus, Ohio.</location>
<marker>CHEN, 1993</marker>
<rawString>CHEN, STANLEY. 1993. Aligning sentences in bilingual corpora using lexical information. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, 9-16, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K CHURCH</author>
<author>I DAGAN</author>
<author>W GALE</author>
<author>P FUNG</author>
<author>J HELFMAN</author>
<author>B SATISH</author>
</authors>
<title>Aligning parallel texts: Do methods developed for EnglishFrench generalize to Asian languages?</title>
<date>1993</date>
<booktitle>In Proceedings of Pacific Asia Conference on Formal and Computational Linguistics.</booktitle>
<marker>CHURCH, DAGAN, GALE, FUNG, HELFMAN, SATISH, 1993</marker>
<rawString>CHURCH, K., I. DAGAN, W. GALE, P. FUNG, J. HELFMAN, &amp; B. SATISH. 1993. Aligning parallel texts: Do methods developed for EnglishFrench generalize to Asian languages? In Proceedings of Pacific Asia Conference on Formal and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KENNETH CHURCH</author>
</authors>
<title>Char_align: A program for aligning parallel texts at the character level.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Columbus, Ohio.</location>
<marker>CHURCH, 1993</marker>
<rawString>CHURCH, KENNETH. 1993. Char_align: A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, 1-8, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IDO DAGAN</author>
<author>KENNETH W CHURCH</author>
<author>WILLIAM A GALE</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives,</booktitle>
<pages>1--8</pages>
<location>Columbus, Ohio.</location>
<marker>DAGAN, CHURCH, GALE, 1993</marker>
<rawString>DAGAN, IDO, KENNETH W. CHURCH, &amp; WILLIAM A. GALE. 1993. Robust bilingual word alignment for machine aided translation. In Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, 1-8, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PASCALE FUNG</author>
<author>KENNETH CHURCH</author>
</authors>
<title>Kvec: A new approach for aligning parallel texts.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING 94,</booktitle>
<pages>1096--1102</pages>
<location>Kyoto, Japan.</location>
<marker>FUNG, CHURCH, 1994</marker>
<rawString>FUNG, PASCALE &amp; KENNETH CHURCH. 1994. Kvec: A new approach for aligning parallel texts. In Proceedings of COLING 94, 1096-1102, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PASCALE FUNG</author>
<author>KATHLEEN MCKEOWN</author>
</authors>
<title>Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>81--88</pages>
<location>Columbia, Maryland.</location>
<marker>FUNG, MCKEOWN, 1994</marker>
<rawString>FUNG, PASCALE &amp; KATHLEEN MCKEOWN. 1994. Aligning noisy parallel corpora across language groups: Word pair feature matching by dynamic time warping. In Proceedings of the First Conference of the Association for Machine Translation in the Americas, 81-88, Columbia, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PASCALE FUNG</author>
</authors>
<title>Statistical augmentation of a Chinese machine-readable dictionary.</title>
<date>1994</date>
<booktitle>In Proceedings of the 2nd Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto, Japan.</location>
<marker>FUNG, 1994</marker>
<rawString>FUNG, PASCALE 8.1 DEKAI Wu. 1994. Statistical augmentation of a Chinese machine-readable dictionary. In Proceedings of the 2nd Annual Workshop on Very Large Corpora, 69-85, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WILLIAM A GALE</author>
<author>KENNETH W CHURCH</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<marker>GALE, CHURCH, 1993</marker>
<rawString>GALE, WILLIAM A. &amp; KENNETH W. CHURCH. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(1):75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MARTIN KAY</author>
<author>MARTIN RoSCHEISEN</author>
</authors>
<date>1993</date>
<booktitle>TextTranslation alignment. Computational Linguistics,</booktitle>
<pages>19--1</pages>
<marker>KAY, RoSCHEISEN, 1993</marker>
<rawString>KAY, MARTIN &amp; MARTIN RoSCHEISEN. 1993. TextTranslation alignment. Computational Linguistics, 19(1):121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AKIRA KUMANO</author>
<author>HIDEKI HIRAKAWA</author>
</authors>
<title>Building an mt dictionary from parallel texts based on linguistic and statistical information.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics COLING 94,</booktitle>
<pages>76--81</pages>
<location>Kyoto, Japan.</location>
<marker>KUMANO, HIRAKAWA, 1994</marker>
<rawString>KUMANO, AKIRA &amp; HIDEKI HIRAKAWA. 1994. Building an mt dictionary from parallel texts based on linguistic and statistical information. In Proceedings of the 15th International Conference on Computational Linguistics COLING 94, 76-81, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JULIAN KUPIEC</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<location>Columbus, Ohio.</location>
<marker>KUPIEC, 1993</marker>
<rawString>KUPIEC, JULIAN. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, 17-22, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>FRANK SMADJA</author>
<author>KATHLEEN MCKEOWN</author>
</authors>
<title>Translating collocations for use in bilingual lexicons.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop 94,</booktitle>
<location>Plainsboro, New Jersey.</location>
<marker>SMADJA, MCKEOWN, 1994</marker>
<rawString>SMADJA, FRANK &amp; KATHLEEN MCKEOWN. 1994. Translating collocations for use in bilingual lexicons. In Proceedings of the ARPA Human Language Technology Workshop 94, Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DEKAI Wu</author>
</authors>
<title>Aligning a parallel EnglishChinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="1746" citStr="Wu 1994" startWordPosition="266" endWordPosition="267">re nouns, proper nouns or noun phrases, compiling a bilingual lexicon of these word groups is an important first step. We have been studying robust lexicon compilation methods which do not rely on sentence alignment. Existing lexicon compilation methods (Kupiec 1993; Smadja &amp; McKeown 1994; Kumano &amp; Hirakawa 1994; Dagan et al. 1993; Wu &amp; Xia 1994) all attempt to extract pairs of words or compounds that are translations of each other from previously sentencealigned, parallel texts. However, sentence alignment (Brown et al. 1991; Kay &amp; Roscheisen 1993; Gale &amp; Church 1993; Church 1993; Chen 1993; Wu 1994) is not always practical when corpora have unclear sentence boundaries or with noisy text segments present in only one language. Our proposed algorithm for bilingual lexicon acquisition bootstraps off of corpus alignment procedures we developed earlier (Fung &amp; Church 1994; Fung &amp; McKeown 1994). Those procedures attempted to align texts by finding matching word pairs and have demonstrated their effectiveness for Chinese/English and Japanese/English. The main focus then was accurate alignment, but the procedure produced a small number of word translations as a by-product. In contrast, our new al</context>
<context position="20601" citStr="Wu 1994" startWordPosition="3560" endWordPosition="3561">lural forms of the same term. This also led to some error in the vector representation. The evaluators&apos; knowledge of the language and familiarity with the domain also influenced the results. Apart from single word to single word translation such as Governor/ eV and prosperity/, we also found many single word translations which show potential towards being translated as compound domain-specific terms such as follows: • finding Chinese words: Chinese texts do not have word boundaries such as space in English, therefore our text was tokenized into words by a statistical Chinese tokenizer (Fung &amp; Wu 1994). Tokenizer error caused some Chinese characters to be not grouped together as one word. Our program located some of these words. For example, Green was aligned to 0, and which suggests that 4tr, could be a single Chinese word. It indeed is the name for Green Paper — a government document. • compound noun translations: carbon could be translated as kX, and monoxide as —*IL. If carbon monoxide were translated separately, we would get V( . However, our algorithm found both carbon and monoxide to be most likely translated to the single Chinese word --&amp;quot;a ILTIX which is the correct translation for </context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>Wu, DEKAI. 1994. Aligning a parallel EnglishChinese corpus statistically with lexical criteria. In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics, 80-87, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DEKAI Wu</author>
<author>XUANYIN XIA</author>
</authors>
<title>Learning an English-Chinese lexicon from a parallel corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the First Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>206--213</pages>
<location>Columbia, Maryland.</location>
<marker>Wu, XIA, 1994</marker>
<rawString>Wu, DEKAI &amp; XUANYIN XIA. 1994. Learning an English-Chinese lexicon from a parallel corpus. In Proceedings of the First Conference of the Association for Machine Translation in the Americas, 206-213, Columbia, Maryland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>