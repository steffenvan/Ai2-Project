<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001314">
<title confidence="0.97902">
Extracting Paraphrases from Definition Sentences on the Web
</title>
<author confidence="0.389615">
Chikara Hashimoto∗ Kentaro Torisawa† Stijn De Saeger‡
</author>
<affiliation confidence="0.185258">
Jun’ichi Kazama§ Sadao Kurohashi¶
</affiliation>
<address confidence="0.6460825">
∗ † ‡ § National Institute of Information and Communications Technology
Kyoto, 619-0237, JAPAN
∗ ¶Graduate School of Informatics, Kyoto University
Kyoto, 606-8501, JAPAN
</address>
<email confidence="0.8912865">
{∗ ch,† torisawa, ‡ stijn,§ kazama}@nict.go.jp
¶kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.998582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999490625">
We propose an automatic method of extracting
paraphrases from definition sentences, which
are also automatically acquired from the Web.
We observe that a huge number of concepts
are defined in Web documents, and that the
sentences that define the same concept tend
to convey mostly the same information using
different expressions and thus contain many
paraphrases. We show that a large number
of paraphrases can be automatically extracted
with high precision by regarding the sentences
that define the same concept as parallel cor-
pora. Experimental results indicated that with
our method it was possible to extract about
300,000 paraphrases from 6 x 108 Web docu-
ments with a precision rate of about 94%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883464285714">
Natural language allows us to express the same in-
formation in many ways, which makes natural lan-
guage processing (NLP) a challenging area. Ac-
cordingly, many researchers have recognized that
automatic paraphrasing is an indispensable compo-
nent of intelligent NLP systems (Iordanskaja et al.,
1991; McKeown et al., 2002; Lin and Pantel, 2001;
Ravichandran and Hovy, 2002; Kauchak and Barzi-
lay, 2006; Callison-Burch et al., 2006) and have tried
to acquire a large amount of paraphrase knowledge,
which is a key to achieving robust automatic para-
phrasing, from corpora (Lin and Pantel, 2001; Barzi-
lay and McKeown, 2001; Shinyama et al., 2002;
Barzilay and Lee, 2003).
We propose a method to extract phrasal para-
phrases from pairs of sentences that define the same
concept. The method is based on our observation
that two sentences defining the same concept can
be regarded as a parallel corpus since they largely
convey the same information using different expres-
sions. Such definition sentences abound on the Web.
This suggests that we may be able to extract a large
amount of phrasal paraphrase knowledge from the
definition sentences on the Web.
For instance, the following two sentences, both of
which define the same concept “osteoporosis”, in-
clude two pairs of phrasal paraphrases, which are
indicated by underlines O and (D, respectively.
</bodyText>
<listItem confidence="0.5747345">
(1) a. Osteoporosis is a disease that (i decreases the
quantity of bone and 0 makes bones fragile.
</listItem>
<bodyText confidence="0.976043368421053">
b. Osteoporosis is a disease that (i reduces bone
mass and 0 increases the risk of bone fracture.
We define paraphrase as a pair of expressions be-
tween which entailment relations of both directions
hold. (Androutsopoulos and Malakasiotis, 2010).
Our objective is to extract phrasal paraphrases
from pairs of sentences that define the same con-
cept. We propose a supervised method that exploits
various kinds of lexical similarity features and con-
textual features. Sentences defining certain concepts
are acquired automatically on a large scale from the
Web by applying a quite simple supervised method.
Previous methods most relevant to our work
used parallel corpora such as multiple translations
of the same source text (Barzilay and McKeown,
2001) or automatically acquired parallel news texts
(Shinyama et al., 2002; Barzilay and Lee, 2003;
Dolan et al., 2004). The former requires a large
amount of manual labor to translate the same texts
</bodyText>
<page confidence="0.969837">
1087
</page>
<note confidence="0.979407">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.995792863636364">
in several ways. The latter would suffer from the ever, the precision of these methods has been rela-
fact that it is not easy to automatically retrieve large tively low. This is due to the fact that the evidence,
bodies of parallel news text with high accuracy. On i.e., distributional similarity, is just indirect evidence
the contrary, recognizing definition sentences for of paraphrase/entailment. Accordingly, these meth-
the same concept is quite an easy task at least for ods occasionally mistake antonymous pairs for para-
Japanese, as we will show, and we were able to find phrases/entailment pairs, since an expression and its
a huge amount of definition sentence pairs from nor- antonymous counterpart are also likely to have a
mal Web texts. In our experiments, about 30 million large distributional similarity. Another limitation of
definition sentence pairs were extracted from 6×108 these methods is that they can find only paraphrases
Web documents, and the estimated number of para- consisting of frequently observed expressions since
phrases recognized in the definition sentences using they must have reliable distributional similarity val-
our method was about 300,000, for a precision rate ues for expressions that constitute paraphrases.
of about 94%. Also, our experimental results show The second category is a parallel corpus approach
that our method is superior to well-known compet- (Barzilay and McKeown, 2001; Shinyama et al.,
ing methods (Barzilay and McKeown, 2001; Koehn 2002; Barzilay and Lee, 2003; Dolan et al., 2004).
et al., 2007) for extracting paraphrases from defini- Our method belongs to this category. This approach
tion sentence pairs. aligns expressions between two sentences in par-
Our evaluation is based on bidirectional check- allel corpora, based on, for example, the overlap
ing of entailment relations between paraphrases that of words/contexts. The aligned expressions are as-
considers the context dependence of a paraphrase. sumed to be paraphrases. In this approach, the ex-
Note that using definition sentences is only the pressions do not need to appear frequently in the
beginning of our research on paraphrase extraction. corpora. Furthermore, the approach rarely mistakes
We have a more general hypothesis that sentences antonymous pairs for paraphrases/entailment pairs.
fulfilling the same pragmatic function (e.g. defini- However, its limitation is the difficulty in preparing
tion) for the same topic (e.g. osteoporosis) convey a large amount of parallel corpora, as noted before.
mostly the same information using different expres- We avoid this by using definition sentences, which
sions. Such functions other than definition may in- can be easily acquired on a large scale from the Web,
clude the usage of the same Linux command, the as parallel corpora.
recipe for the same cuisine, or the description of re- Murata et al. (2004) used definition sentences in
lated work on the same research issue. two manually compiled dictionaries, which are con-
Section 2 describes related works. Section 3 siderably fewer in the number of definition sen-
presents our proposed method. Section 4 reports on tences than those on the Web. Thus, the coverage of
evaluation results. Section 5 concludes the paper. their method should be quite limited. Furthermore,
2 Related Work the precision of their method is much poorer than
The existing work for paraphrase extraction is cat- ours as we report in Section 4.
egorized into two groups. The first involves a dis- For a more extensive survey on paraphrasing
tributional similarity approach pioneered by Lin and methods, see Androutsopoulos and Malakasiotis
Pantel (2001). Basically, this approach assumes that (2010) and Madnani and Dorr (2010).
two expressions that have a large distributional simi- 3 Proposed method
larity are paraphrases. There are also variants of this Our method, targeting the Japanese language, con-
approach that address entailment acquisition (Geffet sists of two steps: definition sentence acquisition
and Dagan, 2005; Bhagat et al., 2007; Szpektor and and paraphrase extraction. We describe them below.
Dagan, 2008; Hashimoto et al., 2009). These meth- 3.1 Definition sentence acquisition
ods can be applied to a normal monolingual corpus, We acquire sentences that define a concept (defini-
and it has been shown that a large number of para- tion sentences) as in Example (2), which defines “骨
phrases or entailment rules could be extracted. How-
1088
W,M” (osteoporosis), from the 6 x 108 Web pages 92.2, and 91.4, respectively. Using the classifier,
(Akamine et al., 2010) and the Japanese Wikipedia. we acquired 1,925,052 positive sentences from all
(2) *CP.92:Lt.*�t6C4c--DICLJZ�AM. of the collected sentences. After adding definition
(Osteoporosis is a disease that makes bones fragile.) sentences from Wikipedia articles, which are typi-
Fujii and Ishikawa (2002) developed an unsuper- cally the first sentence of the body of each article
vised method to find definition sentences from the (Kazama and Torisawa, 2007), we obtained a total
Web using 18 sentential templates and a language of 2,141,878 definition sentence candidates, which
model constructed from an encyclopedia. On the covered 867,321 concepts ranging from weapons to
other hand, we developed a supervised method to rules of baseball. Then, we coupled two definition
achieve a higher precision. sentences whose defined concepts were the same
We use one sentential template and an SVM clas- and obtained 29,661,812 definition sentence pairs.
sifier. Specifically, we first collect definition sen- Obviously, our method is tailored to Japanese. For
tence candidates by a template “ˆNP 2:Lt.*”, where a language-independent method of definition acqui-
ˆ is the beginning of sentence and NP is the noun sition, see Navigli and Velardi (2010) as an example.
phrase expressing the concept to be defined followed
by a particle sequence, “2:” (comitative) and “Lt”
(topic) (and optionally followed by comma), as ex-
emplified in (2). As a result, we collected 3,027,101
sentences. Although the particle sequence tends
to mark the topic of the definition sentence, it can
also appear in interrogative sentences and normal as-
sertive sentences in which a topic is strongly empha-
sized. To remove such non-definition sentences, we
classify the candidate sentences using an SVM clas-
sifier with a polynominal kernel (d = 2).1 Since
Japanese is a head-final language and we can judge
whether a sentence is interrogative or not from the
last words in the sentence, we included morpheme
N-grams and bag-of-words (with the window size
of N) at the end of sentences in the feature set. The
features are also useful for confirming that the head
verb is in the present tense, which definition sen-
tences should be. Also, we added the morpheme
N-grams and bag-of-words right after the particle
sequence in the feature set since we observe that
non-definition sentences tend to have interrogative
related words like “�” (what) or “�#:” ((what) on
earth) right after the particle sequence. We chose 5
as N from our preliminary experiments.
Our training data was constructed from 2,911 sen-
tences randomly sampled from all of the collected
sentences. 61.1% of them were labeled as positive.
In the 10-fold cross validation, the classifier’s ac-
curacy, precision, recall, and F1 were 89.4, 90.7,
3.2 Paraphrase extraction
Paraphrase extraction proceeds as follows. First,
each sentence in a pair is parsed by the depen-
dency parser KNP2 and dependency tree frag-
ments that constitute linguistically well-formed con-
stituents are extracted. The extracted dependency
tree fragments are called candidate phrases here-
after. We restricted candidate phrases to predicate
phrases that consist of at least one dependency re-
lation, do not contain demonstratives, and in which
all the leaf nodes are nominal and all of the con-
stituents are consecutive in the sentence. KNP indi-
cates whether each candidate phrase is a predicate
based on the POS of the head morpheme. Then,
we check all the pairs of candidate phrases between
two definition sentences to find paraphrase pairs.3
In (1), repeated in (3), candidate phrase pairs to be
checked include ( Q decreases the quantity of bone,
O reduces bone mass), ( (D decreases the quantity
of bone, Q increases the risk of bone fracture), ( Q
makes bones fragile, (D reduces bone mass), and ( 0
makes bones fragile, ® increases the risk of bone
fracture).
(3) a. Osteoporosis is a disease that (i decreases the
quantity of bone and 0 makes bones fragile.
b. Osteoporosis is a disease that (i reduces bone
mass and 0 increases the risk of bone fracture.
2http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp.html.
3Our method discards candidate phrase pairs in which one
subsumes the other in terms of their character string, or the dif-
ference is only one proper noun like “toner cartridges that Ap-
ple Inc. made” and “toner cartridges that Xerox made.” Proper
nouns are recognized by KNP.
1We use SVMlight available at http://svmlight.
joachims.org/.
1089
f1 The ratio of the number of morphemes shared between two candidate phrases to the number of all of the morphemes in the two phrases.
f2 The ratio of the number of a candidate phrase’s morphemes, for which there is a morpheme with small edit distance (1 in our experiment) in
another candidate phrase, to the number of all of the morphemes in the two phrases. Note that Japanese has many orthographical variations
and edit distance is useful for identifying them.
f3 The ratio of the number of a candidate phrase’s morphemes, for which there is a morpheme with the same pronunciation in another candidate
phrase, to the number of all of the morphemes in the two phrases. Pronunciation is also useful for identifying orthographic variations.
Pronunciation is given by KNP.
f4 The ratio of the number of morphemes of a shorter candidate phrase to that of a longer one.
f5 The identity of the inflected form of the head morpheme between two candidate phrases: 1 if they are identical, 0 otherwise.
f6 The identity of the POS of the head morpheme between two candidate phrases: 1 or 0.
f7 The identity of the inflection (conjugation) of the head morpheme between two candidate phrases: 1 or 0.
f8 The ratio of the number of morphemes that appear in a candidate phrase segment of a definition sentence s1 and in a segment that is NOT a
part of the candidate phrase of another definition sentence s2 to the number of all of the morphemes of s1’s candidate phrase, i.e. how many
extra morphemes are incorporated into s1’s candidate phrase.
f9 The reversed (s1 � s2) version of f8.
f10 The ratio of the number of parent dependency tree fragments that are shared by two candidate phrases to the number of all of the parent de-
pendency tree fragments of the two phrases. Dependency tree fragments are represented by the pronunciation of their component morphemes.
f11 A variation of f10; tree fragments are represented by the base form of their component morphemes.
f12 A variation of f10; tree fragments are represented by the POS of their component morphemes.
f13 The ratio of the number of unigrams (morphemes) that appear in the child context of both candidate phrases to the number of all of the child
context morphemes of both candidate phrases. Unigrams are represented by the pronunciation of the morpheme.
f14 A variation of f13; unigrams are represented by the base form of the morpheme.
f15 A variation of f14; the numerator is the number of child context unigrams that are adjacent to both candidate phrases.
f16 The ratio of the number of trigrams that appear in the child context of both candidate phrases to the number of all of the child context
morphemes of both candidate phrases. Trigrams are represented by the pronunciation of the morpheme.
f17 Cosine similarity between two definition sentences from which a candidate phrase pair is extracted.
</bodyText>
<tableCaption confidence="0.996737">
Table 1: Features used by paraphrase classifier.
</tableCaption>
<bodyText confidence="0.9999167">
The paraphrase checking of candidate phrase
pairs is performed by an SVM classifier with a linear
kernel that classifies each pair of candidate phrases
into a paraphrase or a non-paraphrase.4 Candidate
phrase pairs are ranked by their distance from the
SVM’s hyperplane. Features for the classifier are
based on our observation that two candidate phrases
tend to be paraphrases if the candidate phrases them-
selves are sufficiently similar and/or their surround-
ing contexts are sufficiently similar. Table 1 lists the
features used by the classifier.5 Basically, they rep-
resent either the similarity of candidate phrases (f1-
9) or that of their contexts (f10-17). We think that
they have various degrees of discriminative power,
and thus we use the SVM to adjust their weights.
Figure 1 illustrates features f8-12, for which you
may need supplemental remarks. English is used for
ease of explanation. In the figure, f8 has a positive
value since the candidate phrase of s1 contains mor-
phemes “of bone”, which do not appear in the can-
</bodyText>
<footnote confidence="0.879498833333333">
4We use SVM-vert available at http://svmlight.
joachims.org/svm perf.html.
5In the table, the parent context of a candidate phrase con-
sists of expressions that appear in ancestor nodes of the candi-
date phrase in terms of the dependency structure of the sentence.
Child contexts are defined similarly.
</footnote>
<figureCaption confidence="0.999954">
Figure 1: Illustration of features f8-12.
</figureCaption>
<bodyText confidence="0.999982">
didate phrase of s2 but do appear in the other part
of s2, i.e. they are extra morphemes for s1’s candi-
date phrase. On the other hand, f9 is zero since there
is no such extra morpheme in s2’s candidate phrase.
Also, features f10-12 have positive values since the
two candidate phrases share two parent dependency
tree fragments, (that increases) and (offracture).
We have also tried the following features, which
we do not detail due to space limitation: the sim-
ilarity of candidate phrases based on semantically
similar nouns (Kazama and Torisawa, 2008), entail-
ing/entailed verbs (Hashimoto et al., 2009), and the
identity of the pronunciation and base form of the
head morpheme; N-grams (N=1,2,3) of child and
parent contexts represented by either the inflected
form, base form, pronunciation, or POS of mor-
</bodyText>
<page confidence="0.957975">
1090
</page>
<bodyText confidence="0.978373333333333">
Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (si, s2)
s1: Osteoporosis is a disease that reduces bone mass and makes bones
fragile.
s2: Osteoporosis is a disease that decreases the quantity of bone and
increases the risk of bone fracture.
si: Osteoporosis is a disease that decreases the quantity of bone and
makes bones fragile.
s2: Osteoporosis is a disease that reduces bone mass and increases
the risk of bone fracture.
</bodyText>
<figureCaption confidence="0.9115935">
Figure 2: Bidirectional checking of entailment relation (→) of p1 → p2 and p2 → p1. p1 is “reduces bone mass”
in s1 and p2 is “decreases the quantity of bone” in s2. p1 and p2 are exchanged between s1 and s2 to generate
corresponding paraphrased sentences si and s2. p1 → p2 (p2 → p1) is verified if s1 → si (s2 → s2) holds. In this
case, both of them hold. English is used for ease of explanation.
</figureCaption>
<bodyText confidence="0.999974981818182">
pheme; parent/child dependency tree fragments rep-
resented by either the inflected form, base form, pro-
nunciation, or POS; adjacent versions (cf. f15) of
N-gram features and parent/child dependency tree
features. These amount to 78 features, but we even-
tually settled on the 17 features in Table 1 through
ablation tests to evaluate the discriminative power
of each feature.
The ablation tests were conducted using training
data that we prepared. In preparing the training data,
we faced the problem that the completely random
sampling of candidate paraphrase pairs provided us
with only a small number of positive examples.
Thus, we automatically collected candidate para-
phrase pairs that were expected to have a high like-
lihood of being positive as examples to be labeled.
The likelihood was calculated by simply summing
all of the 78 feature values that we have tried, since
they indicate the likelihood of a given candidate
paraphrase pair’s being a paraphrase. Note that val-
ues of the features f8 and f9 are weighted with −1,
since they indicate the unlikelihood. Specifically,
we first randomly sampled 30,000 definition sen-
tence pairs from the 29,661,812 pairs, and collected
3,000 candidate phrase pairs that had the highest
likelihood from them. The manual labeling of each
candidate phrase pair (p1, p2) was based on bidirec-
tional checking of entailment relation, p1 —* p2 and
p2 —* p1, with p1 and p2 embedded in contexts.
This scheme is similar to the one proposed by
Szpektor et al. (2007). We adopt this scheme since
paraphrase judgment might be unstable between an-
notators unless they are given a particular context
based on which they make a judgment. As de-
scribed below, we use definition sentences as con-
texts. We admit that annotators might be biased by
this in some unexpected way, but we believe that
this is a more stable method than that without con-
texts. The labeling process is as follows. First, from
each candidate phrase pair (p1, p2) and its source
definition sentence pair (s1, s2), we create two para-
phrase sentence pairs (si, s2) by exchanging p1 and
p2 between s1 and s2. Then, annotators check if s1
entails si and s2 entails s2 so that entailment rela-
tions of both directions p1 —* p2 and p2 —* p1 are
checked. Figure 2 shows an example of bidirectional
checking. In this example, both entailment relations,
s1 —* si and s2 —* s2, hold, and thus the candidate
phrase pair (p1, p2) is judged as positive. We used
(p1, p2), for which entailment relations of both di-
rections held, as positive examples (1,092 pairs) and
the others as negative ones (1,872 pairs).6
We built the paraphrase classifier from the train-
ing data. As mentioned, candidate phrase pairs were
ranked by the distance from the SVM’s hyperplane.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="introduction">
4 Experiment
</sectionHeader>
<bodyText confidence="0.989293538461538">
In this paper, our claims are twofold.
I. Definition sentences on the Web are a treasure
trove of paraphrase knowledge (Section 4.2).
II. Our method of paraphrase acquisition from
definition sentences is more accurate than well-
known competing methods (Section 4.1).
We first verify claim II by comparing our method
with that of Barzilay and McKeown (2001) (BM
method), Moses7 (Koehn et al., 2007) (SMT
method), and that of Murata et al. (2004) (Mrt
method). The first two methods are well known for
accurately extracting semantically equivalent phrase
pairs from parallel corpora.8 Then, we verify claim
</bodyText>
<footnote confidence="0.997702">
6The remaining 36 pairs were discarded as they contained
garbled characters of Japanese.
7http://www.statmt.org/moses/
8As anonymous reviewers pointed out, they are unsuper-
vised methods and thus unable to be adapted to definition sen-
</footnote>
<page confidence="0.99207">
1091
</page>
<bodyText confidence="0.9987983">
I by comparing definition sentence pairs with sen-
tence pairs that are acquired from the Web using Ya-
hoo!JAPAN API9 as a paraphrase knowledge source.
In the latter data set, two sentences of each pair
are expected to be semantically similar regardless of
whether they are definition sentences. Both sets con-
tain 100,000 pairs.
Three annotators (not the authors) checked evalu-
ation samples. Fleiss’ kappa (Fleiss, 1971) was 0.69
(substantial agreement (Landis and Koch, 1977)).
</bodyText>
<subsectionHeader confidence="0.977741">
4.1 Our method vs. competing methods
</subsectionHeader>
<bodyText confidence="0.966887090909091">
In this experiment, paraphrase pairs are extracted
from 100,000 definition sentence pairs that are ran-
domly sampled from the 29,661,812 pairs. Before
reporting the experimental results, we briefly de-
scribe the BM, SMT, and Mrt methods.
BM method Given parallel sentences like multi-
ple translations of the same source text, the BM
method works iteratively as follows. First, it collects
from the parallel sentences identical word pairs and
their contexts (POS N-grams with indices indicat-
ing corresponding words between paired contexts)
as positive examples and those of different word
pairs as negative ones. Then, each context is ranked
based on the frequency with which it appears in pos-
itive (negative) examples. The most likely K posi-
tive (negative) contexts are used to extract positive
(negative) paraphrases from the parallel sentences.
Extracted positive (negative) paraphrases and their
morpho-syntactic patterns are used to collect addi-
tional positive (negative) contexts. All the positive
(negative) contexts are ranked, and additional para-
phrases and their morpho-syntactic patterns are ex-
tracted again. This iterative process finishes if no
further paraphrase is extracted or the number of iter-
ations reaches a predefined threshold T. In this ex-
periment, following Barzilay and McKeown (2001),
K is 10 and N is 1 to 3. The value of T is not given
in their paper. We chose 3 as its value based on our
preliminary experiments. Note that paraphrases ex-
tracted by this method are not ranked.
tences. Nevertheless, we believe that comparing these methods
with ours is very informative, since they are known to be accu-
rate and have been influential.
</bodyText>
<footnote confidence="0.886855">
9http://developer.yahoo.co.jp/webapi/
</footnote>
<bodyText confidence="0.999240956521739">
SMT method Our SMT method uses Moses
(Koehn et al., 2007) and extracts a phrase table, a
set of two phrases that are translations of each other,
given a set of two sentences that are translations of
each other. If you give Moses monolingual parallel
sentence pairs, it should extract a set of two phrases
that are paraphrases of each other. In this experi-
ment, default values were used for all parameters.
To rank extracted phrase pairs, we assigned each of
them the product of two phrase translation probabil-
ities of both directions that were given by Moses.
For other SMT methods, see Quirk et al. (2004) and
Bannard and Callison-Burch (2005) among others.
Mrt method Murata et al. (2004) proposed a
method to extract paraphrases from two manually
compiled dictionaries. It simply regards a difference
between two definition sentences of the same word
as a paraphrase candidate. Paraphrase candidates are
ranked according to an unsupervised scoring scheme
that implements their assumption. They assume that
a paraphrase candidate tends to be a valid paraphrase
if it is surrounded by infrequent strings and/or if it
appears multiple times in the data.
In this experiment, we evaluated the unsupervised
version of our method in addition to the supervised
one described in Section 3.2, in order to compare
it fairly with the other methods. The unsupervised
method works in the same way as the supervised
one, except that it ranks candidate phrase pairs by
the sum of all 17 feature values, instead of the dis-
tance from the SVM’s hyperplane. In other words,
no supervised learning is used. All the feature val-
ues are weighted with 1, except for f8 and f9, which
are weighted with −1 since they indicate the unlike-
lihood of a candidate phrase pair being paraphrases.
BM, SMT, Mrt, and the two versions of our method
were used to extract paraphrase pairs from the same
100,000 definition sentence pairs.
Evaluation scheme Evaluation of each para-
phrase pair (p1, p2) was based on bidirectional
checking of entailment relations p1 —* p2 and p2 —*
p1 in a way similar to the labeling of the training
data. The difference is that contexts for evaluation
are two sentences that are retrieved from the Web
and contain p1 and p2, instead of definition sen-
tences from which p1 and p2 are extracted. This
</bodyText>
<page confidence="0.991576">
1092
</page>
<bodyText confidence="0.999873666666667">
is intended to check whether extracted paraphrases
are also valid for contexts other than those from
which they are extracted. The evaluation proceeds
as follows. For the top m paraphrase pairs of each
method (in the case of the BM method, randomly
sampled m pairs were used, since the method does
not rank paraphrase pairs), we retrieved a sentence
pair (s1, s2) for each paraphrase pair (p1, p2) from
the Web, such that s1 contains p1 and s2 contains p2.
In doing so, we make sure that neither s1 nor s2 are
the definition sentences from which p1 and p2 are
extracted. For each method, we randomly sample
n samples from all of the paraphrase pairs (p1, p2)
for which both s1 and s2 are retrieved. Then, from
each (p1, p2) and (s1, s2), we create two paraphrase
sentence pairs (s01, s02) by exchanging p1 and p2 be-
tween s1 and s2. All samples, each consisting of
(p1, p2), (s1, s2), and (s01, s02), are checked by three
human annotators to determine whether s1 entails
s01 and s2 entails s02 so that entailment relations of
both directions are verified. In advance of evaluation
annotation, all the evaluation samples are shuffled
so that the annotators cannot find out which sample
is given by which method for fairness. We regard
each paraphrase pair as correct if at least two annota-
tors judge that entailment relations of both directions
hold for it. You may wonder whether only one pair
of sentences (s1, s2) is enough for evaluation since a
correct (wrong) paraphrase pair might be judged as
wrong (correct) accidentally. Nevertheless, we sup-
pose that the final evaluation results are reliable if
the number of evaluation samples is sufficient. In
this experiment, m is 5,000 and n is 200. We use
Yahoo!JAPAN API to retrieve sentences.
Graph (a) in Figure 3 shows a precision curve
for each method. Sup and Uns respectively indi-
cate the supervised and unsupervised versions of our
method. The figure indicates that Sup outperforms
all the others and shows a high precision rate of
about 94% at the top 1,000. Remember that this
is the result of using 100,000 definition sentence
pairs. Thus, we estimate that Sup can extract about
300,000 paraphrase pairs with a precision rate of
about 94%, if we use all 29,661,812 definition sen-
tence pairs that we acquired.
Furthermore, we measured precision after trivial
paraphrase pairs were discarded from the evaluation
samples of each method. A candidate phrase pair
</bodyText>
<table confidence="0.999651666666666">
Definition sentence pairs Sup Uns BM SMT Mrt
with trivial 1,381,424 24,049 9,562 18,184
without trivial 1,377,573 23,490 7,256 18,139
Web sentence pairs Sup Uns BM SMT Mrt
with trivial 277,172 5,101 4,586 4,978
without trivial 274,720 4,399 2,342 4,958
</table>
<tableCaption confidence="0.999233">
Table 2: Number of extracted paraphrases.
</tableCaption>
<bodyText confidence="0.9997783125">
(p1, p2) is regarded as trivial if the pronunciation is
the same between p1 and p2,10 or all of the con-
tent words contained in p1 are the same as those
of p2. Graph (b) gives a precision curve for each
method. Again, Sup outperforms the others too, and
maintains a precision rate of about 90% until the top
1,000. These results support our claim II.
The upper half of Table 2 shows the number of
extracted paraphrases with/without trivial pairs for
each method.11 Sup and Uns extracted many more
paraphrases. It is noteworthy that Sup performed the
best in terms of both precision rate and the number
of extracted paraphrases.
Table 3 shows examples of correct and incorrect
outputs of Sup. As the examples indicate, many of
the extracted paraphrases are not specific to defini-
tion sentences and seem very reusable. However,
there are few paraphrases involving metaphors or id-
ioms in the outputs due to the nature of definition
sentences. In this regard, we do not claim that our
method is almighty. We agree with Sekine (2005)
who claims that several different methods are re-
quired to discover a wider variety of paraphrases.
In graphs (a) and (b), the precision of the SMT
method goes up as rank goes down. This strange be-
havior is due to the scoring by Moses that worked
poorly for the data; it gave 1.0 to 82.5% of all the
samples, 38.8% of which were incorrect. We suspect
SMT methods are poor at monolingual alignment for
paraphrasing or entailment tasks since, in the tasks,
data is much noisier than that used for SMT. See
MacCartney et al. (2008) for similar discussion.
</bodyText>
<subsectionHeader confidence="0.985281">
4.2 Definition pairs vs. Web sentence pairs
</subsectionHeader>
<bodyText confidence="0.758872">
To collect Web sentence pairs, first, we randomly
sampled 1.8 million sentences from the Web corpus.
</bodyText>
<footnote confidence="0.88037075">
10There are many kinds of orthographic variants in Japanese,
which can be identified by their pronunciation.
11We set no threshold for candidate phrase pairs of each
method, and counted all the candidate phrase pairs in Table 2.
</footnote>
<page confidence="0.969837">
1093
</page>
<figure confidence="0.998457534482759">
Precision
Precision
0 1000 2000 3000 4000 5000
Top-N
0 1000 2000 3000 4000 5000
Top-N
1
0.8
0.6
0.4
0.2
0
’Sup_der
’Uns_def
’SMT der
BMder
’Mrt_def
0.8
0.6
0.4
0.2
0
1
’Sup_def_n’
’Uns_def_n’
’SMT def n’
’BMdefn’
’Mrt dein’
(a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases
Precision
0.8
0.6
0.4
0.2
0
1
’Sup_www n’
’Uns www n’
’SMT_www_n’
’BMwww n’
’Mr
M twww n’
0 1000 2000 3000 4000 5000
Top-N
0 1000 2000 3000 4000 5000
Top-N
’Sup_www:
’Uns www&apos;
’SMT_www’
’BM_www’
’Mrt_www’
Precision 1
0.8
0.6
0.4
0.2
0
(c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrases
</figure>
<figureCaption confidence="0.999569">
Figure 3: Precision curves of paraphrase extraction.
</figureCaption>
<table confidence="0.999933965517241">
Rank Paraphrase pair
13 メールアドレスにメールを送る (send a message to the e-mail address) ⇔ メールアドレスに電子メールを送る (send
an e-mail message to the e-mail address)
19 お客様の依頼による (requested by a customer) ⇔ お客様の委託による (commissioned by a customer)
70 企業の財政状況を表す (describe the fiscal condition of company) ⇔ 企業の財政状態を示す (indicate the fiscal state
of company)
112 インフォメーションを得る (get information) ⇔ ニュースを得る (get news)
656 きまりのことです (it is a convention) ⇔ ルールのことです (it is a rule)
841 地震のエネルギー規模をあらわす (represent the energy scale of earthquake) ⇔ 地震の規模を表す (represent the scale
of earthquake)
929 細胞を酸化させる (cause the oxidation of cells) ⇔ 細胞を老化させる (cause cellular aging)
1,553 角質を取り除く (remove dead skin cells) ⇔ 角質をはがす (peel off dead skin cells)
Correct 2,243 胎児の発育に必要だ (required for the development of fetus) ⇔ 胎児の発育成長に必要不可欠だ (indispensable for the
growth and development of fetus)
2,855 視力を矯正する (correct eyesight) ⇔ 視力矯正を行う (perform eyesight correction)
2,931 チャラにしてもらう (call it even) ⇔ 帳消しにしてもらう (call it quits)
3,667 ハードディスク上に蓄積される (accumulated on a hard disk) ⇔ ハードディスクドライブに保存される (stored on a
hard disk drive)
4,870 有害物質を排泄する (excrete harmful substance) ⇔ 有害毒素を排出する (discharge harmful toxin)
5,501 1つのCPUの内部に2つのプロセッサコアを搭載する (mount two processor cores on one CPU) ⇔ 1つのパッケー
ジに2つのプロセッサコアを集積する (build two processor cores into one package)
10,675 外貨を売買する (trade foreign currencies) ⇔ 通貨を交換する (exchange one currency for another)
112,819 派遣先企業の社員になる (become a regular staff member of the company where (s)he has worked as a temp) ⇔ 派遣
先に直接雇用される (employed by the company where (s)he has worked as a temp)
193,553 Webサイトにアクセスする (access Web sites) ⇔ WWWサイトを訪れる (visit WWW sites)
903 ブラウザに送信される (send to a Web browser) ⇔ パソコンに送信される (send to a PC)
Incorrect 2,530 調和をはかる (intend to balance) ⇔ リフレッシュを図る (intend to refresh)
3,008 消化酵素では消化できない (unable to digest with digestive enzymes) ⇔ 消化酵素で消化され難い (hard to digest with
digestive enzymes)
</table>
<tableCaption confidence="0.999805">
Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.
</tableCaption>
<page confidence="0.9633">
1094
</page>
<bodyText confidence="0.997551574468085">
We call them sampled sentences. Then, using Ya- 5 Conclusion
hoo!JAPAN API, we retrieved up to 20 snippets rele- We proposed a method of extracting paraphrases
vant to each sampled sentence using all of the nouns from definition sentences on the Web. From the ex-
in each sentence as a query. After that, each snippet perimental results, we conclude that the following
was split into sentences, which we call snippet sen- two claims of this paper are verified.
tences. We paired a sampled sentence and a snippet 1. Definition sentences on the Web are a treasure
sentence that was the most similar to the sampled trove of paraphrase knowledge.
sentence. Similarity is the number of nouns shared 2. Our method extracts many paraphrases from
by the two sentences. Finally, we randomly sampled the definition sentences on the Web accurately;
100,000 pairs from all the pairs. it can extract about 300,000 paraphrases from
Paraphrase pairs were extracted from the Web 6 x 108 Web documents with a precision rate
sentence pairs by using BM, SMT, Mrt and the su- of about 94%.
pervised and unsupervised versions of our method. Our future work is threefold. First, we will release
The features used with our methods were selected extracted paraphrases from all of the 29,661,812
from all of the 78 features mentioned in Section 3.2 definition sentence pairs that we acquired, after hu-
so that they performed well for Web sentence pairs. man annotators check their validity. The result will
Specifically, the features were selected by ablation be available through the ALAGIN forum.13
tests using training data that was tailored to Web Second, we plan to induce paraphrase rules
sentence pairs. The training data consisted of 2,741 from paraphrase instances. Though our method
sentence pairs that were collected in the same way as can extract a variety of paraphrase instances on
the Web sentence pairs and was labeled in the same a large scale, their coverage might be insufficient
way as described in Section 3.2. for real NLP applications since some paraphrase
Graph (c) of Figure 3 shows precision curves. We phenomena are highly productive. Therefore, we
also measured precision without trivial pairs in the need paraphrase rules in addition to paraphrase in-
same way as the previous experiment. Graph (d) stances. Barzilay and McKeown (2001) induced
shows the results. The lower half of Table 2 shows simple POS-based paraphrase rules from paraphrase
the number of extracted paraphrases with/without instances, which can be a good starting point.
trivial pairs for each method. Finally, as mentioned in Section 1, the work in
Note that precision figures of our methods in this paper is only the beginning of our research on
graphs (c) and (d) are lower than those of our meth- paraphrase extraction. We are trying to extract far
ods in graphs (a) and (b). Additionally, none of the more paraphrases from a set of sentences fulfilling
methods achieved a precision rate of 90% using Web the same pragmatic function (e.g. definition) for the
sentence pairs.12 We think that a precision rate of same topic (e.g. osteoporosis) on the Web. Such
at least 90% would be necessary if you apply auto- functions other than definition may include the us-
matically extracted paraphrases to NLP tasks with- age of the same Linux command, the recipe for the
out manual annotation. Only the combination of Sup same cuisine, or the description of related work on
and definition sentence pairs achieved that precision. the same research issue.
Also note that, for all of the methods, the numbers Acknowledgments
of extracted paraphrases from Web sentence pairs We would like to thank Atsushi Fujita, Francis
are fewer than those from definition sentence pairs. Bond, and all of the members of the Information
From all of these results, we conclude that our Analysis Laboratory, Universal Communication Re-
claim I is verified. search Institute at NICT.
12Precision of SMT is unexpectedly good. We found some
Web sentence pairs consisting of two mostly identical sentences
on rare occasions. The method worked relatively well for them.
13http://alagin.jp/
1095
</bodyText>
<sectionHeader confidence="0.99539" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998626914285714">
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122–129.
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entailment
methods. Journal of Artificial Intelligence Research,
38:135–187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL-2005), pages 597–
604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16–23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50–57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161–170.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the 2006 Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL 2006), pages 17–24.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350–
356.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.
Atsushi Fujii and Tetsuya Ishikawa. 2002. Extraction
and organization of encyclopedic knowledge informa-
tion using the World Wide Web (written in Japanese).
Institute of Electronics, Information, and Communica-
tion Engineers, J85-D-II(2):300–307.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107–114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172–1181.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu`ere. 1991. Lexical selection and paraphrase in
a meaning-text generation model. In C´ecile L. Paris,
William R. Swartout, and William C. Mann, editors,
Natural language generation in artificial intelligence
and computational linguistics, pages 293–312. Kluwer
Academic Press.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the 2006 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2006), pages
455–462.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 698–707, June.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL-08: HLT), pages 407–415.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2007), pages
177–180.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343–360.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
</reference>
<page confidence="0.787432">
1096
</page>
<reference confidence="0.9997039">
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2008), pages 802–811.
Nitin Madnani and Bonnie Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with columbia’s newsblaster. In Pro-
ceedings of the 2nd international conference on Hu-
man Language Technology Research, pages 280–285.
Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isa-
hara. 2004. Automatic paraphrase acquisition based
on matching of definition sentences in plural dictionar-
ies (written in Japanese). Journal ofNatural Language
Processing, 11(5):135–149.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
2010), pages 1318–1327.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pages 142–149.
Deepak Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 41–47.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of the Third International Workshop on
Paraphrasing (IWP-2005), pages 80–87.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313–318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849–856.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acquisi-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 456–463.
</reference>
<page confidence="0.993188">
1097
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.169231">
<title confidence="0.999059">Extracting Paraphrases from Definition Sentences on the Web</title>
<author confidence="0.636475">Kentaro De</author>
<note confidence="0.3916875">Institute of Information and Communications Kyoto, 619-0237, School of Informatics, Kyoto Kyoto, 606-8501,</note>
<abstract confidence="0.985803823529412">We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about paraphrases from Web documents with a precision rate of about 94%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susumu Akamine</author>
<author>Daisuke Kawahara</author>
<author>Yoshikiyo Kato</author>
<author>Tetsuji Nakagawa</author>
<author>Yutaka I Leon-Suematsu</author>
</authors>
<title>Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara.</title>
<date>2010</date>
<booktitle>In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010),</booktitle>
<pages>122--129</pages>
<contexts>
<context position="8282" citStr="Akamine et al., 2010" startWordPosition="1283" endWordPosition="1286">ists of two steps: definition sentence acquisition and Dagan, 2005; Bhagat et al., 2007; Szpektor and and paraphrase extraction. We describe them below. Dagan, 2008; Hashimoto et al., 2009). These meth- 3.1 Definition sentence acquisition ods can be applied to a normal monolingual corpus, We acquire sentences that define a concept (definiand it has been shown that a large number of para- tion sentences) as in Example (2), which defines “骨 phrases or entailment rules could be extracted. How1088 W,M” (osteoporosis), from the 6 x 108 Web pages 92.2, and 91.4, respectively. Using the classifier, (Akamine et al., 2010) and the Japanese Wikipedia. we acquired 1,925,052 positive sentences from all (2) *CP.92:Lt.*�t6C4c--DICLJZ�AM. of the collected sentences. After adding definition (Osteoporosis is a disease that makes bones fragile.) sentences from Wikipedia articles, which are typiFujii and Ishikawa (2002) developed an unsuper- cally the first sentence of the body of each article vised method to find definition sentences from the (Kazama and Torisawa, 2007), we obtained a total Web using 18 sentential templates and a language of 2,141,878 definition sentence candidates, which model constructed from an encyc</context>
</contexts>
<marker>Akamine, Kawahara, Kato, Nakagawa, Leon-Suematsu, 2010</marker>
<rawString>Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara. 2010. Organizing information on the web to support user judgments on information credibility. In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>38--135</pages>
<contexts>
<context position="2801" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="435" endWordPosition="438">o extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporosis”, include two pairs of phrasal paraphrases, which are indicated by underlines O and (D, respectively. (1) a. Osteoporosis is a disease that (i decreases the quantity of bone and 0 makes bones fragile. b. Osteoporosis is a disease that (i reduces bone mass and 0 increases the risk of bone fracture. We define paraphrase as a pair of expressions between which entailment relations of both directions hold. (Androutsopoulos and Malakasiotis, 2010). Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; </context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38:135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>597--604</pages>
<contexts>
<context position="25130" citStr="Bannard and Callison-Burch (2005)" startWordPosition="4046" endWordPosition="4049">Our SMT method uses Moses (Koehn et al., 2007) and extracts a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other. If you give Moses monolingual parallel sentence pairs, it should extract a set of two phrases that are paraphrases of each other. In this experiment, default values were used for all parameters. To rank extracted phrase pairs, we assigned each of them the product of two phrase translation probabilities of both directions that were given by Moses. For other SMT methods, see Quirk et al. (2004) and Bannard and Callison-Burch (2005) among others. Mrt method Murata et al. (2004) proposed a method to extract paraphrases from two manually compiled dictionaries. It simply regards a difference between two definition sentences of the same word as a paraphrase candidate. Paraphrase candidates are ranked according to an unsupervised scoring scheme that implements their assumption. They assume that a paraphrase candidate tends to be a valid paraphrase if it is surrounded by infrequent strings and/or if it appears multiple times in the data. In this experiment, we evaluated the unsupervised version of our method in addition to the</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005), pages 597– 604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1778" citStr="Barzilay and Lee, 2003" startWordPosition="268" endWordPosition="271">he same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporosis”, include two pairs of</context>
<context position="3399" citStr="Barzilay and Lee, 2003" startWordPosition="527" endWordPosition="530"> and Malakasiotis, 2010). Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts 1087 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics in several ways. The latter would suffer from the ever, the precision of these methods has been relafact that it is not easy to automatically retrieve large tively low. This is due to the fact that the evidence, bodies of parallel news text with high accuracy. On i.e., distributional similarity, is ju</context>
<context position="5220" citStr="Barzilay and Lee, 2003" startWordPosition="806" endWordPosition="809">hods is that they can find only paraphrases Web documents, and the estimated number of para- consisting of frequently observed expressions since phrases recognized in the definition sentences using they must have reliable distributional similarity valour method was about 300,000, for a precision rate ues for expressions that constitute paraphrases. of about 94%. Also, our experimental results show The second category is a parallel corpus approach that our method is superior to well-known compet- (Barzilay and McKeown, 2001; Shinyama et al., ing methods (Barzilay and McKeown, 2001; Koehn 2002; Barzilay and Lee, 2003; Dolan et al., 2004). et al., 2007) for extracting paraphrases from defini- Our method belongs to this category. This approach tion sentence pairs. aligns expressions between two sentences in parOur evaluation is based on bidirectional check- allel corpora, based on, for example, the overlap ing of entailment relations between paraphrases that of words/contexts. The aligned expressions are asconsiders the context dependence of a paraphrase. sumed to be paraphrases. In this approach, the exNote that using definition sentences is only the pressions do not need to appear frequently in the beginn</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings of HLT-NAACL 2003, pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL joint with the 10th Meeting of the European Chapter of the ACL (ACL/EACL</booktitle>
<pages>50--57</pages>
<contexts>
<context position="1730" citStr="Barzilay and McKeown, 2001" startWordPosition="259" endWordPosition="263">ntroduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the s</context>
<context position="3306" citStr="Barzilay and McKeown, 2001" startWordPosition="513" endWordPosition="516">a pair of expressions between which entailment relations of both directions hold. (Androutsopoulos and Malakasiotis, 2010). Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts 1087 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics in several ways. The latter would suffer from the ever, the precision of these methods has been relafact that it is not easy to automatically retrieve large tively low. This is due to the fact that the evidenc</context>
<context position="5126" citStr="Barzilay and McKeown, 2001" startWordPosition="791" endWordPosition="794">al similarity. Another limitation of definition sentence pairs were extracted from 6×108 these methods is that they can find only paraphrases Web documents, and the estimated number of para- consisting of frequently observed expressions since phrases recognized in the definition sentences using they must have reliable distributional similarity valour method was about 300,000, for a precision rate ues for expressions that constitute paraphrases. of about 94%. Also, our experimental results show The second category is a parallel corpus approach that our method is superior to well-known compet- (Barzilay and McKeown, 2001; Shinyama et al., ing methods (Barzilay and McKeown, 2001; Koehn 2002; Barzilay and Lee, 2003; Dolan et al., 2004). et al., 2007) for extracting paraphrases from defini- Our method belongs to this category. This approach tion sentence pairs. aligns expressions between two sentences in parOur evaluation is based on bidirectional check- allel corpora, based on, for example, the overlap ing of entailment relations between paraphrases that of words/contexts. The aligned expressions are asconsiders the context dependence of a paraphrase. sumed to be paraphrases. In this approach, the exNote that u</context>
<context position="21791" citStr="Barzilay and McKeown (2001)" startWordPosition="3517" endWordPosition="3520">of both directions held, as positive examples (1,092 pairs) and the others as negative ones (1,872 pairs).6 We built the paraphrase classifier from the training data. As mentioned, candidate phrase pairs were ranked by the distance from the SVM’s hyperplane. 4 Experiment In this paper, our claims are twofold. I. Definition sentences on the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim 6The remaining 36 pairs were discarded as they contained garbled characters of Japanese. 7http://www.statmt.org/moses/ 8As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen1091 I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API</context>
<context position="24096" citStr="Barzilay and McKeown (2001)" startWordPosition="3867" endWordPosition="3870">ears in positive (negative) examples. The most likely K positive (negative) contexts are used to extract positive (negative) paraphrases from the parallel sentences. Extracted positive (negative) paraphrases and their morpho-syntactic patterns are used to collect additional positive (negative) contexts. All the positive (negative) contexts are ranked, and additional paraphrases and their morpho-syntactic patterns are extracted again. This iterative process finishes if no further paraphrase is extracted or the number of iterations reaches a predefined threshold T. In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. The value of T is not given in their paper. We chose 3 as its value based on our preliminary experiments. Note that paraphrases extracted by this method are not ranked. tences. Nevertheless, we believe that comparing these methods with ours is very informative, since they are known to be accurate and have been influential. 9http://developer.yahoo.co.jp/webapi/ SMT method Our SMT method uses Moses (Koehn et al., 2007) and extracts a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other.</context>
<context position="36523" citStr="Barzilay and McKeown (2001)" startWordPosition="5932" endWordPosition="5935">ted of 2,741 from paraphrase instances. Though our method sentence pairs that were collected in the same way as can extract a variety of paraphrase instances on the Web sentence pairs and was labeled in the same a large scale, their coverage might be insufficient way as described in Section 3.2. for real NLP applications since some paraphrase Graph (c) of Figure 3 shows precision curves. We phenomena are highly productive. Therefore, we also measured precision without trivial pairs in the need paraphrase rules in addition to paraphrase insame way as the previous experiment. Graph (d) stances. Barzilay and McKeown (2001) induced shows the results. The lower half of Table 2 shows simple POS-based paraphrase rules from paraphrase the number of extracted paraphrases with/without instances, which can be a good starting point. trivial pairs for each method. Finally, as mentioned in Section 1, the work in Note that precision figures of our methods in this paper is only the beginning of our research on graphs (c) and (d) are lower than those of our meth- paraphrase extraction. We are trying to extract far ods in graphs (a) and (b). Additionally, none of the more paraphrases from a set of sentences fulfilling methods</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting of the ACL joint with the 10th Meeting of the European Chapter of the ACL (ACL/EACL 2001), pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP2007),</booktitle>
<pages>161--170</pages>
<contexts>
<context position="7748" citStr="Bhagat et al., 2007" startWordPosition="1196" endWordPosition="1199">rt in Section 4. egorized into two groups. The first involves a dis- For a more extensive survey on paraphrasing tributional similarity approach pioneered by Lin and methods, see Androutsopoulos and Malakasiotis Pantel (2001). Basically, this approach assumes that (2010) and Madnani and Dorr (2010). two expressions that have a large distributional simi- 3 Proposed method larity are paraphrases. There are also variants of this Our method, targeting the Japanese language, conapproach that address entailment acquisition (Geffet sists of two steps: definition sentence acquisition and Dagan, 2005; Bhagat et al., 2007; Szpektor and and paraphrase extraction. We describe them below. Dagan, 2008; Hashimoto et al., 2009). These meth- 3.1 Definition sentence acquisition ods can be applied to a normal monolingual corpus, We acquire sentences that define a concept (definiand it has been shown that a large number of para- tion sentences) as in Example (2), which defines “骨 phrases or entailment rules could be extracted. How1088 W,M” (osteoporosis), from the 6 x 108 Web pages 92.2, and 91.4, respectively. Using the classifier, (Akamine et al., 2010) and the Japanese Wikipedia. we acquired 1,925,052 positive senten</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP2007), pages 161–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1542" citStr="Callison-Burch et al., 2006" startWordPosition="228" endWordPosition="231"> parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%. 1 Introduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests t</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006), pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>350--356</pages>
<contexts>
<context position="3420" citStr="Dolan et al., 2004" startWordPosition="531" endWordPosition="534">. Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts 1087 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics in several ways. The latter would suffer from the ever, the precision of these methods has been relafact that it is not easy to automatically retrieve large tively low. This is due to the fact that the evidence, bodies of parallel news text with high accuracy. On i.e., distributional similarity, is just indirect evidence </context>
<context position="5241" citStr="Dolan et al., 2004" startWordPosition="810" endWordPosition="813">nd only paraphrases Web documents, and the estimated number of para- consisting of frequently observed expressions since phrases recognized in the definition sentences using they must have reliable distributional similarity valour method was about 300,000, for a precision rate ues for expressions that constitute paraphrases. of about 94%. Also, our experimental results show The second category is a parallel corpus approach that our method is superior to well-known compet- (Barzilay and McKeown, 2001; Shinyama et al., ing methods (Barzilay and McKeown, 2001; Koehn 2002; Barzilay and Lee, 2003; Dolan et al., 2004). et al., 2007) for extracting paraphrases from defini- Our method belongs to this category. This approach tion sentence pairs. aligns expressions between two sentences in parOur evaluation is based on bidirectional check- allel corpora, based on, for example, the overlap ing of entailment relations between paraphrases that of words/contexts. The aligned expressions are asconsiders the context dependence of a paraphrase. sumed to be paraphrases. In this approach, the exNote that using definition sentences is only the pressions do not need to appear frequently in the beginning of our research o</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics (COLING 2004), pages 350– 356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="22695" citStr="Fleiss, 1971" startWordPosition="3659" endWordPosition="3660">they contained garbled characters of Japanese. 7http://www.statmt.org/moses/ 8As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen1091 I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API9 as a paraphrase knowledge source. In the latter data set, two sentences of each pair are expected to be semantically similar regardless of whether they are definition sentences. Both sets contain 100,000 pairs. Three annotators (not the authors) checked evaluation samples. Fleiss’ kappa (Fleiss, 1971) was 0.69 (substantial agreement (Landis and Koch, 1977)). 4.1 Our method vs. competing methods In this experiment, paraphrase pairs are extracted from 100,000 definition sentence pairs that are randomly sampled from the 29,661,812 pairs. Before reporting the experimental results, we briefly describe the BM, SMT, and Mrt methods. BM method Given parallel sentences like multiple translations of the same source text, the BM method works iteratively as follows. First, it collects from the parallel sentences identical word pairs and their contexts (POS N-grams with indices indicating corresponding</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Tetsuya Ishikawa</author>
</authors>
<title>Extraction and organization of encyclopedic knowledge information using the World Wide Web (written</title>
<date>2002</date>
<booktitle>in Japanese). Institute of Electronics, Information, and Communication Engineers,</booktitle>
<pages>85--2</pages>
<contexts>
<context position="8575" citStr="Fujii and Ishikawa (2002)" startWordPosition="1321" endWordPosition="1325"> We acquire sentences that define a concept (definiand it has been shown that a large number of para- tion sentences) as in Example (2), which defines “骨 phrases or entailment rules could be extracted. How1088 W,M” (osteoporosis), from the 6 x 108 Web pages 92.2, and 91.4, respectively. Using the classifier, (Akamine et al., 2010) and the Japanese Wikipedia. we acquired 1,925,052 positive sentences from all (2) *CP.92:Lt.*�t6C4c--DICLJZ�AM. of the collected sentences. After adding definition (Osteoporosis is a disease that makes bones fragile.) sentences from Wikipedia articles, which are typiFujii and Ishikawa (2002) developed an unsuper- cally the first sentence of the body of each article vised method to find definition sentences from the (Kazama and Torisawa, 2007), we obtained a total Web using 18 sentential templates and a language of 2,141,878 definition sentence candidates, which model constructed from an encyclopedia. On the covered 867,321 concepts ranging from weapons to other hand, we developed a supervised method to rules of baseball. Then, we coupled two definition achieve a higher precision. sentences whose defined concepts were the same We use one sentential template and an SVM clas- and ob</context>
</contexts>
<marker>Fujii, Ishikawa, 2002</marker>
<rawString>Atsushi Fujii and Tetsuya Ishikawa. 2002. Extraction and organization of encyclopedic knowledge information using the World Wide Web (written in Japanese). Institute of Electronics, Information, and Communication Engineers, J85-D-II(2):300–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>107--114</pages>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Stijn De Saeger</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Large-scale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1172--1181</pages>
<marker>Hashimoto, Torisawa, Kuroda, De Saeger, Murata, Kazama, 2009</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquisition from the web. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 1172–1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polgu`ere</author>
</authors>
<title>Lexical selection and paraphrase in a meaning-text generation model.</title>
<date>1991</date>
<booktitle>Natural language generation in artificial intelligence and computational linguistics,</booktitle>
<pages>293--312</pages>
<editor>In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors,</editor>
<publisher>Kluwer Academic Press.</publisher>
<marker>Iordanskaja, Kittredge, Polgu`ere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polgu`ere. 1991. Lexical selection and paraphrase in a meaning-text generation model. In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors, Natural language generation in artificial intelligence and computational linguistics, pages 293–312. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>455--462</pages>
<contexts>
<context position="1512" citStr="Kauchak and Barzilay, 2006" startWordPosition="223" endWordPosition="227">t define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%. 1 Introduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abou</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006), pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>698--707</pages>
<contexts>
<context position="8729" citStr="Kazama and Torisawa, 2007" startWordPosition="1347" endWordPosition="1350"> phrases or entailment rules could be extracted. How1088 W,M” (osteoporosis), from the 6 x 108 Web pages 92.2, and 91.4, respectively. Using the classifier, (Akamine et al., 2010) and the Japanese Wikipedia. we acquired 1,925,052 positive sentences from all (2) *CP.92:Lt.*�t6C4c--DICLJZ�AM. of the collected sentences. After adding definition (Osteoporosis is a disease that makes bones fragile.) sentences from Wikipedia articles, which are typiFujii and Ishikawa (2002) developed an unsuper- cally the first sentence of the body of each article vised method to find definition sentences from the (Kazama and Torisawa, 2007), we obtained a total Web using 18 sentential templates and a language of 2,141,878 definition sentence candidates, which model constructed from an encyclopedia. On the covered 867,321 concepts ranging from weapons to other hand, we developed a supervised method to rules of baseball. Then, we coupled two definition achieve a higher precision. sentences whose defined concepts were the same We use one sentential template and an SVM clas- and obtained 29,661,812 definition sentence pairs. sifier. Specifically, we first collect definition sen- Obviously, our method is tailored to Japanese. For ten</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2007), pages 698–707, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>407--415</pages>
<contexts>
<context position="17559" citStr="Kazama and Torisawa, 2008" startWordPosition="2800" endWordPosition="2803">ntexts are defined similarly. Figure 1: Illustration of features f8-12. didate phrase of s2 but do appear in the other part of s2, i.e. they are extra morphemes for s1’s candidate phrase. On the other hand, f9 is zero since there is no such extra morpheme in s2’s candidate phrase. Also, features f10-12 have positive values since the two candidate phrases share two parent dependency tree fragments, (that increases) and (offracture). We have also tried the following features, which we do not detail due to space limitation: the similarity of candidate phrases based on semantically similar nouns (Kazama and Torisawa, 2008), entailing/entailed verbs (Hashimoto et al., 2009), and the identity of the pronunciation and base form of the head morpheme; N-grams (N=1,2,3) of child and parent contexts represented by either the inflected form, base form, pronunciation, or POS of mor1090 Original definition sentence pair (s1, s2) Paraphrased definition sentence pair (si, s2) s1: Osteoporosis is a disease that reduces bone mass and makes bones fragile. s2: Osteoporosis is a disease that decreases the quantity of bone and increases the risk of bone fracture. si: Osteoporosis is a disease that decreases the quantity of bone </context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 407–415.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="21832" citStr="Koehn et al., 2007" startWordPosition="3524" endWordPosition="3527">092 pairs) and the others as negative ones (1,872 pairs).6 We built the paraphrase classifier from the training data. As mentioned, candidate phrase pairs were ranked by the distance from the SVM’s hyperplane. 4 Experiment In this paper, our claims are twofold. I. Definition sentences on the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim 6The remaining 36 pairs were discarded as they contained garbled characters of Japanese. 7http://www.statmt.org/moses/ 8As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen1091 I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API9 as a paraphrase knowledge source. In th</context>
<context position="24543" citStr="Koehn et al., 2007" startWordPosition="3945" endWordPosition="3948">ocess finishes if no further paraphrase is extracted or the number of iterations reaches a predefined threshold T. In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. The value of T is not given in their paper. We chose 3 as its value based on our preliminary experiments. Note that paraphrases extracted by this method are not ranked. tences. Nevertheless, we believe that comparing these methods with ours is very informative, since they are known to be accurate and have been influential. 9http://developer.yahoo.co.jp/webapi/ SMT method Our SMT method uses Moses (Koehn et al., 2007) and extracts a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other. If you give Moses monolingual parallel sentence pairs, it should extract a set of two phrases that are paraphrases of each other. In this experiment, default values were used for all parameters. To rank extracted phrase pairs, we assigned each of them the product of two phrase translation probabilities of both directions that were given by Moses. For other SMT methods, see Quirk et al. (2004) and Bannard and Callison-Burch (2005) among others</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="22751" citStr="Landis and Koch, 1977" startWordPosition="3665" endWordPosition="3668">7http://www.statmt.org/moses/ 8As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen1091 I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API9 as a paraphrase knowledge source. In the latter data set, two sentences of each pair are expected to be semantically similar regardless of whether they are definition sentences. Both sets contain 100,000 pairs. Three annotators (not the authors) checked evaluation samples. Fleiss’ kappa (Fleiss, 1971) was 0.69 (substantial agreement (Landis and Koch, 1977)). 4.1 Our method vs. competing methods In this experiment, paraphrase pairs are extracted from 100,000 definition sentence pairs that are randomly sampled from the 29,661,812 pairs. Before reporting the experimental results, we briefly describe the BM, SMT, and Mrt methods. BM method Given parallel sentences like multiple translations of the same source text, the BM method works iteratively as follows. First, it collects from the parallel sentences identical word pairs and their contexts (POS N-grams with indices indicating corresponding words between paired contexts) as positive examples and</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1455" citStr="Lin and Pantel, 2001" startWordPosition="215" endWordPosition="218"> with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%. 1 Introduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information us</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A phrase-based alignment model for natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-2008),</booktitle>
<pages>802--811</pages>
<contexts>
<context position="31051" citStr="MacCartney et al. (2008)" startWordPosition="5060" endWordPosition="5063">tences. In this regard, we do not claim that our method is almighty. We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases. In graphs (a) and (b), the precision of the SMT method goes up as rank goes down. This strange behavior is due to the scoring by Moses that worked poorly for the data; it gave 1.0 to 82.5% of all the samples, 38.8% of which were incorrect. We suspect SMT methods are poor at monolingual alignment for paraphrasing or entailment tasks since, in the tasks, data is much noisier than that used for SMT. See MacCartney et al. (2008) for similar discussion. 4.2 Definition pairs vs. Web sentence pairs To collect Web sentence pairs, first, we randomly sampled 1.8 million sentences from the Web corpus. 10There are many kinds of orthographic variants in Japanese, which can be identified by their pronunciation. 11We set no threshold for candidate phrase pairs of each method, and counted all the candidate phrase pairs in Table 2. 1093 Precision Precision 0 1000 2000 3000 4000 5000 Top-N 0 1000 2000 3000 4000 5000 Top-N 1 0.8 0.6 0.4 0.2 0 ’Sup_der ’Uns_def ’SMT der BMder ’Mrt_def 0.8 0.6 0.4 0.2 0 1 ’Sup_def_n’ ’Uns_def_n’ ’SMT</context>
</contexts>
<marker>MacCartney, Galley, Manning, 2008</marker>
<rawString>Bill MacCartney, Michel Galley, and Christopher D. Manning. 2008. A phrase-based alignment model for natural language inference. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-2008), pages 802–811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="7428" citStr="Madnani and Dorr (2010)" startWordPosition="1149" endWordPosition="1152">proposed method. Section 4 reports on tences than those on the Web. Thus, the coverage of evaluation results. Section 5 concludes the paper. their method should be quite limited. Furthermore, 2 Related Work the precision of their method is much poorer than The existing work for paraphrase extraction is cat- ours as we report in Section 4. egorized into two groups. The first involves a dis- For a more extensive survey on paraphrasing tributional similarity approach pioneered by Lin and methods, see Androutsopoulos and Malakasiotis Pantel (2001). Basically, this approach assumes that (2010) and Madnani and Dorr (2010). two expressions that have a large distributional simi- 3 Proposed method larity are paraphrases. There are also variants of this Our method, targeting the Japanese language, conapproach that address entailment acquisition (Geffet sists of two steps: definition sentence acquisition and Dagan, 2005; Bhagat et al., 2007; Szpektor and and paraphrase extraction. We describe them below. Dagan, 2008; Hashimoto et al., 2009). These meth- 3.1 Definition sentence acquisition ods can be applied to a normal monolingual corpus, We acquire sentences that define a concept (definiand it has been shown that </context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Computational Linguistics, 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Ani Nenkova</author>
<author>Carl Sable</author>
<author>Barry Schiffman</author>
<author>Sergey Sigelman</author>
</authors>
<title>Tracking and summarizing news on a daily basis with columbia’s newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd international conference on Human Language Technology Research,</booktitle>
<pages>280--285</pages>
<contexts>
<context position="1433" citStr="McKeown et al., 2002" startWordPosition="211" endWordPosition="214">utomatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%. 1 Introduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey t</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Judith L. Klavans, Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey Sigelman. 2002. Tracking and summarizing news on a daily basis with columbia’s newsblaster. In Proceedings of the 2nd international conference on Human Language Technology Research, pages 280–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Toshiyuki Kanemaru</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Automatic paraphrase acquisition based on matching of definition sentences in plural dictionaries (written in Japanese).</title>
<date>2004</date>
<journal>Journal ofNatural Language Processing,</journal>
<volume>11</volume>
<issue>5</issue>
<contexts>
<context position="6582" citStr="Murata et al. (2004)" startWordPosition="1016" endWordPosition="1019">ces antonymous pairs for paraphrases/entailment pairs. fulfilling the same pragmatic function (e.g. defini- However, its limitation is the difficulty in preparing tion) for the same topic (e.g. osteoporosis) convey a large amount of parallel corpora, as noted before. mostly the same information using different expres- We avoid this by using definition sentences, which sions. Such functions other than definition may in- can be easily acquired on a large scale from the Web, clude the usage of the same Linux command, the as parallel corpora. recipe for the same cuisine, or the description of re- Murata et al. (2004) used definition sentences in lated work on the same research issue. two manually compiled dictionaries, which are conSection 2 describes related works. Section 3 siderably fewer in the number of definition senpresents our proposed method. Section 4 reports on tences than those on the Web. Thus, the coverage of evaluation results. Section 5 concludes the paper. their method should be quite limited. Furthermore, 2 Related Work the precision of their method is much poorer than The existing work for paraphrase extraction is cat- ours as we report in Section 4. egorized into two groups. The first </context>
<context position="21879" citStr="Murata et al. (2004)" startWordPosition="3533" endWordPosition="3536">872 pairs).6 We built the paraphrase classifier from the training data. As mentioned, candidate phrase pairs were ranked by the distance from the SVM’s hyperplane. 4 Experiment In this paper, our claims are twofold. I. Definition sentences on the Web are a treasure trove of paraphrase knowledge (Section 4.2). II. Our method of paraphrase acquisition from definition sentences is more accurate than wellknown competing methods (Section 4.1). We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al., 2007) (SMT method), and that of Murata et al. (2004) (Mrt method). The first two methods are well known for accurately extracting semantically equivalent phrase pairs from parallel corpora.8 Then, we verify claim 6The remaining 36 pairs were discarded as they contained garbled characters of Japanese. 7http://www.statmt.org/moses/ 8As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen1091 I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API9 as a paraphrase knowledge source. In the latter data set, two sentences of each pair a</context>
<context position="25176" citStr="Murata et al. (2004)" startWordPosition="4054" endWordPosition="4057"> a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other. If you give Moses monolingual parallel sentence pairs, it should extract a set of two phrases that are paraphrases of each other. In this experiment, default values were used for all parameters. To rank extracted phrase pairs, we assigned each of them the product of two phrase translation probabilities of both directions that were given by Moses. For other SMT methods, see Quirk et al. (2004) and Bannard and Callison-Burch (2005) among others. Mrt method Murata et al. (2004) proposed a method to extract paraphrases from two manually compiled dictionaries. It simply regards a difference between two definition sentences of the same word as a paraphrase candidate. Paraphrase candidates are ranked according to an unsupervised scoring scheme that implements their assumption. They assume that a paraphrase candidate tends to be a valid paraphrase if it is surrounded by infrequent strings and/or if it appears multiple times in the data. In this experiment, we evaluated the unsupervised version of our method in addition to the supervised one described in Section 3.2, in o</context>
</contexts>
<marker>Murata, Kanemaru, Isahara, 2004</marker>
<rawString>Masaki Murata, Toshiyuki Kanemaru, and Hitoshi Isahara. 2004. Automatic paraphrase acquisition based on matching of definition sentences in plural dictionaries (written in Japanese). Journal ofNatural Language Processing, 11(5):135–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="9514" citStr="Navigli and Velardi (2010)" startWordPosition="1467" endWordPosition="1470">a. On the covered 867,321 concepts ranging from weapons to other hand, we developed a supervised method to rules of baseball. Then, we coupled two definition achieve a higher precision. sentences whose defined concepts were the same We use one sentential template and an SVM clas- and obtained 29,661,812 definition sentence pairs. sifier. Specifically, we first collect definition sen- Obviously, our method is tailored to Japanese. For tence candidates by a template “ˆNP 2:Lt.*”, where a language-independent method of definition acquiˆ is the beginning of sentence and NP is the noun sition, see Navigli and Velardi (2010) as an example. phrase expressing the concept to be defined followed by a particle sequence, “2:” (comitative) and “Lt” (topic) (and optionally followed by comma), as exemplified in (2). As a result, we collected 3,027,101 sentences. Although the particle sequence tends to mark the topic of the definition sentence, it can also appear in interrogative sentences and normal assertive sentences in which a topic is strongly emphasized. To remove such non-definition sentences, we classify the candidate sentences using an SVM classifier with a polynominal kernel (d = 2).1 Since Japanese is a head-fin</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1318–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004),</booktitle>
<pages>142--149</pages>
<contexts>
<context position="25092" citStr="Quirk et al. (2004)" startWordPosition="4041" endWordPosition="4044">o.jp/webapi/ SMT method Our SMT method uses Moses (Koehn et al., 2007) and extracts a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other. If you give Moses monolingual parallel sentence pairs, it should extract a set of two phrases that are paraphrases of each other. In this experiment, default values were used for all parameters. To rank extracted phrase pairs, we assigned each of them the product of two phrase translation probabilities of both directions that were given by Moses. For other SMT methods, see Quirk et al. (2004) and Bannard and Callison-Burch (2005) among others. Mrt method Murata et al. (2004) proposed a method to extract paraphrases from two manually compiled dictionaries. It simply regards a difference between two definition sentences of the same word as a paraphrase candidate. Paraphrase candidates are ranked according to an unsupervised scoring scheme that implements their assumption. They assume that a paraphrase candidate tends to be a valid paraphrase if it is surrounded by infrequent strings and/or if it appears multiple times in the data. In this experiment, we evaluated the unsupervised ve</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP-2004), pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard H Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1484" citStr="Ravichandran and Hovy, 2002" startWordPosition="219" endWordPosition="222">y regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%. 1 Introduction Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Su</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard H. Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing (IWP-2005),</booktitle>
<pages>80--87</pages>
<contexts>
<context position="30523" citStr="Sekine (2005)" startWordPosition="4966" endWordPosition="4967">ses with/without trivial pairs for each method.11 Sup and Uns extracted many more paraphrases. It is noteworthy that Sup performed the best in terms of both precision rate and the number of extracted paraphrases. Table 3 shows examples of correct and incorrect outputs of Sup. As the examples indicate, many of the extracted paraphrases are not specific to definition sentences and seem very reusable. However, there are few paraphrases involving metaphors or idioms in the outputs due to the nature of definition sentences. In this regard, we do not claim that our method is almighty. We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases. In graphs (a) and (b), the precision of the SMT method goes up as rank goes down. This strange behavior is due to the scoring by Moses that worked poorly for the data; it gave 1.0 to 82.5% of all the samples, 38.8% of which were incorrect. We suspect SMT methods are poor at monolingual alignment for paraphrasing or entailment tasks since, in the tasks, data is much noisier than that used for SMT. See MacCartney et al. (2008) for similar discussion. 4.2 Definition pairs vs. Web sentence pairs To </context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings of the Third International Workshop on Paraphrasing (IWP-2005), pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd international Conference on Human Language Technology Research (HLT2002),</booktitle>
<pages>313--318</pages>
<contexts>
<context position="1753" citStr="Shinyama et al., 2002" startWordPosition="264" endWordPosition="267"> allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area. Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003). We propose a method to extract phrasal paraphrases from pairs of sentences that define the same concept. The method is based on our observation that two sentences defining the same concept can be regarded as a parallel corpus since they largely convey the same information using different expressions. Such definition sentences abound on the Web. This suggests that we may be able to extract a large amount of phrasal paraphrase knowledge from the definition sentences on the Web. For instance, the following two sentences, both of which define the same concept “osteoporos</context>
<context position="3375" citStr="Shinyama et al., 2002" startWordPosition="523" endWordPosition="526"> hold. (Androutsopoulos and Malakasiotis, 2010). Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept. We propose a supervised method that exploits various kinds of lexical similarity features and contextual features. Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method. Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004). The former requires a large amount of manual labor to translate the same texts 1087 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics in several ways. The latter would suffer from the ever, the precision of these methods has been relafact that it is not easy to automatically retrieve large tively low. This is due to the fact that the evidence, bodies of parallel news text with high accuracy. On i.e., distribu</context>
</contexts>
<marker>Shinyama, Sekine, Sudo, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo. 2002. Automatic paraphrase acquisition from news articles. In Proceedings of the 2nd international Conference on Human Language Technology Research (HLT2002), pages 313–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary template.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008),</booktitle>
<pages>849--856</pages>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary template. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008), pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>456--463</pages>
<contexts>
<context position="20178" citStr="Szpektor et al. (2007)" startWordPosition="3240" endWordPosition="3243">ey indicate the likelihood of a given candidate paraphrase pair’s being a paraphrase. Note that values of the features f8 and f9 are weighted with −1, since they indicate the unlikelihood. Specifically, we first randomly sampled 30,000 definition sentence pairs from the 29,661,812 pairs, and collected 3,000 candidate phrase pairs that had the highest likelihood from them. The manual labeling of each candidate phrase pair (p1, p2) was based on bidirectional checking of entailment relation, p1 —* p2 and p2 —* p1, with p1 and p2 embedded in contexts. This scheme is similar to the one proposed by Szpektor et al. (2007). We adopt this scheme since paraphrase judgment might be unstable between annotators unless they are given a particular context based on which they make a judgment. As described below, we use definition sentences as contexts. We admit that annotators might be biased by this in some unexpected way, but we believe that this is a more stable method than that without contexts. The labeling process is as follows. First, from each candidate phrase pair (p1, p2) and its source definition sentence pair (s1, s2), we create two paraphrase sentence pairs (si, s2) by exchanging p1 and p2 between s1 and s</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 456–463.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>