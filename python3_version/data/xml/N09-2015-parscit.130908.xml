<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000242">
<title confidence="0.602477">
Spherical Discriminant Analysis in Semi-supervised Speaker Clustering*
</title>
<email confidence="0.956526">
haotang2@ifp.uiuc.edu
</email>
<author confidence="0.99129">
Hao Tang
</author>
<affiliation confidence="0.952133666666667">
Dept. of ECE
University of Illinois
Urbana, IL 61801, USA
</affiliation>
<author confidence="0.480745666666667">
Stephen M. Chu
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
</author>
<email confidence="0.977352">
schu@us.ibm.com
</email>
<author confidence="0.967277">
Thomas S. Huang
</author>
<affiliation confidence="0.842219333333333">
Dept. of ECE
University of Illinois
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998798">
huang@ifp.uiuc.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99940055">
Semi-supervised speaker clustering refers to
the use of our prior knowledge of speakers
in general to assist the unsupervised speaker
clustering process. In the form of an in-
dependent training set, the prior knowledge
helps us learn a speaker-discriminative fea-
ture transformation, a universal speaker prior
model, and a discriminative speaker subspace,
or equivalently a speaker-discriminative dis-
tance metric. The directional scattering pat-
terns of Gaussian mixture model mean su-
pervectors motivate us to perform discrimi-
nant analysis on the unit hypersphere rather
than in the Euclidean space, which leads to
a novel dimensionality reduction technique
called spherical discriminant analysis (SDA).
Our experiment results show that in the
SDA subspace, speaker clustering yields su-
perior performance than that in other reduced-
dimensional subspaces (e.g., PCA and LDA).
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988780666666667">
Speaker clustering is a critical part of speaker di-
arization (a.k.a. speaker segmentation and cluster-
ing) (Barras et al., 2006; Tranter and Reynolds,
2006; Wooters and Huijbregts, 2007; Han et al.,
2008). Unlike speaker recognition, where we have
the training data of a set of known speakers and thus
recognition can be done supervised, speaker cluster-
ing is usually performed in a completely unsuper-
vised manner. The output of speaker clustering is the
internal labels relative to a dataset rather than real
This work was funded in part by DARPA contract HR0011-
06-2-0001.
</bodyText>
<page confidence="0.976973">
57
</page>
<bodyText confidence="0.999386307692308">
speaker identities. An interesting question is: Can
we do semi-supervised speaker clustering? That is,
can we make use of any available information that
can be helpful to speaker clustering?
Our answer to this question is positive. Here,
semi-supervision refers to the use of our prior
knowledge of speakers in general to assist the un-
supervised speaker clustering process. In the form
of an independent training set, the prior knowledge
helps us learn a speaker-discriminative feature trans-
formation, a universal speaker prior model, and a
discriminative speaker subspace, or equivalently a
speaker-discriminative distance metric.
</bodyText>
<sectionHeader confidence="0.810054" genericHeader="method">
2 Semi-supervised Speaker Clustering
</sectionHeader>
<bodyText confidence="0.9999565">
A general pipeline of speaker clustering consists
of four essential elements, namely feature extrac-
tion, utterance representation, distance metric, and
clustering. We incorporate our prior knowledge
of speakers into the various stages of this pipeline
through an independent training set.
</bodyText>
<subsectionHeader confidence="0.995398">
2.1 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999288636363636">
The most popular speech features are spectrum-
based acoustic features such as mel-frequency cep-
stral coefficients (MFCCs) and perceptual linear pre-
dictive (PLP) coefficients. In order to account for
the dynamics of spectrum changes over time, the
basic acoustic features are often supplemented by
their first and second derivatives. We pursue a dif-
ferent avenue in which we augment the basic acous-
tic features of every frame with those of the neigh-
boring frames. Specifically, the acoustic features
of the current frame and those of the KL frames
</bodyText>
<subsubsectionHeader confidence="0.834457">
Proceedings of NAACL HLT 2009: Short Papers, pages 57–60,
</subsubsectionHeader>
<bodyText confidence="0.957378222222222">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
to the left and KR frames to the right are con-
catenated to form a high-dimensional feature vec-
tor. In the context-expanded feature vector space, we
learn a speaker-discriminative feature transforma-
tion by linear discriminant analysis (LDA) based on
the known speaker labels of the independent training
set. The resulting low-dimensional feature subspace
is expected to provide optimal speaker separability.
</bodyText>
<subsectionHeader confidence="0.999738">
2.2 Utterance Representation
</subsectionHeader>
<bodyText confidence="0.999986428571429">
Deviating from the mainstream “bag of acoustic fea-
tures” representation where the extracted acoustic
features are represented by a statistical model such
as a Gaussian mixture model (GMM), we adopt the
GMM mean supervector representation which has
emerged in the speaker recognition area (Campbell
et al., 2006). Such representation is obtained by
maximum a posteriori (MAP) adapting a universal
background model (UBM), which has been finely
trained with all the data in the training set, to a
particular utterance. The component means of the
adapted GMM are stacked to form a column vector
conventionally called a GMM mean supervector. In
this way, we are allowed to represent an utterance
as a point in a high-dimensional space where tra-
ditional distance metrics and clustering techniques
can be naturally applied. The UBM, which can be
deemed as a universal speaker prior model inferred
from the independent training set, imposes generic
speaker constraints to the GMM mean supervector
space.
</bodyText>
<subsectionHeader confidence="0.999122">
2.3 Distance Metric
</subsectionHeader>
<bodyText confidence="0.999426473684211">
In the GMM mean supervector space, a naturally
arising distance metric is the Euclidean distance
metric. However, it is observed that the supervec-
tors show strong directional scattering patterns. The
directions of the data points seem to be more indica-
tive than their magnitudes. This observation moti-
vates us to favor the cosine distance metric over the
Euclidean distance metric for speaker clustering.
Although the cosine distance metric can be used
in the GMM mean supervector space, it is optimal
only if the data points are uniformly spread in all di-
rections in the entire space. In a high-dimensional
space, most often the data lies in or near a low-
dimensional manifold or subspace. It is advanta-
geous to learn an optimal distance metric from the
data directly.
The general cosine distance between two data
points x and y can be defined and manipulated as
follows.
</bodyText>
<equation confidence="0.9269408">
T
d(x, y) = 1 − �xT xxT Ay (1)
√yTA Y
(A1/2x)T (A1/2y)
= 1 − √(W T x)T (W T x)√(WT y)T (WT y)
</equation>
<bodyText confidence="0.999874166666667">
The general cosine distance can be casted as the
cosine distance between two transformed data points
WT x and WT y where WT = A1/2. In this sense,
learning an optimal distance metric is equivalent to
learning an optimal linear subspace of the original
high-dimensional space.
</bodyText>
<sectionHeader confidence="0.998038" genericHeader="method">
3 Spherical Discriminant Analysis
</sectionHeader>
<bodyText confidence="0.914382772727273">
Most existing linear subspace learning techniques
(e.g. PCA and LDA) are based on the Euclidean
distance metric. In the GMM mean supervector
space, we seek to perform discriminant analysis in
the cosine distance metric space. We coin the phrase
“spherical discriminant analysis” to denote discrim-
inant analysis on the unit hypersphere. We define
a projection from a d-dimensional hypersphere to a
d&apos;-dimensional hypersphere where d&apos; &lt; d
y= W T x
(2)
kWTxk
We note that such a projection is nonlinear. How-
ever, under two mild conditions, this projection can
be linearized. One is that the objective function for
learning the projection only involves the cosine dis-
tance. The other is that only the cosine distance is
used in the projected space. In this case, the norm of
the projected vector y has no impact on the objective
function and distance computation in the projected
space. Thus, the denominator term of Equation 2
can be safely dropped, leading to a linear projection.
</bodyText>
<subsectionHeader confidence="0.981271">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.999955">
The goal of SDA is to seek a linear transformation
W such that the average within-class cosine similar-
ity of the projected data set is maximized while the
</bodyText>
<equation confidence="0.9247985">
= 1 − √(A1/2x)T (A1/2x)√(A1/2y)T (A1/2y)
(WT x)T (WT y)
</equation>
<page confidence="0.98747">
58
</page>
<bodyText confidence="0.9997592">
average between-class cosine similarity of the pro-
jected data set is minimized. Assuming that there are
c classes, the average within-class cosine similarity
can be written in terms of the unknown projection
matrix W and the original data points x
</bodyText>
<equation confidence="0.984983333333333">
Si (3)
X
1
Si = |Di||Di|
yj,yk∈Di
qxTj WWT xj√xTk WWT xk
</equation>
<bodyText confidence="0.999966">
where |Di |denotes the number of data points in the
ith class. Similarly, the average between-class co-
sine similarity can be written in terms of W and x
</bodyText>
<equation confidence="0.635788666666667">
xTj WWT xk
=
qxT j WWT xj√xTkWWT xk
</equation>
<bodyText confidence="0.8294205">
where |Dm |and |Dn |denote the number of data
points in the mth and nth classes, respectively.
The SDA criterion is to maximize SW while min-
imizing SB
</bodyText>
<equation confidence="0.997866">
W = arg max(SW − SB) (5)
W
</equation>
<bodyText confidence="0.998798">
Our SDA formulation is similar to the work of Ma
et al. (2007). However, we solve it efficiently in a
general dimensionality reduction framework known
as graph embedding (Yan et al., 2007).
</bodyText>
<subsectionHeader confidence="0.99922">
3.2 Graph Embedding Solution
</subsectionHeader>
<bodyText confidence="0.999982857142857">
In graph embedding, a weighted graph with vertex
set X and similarity matrix S is used to characterize
certain statistical or geometrical properties of a data
set. A vertex in X represents a data point and an
entry sij in S represents the similarity between the
data points xi and xj. For a specific dimensional-
ity reduction algorithm, there may exist two graphs.
The intrinsic graph {X, S(i)} characterizes the data
properties that the algorithm aims to preserve and
the penalty graph {X, S(p)} characterizes the data
properties that the algorithm aims to avoid. The goal
of graph embedding is to represent each vertex in X
as a low dimensional vector that preserves the simi-
larities in S. The objective function is
</bodyText>
<equation confidence="0.91027925">
P
W=arg minW i6=j kf(xi,W)−f(xj,W)k2(s(i)
ij −s(p)
ij ) (6)
</equation>
<bodyText confidence="0.996267333333333">
where f(x, W) is a general projection with param-
eters W. If we take the projection to be of the form
in Equation 2, the objective function becomes
</bodyText>
<equation confidence="0.9883335">
2
(s(i)
ij −s(p)
ij ) (7)
</equation>
<bodyText confidence="0.99913">
It is shown that the solution to the graph embed-
ding problem of Equation 7 may be obtained by
a steepest descent algorithm (Fu et al., 2008). If
we expand the L2 norm terms of Equation 7, it is
straightforward to show that Equation 7 is equiva-
lent to Equation 5 provided that the graph weights
are set to proper values, as follows.
</bodyText>
<equation confidence="0.925943333333333">
1
sjk ←
(i) |D||D |if xj, xk ∈ Di, i = 1, ..., c
</equation>
<bodyText confidence="0.8517285">
sjk ←c(c − 1)|Dm||Dn |if xj ∈ Dm, xk ∈ Dn
m, n = 1, ..., c, m =6 n (8)
That is, by assigning appropriate values to the
weights of the intrinsic and penalty graphs, the SDA
optimization problem in Equation 5 can be solved
within the elegant graph embedding framework.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999901454545455">
Our speaker clustering experiments are based on a
test set of 630 speakers and 19024 utterances se-
lected from the GALE database (Chu et al., 2008),
which contains about 1900 hours of broadcasting
news speech data collected from various TV pro-
grams. An independent training set of 498 speak-
ers and 18327 utterances is also selected from the
GALE database. In either data set, there are an aver-
age of 30-40 utterances per speaker and the average
duration of the utterances is about 3-4 seconds. Note
that there are no overlapping speakers in the two data
</bodyText>
<figure confidence="0.993019416666667">
c
X
m=1
c
X
n=1
1
SB = c(c − 1)
Smn (m =6 n) (4)
1
Smn =
X
yj∈Dm
yk∈Dn
yTj yk
qyTj yj√yTk yk
|Dm||Dn|
1
SW =
c
Xc
i=1
X1
|Di||Di|
xj,xk∈Di
yTj yk
qyTj yj√yTk yk
xTj WWT xk
1 X
|Dm||Dn |xj∈Dm
xk∈Dn
P ° ° ° ° WT Xi WTXj ° ° ° °
W=arg minW i6=j kWT Xik − kWT Xjk
ii
c
(p) 1
</figure>
<page confidence="0.99459">
59
</page>
<bodyText confidence="0.999936261904762">
sets – speakers in the test set are not present in the
independent training set.
The acoustic features are 13 basic PLP features
with cepstrum mean subtraction. In computing the
LDA feature transformation using the independent
training set, KL and KR are both set to 4, and the di-
mensionality of the low-dimensional feature space is
set to 40. The entire independent training set is used
to train a UBM via the EM algorithm, and a GMM
mean supervector is obtained for every utterance in
the test set via MAP adaptation. The trained UBM
has 64 mixture components. Thus, the dimension of
the GMM mean supervectors is 2560.
We employ the hierarchical agglomerative clus-
tering technique with the “ward” linkage method.
Our experiments are carried out as follows. In each
experiment, we perform 4 cases, each of which is as-
sociated with a specific number of test speakers, i.e.,
5, 10, 20, and 50, respectively. In each case, the
corresponding number of speakers are drawn ran-
domly from the test set, and all the utterances from
the selected speakers are used for clustering. For
each case, 100 trials are run, each of which involves
a random draw of the test speakers, and the average
of the clustering accuracies across the 100 trials is
recorded.
First, we perform speaker clustering in the orig-
inal GMM mean supervector space using the Eu-
clidean distance metric and the cosine distance met-
ric, respectively. The results indicate that the cosine
distance metric consistently outperforms the Eu-
clidean distance metric. Next, we perform speaker
clustering in the reduced-dimensional subspaces us-
ing the eigenvoice (PCA) and fishervoice (LDA)
approaches, respectively. The results show that
the fishervoice approach significantly outperforms
the eigenvoice approach in all cases. Finally, we
perform speaker clustering in the SDA subspace.
The results demonstrate that in the SDA subspace,
speaker clustering yields superior performance than
that in other reduced-dimensional subspaces (e.g.,
PCA and LDA). Table 1 presents these results.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998264666666667">
This paper proposes semi-supervised speaker clus-
tering in which we learn a speaker-discriminative
feature transformation, a universal speaker prior
</bodyText>
<table confidence="0.9998105">
Metric Subspace 5 10 20 50
Orig 85.0 82.6 78.1 69.4
Euc PCA 85.5 82.9 79.3 69.9
LDA 94.0 90.8 86.6 79.6
Cos Orig 90.7 86.5 82.2 77.7
SDA 98.0 94.7 90.0 85.9
</table>
<tableCaption confidence="0.999959">
Table 1: Average speaker clustering accuracies (unit:%).
</tableCaption>
<bodyText confidence="0.9999055">
model, and a speaker-discriminative distance metric
through an independent training set. Motivated by
the directional scattering patterns of the GMM mean
supervectors, we peroform discriminant analysis on
the unit hypersphere rather than in the Euclidean
space, leading to a novel dimensionality reduction
technique “SDA”. Our experiment results indicate
that in the SDA subspace, speaker clustering yields
superior performance than that in other reduced-
dimensional subspaces (e.g., PCA and LDA).
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945785714286">
C. Barras, X. Zhu, S. Meignier, and J. Gauvain. 2006.
Multistage speaker diarization of broadcast news.
IEEE Trans. ASLP, 14(5):1505–1512.
W. Campbell, D. Sturim, D. Reynolds. 2006. Support
vector machines using GMM supervectors for speaker
verification. Signal Processing Letters 13(5):308-311.
S. Chu, H. Kuo, L. Mangu, Y. Liu, Y. Qin, and Q. Shi.
2008. Recent advances in the IBM GALE mandarin
transcription system. Proc. ICASSP.
Y. Fu, S. Yan and T. Huang. 2008. Correlation Met-
ric for Generalized Feature Extraction. IEEE Trans.
PAMI 30(12):2229–2235.
K. Han, S. Kim, and S. Narayanan. 2008. Strategies to
Improve the Robustness of Agglomerative Hierarchi-
cal Clustering under Data Source Variation for Speaker
Diarization. IEEE Trans. SALP 16(8):1590–1601.
Y. Ma, S. Lao, E. Takikawa, and M. Kawade. 2007. Dis-
criminant Analysis in Correlation Similarity Measure
Space. Proc. ICML (227):577–584.
S. Tranter and D. Reynolds. 2006. An Overview of
Automatic Speaker Diarization Systems. IEEE Trans.
ASLP, 14(5):1557–1565.
C. Wooters and M. Huijbregts. 2007. The ICSI RT07s
Speaker Diarization System. LNCS.
S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, S. Lin.
2007. Graph Embedding and Extensions: A Gen-
eral Framework for Dimensionality Reduction. IEEE
Trans. PAMI 29(1):40–51.
</reference>
<page confidence="0.998414">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212095">
<note confidence="0.507618">Discriminant Analysis in Semi-supervised Speaker</note>
<email confidence="0.959347">haotang2@ifp.uiuc.edu</email>
<author confidence="0.893992">Hao</author>
<affiliation confidence="0.9993015">Dept. of University of</affiliation>
<address confidence="0.992958">Urbana, IL 61801, USA</address>
<author confidence="0.999605">M Stephen</author>
<affiliation confidence="0.823302">IBM T. J. Watson Research Yorktown Heights, NY 10598,</affiliation>
<email confidence="0.996893">schu@us.ibm.com</email>
<author confidence="0.996439">S Thomas</author>
<affiliation confidence="0.9996155">Dept. of University of</affiliation>
<address confidence="0.927231">Urbana, IL 61801,</address>
<email confidence="0.999534">huang@ifp.uiuc.edu</email>
<abstract confidence="0.991548571428572">Semi-supervised speaker clustering refers to the use of our prior knowledge of speakers in general to assist the unsupervised speaker clustering process. In the form of an independent training set, the prior knowledge helps us learn a speaker-discriminative feature transformation, a universal speaker prior model, and a discriminative speaker subspace, or equivalently a speaker-discriminative distance metric. The directional scattering patterns of Gaussian mixture model mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Barras</author>
<author>X Zhu</author>
<author>S Meignier</author>
<author>J Gauvain</author>
</authors>
<title>Multistage speaker diarization of broadcast news.</title>
<date>2006</date>
<journal>IEEE Trans. ASLP,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1370" citStr="Barras et al., 2006" startWordPosition="195" endWordPosition="198">e distance metric. The directional scattering patterns of Gaussian mixture model mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA). 1 Introduction Speaker clustering is a critical part of speaker diarization (a.k.a. speaker segmentation and clustering) (Barras et al., 2006; Tranter and Reynolds, 2006; Wooters and Huijbregts, 2007; Han et al., 2008). Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker clustering is usually performed in a completely unsupervised manner. The output of speaker clustering is the internal labels relative to a dataset rather than real This work was funded in part by DARPA contract HR0011- 06-2-0001. 57 speaker identities. An interesting question is: Can we do semi-supervised speaker clustering? That is, can we make use of any available information </context>
</contexts>
<marker>Barras, Zhu, Meignier, Gauvain, 2006</marker>
<rawString>C. Barras, X. Zhu, S. Meignier, and J. Gauvain. 2006. Multistage speaker diarization of broadcast news. IEEE Trans. ASLP, 14(5):1505–1512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Campbell</author>
<author>D Sturim</author>
<author>D Reynolds</author>
</authors>
<title>Support vector machines using GMM supervectors for speaker verification.</title>
<date>2006</date>
<journal>Signal Processing Letters</journal>
<pages>13--5</pages>
<contexts>
<context position="4233" citStr="Campbell et al., 2006" startWordPosition="625" endWordPosition="628">ature vector space, we learn a speaker-discriminative feature transformation by linear discriminant analysis (LDA) based on the known speaker labels of the independent training set. The resulting low-dimensional feature subspace is expected to provide optimal speaker separability. 2.2 Utterance Representation Deviating from the mainstream “bag of acoustic features” representation where the extracted acoustic features are represented by a statistical model such as a Gaussian mixture model (GMM), we adopt the GMM mean supervector representation which has emerged in the speaker recognition area (Campbell et al., 2006). Such representation is obtained by maximum a posteriori (MAP) adapting a universal background model (UBM), which has been finely trained with all the data in the training set, to a particular utterance. The component means of the adapted GMM are stacked to form a column vector conventionally called a GMM mean supervector. In this way, we are allowed to represent an utterance as a point in a high-dimensional space where traditional distance metrics and clustering techniques can be naturally applied. The UBM, which can be deemed as a universal speaker prior model inferred from the independent </context>
</contexts>
<marker>Campbell, Sturim, Reynolds, 2006</marker>
<rawString>W. Campbell, D. Sturim, D. Reynolds. 2006. Support vector machines using GMM supervectors for speaker verification. Signal Processing Letters 13(5):308-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chu</author>
<author>H Kuo</author>
<author>L Mangu</author>
<author>Y Liu</author>
<author>Y Qin</author>
<author>Q Shi</author>
</authors>
<date>2008</date>
<booktitle>Recent advances in the IBM GALE mandarin transcription system. Proc. ICASSP.</booktitle>
<contexts>
<context position="10072" citStr="Chu et al., 2008" startWordPosition="1649" endWordPosition="1652">is straightforward to show that Equation 7 is equivalent to Equation 5 provided that the graph weights are set to proper values, as follows. 1 sjk ← (i) |D||D |if xj, xk ∈ Di, i = 1, ..., c sjk ←c(c − 1)|Dm||Dn |if xj ∈ Dm, xk ∈ Dn m, n = 1, ..., c, m =6 n (8) That is, by assigning appropriate values to the weights of the intrinsic and penalty graphs, the SDA optimization problem in Equation 5 can be solved within the elegant graph embedding framework. 4 Experiments Our speaker clustering experiments are based on a test set of 630 speakers and 19024 utterances selected from the GALE database (Chu et al., 2008), which contains about 1900 hours of broadcasting news speech data collected from various TV programs. An independent training set of 498 speakers and 18327 utterances is also selected from the GALE database. In either data set, there are an average of 30-40 utterances per speaker and the average duration of the utterances is about 3-4 seconds. Note that there are no overlapping speakers in the two data c X m=1 c X n=1 1 SB = c(c − 1) Smn (m =6 n) (4) 1 Smn = X yj∈Dm yk∈Dn yTj yk qyTj yj√yTk yk |Dm||Dn| 1 SW = c Xc i=1 X1 |Di||Di| xj,xk∈Di yTj yk qyTj yj√yTk yk xTj WWT xk 1 X |Dm||Dn |xj∈Dm xk</context>
</contexts>
<marker>Chu, Kuo, Mangu, Liu, Qin, Shi, 2008</marker>
<rawString>S. Chu, H. Kuo, L. Mangu, Y. Liu, Y. Qin, and Q. Shi. 2008. Recent advances in the IBM GALE mandarin transcription system. Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fu</author>
<author>S Yan</author>
<author>T Huang</author>
</authors>
<title>Correlation Metric for Generalized Feature Extraction.</title>
<date>2008</date>
<journal>IEEE Trans. PAMI</journal>
<volume>30</volume>
<issue>12</issue>
<contexts>
<context position="9404" citStr="Fu et al., 2008" startWordPosition="1518" endWordPosition="1521">nalty graph {X, S(p)} characterizes the data properties that the algorithm aims to avoid. The goal of graph embedding is to represent each vertex in X as a low dimensional vector that preserves the similarities in S. The objective function is P W=arg minW i6=j kf(xi,W)−f(xj,W)k2(s(i) ij −s(p) ij ) (6) where f(x, W) is a general projection with parameters W. If we take the projection to be of the form in Equation 2, the objective function becomes 2 (s(i) ij −s(p) ij ) (7) It is shown that the solution to the graph embedding problem of Equation 7 may be obtained by a steepest descent algorithm (Fu et al., 2008). If we expand the L2 norm terms of Equation 7, it is straightforward to show that Equation 7 is equivalent to Equation 5 provided that the graph weights are set to proper values, as follows. 1 sjk ← (i) |D||D |if xj, xk ∈ Di, i = 1, ..., c sjk ←c(c − 1)|Dm||Dn |if xj ∈ Dm, xk ∈ Dn m, n = 1, ..., c, m =6 n (8) That is, by assigning appropriate values to the weights of the intrinsic and penalty graphs, the SDA optimization problem in Equation 5 can be solved within the elegant graph embedding framework. 4 Experiments Our speaker clustering experiments are based on a test set of 630 speakers and</context>
</contexts>
<marker>Fu, Yan, Huang, 2008</marker>
<rawString>Y. Fu, S. Yan and T. Huang. 2008. Correlation Metric for Generalized Feature Extraction. IEEE Trans. PAMI 30(12):2229–2235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Han</author>
<author>S Kim</author>
<author>S Narayanan</author>
</authors>
<title>Strategies to Improve the Robustness of Agglomerative Hierarchical Clustering under Data Source Variation for Speaker Diarization.</title>
<date>2008</date>
<journal>IEEE Trans. SALP</journal>
<volume>16</volume>
<issue>8</issue>
<contexts>
<context position="1447" citStr="Han et al., 2008" startWordPosition="207" endWordPosition="210">l mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA). 1 Introduction Speaker clustering is a critical part of speaker diarization (a.k.a. speaker segmentation and clustering) (Barras et al., 2006; Tranter and Reynolds, 2006; Wooters and Huijbregts, 2007; Han et al., 2008). Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker clustering is usually performed in a completely unsupervised manner. The output of speaker clustering is the internal labels relative to a dataset rather than real This work was funded in part by DARPA contract HR0011- 06-2-0001. 57 speaker identities. An interesting question is: Can we do semi-supervised speaker clustering? That is, can we make use of any available information that can be helpful to speaker clustering? Our answer to this question is pos</context>
</contexts>
<marker>Han, Kim, Narayanan, 2008</marker>
<rawString>K. Han, S. Kim, and S. Narayanan. 2008. Strategies to Improve the Robustness of Agglomerative Hierarchical Clustering under Data Source Variation for Speaker Diarization. IEEE Trans. SALP 16(8):1590–1601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ma</author>
<author>S Lao</author>
<author>E Takikawa</author>
<author>M Kawade</author>
</authors>
<date>2007</date>
<booktitle>Discriminant Analysis in Correlation Similarity Measure Space. Proc. ICML</booktitle>
<pages>227--577</pages>
<contexts>
<context position="8157" citStr="Ma et al. (2007)" startWordPosition="1301" endWordPosition="1304"> average within-class cosine similarity can be written in terms of the unknown projection matrix W and the original data points x Si (3) X 1 Si = |Di||Di| yj,yk∈Di qxTj WWT xj√xTk WWT xk where |Di |denotes the number of data points in the ith class. Similarly, the average between-class cosine similarity can be written in terms of W and x xTj WWT xk = qxT j WWT xj√xTkWWT xk where |Dm |and |Dn |denote the number of data points in the mth and nth classes, respectively. The SDA criterion is to maximize SW while minimizing SB W = arg max(SW − SB) (5) W Our SDA formulation is similar to the work of Ma et al. (2007). However, we solve it efficiently in a general dimensionality reduction framework known as graph embedding (Yan et al., 2007). 3.2 Graph Embedding Solution In graph embedding, a weighted graph with vertex set X and similarity matrix S is used to characterize certain statistical or geometrical properties of a data set. A vertex in X represents a data point and an entry sij in S represents the similarity between the data points xi and xj. For a specific dimensionality reduction algorithm, there may exist two graphs. The intrinsic graph {X, S(i)} characterizes the data properties that the algori</context>
</contexts>
<marker>Ma, Lao, Takikawa, Kawade, 2007</marker>
<rawString>Y. Ma, S. Lao, E. Takikawa, and M. Kawade. 2007. Discriminant Analysis in Correlation Similarity Measure Space. Proc. ICML (227):577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tranter</author>
<author>D Reynolds</author>
</authors>
<title>An Overview of Automatic Speaker Diarization Systems.</title>
<date>2006</date>
<journal>IEEE Trans. ASLP,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1398" citStr="Tranter and Reynolds, 2006" startWordPosition="199" endWordPosition="202">e directional scattering patterns of Gaussian mixture model mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA). 1 Introduction Speaker clustering is a critical part of speaker diarization (a.k.a. speaker segmentation and clustering) (Barras et al., 2006; Tranter and Reynolds, 2006; Wooters and Huijbregts, 2007; Han et al., 2008). Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker clustering is usually performed in a completely unsupervised manner. The output of speaker clustering is the internal labels relative to a dataset rather than real This work was funded in part by DARPA contract HR0011- 06-2-0001. 57 speaker identities. An interesting question is: Can we do semi-supervised speaker clustering? That is, can we make use of any available information that can be helpful to speak</context>
</contexts>
<marker>Tranter, Reynolds, 2006</marker>
<rawString>S. Tranter and D. Reynolds. 2006. An Overview of Automatic Speaker Diarization Systems. IEEE Trans. ASLP, 14(5):1557–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wooters</author>
<author>M Huijbregts</author>
</authors>
<date>2007</date>
<booktitle>The ICSI RT07s Speaker Diarization System. LNCS.</booktitle>
<contexts>
<context position="1428" citStr="Wooters and Huijbregts, 2007" startWordPosition="203" endWordPosition="206">terns of Gaussian mixture model mean supervectors motivate us to perform discriminant analysis on the unit hypersphere rather than in the Euclidean space, which leads to a novel dimensionality reduction technique called spherical discriminant analysis (SDA). Our experiment results show that in the SDA subspace, speaker clustering yields superior performance than that in other reduceddimensional subspaces (e.g., PCA and LDA). 1 Introduction Speaker clustering is a critical part of speaker diarization (a.k.a. speaker segmentation and clustering) (Barras et al., 2006; Tranter and Reynolds, 2006; Wooters and Huijbregts, 2007; Han et al., 2008). Unlike speaker recognition, where we have the training data of a set of known speakers and thus recognition can be done supervised, speaker clustering is usually performed in a completely unsupervised manner. The output of speaker clustering is the internal labels relative to a dataset rather than real This work was funded in part by DARPA contract HR0011- 06-2-0001. 57 speaker identities. An interesting question is: Can we do semi-supervised speaker clustering? That is, can we make use of any available information that can be helpful to speaker clustering? Our answer to t</context>
</contexts>
<marker>Wooters, Huijbregts, 2007</marker>
<rawString>C. Wooters and M. Huijbregts. 2007. The ICSI RT07s Speaker Diarization System. LNCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yan</author>
<author>D Xu</author>
<author>B Zhang</author>
<author>H Zhang</author>
<author>Q Yang</author>
<author>S Lin</author>
</authors>
<title>Graph Embedding and Extensions: A General Framework for Dimensionality Reduction.</title>
<date>2007</date>
<journal>IEEE Trans. PAMI</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8283" citStr="Yan et al., 2007" startWordPosition="1320" endWordPosition="1323">ts x Si (3) X 1 Si = |Di||Di| yj,yk∈Di qxTj WWT xj√xTk WWT xk where |Di |denotes the number of data points in the ith class. Similarly, the average between-class cosine similarity can be written in terms of W and x xTj WWT xk = qxT j WWT xj√xTkWWT xk where |Dm |and |Dn |denote the number of data points in the mth and nth classes, respectively. The SDA criterion is to maximize SW while minimizing SB W = arg max(SW − SB) (5) W Our SDA formulation is similar to the work of Ma et al. (2007). However, we solve it efficiently in a general dimensionality reduction framework known as graph embedding (Yan et al., 2007). 3.2 Graph Embedding Solution In graph embedding, a weighted graph with vertex set X and similarity matrix S is used to characterize certain statistical or geometrical properties of a data set. A vertex in X represents a data point and an entry sij in S represents the similarity between the data points xi and xj. For a specific dimensionality reduction algorithm, there may exist two graphs. The intrinsic graph {X, S(i)} characterizes the data properties that the algorithm aims to preserve and the penalty graph {X, S(p)} characterizes the data properties that the algorithm aims to avoid. The g</context>
</contexts>
<marker>Yan, Xu, Zhang, Zhang, Yang, Lin, 2007</marker>
<rawString>S. Yan, D. Xu, B. Zhang, H. Zhang, Q. Yang, S. Lin. 2007. Graph Embedding and Extensions: A General Framework for Dimensionality Reduction. IEEE Trans. PAMI 29(1):40–51.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>