<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9991815">
Coordinate Structure Analysis with Global Structural Constraints and
Alignment-Based Local Features
</title>
<author confidence="0.993882">
Kazuo Hara Masashi Shimbo Hideharu Okuma Yuji Matsumoto
</author>
<affiliation confidence="0.963682">
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
</affiliation>
<email confidence="0.997979">
{kazuo-h,shimbo,hideharu-o,matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.993882" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982">
We propose a hybrid approach to coor-
dinate structure analysis that combines
a simple grammar to ensure consistent
global structure of coordinations in a sen-
tence, and features based on sequence
alignment to capture local symmetry of
conjuncts. The weight of the alignment-
based features, which in turn determines
the score of coordinate structures, is op-
timized by perceptron training on a given
corpus. A bottom-up chart parsing al-
gorithm efficiently finds the best scor-
ing structure, taking both nested or non-
overlapping flat coordinations into ac-
count. We demonstrate that our approach
outperforms existing parsers in coordina-
tion scope detection on the Genia corpus.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.92695425">
Coordinate structures are common in life sci-
ence literature. In Genia Treebank Beta (Kim et
al., 2003), the number of coordinate structures is
nearly equal to that of sentences. In clinical pa-
pers, the outcome of clinical trials is typically de-
scribed with coordination, as in
Median times to progression and median
survival times were 6.1 months and 8.9
months in arm A and 7.2 months and 9.5
months in arm B. (Schuette et al., 2006)
Despite the frequency and implied importance
of coordinate structures, coordination disambigua-
tion remains a difficult problem even for state-of-
the-art parsers. Figure 1(a) shows the coordinate
structure extracted from the output of Charniak
and Johnson’s (2005) parser on the above exam-
ple. This is somewhat surprising, given that the
symmetry of conjuncts in the sentence is obvious
to human eyes, and its correct coordinate structure
shown in Figure 1(b) can be readily observed.
in and 7.2 and 9.5 in
arm A months months arm B
6.1 and 8.9 in and 7.2 and 9.5 in
months months arm A months months arm B
</bodyText>
<figureCaption confidence="0.9650405">
Figure 1: (a) Output from the Charniak-Johnson
parser and (b) the correct coordinate structure.
</figureCaption>
<bodyText confidence="0.987616357142857">
Structural and semantic symmetry of conjuncts
is one of the frequently observed features of coor-
dination. This feature has been explored by previ-
ous studies on coordination, but these studies often
dealt with a restricted form of coordination with
apparently too much information provided from
outside. Sometimes it was assumed that the co-
ordinate structure contained two conjuncts each
solely composed of a few nouns; and in many
cases, the longest span of coordination (e.g., outer
noun phrase scopes) was given a priori. Such rich
information might be given by parsers, but this is
still an unfounded assumption.
In this paper, we approach coordination by tak-
ing an extreme stance, and assume that the input is
a whole sentence with no subsidiary information
except for the parts-of-speech of words.
As it assumes minimal information about syn-
tactic constructs, our method provides a baseline
for future work exploiting deeper syntactic infor-
mation for coordinate structure analysis. More-
over, this stand-alone approach has its own merits
as well:
1. Even apart from parsing, the output coordi-
nate structure alone may provide valuable in-
formation for higher-level applications, in the
same vein as the recent success of named
entity recognition and other shallow parsing
</bodyText>
<figure confidence="0.985212">
6.1 and 8.9
(a) months months
</figure>
<page confidence="0.963883">
967
</page>
<note confidence="0.999589">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 967–975,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.982883333333333">
technologies. One such potential application
is extracting the outcome of clinical tests as
illustrated above.
</bodyText>
<listItem confidence="0.9989264">
2. As the system is designed independently
from parsers, it can be combined with any
types of parsers (e.g., phrase structure or de-
pendency parsers), if necessary.
3. Because coordination bracketing is some-
</listItem>
<bodyText confidence="0.878807290322581">
times inconsistent with phrase structure
bracketing, processing coordinations apart
from phrase structures might be beneficial.
Consider, for example,
John likes, and Bill adores, Sue.
(Carston and Blakemore, 2005)
This kind of structure might be treated by as-
suming the presence of null elements, but the
current parsers have limited ability to detect
them. On the other hand, the symmetry of
conjuncts, John likes and Bill adores, is rather
obvious and should be easy to detect.
The method proposed in this paper builds a
tree-like coordinate structure from the input sen-
tence annotated with parts-of-speech. Each tree
is associated with a score, which is defined in
terms of features based on sequence alignment be-
tween conjuncts occurring in the tree. The feature
weights are optimized with a perceptron algorithm
on a training corpus annotated with the scopes of
conjuncts.
The reason we build a tree of coordinations is to
cope with nested coordinations, which are in fact
quite common. In Genia Treebank Beta, for ex-
ample, about 1/3 of the whole coordinations are
nested. The method proposed in this paper im-
proves upon our previous work (Shimbo and Hara,
2007) which also takes a sentence as input but is
restricted to flat coordinations. Our new method,
on the other hand, can successfully output the cor-
rect nested structure of Figure 1(b).
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999573533333333">
Resnik (1999) disambiguated coordinations of the
form [n1 and n2 n3], where ni are all nouns. This
type of phrase has two possible readings: [(n1)
and (n2 n3)] and [((n1) and (n2)) n3]. He demon-
strated the effectiveness of semantic similarity cal-
culated from a large text collection, and agreement
of numbers between n1 and n2 and between n1 and
n3. Nakov and Hearst (2005) collected web-based
statistics with search engines and applied them to
a task similar to Resnik’s.
Hogan (2007) improved the parsing accuracy
of sentences in which coordinated noun phrases
are known to exist. She presented a generative
model incorporating symmetry in conjunct struc-
tures and dependencies between coordinated head
words. The model was then used to rerank the n-
best outputs of the Bikel parser (2005).
Recently, Buyko et al. (2007; 2008) and
Shimbo and Hara (2007) applied discriminative
learning methods to coordinate structure analysis.
Buyko et al. used a linear-chain CRF, whereas
Shimbo and Hara proposed an approach based on
perceptron learning of edit distance between con-
juncts.
Shimbo and Hara’s approach has its root in
Kurohashi and Nagao’s (1994) rule-based method
for Japanese coordinations. Other studies on co-
ordination include (Agarwal and Boggess, 1992;
Chantree et al., 2005; Goldberg, 1999; Okumura
and Muraki, 1994).
</bodyText>
<sectionHeader confidence="0.99378" genericHeader="method">
3 Proposed method
</sectionHeader>
<bodyText confidence="0.999986428571429">
We propose a method for learning and detecting
the scopes of coordinations. It makes no assump-
tion about the number of coordinations in a sen-
tence, and the sentence can contain either nested
coordinations, multiple flat coordinations, or both.
The method consists of (i) a simple gram-
mar tailored for coordinate structure, and (ii) a
perceptron-based algorithm for learning feature
weights. The features are defined in terms of se-
quence alignment between conjuncts.
We thus use the grammar to filter out incon-
sistent nested coordinations and non-valid (over-
lapping) conjunct scopes, and the alignment-based
features to evaluate the similarity of conjuncts.
</bodyText>
<subsectionHeader confidence="0.97549">
3.1 Grammar for coordinations
</subsectionHeader>
<bodyText confidence="0.9998091">
The sole objective of the grammar we present be-
low is to ensure the consistency of two or more
coordinations in a sentence; i.e., for any two co-
ordinations, either (i) they must be totally non-
overlapping (non-nested coordinations), or (ii) one
coordination must be embedded within the scope
of a conjunct of the other coordination (nested co-
ordinations).
Below, we call a parse tree built from the gram-
mar a coordination tree.
</bodyText>
<page confidence="0.999356">
968
</page>
<tableCaption confidence="0.997998">
Table 1: Non-terminals
</tableCaption>
<table confidence="0.6128678">
COORD Complete coordination.
COORD&apos; Partially-built coordination.
CJT Conjunct.
N Non-coordination.
CC Coordinate conjunction like “and,”
</table>
<tableCaption confidence="0.910112">
“or,” and “but”.
SEP Connector of conjuncts other than CC:
e.g., punctuations like “,” and “;”.
W Any word.
Table 2: Production rules for coordination trees.
</tableCaption>
<bodyText confidence="0.433592">
(...  |...  |...) denotes a disjunction (matches any
one of the elements). A ‘*’ matches any word.
</bodyText>
<subsectionHeader confidence="0.645573">
Rules for coordinations:
</subsectionHeader>
<equation confidence="0.3854425">
(i) COORDi,m — CJTi, j CC j+1,k−1 CJTk,m
(ii) COORDi,n — CJTi, j SEPj+1,k−1 COORD&apos;k,n[m]
(iii) COORD&apos;i,m[ j] — CJTi, j CC j+1,k−1 CJTk,m
(iv) COORD&apos;i,n[ j] — CJTi, j SEP j+1,k−1 COORD&apos;k,n[m]
Rules for conjuncts:
(v) CJTi, j — (COORD  |N)i, j
</equation>
<subsectionHeader confidence="0.931915">
Rules for non-coordinations:
</subsectionHeader>
<listItem confidence="0.964847666666667">
(vi) Ni,k — COORDi, j N j+1,k
(vii) Ni,j — Wi,i (COORD|N)i+1,j
(viii) Ni,i — Wi,i
</listItem>
<subsectionHeader confidence="0.9932155">
Rules for pre-terminals:
3.1.1 Non-terminals
</subsectionHeader>
<bodyText confidence="0.999946571428572">
The grammar is composed of non-terminal sym-
bols listed in Table 1. The distinction between
COORD and COORD&apos; is made to cope with three or
more conjuncts in a coordination. For example
“a , b and c” is treated as a tree of the form (a ,
(b and c))), and the inner tree (b and c) is not a
complete coordination, until it is conjoined with
the first conjunct a. We represent this inner tree
by a COORD&apos; (partial coordination), to distinguish it
from a complete coordination represented by CO-
ORD. Compare Figures 2(a) and (b), which respec-
tively depict the coordination tree for this exam-
ple, and a tree for nested coordination with a sim-
ilar structure.
</bodyText>
<subsectionHeader confidence="0.948995">
3.1.2 Production rules
</subsectionHeader>
<bodyText confidence="0.9999514375">
Table 2 lists the production rules. Rules are shown
with explicit subscripts indicating the span of their
production. The subscript to a terminal word
(shown in a box) specifies its position within a sen-
tence (word index). Non-terminals have two sub-
script indices denoting the span of the production.
COORD&apos; in rules (iii) and (iv) has an extra in-
dex j shown in brackets. This bracketed index
maintains the end of the first conjunct (CJT) on
the right-hand side. After a COORD&apos; is produced
by these rules, it may later constitute a larger CO-
ORD or COORD&apos; through the application of produc-
tions (ii) or (iv). At this point, the bracketed in-
dex of the constituent COORD&apos; allows us to identify
the scope of the first conjunct immediately under-
neath. As we describe in Section 3.2.4, the scope
of this conjunct is necessary to compute the score
of coordination trees.
These grammar rules are admittedly minimal
and need further elaboration to cover all real use
cases of coordination (e.g., conjunctive phrases
like “as well as”, etc.). Yet they are sufficient to
generate the basic trees illustrated in Figure 2. The
experiments of Section 5 will apply this grammar
on a real biomedical corpus.
Note that although non-conjunction cue expres-
sions, such as “both” and “either,” are not the
part of this grammar, such cues can be learned
(through perceptron training) from training exam-
ples if appropriate features are introduced. Indeed,
in Section 5 we use features indicating which
words precede coordinations.
</bodyText>
<subsectionHeader confidence="0.999974">
3.2 Score of a coordination tree
</subsectionHeader>
<bodyText confidence="0.99996675">
Given a sentence, our system outputs the coordina-
tion tree with the highest score among all possible
trees for the sentence. The score of a coordination
tree is simply the sum of the scores of all its nodes,
and the node scores are computed independently
from each other. Hence a bottom-up chart parsing
algorithm can be designed to efficiently compute
the highest scoring tree.
While scores can be assigned to any nodes, we
have chosen to assign a non-zero score only to two
types of coordination nodes, namely COORD and
COORD&apos;, in the experiment of Section 5; all other
nodes are ignored in score computation. The score
of a coordination node is defined via sequence
alignment (Gusfield, 1997) between conjuncts be-
low the node, to capture the symmetry of these
</bodyText>
<figure confidence="0.985053392857143">
(ix) SEPi,i — (,  |; )i
(x) Wi,i — *i
(xi) CCi,i — (and
 |or  |but
)i
(xii) CCi,i+1 — (,  |; )i (and
 |or  |but )i+1
969
COORD N
COORD
N
W
W
N
W
c
b
(c) a
COORD&apos;
N SEP CC N N CC
W N and W W or
(a) a W c (b) a N
, b W
b
CC N
W
and c
COORD
</figure>
<figureCaption confidence="0.998712857142857">
Figure 2: Coordination trees for (a) a coordination with three conjuncts, (b) nested coordinations, and
(c) a non-coordination. The CJT nodes in (a) and (b) are omitted for brevity.
Figure 3: A coordination tree for the example sen-
tence presented in Section 1, with the edit graphs
attached to COORD nodes.
Figure 4: An edit graph and an alignment path
(bold line).
</figureCaption>
<bodyText confidence="0.953599375">
conjuncts.
Figure 3 schematically illustrates the relation
between a coordination tree and alignment-based
computation of the coordination nodes. The score
of this tree is given by the sum of the scores of the
four COORD nodes, and the score of a COORD node
is computed with the edit graph shown above the
node.
</bodyText>
<subsectionHeader confidence="0.908768">
3.2.1 Edit graph
</subsectionHeader>
<bodyText confidence="0.999981142857143">
The edit graph is a basic data structure for comput-
ing sequence alignment. An example edit graph is
depicted in Figure 4 for word sequences “Median
times to progression” and “median survival times.”
A diagonal edge represents alignment (or sub-
stitution) between the word at the top of the edge
and the one on the left, while horizontal and ver-
tical edges represent skipping (or deletion) of re-
spective word. With this representation, a path
starting from the top-left corner (initial vertex) and
arriving at the bottom-right corner (terminal ver-
tex) corresponds one-to-one to a sequence of edit
operations transforming one word sequence to the
other.
In standard sequence alignment, each edge of an
edit graph is associated with a score representing
the merit of the corresponding edit operation. By
defining the score of a path as the total score of its
component edges, we can assess the similarity of
a pair of sequences as the maximum score over all
paths in its edit graph.
</bodyText>
<subsectionHeader confidence="0.508174">
3.2.2 Features
</subsectionHeader>
<bodyText confidence="0.999967913043478">
In our model, instead of assigning a score inde-
pendently to edges of an edit graph, we assign a
vector of features to edges. The score of an edge
is the inner product of this feature vector and an-
other vector w, called global weight vector. Fea-
ture vectors may differ from one edge to another,
but the vector w is unique in the entire system and
consistently determines the relative importance of
individual features.
In parallel to the definition of a path score, the
feature vector of a path can be defined as the sum
of the feature vectors assigned to its component
edges. Then the score of a path is equal to the
inner product (w,f) of w and the feature vector f
of the path.
A feature assigned to an edge can be an arbi-
trary indicator of edge directions (horizontal, ver-
tical, or diagonal), edge coordinates in the edit
graph, attributes (such as the surface form, part-
of-speech, and the location in the sentence) of the
current or surrounding words, or their combina-
tion. Section 5.3 will describe the exact features
used in our experiments.
</bodyText>
<figure confidence="0.984025094736842">
W W W CC W W W W W CC W W CC W W W W W
W W W W CC W W W
7.2
months
and
9.5
months
in
arm
B
median
survival
times
N
N
6.1
months
and
8.9
months
in
arm
A
Median
times
to
progression
COORD
months
6.1
8.9
months
N
COORD
7.2
months
9.5
months
N
N
N
N
N
N
N
COORD
N
N
N
N
N
N
N
N
N
N
N
N
N N COORD
N
Median
times
to
progression
terminal vertex
initial vertex
median
survival
times
Median
times
to
progression
and
median
survival
times
were
6.1
months
and
8.9
months
in
arm
A
and
7.2
months
and
9.5
months
in
arm
B
</figure>
<page confidence="0.812678">
970
</page>
<bodyText confidence="0.967704292682927">
3.2.3 Averaged path score as the score of a
coordination node
Finally, we define the score of a COORD (or COORD&apos;)
node in a coordination tree as the average score
of all paths in its associated edit graph. This is an-
other deviation from standard sequence alignment,
in that we do not take the maximum scoring paths
as representing the similarity of conjuncts, but in-
stead use the average over all paths.
Notice that the average is taken over paths, and
not edges. In this way, a natural bias is incurred
towards features occurring near the diagonal con-
necting the initial vertex and the terminal vertex.
For instance, in an edit graph of size 8 x 8, there
is only one path that goes through the vertex at the
top-right corner, while more than 3,600 paths pass
through the vertex at the center of the graph. In
other words, the features associated with the cen-
ter vertex receives 3,600 times more weights than
those at the top-right corner after averaging.
The major benefit of this averaging is the re-
duced computation during training. During the
perceptron training, the global weight vector w
changes and the score of individual paths changes
accordingly. On the other hand, the average fea-
ture vector f (as opposed to the average score
(w,f)) over all paths in the edit graph remains
constant. This means that f can be pre-computed
once before the training starts, and the score com-
putation during training reduces to simply taking
the inner product of the current w and the pre-
computed f.
Alternatively, the alignment score could be de-
fined as that of the best scoring path with respect
to the current w, following the standard sequence
alignment computation. However, it would require
running the Viterbi algorithm in each iteration of
the perceptron training, for all possible spans of
conjuncts. While we first pursued this direction,
it was abandoned as the training was intolerably
slow.
</bodyText>
<subsectionHeader confidence="0.4888105">
3.2.4 Coordination with three or more
conjuncts
</subsectionHeader>
<bodyText confidence="0.99995775">
For a coordination with three or more conjuncts,
we define its score as the sum of the similarity
scores of all pairwise consecutive conjuncts; i.e.,
for a coordination “a, b, c, and d” with four con-
juncts, the score is the sum of the similarity scores
for conjunct pairs (a, b), (b, c), and (c, d). Ide-
ally, we should take all combinations of conjuncts
into account, but it would lead to a combinatorial
</bodyText>
<figure confidence="0.95649725">
COORD
N SEP N SEP N CC N
W W W W
a , b , c and d
</figure>
<figureCaption confidence="0.9791275">
Figure 5: A coordination tree with four conjuncts.
All CJT nodes are omitted.
</figureCaption>
<bodyText confidence="0.986042444444445">
explosion and is impractical.
Recall that in the grammar introduced in Sec-
tion 3.1, we attached a bracketed index to COORD&apos;.
This bracketed index was introduced for the com-
putation of this pairwise similarity.
Figure 5 shows the coordination tree for “a, b,
c, and d.” The pairwise similarity scores for (a,
b), (b, c), and (c, d) are respectively computed at
the top COORD, left COORD&apos;, and right COORD&apos; nodes,
using the scheme described in Section 3.2.3. To
compute the similarity of a and b, we need to lift
the information about the end position of b upward
to the COORD node. The same applies to computing
the similarity of b and c; the end position of c is
needed at the left COORD&apos;. The bracketed index of
COORD&apos; exactly maintains this information, i.e., the
end of the first conjunct below the COORD&apos;. See
production rules (iii) and (iv) in Table 2.
</bodyText>
<subsectionHeader confidence="0.997255">
3.3 Perceptron learning of feature weights
</subsectionHeader>
<bodyText confidence="0.9999446">
As we saw above, our model is a linear model with
the global weight vector w acting as the coefficient
vector, and hence various existing techniques can
be exploited to optimize w.
In this paper, we use the averaged perceptron
learning (Collins, 2002; Freund and Schapire,
1999) to optimize w on a training corpus, so that
the system assigns the highest score to the correct
coordination tree among all possible trees for each
training sentence.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999411">
4.1 Computational complexity
</subsectionHeader>
<bodyText confidence="0.999771857142857">
Given an input sentence of N words, finding its
maximum scoring coordination tree by a bottom-
up chart parsing algorithm incurs a time complex-
ity of O(N3).
While the right-hand side of rules (i)–(iv) in-
volves more than three variables and thus appears
to increase complexity, this is not the case since
</bodyText>
<figure confidence="0.648531">
COORD&apos;
COORD&apos;
</figure>
<page confidence="0.991051">
971
</page>
<bodyText confidence="0.999907722222222">
some of the variables (j and k in rules (i) and (iii),
and j, k, and m in rules (ii) and (iv)) are con-
strained by the location of conjunct connectors (CC
and SEP), whose number in a sentence is negligi-
ble compared to the sentence length N. As a result,
these rules can be processed in O(N2) time. Hence
the run-time complexity is dominated by rule (vi),
which has three variables and leads to O(N3).
Each iteration of the perceptron algorithm for
a sentence of length N also incurs O(N3) for the
same reason.
Our method also requires pre-processing in the
beginning of perceptron training, to compute the
average feature vectors f for all possible spans
(i, j) and (k,m) of conjuncts in a sentence. With a
reasoning similar to the complexity analysis of the
chart parsing algorithm above, we can show that
the pre-processing takes O(N4) time.
</bodyText>
<subsectionHeader confidence="0.7102725">
4.2 Difference from Shimbo and Hara’s
method
</subsectionHeader>
<bodyText confidence="0.99996375">
The method proposed in this paper extends the
work of Shimbo and Hara (2007). Both take a
whole sentence as input and use perceptron learn-
ing, and the difference lies in how hypothesis co-
ordination(s) are encoded as a feature vector.
Unlike our new method which constructs a tree
of coordinations, Shimbo and Hara used a chain-
able partial paths (representing non-overlapping
series of local alignments; see (Shimbo and Hara,
2007, Figure 5)) in a global triangular edit graph.
In our method, we compute many edit graphs of
smaller size, one for each possible conjunct pair in
a sentence. We use global alignment (a complete
path) in these smaller graphs, as opposed to chain-
able local alignment (partial paths) in a global edit
graph used by Shimbo and Hara.
Since nested coordinations cannot be encoded
as chainable partial paths (Shimbo and Hara,
2007), their method cannot cope with nested coor-
dinations such as those illustrated in Figure 2(b).
</bodyText>
<subsectionHeader confidence="0.99967">
4.3 Integration with parsers
</subsectionHeader>
<bodyText confidence="0.999930266666667">
Charniak and Johnson (2005) reported an im-
proved parsing accuracy by reranking n-best parse
trees, using features based on similarity of coor-
dinated phrases, among others. It should be inter-
esting to investigate whether alignment-based fea-
tures like ours can be built into their reranker, or
more generally, whether the coordination scopes
output by our method help improving parsing ac-
curacy.
The combinatory categorial grammar (CCG)
(Steedman, 2000) provides an account for vari-
ous coordination constructs in an elegant manner,
and incorporating alignment-based features into
the CCG parser (Clark and Curran, 2007) is also
a viable possibility.
</bodyText>
<sectionHeader confidence="0.994772" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999418">
We evaluated the performance of our method1 on
the Genia corpus (Kim et al., 2003).
</bodyText>
<subsectionHeader confidence="0.978253">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999904363636364">
Genia Treebank Beta is a collection of Penn
Treebank-like phrase structure trees for 4529 sen-
tences from Medline abstracts.
In this corpus, each scope of coordinate struc-
tures is annotated with an explicit tag, and the
conjuncts are always placed inside brackets. Not
many treebanks explicitly mark the scope of con-
juncts; for example, the Penn Treebank frequently
omits bracketing of coordination and conjunct
scopes, leaving them as a flat structure.
Genia contains a total of 4129 occurrences of
COOD tags indicating coordination. These tags are
further subcategorized into phrase types such as
NP-COOD and VP-COOD. Among coordinations anno-
tated with COOD tags, we selected those surround-
ing “and,” “or,” and “but.” This yielded 3598 co-
ordinations (2997, 355, and 246 for “and,” “or,”
and “but,” respectively) in 2508 sentences. These
coordinations constitute nearly 90% of all coordi-
nations in Genia, and we used them as the evalua-
tion dataset. The length of these sentences is 30.0
words on average.
</bodyText>
<subsectionHeader confidence="0.998571">
5.2 Evaluation method
</subsectionHeader>
<bodyText confidence="0.811576222222222">
We tested the proposed method in two tasks:
(i) identify the scope of coordinations regardless
of phrase types, and
(ii) detect noun phrase (NP) coordinations and
identify their scopes.
While the goal of task (i) is to determine the scopes
of 3598 coordinations, task (ii) demands both to
judge whether each of the coordinations constructs
an NP, and if it does, to determine its scope.
</bodyText>
<footnote confidence="0.99736175">
1A C++ implementation of our method can be found
at http://cl.naist.jp/project/coordination/, along with supple-
mentary materials including the preliminary experimental re-
sults of the CCG parser on the same dataset.
</footnote>
<page confidence="0.992738">
972
</page>
<tableCaption confidence="0.998508">
Table 3: Features in the edit graph for conjuncts wkwk+1 ···wm and wlwl+1 ···wn.
</tableCaption>
<table confidence="0.998517315789474">
edge/vertex type ..··· vertical edge ··· horizontal ··· .. .. ··· ··· diagonal wj edge initial vertex terminal wn−1 vertex
. wj−1 wj wj+1 . wj−1 wj edge . wj−1 wj+1 ··· wl wl+1 ··· .··· wn
wj+1
wi−1 wi−1 wi−1
wi wi wi wk ..
wm−1
wi+1 wi+1 wi+1 wk+1
wm
. . ........ . .
vertical bigrams wi−1wi wi−1wi wi−1wi wk−2wk−1 wm−2wm−1
wiwi+1 wiwi+1 wk−1wk wm−1wm
wkwk+1 wmwm+1
horizontal bigrams wj−1wj wj−1wj wj−1wj wl−2wl−1 wn−2wn−1
wjwj+1 wjwj+1 wl−1wl wn−1wn
wlwl+1 wnwn+1
orthogonal bigrams wiwj wk−1wl−1 wm−1wn−1
wk−1wl wm−1wn
wkwl−1 wmwn−1
wkwl wmwn
</table>
<bodyText confidence="0.999933454545455">
For comparison, two parsers, the Bikel-Collins
parser (Bikel, 2005)2 and Charniak-Johnson
reranking parser3, were applied in both tasks.
Task (ii) imitates the evaluation reported by
Shimbo and Hara (2007), and to compare our
method with their coordination analysis method.
Because their method can only process flat coordi-
nations, in task (ii) we only used 1613 sentences in
which “and” occurs just once, following (Shimbo
and Hara, 2007). Note however that the split of
data is different from their experiments.
We evaluate the performance of the tested meth-
ods by the accuracy of coordination-level brack-
eting (Shimbo and Hara, 2007); i.e., we count
each of the coordination (as opposed to conjunct)
scopes as one output of the system, and the system
output is deemed correct if the beginning of the
first output conjunct and the end of the last con-
junct both match annotations in the Genia Tree-
bank.
In both tasks, we report the micro-averaged re-
sults of five-fold cross validation.
The Bikel-Collins and Charniak-Johnson
parsers were trained on Genia, using all the phrase
structure trees in the corpus except the test set;
i.e., the training set also contains (in addition to
the four folds) 2021(= 4129 − 2508) sentences
which are not in the five folds. Since the two
parsers were also trained on Genia, we interpret
the bracketing above each conjunction in the
parse tree output by them as the coordination
scope output by the parsers, in accordance with
how coordinations are annotated in Genia. In
</bodyText>
<footnote confidence="0.996275">
2http://www.cis.upenn.edu/∼dbikel/software.html
3ftp://ftp.cs.brown.edu/pub/nlparser/
reranking-parserAug06.tar.gz
</footnote>
<bodyText confidence="0.99935925">
testing, the Bikel-Collins parser and Shimbo-Hara
method were given the gold parts-of-speech
(POS) of the test sentences in Genia. We trained
the proposed method twice, once with the gold
POS tags and once with the POS tags output by
the Charniak-Johnson parser. This is because the
Charniak-Johnson parser does not accept POS
tags of the test sentences.
</bodyText>
<subsectionHeader confidence="0.854035">
5.3 Features
</subsectionHeader>
<bodyText confidence="0.999993538461538">
To compute features for our method, each word
in a sentence was represented as a list of at-
tributes. The attributes include the surface word,
part-of-speech, suffix, prefix, and the indicators
of whether the word is capitalized, whether it is
composed of all uppercase letters or digits, and
whether it contains digits or hyphens. All fea-
tures are defined as an indicator of an attribute in
two words coming from either a single conjunct
(either horizontal or vertical word sequences asso-
ciated with the edit graph) or two conjuncts (one
from the horizontal word sequence and one from
the vertical sequence). We call the first type hori-
zontal/vertical bigrams and the second orthogonal
bigrams.
Table 3 summarizes the features in an edit
graph for two conjuncts (wkwk+1 ···wm) and
(wlwl+1 ···wn), where wi denotes the ith word in
the sentence.
As seen from the table, features are assigned
to the initial and terminal vertices as well as to
edges. A wiwj in the table indicates that for each
attribute (e.g., part-of-speech, etc.), an indicator
function for the combination of the attribute val-
ues in wi and wj is assigned to the vertex or edge
shown in the figure above. Note that the features
</bodyText>
<page confidence="0.998795">
973
</page>
<tableCaption confidence="0.856014">
Table 4: Results of Task (i). The number of coor-
</tableCaption>
<bodyText confidence="0.756327">
dinations of each type (#), and the recall (%) for
the proposed method, Bikel-Collins parser (BC),
and Charniak-Johnson parser (CJ).
</bodyText>
<table confidence="0.999523083333334">
gold POS CJ POS
COOD # Proposed BC Proposed CJ
Overall 3598 61.5 52.1 57.5 52.9
NP 2317 64.2 45.5 62.5 50.1
VP 465 54.2 67.7 42.6 61.9
ADJP 321 80.4 66.4 76.3 48.6
S 188 22.9 67.0 15.4 63.3
PP 167 59.9 53.3 53.9 58.1
UCP 60 36.7 18.3 38.3 26.7
SBAR 56 51.8 85.7 33.9 83.9
ADVP 21 85.7 90.5 85.7 90.5
Others 3 66.7 33.3 33.3 0.0
</table>
<bodyText confidence="0.97157725">
assigned to different types of vertex or edge are
treated as distinct even if the word indices i and j
are identical; i.e., all features are conditioned on
edge/vertex types to which they are assigned.
</bodyText>
<subsectionHeader confidence="0.592218">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.998686433333333">
Task (i) Table 4 shows the results of task (i). We
only list the recall score in the table, as precision
(and hence F1-measure, too) was equal to recall
for all methods in this task; this is not surpris-
ing given that in this data set, conjunctions “and”,
“or”, and “but” always indicate the existence of a
coordination, and all methods successfully learned
this trend from the training data.
The proposed method outperformed parsers on
the coordination scope identification overall. The
table also indicates that our method considerably
outperformed two parsers on NP-COOD, ADJP-COOD,
and UCP-COOD categories, but it did not work well
on VP-COOD, S-COOD, and SBAR-COOD. In contrast,
the parsers performed quite well in the latter cate-
gories.
Task (ii) Table 5 lists the results of task (ii).
The proposed method outperformed Shimbo-Hara
method in this task, although the setting of this
task is mostly identical to (Shimbo and Hara,
2007) and does not include nested coordinations.
Note also that both methods use roughly equiva-
lent features.
One reason should be that our grammar rules
can strictly enforce the scope consistency of con-
juncts in coordinations with three or more con-
juncts. Because the Shimbo-Hara method repre-
sents such coordinations as a series of sub-paths
in an edit graph which are output independently
of each other without enforcing consistency, their
</bodyText>
<tableCaption confidence="0.517968333333333">
Table 5: Results of Task (ii). Proposed method,
BC: Bikel-Collins, CJ: Charniak-Johnson, SH:
Shimbo-Hara.
</tableCaption>
<table confidence="0.9989058">
gold POS CJ POS
Proposed BC SH Proposed CJ
Precision 61.7 45.6 55.9 60.2 49.0
Recall 57.9 46.1 53.7 55.6 46.8
F1 59.7 45.8 54.8 57.8 47.9
</table>
<bodyText confidence="0.966994555555556">
method can produce inconsistent scopes of con-
juncts in the middle.
In fact, the advantage of the proposed method in
task (ii) is noticeable especially in coordinations
with three or more conjuncts; if we restrict the test
set only to coordinations with three or more con-
juncts, the F-measures in the proposed method and
Shimbo-Hara become 53.0 and 42.3, respectively;
i.e., the margin increases to 10.7 from 4.9 points.
</bodyText>
<sectionHeader confidence="0.997286" genericHeader="conclusions">
6 Conclusion and outlook
</sectionHeader>
<bodyText confidence="0.999968857142857">
We have proposed a method for learning and
analyzing generic coordinate structures including
nested coordinations. It consists of a simple gram-
mar for coordination and perceptron learning of
alignment-based features.
The method performed well overall and on co-
ordinated noun and adjective phrases, but not on
coordinated verb phrases and sentences. The lat-
ter coordination types are in fact easy for parsers,
as the experimental results show.
The proposed method failing in verbal and sen-
tential coordinations is as expected, since con-
juncts in these coordinations are not necessarily
similar, if they are viewed as a sequence of words.
We will investigate similarity measures different
from sequence alignment, to better capture the
symmetry of these conjuncts.
We will also pursue integration of our method
with parsers. Because they have advantages in dif-
ferent coordination phrase types, their integration
looks promising.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99993">
We thank anonymous reviewers for helpful com-
ments and the pointer to the combinatory catego-
rial grammar.
</bodyText>
<sectionHeader confidence="0.99642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.640258">
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Associa-
</reference>
<page confidence="0.9913">
974
</page>
<reference confidence="0.999852448979592">
tion for Computational Linguistics (ACL’92), pages
15–21.
Daniel M. Bikel. 2005. Multilingual statistical pars-
ing engine version 0.9.9c. http://www.cis.upenn.
edu/∼dbikel/software.html.
Ekaterina Buyko and Udo Hahn. 2008. Are morpho-
syntactic features more predicative for the resolution
of noun phrase coordination ambiguity than lexico-
semantic similarity scores. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING 2008), pages 89–96, Manch-
ester, UK.
Ekaterina Buyko, Katrin Tomanek, and Udo Hahn.
2007. Resolution of coordination ellipses in bi-
ological named entities using conditional random
fields. In Proceedings of the Pacific Association
for Computational Linguistics (PACLIC’07), pages
163–171.
Robyn Carston and Diane Blakemore. 2005. Editorial:
Introduction to coordination: syntax, semantics and
pragmatics. Lingua, 115:353–358.
Francis Chantree, Adam Kilgarriff, Anne de Roeck,
and Alistair Willis. 2005. Disambiguating coor-
dinations using word distribution information. In
Proceedings of the Int’l Conference on Recent Ad-
vances in Natural Language Processing, Borovets,
Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), pages 173–180, Ann Arbor, Michigan,
USA.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2002), pages 1–
8, Philadelphia, PA, USA.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning, 37(3):277–296.
Miriam Goldberg. 1999. An unsupervised model
for statistically determining coordinate phrase at-
tachment. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics
(ACL 1999), pages 610–614, College Park, Mary-
land, USA.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007), pages 680–
687, Prague, Czech Republic.
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003.
GENIA corpus: a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl. 1):i180–
i182.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20:507–534.
Preslav Nakov and Marti Hearst. 2005. Using the web
as an implicit training set: application to structural
ambiguity resolution. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language (HLT-
EMNLP 2005), pages 835–842, Vancouver, Canada.
Akitoshi Okumura and Kazunori Muraki. 1994. Sym-
metric pattern matching analysis for English coordi-
nate structures. In Proceedings of the Fourth Con-
ference on Applied Natural Language Processing,
pages 41–46.
Philip Resnik. 1999. Semantic similarity in a tax-
onomy. Journal of Artificial Intelligence Research,
11:95–130.
Wolfgang Schuette, Thomas Blankenburg, Wolf
Guschall, Ina Dittrich, Michael Schroeder, Hans
Schweisfurth, Assaad Chemaissani, Christian Schu-
mann, Nikolas Dickgreber, Tabea Appel, and Di-
eter Ukena. 2006. Multicenter randomized trial for
stage iiib/iv non-small-cell lung cancer using every-
3-week versus weekly paclitaxel/carboplatin. Clini-
cal Lung Cancer, 7:338–343.
Masashi Shimbo and Kazuo Hara. 2007. A discrimi-
native learning model for coordinate conjunctions.
In Proceedings of Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL 2007), pages 610–619, Prague, Czech Re-
public.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
</reference>
<page confidence="0.998858">
975
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938703">
<title confidence="0.9991455">Coordinate Structure Analysis with Global Structural Constraints and Alignment-Based Local Features</title>
<author confidence="0.998528">Kazuo Hara Masashi Shimbo Hideharu Okuma Yuji Matsumoto</author>
<affiliation confidence="0.9994345">Graduate School of Information Science Nara Institute of Science and Technology</affiliation>
<address confidence="0.997679">Ikoma, Nara 630-0192, Japan</address>
<abstract confidence="0.996920666666667">We propose a hybrid approach to coordinate structure analysis that combines a simple grammar to ensure consistent global structure of coordinations in a sentence, and features based on sequence alignment to capture local symmetry of conjuncts. The weight of the alignmentbased features, which in turn determines the score of coordinate structures, is optimized by perceptron training on a given corpus. A bottom-up chart parsing algorithm efficiently finds the best scoring structure, taking both nested or nonoverlapping flat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajeev Agarwal</author>
<author>Lois Boggess</author>
</authors>
<title>A simple but useful approach to conjunct identification.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL’92),</booktitle>
<pages>15--21</pages>
<contexts>
<context position="6544" citStr="Agarwal and Boggess, 1992" startWordPosition="1037" endWordPosition="1040">metry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). 3 Proposed method We propose a method for learning and detecting the scopes of coordinations. It makes no assumption about the number of coordinations in a sentence, and the sentence can contain either nested coordinations, multiple flat coordinations, or both. The method consists of (i) a simple grammar tailored for coordinate structure, and (ii) a perceptron-based algorithm for learning feature weights. The features are defined in terms of sequence alignment between conjuncts. We thus use the grammar to filter out inconsiste</context>
</contexts>
<marker>Agarwal, Boggess, 1992</marker>
<rawString>Rajeev Agarwal and Lois Boggess. 1992. A simple but useful approach to conjunct identification. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL’92), pages 15–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Multilingual statistical parsing engine version 0.9.9c.</title>
<date>2005</date>
<note>http://www.cis.upenn. edu/∼dbikel/software.html.</note>
<contexts>
<context position="24181" citStr="Bikel, 2005" startWordPosition="4081" endWordPosition="4082">ge/vertex type ..··· vertical edge ··· horizontal ··· .. .. ··· ··· diagonal wj edge initial vertex terminal wn−1 vertex . wj−1 wj wj+1 . wj−1 wj edge . wj−1 wj+1 ··· wl wl+1 ··· .··· wn wj+1 wi−1 wi−1 wi−1 wi wi wi wk .. wm−1 wi+1 wi+1 wi+1 wk+1 wm . . ........ . . vertical bigrams wi−1wi wi−1wi wi−1wi wk−2wk−1 wm−2wm−1 wiwi+1 wiwi+1 wk−1wk wm−1wm wkwk+1 wmwm+1 horizontal bigrams wj−1wj wj−1wj wj−1wj wl−2wl−1 wn−2wn−1 wjwj+1 wjwj+1 wl−1wl wn−1wn wlwl+1 wnwn+1 orthogonal bigrams wiwj wk−1wl−1 wm−1wn−1 wk−1wl wm−1wn wkwl−1 wmwn−1 wkwl wmwn For comparison, two parsers, the Bikel-Collins parser (Bikel, 2005)2 and Charniak-Johnson reranking parser3, were applied in both tasks. Task (ii) imitates the evaluation reported by Shimbo and Hara (2007), and to compare our method with their coordination analysis method. Because their method can only process flat coordinations, in task (ii) we only used 1613 sentences in which “and” occurs just once, following (Shimbo and Hara, 2007). Note however that the split of data is different from their experiments. We evaluate the performance of the tested methods by the accuracy of coordination-level bracketing (Shimbo and Hara, 2007); i.e., we count each of the co</context>
</contexts>
<marker>Bikel, 2005</marker>
<rawString>Daniel M. Bikel. 2005. Multilingual statistical parsing engine version 0.9.9c. http://www.cis.upenn. edu/∼dbikel/software.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Buyko</author>
<author>Udo Hahn</author>
</authors>
<title>Are morphosyntactic features more predicative for the resolution of noun phrase coordination ambiguity than lexicosemantic similarity scores.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING</booktitle>
<pages>89--96</pages>
<location>Manchester, UK.</location>
<marker>Buyko, Hahn, 2008</marker>
<rawString>Ekaterina Buyko and Udo Hahn. 2008. Are morphosyntactic features more predicative for the resolution of noun phrase coordination ambiguity than lexicosemantic similarity scores. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pages 89–96, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Buyko</author>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
</authors>
<title>Resolution of coordination ellipses in biological named entities using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the Pacific Association for Computational Linguistics (PACLIC’07),</booktitle>
<pages>163--171</pages>
<contexts>
<context position="6105" citStr="Buyko et al. (2007" startWordPosition="972" endWordPosition="975"> the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) collected web-based statistics with search engines and applied them to a task similar to Resnik’s. Hogan (2007) improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. She presented a generative model incorporating symmetry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). 3 Proposed method We propose a method for learning and detecting the scopes of coordinations.</context>
</contexts>
<marker>Buyko, Tomanek, Hahn, 2007</marker>
<rawString>Ekaterina Buyko, Katrin Tomanek, and Udo Hahn. 2007. Resolution of coordination ellipses in biological named entities using conditional random fields. In Proceedings of the Pacific Association for Computational Linguistics (PACLIC’07), pages 163–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robyn Carston</author>
<author>Diane Blakemore</author>
</authors>
<title>Editorial: Introduction to coordination: syntax, semantics and pragmatics.</title>
<date>2005</date>
<booktitle>Lingua,</booktitle>
<pages>115--353</pages>
<contexts>
<context position="4134" citStr="Carston and Blakemore, 2005" startWordPosition="642" endWordPosition="645">4th IJCNLP of the AFNLP, pages 967–975, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP technologies. One such potential application is extracting the outcome of clinical tests as illustrated above. 2. As the system is designed independently from parsers, it can be combined with any types of parsers (e.g., phrase structure or dependency parsers), if necessary. 3. Because coordination bracketing is sometimes inconsistent with phrase structure bracketing, processing coordinations apart from phrase structures might be beneficial. Consider, for example, John likes, and Bill adores, Sue. (Carston and Blakemore, 2005) This kind of structure might be treated by assuming the presence of null elements, but the current parsers have limited ability to detect them. On the other hand, the symmetry of conjuncts, John likes and Bill adores, is rather obvious and should be easy to detect. The method proposed in this paper builds a tree-like coordinate structure from the input sentence annotated with parts-of-speech. Each tree is associated with a score, which is defined in terms of features based on sequence alignment between conjuncts occurring in the tree. The feature weights are optimized with a perceptron algori</context>
</contexts>
<marker>Carston, Blakemore, 2005</marker>
<rawString>Robyn Carston and Diane Blakemore. 2005. Editorial: Introduction to coordination: syntax, semantics and pragmatics. Lingua, 115:353–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Chantree</author>
<author>Adam Kilgarriff</author>
<author>Anne de Roeck</author>
<author>Alistair Willis</author>
</authors>
<title>Disambiguating coordinations using word distribution information.</title>
<date>2005</date>
<booktitle>In Proceedings of the Int’l Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Borovets, Bulgaria.</location>
<marker>Chantree, Kilgarriff, de Roeck, Willis, 2005</marker>
<rawString>Francis Chantree, Adam Kilgarriff, Anne de Roeck, and Alistair Willis. 2005. Disambiguating coordinations using word distribution information. In Proceedings of the Int’l Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="21123" citStr="Charniak and Johnson (2005)" startWordPosition="3591" endWordPosition="3594">ies of local alignments; see (Shimbo and Hara, 2007, Figure 5)) in a global triangular edit graph. In our method, we compute many edit graphs of smaller size, one for each possible conjunct pair in a sentence. We use global alignment (a complete path) in these smaller graphs, as opposed to chainable local alignment (partial paths) in a global edit graph used by Shimbo and Hara. Since nested coordinations cannot be encoded as chainable partial paths (Shimbo and Hara, 2007), their method cannot cope with nested coordinations such as those illustrated in Figure 2(b). 4.3 Integration with parsers Charniak and Johnson (2005) reported an improved parsing accuracy by reranking n-best parse trees, using features based on similarity of coordinated phrases, among others. It should be interesting to investigate whether alignment-based features like ours can be built into their reranker, or more generally, whether the coordination scopes output by our method help improving parsing accuracy. The combinatory categorial grammar (CCG) (Steedman, 2000) provides an account for various coordination constructs in an elegant manner, and incorporating alignment-based features into the CCG parser (Clark and Curran, 2007) is also a</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 173–180, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="21713" citStr="Clark and Curran, 2007" startWordPosition="3679" endWordPosition="3682">rsers Charniak and Johnson (2005) reported an improved parsing accuracy by reranking n-best parse trees, using features based on similarity of coordinated phrases, among others. It should be interesting to investigate whether alignment-based features like ours can be built into their reranker, or more generally, whether the coordination scopes output by our method help improving parsing accuracy. The combinatory categorial grammar (CCG) (Steedman, 2000) provides an account for various coordination constructs in an elegant manner, and incorporating alignment-based features into the CCG parser (Clark and Curran, 2007) is also a viable possibility. 5 Evaluation We evaluated the performance of our method1 on the Genia corpus (Kim et al., 2003). 5.1 Dataset Genia Treebank Beta is a collection of Penn Treebank-like phrase structure trees for 4529 sentences from Medline abstracts. In this corpus, each scope of coordinate structures is annotated with an explicit tag, and the conjuncts are always placed inside brackets. Not many treebanks explicitly mark the scope of conjuncts; for example, the Penn Treebank frequently omits bracketing of coordination and conjunct scopes, leaving them as a flat structure. Genia c</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<volume>1</volume>
<pages>pages</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="18672" citStr="Collins, 2002" startWordPosition="3179" endWordPosition="3180">nd position of b upward to the COORD node. The same applies to computing the similarity of b and c; the end position of c is needed at the left COORD&apos;. The bracketed index of COORD&apos; exactly maintains this information, i.e., the end of the first conjunct below the COORD&apos;. See production rules (iii) and (iv) in Table 2. 3.3 Perceptron learning of feature weights As we saw above, our model is a linear model with the global weight vector w acting as the coefficient vector, and hence various existing techniques can be exploited to optimize w. In this paper, we use the averaged perceptron learning (Collins, 2002; Freund and Schapire, 1999) to optimize w on a training corpus, so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence. 4 Discussion 4.1 Computational complexity Given an input sentence of N words, finding its maximum scoring coordination tree by a bottomup chart parsing algorithm incurs a time complexity of O(N3). While the right-hand side of rules (i)–(iv) involves more than three variables and thus appears to increase complexity, this is not the case since COORD&apos; COORD&apos; 971 some of the variables (j and k in rules (i</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 1– 8, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="18700" citStr="Freund and Schapire, 1999" startWordPosition="3181" endWordPosition="3184">b upward to the COORD node. The same applies to computing the similarity of b and c; the end position of c is needed at the left COORD&apos;. The bracketed index of COORD&apos; exactly maintains this information, i.e., the end of the first conjunct below the COORD&apos;. See production rules (iii) and (iv) in Table 2. 3.3 Perceptron learning of feature weights As we saw above, our model is a linear model with the global weight vector w acting as the coefficient vector, and hence various existing techniques can be exploited to optimize w. In this paper, we use the averaged perceptron learning (Collins, 2002; Freund and Schapire, 1999) to optimize w on a training corpus, so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence. 4 Discussion 4.1 Computational complexity Given an input sentence of N words, finding its maximum scoring coordination tree by a bottomup chart parsing algorithm incurs a time complexity of O(N3). While the right-hand side of rules (i)–(iv) involves more than three variables and thus appears to increase complexity, this is not the case since COORD&apos; COORD&apos; 971 some of the variables (j and k in rules (i) and (iii), and j, k, and m</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Goldberg</author>
</authors>
<title>An unsupervised model for statistically determining coordinate phrase attachment.</title>
<date>1999</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>610--614</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="6583" citStr="Goldberg, 1999" startWordPosition="1045" endWordPosition="1046">een coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). 3 Proposed method We propose a method for learning and detecting the scopes of coordinations. It makes no assumption about the number of coordinations in a sentence, and the sentence can contain either nested coordinations, multiple flat coordinations, or both. The method consists of (i) a simple grammar tailored for coordinate structure, and (ii) a perceptron-based algorithm for learning feature weights. The features are defined in terms of sequence alignment between conjuncts. We thus use the grammar to filter out inconsistent nested coordinations and non-valid (</context>
</contexts>
<marker>Goldberg, 1999</marker>
<rawString>Miriam Goldberg. 1999. An unsupervised model for statistically determining coordinate phrase attachment. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 1999), pages 610–614, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11495" citStr="Gusfield, 1997" startWordPosition="1859" endWordPosition="1860">e among all possible trees for the sentence. The score of a coordination tree is simply the sum of the scores of all its nodes, and the node scores are computed independently from each other. Hence a bottom-up chart parsing algorithm can be designed to efficiently compute the highest scoring tree. While scores can be assigned to any nodes, we have chosen to assign a non-zero score only to two types of coordination nodes, namely COORD and COORD&apos;, in the experiment of Section 5; all other nodes are ignored in score computation. The score of a coordination node is defined via sequence alignment (Gusfield, 1997) between conjuncts below the node, to capture the symmetry of these (ix) SEPi,i — (, |; )i (x) Wi,i — *i (xi) CCi,i — (and |or |but )i (xii) CCi,i+1 — (, |; )i (and |or |but )i+1 969 COORD N COORD N W W N W c b (c) a COORD&apos; N SEP CC N N CC W N and W W or (a) a W c (b) a N , b W b CC N W and c COORD Figure 2: Coordination trees for (a) a coordination with three conjuncts, (b) nested coordinations, and (c) a non-coordination. The CJT nodes in (a) and (b) are omitted for brevity. Figure 3: A coordination tree for the example sentence presented in Section 1, with the edit graphs attached to COORD </context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
</authors>
<title>Coordinate noun phrase disambiguation in a generative parsing model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>680--687</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5771" citStr="Hogan (2007)" startWordPosition="921" endWordPosition="922">ations. Our new method, on the other hand, can successfully output the correct nested structure of Figure 1(b). 2 Related work Resnik (1999) disambiguated coordinations of the form [n1 and n2 n3], where ni are all nouns. This type of phrase has two possible readings: [(n1) and (n2 n3)] and [((n1) and (n2)) n3]. He demonstrated the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) collected web-based statistics with search engines and applied them to a task similar to Resnik’s. Hogan (2007) improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. She presented a generative model incorporating symmetry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo an</context>
</contexts>
<marker>Hogan, 2007</marker>
<rawString>Deirdre Hogan. 2007. Coordinate noun phrase disambiguation in a generative parsing model. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL 2007), pages 680– 687, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus: a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="1106" citStr="Kim et al., 2003" startWordPosition="156" endWordPosition="159">tures based on sequence alignment to capture local symmetry of conjuncts. The weight of the alignmentbased features, which in turn determines the score of coordinate structures, is optimized by perceptron training on a given corpus. A bottom-up chart parsing algorithm efficiently finds the best scoring structure, taking both nested or nonoverlapping flat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus. 1 Introduction Coordinate structures are common in life science literature. In Genia Treebank Beta (Kim et al., 2003), the number of coordinate structures is nearly equal to that of sentences. In clinical papers, the outcome of clinical trials is typically described with coordination, as in Median times to progression and median survival times were 6.1 months and 8.9 months in arm A and 7.2 months and 9.5 months in arm B. (Schuette et al., 2006) Despite the frequency and implied importance of coordinate structures, coordination disambiguation remains a difficult problem even for state-ofthe-art parsers. Figure 1(a) shows the coordinate structure extracted from the output of Charniak and Johnson’s (2005) pars</context>
<context position="21839" citStr="Kim et al., 2003" startWordPosition="3701" endWordPosition="3704">ilarity of coordinated phrases, among others. It should be interesting to investigate whether alignment-based features like ours can be built into their reranker, or more generally, whether the coordination scopes output by our method help improving parsing accuracy. The combinatory categorial grammar (CCG) (Steedman, 2000) provides an account for various coordination constructs in an elegant manner, and incorporating alignment-based features into the CCG parser (Clark and Curran, 2007) is also a viable possibility. 5 Evaluation We evaluated the performance of our method1 on the Genia corpus (Kim et al., 2003). 5.1 Dataset Genia Treebank Beta is a collection of Penn Treebank-like phrase structure trees for 4529 sentences from Medline abstracts. In this corpus, each scope of coordinate structures is annotated with an explicit tag, and the conjuncts are always placed inside brackets. Not many treebanks explicitly mark the scope of conjuncts; for example, the Penn Treebank frequently omits bracketing of coordination and conjunct scopes, leaving them as a flat structure. Genia contains a total of 4129 occurrences of COOD tags indicating coordination. These tags are further subcategorized into phrase ty</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GENIA corpus: a semantically annotated corpus for bio-textmining. Bioinformatics, 19(Suppl. 1):i180– i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--507</pages>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20:507–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the web as an implicit training set: application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLTEMNLP</booktitle>
<pages>835--842</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5659" citStr="Nakov and Hearst (2005)" startWordPosition="902" endWordPosition="905">roves upon our previous work (Shimbo and Hara, 2007) which also takes a sentence as input but is restricted to flat coordinations. Our new method, on the other hand, can successfully output the correct nested structure of Figure 1(b). 2 Related work Resnik (1999) disambiguated coordinations of the form [n1 and n2 n3], where ni are all nouns. This type of phrase has two possible readings: [(n1) and (n2 n3)] and [((n1) and (n2)) n3]. He demonstrated the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) collected web-based statistics with search engines and applied them to a task similar to Resnik’s. Hogan (2007) improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. She presented a generative model incorporating symmetry in conjunct structures and dependencies between coordinated head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the web as an implicit training set: application to structural ambiguity resolution. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language (HLTEMNLP 2005), pages 835–842, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akitoshi Okumura</author>
<author>Kazunori Muraki</author>
</authors>
<title>Symmetric pattern matching analysis for English coordinate structures.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing,</booktitle>
<pages>41--46</pages>
<contexts>
<context position="6610" citStr="Okumura and Muraki, 1994" startWordPosition="1047" endWordPosition="1050">head words. The model was then used to rerank the nbest outputs of the Bikel parser (2005). Recently, Buyko et al. (2007; 2008) and Shimbo and Hara (2007) applied discriminative learning methods to coordinate structure analysis. Buyko et al. used a linear-chain CRF, whereas Shimbo and Hara proposed an approach based on perceptron learning of edit distance between conjuncts. Shimbo and Hara’s approach has its root in Kurohashi and Nagao’s (1994) rule-based method for Japanese coordinations. Other studies on coordination include (Agarwal and Boggess, 1992; Chantree et al., 2005; Goldberg, 1999; Okumura and Muraki, 1994). 3 Proposed method We propose a method for learning and detecting the scopes of coordinations. It makes no assumption about the number of coordinations in a sentence, and the sentence can contain either nested coordinations, multiple flat coordinations, or both. The method consists of (i) a simple grammar tailored for coordinate structure, and (ii) a perceptron-based algorithm for learning feature weights. The features are defined in terms of sequence alignment between conjuncts. We thus use the grammar to filter out inconsistent nested coordinations and non-valid (overlapping) conjunct scope</context>
</contexts>
<marker>Okumura, Muraki, 1994</marker>
<rawString>Akitoshi Okumura and Kazunori Muraki. 1994. Symmetric pattern matching analysis for English coordinate structures. In Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 41–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<contexts>
<context position="5299" citStr="Resnik (1999)" startWordPosition="841" endWordPosition="842">e weights are optimized with a perceptron algorithm on a training corpus annotated with the scopes of conjuncts. The reason we build a tree of coordinations is to cope with nested coordinations, which are in fact quite common. In Genia Treebank Beta, for example, about 1/3 of the whole coordinations are nested. The method proposed in this paper improves upon our previous work (Shimbo and Hara, 2007) which also takes a sentence as input but is restricted to flat coordinations. Our new method, on the other hand, can successfully output the correct nested structure of Figure 1(b). 2 Related work Resnik (1999) disambiguated coordinations of the form [n1 and n2 n3], where ni are all nouns. This type of phrase has two possible readings: [(n1) and (n2 n3)] and [((n1) and (n2)) n3]. He demonstrated the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) collected web-based statistics with search engines and applied them to a task similar to Resnik’s. Hogan (2007) improved the parsing accuracy of sentences in which coordinated noun phrases are known to exist. She presented a generative mod</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Schuette</author>
<author>Thomas Blankenburg</author>
<author>Wolf Guschall</author>
<author>Ina Dittrich</author>
<author>Michael Schroeder</author>
<author>Hans Schweisfurth</author>
<author>Assaad Chemaissani</author>
<author>Christian Schumann</author>
<author>Nikolas Dickgreber</author>
<author>Tabea Appel</author>
<author>Dieter Ukena</author>
</authors>
<title>Multicenter randomized trial for stage iiib/iv non-small-cell lung cancer using every3-week versus weekly paclitaxel/carboplatin. Clinical Lung Cancer,</title>
<date>2006</date>
<pages>7--338</pages>
<contexts>
<context position="1438" citStr="Schuette et al., 2006" startWordPosition="215" endWordPosition="218">nested or nonoverlapping flat coordinations into account. We demonstrate that our approach outperforms existing parsers in coordination scope detection on the Genia corpus. 1 Introduction Coordinate structures are common in life science literature. In Genia Treebank Beta (Kim et al., 2003), the number of coordinate structures is nearly equal to that of sentences. In clinical papers, the outcome of clinical trials is typically described with coordination, as in Median times to progression and median survival times were 6.1 months and 8.9 months in arm A and 7.2 months and 9.5 months in arm B. (Schuette et al., 2006) Despite the frequency and implied importance of coordinate structures, coordination disambiguation remains a difficult problem even for state-ofthe-art parsers. Figure 1(a) shows the coordinate structure extracted from the output of Charniak and Johnson’s (2005) parser on the above example. This is somewhat surprising, given that the symmetry of conjuncts in the sentence is obvious to human eyes, and its correct coordinate structure shown in Figure 1(b) can be readily observed. in and 7.2 and 9.5 in arm A months months arm B 6.1 and 8.9 in and 7.2 and 9.5 in months months arm A months months </context>
</contexts>
<marker>Schuette, Blankenburg, Guschall, Dittrich, Schroeder, Schweisfurth, Chemaissani, Schumann, Dickgreber, Appel, Ukena, 2006</marker>
<rawString>Wolfgang Schuette, Thomas Blankenburg, Wolf Guschall, Ina Dittrich, Michael Schroeder, Hans Schweisfurth, Assaad Chemaissani, Christian Schumann, Nikolas Dickgreber, Tabea Appel, and Dieter Ukena. 2006. Multicenter randomized trial for stage iiib/iv non-small-cell lung cancer using every3-week versus weekly paclitaxel/carboplatin. Clinical Lung Cancer, 7:338–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masashi Shimbo</author>
<author>Kazuo Hara</author>
</authors>
<title>A discriminative learning model for coordinate conjunctions.</title>
<date>2007</date>
<booktitle>In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>610--619</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5088" citStr="Shimbo and Hara, 2007" startWordPosition="803" endWordPosition="806">tructure from the input sentence annotated with parts-of-speech. Each tree is associated with a score, which is defined in terms of features based on sequence alignment between conjuncts occurring in the tree. The feature weights are optimized with a perceptron algorithm on a training corpus annotated with the scopes of conjuncts. The reason we build a tree of coordinations is to cope with nested coordinations, which are in fact quite common. In Genia Treebank Beta, for example, about 1/3 of the whole coordinations are nested. The method proposed in this paper improves upon our previous work (Shimbo and Hara, 2007) which also takes a sentence as input but is restricted to flat coordinations. Our new method, on the other hand, can successfully output the correct nested structure of Figure 1(b). 2 Related work Resnik (1999) disambiguated coordinations of the form [n1 and n2 n3], where ni are all nouns. This type of phrase has two possible readings: [(n1) and (n2 n3)] and [((n1) and (n2)) n3]. He demonstrated the effectiveness of semantic similarity calculated from a large text collection, and agreement of numbers between n1 and n2 and between n1 and n3. Nakov and Hearst (2005) collected web-based statisti</context>
<context position="20194" citStr="Shimbo and Hara (2007)" startWordPosition="3439" endWordPosition="3442"> rule (vi), which has three variables and leads to O(N3). Each iteration of the perceptron algorithm for a sentence of length N also incurs O(N3) for the same reason. Our method also requires pre-processing in the beginning of perceptron training, to compute the average feature vectors f for all possible spans (i, j) and (k,m) of conjuncts in a sentence. With a reasoning similar to the complexity analysis of the chart parsing algorithm above, we can show that the pre-processing takes O(N4) time. 4.2 Difference from Shimbo and Hara’s method The method proposed in this paper extends the work of Shimbo and Hara (2007). Both take a whole sentence as input and use perceptron learning, and the difference lies in how hypothesis coordination(s) are encoded as a feature vector. Unlike our new method which constructs a tree of coordinations, Shimbo and Hara used a chainable partial paths (representing non-overlapping series of local alignments; see (Shimbo and Hara, 2007, Figure 5)) in a global triangular edit graph. In our method, we compute many edit graphs of smaller size, one for each possible conjunct pair in a sentence. We use global alignment (a complete path) in these smaller graphs, as opposed to chainab</context>
<context position="24319" citStr="Shimbo and Hara (2007)" startWordPosition="4099" endWordPosition="4102">wj wj+1 . wj−1 wj edge . wj−1 wj+1 ··· wl wl+1 ··· .··· wn wj+1 wi−1 wi−1 wi−1 wi wi wi wk .. wm−1 wi+1 wi+1 wi+1 wk+1 wm . . ........ . . vertical bigrams wi−1wi wi−1wi wi−1wi wk−2wk−1 wm−2wm−1 wiwi+1 wiwi+1 wk−1wk wm−1wm wkwk+1 wmwm+1 horizontal bigrams wj−1wj wj−1wj wj−1wj wl−2wl−1 wn−2wn−1 wjwj+1 wjwj+1 wl−1wl wn−1wn wlwl+1 wnwn+1 orthogonal bigrams wiwj wk−1wl−1 wm−1wn−1 wk−1wl wm−1wn wkwl−1 wmwn−1 wkwl wmwn For comparison, two parsers, the Bikel-Collins parser (Bikel, 2005)2 and Charniak-Johnson reranking parser3, were applied in both tasks. Task (ii) imitates the evaluation reported by Shimbo and Hara (2007), and to compare our method with their coordination analysis method. Because their method can only process flat coordinations, in task (ii) we only used 1613 sentences in which “and” occurs just once, following (Shimbo and Hara, 2007). Note however that the split of data is different from their experiments. We evaluate the performance of the tested methods by the accuracy of coordination-level bracketing (Shimbo and Hara, 2007); i.e., we count each of the coordination (as opposed to conjunct) scopes as one output of the system, and the system output is deemed correct if the beginning of the fi</context>
<context position="28965" citStr="Shimbo and Hara, 2007" startWordPosition="4874" endWordPosition="4877">on, and all methods successfully learned this trend from the training data. The proposed method outperformed parsers on the coordination scope identification overall. The table also indicates that our method considerably outperformed two parsers on NP-COOD, ADJP-COOD, and UCP-COOD categories, but it did not work well on VP-COOD, S-COOD, and SBAR-COOD. In contrast, the parsers performed quite well in the latter categories. Task (ii) Table 5 lists the results of task (ii). The proposed method outperformed Shimbo-Hara method in this task, although the setting of this task is mostly identical to (Shimbo and Hara, 2007) and does not include nested coordinations. Note also that both methods use roughly equivalent features. One reason should be that our grammar rules can strictly enforce the scope consistency of conjuncts in coordinations with three or more conjuncts. Because the Shimbo-Hara method represents such coordinations as a series of sub-paths in an edit graph which are output independently of each other without enforcing consistency, their Table 5: Results of Task (ii). Proposed method, BC: Bikel-Collins, CJ: Charniak-Johnson, SH: Shimbo-Hara. gold POS CJ POS Proposed BC SH Proposed CJ Precision 61.7</context>
</contexts>
<marker>Shimbo, Hara, 2007</marker>
<rawString>Masashi Shimbo and Kazuo Hara. 2007. A discriminative learning model for coordinate conjunctions. In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL 2007), pages 610–619, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="21547" citStr="Steedman, 2000" startWordPosition="3657" endWordPosition="3658">le partial paths (Shimbo and Hara, 2007), their method cannot cope with nested coordinations such as those illustrated in Figure 2(b). 4.3 Integration with parsers Charniak and Johnson (2005) reported an improved parsing accuracy by reranking n-best parse trees, using features based on similarity of coordinated phrases, among others. It should be interesting to investigate whether alignment-based features like ours can be built into their reranker, or more generally, whether the coordination scopes output by our method help improving parsing accuracy. The combinatory categorial grammar (CCG) (Steedman, 2000) provides an account for various coordination constructs in an elegant manner, and incorporating alignment-based features into the CCG parser (Clark and Curran, 2007) is also a viable possibility. 5 Evaluation We evaluated the performance of our method1 on the Genia corpus (Kim et al., 2003). 5.1 Dataset Genia Treebank Beta is a collection of Penn Treebank-like phrase structure trees for 4529 sentences from Medline abstracts. In this corpus, each scope of coordinate structures is annotated with an explicit tag, and the conjuncts are always placed inside brackets. Not many treebanks explicitly </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>