<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008346">
<title confidence="0.991411">
Models of Semantic Representation with Visual Attributes
</title>
<author confidence="0.999808">
Carina Silberer, Vittorio Ferrari, Mirella Lapata
</author>
<affiliation confidence="0.999937">
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.992212">
10 Crichton Street, Edinburgh EH8 9AB
</address>
<email confidence="0.996396">
c.silberer@ed.ac.uk, vferrari@inf.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899333333333">
We consider the problem of grounding the
meaning of words in the physical world
and focus on the visual modality which we
represent by visual attributes. We create
a new large-scale taxonomy of visual at-
tributes covering more than 500 concepts
and their corresponding 688K images. We
use this dataset to train attribute classi-
fiers and integrate their predictions with
text-based distributional models of word
meaning. We show that these bimodal
models give a better fit to human word as-
sociation data compared to amodal models
and word representations based on hand-
crafted norming data.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991555">
Recent years have seen increased interest in
grounded language acquisition, where the goal is
to extract representations of the meaning of nat-
ural language tied to the physical world. The
language grounding problem has assumed sev-
eral guises in the literature such as semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Kate and Mooney, 2007; Lu et
al., 2008; B¨orschinger et al., 2011), mapping nat-
ural language instructions to executable actions
(Branavan et al., 2009; Tellex et al., 2011), associ-
ating simplified language to perceptual data such
as images or video (Siskind, 2001; Roy and Pent-
land, 2002; Gorniak and Roy, 2004; Yu and Bal-
lard, 2007), and learning the meaning of words
based on linguistic and perceptual input (Bruni
et al., 2012b; Feng and Lapata, 2010; Johns and
Jones, 2012; Andrews et al., 2009; Silberer and
Lapata, 2012).
In this paper we are concerned with the latter
task, namely constructing perceptually grounded
distributional models. The motivation for models
that do not learn exclusively from text is twofold.
From a cognitive perspective, there is mounting
experimental evidence suggesting that our inter-
action with the physical world plays an impor-
tant role in the way we process language (Barsa-
lou, 2008; Bornstein et al., 2004; Landau et al.,
1998). From an engineering perspective, the abil-
ity to learn representations for multimodal data has
many practical applications including image re-
trieval (Datta et al., 2008) and annotation (Chai
and Hung, 2008), text illustration (Joshi et al.,
2006), object and scene recognition (Lowe, 1999;
Oliva and Torralba, 2007; Fei-Fei and Perona,
2005), and robot navigation (Tellex et al., 2011).
One strand of research uses feature norms as a
stand-in for sensorimotor experience (Johns and
Jones, 2012; Andrews et al., 2009; Steyvers, 2010;
Silberer and Lapata, 2012). Feature norms are ob-
tained by asking native speakers to write down at-
tributes they consider important in describing the
meaning of a word. The attributes represent per-
ceived physical and functional properties associ-
ated with the referents of words. For example,
apples are typically green or red, round, shiny,
smooth, crunchy, tasty, and so on; dogs have four
legs and bark, whereas chairs are used for sit-
ting. Feature norms are instrumental in reveal-
ing which dimensions of meaning are psychologi-
cally salient, however, their use as a proxy for peo-
ple’s perceptual representations can itself be prob-
lematic (Sloman and Ripps, 1998; Zeigenfuse and
Lee, 2010). The number and types of attributes
generated can vary substantially as a function of
the amount of time devoted to each concept. It is
not entirely clear how people generate attributes
and whether all of these are important for repre-
senting concepts. Finally, multiple participants are
required to create a representation for each con-
</bodyText>
<page confidence="0.950914">
572
</page>
<note confidence="0.9142405">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999970315789474">
cept, which limits elicitation studies to a small
number of concepts and the scope of any compu-
tational model based on feature norms.
Another strand of research focuses exclusively
on the visual modality, even though the grounding
problem could involve auditory, motor, and hap-
tic modalities as well. This is not entirely sur-
prising. Visual input represents a major source of
data from which humans can learn semantic rep-
resentations of linguistic and non-linguistic com-
municative actions (Regier, 1996). Furthermore,
since images are ubiquitous, visual data can be
gathered far easier than some of the other modali-
ties. Distributional models that integrate the visual
modality have been learned from texts and im-
ages (Feng and Lapata, 2010; Bruni et al., 2012b)
or from ImageNet (Deng et al., 2009), e.g., by
exploiting the fact that images in this database
are hierarchically organized according to WordNet
synsets (Leong and Mihalcea, 2011). Images are
typically represented on the basis of low-level fea-
tures such as SIFT (Lowe, 2004), whereas texts
are treated as bags of words.
Our work also focuses on images as a way
of physically grounding the meaning of words.
We, however, represent them by high-level vi-
sual attributes instead of low-level image fea-
tures. Attributes are not concept or category spe-
cific (e.g., animals have stripes and so do cloth-
ing items; balls are round, and so are oranges and
coins), and thus allow us to express similarities
and differences across concepts more easily. Fur-
thermore, attributes allow us to generalize to un-
seen objects; it is possible to say something about
them even though we cannot identify them (e.g., it
has a beak and a long tail). We show that this
attribute-centric approach to representing images
is beneficial for distributional models of lexical
meaning. Our attributes are similar to those pro-
vided by participants in norming studies, however,
importantly they are learned from training data (a
database of images and their visual attributes) and
thus generalize to new images without additional
human involvement.
In the following we describe our efforts to cre-
ate a new large-scale dataset that consists of 688K
images that match the same concrete concepts
used in the feature norming study of McRae et al.
(2005). We derive a taxonomy of 412 visual at-
tributes and explain how we learn attribute clas-
sifiers following recent work in computer vision
(Lampert et al., 2009; Farhadi et al., 2009). Next,
we show that this attribute-based image represen-
tation can be usefully integrated with textual data
to create distributional models that give a better fit
to human word association data over models that
rely on human generated feature norms.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999148844444444">
Grounding semantic representations with visual
information is an instance of multimodal learn-
ing. In this setting the data consists of multiple
input modalities with different representations and
the learner’s objective is to extract a unified repre-
sentation that fuses the modalities together. The
literature describes several successful approaches
to multimodal learning using different variants of
deep networks (Ngiam et al., 2011; Srivastava and
Salakhutdinov, 2012) and data sources including
text, images, audio, and video.
Special-purpose models that address the fusion
of distributional meaning with visual information
have been also proposed. Feng and Lapata (2010)
represent documents and images by a common
multimodal vocabulary consisting of textual words
and visual terms which they obtain by quantizing
SIFT descriptors (Lowe, 2004). Their model is es-
sentially Latent Dirichlet Allocation (LDA, Blei et
al., 2003) trained on a corpus of multimodal docu-
ments (i.e., BBC news articles and their associated
images). Meaning in this model is represented as
a vector whose components correspond to word-
topic distributions. A related model has been pro-
posed by Bruni et al. (2012b) who obtain distinct
representations for the textual and visual modali-
ties. Specifically, they extract a visual space from
images contained in the ESP-Game data set (von
Ahn and Dabbish, 2004) and a text-based seman-
tic space from a large corpus collection totaling
approximately two billion words. They concate-
nate the two modalities and subsequently project
them to a lower-dimensionality space using Sin-
gular Value Decomposition (Golub et al., 1981).
Traditionally, computer vision algorithms de-
scribe visual phenomena (e.g., objects, scenes,
faces, actions) by giving each instance a categor-
ical label (e.g., cat, beer garden, Brad Pitt, drink-
ing). The ability to describe images by their at-
tributes allows to generalize to new instances for
which there are no training examples available.
Moreover, attributes can transcend category and
task boundaries and thus provide a generic de-
scription of visual data.
Initial work (Ferrari and Zisserman, 2007)
</bodyText>
<page confidence="0.997556">
573
</page>
<bodyText confidence="0.963397245901639">
focused on simple color and texture attributes
(e.g., blue, stripes) and showed that these can be
learned in a weakly supervised setting from im-
ages returned by a search engine when using the
attribute as a query. Farhadi et al. (2009) were
among the first to use visual attributes in an ob-
ject recognition task. Using an inventory of 64 at-
tribute labels, they developed a dataset of approx-
imately 12,000 instances representing 20 objects
from the PASCAL Visual Object Classes Chal-
lenge 2008 (Everingham et al., 2008). Visual
semantic attributes (e.g., hairy, four-legged) were
used to identify familiar objects and to describe
unfamiliar objects when new images and bound-
ing box annotations were provided. Lampert et al.
(2009) showed that attribute-based representations
can be used to classify objects when there are no
training examples of the target classes available.
Their dataset contained over 30,000 images repre-
senting 50 animal concepts and used 85 attributes
from the norming study of Osherson et al. (1991).
Attribute-based representations have also been ap-
plied to the tasks of face detection (Kumar et al.,
2009), action identification (Liu et al., 2011), and
scene recognition (Patterson and Hays, 2012).
The use of visual attributes in models of distri-
butional semantics is novel to our knowledge. We
argue that they are advantageous for two reasons.
Firstly, they are cognitively plausible; humans em-
ploy visual attributes when describing the proper-
ties of concept classes. Secondly, they occupy the
middle ground between non-linguistic low-level
image features and linguistic words. Attributes
crucially represent image properties, however by
being words themselves, they can be easily inte-
grated in any text-based distributional model thus
eschewing known difficulties with rendering im-
ages into word-like units.
A key prerequisite in describing images by
their attributes is the availability of training data
for learning attribute classifiers. Although our
database shares many features with previous work
(Lampert et al., 2009; Farhadi et al., 2009) it dif-
fers in focus and scope. Since our goal is to
develop distributional models that are applicable
to many words, it contains a considerably larger
number of concepts (i.e., more than 500) and at-
tributes (i.e., 412) based on a detailed taxonomy
which we argue is cognitively plausible and ben-
eficial for image and natural language processing
tasks. Our experiments evaluate a number of mod-
els previously proposed in the literature and in
Attribute Categories Example Attributes
color patterns (25) is red, has stripes
diet eats nuts, eats grass
shape size (16) is small, is chubby
parts (125) has legs, has wheels
botany;anatomy (25;78) has seeds, has fur
behavior (in)animate (55) flies, waddles, pecks
texture material made of metal, is shiny
structure (3) 2 pieces, has pleats
</bodyText>
<tableCaption confidence="0.788485333333333">
Table 1: Attribute categories and examples of at-
tribute instances. Parentheses denote the number
of attributes per category.
</tableCaption>
<bodyText confidence="0.805620833333333">
all cases show that the attribute-based represen-
tation brings performance improvements over just
using the textual modality. Moreover, we show
that automatically computed attributes are compa-
rable and in some cases superior to those provided
by humans (e.g., in norming studies).
</bodyText>
<sectionHeader confidence="0.986502" genericHeader="method">
3 The Attribute Dataset
</sectionHeader>
<bodyText confidence="0.999833111111111">
Concepts and Images We created a dataset of
images and their visual attributes for the nouns
contained in McRae et al.’s (2005) feature norms.
The norms cover a wide range of concrete con-
cepts including animate and inanimate things
(e.g., animals, clothing, vehicles, utensils, fruits,
and vegetables) and were collected by presenting
participants with words and asking them to list
properties of the objects to which the words re-
ferred. To avoid confusion, in the remainder of
this paper we will use the term attribute to refer to
properties of concepts and the term feature to refer
to image features, such as color or edges.
Images for the concepts in McRae et al.’s (2005)
production norms were harvested from ImageNet
(Deng et al., 2009), an ontology of images based
on the nominal hierarchy of WordNet (Fellbaum,
1998). ImageNet has more than 14 million im-
ages spanning 21K WordNet synsets. We chose
this database due to its high coverage and the high
quality of its images (i.e., cleanly labeled and high
resolution). McRae et al.’s norms contain 541 con-
cepts out of which 516 appear in ImageNet1 and
are represented by 688K images overall. The av-
erage number of images per concept is 1,310 with
the most popular being closet (2,149 images) and
the least popular prune (5 images).
</bodyText>
<footnote confidence="0.9989315">
1Some words had to be modified in order to match the cor-
rect synset, e.g., tank (container) was found as storage tank.
</footnote>
<page confidence="0.996572">
574
</page>
<bodyText confidence="0.9878134">
behavior eats, walks, climbs, swims, runs
diet drinks water, eats anything
shape size is tall, is large
anatomy has mouth, has head, has nose, has tail, has claws,
has jaws, has neck, has snout, has feet, has tongue
color patterns is black, is brown, is white
botany has skin, has seeds, has stem, has leaves, has pulp
color patterns purple, white, green, has green top
shape size is oval, is long
texture material is shiny
behavior rolls
parts has step through frame, has fork, has 2 wheels, has chain, has pedals
has gears, has handlebar, has bell, has breaks has seat, has spokes
texture material made of metal
color patterns different colors, is black, is red, is grey, is silver
</bodyText>
<tableCaption confidence="0.976358">
Table 2: Human-authored attributes for bear, eggplant, and bike.
</tableCaption>
<bodyText confidence="0.999994029411765">
The images depicting each concept were ran-
domly partitioned into a training, development,
and test set. For most concepts the development
set contained a maximum of 100 images and the
test set a maximum of 200 images. Concepts with
less than 800 images in total were split into 1/8
test and development set each, and 3/4 training set.
The development set was used for devising and re-
fining our attribute annotation scheme. The train-
ing and test sets were used for learning and eval-
uating, respectively, attribute classifiers (see Sec-
tion 4).
Attribute Annotation Our aim was to develop a
set of visual attributes that are both discriminating
and cognitively plausible, i.e., humans would gen-
erally use them to describe a concrete concept. As
a starting point, we thus used the visual attributes
from McRae et al.’s (2005) norming study. At-
tributes capturing other primary sensory informa-
tion (e.g., smell, sound), functional/motor proper-
ties, or encyclopaedic information were not taken
into account. For example, is purple is a valid vi-
sual attribute for an eggplant, whereas a vegetable
is not, since it cannot be visualized. Collating all
the visual attributes in the norms resulted in a to-
tal of 673 which we further modified and extended
during the annotation process explained below.
The annotation was conducted on aper-concept
rather than a per-image basis (as for example in
Farhadi et al. (2009)). For each concept (e.g., bear
or eggplant), we inspected the images in the devel-
opment set and chose all McRae et al. (2005) vi-
sual attributes that applied. If an attribute was gen-
erally true for the concept, but the images did not
provide enough evidence, the attribute was never-
theless chosen and labeled with &lt;no evidence&gt;.
For example, a plum has a pit, but most images in
ImageNet show plums where only the outer part
of the fruit is visible. Attributes supported by
the image data but missing from the norms were
added. For example, has lights and has bumper
are attributes of cars but are not included in the
norms. Attributes were grouped in eight general
classes shown in Table 1. Annotation proceeded
on a category-by-category basis, e.g., first all food-
related concepts were annotated, then animals, ve-
hicles, and so on. Two annotators (both co-authors
of this paper) developed the set of attributes for
each category. One annotator first labeled con-
cepts with their attributes, and the other annota-
tor reviewed the annotations, making changes if
needed. Annotations were revised and compared
per category in order to ensure consistency across
all concepts of that category.
Our methodology is slightly different from
Lampert et al. (2009) in that we did not simply
transfer the attributes from the norms to the con-
cepts in question but refined and extended them
according to the visual data. There are several
reasons for this. Firstly, it makes sense to se-
lect attributes corroborated by the images. Sec-
ondly, by looking at the actual images, we could
eliminate errors in McRae et al.’s (2005) norms.
For example, eight study participants erroneously
thought that a catfish has scales. Thirdly, dur-
ing the annotation process, we normalized syn-
onymous attributes (e.g., has pit and has stone)
and attributes that exhibited negligible variations
</bodyText>
<page confidence="0.990165">
575
</page>
<bodyText confidence="0.999927">
has 2 pieces, has pointed end, has strap, has thumb, has buckles, has heels
has shoe laces, has soles, is black, is brown, is white, made of leather, made of rubber
climbs, climbs trees, crawls, hops, jumps, eats, eats nuts, is small, has bushy tail
has 4 legs, has head, has neck, has nose, has snout, has tail, has claws
has eyes, has feet, has toes,
diff colours, has 2 legs, has 2 wheels, has windshield, has floorboard, has stand, has tank
has mudguard, has seat, has exhaust pipe, has frame, has handlebar, has lights, has mirror
has step-through frame, is black, is blue, is red, is white, made of aluminum, made of steel
</bodyText>
<tableCaption confidence="0.989745">
Table 3: Attribute predictions for sandals, squirrel, and motorcycle.
</tableCaption>
<bodyText confidence="0.9999835">
in meaning (e.g., has stem and has stalk). Finally,
our aim was to collect an exhaustive list of vi-
sual attributes for each concept which is consis-
tent across all members of a category. This is un-
fortunately not the case in McRae et al.’s norms.
Participants were asked to list up to 14 different
properties that describe a concept. As a result, the
attributes of a concept denote the set of properties
humans consider most salient. For example, both,
lemons and oranges have pulp. But the norms pro-
vide this attribute only for the second concept.
On average, each concept was annotated with
19 attributes; approximately 14.5 of these were
not part of the semantic representation created by
McRae et al.’s (2005) participants for that con-
cept even though they figured in the representa-
tions of other concepts. Furthermore, on average
two McRae et al. attributes per concept were dis-
carded. Examples of concepts and their attributes
from our database2 are shown in Table 2.
</bodyText>
<sectionHeader confidence="0.998311" genericHeader="method">
4 Attribute-based Classification
</sectionHeader>
<bodyText confidence="0.999989">
Following previous work (Farhadi et al., 2009;
Lampert et al., 2009) we learned one classifier per
attribute (i.e., 350 classifiers in total).3 The train-
ing set consisted of 91,980 images (with a maxi-
mum of 350 images per concept). We used an L2-
regularized L2-loss linear SVM (Fan et al., 2008)
to learn the attribute predictions. We adopted the
training procedure of Farhadi al. (2009).4 To learn
a classifier for a particular attribute, we used all
images in the training data. Images of concepts
annotated with the attribute were used as positive
examples, and the rest as negative examples. The
</bodyText>
<footnote confidence="0.9990374">
2Available from http://homepages.inf.ed.ac.uk/
mlap/index.php?page=resources.
3We only trained classifiers for attributes corroborated by
the images and excluded those labeled with &lt;no evidence&gt;.
4http://vision.cs.uiuc.edu/attributes/
</footnote>
<bodyText confidence="0.999826371428571">
data was randomly split into a training and valida-
tion set of equal size in order to find the optimal
cost parameter C. The final SVM for the attribute
was trained on the entire training data, i.e., on all
positive and negative examples.
The SVM learners used the four different fea-
ture types proposed in Farhadi et al. (2009),
namely color, texture, visual words, and edges.
Texture descriptors were computed for each pixel
and quantized to the nearest 256 k-means centers.
Visual words were constructed with a HOG spa-
tial pyramid. HOG descriptors were quantized
into 1000 k-means centers. Edges were detected
using a standard Canny detector and their orien-
tations were quantized into eight bins. Color de-
scriptors were sampled for each pixel and quan-
tized to the nearest 128 k-means centers. Shapes
and locations were represented by generating his-
tograms for each feature type for each cell in a grid
of three vertical and horizontal blocks. Our clas-
sifiers used 9,688 features in total. Table 3 shows
their predictions for three test images.
Note that attributes are predicted on an image-
by-image basis; our task, however, is to describe a
concept w by its visual attributes. Since concepts
are represented by many images we must some-
how aggregate their attributes into a single repre-
sentation. For each image iw E Iw of concept w,
we output an F-dimensional vector containing pre-
diction scores scorea(iw) for attributes a = 1,...,F.
We transform these attribute vectors into a single
vector pw E [0,1]1×F, by computing the centroid
of all vectors for concept w. The vector is nor-
malized to obtain a probability distribution over
attributes given w:
</bodyText>
<equation confidence="0.973788666666667">
(∑iwEIw scorea(iw))a=1,...,F
Pw = F (1)
∑a=1 ∑iwEIw scorea(iw)
</equation>
<bodyText confidence="0.991862">
We additionally impose a threshold S on pw by set-
</bodyText>
<page confidence="0.987868">
576
</page>
<figure confidence="0.998386333333333">
1
0.9
0.8
Precision 0.7
0.6
0.5
0.4
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
</figure>
<figureCaption confidence="0.992493">
Figure 1: Attribute classifier performance for dif-
ferent thresholds δ (test set).
</figureCaption>
<bodyText confidence="0.9421606">
ting each entry less than δ to zero.
Figure 1 shows the results of the attribute pre-
diction on the test set on the basis of the computed
centroids; specifically, we plot recall against pre-
cision based on threshold δ.5 Table 4 shows the
10 nearest neighbors for five example concepts
from our dataset. Again, we measure the cosine
similarity between a concept and all other con-
cepts in the dataset when these are represented by
their visual attribute vector pw.
</bodyText>
<sectionHeader confidence="0.983905" genericHeader="method">
5 Attribute-based Semantic Models
</sectionHeader>
<bodyText confidence="0.999986857142857">
We evaluated the effectiveness of our attribute
classifiers by integrating their predictions with tra-
ditional text-only models of semantic representa-
tion. These models have been previously proposed
in the literature and were also described in a recent
comparative study (Silberer and Lapata, 2012).
We represent the visual modality by attribute
vectors computed as shown in Equation (1). The
linguistic environment is approximated by textual
attributes. We used Strudel (Baroni et al., 2010)
to obtain these attributes for the nouns in our
dataset. Given a list of target words, Strudel ex-
tracts weighted word-attribute pairs from a lem-
matized and pos-tagged text corpus (e.g., egg-
plant–cook-v, eggplant–vegetable-n). The weight
of each word-attribute pair is a log-likelihood ratio
score expressing the pair’s strength of association.
In our experiments we learned word-attribute pairs
from a lemmatized and pos-tagged (2009) dump
of the English Wikipedia.6 In the remainder of
this section we will briefly describe the models we
</bodyText>
<footnote confidence="0.979555333333333">
5Threshold values ranged from 0 to 0.9 with 0.1 stepsize.
6The corpus can be downloaded from http://wacky.
sslmit.unibo.it/doku.php?id=corpora.
</footnote>
<table confidence="0.998955727272727">
Concept Nearest Neighbors
boat ship, sailboat, yacht, submarine, canoe,
whale, airplane, jet, helicopter, tank (army)
rooster chicken, turkey, owl, pheasant, peacock, stork,
pigeon, woodpecker, dove, raven
shirt blouse, robe, cape, vest, dress, coat, jacket,
skirt, camisole, nightgown
spinach lettuce, parsley, peas, celery, broccoli, cab-
bage, cucumber, rhubarb, zucchini, asparagus
squirrel chipmunk, raccoon, groundhog, gopher, por-
cupine, hare, rabbit, fox, mole, emu
</table>
<tableCaption confidence="0.878107666666667">
Table 4: Ten most similar concepts computed on
the basis of averaged attribute vectors and ordered
according to cosine similarity.
</tableCaption>
<bodyText confidence="0.998777583333333">
used in our study and how the textual and visual
modalities were fused to create a joint representa-
tion.
Concatenation Model Variants of this model
were originally proposed in Bruni et al. (2011)
and Johns and Jones (2012). Let T ∈ RN×D de-
note a term-attribute co-occurrence matrix, where
each cell records a weighted co-occurrence score
of a word and a textual attribute. Let P ∈ [0,1]N×F
denote a visual matrix, representing a probability
distribution over visual attributes for each word.
A word’s meaning can be then represented by the
concatenation of its normalized textual and visual
vectors.
Canonical Correlation Analysis The second
model uses Canonical Correlation Analysis (CCA,
Hardoon et al. (2004)) to learn a joint semantic
representation from the textual and visual modali-
ties. Given two random variables x and y (or two
sets of vectors), CCA can be seen as determining
two sets of basis vectors in such a way, that the cor-
relation between the projections of the variables
onto these bases is mutually maximized (Borga,
2001). In effect, the representation-specific de-
tails pertaining to the two views of the same phe-
nomenon are discarded and the underlying hidden
factors responsible for the correlation are revealed.
The linguistic and visual views are the same as
in the simple concatenation model just explained.
We use a kernelized version of CCA (Hardoon et
al., 2004) that first projects the data into a higher-
dimensional feature space and then performs CCA
in this new feature space. The two kernel matrices
are KT = TT0 and KP = PP0. After applying CCA
we obtain two matrices projected onto l basis vec-
tors, T˜ ∈ RN×l, resulting from the projection of the
</bodyText>
<page confidence="0.99297">
577
</page>
<bodyText confidence="0.99887837037037">
textual matrix T onto the new basis and P˜ E RNxl,
resulting from the projection of the corresponding
visual attribute matrix. The meaning of a word is
then represented by T˜ or ˜P.
Attribute-topic Model Andrews et al. (2009)
present an extension of LDA (Blei et al., 2003)
where words in documents and their associated
attributes are treated as observed variables that
are explained by a generative process. The
idea is that each document in a document col-
lection D is generated by a mixture of com-
ponents {x1,...,xc,...,xCJ E C, where a compo-
nent xc comprises a latent discourse topic coupled
with an attribute cluster. Inducing these attribute-
topic components from D with the extended LDA
model gives two sets of parameters: word prob-
abilities given components PW (wi|X = xc) for wi,
i = 1,...,n, and attribute probabilities given com-
ponents PA(ak|X = xc) for ak, k = 1,...,F. For ex-
ample, most of the probability mass of a compo-
nent x would be reserved for the words shirt, coat,
dress and the attributes has 1 piece, has seams,
made of material and so on.
Word meaning in this model is represented by
the distribution PX|W over the learned compo-
nents. Assuming a uniform distribution over com-
ponents xc in D, PX|W can be approximated as:
</bodyText>
<equation confidence="0.9984055">
PX=xc|W=wi = P(wi|xc)P(xc) r
P(wi)
</equation>
<bodyText confidence="0.999233363636364">
where C is the total number of components.
In our work, the training data is a corpus D of
textual attributes (rather than documents). Each
attribute is represented as a bag-of-concepts,
i.e., words demonstrating the property expressed
by the attribute (e.g., vegetable-n is a property of
eggplant, spinach, carrot). For some of these con-
cepts, our classifiers predict visual attributes. In
this case, the concepts are paired with one of their
visual attributes. We sample attributes for a con-
cept w from their distribution given w (Eq. (1)).
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999823666666667">
Evaluation Task We evaluated the distribu-
tional models presented in Section 5 on the
word association norms collected by Nelson et al.
(1998).7 These were established by presenting
a large number of participants with a cue word
(e.g., rice) and asking them to name an associate
</bodyText>
<footnote confidence="0.964778">
7From http://w3.usf.edu/FreeAssociation/.
</footnote>
<bodyText confidence="0.997654807692308">
word in response (e.g., Chinese, wedding, food,
white). For each cue, the norms provide a set
of associates and the frequencies with which they
were named. We can thus compute the prob-
ability distribution over associates for each cue.
Analogously, we can estimate the degree of sim-
ilarity between a cue and its associates using our
models. The norms contain 63,619 unique cue-
associate pairs. Of these, 435 pairs were covered
by McRae et al. (2005) and our models. We also
experimented with 1,716 pairs that were not part
of McRae et al.’s study but belonged to concepts
covered by our attribute taxonomy (e.g., animals,
vehicles), and were present in our corpus and Ima-
geNet. Using correlation analysis (Spearman’s p),
we examined the degree of linear relationship be-
tween the human cue-associate probabilities and
the automatically derived similarity values.8
Parameter Settings In order to integrate the vi-
sual attributes with the models described in Sec-
tion 5 we must select the appropriate threshold
value S (see Eq. (1)). We optimized this value
on the development set and obtained best results
with S = 0. We also experimented with thresh-
olding the attribute prediction scores and with ex-
cluding attributes with low precision. In both
cases, we obtained best results when using all at-
tributes. We could apply CCA to the vectors rep-
resenting each image separately and then compute
a weighted centroid on the projected vectors. We
refrained from doing this as it involves additional
parameters and assumes input different from the
other models. We measured the similarity between
two words using the cosine of the angle. For the
attribute-topic model, the number of predefined
components C was set to 10. In this model, sim-
ilarity was measured as defined by Griffiths et al.
(2007). The underlying idea is that word associa-
tion can be expressed as a conditional distribution.
With regard to the textual attributes, we
obtained a 9,394-dimensional semantic space
after discarding word-attribute pairs with a
log-likelihood ratio score less than 19.9 We also
discarded attributes co-occurring with less than
two different words.
8Previous work (Griffiths et al., 2007) which also predicts
word association reports how many times the word with the
highest score under the model was the first associate in the
human norms. This evaluation metric assumes that there are
many associates for a given cue which unfortunately is not
the case in our study which is restricted to the concepts rep-
resented in our attribute taxonomy.
</bodyText>
<equation confidence="0.798442714285714">
9Baroni et al. (2010) use a similar threshold of 19.51.
(2)
C
∑
l=1
P(wi|xc)
P(wi|xl)
</equation>
<page confidence="0.994824">
578
</page>
<table confidence="0.999704166666667">
Nelson Concat CCA TopicAttr TextAttr
Concat 0.24
CCA 0.30 0.72
TopicAttr 0.26 0.55 0.28
TextAttr 0.21 0.80 0.83 0.34
VisAttr 0.23 0.65 0.52 0.40 0.39
</table>
<tableCaption confidence="0.8124165">
Table 5: Correlation matrix for seen Nelson et al.
(1998) cue-associate pairs and five distributional
models. All correlation coefficients are statisti-
cally significant (p &lt; 0.01, N = 435).
</tableCaption>
<table confidence="0.999765333333333">
Nelson Concat CCA TopicAttr TextAttr
Concat 0.11
CCA 0.15 0.66
TopicAttr 0.17 0.69 0.48
TextAttr 0.11 0.65 0.25 0.39
VisAttr 0.13 0.57 0.87 0.57 0.34
</table>
<tableCaption confidence="0.841779">
Table 6: Correlation matrix for unseen Nelson
et al. (1998) cue-associate pairs and five distribu-
tional models. All correlation coefficients are sta-
tistically significant (p &lt; 0.01, N = 1,716).
</tableCaption>
<sectionHeader confidence="0.999482" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999976215189874">
Our experiments were designed to answer four
questions: (1) Do visual attributes improve the
performance of distributional models? (2) Are
there performance differences among different
models, i.e., are some models better suited to the
integration of visual information? (3) How do
computational models fare against gold standard
norming data? (4) Does the attribute-based repre-
sentation bring advantages over more conventional
approaches based on raw image features?
Our results are broken down into seen (Table 5)
and unseen (Table 6) concepts. The former are
known to the attribute classifiers and form part
of our database, whereas the latter are unknown
and are not included in McRae et al.’s (2005)
norms. We report the correlation coefficients we
obtain when human-derived cue-associate proba-
bilities (Nelson et al., 1998) are compared against
the simple concatenation model (Concat), CCA,
and Andrews et al.’s (2009) attribute-topic model
(TopicAttr). We also report the performance of
a distributional model that is based solely on the
output of our attribute classifiers, i.e., without any
textual input (VisAttr) and conversely the perfor-
mance of a model that uses textual information
only (i.e., Strudel attributes) without any visual in-
put (TextAttr). The results are displayed as a cor-
relation matrix so that inter-model correlations can
also be observed.
As can be seen in Table 5 (second column), two
modalities are in most cases better than one when
evaluating model performance on seen data. Dif-
ferences in correlation coefficients between mod-
els with two versus one modality are all statis-
tically significant (p &lt; 0.01 using a t-test), with
the exception of Concat when compared against
VisAttr. It is also interesting to note that Topi-
cAttr is the least correlated model when compared
against other bimodal models or single modali-
ties. This indicates that the latent space obtained
by this model is most distinct from its constituent
parts (i.e., visual and textual attributes). Perhaps
unsuprisingly Concat, CCA, VisAttr, and TextAttr
are also highly intercorrelated.
On unseen pairs (see Table 6), Concat fares
worse than CCA and TopicAttr, achieving simi-
lar performance to TextAttr. CCA and TopicAttr
are significantly better than TextAttr and VisAttr
(p &lt; 0.01). This indicates that our attribute classi-
fiers generalize well beyond the concepts found in
our database and can produce useful visual infor-
mation even on unseen images. Compared to Con-
cat and CCA, TopicAttr obtains a better fit with the
human association norms on the unseen data.
To answer our third question, we obtained dis-
tributional models from McRae et al.’s (2005)
norms and assessed how well they predict Nelson
et al.’s (1998) word-associate similarities. Each
concept was represented as a vector with dimen-
sions corresponding to attributes generated by par-
ticipants of the norming study. Vector components
were set to the (normalized) frequency with which
participants generated the corresponding attribute
when presented with the concept. We measured
the similarity between two words using the co-
sine coefficient. Table 7 presents results for dif-
ferent model variants which we created by ma-
nipulating the number and type of attributes in-
volved. The first model uses the full set of at-
tributes present in the norms (All Attributes). The
second model (Text Attributes) uses all attributes
but those classified as visual (e.g., functional, en-
cyclopaedic). The third model (Visual Attributes)
considers solely visual attributes.
We observe a similar trend as with our compu-
tational models. Taking visual attributes into ac-
count increases the fit with Nelson’s (1998) associ-
ation norms, whereas visual and textual attributes
on their own perform worse. Interestingly, CCA’s
</bodyText>
<page confidence="0.995205">
579
</page>
<table confidence="0.9993295">
Models Seen
All Attributes 0.28
Text Attributes 0.20
Visual Attributes 0.25
</table>
<tableCaption confidence="0.9489194">
Table 7: Model performance on seen Nelson et
al. (1998) cue-associate pairs; models are based
on gold human generated attributes (McRae et al.,
2005). All correlation coefficients are statistically
significant (p &lt; 0.01, N = 435).
</tableCaption>
<table confidence="0.999817714285714">
Models Seen Unseen
Concat 0.22 0.10
CCA 0.26 0.15
TopicAttr 0.23 0.19
TextAttr 0.20 0.08
VisAttr 0.21 0.13
MixLDA 0.16 0.11
</table>
<tableCaption confidence="0.995507">
Table 8: Model performance on a subset of Nelson
</tableCaption>
<bodyText confidence="0.996933159090909">
et al. (1998) cue-associate pairs. Seen are concepts
known to the attribute classifiers and covered by
MixLDA (N = 85). Unseen are concepts covered
by LDA but unknown to the attribute classifiers
(N = 388). All correlation coefficients are statisti-
cally significant (p &lt; 0.05).
performance is comparable to the All Attributes
model (see Table 5, second column), despite us-
ing automatic attributes (both textual and visual).
Furthermore, visual attributes obtained through
our classifiers (see Table 5) achieve a marginally
lower correlation coefficient against human gener-
ated ones (see Table 7).
Finally, to address our last question, we com-
pared our approach against Feng and Lapata
(2010) who represent visual information via quan-
tized SIFT features. We trained their MixLDA
model on their corpus consisting of 3,361 BBC
news documents and corresponding images (Feng
and Lapata, 2008). We optimized the model pa-
rameters on a development set consisting of cue-
associate pairs from Nelson et al. (1998), exclud-
ing the concepts in McRae et al. (2005). We
used a vocabulary of approximately 6,000 words.
The best performing model on the development set
used 500 visual terms and 750 topics and the asso-
ciation measure proposed in Griffiths et al. (2007).
The test set consisted of 85 seen and 388 unseen
cue-associate pairs that were covered by our mod-
els and MixLDA.
Table 8 reports correlation coefficients for our
models and MixLDA against human probabili-
ties. All attribute-based models significantly out-
perform MixLDA on seen pairs (p &lt; 0.05 using a
t-test). MixLDA performs on a par with the con-
catenation model on unseen pairs, however CCA,
TopicAttr, and VisAttr are all superior. Although
these comparisons should be taken with a grain
of salt, given that MixLDA and our models are
trained on different corpora (MixLDA assumes
that texts and images are collocated, whereas our
images do not have collateral text), they seem to
indicate that attribute-based information is indeed
beneficial.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999723357142857">
In this paper we proposed the use of automatically
computed visual attributes as a way of physically
grounding word meaning. Our results demonstrate
that visual attributes improve the performance of
distributional models across the board. On a
word association task, CCA and the attribute-topic
model give a better fit to human data when com-
pared against simple concatenation and models
based on a single modality. CCA consistently out-
performs the attribute-topic model on seen data (it
is in fact slightly better over a model that uses gold
standard human generated attributes), whereas the
attribute-topic model generalizes better on unseen
data (see Tables 5, 6, and 8). Since the attribute-
based representation is general and text-based we
argue that it can be conveniently integrated with
any type of distributional model or indeed other
grounded models that rely on low-level image fea-
tures (Bruni et al., 2012a; Feng and Lapata, 2010)
In the future, we would like to extend our
database to actions and show that this attribute-
centric representation is useful for more applied
tasks such as image description generation and ob-
ject recognition. Finally, we have only scratched
the surface in terms of possible models for inte-
grating the textual and visual modality. Interest-
ing frameworks which we plan to explore are deep
belief networks and Bayesian non-parametrics.
</bodyText>
<sectionHeader confidence="0.999007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999602666666667">
M. Andrews, G. Vigliocco, and D. Vinson. 2009.
Integrating Experiential and Distributional Data to
Learn Semantic Representations. Psychological Re-
view, 116(3):463–498.
M. Baroni, B. Murphy, E. Barbu, and M. Poesio.
2010. Strudel: A Corpus-Based Semantic Model
</reference>
<page confidence="0.989324">
580
</page>
<reference confidence="0.992076780701754">
Based on Properties and Types. Cognitive Science,
34(2):222–254.
L. W. Barsalou. 2008. Grounded Cognition. Annual
Review of Psychology, 59:617–845.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet Allocation. Journal of Machine Learning
Research, 3:993–1022, March.
M. Borga. 2001. Canonical Correlation – a Tutorial,
January.
M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y.
Park, L. Pascual, M. G. Pˆecheux, J. Ruel, P. Venuti,
and A. Vyt. 2004. Cross-linguistic Analysis of
Vocabulary in Young Children: Spanish, Dutch,
French, Hebrew, Italian, Korean, and American En-
glish. Child Development, 75(4):1115–1139.
B. B¨orschinger, B. K. Jones, and M. Johnson. 2011.
Reducing Grounded Learning Tasks to Grammatical
Inference. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1416–1425, Edinburgh, UK.
S.R.K. Branavan, H. Chen, L. S. Zettlemoyer, and
R. Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 82–90, Suntec, Singapore.
E. Bruni, G. Tran, and M. Baroni. 2011. Distributional
Semantics from Text and Images. In Proceedings of
the GEMS 2011 Workshop on GEometrical Models
of Natural Language Semantics, pages 22–32, Edin-
burgh, UK.
E. Bruni, G. Boleda, M. Baroni, and N. Tran. 2012a.
Distributional Semantics in Technicolor. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 136–145, Jeju Island, Korea.
E. Bruni, J. Uijlings, M. Baroni, and N. Sebe. 2012b.
Distributional semantics with eyes: Using im-
age analysis to improve computational representa-
tions of word meaning. In Proceedings of the
20th ACM International Conference on Multimedia,
pages 1219–1228., New York, NY.
C. Chai and C. Hung. 2008. Automatically Annotating
Images with Keywords: A Review of Image Annota-
tion Systems. Recent Patents on Computer Science,
1:55–68.
R. Datta, D. Joshi, J. Li, and J. Z. Wang. 2008. Image
Retrieval: Ideas, Influences, and Trends of the New
Age. ACM Computing Surveys, 40(2):1–60.
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchi-
cal Image Database. In Proceedings of the IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, pages 248–255, Miami,
Florida.
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2008. The
PASCAL Visual Object Classes Challenge
2008 (VOC2008) Results. http://www.pascal-
network.org/challenges/VOC/voc2008/workshop.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.
2008. LIBLINEAR: A Library for Large Linear
Classification. Journal of Machine Learning Re-
search, 9:1871–1874.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing Objects by their Attributes. In Proceed-
ings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pages
1778–1785, Miami Beach, Florida.
L. Fei-Fei and P. Perona. 2005. A Bayesian Hierarchi-
cal Model for Learning Natural Scene Categories. In
Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition,
pages 524–531, San Diego, California.
C. Fellbaum, editor. 1998. WordNet: an Electronic
Lexical Database. MIT Press.
Y. Feng and M. Lapata. 2008. Automatic image anno-
tation using auxiliary text information. In Proceed-
ings of ACL-08: HLT, pages 272–280, Columbus,
Ohio.
Y. Feng and M. Lapata. 2010. Visual Informa-
tion in Semantic Representation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 91–99, Los
Angeles, California. ACL.
V. Ferrari and A. Zisserman. 2007. Learning Visual
Attributes. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20, pages 433–440. MIT Press,
Cambridge, Massachusetts.
G. H. Golub, F. T. Luk, and M. L. Overton. 1981.
A block lanczoz method for computing the singular
values and corresponding singular vectors of a ma-
trix. ACM Transactions on Mathematical Software,
7:149–169.
P. Gorniak and D. Roy. 2004. Grounded Semantic
Composition for Visual Scenes. Journal ofArtificial
Intelligence Research, 21:429–470.
T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum.
2007. Topics in Semantic Representation. Psycho-
logical Review, 114(2):211–244.
D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-
Taylor. 2004. Canonical Correlation Analysis: An
Overview with Application to Learning Methods.
Neural Computation, 16(12):2639–2664.
B. T. Johns and M. N. Jones. 2012. Perceptual Infer-
ence through Global Lexical Similarity. Topics in
Cognitive Science, 4(1):103–120.
D. Joshi, J.Z. Wang, and J. Li. 2006. The Story Pictur-
ing Engine—A System for Automatic Text illustra-
tion. ACM Transactions on Multimedia Computing,
Communications, and Applications, 2(1):68–89.
</reference>
<page confidence="0.978676">
581
</page>
<reference confidence="0.998467617391305">
R. J. Kate and R. J. Mooney. 2007. Learning Lan-
guage Semantics from Ambiguous Supervision. In
Proceedings of the 22nd Conference on Artificial In-
telligence, pages 895–900, Vancouver, Canada.
N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Na-
yar. 2009. Attribute and Simile Classifiers for Face
Verification. In Proceedings of the IEEE 12th In-
ternational Conference on Computer Vision, pages
365–372, Kyoto, Japan.
C. H. Lampert, H. Nickisch, and S. Harmeling. 2009.
Learning To Detect Unseen Object Classes by
Between-Class Attribute Transfer. In Computer Vi-
sion and Pattern Recognition, pages 951–958, Mi-
ami Beach, Florida.
B. Landau, L. Smith, and S. Jones. 1998. Object Per-
ception and Object Naming in Early Development.
Trends in Cognitive Science, 27:19–24.
C. Leong and R. Mihalcea. 2011. Going Beyond
Text: A Hybrid Image-Text Approach for Measuring
Word Relatedness. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1403–1407, Chiang Mai, Thailand.
J. Liu, B. Kuipers, and S. Savarese. 2011. Recognizing
Human Actions by Attributes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition, pages 3337–3344, Colorado Springs,
Colorado.
D. G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the In-
ternational Conference on Computer Vision, pages
1150–1157, Corfu, Greece.
D. Lowe. 2004. Distinctive Image Features from
Scale-invariant Keypoints. International Journal of
Computer Vision, 60(2):91–110.
W. Lu, H. T. Ng, W.S. Lee, and L. S. Zettlemoyer.
2008. A Generative Model for Parsing Natural Lan-
guage to Meaning Representations. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 783–792, Hon-
olulu, Hawaii.
K. McRae, G. S. Cree, M. S. Seidenberg, and C. Mc-
Norgan. 2005. Semantic Feature Production Norms
for a Large Set of Living and Nonliving Things. Be-
havior Research Methods, 37(4):547–559.
D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998.
The University of South Florida Word Association,
Rhyme, and Word Fragment Norms.
J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and
A. Y. Ng. 2011. Multimodal deep learning. In
Proceedings of the 28th International Conference on
Machine Leanring, pages 689–696, Bellevue, Wash-
ington.
A. Oliva and A. Torralba. 2007. The Role of Context in
Object Recognition. Trends in Cognitive Sciences,
11(12):520–527.
D. N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E.
Smith. 1991. Default Probability. Cognitive Sci-
ence, 2(15):251–269.
G. Patterson and J. Hays. 2012. SUN Attribute
Database: Discovering, Annotating and Recogniz-
ing Scene Attributes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 2751–2758, Providence, Rhode Island.
T. Regier. 1996. The Human Semantic Potential. MIT
Press, Cambridge, Massachusetts.
D. Roy and A. Pentland. 2002. Learning Words from
Sights and Sounds: A Computational Model. Cog-
nitive Science, 26(1):113–146.
C. Silberer and M. Lapata. 2012. Grounded Mod-
els of Semantic Representation. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433, Jeju
Island, Korea.
J. M. Siskind. 2001. Grounding the Lexical Semantics
of Verbs in Visual Perception using Force Dynamics
and Event Logic. Journal of Artificial Intelligence
Research, 15:31–90.
S. A. Sloman and L. J. Ripps. 1998. Similarity as an
Explanatory Construct. Cognition, 65:87–101.
N. Srivastava and R. Salakhutdinov. 2012. Multimodal
learning with deep boltzmann machines. In Pro-
ceedings of the 26th Annual Conference on Neural
Information Processing Systems, pages 2231–2239,
Lake Tahoe, Nevada.
M. Steyvers. 2010. Combining feature norms and
text data with topic models. Acta Psychologica,
133(3):234–342.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter,
A. Gopal Banerjee, S. Teller, and N. Roy. 2011.
Understanding Natural Language Commands for
Robotic Navigation and Manipulation. In Proceed-
ings of the 25th National Conference on Artificial
Intelligence, pages 1507–1514, San Francisco, Cali-
fornia.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceeings of the Human
Factors in Computing Systems Conference, pages
319–326, Vienna, Austria.
C. Yu and D. H. Ballard. 2007. A Unified Model of
Early Word Learning Integrating Statistical and So-
cial Cues. Neurocomputing, 70:2149–2165.
M. D. Zeigenfuse and M. D. Lee. 2010. Finding the
Features that Represent Stimuli. Acta Psychologi-
cal, 133(3):283–295.
J. M. Zelle and R. J. Mooney. 1996. Learning to Parse
Database Queries Using Inductive Logic Program-
ming. In Proceedings of the 13th National Con-
ference on Artificial Intelligence, pages 1050–1055,
Portland, Oregon.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the Conference on Uncertainty in Ar-
tificial Intelligence, pages 658–666, Edinburgh, UK.
</reference>
<page confidence="0.997844">
582
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959300">
<title confidence="0.999905">Models of Semantic Representation with Visual Attributes</title>
<author confidence="0.999705">Carina Silberer</author>
<author confidence="0.999705">Vittorio Ferrari</author>
<author confidence="0.999705">Mirella</author>
<affiliation confidence="0.998208">School of Informatics, University of</affiliation>
<address confidence="0.986191">10 Crichton Street, Edinburgh EH8</address>
<email confidence="0.998141">c.silberer@ed.ac.uk,vferrari@inf.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.998441375">We consider the problem of grounding the meaning of words in the physical world and focus on the visual modality which we by We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Andrews</author>
<author>G Vigliocco</author>
<author>D Vinson</author>
</authors>
<title>Integrating Experiential and Distributional Data to Learn Semantic Representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="1705" citStr="Andrews et al., 2009" startWordPosition="260" endWordPosition="263">assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications includin</context>
<context position="26165" citStr="Andrews et al. (2009)" startWordPosition="4191" endWordPosition="4194"> simple concatenation model just explained. We use a kernelized version of CCA (Hardoon et al., 2004) that first projects the data into a higherdimensional feature space and then performs CCA in this new feature space. The two kernel matrices are KT = TT0 and KP = PP0. After applying CCA we obtain two matrices projected onto l basis vectors, T˜ ∈ RN×l, resulting from the projection of the 577 textual matrix T onto the new basis and P˜ E RNxl, resulting from the projection of the corresponding visual attribute matrix. The meaning of a word is then represented by T˜ or ˜P. Attribute-topic Model Andrews et al. (2009) present an extension of LDA (Blei et al., 2003) where words in documents and their associated attributes are treated as observed variables that are explained by a generative process. The idea is that each document in a document collection D is generated by a mixture of components {x1,...,xc,...,xCJ E C, where a component xc comprises a latent discourse topic coupled with an attribute cluster. Inducing these attributetopic components from D with the extended LDA model gives two sets of parameters: word probabilities given components PW (wi|X = xc) for wi, i = 1,...,n, and attribute probabiliti</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>M. Andrews, G. Vigliocco, and D. Vinson. 2009. Integrating Experiential and Distributional Data to Learn Semantic Representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>B Murphy</author>
<author>E Barbu</author>
<author>M Poesio</author>
</authors>
<title>Strudel: A Corpus-Based Semantic Model Based on Properties and Types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="22969" citStr="Baroni et al., 2010" startWordPosition="3689" endWordPosition="3692"> other concepts in the dataset when these are represented by their visual attribute vector pw. 5 Attribute-based Semantic Models We evaluated the effectiveness of our attribute classifiers by integrating their predictions with traditional text-only models of semantic representation. These models have been previously proposed in the literature and were also described in a recent comparative study (Silberer and Lapata, 2012). We represent the visual modality by attribute vectors computed as shown in Equation (1). The linguistic environment is approximated by textual attributes. We used Strudel (Baroni et al., 2010) to obtain these attributes for the nouns in our dataset. Given a list of target words, Strudel extracts weighted word-attribute pairs from a lemmatized and pos-tagged text corpus (e.g., eggplant–cook-v, eggplant–vegetable-n). The weight of each word-attribute pair is a log-likelihood ratio score expressing the pair’s strength of association. In our experiments we learned word-attribute pairs from a lemmatized and pos-tagged (2009) dump of the English Wikipedia.6 In the remainder of this section we will briefly describe the models we 5Threshold values ranged from 0 to 0.9 with 0.1 stepsize. 6T</context>
<context position="30648" citStr="Baroni et al. (2010)" startWordPosition="4929" endWordPosition="4932">ined a 9,394-dimensional semantic space after discarding word-attribute pairs with a log-likelihood ratio score less than 19.9 We also discarded attributes co-occurring with less than two different words. 8Previous work (Griffiths et al., 2007) which also predicts word association reports how many times the word with the highest score under the model was the first associate in the human norms. This evaluation metric assumes that there are many associates for a given cue which unfortunately is not the case in our study which is restricted to the concepts represented in our attribute taxonomy. 9Baroni et al. (2010) use a similar threshold of 19.51. (2) C ∑ l=1 P(wi|xc) P(wi|xl) 578 Nelson Concat CCA TopicAttr TextAttr Concat 0.24 CCA 0.30 0.72 TopicAttr 0.26 0.55 0.28 TextAttr 0.21 0.80 0.83 0.34 VisAttr 0.23 0.65 0.52 0.40 0.39 Table 5: Correlation matrix for seen Nelson et al. (1998) cue-associate pairs and five distributional models. All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Nelson Concat CCA TopicAttr TextAttr Concat 0.11 CCA 0.15 0.66 TopicAttr 0.17 0.69 0.48 TextAttr 0.11 0.65 0.25 0.39 VisAttr 0.13 0.57 0.87 0.57 0.34 Table 6: Correlation matrix for unseen Ne</context>
</contexts>
<marker>Baroni, Murphy, Barbu, Poesio, 2010</marker>
<rawString>M. Baroni, B. Murphy, E. Barbu, and M. Poesio. 2010. Strudel: A Corpus-Based Semantic Model Based on Properties and Types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Barsalou</author>
</authors>
<title>Grounded Cognition. Annual Review of Psychology,</title>
<date>2008</date>
<pages>59--617</pages>
<contexts>
<context position="2127" citStr="Barsalou, 2008" startWordPosition="326" endWordPosition="328">2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata,</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>L. W. Barsalou. 2008. Grounded Cognition. Annual Review of Psychology, 59:617–845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7573" citStr="Blei et al., 2003" startWordPosition="1180" endWordPosition="1183">describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Special-purpose models that address the fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifically, they extract a visual space from images contained in the ESP-Game data set (von Ahn and Dabbish, 2004) and a text-based semantic space from a large corpus collection totaling approximately two billion words. They concatenate the two modalities and </context>
<context position="26213" citStr="Blei et al., 2003" startWordPosition="4200" endWordPosition="4203">a kernelized version of CCA (Hardoon et al., 2004) that first projects the data into a higherdimensional feature space and then performs CCA in this new feature space. The two kernel matrices are KT = TT0 and KP = PP0. After applying CCA we obtain two matrices projected onto l basis vectors, T˜ ∈ RN×l, resulting from the projection of the 577 textual matrix T onto the new basis and P˜ E RNxl, resulting from the projection of the corresponding visual attribute matrix. The meaning of a word is then represented by T˜ or ˜P. Attribute-topic Model Andrews et al. (2009) present an extension of LDA (Blei et al., 2003) where words in documents and their associated attributes are treated as observed variables that are explained by a generative process. The idea is that each document in a document collection D is generated by a mixture of components {x1,...,xc,...,xCJ E C, where a component xc comprises a latent discourse topic coupled with an attribute cluster. Inducing these attributetopic components from D with the extended LDA model gives two sets of parameters: word probabilities given components PW (wi|X = xc) for wi, i = 1,...,n, and attribute probabilities given components PA(ak|X = xc) for ak, k = 1,</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Borga</author>
</authors>
<date>2001</date>
<booktitle>Canonical Correlation – a Tutorial,</booktitle>
<contexts>
<context position="25296" citStr="Borga, 2001" startWordPosition="4043" endWordPosition="4044">probability distribution over visual attributes for each word. A word’s meaning can be then represented by the concatenation of its normalized textual and visual vectors. Canonical Correlation Analysis The second model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and visual modalities. Given two random variables x and y (or two sets of vectors), CCA can be seen as determining two sets of basis vectors in such a way, that the correlation between the projections of the variables onto these bases is mutually maximized (Borga, 2001). In effect, the representation-specific details pertaining to the two views of the same phenomenon are discarded and the underlying hidden factors responsible for the correlation are revealed. The linguistic and visual views are the same as in the simple concatenation model just explained. We use a kernelized version of CCA (Hardoon et al., 2004) that first projects the data into a higherdimensional feature space and then performs CCA in this new feature space. The two kernel matrices are KT = TT0 and KP = PP0. After applying CCA we obtain two matrices projected onto l basis vectors, T˜ ∈ RN×</context>
</contexts>
<marker>Borga, 2001</marker>
<rawString>M. Borga. 2001. Canonical Correlation – a Tutorial, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Bornstein</author>
<author>L R Cote</author>
<author>S Maital</author>
<author>K Painter</author>
<author>S-Y Park</author>
<author>L Pascual</author>
<author>M G Pˆecheux</author>
<author>J Ruel</author>
<author>P Venuti</author>
<author>A Vyt</author>
</authors>
<title>Cross-linguistic Analysis of Vocabulary in</title>
<date>2004</date>
<journal>Child Development,</journal>
<volume>75</volume>
<issue>4</issue>
<location>Young Children: Spanish, Dutch, French, Hebrew, Italian, Korean, and</location>
<marker>Bornstein, Cote, Maital, Painter, Park, Pascual, Pˆecheux, Ruel, Venuti, Vyt, 2004</marker>
<rawString>M. H. Bornstein, L. R. Cote, S. Maital, K. Painter, S.-Y. Park, L. Pascual, M. G. Pˆecheux, J. Ruel, P. Venuti, and A. Vyt. 2004. Cross-linguistic Analysis of Vocabulary in Young Children: Spanish, Dutch, French, Hebrew, Italian, Korean, and American English. Child Development, 75(4):1115–1139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B¨orschinger</author>
<author>B K Jones</author>
<author>M Johnson</author>
</authors>
<title>Reducing Grounded Learning Tasks to Grammatical Inference.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1416--1425</pages>
<location>Edinburgh, UK.</location>
<marker>B¨orschinger, Jones, Johnson, 2011</marker>
<rawString>B. B¨orschinger, B. K. Jones, and M. Johnson. 2011. Reducing Grounded Learning Tasks to Grammatical Inference. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416–1425, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>H Chen</author>
<author>L S Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Reinforcement Learning for Mapping Instructions to Actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>82--90</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="1357" citStr="Branavan et al., 2009" startWordPosition="200" endWordPosition="203"> human word association data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspecti</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement Learning for Mapping Instructions to Actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82–90, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>G Tran</author>
<author>M Baroni</author>
</authors>
<title>Distributional Semantics from Text and Images.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>22--32</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="24450" citStr="Bruni et al. (2011)" startWordPosition="3903" endWordPosition="3906">on, woodpecker, dove, raven shirt blouse, robe, cape, vest, dress, coat, jacket, skirt, camisole, nightgown spinach lettuce, parsley, peas, celery, broccoli, cabbage, cucumber, rhubarb, zucchini, asparagus squirrel chipmunk, raccoon, groundhog, gopher, porcupine, hare, rabbit, fox, mole, emu Table 4: Ten most similar concepts computed on the basis of averaged attribute vectors and ordered according to cosine similarity. used in our study and how the textual and visual modalities were fused to create a joint representation. Concatenation Model Variants of this model were originally proposed in Bruni et al. (2011) and Johns and Jones (2012). Let T ∈ RN×D denote a term-attribute co-occurrence matrix, where each cell records a weighted co-occurrence score of a word and a textual attribute. Let P ∈ [0,1]N×F denote a visual matrix, representing a probability distribution over visual attributes for each word. A word’s meaning can be then represented by the concatenation of its normalized textual and visual vectors. Canonical Correlation Analysis The second model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and visual modalities. G</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>E. Bruni, G. Tran, and M. Baroni. 2011. Distributional Semantics from Text and Images. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 22–32, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>G Boleda</author>
<author>M Baroni</author>
<author>N Tran</author>
</authors>
<title>Distributional Semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>136--145</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1636" citStr="Bruni et al., 2012" startWordPosition="248" endWordPosition="251">age tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn represe</context>
<context position="4682" citStr="Bruni et al., 2012" startWordPosition="726" endWordPosition="729">strand of research focuses exclusively on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well. This is not entirely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothi</context>
<context position="7833" citStr="Bruni et al. (2012" startWordPosition="1224" endWordPosition="1227"> fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifically, they extract a visual space from images contained in the ESP-Game data set (von Ahn and Dabbish, 2004) and a text-based semantic space from a large corpus collection totaling approximately two billion words. They concatenate the two modalities and subsequently project them to a lower-dimensionality space using Singular Value Decomposition (Golub et al., 1981). Traditionally, computer vision algorithms describe visual phenomena (e.g., objects, scenes, faces, actions) by giving each instance a categorical</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>E. Bruni, G. Boleda, M. Baroni, and N. Tran. 2012a. Distributional Semantics in Technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 136–145, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruni</author>
<author>J Uijlings</author>
<author>M Baroni</author>
<author>N Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Multimedia,</booktitle>
<pages>1219--1228</pages>
<location>New York, NY.</location>
<contexts>
<context position="1636" citStr="Bruni et al., 2012" startWordPosition="248" endWordPosition="251">age tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn represe</context>
<context position="4682" citStr="Bruni et al., 2012" startWordPosition="726" endWordPosition="729">strand of research focuses exclusively on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well. This is not entirely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothi</context>
<context position="7833" citStr="Bruni et al. (2012" startWordPosition="1224" endWordPosition="1227"> fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifically, they extract a visual space from images contained in the ESP-Game data set (von Ahn and Dabbish, 2004) and a text-based semantic space from a large corpus collection totaling approximately two billion words. They concatenate the two modalities and subsequently project them to a lower-dimensionality space using Singular Value Decomposition (Golub et al., 1981). Traditionally, computer vision algorithms describe visual phenomena (e.g., objects, scenes, faces, actions) by giving each instance a categorical</context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>E. Bruni, J. Uijlings, M. Baroni, and N. Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings of the 20th ACM International Conference on Multimedia, pages 1219–1228., New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chai</author>
<author>C Hung</author>
</authors>
<title>Automatically Annotating Images with Keywords: A Review of Image Annotation Systems. Recent Patents on Computer Science,</title>
<date>2008</date>
<pages>1--55</pages>
<contexts>
<context position="2380" citStr="Chai and Hung, 2008" startWordPosition="364" endWordPosition="367">erned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of wor</context>
</contexts>
<marker>Chai, Hung, 2008</marker>
<rawString>C. Chai and C. Hung. 2008. Automatically Annotating Images with Keywords: A Review of Image Annotation Systems. Recent Patents on Computer Science, 1:55–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Datta</author>
<author>D Joshi</author>
<author>J Li</author>
<author>J Z Wang</author>
</authors>
<title>Image Retrieval: Ideas, Influences, and Trends of the New Age.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="2343" citStr="Datta et al., 2008" startWordPosition="358" endWordPosition="361">ta, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties</context>
</contexts>
<marker>Datta, Joshi, Li, Wang, 2008</marker>
<rawString>R. Datta, D. Joshi, J. Li, and J. Z. Wang. 2008. Image Retrieval: Ideas, Influences, and Trends of the New Age. ACM Computing Surveys, 40(2):1–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Deng</author>
<author>W Dong</author>
<author>R Socher</author>
<author>L Li</author>
<author>K Li</author>
<author>L FeiFei</author>
</authors>
<title>ImageNet: A Large-Scale Hierarchical Image Database.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>248--255</pages>
<location>Miami, Florida.</location>
<contexts>
<context position="4721" citStr="Deng et al., 2009" startWordPosition="733" endWordPosition="736">on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well. This is not entirely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothing items; balls are round, and so are o</context>
<context position="12822" citStr="Deng et al., 2009" startWordPosition="2004" endWordPosition="2007">05) feature norms. The norms cover a wide range of concrete concepts including animate and inanimate things (e.g., animals, clothing, vehicles, utensils, fruits, and vegetables) and were collected by presenting participants with words and asking them to list properties of the objects to which the words referred. To avoid confusion, in the remainder of this paper we will use the term attribute to refer to properties of concepts and the term feature to refer to image features, such as color or edges. Images for the concepts in McRae et al.’s (2005) production norms were harvested from ImageNet (Deng et al., 2009), an ontology of images based on the nominal hierarchy of WordNet (Fellbaum, 1998). ImageNet has more than 14 million images spanning 21K WordNet synsets. We chose this database due to its high coverage and the high quality of its images (i.e., cleanly labeled and high resolution). McRae et al.’s norms contain 541 concepts out of which 516 appear in ImageNet1 and are represented by 688K images overall. The average number of images per concept is 1,310 with the most popular being closet (2,149 images) and the least popular prune (5 images). 1Some words had to be modified in order to match the c</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, FeiFei, 2009</marker>
<rawString>J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 248–255, Miami, Florida.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Everingham</author>
<author>L Van Gool</author>
<author>C K I Williams</author>
</authors>
<marker>Everingham, Van Gool, Williams, </marker>
<rawString>M. Everingham, L. Van Gool, C. K. I. Williams,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Winn</author>
<author>A Zisserman</author>
</authors>
<date>2008</date>
<booktitle>The PASCAL Visual Object Classes Challenge</booktitle>
<note>(VOC2008) Results. http://www.pascalnetwork.org/challenges/VOC/voc2008/workshop.</note>
<marker>Winn, Zisserman, 2008</marker>
<rawString>J. Winn, and A. Zisserman. 2008. The PASCAL Visual Object Classes Challenge 2008 (VOC2008) Results. http://www.pascalnetwork.org/challenges/VOC/voc2008/workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fan</author>
<author>K Chang</author>
<author>C Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="19509" citStr="Fan et al., 2008" startWordPosition="3132" endWordPosition="3135">ed by McRae et al.’s (2005) participants for that concept even though they figured in the representations of other concepts. Furthermore, on average two McRae et al. attributes per concept were discarded. Examples of concepts and their attributes from our database2 are shown in Table 2. 4 Attribute-based Classification Following previous work (Farhadi et al., 2009; Lampert et al., 2009) we learned one classifier per attribute (i.e., 350 classifiers in total).3 The training set consisted of 91,980 images (with a maximum of 350 images per concept). We used an L2- regularized L2-loss linear SVM (Fan et al., 2008) to learn the attribute predictions. We adopted the training procedure of Farhadi al. (2009).4 To learn a classifier for a particular attribute, we used all images in the training data. Images of concepts annotated with the attribute were used as positive examples, and the rest as negative examples. The 2Available from http://homepages.inf.ed.ac.uk/ mlap/index.php?page=resources. 3We only trained classifiers for attributes corroborated by the images and excluded those labeled with &lt;no evidence&gt;. 4http://vision.cs.uiuc.edu/attributes/ data was randomly split into a training and validation set o</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farhadi</author>
<author>I Endres</author>
<author>D Hoiem</author>
<author>D Forsyth</author>
</authors>
<title>Describing Objects by their Attributes.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1778--1785</pages>
<location>Miami Beach, Florida.</location>
<contexts>
<context position="6380" citStr="Farhadi et al., 2009" startWordPosition="1006" endWordPosition="1009">imilar to those provided by participants in norming studies, however, importantly they are learned from training data (a database of images and their visual attributes) and thus generalize to new images without additional human involvement. In the following we describe our efforts to create a new large-scale dataset that consists of 688K images that match the same concrete concepts used in the feature norming study of McRae et al. (2005). We derive a taxonomy of 412 visual attributes and explain how we learn attribute classifiers following recent work in computer vision (Lampert et al., 2009; Farhadi et al., 2009). Next, we show that this attribute-based image representation can be usefully integrated with textual data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms. 2 Related Work Grounding semantic representations with visual information is an instance of multimodal learning. In this setting the data consists of multiple input modalities with different representations and the learner’s objective is to extract a unified representation that fuses the modalities together. The literature describes several success</context>
<context position="9029" citStr="Farhadi et al. (2009)" startWordPosition="1411" endWordPosition="1414">ch instance a categorical label (e.g., cat, beer garden, Brad Pitt, drinking). The ability to describe images by their attributes allows to generalize to new instances for which there are no training examples available. Moreover, attributes can transcend category and task boundaries and thus provide a generic description of visual data. Initial work (Ferrari and Zisserman, 2007) 573 focused on simple color and texture attributes (e.g., blue, stripes) and showed that these can be learned in a weakly supervised setting from images returned by a search engine when using the attribute as a query. Farhadi et al. (2009) were among the first to use visual attributes in an object recognition task. Using an inventory of 64 attribute labels, they developed a dataset of approximately 12,000 instances representing 20 objects from the PASCAL Visual Object Classes Challenge 2008 (Everingham et al., 2008). Visual semantic attributes (e.g., hairy, four-legged) were used to identify familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training exa</context>
<context position="10867" citStr="Farhadi et al., 2009" startWordPosition="1689" endWordPosition="1692">ibing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucially represent image properties, however by being words themselves, they can be easily integrated in any text-based distributional model thus eschewing known difficulties with rendering images into word-like units. A key prerequisite in describing images by their attributes is the availability of training data for learning attribute classifiers. Although our database shares many features with previous work (Lampert et al., 2009; Farhadi et al., 2009) it differs in focus and scope. Since our goal is to develop distributional models that are applicable to many words, it contains a considerably larger number of concepts (i.e., more than 500) and attributes (i.e., 412) based on a detailed taxonomy which we argue is cognitively plausible and beneficial for image and natural language processing tasks. Our experiments evaluate a number of models previously proposed in the literature and in Attribute Categories Example Attributes color patterns (25) is red, has stripes diet eats nuts, eats grass shape size (16) is small, is chubby parts (125) has</context>
<context position="15645" citStr="Farhadi et al. (2009)" startWordPosition="2481" endWordPosition="2484">attributes from McRae et al.’s (2005) norming study. Attributes capturing other primary sensory information (e.g., smell, sound), functional/motor properties, or encyclopaedic information were not taken into account. For example, is purple is a valid visual attribute for an eggplant, whereas a vegetable is not, since it cannot be visualized. Collating all the visual attributes in the norms resulted in a total of 673 which we further modified and extended during the annotation process explained below. The annotation was conducted on aper-concept rather than a per-image basis (as for example in Farhadi et al. (2009)). For each concept (e.g., bear or eggplant), we inspected the images in the development set and chose all McRae et al. (2005) visual attributes that applied. If an attribute was generally true for the concept, but the images did not provide enough evidence, the attribute was nevertheless chosen and labeled with &lt;no evidence&gt;. For example, a plum has a pit, but most images in ImageNet show plums where only the outer part of the fruit is visible. Attributes supported by the image data but missing from the norms were added. For example, has lights and has bumper are attributes of cars but are no</context>
<context position="19258" citStr="Farhadi et al., 2009" startWordPosition="3088" endWordPosition="3091">. For example, both, lemons and oranges have pulp. But the norms provide this attribute only for the second concept. On average, each concept was annotated with 19 attributes; approximately 14.5 of these were not part of the semantic representation created by McRae et al.’s (2005) participants for that concept even though they figured in the representations of other concepts. Furthermore, on average two McRae et al. attributes per concept were discarded. Examples of concepts and their attributes from our database2 are shown in Table 2. 4 Attribute-based Classification Following previous work (Farhadi et al., 2009; Lampert et al., 2009) we learned one classifier per attribute (i.e., 350 classifiers in total).3 The training set consisted of 91,980 images (with a maximum of 350 images per concept). We used an L2- regularized L2-loss linear SVM (Fan et al., 2008) to learn the attribute predictions. We adopted the training procedure of Farhadi al. (2009).4 To learn a classifier for a particular attribute, we used all images in the training data. Images of concepts annotated with the attribute were used as positive examples, and the rest as negative examples. The 2Available from http://homepages.inf.ed.ac.u</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing Objects by their Attributes. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 1778–1785, Miami Beach, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fei-Fei</author>
<author>P Perona</author>
</authors>
<title>A Bayesian Hierarchical Model for Learning Natural Scene Categories.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>524--531</pages>
<location>San Diego, California.</location>
<contexts>
<context position="2515" citStr="Fei-Fei and Perona, 2005" startWordPosition="384" endWordPosition="387"> learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, wher</context>
</contexts>
<marker>Fei-Fei, Perona, 2005</marker>
<rawString>L. Fei-Fei and P. Perona. 2005. A Bayesian Hierarchical Model for Learning Natural Scene Categories. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 524–531, San Diego, California.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="27923" citStr="(1998)" startWordPosition="4492" endWordPosition="4492">attributes (rather than documents). Each attribute is represented as a bag-of-concepts, i.e., words demonstrating the property expressed by the attribute (e.g., vegetable-n is a property of eggplant, spinach, carrot). For some of these concepts, our classifiers predict visual attributes. In this case, the concepts are paired with one of their visual attributes. We sample attributes for a concept w from their distribution given w (Eq. (1)). 6 Experimental Setup Evaluation Task We evaluated the distributional models presented in Section 5 on the word association norms collected by Nelson et al. (1998).7 These were established by presenting a large number of participants with a cue word (e.g., rice) and asking them to name an associate 7From http://w3.usf.edu/FreeAssociation/. word in response (e.g., Chinese, wedding, food, white). For each cue, the norms provide a set of associates and the frequencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the degree of similarity between a cue and its associates using our models. The norms contain 63,619 unique cueassociate pairs. Of these, 435 pairs were cove</context>
<context position="30924" citStr="(1998)" startWordPosition="4980" endWordPosition="4980">how many times the word with the highest score under the model was the first associate in the human norms. This evaluation metric assumes that there are many associates for a given cue which unfortunately is not the case in our study which is restricted to the concepts represented in our attribute taxonomy. 9Baroni et al. (2010) use a similar threshold of 19.51. (2) C ∑ l=1 P(wi|xc) P(wi|xl) 578 Nelson Concat CCA TopicAttr TextAttr Concat 0.24 CCA 0.30 0.72 TopicAttr 0.26 0.55 0.28 TextAttr 0.21 0.80 0.83 0.34 VisAttr 0.23 0.65 0.52 0.40 0.39 Table 5: Correlation matrix for seen Nelson et al. (1998) cue-associate pairs and five distributional models. All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Nelson Concat CCA TopicAttr TextAttr Concat 0.11 CCA 0.15 0.66 TopicAttr 0.17 0.69 0.48 TextAttr 0.11 0.65 0.25 0.39 VisAttr 0.13 0.57 0.87 0.57 0.34 Table 6: Correlation matrix for unseen Nelson et al. (1998) cue-associate pairs and five distributional models. All correlation coefficients are statistically significant (p &lt; 0.01, N = 1,716). 7 Results Our experiments were designed to answer four questions: (1) Do visual attributes improve the performance of distr</context>
<context position="34133" citStr="(1998)" startWordPosition="5483" endWordPosition="5483">6), Concat fares worse than CCA and TopicAttr, achieving similar performance to TextAttr. CCA and TopicAttr are significantly better than TextAttr and VisAttr (p &lt; 0.01). This indicates that our attribute classifiers generalize well beyond the concepts found in our database and can produce useful visual information even on unseen images. Compared to Concat and CCA, TopicAttr obtains a better fit with the human association norms on the unseen data. To answer our third question, we obtained distributional models from McRae et al.’s (2005) norms and assessed how well they predict Nelson et al.’s (1998) word-associate similarities. Each concept was represented as a vector with dimensions corresponding to attributes generated by participants of the norming study. Vector components were set to the (normalized) frequency with which participants generated the corresponding attribute when presented with the concept. We measured the similarity between two words using the cosine coefficient. Table 7 presents results for different model variants which we created by manipulating the number and type of attributes involved. The first model uses the full set of attributes present in the norms (All Attri</context>
<context position="35675" citStr="(1998)" startWordPosition="5722" endWordPosition="5722">8) association norms, whereas visual and textual attributes on their own perform worse. Interestingly, CCA’s 579 Models Seen All Attributes 0.28 Text Attributes 0.20 Visual Attributes 0.25 Table 7: Model performance on seen Nelson et al. (1998) cue-associate pairs; models are based on gold human generated attributes (McRae et al., 2005). All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Models Seen Unseen Concat 0.22 0.10 CCA 0.26 0.15 TopicAttr 0.23 0.19 TextAttr 0.20 0.08 VisAttr 0.21 0.13 MixLDA 0.16 0.11 Table 8: Model performance on a subset of Nelson et al. (1998) cue-associate pairs. Seen are concepts known to the attribute classifiers and covered by MixLDA (N = 85). Unseen are concepts covered by LDA but unknown to the attribute classifiers (N = 388). All correlation coefficients are statistically significant (p &lt; 0.05). performance is comparable to the All Attributes model (see Table 5, second column), despite using automatic attributes (both textual and visual). Furthermore, visual attributes obtained through our classifiers (see Table 5) achieve a marginally lower correlation coefficient against human generated ones (see Table 7). Finally, to addr</context>
</contexts>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: an Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Feng</author>
<author>M Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>272--280</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="36549" citStr="Feng and Lapata, 2008" startWordPosition="5853" endWordPosition="5856"> (p &lt; 0.05). performance is comparable to the All Attributes model (see Table 5, second column), despite using automatic attributes (both textual and visual). Furthermore, visual attributes obtained through our classifiers (see Table 5) achieve a marginally lower correlation coefficient against human generated ones (see Table 7). Finally, to address our last question, we compared our approach against Feng and Lapata (2010) who represent visual information via quantized SIFT features. We trained their MixLDA model on their corpus consisting of 3,361 BBC news documents and corresponding images (Feng and Lapata, 2008). We optimized the model parameters on a development set consisting of cueassociate pairs from Nelson et al. (1998), excluding the concepts in McRae et al. (2005). We used a vocabulary of approximately 6,000 words. The best performing model on the development set used 500 visual terms and 750 topics and the association measure proposed in Griffiths et al. (2007). The test set consisted of 85 seen and 388 unseen cue-associate pairs that were covered by our models and MixLDA. Table 8 reports correlation coefficients for our models and MixLDA against human probabilities. All attribute-based model</context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Y. Feng and M. Lapata. 2008. Automatic image annotation using auxiliary text information. In Proceedings of ACL-08: HLT, pages 272–280, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Feng</author>
<author>M Lapata</author>
</authors>
<title>Visual Information in Semantic Representation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>91--99</pages>
<publisher>ACL.</publisher>
<location>Los Angeles, California.</location>
<contexts>
<context position="1660" citStr="Feng and Lapata, 2010" startWordPosition="252" endWordPosition="255">cal world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal </context>
<context position="4662" citStr="Feng and Lapata, 2010" startWordPosition="722" endWordPosition="725">feature norms. Another strand of research focuses exclusively on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well. This is not entirely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stri</context>
<context position="7320" citStr="Feng and Lapata (2010)" startWordPosition="1142" endWordPosition="1145">ation is an instance of multimodal learning. In this setting the data consists of multiple input modalities with different representations and the learner’s objective is to extract a unified representation that fuses the modalities together. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Special-purpose models that address the fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifica</context>
<context position="36353" citStr="Feng and Lapata (2010)" startWordPosition="5823" endWordPosition="5826">ribute classifiers and covered by MixLDA (N = 85). Unseen are concepts covered by LDA but unknown to the attribute classifiers (N = 388). All correlation coefficients are statistically significant (p &lt; 0.05). performance is comparable to the All Attributes model (see Table 5, second column), despite using automatic attributes (both textual and visual). Furthermore, visual attributes obtained through our classifiers (see Table 5) achieve a marginally lower correlation coefficient against human generated ones (see Table 7). Finally, to address our last question, we compared our approach against Feng and Lapata (2010) who represent visual information via quantized SIFT features. We trained their MixLDA model on their corpus consisting of 3,361 BBC news documents and corresponding images (Feng and Lapata, 2008). We optimized the model parameters on a development set consisting of cueassociate pairs from Nelson et al. (1998), excluding the concepts in McRae et al. (2005). We used a vocabulary of approximately 6,000 words. The best performing model on the development set used 500 visual terms and 750 topics and the association measure proposed in Griffiths et al. (2007). The test set consisted of 85 seen and </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Y. Feng and M. Lapata. 2010. Visual Information in Semantic Representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 91–99, Los Angeles, California. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ferrari</author>
<author>A Zisserman</author>
</authors>
<title>Learning Visual Attributes.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 20,</booktitle>
<pages>433--440</pages>
<editor>In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="8789" citStr="Ferrari and Zisserman, 2007" startWordPosition="1369" endWordPosition="1372">alities and subsequently project them to a lower-dimensionality space using Singular Value Decomposition (Golub et al., 1981). Traditionally, computer vision algorithms describe visual phenomena (e.g., objects, scenes, faces, actions) by giving each instance a categorical label (e.g., cat, beer garden, Brad Pitt, drinking). The ability to describe images by their attributes allows to generalize to new instances for which there are no training examples available. Moreover, attributes can transcend category and task boundaries and thus provide a generic description of visual data. Initial work (Ferrari and Zisserman, 2007) 573 focused on simple color and texture attributes (e.g., blue, stripes) and showed that these can be learned in a weakly supervised setting from images returned by a search engine when using the attribute as a query. Farhadi et al. (2009) were among the first to use visual attributes in an object recognition task. Using an inventory of 64 attribute labels, they developed a dataset of approximately 12,000 instances representing 20 objects from the PASCAL Visual Object Classes Challenge 2008 (Everingham et al., 2008). Visual semantic attributes (e.g., hairy, four-legged) were used to identify </context>
</contexts>
<marker>Ferrari, Zisserman, 2007</marker>
<rawString>V. Ferrari and A. Zisserman. 2007. Learning Visual Attributes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 433–440. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H Golub</author>
<author>F T Luk</author>
<author>M L Overton</author>
</authors>
<title>A block lanczoz method for computing the singular values and corresponding singular vectors of a matrix.</title>
<date>1981</date>
<journal>ACM Transactions on Mathematical Software,</journal>
<pages>7--149</pages>
<contexts>
<context position="8286" citStr="Golub et al., 1981" startWordPosition="1293" endWordPosition="1296">ges). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifically, they extract a visual space from images contained in the ESP-Game data set (von Ahn and Dabbish, 2004) and a text-based semantic space from a large corpus collection totaling approximately two billion words. They concatenate the two modalities and subsequently project them to a lower-dimensionality space using Singular Value Decomposition (Golub et al., 1981). Traditionally, computer vision algorithms describe visual phenomena (e.g., objects, scenes, faces, actions) by giving each instance a categorical label (e.g., cat, beer garden, Brad Pitt, drinking). The ability to describe images by their attributes allows to generalize to new instances for which there are no training examples available. Moreover, attributes can transcend category and task boundaries and thus provide a generic description of visual data. Initial work (Ferrari and Zisserman, 2007) 573 focused on simple color and texture attributes (e.g., blue, stripes) and showed that these c</context>
</contexts>
<marker>Golub, Luk, Overton, 1981</marker>
<rawString>G. H. Golub, F. T. Luk, and M. L. Overton. 1981. A block lanczoz method for computing the singular values and corresponding singular vectors of a matrix. ACM Transactions on Mathematical Software, 7:149–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gorniak</author>
<author>D Roy</author>
</authors>
<title>Grounded Semantic Composition for Visual Scenes.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>21--429</pages>
<contexts>
<context position="1517" citStr="Gorniak and Roy, 2004" startWordPosition="227" endWordPosition="230">ed interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Bars</context>
</contexts>
<marker>Gorniak, Roy, 2004</marker>
<rawString>P. Gorniak and D. Roy. 2004. Grounded Semantic Composition for Visual Scenes. Journal ofArtificial Intelligence Research, 21:429–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>J B Tenenbaum</author>
</authors>
<title>Topics in Semantic Representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="29887" citStr="Griffiths et al. (2007)" startWordPosition="4809" endWordPosition="4812">te prediction scores and with excluding attributes with low precision. In both cases, we obtained best results when using all attributes. We could apply CCA to the vectors representing each image separately and then compute a weighted centroid on the projected vectors. We refrained from doing this as it involves additional parameters and assumes input different from the other models. We measured the similarity between two words using the cosine of the angle. For the attribute-topic model, the number of predefined components C was set to 10. In this model, similarity was measured as defined by Griffiths et al. (2007). The underlying idea is that word association can be expressed as a conditional distribution. With regard to the textual attributes, we obtained a 9,394-dimensional semantic space after discarding word-attribute pairs with a log-likelihood ratio score less than 19.9 We also discarded attributes co-occurring with less than two different words. 8Previous work (Griffiths et al., 2007) which also predicts word association reports how many times the word with the highest score under the model was the first associate in the human norms. This evaluation metric assumes that there are many associates </context>
<context position="36913" citStr="Griffiths et al. (2007)" startWordPosition="5916" endWordPosition="5919">estion, we compared our approach against Feng and Lapata (2010) who represent visual information via quantized SIFT features. We trained their MixLDA model on their corpus consisting of 3,361 BBC news documents and corresponding images (Feng and Lapata, 2008). We optimized the model parameters on a development set consisting of cueassociate pairs from Nelson et al. (1998), excluding the concepts in McRae et al. (2005). We used a vocabulary of approximately 6,000 words. The best performing model on the development set used 500 visual terms and 750 topics and the association measure proposed in Griffiths et al. (2007). The test set consisted of 85 seen and 388 unseen cue-associate pairs that were covered by our models and MixLDA. Table 8 reports correlation coefficients for our models and MixLDA against human probabilities. All attribute-based models significantly outperform MixLDA on seen pairs (p &lt; 0.05 using a t-test). MixLDA performs on a par with the concatenation model on unseen pairs, however CCA, TopicAttr, and VisAttr are all superior. Although these comparisons should be taken with a grain of salt, given that MixLDA and our models are trained on different corpora (MixLDA assumes that texts and im</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>T. L. Griffiths, M. Steyvers, and J. B. Tenenbaum. 2007. Topics in Semantic Representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Hardoon</author>
<author>S R Szedmak</author>
<author>J R ShaweTaylor</author>
</authors>
<title>Canonical Correlation Analysis: An Overview with Application to Learning Methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="24966" citStr="Hardoon et al. (2004)" startWordPosition="3983" endWordPosition="3986">epresentation. Concatenation Model Variants of this model were originally proposed in Bruni et al. (2011) and Johns and Jones (2012). Let T ∈ RN×D denote a term-attribute co-occurrence matrix, where each cell records a weighted co-occurrence score of a word and a textual attribute. Let P ∈ [0,1]N×F denote a visual matrix, representing a probability distribution over visual attributes for each word. A word’s meaning can be then represented by the concatenation of its normalized textual and visual vectors. Canonical Correlation Analysis The second model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and visual modalities. Given two random variables x and y (or two sets of vectors), CCA can be seen as determining two sets of basis vectors in such a way, that the correlation between the projections of the variables onto these bases is mutually maximized (Borga, 2001). In effect, the representation-specific details pertaining to the two views of the same phenomenon are discarded and the underlying hidden factors responsible for the correlation are revealed. The linguistic and visual views are the same as in the simple concatenation </context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>D. R. Hardoon, S. R. Szedmak, and J. R. ShaweTaylor. 2004. Canonical Correlation Analysis: An Overview with Application to Learning Methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T Johns</author>
<author>M N Jones</author>
</authors>
<title>Perceptual Inference through Global Lexical Similarity. Topics in</title>
<date>2012</date>
<journal>Cognitive Science,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1683" citStr="Johns and Jones, 2012" startWordPosition="256" endWordPosition="259"> grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical</context>
<context position="24477" citStr="Johns and Jones (2012)" startWordPosition="3908" endWordPosition="3911">ven shirt blouse, robe, cape, vest, dress, coat, jacket, skirt, camisole, nightgown spinach lettuce, parsley, peas, celery, broccoli, cabbage, cucumber, rhubarb, zucchini, asparagus squirrel chipmunk, raccoon, groundhog, gopher, porcupine, hare, rabbit, fox, mole, emu Table 4: Ten most similar concepts computed on the basis of averaged attribute vectors and ordered according to cosine similarity. used in our study and how the textual and visual modalities were fused to create a joint representation. Concatenation Model Variants of this model were originally proposed in Bruni et al. (2011) and Johns and Jones (2012). Let T ∈ RN×D denote a term-attribute co-occurrence matrix, where each cell records a weighted co-occurrence score of a word and a textual attribute. Let P ∈ [0,1]N×F denote a visual matrix, representing a probability distribution over visual attributes for each word. A word’s meaning can be then represented by the concatenation of its normalized textual and visual vectors. Canonical Correlation Analysis The second model uses Canonical Correlation Analysis (CCA, Hardoon et al. (2004)) to learn a joint semantic representation from the textual and visual modalities. Given two random variables x</context>
</contexts>
<marker>Johns, Jones, 2012</marker>
<rawString>B. T. Johns and M. N. Jones. 2012. Perceptual Inference through Global Lexical Similarity. Topics in Cognitive Science, 4(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Joshi</author>
<author>J Z Wang</author>
<author>J Li</author>
</authors>
<title>The Story Picturing Engine—A System for Automatic Text illustration.</title>
<date>2006</date>
<journal>ACM Transactions on Multimedia Computing, Communications, and Applications,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="2420" citStr="Joshi et al., 2006" startWordPosition="370" endWordPosition="373">ucting perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically gr</context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>D. Joshi, J.Z. Wang, and J. Li. 2006. The Story Picturing Engine—A System for Automatic Text illustration. ACM Transactions on Multimedia Computing, Communications, and Applications, 2(1):68–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Learning Language Semantics from Ambiguous Supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd Conference on Artificial Intelligence,</booktitle>
<pages>895--900</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1228" citStr="Kate and Mooney, 2007" startWordPosition="180" endWordPosition="183">e their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounde</context>
</contexts>
<marker>Kate, Mooney, 2007</marker>
<rawString>R. J. Kate and R. J. Mooney. 2007. Learning Language Semantics from Ambiguous Supervision. In Proceedings of the 22nd Conference on Artificial Intelligence, pages 895–900, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kumar</author>
<author>A C Berg</author>
<author>P N Belhumeur</author>
<author>S K Nayar</author>
</authors>
<title>Attribute and Simile Classifiers for Face Verification.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE 12th International Conference on Computer Vision,</booktitle>
<pages>365--372</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="9922" citStr="Kumar et al., 2009" startWordPosition="1551" endWordPosition="1554">., 2008). Visual semantic attributes (e.g., hairy, four-legged) were used to identify familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training examples of the target classes available. Their dataset contained over 30,000 images representing 50 animal concepts and used 85 attributes from the norming study of Osherson et al. (1991). Attribute-based representations have also been applied to the tasks of face detection (Kumar et al., 2009), action identification (Liu et al., 2011), and scene recognition (Patterson and Hays, 2012). The use of visual attributes in models of distributional semantics is novel to our knowledge. We argue that they are advantageous for two reasons. Firstly, they are cognitively plausible; humans employ visual attributes when describing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucially represent image properties, however by being words themselves, they can be easily integrated in any text</context>
</contexts>
<marker>Kumar, Berg, Belhumeur, Nayar, 2009</marker>
<rawString>N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. 2009. Attribute and Simile Classifiers for Face Verification. In Proceedings of the IEEE 12th International Conference on Computer Vision, pages 365–372, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Lampert</author>
<author>H Nickisch</author>
<author>S Harmeling</author>
</authors>
<title>Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>951--958</pages>
<location>Miami Beach, Florida.</location>
<contexts>
<context position="6357" citStr="Lampert et al., 2009" startWordPosition="1002" endWordPosition="1005">. Our attributes are similar to those provided by participants in norming studies, however, importantly they are learned from training data (a database of images and their visual attributes) and thus generalize to new images without additional human involvement. In the following we describe our efforts to create a new large-scale dataset that consists of 688K images that match the same concrete concepts used in the feature norming study of McRae et al. (2005). We derive a taxonomy of 412 visual attributes and explain how we learn attribute classifiers following recent work in computer vision (Lampert et al., 2009; Farhadi et al., 2009). Next, we show that this attribute-based image representation can be usefully integrated with textual data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms. 2 Related Work Grounding semantic representations with visual information is an instance of multimodal learning. In this setting the data consists of multiple input modalities with different representations and the learner’s objective is to extract a unified representation that fuses the modalities together. The literature de</context>
<context position="9522" citStr="Lampert et al. (2009)" startWordPosition="1489" endWordPosition="1492">n a weakly supervised setting from images returned by a search engine when using the attribute as a query. Farhadi et al. (2009) were among the first to use visual attributes in an object recognition task. Using an inventory of 64 attribute labels, they developed a dataset of approximately 12,000 instances representing 20 objects from the PASCAL Visual Object Classes Challenge 2008 (Everingham et al., 2008). Visual semantic attributes (e.g., hairy, four-legged) were used to identify familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training examples of the target classes available. Their dataset contained over 30,000 images representing 50 animal concepts and used 85 attributes from the norming study of Osherson et al. (1991). Attribute-based representations have also been applied to the tasks of face detection (Kumar et al., 2009), action identification (Liu et al., 2011), and scene recognition (Patterson and Hays, 2012). The use of visual attributes in models of distributional semantics is novel to our knowledge. We argue tha</context>
<context position="10844" citStr="Lampert et al., 2009" startWordPosition="1685" endWordPosition="1688"> attributes when describing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucially represent image properties, however by being words themselves, they can be easily integrated in any text-based distributional model thus eschewing known difficulties with rendering images into word-like units. A key prerequisite in describing images by their attributes is the availability of training data for learning attribute classifiers. Although our database shares many features with previous work (Lampert et al., 2009; Farhadi et al., 2009) it differs in focus and scope. Since our goal is to develop distributional models that are applicable to many words, it contains a considerably larger number of concepts (i.e., more than 500) and attributes (i.e., 412) based on a detailed taxonomy which we argue is cognitively plausible and beneficial for image and natural language processing tasks. Our experiments evaluate a number of models previously proposed in the literature and in Attribute Categories Example Attributes color patterns (25) is red, has stripes diet eats nuts, eats grass shape size (16) is small, is</context>
<context position="16897" citStr="Lampert et al. (2009)" startWordPosition="2689" endWordPosition="2692">utes were grouped in eight general classes shown in Table 1. Annotation proceeded on a category-by-category basis, e.g., first all foodrelated concepts were annotated, then animals, vehicles, and so on. Two annotators (both co-authors of this paper) developed the set of attributes for each category. One annotator first labeled concepts with their attributes, and the other annotator reviewed the annotations, making changes if needed. Annotations were revised and compared per category in order to ensure consistency across all concepts of that category. Our methodology is slightly different from Lampert et al. (2009) in that we did not simply transfer the attributes from the norms to the concepts in question but refined and extended them according to the visual data. There are several reasons for this. Firstly, it makes sense to select attributes corroborated by the images. Secondly, by looking at the actual images, we could eliminate errors in McRae et al.’s (2005) norms. For example, eight study participants erroneously thought that a catfish has scales. Thirdly, during the annotation process, we normalized synonymous attributes (e.g., has pit and has stone) and attributes that exhibited negligible vari</context>
<context position="19281" citStr="Lampert et al., 2009" startWordPosition="3092" endWordPosition="3095">emons and oranges have pulp. But the norms provide this attribute only for the second concept. On average, each concept was annotated with 19 attributes; approximately 14.5 of these were not part of the semantic representation created by McRae et al.’s (2005) participants for that concept even though they figured in the representations of other concepts. Furthermore, on average two McRae et al. attributes per concept were discarded. Examples of concepts and their attributes from our database2 are shown in Table 2. 4 Attribute-based Classification Following previous work (Farhadi et al., 2009; Lampert et al., 2009) we learned one classifier per attribute (i.e., 350 classifiers in total).3 The training set consisted of 91,980 images (with a maximum of 350 images per concept). We used an L2- regularized L2-loss linear SVM (Fan et al., 2008) to learn the attribute predictions. We adopted the training procedure of Farhadi al. (2009).4 To learn a classifier for a particular attribute, we used all images in the training data. Images of concepts annotated with the attribute were used as positive examples, and the rest as negative examples. The 2Available from http://homepages.inf.ed.ac.uk/ mlap/index.php?page=</context>
</contexts>
<marker>Lampert, Nickisch, Harmeling, 2009</marker>
<rawString>C. H. Lampert, H. Nickisch, and S. Harmeling. 2009. Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer. In Computer Vision and Pattern Recognition, pages 951–958, Miami Beach, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Landau</author>
<author>L Smith</author>
<author>S Jones</author>
</authors>
<title>Object Perception and Object Naming in Early Development. Trends in Cognitive Science,</title>
<date>1998</date>
<pages>27--19</pages>
<contexts>
<context position="2173" citStr="Landau et al., 1998" startWordPosition="333" endWordPosition="336">g the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking n</context>
</contexts>
<marker>Landau, Smith, Jones, 1998</marker>
<rawString>B. Landau, L. Smith, and S. Jones. 1998. Object Perception and Object Naming in Early Development. Trends in Cognitive Science, 27:19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leong</author>
<author>R Mihalcea</author>
</authors>
<title>Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1403--1407</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="4865" citStr="Leong and Mihalcea, 2011" startWordPosition="754" endWordPosition="757">ely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothing items; balls are round, and so are oranges and coins), and thus allow us to express similarities and differences across concepts more easily. Furthermore, attributes allow us to ge</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>C. Leong and R. Mihalcea. 2011. Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1403–1407, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>B Kuipers</author>
<author>S Savarese</author>
</authors>
<title>Recognizing Human Actions by Attributes.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>3337--3344</pages>
<location>Colorado Springs, Colorado.</location>
<contexts>
<context position="9964" citStr="Liu et al., 2011" startWordPosition="1557" endWordPosition="1560">hairy, four-legged) were used to identify familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training examples of the target classes available. Their dataset contained over 30,000 images representing 50 animal concepts and used 85 attributes from the norming study of Osherson et al. (1991). Attribute-based representations have also been applied to the tasks of face detection (Kumar et al., 2009), action identification (Liu et al., 2011), and scene recognition (Patterson and Hays, 2012). The use of visual attributes in models of distributional semantics is novel to our knowledge. We argue that they are advantageous for two reasons. Firstly, they are cognitively plausible; humans employ visual attributes when describing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucially represent image properties, however by being words themselves, they can be easily integrated in any text-based distributional model thus eschewing</context>
</contexts>
<marker>Liu, Kuipers, Savarese, 2011</marker>
<rawString>J. Liu, B. Kuipers, and S. Savarese. 2011. Recognizing Human Actions by Attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3337–3344, Colorado Springs, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Lowe</author>
</authors>
<title>Object Recognition from Local Scale-invariant Features.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Computer Vision,</booktitle>
<pages>1150--1157</pages>
<location>Corfu, Greece.</location>
<contexts>
<context position="2462" citStr="Lowe, 1999" startWordPosition="378" endWordPosition="379"> The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy,</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>D. G. Lowe. 1999. Object Recognition from Local Scale-invariant Features. In Proceedings of the International Conference on Computer Vision, pages 1150–1157, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lowe</author>
</authors>
<title>Distinctive Image Features from Scale-invariant Keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="4960" citStr="Lowe, 2004" startWordPosition="772" endWordPosition="773">ations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothing items; balls are round, and so are oranges and coins), and thus allow us to express similarities and differences across concepts more easily. Furthermore, attributes allow us to generalize to unseen objects; it is possible to say something about them even though we cannot id</context>
<context position="7492" citStr="Lowe, 2004" startWordPosition="1169" endWordPosition="1170">unified representation that fuses the modalities together. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Special-purpose models that address the fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a vector whose components correspond to wordtopic distributions. A related model has been proposed by Bruni et al. (2012b) who obtain distinct representations for the textual and visual modalities. Specifically, they extract a visual space from images contained in the ESP-Game data set (von Ahn and Dabbish, 2004) and a text-based semantic space from a large corpus collection t</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>D. Lowe. 2004. Distinctive Image Features from Scale-invariant Keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lu</author>
<author>H T Ng</author>
<author>W S Lee</author>
<author>L S Zettlemoyer</author>
</authors>
<title>A Generative Model for Parsing Natural Language to Meaning Representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>783--792</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1245" citStr="Lu et al., 2008" startWordPosition="184" endWordPosition="187">h text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional </context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>W. Lu, H. T. Ng, W.S. Lee, and L. S. Zettlemoyer. 2008. A Generative Model for Parsing Natural Language to Meaning Representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783–792, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McRae</author>
<author>G S Cree</author>
<author>M S Seidenberg</author>
<author>C McNorgan</author>
</authors>
<title>Semantic Feature Production Norms for a Large Set of Living and Nonliving Things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="6200" citStr="McRae et al. (2005)" startWordPosition="975" endWordPosition="978"> has a beak and a long tail). We show that this attribute-centric approach to representing images is beneficial for distributional models of lexical meaning. Our attributes are similar to those provided by participants in norming studies, however, importantly they are learned from training data (a database of images and their visual attributes) and thus generalize to new images without additional human involvement. In the following we describe our efforts to create a new large-scale dataset that consists of 688K images that match the same concrete concepts used in the feature norming study of McRae et al. (2005). We derive a taxonomy of 412 visual attributes and explain how we learn attribute classifiers following recent work in computer vision (Lampert et al., 2009; Farhadi et al., 2009). Next, we show that this attribute-based image representation can be usefully integrated with textual data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms. 2 Related Work Grounding semantic representations with visual information is an instance of multimodal learning. In this setting the data consists of multiple input modal</context>
<context position="15771" citStr="McRae et al. (2005)" startWordPosition="2504" endWordPosition="2507">, functional/motor properties, or encyclopaedic information were not taken into account. For example, is purple is a valid visual attribute for an eggplant, whereas a vegetable is not, since it cannot be visualized. Collating all the visual attributes in the norms resulted in a total of 673 which we further modified and extended during the annotation process explained below. The annotation was conducted on aper-concept rather than a per-image basis (as for example in Farhadi et al. (2009)). For each concept (e.g., bear or eggplant), we inspected the images in the development set and chose all McRae et al. (2005) visual attributes that applied. If an attribute was generally true for the concept, but the images did not provide enough evidence, the attribute was nevertheless chosen and labeled with &lt;no evidence&gt;. For example, a plum has a pit, but most images in ImageNet show plums where only the outer part of the fruit is visible. Attributes supported by the image data but missing from the norms were added. For example, has lights and has bumper are attributes of cars but are not included in the norms. Attributes were grouped in eight general classes shown in Table 1. Annotation proceeded on a category</context>
<context position="28549" citStr="McRae et al. (2005)" startWordPosition="4590" endWordPosition="4593">e were established by presenting a large number of participants with a cue word (e.g., rice) and asking them to name an associate 7From http://w3.usf.edu/FreeAssociation/. word in response (e.g., Chinese, wedding, food, white). For each cue, the norms provide a set of associates and the frequencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the degree of similarity between a cue and its associates using our models. The norms contain 63,619 unique cueassociate pairs. Of these, 435 pairs were covered by McRae et al. (2005) and our models. We also experimented with 1,716 pairs that were not part of McRae et al.’s study but belonged to concepts covered by our attribute taxonomy (e.g., animals, vehicles), and were present in our corpus and ImageNet. Using correlation analysis (Spearman’s p), we examined the degree of linear relationship between the human cue-associate probabilities and the automatically derived similarity values.8 Parameter Settings In order to integrate the visual attributes with the models described in Section 5 we must select the appropriate threshold value S (see Eq. (1)). We optimized this va</context>
<context position="35407" citStr="McRae et al., 2005" startWordPosition="5674" endWordPosition="5677">ributes but those classified as visual (e.g., functional, encyclopaedic). The third model (Visual Attributes) considers solely visual attributes. We observe a similar trend as with our computational models. Taking visual attributes into account increases the fit with Nelson’s (1998) association norms, whereas visual and textual attributes on their own perform worse. Interestingly, CCA’s 579 Models Seen All Attributes 0.28 Text Attributes 0.20 Visual Attributes 0.25 Table 7: Model performance on seen Nelson et al. (1998) cue-associate pairs; models are based on gold human generated attributes (McRae et al., 2005). All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Models Seen Unseen Concat 0.22 0.10 CCA 0.26 0.15 TopicAttr 0.23 0.19 TextAttr 0.20 0.08 VisAttr 0.21 0.13 MixLDA 0.16 0.11 Table 8: Model performance on a subset of Nelson et al. (1998) cue-associate pairs. Seen are concepts known to the attribute classifiers and covered by MixLDA (N = 85). Unseen are concepts covered by LDA but unknown to the attribute classifiers (N = 388). All correlation coefficients are statistically significant (p &lt; 0.05). performance is comparable to the All Attributes model (see Table 5,</context>
<context position="36711" citStr="McRae et al. (2005)" startWordPosition="5882" endWordPosition="5885">rmore, visual attributes obtained through our classifiers (see Table 5) achieve a marginally lower correlation coefficient against human generated ones (see Table 7). Finally, to address our last question, we compared our approach against Feng and Lapata (2010) who represent visual information via quantized SIFT features. We trained their MixLDA model on their corpus consisting of 3,361 BBC news documents and corresponding images (Feng and Lapata, 2008). We optimized the model parameters on a development set consisting of cueassociate pairs from Nelson et al. (1998), excluding the concepts in McRae et al. (2005). We used a vocabulary of approximately 6,000 words. The best performing model on the development set used 500 visual terms and 750 topics and the association measure proposed in Griffiths et al. (2007). The test set consisted of 85 seen and 388 unseen cue-associate pairs that were covered by our models and MixLDA. Table 8 reports correlation coefficients for our models and MixLDA against human probabilities. All attribute-based models significantly outperform MixLDA on seen pairs (p &lt; 0.05 using a t-test). MixLDA performs on a par with the concatenation model on unseen pairs, however CCA, Top</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>K. McRae, G. S. Cree, M. S. Seidenberg, and C. McNorgan. 2005. Semantic Feature Production Norms for a Large Set of Living and Nonliving Things. Behavior Research Methods, 37(4):547–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Nelson</author>
<author>C L McEvoy</author>
<author>T A Schreiber</author>
</authors>
<date>1998</date>
<institution>The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</institution>
<contexts>
<context position="27923" citStr="Nelson et al. (1998)" startWordPosition="4489" endWordPosition="4492"> D of textual attributes (rather than documents). Each attribute is represented as a bag-of-concepts, i.e., words demonstrating the property expressed by the attribute (e.g., vegetable-n is a property of eggplant, spinach, carrot). For some of these concepts, our classifiers predict visual attributes. In this case, the concepts are paired with one of their visual attributes. We sample attributes for a concept w from their distribution given w (Eq. (1)). 6 Experimental Setup Evaluation Task We evaluated the distributional models presented in Section 5 on the word association norms collected by Nelson et al. (1998).7 These were established by presenting a large number of participants with a cue word (e.g., rice) and asking them to name an associate 7From http://w3.usf.edu/FreeAssociation/. word in response (e.g., Chinese, wedding, food, white). For each cue, the norms provide a set of associates and the frequencies with which they were named. We can thus compute the probability distribution over associates for each cue. Analogously, we can estimate the degree of similarity between a cue and its associates using our models. The norms contain 63,619 unique cueassociate pairs. Of these, 435 pairs were cove</context>
<context position="30924" citStr="Nelson et al. (1998)" startWordPosition="4977" endWordPosition="4980">ation reports how many times the word with the highest score under the model was the first associate in the human norms. This evaluation metric assumes that there are many associates for a given cue which unfortunately is not the case in our study which is restricted to the concepts represented in our attribute taxonomy. 9Baroni et al. (2010) use a similar threshold of 19.51. (2) C ∑ l=1 P(wi|xc) P(wi|xl) 578 Nelson Concat CCA TopicAttr TextAttr Concat 0.24 CCA 0.30 0.72 TopicAttr 0.26 0.55 0.28 TextAttr 0.21 0.80 0.83 0.34 VisAttr 0.23 0.65 0.52 0.40 0.39 Table 5: Correlation matrix for seen Nelson et al. (1998) cue-associate pairs and five distributional models. All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Nelson Concat CCA TopicAttr TextAttr Concat 0.11 CCA 0.15 0.66 TopicAttr 0.17 0.69 0.48 TextAttr 0.11 0.65 0.25 0.39 VisAttr 0.13 0.57 0.87 0.57 0.34 Table 6: Correlation matrix for unseen Nelson et al. (1998) cue-associate pairs and five distributional models. All correlation coefficients are statistically significant (p &lt; 0.01, N = 1,716). 7 Results Our experiments were designed to answer four questions: (1) Do visual attributes improve the performance of distr</context>
<context position="32240" citStr="Nelson et al., 1998" startWordPosition="5178" endWordPosition="5181">odels better suited to the integration of visual information? (3) How do computational models fare against gold standard norming data? (4) Does the attribute-based representation bring advantages over more conventional approaches based on raw image features? Our results are broken down into seen (Table 5) and unseen (Table 6) concepts. The former are known to the attribute classifiers and form part of our database, whereas the latter are unknown and are not included in McRae et al.’s (2005) norms. We report the correlation coefficients we obtain when human-derived cue-associate probabilities (Nelson et al., 1998) are compared against the simple concatenation model (Concat), CCA, and Andrews et al.’s (2009) attribute-topic model (TopicAttr). We also report the performance of a distributional model that is based solely on the output of our attribute classifiers, i.e., without any textual input (VisAttr) and conversely the performance of a model that uses textual information only (i.e., Strudel attributes) without any visual input (TextAttr). The results are displayed as a correlation matrix so that inter-model correlations can also be observed. As can be seen in Table 5 (second column), two modalities a</context>
<context position="35313" citStr="Nelson et al. (1998)" startWordPosition="5660" endWordPosition="5663">tributes present in the norms (All Attributes). The second model (Text Attributes) uses all attributes but those classified as visual (e.g., functional, encyclopaedic). The third model (Visual Attributes) considers solely visual attributes. We observe a similar trend as with our computational models. Taking visual attributes into account increases the fit with Nelson’s (1998) association norms, whereas visual and textual attributes on their own perform worse. Interestingly, CCA’s 579 Models Seen All Attributes 0.28 Text Attributes 0.20 Visual Attributes 0.25 Table 7: Model performance on seen Nelson et al. (1998) cue-associate pairs; models are based on gold human generated attributes (McRae et al., 2005). All correlation coefficients are statistically significant (p &lt; 0.01, N = 435). Models Seen Unseen Concat 0.22 0.10 CCA 0.26 0.15 TopicAttr 0.23 0.19 TextAttr 0.20 0.08 VisAttr 0.21 0.13 MixLDA 0.16 0.11 Table 8: Model performance on a subset of Nelson et al. (1998) cue-associate pairs. Seen are concepts known to the attribute classifiers and covered by MixLDA (N = 85). Unseen are concepts covered by LDA but unknown to the attribute classifiers (N = 388). All correlation coefficients are statistical</context>
<context position="36664" citStr="Nelson et al. (1998)" startWordPosition="5873" endWordPosition="5876">tic attributes (both textual and visual). Furthermore, visual attributes obtained through our classifiers (see Table 5) achieve a marginally lower correlation coefficient against human generated ones (see Table 7). Finally, to address our last question, we compared our approach against Feng and Lapata (2010) who represent visual information via quantized SIFT features. We trained their MixLDA model on their corpus consisting of 3,361 BBC news documents and corresponding images (Feng and Lapata, 2008). We optimized the model parameters on a development set consisting of cueassociate pairs from Nelson et al. (1998), excluding the concepts in McRae et al. (2005). We used a vocabulary of approximately 6,000 words. The best performing model on the development set used 500 visual terms and 750 topics and the association measure proposed in Griffiths et al. (2007). The test set consisted of 85 seen and 388 unseen cue-associate pairs that were covered by our models and MixLDA. Table 8 reports correlation coefficients for our models and MixLDA against human probabilities. All attribute-based models significantly outperform MixLDA on seen pairs (p &lt; 0.05 using a t-test). MixLDA performs on a par with the concat</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>D. L. Nelson, C. L. McEvoy, and T. A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ngiam</author>
<author>A Khosla</author>
<author>M Kim</author>
<author>J Nam</author>
<author>H Lee</author>
<author>A Y Ng</author>
</authors>
<title>Multimodal deep learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Leanring,</booktitle>
<pages>689--696</pages>
<location>Bellevue, Washington.</location>
<contexts>
<context position="7079" citStr="Ngiam et al., 2011" startWordPosition="1109" endWordPosition="1112">tegrated with textual data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms. 2 Related Work Grounding semantic representations with visual information is an instance of multimodal learning. In this setting the data consists of multiple input modalities with different representations and the learner’s objective is to extract a unified representation that fuses the modalities together. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Special-purpose models that address the fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meanin</context>
</contexts>
<marker>Ngiam, Khosla, Kim, Nam, Lee, Ng, 2011</marker>
<rawString>J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. 2011. Multimodal deep learning. In Proceedings of the 28th International Conference on Machine Leanring, pages 689–696, Bellevue, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oliva</author>
<author>A Torralba</author>
</authors>
<title>The Role of Context in Object Recognition.</title>
<date>2007</date>
<booktitle>Trends in Cognitive Sciences,</booktitle>
<volume>11</volume>
<issue>12</issue>
<contexts>
<context position="2488" citStr="Oliva and Torralba, 2007" startWordPosition="380" endWordPosition="383">ion for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs ha</context>
</contexts>
<marker>Oliva, Torralba, 2007</marker>
<rawString>A. Oliva and A. Torralba. 2007. The Role of Context in Object Recognition. Trends in Cognitive Sciences, 11(12):520–527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D N Osherson</author>
<author>J Stern</author>
<author>O Wilkie</author>
<author>M Stob</author>
<author>E E Smith</author>
</authors>
<title>Default Probability.</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<volume>2</volume>
<issue>15</issue>
<contexts>
<context position="9814" citStr="Osherson et al. (1991)" startWordPosition="1534" endWordPosition="1537">12,000 instances representing 20 objects from the PASCAL Visual Object Classes Challenge 2008 (Everingham et al., 2008). Visual semantic attributes (e.g., hairy, four-legged) were used to identify familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training examples of the target classes available. Their dataset contained over 30,000 images representing 50 animal concepts and used 85 attributes from the norming study of Osherson et al. (1991). Attribute-based representations have also been applied to the tasks of face detection (Kumar et al., 2009), action identification (Liu et al., 2011), and scene recognition (Patterson and Hays, 2012). The use of visual attributes in models of distributional semantics is novel to our knowledge. We argue that they are advantageous for two reasons. Firstly, they are cognitively plausible; humans employ visual attributes when describing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucia</context>
</contexts>
<marker>Osherson, Stern, Wilkie, Stob, Smith, 1991</marker>
<rawString>D. N. Osherson, J. Stern, O. Wilkie, M. Stob, and E. E. Smith. 1991. Default Probability. Cognitive Science, 2(15):251–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Patterson</author>
<author>J Hays</author>
</authors>
<title>SUN Attribute Database: Discovering, Annotating and Recognizing Scene Attributes.</title>
<date>2012</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>2751--2758</pages>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="10014" citStr="Patterson and Hays, 2012" startWordPosition="1564" endWordPosition="1567">familiar objects and to describe unfamiliar objects when new images and bounding box annotations were provided. Lampert et al. (2009) showed that attribute-based representations can be used to classify objects when there are no training examples of the target classes available. Their dataset contained over 30,000 images representing 50 animal concepts and used 85 attributes from the norming study of Osherson et al. (1991). Attribute-based representations have also been applied to the tasks of face detection (Kumar et al., 2009), action identification (Liu et al., 2011), and scene recognition (Patterson and Hays, 2012). The use of visual attributes in models of distributional semantics is novel to our knowledge. We argue that they are advantageous for two reasons. Firstly, they are cognitively plausible; humans employ visual attributes when describing the properties of concept classes. Secondly, they occupy the middle ground between non-linguistic low-level image features and linguistic words. Attributes crucially represent image properties, however by being words themselves, they can be easily integrated in any text-based distributional model thus eschewing known difficulties with rendering images into wor</context>
</contexts>
<marker>Patterson, Hays, 2012</marker>
<rawString>G. Patterson and J. Hays. 2012. SUN Attribute Database: Discovering, Annotating and Recognizing Scene Attributes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2751–2758, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Regier</author>
</authors>
<title>The Human Semantic Potential.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4425" citStr="Regier, 1996" startWordPosition="686" endWordPosition="687">inguistics, pages 572–582, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cept, which limits elicitation studies to a small number of concepts and the scope of any computational model based on feature norms. Another strand of research focuses exclusively on the visual modality, even though the grounding problem could involve auditory, motor, and haptic modalities as well. This is not entirely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focus</context>
</contexts>
<marker>Regier, 1996</marker>
<rawString>T. Regier. 1996. The Human Semantic Potential. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>A Pentland</author>
</authors>
<title>Learning Words from Sights and Sounds: A Computational Model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1494" citStr="Roy and Pentland, 2002" startWordPosition="222" endWordPosition="226"> years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>D. Roy and A. Pentland. 2002. Learning Words from Sights and Sounds: A Computational Model. Cognitive Science, 26(1):113–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Silberer</author>
<author>M Lapata</author>
</authors>
<title>Grounded Models of Semantic Representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1733" citStr="Silberer and Lapata, 2012" startWordPosition="264" endWordPosition="267"> in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et </context>
<context position="22775" citStr="Silberer and Lapata, 2012" startWordPosition="3660" endWordPosition="3663">lot recall against precision based on threshold δ.5 Table 4 shows the 10 nearest neighbors for five example concepts from our dataset. Again, we measure the cosine similarity between a concept and all other concepts in the dataset when these are represented by their visual attribute vector pw. 5 Attribute-based Semantic Models We evaluated the effectiveness of our attribute classifiers by integrating their predictions with traditional text-only models of semantic representation. These models have been previously proposed in the literature and were also described in a recent comparative study (Silberer and Lapata, 2012). We represent the visual modality by attribute vectors computed as shown in Equation (1). The linguistic environment is approximated by textual attributes. We used Strudel (Baroni et al., 2010) to obtain these attributes for the nouns in our dataset. Given a list of target words, Strudel extracts weighted word-attribute pairs from a lemmatized and pos-tagged text corpus (e.g., eggplant–cook-v, eggplant–vegetable-n). The weight of each word-attribute pair is a log-likelihood ratio score expressing the pair’s strength of association. In our experiments we learned word-attribute pairs from a lem</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>C. Silberer and M. Lapata. 2012. Grounded Models of Semantic Representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Siskind</author>
</authors>
<title>Grounding the Lexical Semantics of Verbs in Visual Perception using Force Dynamics and Event Logic.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>15--31</pages>
<contexts>
<context position="1470" citStr="Siskind, 2001" startWordPosition="220" endWordPosition="221">oduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an impo</context>
</contexts>
<marker>Siskind, 2001</marker>
<rawString>J. M. Siskind. 2001. Grounding the Lexical Semantics of Verbs in Visual Perception using Force Dynamics and Event Logic. Journal of Artificial Intelligence Research, 15:31–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Sloman</author>
<author>L J Ripps</author>
</authors>
<title>Similarity as an Explanatory Construct.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>65--87</pages>
<contexts>
<context position="3368" citStr="Sloman and Ripps, 1998" startWordPosition="522" endWordPosition="525">ms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Feature norms are instrumental in revealing which dimensions of meaning are psychologically salient, however, their use as a proxy for people’s perceptual representations can itself be problematic (Sloman and Ripps, 1998; Zeigenfuse and Lee, 2010). The number and types of attributes generated can vary substantially as a function of the amount of time devoted to each concept. It is not entirely clear how people generate attributes and whether all of these are important for representing concepts. Finally, multiple participants are required to create a representation for each con572 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cept, which limits elicitation studies to a sm</context>
</contexts>
<marker>Sloman, Ripps, 1998</marker>
<rawString>S. A. Sloman and L. J. Ripps. 1998. Similarity as an Explanatory Construct. Cognition, 65:87–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Srivastava</author>
<author>R Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep boltzmann machines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>2231--2239</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="7116" citStr="Srivastava and Salakhutdinov, 2012" startWordPosition="1113" endWordPosition="1116">l data to create distributional models that give a better fit to human word association data over models that rely on human generated feature norms. 2 Related Work Grounding semantic representations with visual information is an instance of multimodal learning. In this setting the data consists of multiple input modalities with different representations and the learner’s objective is to extract a unified representation that fuses the modalities together. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Special-purpose models that address the fusion of distributional meaning with visual information have been also proposed. Feng and Lapata (2010) represent documents and images by a common multimodal vocabulary consisting of textual words and visual terms which they obtain by quantizing SIFT descriptors (Lowe, 2004). Their model is essentially Latent Dirichlet Allocation (LDA, Blei et al., 2003) trained on a corpus of multimodal documents (i.e., BBC news articles and their associated images). Meaning in this model is represented as a v</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2012</marker>
<rawString>N. Srivastava and R. Salakhutdinov. 2012. Multimodal learning with deep boltzmann machines. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 2231–2239, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steyvers</author>
</authors>
<title>Combining feature norms and text data with topic models.</title>
<date>2010</date>
<journal>Acta Psychologica,</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="2705" citStr="Steyvers, 2010" startWordPosition="416" endWordPosition="417"> we process language (Barsalou, 2008; Bornstein et al., 2004; Landau et al., 1998). From an engineering perspective, the ability to learn representations for multimodal data has many practical applications including image retrieval (Datta et al., 2008) and annotation (Chai and Hung, 2008), text illustration (Joshi et al., 2006), object and scene recognition (Lowe, 1999; Oliva and Torralba, 2007; Fei-Fei and Perona, 2005), and robot navigation (Tellex et al., 2011). One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Feature norms are instrumental in revealing which dimensions of meaning are psychologically salient, however, their use as a proxy for people’s perceptual re</context>
</contexts>
<marker>Steyvers, 2010</marker>
<rawString>M. Steyvers. 2010. Combining feature norms and text data with topic models. Acta Psychologica, 133(3):234–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>A Gopal Banerjee</author>
<author>S Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding Natural Language Commands for Robotic Navigation and Manipulation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th National Conference on Artificial Intelligence,</booktitle>
<pages>1507--1514</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="1379" citStr="Tellex et al., 2011" startWordPosition="204" endWordPosition="207"> data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting </context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. Gopal Banerjee, S. Teller, and N. Roy. 2011. Understanding Natural Language Commands for Robotic Navigation and Manipulation. In Proceedings of the 25th National Conference on Artificial Intelligence, pages 1507–1514, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
<author>L Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceeings of the Human Factors in Computing Systems Conference,</booktitle>
<pages>319--326</pages>
<location>Vienna, Austria.</location>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>L. von Ahn and L. Dabbish. 2004. Labeling images with a computer game. In Proceeings of the Human Factors in Computing Systems Conference, pages 319–326, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>D H Ballard</author>
</authors>
<title>A Unified Model of Early Word Learning Integrating Statistical and Social Cues. Neurocomputing,</title>
<date>2007</date>
<pages>70--2149</pages>
<contexts>
<context position="1540" citStr="Yu and Ballard, 2007" startWordPosition="231" endWordPosition="235"> language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded distributional models. The motivation for models that do not learn exclusively from text is twofold. From a cognitive perspective, there is mounting experimental evidence suggesting that our interaction with the physical world plays an important role in the way we process language (Barsalou, 2008; Bornstein e</context>
</contexts>
<marker>Yu, Ballard, 2007</marker>
<rawString>C. Yu and D. H. Ballard. 2007. A Unified Model of Early Word Learning Integrating Statistical and Social Cues. Neurocomputing, 70:2149–2165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Zeigenfuse</author>
<author>M D Lee</author>
</authors>
<title>Finding the Features that Represent Stimuli.</title>
<date>2010</date>
<journal>Acta Psychological,</journal>
<volume>133</volume>
<issue>3</issue>
<contexts>
<context position="3395" citStr="Zeigenfuse and Lee, 2010" startWordPosition="526" endWordPosition="529">g native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Feature norms are instrumental in revealing which dimensions of meaning are psychologically salient, however, their use as a proxy for people’s perceptual representations can itself be problematic (Sloman and Ripps, 1998; Zeigenfuse and Lee, 2010). The number and types of attributes generated can vary substantially as a function of the amount of time devoted to each concept. It is not entirely clear how people generate attributes and whether all of these are important for representing concepts. Finally, multiple participants are required to create a representation for each con572 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cept, which limits elicitation studies to a small number of concepts and </context>
</contexts>
<marker>Zeigenfuse, Lee, 2010</marker>
<rawString>M. D. Zeigenfuse and M. D. Lee. 2010. Finding the Features that Represent Stimuli. Acta Psychological, 133(3):283–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to Parse Database Queries Using Inductive Logic Programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1174" citStr="Zelle and Mooney, 1996" startWordPosition="172" endWordPosition="175">his dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1996. Learning to Parse Database Queries Using Inductive Logic Programming. In Proceedings of the 13th National Conference on Artificial Intelligence, pages 1050–1055, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="1205" citStr="Zettlemoyer and Collins, 2005" startWordPosition="176" endWordPosition="179">ribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data. 1 Introduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructi</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pages 658–666, Edinburgh, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>