<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999518">
Machine Translation with Inferred
Stochastic Finite-State Transducers
</title>
<author confidence="0.997371">
Francisco Casacuberta∗ Enrique Vidal*
</author>
<bodyText confidence="0.9650375">
Universidad Polit´ecnica de Valencia Universidad Polit´ecnica de Valencia
Finite-state transducers are models that are being used in different areas ofpattern recognition and
computational linguistics. One of these areas is machine translation, in which the approaches that
are based on building models automatically from training examples are becoming more and more
attractive. Finite-state transducers are very adequatefor use in constrained tasks in which training
samples of pairs of sentences are available. A technique for inferring finite-state transducers is
proposed in this article. This technique is based onformal relations betweenfinite-state transducers
and rational grammars. Given a training corpus of source-target pairs of sentences, the proposed
approach uses statistical alignment methods to produce a set of conventional strings from which
a stochastic rational grammar (e.g., an n-gram) is inferred. This grammar is finally converted
into a finite-state transducer. The proposed methods are assessed through a series of machine
translation experiments within the framework of the EuTRANS project.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999925772727273">
Formal transducers give rise to an important framework in syntactic-pattern recogni-
tion (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri
1997). Many tasks in automatic speech recognition can be viewed as simple translations
from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decod-
ing) or from acoustic or lexical sequences to query strings (for database access) or
(robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995;
Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001;
Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001).
Another similar application is the recognition of continuous hand-written char-
acters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers
is language translation, in which input and output can be text, speech, (continuous)
handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001;
Amengual et al. 2000).
Rational transductions (Berstel 1979) constitute an important class within the field
of formal translation. These transductions are realized by the so-called finite-state
transducers. Even though other, more powerful transduction models exist, finite-state
transducers generally entail much more affordable computational costs, thereby mak-
ing these simpler models more interesting in practice.
One of the main reasons for the interest in finite-state machines for language
translation comes from the fact that these machines can be learned automatically from
examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques
exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina,
</bodyText>
<note confidence="0.619574333333333">
* Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071
Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es.
© 2004 Association for Computational Linguistics
Computational Linguistics Volume 30, Number 2
Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and
Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques
</note>
<bodyText confidence="0.999460933333333">
for inferring regular grammars from finite sets of learning strings which have been
used successfully in a number of fields, including automatic speech recognition (Vi-
dal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from
formal language theory. In particular, complex regular grammars can be built by infer-
ring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta
1987).
Here we explore this idea further and propose methods that use (simple) finite-
state grammar learning techniques, such as n-gram modeling, to infer rational trans-
ducers which prove adequate for language translation.
The organization of the article is as follows. Sections 2 and 3 give the basic defini-
tions of a finite-state transducer and the corresponding stochastic extension, presented
within the statistical framework of language translation. In Section 4, the proposed
method for inferring stochastic finite-state transducers is presented. The experiments
are described in Section 5. Finally, Section 6 is devoted to general discussion and
conclusions.
</bodyText>
<sectionHeader confidence="0.999034" genericHeader="keywords">
2. Finite-State Transducers
</sectionHeader>
<bodyText confidence="0.9981362">
A finite-state transducer, T, is a tuple (Σ, ∆, Q, q0, F, δ), in which Σ is a finite set of
source symbols, ∆ is a finite set of target symbols (Σ n ∆ = 0), Q is a finite set of
states, q0 is the initial state, F C Q is a set of final states, and δ C Q x Σ x ∆* x Q is
a set of transitions.1 A translation form φ of length I in T is defined as a sequence of
transitions:
</bodyText>
<equation confidence="0.99746">
φ = (qφ0 ,sφ 1 ,¯tφ 1 ,qφ 1 )(qφ 1 ,sφ2 ,¯t2φ,qφ2 )(qφ2 ,sφ3 ,¯tφ3 ,qφ3 )...(qφI− 1,SO ,io ,q0)
,sI,tI,q0) (1)
</equation>
<bodyText confidence="0.9999713">
where (qφi−1,sφi ,¯tφ i , qφi ) E δ, qφ0 = q0, and qφI E F. A pair (s, t) E Σ* x ∆* is a translation
pair if there is a translation form φ of length I in T such that I =|s |and t = ¯tφ1¯tφ 2 . . .¯tφI .
By d(s, t) we will denote the set of translation forms2 in T associated with the pair (s, t).
A rational translation is the set of all translation pairs of some finite-state transducer T.
This definition of a finite-state transducer is similar to the definition of a regular
or finite-state grammar 9. The main difference is that in a finite-state grammar, the set
of target symbols ∆ does not exist, and the transitions are defined on Q x Σ x Q. A
translation form is the transducer counterpart of a derivation in a finite-state gram-
mar, and the concept of rational translation is reminiscent of the concept of (regular)
language, defined as the set of strings associated with the derivations in the grammar
</bodyText>
<sectionHeader confidence="0.486839" genericHeader="introduction">
9.
</sectionHeader>
<bodyText confidence="0.9191715">
Rational translations exhibit many properties similar to those shown for regular
languages (Berstel 1979). One of these properties can be stated as follows (Berstel 1979):
</bodyText>
<figure confidence="0.53829575">
Theorem 1
T C_ Σ* x ∆* is a rational translation if and only if there exist an alphabet Γ, a regular language
L C Γ*, and two morphisms hΣ : Γ* → Σ* and ho : Γ* → ∆*, such that T = {(hΣ(w), ho(w)) |
w E L�.
</figure>
<footnote confidence="0.91736">
1 By 0∗ and E∗, we denote the set of finite-length strings on 0 and E, respectively.
2 To simplify the notation, we will remove the superscript φ from the components of a translation form
if no confusion is induced.
</footnote>
<page confidence="0.992299">
206
</page>
<note confidence="0.836531">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<bodyText confidence="0.9493585">
As will be discussed later, this theorem directly suggests the transducer inference
methods proposed in this article.
</bodyText>
<sectionHeader confidence="0.946297" genericHeader="method">
3. Statistical Translation Using Finite-State Transducers
</sectionHeader>
<bodyText confidence="0.997463">
In the statistical translation framework, the translation of a given source string s in
E* is a string tˆ E ∆*, such that3
</bodyText>
<equation confidence="0.963608">
tˆ = argmax Pr(t  |s) = argmax Pr(s, t) (2)
tE∆* tE∆*
</equation>
<bodyText confidence="0.94774825">
Pr(s,t) can be modeled by the stochastic extension of a finite-state transducer. A
stochastic finite-state transducer, TP, is defined as a tuple (E, ∆, Q, q0, p,f), in which
Q, q0, Q, E, and ∆ are as in the definition of a finite-state transducer and p and f are
two functions p : Q x E x ∆* x Q → [0, 1] and f : Q → [0, 1] that satisfy, dq E Q,
</bodyText>
<equation confidence="0.9874005">
f(q) + E p(q,a, ω, q&apos;) = 1
(a,w,q�)EEx∆*xQ
</equation>
<bodyText confidence="0.999371">
In this context, T will denote the natural finite-state transducer associated with a
stochastic finite-state transducer TP (characteristic finite-state transducer). The set of
transitions of T is the set of tuples (q, s, t, q&apos;) in TP with probabilities greater than zero,
and the set of final states is the set of states with nonzero final-state probabilities.
The probability of a translation pair (s,t) E E* x ∆* according to TP is the sum
of the probabilities of all the translation forms of (s, t) in T:
</bodyText>
<equation confidence="0.991472">
PTP(s,t) = E PTP(0)
¢Ed(s,t)
</equation>
<bodyText confidence="0.994679">
where the probability of a translation form 0 (as defined in equation (1)) is
</bodyText>
<equation confidence="0.978196">
I
PTP(0) = ri p(qi−1, si,¯ti, qi) · f (qI) (3)
i=0
</equation>
<bodyText confidence="0.9610734">
that is, the product of the probabilities of all the transitions involved in 0.
We are interested only in transducers without useless states, that is, those in which
for every state in T, there is a path leading to a final state. If we further assume that
PTP(s, t) is zero when no translation form exists for (s, t) in T, it can be easily verified
that
</bodyText>
<equation confidence="0.9347825">
PTP(s, t) = 1
(s,t)EE*x∆*
</equation>
<bodyText confidence="0.978008">
That is, PTP is a joint distribution on E* x ∆* which will be called the stochastic
translation defined by TP. 4
Finally, the translation of a source string s E E* by a stochastic finite-state trans-
ducer TP is
</bodyText>
<equation confidence="0.644948">
tˆ = argmax PTP(s, t) (4)
tE∆*
</equation>
<footnote confidence="0.95270575">
3 For the sake of simplicity, we will denote Pr(X = x) as Pr(x) and Pr(X = x I Y = y) as Pr(x I y).
4 This concept is similar to the stochastic regular language for a stochastic regular grammar. In that
case, the probability distribution is defined on the set of finite-length strings rather than on the set of
pairs of strings.
</footnote>
<page confidence="0.978966">
207
</page>
<note confidence="0.722473">
Computational Linguistics Volume 30, Number 2
</note>
<bodyText confidence="0.986219">
A stochastic finite-state transducer has stochastic source and target regular languages
embedded (Pi and Po, respectively.):
</bodyText>
<equation confidence="0.992132">
1] Pi(s) = PTP(s, t), Po(t) = 1] PTP(s, t)
t∈∆* s∈Σ*
</equation>
<bodyText confidence="0.9993204">
In practice, these source or target regular languages are obtained, by dropping the
target or the source symbols, respectively, from each transition of the finite-state trans-
ducer.
The following theorem naturally extends Theorem 1 to the stochastic framework
(Casacuberta, Vidal, and Pic´o 2004):
</bodyText>
<subsectionHeader confidence="0.344524">
Theorem 2
</subsectionHeader>
<bodyText confidence="0.493816666666667">
A distribution PT : E* x ∆* → [0, 1] is a stochastic rational translation if and only if there
exist an alphabet P, two morphisms hΣ : P* → E* and h∆ : P* → ∆*, and a stochastic regular
language PL such that, d(s, t) E E* x ∆*,
</bodyText>
<equation confidence="0.987237">
PT(s,t) = 1] PL(W) (5)
ω ∈ r* :
(hΣ(ω), h∆(ω)) = (s, t)
</equation>
<subsectionHeader confidence="0.999915">
3.1 Search with Stochastic Finite-State Transducers
</subsectionHeader>
<bodyText confidence="0.9993915">
The search for an optimal ˆt in Equation (4) has proved to be a difficult computational
problem (Casacuberta and de la Higuera 2000). In practice, an approximate solution
can be obtained (Casacuberta 2000) on the basis of the following approximation to the
probability of a translation pair (Viterbi score of a translation):
</bodyText>
<equation confidence="0.9100802">
PTP(s, t) ;: VTP(s, t) = max PTP(φ) (6)
¢∈d(s,t)
An approximate translation can now be computed as
˜t = argmax VTP(s, t) = argmax max PTP(φ) (7)
t∈∆* t∈∆* ¢∈d(s,t)
</equation>
<bodyText confidence="0.9997495">
This computation can be carried out efficiently (Casacuberta 1996) by solving the
following recurrence by means of dynamic programming:
</bodyText>
<equation confidence="0.999881">
max VTP(s,t) = max (V(jsj,q) - f(q)) (8)
t∈∆∗ q∈Q
(V(i − 1,q~) - p(q�,si,w,q)) if i =� 0,q =� q0 (9)
V(i,q) = max
q/∈Q,w∈∆*
V(0,q0) = 1 (10)
</equation>
<bodyText confidence="0.9886355">
Finally, the approximate translation ˜t is obtained as the concatenation of the target
strings associated with the translation form
</bodyText>
<equation confidence="0.931011">
φ˜ = (q0, s1,¯t1, q1)(q1, s2,¯t2, q2) ... (qI−1, sI−1,¯tI, qI),
</equation>
<bodyText confidence="0.96834">
corresponding to the optimal sequence of states involved in the solution to Equa-
tion (8); that is,
</bodyText>
<equation confidence="0.975684">
˜t = t1t2 ... tI
</equation>
<page confidence="0.990945">
208
</page>
<figure confidence="0.986602466666667">
Casacuberta and Vidal Translation with Finite-State Transducers
una / a (1.0)
0 1
2
doppia / room (1.0)
7
3
5
camera / double (0.3)
doppia / double room (1.0)
doppia / with two beds (1.0)
6
camera / room (0.4)
camera / λ (0.3)
4
</figure>
<figureCaption confidence="0.994338">
Figure 1
</figureCaption>
<bodyText confidence="0.9905545">
Example of Viterbi score-based suboptimal result. The probability PTP of the pair una camera
doppia/a double room is (1.0 · 0.3 · 1.0) + (1.0 · 0.3 · 1.0) = 0.6. This is greater than the probability
PTP of the pair una camera doppia/a room with two beds, 1.0 · 0.4 · 1.0 = 0.4. However, the Viterbi
score VTP for the first pair is 1.0 · 0.3 · 1.0 = 0.3, which is lower than the Viterbi score VTP for
the second pair, 1.0 · 0.4 · 1.0 = 0.4. Therefore this second pair will be the approximate result
given by equation (7).
The computational cost of the iterative version of this algorithm is O( |s  |· |Q |· B),
where B is the (average) branching factor of the finite-state transducer.
Figure 1 shows a simple example in which Viterbi score maximization (7) leads to
a suboptimal result.
</bodyText>
<sectionHeader confidence="0.938315" genericHeader="method">
4. A Method for Inferring Finite-State Transducers
</sectionHeader>
<bodyText confidence="0.999966125">
Theorems 1 and 2 establish that any (stochastic) rational translation T can be obtained
as a homomorphic image of certain (stochastic) regular language L over an adequate
alphabet Γ. The proofs of these theorems are constructive (Berstel 1979; Casacuberta,
Vidal, and Pico´ 2004) and are based on building a (stochastic) finite-state transducer T
for T by applying certain morphisms hr and ho to the symbols of Γ that are associated
with the rules of a (stochastic) regular grammar that generates L.
This suggests the following general technique for learning a stochastic finite-state
transducer, given a finite sample A of string pairs (s, t) E Σ* x ∆* ( a parallel corpus):
</bodyText>
<listItem confidence="0.997988166666667">
1. Each training pair (s, t) from A is transformed into a string z from an
extended alphabet Γ (strings of Γ-symbols) yielding a sample S of
strings S c Γ�.
2. A (stochastic) regular grammar 9 is inferred from S.
3. The Γ-symbols of the grammar rules are transformed back into pairs of
source/target symbols/strings (from Σ* x ∆*).
</listItem>
<bodyText confidence="0.990448153846154">
This technique, which is very similar to that proposed in Garc´ıa, Vidal, and Casacu-
berta (1987) for the inference of regular grammars, is illustrated in Figure 2.
The first transformation is modeled by the labeling function G : Σ* x ∆* → Γ*,
while the last transformation is carried out by an “inverse labeling function” Λ(·), that
is, one such that Λ(G(A)) = A. Following Theorems 1 and 2, Λ(·) consists of a couple
of morphisms, hr, ho, such that for a string z E Γ*, Λ(z) = (hr(z), ho(z)).
Without loss of generality, we assume that the method used in the second step of
the proposed method consists of the inference of n-grams (Ney, Martin, and Wessel
1997) with final states, which are particular cases of stochastic regular grammars. This
simple method automatically derives, from the strings in S, both the structure of 9
(i.e., the rules—states and transitions) and the associated probabilities.
Since Λ is typically the inverse of G, the morphisms hr and ho needed in the
third step of the proposed approach are determined by the definition of G. So a key
</bodyText>
<page confidence="0.993574">
209
</page>
<figure confidence="0.890907">
Computational Linguistics Volume 30, Number 2
</figure>
<figureCaption confidence="0.956547">
Figure 2
</figureCaption>
<bodyText confidence="0.921341913043478">
Basic scheme for the inference of finite-state transducers. A is a finite sample of training pairs.
S is the finite sample of strings obtained from A using L. 9 is a grammar inferred from S such
that S is a subset of the language, L(9), generated by the grammar 9. T is a finite-state
transducer whose translation (T(T )) includes the training sample A.
point in this approach is its first step, that is, how to conveniently transform a parallel
corpus into a string corpus. In general, there are many possible transformations, but
if the source–target correspondences are complex, the design of an adequate transfor-
mation can become difficult. As a general rule, the labeling process must capture these
source–target word correspondences and must allow for a simple implementation of
the inverse labeling needed in the third step.
A very preliminary, nonstochastic version of this finite-state transducer inference
technique was presented in Vidal, Garc´ıa, and Segarra (1989) An important drawback
of that early proposal was that the methods proposed for building the P* sentences
from the training pairs did not adequately cope with the dependencies between the
words of the source sentences and the words of the corresponding target sentences. In
the following section we show how this drawback can be overcome using statistical
alignments (Brown et al. 1993).
The resulting methodology is called grammatical inference and alignments for
transducer inference (GIATI).5 A related approach was proposed in Bangalore and
Ricardi (2000b). In that case, the extended symbols were also built according to pre-
viously computed alignments, but the order of target words was not preserved. As a
consequence, that approach requires a postprocess to try to restore the target words
to a proper order.
</bodyText>
<subsectionHeader confidence="0.999049">
4.1 Statistical Alignments
</subsectionHeader>
<bodyText confidence="0.994740375">
The statistical translation models introduced by Brown et al. (1993) are based on the
concept of alignment between source and target words (statistical alignment mod-
els). Formally, an alignment of a translation pair (s, t) ∈ E* × ∆* is a function
a : {1, ... , |t|} → {0,...,  |s |}. The particular case a(j) = 0 means that the position j
in t is not aligned with any position in s. All the possible alignments between t and
s are denoted by A(s, t), and the probability of translating a given s into t by an
alignment a is Pr(t, a  |s).
Thus, an optimal alignment between s and t can be computed as
</bodyText>
<equation confidence="0.9842305">
aˆ = argmax Pr(t, a  |s) (11)
a∈A(s,t)
</equation>
<footnote confidence="0.646473">
5 In previous work, this idea was often called morphic generator transducer inference.
</footnote>
<page confidence="0.995046">
210
</page>
<note confidence="0.345633">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<bodyText confidence="0.9989282">
Different approaches for estimating Pr(t, a I s) were proposed in Brown et al. (1993).
These approaches are known as models 1 through 5. Adequate software packages are
publicly available for training these statistical models and for obtaining good align-
ments between pairs of sentences (Al-Onaizan et al. 1999; Och and Ney 2000). An
example of Spanish-English sentence alignment is given below:
</bodyText>
<subsectionHeader confidence="0.753173">
Example 1
</subsectionHeader>
<bodyText confidence="0.9813578">
¿ Cu´anto cuesta una habitaci´on individual por semana ?
how (2) much (2) does (3) a (4) single (6) room (5) cost (3) per (7) week (8) ? (9)
Each number within parentheses in the example represents the position in the source
sentence that is aligned with the (position of the) preceding target word. A graphical
representation of this alignment is shown in Figure 3.
</bodyText>
<subsectionHeader confidence="0.990597">
4.2 First Step of the GIATI Methodology: Transformation of Training Pairs into
Strings
</subsectionHeader>
<bodyText confidence="0.95858575">
The first step of the proposed method consists in a labeling process (G) that builds a
string of certain extended symbols from each training string pair and its corresponding
statistical alignment. The main idea is to assign each word from t to the corresponding
word from s given by the alignment a. But sometimes this assignment produces a
violation of the sequential order of the words in t. To illustrate the GIATI methodology
we will use example 2:
Figure 3
Graphical representation of the alignment between a source (Spanish) sentence (¿ Cu´anto cuesta
una habitaci´on individual por semana ?) and a target (English) sentence (How much does a single
room cost per week ?). Note the correspondence between the Spanish cuesta and the English does
and cost. Note also that the model does not allow for alignments between sets of two or more
source words and one target word.
</bodyText>
<page confidence="0.98956">
211
</page>
<figure confidence="0.970691666666667">
Computational Linguistics Volume 30, Number 2
Example 2
Let A be a training sample composed by the following pairs (Italian/English):
una camera doppia # a double room
una camera # a room
la camera singola # the single room
la camera # the room
Suitable alignments for these pairs are
una camera doppia # a (1) double (3) room (2)
una camera # a (1) room (2)
la camera singola # the (1) single (3) room (2)
la camera # the (1) room (2)
</figure>
<bodyText confidence="0.999856333333333">
In the first pair of this example, the English word double could be assigned to
the third Italian word (doppia) and the English word room to the second Italian word
(camera). This would imply a “reordering” of the words double and room, which is not
appropriate in our finite-state framework.
Given s, t, and a (source and target strings and associated alignment, respectively),
the proposed transformation z = L1(s, t) avoids this problem as follows:
</bodyText>
<equation confidence="0.9949165">
|z |= |s|
1 &lt; i &lt;  |z |
</equation>
<bodyText confidence="0.980202545454545">
(si , tj tj+1 ... tj+l) if Elj : a(j) = i and El |j� &lt; j : a(j&apos;) &gt; a(j)
and for j&amp;quot; : j &lt; j&amp;quot; &lt; j + l, a(j&amp;quot;) &lt; a(j)
(si , A) otherwise
Each word from t is joined with the corresponding word from s given by the alignment
a if the target word order is not violated. Otherwise, the target word is joined with
the first source word that does not violate the target word order.
The application of L1 to example 2 generates the following strings of extended
symbols:
(una , a) (camera, A) (doppia , double room)
(una , a) (camera, room)
(la , the) (camera, A) (singola , single room)
(la , the) (camera, room)
As a more complicated example, the application of this transformation to example 1
generates the following string:
(¿ , A) (Cu´anto , how much) (cuesta , does) (una , a) (habitaci´on , A)
(individual , single room cost) (por , per) (semana , week) (? , ?)
In this case the unaligned token ? has an associated empty target string, and the
target word cost, which is aligned with the source word cuesta, is associated with the
nearby source word individual. This avoids a “reordering” of the target string and
entails an (apparently) lower degree of nonmonotonicity. This is achieved, however,
at the expense of letting the method generalize from word associations which can be
considered improper from a linguistic point of view (e.g., (cuesta, does), (individual, single
</bodyText>
<equation confidence="0.573137">
zi = I
</equation>
<page confidence="0.953409">
212
</page>
<bodyText confidence="0.931965625">
Casacuberta and Vidal Translation with Finite-State Transducers
room cost)). While this would certainly be problematic for general language translation,
it proves not to be so harmful when the sentences to be translated come from limited-
domain languages.
Obviously, other transformations are possible. For example, after the application of
the above procedure, successive isolated source words (without any target word) can
be joined to the first extended word which has target word(s) assigned. Let z = L1(s, t)
be a transformed string obtained from the above procedure and let
</bodyText>
<equation confidence="0.869257">
(sk−1 , tj tj+1 ... tj+m)(sk , λ) ... (sk+l−1 , λ)(sk+l , tj+m+1 ... tj+n)
be a subsequence within z. Then the subsequence
(sk , λ) ... (sk+l−1 , λ)(sk+l , tj+m+1 ... tj+n)
is transformed by L2 into
(sk ... sk+l−1 sk+l , tj+m+1 ... tj+n)
</equation>
<bodyText confidence="0.771977285714286">
The application of L2 to example 2 leads to
(una , a) (camera doppia , double room)
(una , a) (camera, room)
(la , the) (camera singola , single room)
(la , the) (camera, room)
Although many other sophisticated transformations can be defined following the
above ideas, only the simple L1 will be used in the experiments reported in this article.
</bodyText>
<subsectionHeader confidence="0.9989325">
4.3 Second Step of the GIATI Methodology: Inferring a Stochastic Regular Grammar
from a Set of Strings
</subsectionHeader>
<bodyText confidence="0.9998639">
Many grammatical inference techniques are available to implement the second step
of the proposed procedure. In this work, (smoothed) n-grams are used. These models
have proven quite successful in many areas such as language modeling (Clarkson and
Rosenfeld 1997; Ney, Martin, and Wessel 1997).
Figures 4 and 5 show the (nonsmoothed) bigram models inferred from the sample
obtained using L1 and L2, respectively, in example 2. Note that the generalization
achieved by the first model is greater than that of the second.
The probabilities of the n-grams are computed from the corresponding counts in
the training set of extended strings. The probability of an extended word zj = (si,¯ti)
given the sequence of extended words zi−n+1, . . . , zi−1 = (si−n+1,¯ti−n+1) . . . (si−1,¯ti−1)
</bodyText>
<figureCaption confidence="0.743926">
Figure 4
</figureCaption>
<figure confidence="0.905862733333333">
Bigram model inferred from strings obtained by the transformation L1 in example 2.
(doppia , double room)
3
(camera , λ)
2
(una , a)
1
6
(camera , room)
0
(la , the)
4
(camera , λ)
(singola , single room)
(camera , room)
</figure>
<page confidence="0.7428885">
5
213
</page>
<figure confidence="0.922749">
Computational Linguistics Volume 30, Number 2
</figure>
<figureCaption confidence="0.927406">
Figure 5
</figureCaption>
<bodyText confidence="0.69338">
Bigram model inferred from strings obtained by the transformation L2 in example 2.
is estimated as
</bodyText>
<equation confidence="0.958684">
c(zi−n+1, . . . , zi−1, zi)
pn(zi  |zi−n+1 ... zi−1) = (12)
c(zi−n+1, . . . , zi−1)
</equation>
<bodyText confidence="0.99996925">
where c(·) is the number of times that an event occurs in the training set. To deal with
unseen n-grams, the back-off smoothing technique from the CMU Statistical Language
Modeling (SLM) Toolkit (Rosenfeld 1995) has been used.
The (smoothed) n-gram model obtained from the set of extended symbols is repre-
sented as a stochastic finite-state automaton (Llorens, Vilar, and Casacuberta 2002). The
states of the automaton are the observed (n − 1)-grams. For the n-gram (zi−n+1 ... zi),
there is a transition from state (zi−n+1 ... zi−1) to state (zi−n+2 ... zi) with the associ-
ated extended word zi and a probability pn(zi  |zi−n+1 ... zi−1). The back-off smoothing
method supplied by the SLM Toolkit is represented by the states corresponding to k-
grams (k &lt; n) and by special transitions between k-gram states and (k − 1)-gram
states (Llorens, Vilar, and Casacuberta 2002). The final-state probability is computed
as the probability of a transition with an end-of-sentence mark.
</bodyText>
<subsectionHeader confidence="0.9653585">
4.4 Third Step of the GIATI Methodology: Transforming a Stochastic Regular Gram-
mar into a Stochastic Finite-State Transducer
</subsectionHeader>
<bodyText confidence="0.623998285714286">
In order to obtain a finite-state transducer from a grammar of G1–transformed symbols,
an “inverse transformation” A(·) is used which is based on two simple morphisms:
if (a, b1b2 ... bk) E r with a E E and b1, b2,. . . , bk E O,
hr((a, b1b2 ... bk)) = a
ho((a, b1b2 ... bk)) = b1 b2 ... bk
It can be verified that this constitutes a true inverse transformation; that is, for every
training pair d(s, t) E A
</bodyText>
<equation confidence="0.986428">
s = hr(G1(s,t)), t = ho(G1(s,t))
</equation>
<bodyText confidence="0.995692142857143">
If zi is a transition of the inferred regular grammar, where zi = (a,b1b2 ... bk) E r, the
corresponding transition of the resulting finite-state transducer is (q, a,b1b2 · · · bk, q&apos;).
This construction is illustrated in Figures 6 and 7 for the bigrams of Figures 4 and 5,
respectively. Note that in the second case, this construction entails the trivial addition of
a few states which did not exist in the corresponding bigram. As previously discussed,
the first transformation (G1) definitely leads to a greater translation generalization than
the second (G2) (Casacuberta, Vidal, and Pic´o 2004). The probabilities associated with
</bodyText>
<figure confidence="0.92148575">
(camera doppia , double room)
2
(una , a)
(camera , room)
1
0
(la , the)
4
(camera , room)
3
(camera singola , single room)
5
214
Casacuberta and Vidal Translation with Finite-State Transducers
Figure 6
A finite-state transducer built from the n-gram of Figure 4.
</figure>
<figureCaption confidence="0.995329">
Figure 7
</figureCaption>
<bodyText confidence="0.993878535714286">
A finite-state transducer built from the n-gram of Figure 5.
the transitions and the final states of the finite-state transducer are the same as those
of the original stochastic regular grammar.
Since we are using n-grams in the second step, a transition (q, a, b1b2 · · · bk, q&apos;) is in
the finite-state transducer if the states q and q&apos; are (zi−n+1 . . . zi−1), (zi−n+2 . . . zi), respec-
tively, and (a,b1b2 · · · bk) is zi. The probability of the transition is pn(zi  |zi−n+1 . . . zi−1).
The transitions associated with back-off are labeled with a special source symbol (not
in the source vocabulary) and with an empty target string. The number of states is
the overall number of k-grams (k &lt; n) that appear in the training set of extended
strings plus one (the unigram state). The number of transitions is the overall num-
ber of k-grams (k &lt; n) plus the number of states (back-off transitions). The actual
number of these k-grams depends on the degree of nonmonotonicity of the original
bilingual training corpus. If the corpus if completely monotone, this number would be
approximately the same as the number of k-grams in the source or target parts of the
training corpus. If the corpus in not monotone, the vocabulary of expanded strings
becomes large, and the number of k-grams can be much larger than the number of
training source or target k-grams. As a consequence, an interesting property of this
type of transformations is that the source and target languages embedded in the final
finite-state transducer are more constrained than the corresponding n-gram models
obtained from either the source or the target strings, respectively, of the same training
pairs (Casacuberta, Vidal, and Pic´o 2004).
While n-grams are deterministic (hence nonambiguous) models, the finite-state
transducers obtained after the third-step inverse transformations (hr,ho) are often
nondeterministic and generally ambiguous; that is, there are source strings which can
be parsed through more than one path. This is in fact a fundamental property, directly
coming from expression (5) of Theorem 2, on which the whole GIATI approach is
essentially based. As a consequence, all the search issues discussed in Section 3.1 do
apply to GIATI-learned transducers.
</bodyText>
<figure confidence="0.992010965517241">
doppia / double room
camera / a,
2
una / a
1
camera / room
3
6
0
la / the
4
camera / a,
camera / room
5
singola / single room
camera / a,
doppia / double room
2
una / a
1
camera / room
0
la / the
4
camera / room
camera / a,
3
singola / single room
5
</figure>
<page confidence="0.700423">
215
</page>
<note confidence="0.326874">
Computational Linguistics Volume 30, Number 2
</note>
<sectionHeader confidence="0.975686" genericHeader="evaluation">
5. Experimental Results
</sectionHeader>
<bodyText confidence="0.999966">
Different translation tasks of different levels of difficulty were selected to assess the
capabilities of the proposed inference method in the framework of the EuTRANs
project (ITI et al. 2000): two Spanish-English tasks (EuTRANs-0 and EuTRANs-I),
an Italian-English task (EuTRANs-II) and a Spanish-German task (EuTRANs-Ia). The
EuTRANs-0 task, with a large semi-automatically generated training corpus, was used
for studying the convergence of transducer learning algorithms for increasingly large
training sets (Amengual et al. 2000). In this article it is used to get an estimation of per-
formance limits of the GIATI technique by assuming an unbounded amount of training
data. The EuTRANs-I task was similar to EuTRANs-0 but with a more realistically sized
training corpus. This corpus was defined as a first benchmark in the EuTRANs project,
and therefore results with other techniques are available. The EuTRANs-II task, with a
quite small and highly spontaneous natural training set, was a second benchmark of
the project. Finally, EuTRANs-Ia was similar to EuTRANs-I, but with a higher degree
of nonmonotonicity between corresponding words in input/output sentence pairs.
Tables 1, 4, and 7 show some important features of these corpora. As can be seen in
these tables, the training sets of EuTRANs-0, EuTRANs-I and EuTRANs-Ia contain non-
negligible amounts of repeated sentence pairs. Most of these repetitions correspond
to simple and/or usual sentences such as good morning, thank you, and do you have a
single room for tonight. The repetition rate is quite significant for EuTRANs-0, but it was
explicitly reduced in the more realistic benchmark tasks EuTRANs-I and EuTRANs-Ia.
It is worth noting, however, that no repetitions appear in any of the test sets of these
tasks. While repetitions can be helpful for probability estimation, they are completely
useless for inducing the transducer structure. Moreover, since no repetitions appear
in the test sets, the estimated probabilities will not be as useful as they could be if
test data repetitions exhibited the same patterns as those in the corresponding training
materials.
In all the experiments reported in this article, the approximate optimal translations
(equation (7)) of the source test strings were computed and the word error rate (WER),
the sentence error rate (SER), and the bilingual evaluation understudy (BLEU) metric
for the translations were used as assessment criteria. The WER is the minimum number
of substitution, insertion, and deletion operations needed to convert the word string
hypothesized by the translation system into a given single reference word string (ITI
et al. 2000). The SER is the result of a direct comparison between the hypothesized and
reference word strings as a whole. The BLEU metric is based on the n-grams of the
hypothesized translation that occur in the reference translations (Papineni et al 2001).
The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score).
</bodyText>
<subsectionHeader confidence="0.991707">
5.1 The Spanish-English Translation Tasks
</subsectionHeader>
<bodyText confidence="0.999781090909091">
A Spanish-English corpus was semi-automatically generated in the first phase of the
EuTRANs project (Vidal 1997). The domain of the corpus involved typical human-to-
human communication situations at a reception desk of a hotel.
A summary of this corpus (EuTRANs-0) is given in Table 1 (Amengual et al 2000;
Casacuberta et al. 2001). From this (large) corpus, a small subset of ten thousand
training sentence pairs (EuTRANs-I) was randomly selected in order to approach more
realistic training conditions (see also Table 1). From these data, completely disjoint
training and test sets were defined. It was guaranteed, however, that all the words in
the source test sentences were contained in both training sets (closed vocabulary).
Results for the EuTRANs-0 and EuTRANs-I corpora are presented in Tables 2 and 3,
respectively. The best results obtained using the proposed technique were 3.1% WER
</bodyText>
<page confidence="0.998521">
216
</page>
<note confidence="0.718863">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<tableCaption confidence="0.998823">
Table 1
</tableCaption>
<table confidence="0.947795625">
The Spanish-English corpus. There was no overlap between
training and test sentences, and the test set did not contain
out-of-vocabulary words with respect to any of the training sets.
Spanish English
EUTRANS-0 Train: Sentence pairs 490,000
Distinct pairs 168,629
Running words 4,655,000 4,802,000
Vocabulary 686 513
EUTRANS-I Train: Sentence pairs 10,000
Distinct pairs 6,813
Running words 97,131 99,292
Vocabulary 683 513
Test: Sentences 2,996
Running words 35,023 35,590
EUTRANS-0 Bigram test perplexity 6.8 5.8
EUTRANS-I Bigram test perplexity 8.6 6.3
</table>
<tableCaption confidence="0.897795">
Table 2
Results with the standard corpus EUTRANS-0. The
underlying regular models were smoothed n-grams for
different values of n.
</tableCaption>
<table confidence="0.838218166666667">
n-grams States Transitions WER % SER % BLEU
2 4,056 67,235 8.8 50.0 0.86
3 33,619 173,500 4.7 27.2 0.94
4 110,321 364,373 4.2 23.2 0.94
5 147,790 492,840 3.8 20.5 0.95
6 201,319 663,447 3.6 19.0 0.96
7 264,868 857,275 3.4 18.0 0.96
8 331,598 1,050,949 3.3 17.4 0.96
9 391,812 1,218,367 3.3 17.2 0.96
10 438,802 1,345,278 3.2 16.8 0.96
11 471,733 1,432,027 3.1 16.4 0.96
12 492,620 1,485,370 3.1 16.4 0.96
</table>
<tableCaption confidence="0.93065925">
Table 3
Results with the standard corpus EUTRANS-I. The
underlying regular models were smoothed n-grams for
different values of n.
</tableCaption>
<table confidence="0.8449376">
n-grams States Transitions WER % SER % BLEU
2 1,696 17,121 9.0 53.7 0.86
3 8,562 36,763 6.7 38.9 0.90
4 21,338 64,856 6.7 37.9 0.91
5 23,879 72,006 6.6 37.1 0.91
</table>
<page confidence="0.809385666666667">
6 25,947 77,531 6.6 37.0 0.91
7 27,336 81,076 6.6 37.0 0.91
217
</page>
<note confidence="0.383472">
Computational Linguistics Volume 30, Number 2
</note>
<bodyText confidence="0.999954666666667">
for EuTRANS-0 and 6.6% WER for EuTRANS-I. These results were achieved using the
statistical alignments provided by model 5 (Brown et al. 1993; Och and Ney 2000) and
smoothed 11-grams and 6-grams, respectively.
These results were obtained using the first type of transformation described in
Section 4.2 (L1). Similar experiments with the second type of transformation (L2) pro-
duced slightly worse results. However, L2 is interesting because many of the extended
symbols obtained in the experiments involve very good relations between some source
word groups and target word groups which could be useful by themselves. Conse-
quently, more research work has to be done with this second type of transformation.
The results on the (benchmark) EuTRANS-I corpus can be compared with those
obtained using other approaches. GIATI outperforms other finite-state techniques in
similar experimental conditions (with a best result of 8.3% WER, using another trans-
ducer inference technique called OMEGA [ITI et al. 2000]). On the other hand, the
best result achieved by the statistical templates technique (Och and Ney 2000) was
4.4% WER (ITI et al. 2000). However, this result cannot be exactly compared with
that achieved by GIATI, because the statistical templates approach used an explicit
(automatic) categorization of the source and the target words, while only the raw
word forms were used in GIATI. Although GIATI is compatible with different forms
of word categorization, the required finite-state expansion is not straightforward, and
some work is still needed in order to actually allow this technique to be taken advan-
tage of.
</bodyText>
<subsectionHeader confidence="0.999174">
5.2 The Italian-English Task
</subsectionHeader>
<bodyText confidence="0.999841">
The Italian-English translation task of the EuTRANS project (ITI et al. 2000) consisted
of spoken person-to-person telephone communications in the framework of a hotel
reception desk. A text corpus was collected with the transcriptions of dialogues of this
type, along with the corresponding (human-produced) translations. A summary of the
corpus used in the experiments (EuTRANS-II) is given in Table 4. There was a small
overlap of seven pairs between the training set and the test set, but in this case, the
vocabulary was not closed (there were 107 words in the test set that did not exist in
the training-set vocabulary). The processing of words out of the vocabulary was very
simple in this experiment: If the word started with a capital letter, the translation was
the source word; otherwise it was the empty string.
The same translation procedure and evaluation criteria used for EuTRANS-0 and
EuTRANS-I were used for EuTRANS-II. The results are reported in Table 5.
</bodyText>
<tableCaption confidence="0.871212">
Table 4
</tableCaption>
<bodyText confidence="0.9564135">
The EuTRANS-II corpus. There was a small
overlap of seven pairs between the training and
test sets, but 107 source words in the test set were
not in the (training-set-derived) vocabulary.
</bodyText>
<table confidence="0.862234857142857">
Italian English
Train: Sentence pairs 3,038
Running words 55,302 64,176
Vocabulary 2,459 1,712
Test: Sentences 300
Running words 6,121 7,243
Bigram test perplexity 31 25
</table>
<page confidence="0.863281">
218
</page>
<note confidence="0.621819">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<tableCaption confidence="0.814291">
Table 5
Results with the standard EuTRANS-II corpus. The
underlying regular models were smoothed
</tableCaption>
<table confidence="0.9669912">
n-grams (Rosenfeld 1995) for different values of n.
n-grams States Transitions WER % SER % BLEU
2 5,909 49,701 27.2 96.7 0.56
3 24,852 97,893 27.3 96.0 0.56
4 54,102 157,073 27.4 96.0 0.56
</table>
<tableCaption confidence="0.778823857142857">
Table 6
Results with the standard EuTRANS-II corpus. The
underlying regular models were smoothed n-grams
(Rosenfeld 1994) for different values of n. The training
set was (automatically) segmented using a priori
knowledge. The statistical alignments were constrained
to be within each parallel segment.
</tableCaption>
<table confidence="0.701758">
n-grams States Transitions WER % SER % BLEU
2 6,300 52,385 24.9 93.0 0.62
3 26,194 102,941 25.5 93.3 0.61
4 56,856 164,972 25.5 93.3 0.61
</table>
<bodyText confidence="0.99974532">
This corpus contained many long sentences, most of which were composed of
rather short segments connected by punctuation marks. Typically, these segments can
be monotonically aligned with corresponding target segments using a simple dynamic
programming procedure (prior segmentation) (ITI et al. 2000). We explored computing
the statistical alignments within each pair of segments rather than in the entire sen-
tences. Since the segments were shorter than the whole sentences, the alignment prob-
ability distributions were better estimated. In the training phase, extended symbols
were built from these alignments, and the strings of extended symbols corresponding
to the segments of the same original string pair were concatenated. Test sentences
were directly used, without any kind of segmentation.
The translation results using prior segmentation are reported in Table 6. These
results were clearly better than those of the corresponding experiments with nonseg-
mented training data.
The accuracy of GIATI in the EuTRANS-II experiments was significantly worse
than that achieved in EuTRANS-I, and best performance is obtained with a lower-order
n-gram. One obvious reason for this behavior is that this corpus is far more sponta-
neous than the first one, and consequently, it has a much higher degree of variability.
Moreover, the training data set is about three times smaller than the corresponding
data of EuTRANS-I, while the vocabularies are three to four times larger.
The best result achieved with the proposed technique on EuTRANS-II was 24.9%
WER, using prior segmentation of the training pairs and a smoothed bigram model.
This result was comparable to the best among all those reported in RWTH Aachen and
ITI (1999). The previously mentioned statistical templates technique achieved 25.1%
WER in this case. In this application, in which categories are not as important as in
EuTRANS-I, statistical templates and GIATI achieved similar results.
</bodyText>
<page confidence="0.997067">
219
</page>
<table confidence="0.480364">
Computational Linguistics Volume 30, Number 2
</table>
<tableCaption confidence="0.995858">
Table 7
</tableCaption>
<table confidence="0.813628818181818">
The Spanish-German corpus. There was no overlap
between training and test sets and no
out-of-vocabulary words in the test set.
Spanish German
Train: Sentence pairs 10,000
Distinct pairs 6,636
Running words 96,043 90,099
Vocabulary 6,622 4,890
Test: Sentences 2,862
Running words 33,542 31,103
Bigram test perplexity 8.3 6.6
</table>
<tableCaption confidence="0.952014">
Table 8
Results with the standard corpus EuTRANs-Ia. The
underlying regular models were smoothed n-grams for
different values of n.
</tableCaption>
<figure confidence="0.689246428571429">
n-grams States Transitions WER % SER % BLEU
2 2,441 21,181 16.0 78.1 0.74
3 10,592 43,294 11.3 65.3 0.82
4 24,554 74,412 10.6 62.3 0.83
5 27,748 83,553 10.6 62.5 0.83
6 30,501 91,055 10.6 62.4 0.83
7 32,497 96,303 10.7 62.7 0.83
</figure>
<subsectionHeader confidence="0.998778">
5.3 The Spanish-German Task
</subsectionHeader>
<bodyText confidence="0.999960391304348">
The Spanish-German translation task is similar to EuTRANs-I, but here the target
language is German instead of English. It should be noted that Spanish syntax is
significantly more different from that of German than it is from that of English, and
therefore, the corresponding corpus exhibited a higher degree of nonmonotonicity. The
features of this corpus (EuTRANs-Ia) are summarized in Table 7. There was no overlap
between training and test sets, and the vocabulary was closed.
The translation results are reported in Table 8. As expected from the higher degree
of nonmonotonicity of the present task, these results were somewhat worse than those
achieved with EuTRANs-I. This is consistent with the larger number of states and
transitions of the EuTRANs-Ia models: The higher degree of word reordering of these
models is achieved at the expense of a larger number of extended words.
The way GIATI transducers cope with these monotonicity differences can be more
explicitly illustrated by estimating how many target words are produced after some
delay with respect to the source. While directly determining (or even properly defin-
ing) the actual production delay for each individual (test) word is not trivial, an
approximation can be indirectly derived from the number of target words that are
preceded by sequences of λ symbols (from target-empty transitions) in the parsing of
a source test text with a given transducer. This has been done for the EuTRANs-I and
EuTRANs-Ia test sets with GIATI transducers learned with n = 6. On the average, the
EuTRANs-I transducer needed to introduce delays ranging from one to five positions
for approximately 15% of the English target words produced, while the transducer
for EuTRANs-Ia had to introduce similar delays for about 20% of the German target
words produced.
</bodyText>
<page confidence="0.982722">
220
</page>
<note confidence="0.743328">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<subsectionHeader confidence="0.955379">
5.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.99679">
The errors reported in the previous sections can be attributed to four main factors:
</bodyText>
<listItem confidence="0.9991375">
1. Correct translations which differ from the given (single) reference
2. Wrong alignments of training pairs
3. Insufficient or improper generalization of n-gram-based GIATI learning
4. Wrong approximate Viterbi score–based search results
</listItem>
<bodyText confidence="0.995300230769231">
An informal inspection of the target sentences produced by GIATI in all the experi-
ments reveals that the first three factors are responsible for the vast majority of errors.
Table 9 shows typical examples for the results of the EuTRANS-I experiment with
6-gram-based GIATI transducers.
The first three examples correspond to correct translations which have been wrong-
ly counted as errors (factor 1). Examples 4 and 5 are probably due to alignment prob-
lems (factor 2). In fact, more than half of the errors reported in the EuTRANS-I experi-
ments are due to misuse or misplacement of the English word please. Examples 6–8 can
also be considered minor errors, probably resulting from factors 2 and 3. Examples 9
and 10 are clear undergeneralization errors (factor 3). These errors could have been
easily overcome through an adequate use of bilingual lexical categorization. Examples
11 and 12, finally, are more complex errors that can be attributed to (a combination of)
factors 2, 3, and 4.
</bodyText>
<sectionHeader confidence="0.993111" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.999931576923077">
A method has been proposed in this article for inferring stochastic finite-state trans-
ducers from stochastic regular grammars. This method, GIATI, allowed us to achieve
good results in several language translation tasks with different levels of difficulty. It
works better than other finite-state techniques when the training data are scarce and
achieves similar results with sufficient training data.
The GIATI approach produces transducers which generalize the information pro-
vided by the (aligned) training pairs. Thanks to the use of n-grams as a core learning
procedure, a wide range of generalization degrees can be achieved. It is well-known
that a 1-gram entails a maximum generalization, allowing (extended) words to follow
one another. On the other hand, for sufficiently large m, a (nonsmoothed) m-gram is just
an exact representation of the training strings (of extended words, in our case). Such a
representation can thus be considered a simple “translation memory” that just contains
the (aligned) training pairs. For any new source sentence, this “memory” can be eas-
ily and quite efficiently searched through finite-state parsing. For other intermediate
values of n, 1&lt;n&lt;m, GIATI obtains increasing degrees of generalization. As in the
case of language modeling, the generalization degree (n) has to be tuned so as to take
maximum advantage of the available training data. As training pairs become scarce,
more generalization is needed to allow GIATI to adequately accept new test sentences.
This behavior can be clearly observed throughout the results presented in this article.
Another feature of the GIATI approach is the use of smoothed n-grams of extended
words as the basic mechanism for producing smoothed transducers. The combination
of this feature with the intrinsic generalization provided by the n-gram modeling itself
has proved very adequate to deal with the problem of unseen source (sub)strings.
Obviously, the overall quality of the generalizations achieved by GIATI strongly
relies on the quality of the statistical alignments used and on the way word order
is preserved in the source-target strings of each training pair. Taking into account the
</bodyText>
<page confidence="0.994362">
221
</page>
<table confidence="0.475608">
Computational Linguistics Volume 30, Number 2
</table>
<tableCaption confidence="0.990181">
Table 9
</tableCaption>
<bodyText confidence="0.9759145">
Examples of typical errors produced by a 6-gram-based GIATI
transducer in the the EuTRANS-I task. For each Spanish source sentence,
the corresponding target reference and GIATI translations are shown in
successive lines.
</bodyText>
<footnote confidence="0.856128333333333">
1 1 les importaria bajarnos nuestras bolsas a recepci´on ?
would you mind sending our bags down to reception ?
would you mind sending down our bags to reception ?
2 explique la cuenta de la habitaci´on cuatro diecis´eis .
explain the bill for room number four one six.
explain the bill for room number four sixteenth.
3 1 cu´anto vale una habitaci´on doble para cinco dias incluyendo desayuno ?
how much is a double room including breakfast for five days ?
how much is a double room for five days including breakfast ?
</footnote>
<figure confidence="0.886144285714286">
4 por favor , deseo una habitaci´on individual para esta semana .
I want a single room for this week, please .
I want a single room for this week.
5 1 le importaria despertarnos a las cinco ?
would you mind waking us up at five ?
would you mind waking us up at five , please ?
6 1 hay televisi´on , aire acondicionado y caja fuerte en las habitaciones ?
</figure>
<construct confidence="0.7166234">
are there a tv , air conditioning and a safe in the rooms ?
is there a tv , air conditioning and a safe in the rooms ?
7 1 tiene habitaciones libres con tel´efono ?
do you have any rooms with a telephone available ?
do you have any rooms with a telephone ?
8 1 querria llamar a mi taxi ?
would you call my taxi, please ?
would you call my taxi for me , please ?
9 hemos de marcharnos el veintis´eis de marzo por la tarde .
we should leave on March the twenty-sixth in the afternoon.
we should leave on March the twenty-seventh in the afternoon
10 por favor , 1 nos podria dar usted la llave de la ochocientos ochenta y uno ?
could you give us the key to room number eight eight one , please ?
could you give us the key to room number eight oh eight one , please ?
11 quiero cambiarme de habitaci´on .
I want to change rooms.
I want to move.
12 1 tiene televisi´on nuestra habitaci´on ?
does our room have a tv ?
does our room ?
</construct>
<page confidence="0.991839">
222
</page>
<note confidence="0.607018">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<bodyText confidence="0.999723">
finite-state nature of GIATI transducers, certain heuristics have been needed in order to
avoid a direct use of too-long-distance alignments (L1 in Section 4.2). This has proved
adequate for language pairs with not too different (syntactic) structure and more so
if the domains are limited. As we relax these restrictions, we might have to relax the
not-too-long-distance assumption correspondingly. In this respect, the bilingual word
reordering ideas of Vilar, Vidal, and Amengual (1996), Vidal (1997), and Bangalore and
Ricardi (2000a) may certainly prove useful in future developments.
</bodyText>
<sectionHeader confidence="0.982885" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998168714285714">
This work has been partially supported by
the European Union under grants
IT-LTR-OS-30268, IST-2001-32091 and
Spanish project TIC 2000-1599-C02-01. The
authors wish to thank the anonymous
reviewers for their criticisms and
suggestions.
</bodyText>
<sectionHeader confidence="0.994228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998375703296703">
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy,
Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation. Final
Report, JHU Workshop, Johns Hopkins
University, Baltimore.
Amengual, Juan-Carlos, Jose-Miguel Bened´ı,
Francisco Casacuberta, Asunci ´on Casta
no, Antonio Castellanos, Victor Jim ´enez,
David Llorens, Andr ´es Marzal, Mois´es
Pastor, Federico Prat, Enrique Vidal, and
Juan-Miguel Vilar. 2000. The EUTRANS-I
speech translation system. Machine
Translation Journal, 15(1–2):75–103.
Bangalore, Srinivas and Giuseppe Ricardi.
2000a. Finite-state models for lexical
reordering in spoken language
translation. In Proceedings of the
International Conference on Speech and
Language Processing, Beijing, China,
October.
Bangalore, Srinivas and Giuseppe Ricardi.
2000b. Stochastic finite-state models for
spoken language machine translation. In
Proceedings of the Workshop on Embedded
Machine Translation Systems, North
American Association for Computational
Linguistics, pages 52–59, Seattle, May.
Bangalore, Srinivas and Giuseppe Ricardi.
2001. A finite-state approach to machine
translation. In Proceedings of the Second
Meeting of the North American Chapter of the
Association for Computational Linguistics
2001, Pittsburgh, May.
Berstel, Jean. 1979. Transductions and
context-free languages. B. G. Teubner,
Stuttgart.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–310.
Casacuberta, Francisco. 1996. Maximum
mutual information and conditional
maximum likelihood estimation of
stochastic regular syntax-directed
translation schemes. In Grammatical
Inference: Learning Syntax from Sentences
(volume 1147 of Lecture Notes on
Computer Science). Springer-Verlag,
Berlin and Heidelberg, pages 282–291.
Casacuberta, Francisco. 2000. Inference of
finite-state transducers by using regular
grammars and morphisms. In Grammatical
Inference: Algorithms and Applications
(volume 1891 of Lecture Notes in
Artificial Intelligence). Springer-Verlag,
Berlin and Heidelberg, pages 1–14.
Casacuberta, Francisco and Colin de la
Higuera. 2000. Computational complexity
of problems on probabilistic grammars
and transducers. In Grammatical Inference:
Algorithms and Applications (volume 1891
of Lecture Notes in Artificial Intelligence).
Springer-Verlag, Berlin and Heidelberg,
pages 15–24.
Casacuberta, Francisco, David Llorens,
Carlos Martinez, Sirko Molau, Francisco
Nevado, Hermann Ney, Moise´es Pastor,
David Pic ´o, Alberto Sanchis, Enrique
Vidal, and Juan-Miguel Vilar. 2001.
Speech-to-speech translation based on
finite-state transducers. In Proceedings of
the IEEE International Conference on
Acoustic, Speech and Signal Processing,
volume 1. IEEE Press, Piscataway, NJ,
pages 613–616.
Casacuberta, Francisco, Enrique Vidal, and
David Pic ´o. 2004. Inference of finite-state
transducers from regular languages.
Pattern Recognition, forthcoming.
Clarkson, Philip and Ronald Rosenfeld.
1997. Statistical language modeling using
the CMU-Cambridge toolkit. In
Proceedings of EUROSPEECH, volume 5,
pages 2707–2710, Rhodes, September.
Fu, King-Sun. 1982. Syntactic pattern
recognition and applications. Prentice-Hall,
Englewood Cliffs, NJ.
</reference>
<page confidence="0.992878">
223
</page>
<note confidence="0.458805">
Computational Linguistics Volume 30, Number 2
</note>
<reference confidence="0.998014032786886">
Garcia, Pedro, Enrique Vidal, and Francisco
Casacuberta. 1987. Local languages, the
successor method, and a step towards a
general methodology for the inference of
regular grammars. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
9(6):841–845.
Gonz ´alez, Jorge, Ismael Salvador, Alejandro
Toselli, Alfons Juan, Enrique Vidal, and
Francisco Casacuberta. 2000. Offline
recognition of syntax-constrained cursive
handwritten text. In Advances in Pattern
Recognition (volume 1876 of Lecture Notes
in Computer Science). Springer-Verlag,
Berlin and Heidelberg, pages 143–153.
Hazen, Timothy, I. Lee Hetherington, and
Alex Park. 2001. FST-based recognition
techniques for multi-lingual and
multi-domain spontaneous speech. In
Proceedings of EUROSPEECH2001, pages
1591–1594, Aalborg, Denmark, September.
ITI, FUB, RWTH Aachen, and ZERES. 2000.
Example-based language translation
systems: Final report. Technical Report
D0.1c, Instituto Tecnol ´ogico de
Inform ´atica, Fondazione Ugo Bordoni,
Rheinisch Westf¨alische Technische
Hochschule Aachen Lehrstuhl f¨ur
Informatik V and Zeres GmbH Bochum.
Information Technology. Long Term
Research Domain. Open scheme.
Knight, Kevin and Yaser Al-Onaizan. 1998.
Translation with finite-state devices. In
Proceedings of the Fourth. AMTA Conference
(volume 1529 of Lecture Notes in
Artificial Intelligence). Springer-Verlag,
Berlin and Heidelberg, pages 421–437.
Llorens, David, Juna-Miguel Vilar, and
Francisco Casacuberta. 2002. Finite state
language models smoothed using
n-grams. International Journal of Pattern
Recognition and Artificial Intelligence,
16(3):275–289.
M¨akinen, Erkki. 1999. Inferring finite
transducers. Technical Report A-1999-3,
University of Tampere, Tampere, Finland.
Mohri, Mehryar. 1997. Finite-state
transducers in language and speech
processing. Computational Linguistics,
23(2):1–20.
Mou, Xiaolong, Stephanie Seneff, and Victor
Zue. 2001. Context-dependent
probabilistic hierarchical sub-lexical
modelling using finite-state transducers.
In Proceedings of EUROSPEECH2001, pages
451–454, Aalborg, Denmark, September.
Ney, Hermann, Sven Martin, and
Frank Wessel. 1997. Statistical language
modeling using leaving-one-out. In
S. Young and G. Bloothooft, editors,
Corpus-Based Statiscal Methods in Speech and
Language Processing. Kluwer Academic,
Dordrecht, the Netherlands, pages
174–207.
Och, Franz-Josef and Hermann Ney. 2000.
Improved statistical alignment models. In
Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics,
pages 440–447, Hong Kong, October.
Oncina, Jos ´e, Pedro Garcia, and Enrique
Vidal. 1993. Learning subsequential
transducers for pattern recognition
interpretation tasks. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
15(5):448–458.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu: A
method for automatic evaluation of
machine translation. Technical Report
RC22176(W0109-022), IBM Research
Division, Yorktown Heights, NY,
September.
Rosenfeld, Ronald. 1995. The CMU
statistical language modeling toolkit and
its use in the 1994 ARPA CSR evaluation.
In Proceedings of the ARPA Spoken Language
Technology Workshop, Princeton, NJ.
Morgan Kaufmann, San Mateo, CA.
RWTH Aachen and ITI. 1999. Statistical
modeling techniques and results and
search techniques and results. Technical
Report D3.1a and D3.2a,
Rheinisch Westf¨alische Technis-
che Hochschule
Aachen Lehrstuhl f¨ur Informatik VI and
Instituto Tecnol ´ogico de Inform ´atica.
Information Technology. Long Term
Research Domain. Open scheme.
Segarra, Encarna, Maria-Isabel Galiano
Emilio Sanchis, Fernando Garcia, and
Luis Hurtado. 2001. Extracting semantic
information through automatic learning
techniques. In Proceedings of the Spanish
Symposium on Pattern Recognition and Image
Analysis, pages 177–182, Benicasim, Spain,
May.
Seward, Alexander. 2001. Transducer
optimizations for tight-coupled decoding.
In Proceedings of EUROSPEECH2001, pages
1607–1610, Aalborg, Denmark, September.
Vidal, Enrique. 1997. Finite-state
speech-to-speech translation. In
Proceedings of the International Conference on
Acoustic Speech and Signal Processing,
Munich. IEEE Press, Piscataway, NJ,
pages 111–114.
Vidal, Enrique, Francisco Casacuberta, and
Pedro Garcia. 1995. Grammatical inference
and automatic speech recognition. In
A. Rubio, editor, New Advances and Trends
in Speech Recognition and Coding (volume
147 of NATO-ASI Series F: Computer and
</reference>
<page confidence="0.970572">
224
</page>
<note confidence="0.436512">
Casacuberta and Vidal Translation with Finite-State Transducers
</note>
<reference confidence="0.999638347826087">
Systems Sciences). Springer-Verlag, Berlin
and Heidelberg, pages 174–191.
Vidal, Enrique, Pedro Garcia, and Encarna
Segarra. 1989. Inductive learning of
finite-state transducers for the
interpretation of unidimensional objects.
In R. Mohr, T. Pavlidis, and A. Sanfeliu,
editors, Structural Pattern Analysis. World
Scientific, Singapore, pages 17–35.
Vilar, Juan-Miguel. 2000. Improve the
learning of subsequential transducers by
using alignments and dictionaries. In
Grammatical Inference: Algorithms and
Applications (volume 1891 of Lecture Notes
in Artificial Intelligence). Springer-Verlag,
Berlin and Heidelberg, pages 298–312.
Vilar, Juan-Miguel, Enrique Vidal, and
Juan-Carlos Amengual. 1996. Learning
extended finite state models for language
translation. In Andr´as Kornai, editor,
Proceedings of the Extended Finite State
Models of Language Workshop, pages 92–96,
Budapest, August.
</reference>
<page confidence="0.998816">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704727">
<title confidence="0.9995045">Machine Translation with Inferred Stochastic Finite-State Transducers</title>
<author confidence="0.916612">Enrique</author>
<affiliation confidence="0.982955">Universidad Polit´ecnica de Valencia Universidad Polit´ecnica de Valencia</affiliation>
<abstract confidence="0.979214090909091">Finite-state transducers are models that are being used in different areas ofpattern recognition and computational linguistics. One of these areas is machine translation, in which the approaches that are based on building models automatically from training examples are becoming more and more attractive. Finite-state transducers are very adequatefor use in constrained tasks in which training samples of pairs of sentences are available. A technique for inferring finite-state transducers is proposed in this article. This technique is based onformal relations betweenfinite-state transducers and rational grammars. Given a training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which stochastic rational grammar (e.g., an is inferred. This grammar is finally converted into a finite-state transducer. The proposed methods are assessed through a series of machine experiments within the framework of the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Final Report, JHU Workshop,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore.</location>
<contexts>
<context position="17008" citStr="Al-Onaizan et al. 1999" startWordPosition="2863" endWordPosition="2866">lating a given s into t by an alignment a is Pr(t, a |s). Thus, an optimal alignment between s and t can be computed as aˆ = argmax Pr(t, a |s) (11) a∈A(s,t) 5 In previous work, this idea was often called morphic generator transducer inference. 210 Casacuberta and Vidal Translation with Finite-State Transducers Different approaches for estimating Pr(t, a I s) were proposed in Brown et al. (1993). These approaches are known as models 1 through 5. Adequate software packages are publicly available for training these statistical models and for obtaining good alignments between pairs of sentences (Al-Onaizan et al. 1999; Och and Ney 2000). An example of Spanish-English sentence alignment is given below: Example 1 ¿ Cu´anto cuesta una habitaci´on individual por semana ? how (2) much (2) does (3) a (4) single (6) room (5) cost (3) per (7) week (8) ? (9) Each number within parentheses in the example represents the position in the source sentence that is aligned with the (position of the) preceding target word. A graphical representation of this alignment is shown in Figure 3. 4.2 First Step of the GIATI Methodology: Transformation of Training Pairs into Strings The first step of the proposed method consists in </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Al-Onaizan, Yaser, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation. Final Report, JHU Workshop, Johns Hopkins University, Baltimore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan-Carlos Amengual</author>
<author>Jose-Miguel Bened´ı</author>
</authors>
<title>Francisco Casacuberta, Asunci ´on Casta no, Antonio Castellanos, Victor Jim ´enez, David Llorens, Andr ´es Marzal,</title>
<date>2000</date>
<journal>Machine Translation Journal,</journal>
<pages>15--1</pages>
<location>Mois´es Pastor, Federico</location>
<marker>Amengual, Bened´ı, 2000</marker>
<rawString>Amengual, Juan-Carlos, Jose-Miguel Bened´ı, Francisco Casacuberta, Asunci ´on Casta no, Antonio Castellanos, Victor Jim ´enez, David Llorens, Andr ´es Marzal, Mois´es Pastor, Federico Prat, Enrique Vidal, and Juan-Miguel Vilar. 2000. The EUTRANS-I speech translation system. Machine Translation Journal, 15(1–2):75–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Ricardi</author>
</authors>
<title>Finite-state models for lexical reordering in spoken language translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Speech and Language Processing,</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="1790" citStr="Bangalore and Ricardi 2000" startWordPosition="240" endWordPosition="243">ation experiments within the framework of the EuTRANS project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. These transductions are realiz</context>
<context position="3321" citStr="Bangalore and Ricardi 2000" startWordPosition="449" endWordPosition="452">e-state machines for language translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina, * Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 2 Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from finite sets of learning strings which have been used successfully in a number of fields, including automatic speech recognition (Vidal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simple) finitestate grammar lear</context>
<context position="15623" citStr="Bangalore and Ricardi (2000" startWordPosition="2623" endWordPosition="2626">ference technique was presented in Vidal, Garc´ıa, and Segarra (1989) An important drawback of that early proposal was that the methods proposed for building the P* sentences from the training pairs did not adequately cope with the dependencies between the words of the source sentences and the words of the corresponding target sentences. In the following section we show how this drawback can be overcome using statistical alignments (Brown et al. 1993). The resulting methodology is called grammatical inference and alignments for transducer inference (GIATI).5 A related approach was proposed in Bangalore and Ricardi (2000b). In that case, the extended symbols were also built according to previously computed alignments, but the order of target words was not preserved. As a consequence, that approach requires a postprocess to try to restore the target words to a proper order. 4.1 Statistical Alignments The statistical translation models introduced by Brown et al. (1993) are based on the concept of alignment between source and target words (statistical alignment models). Formally, an alignment of a translation pair (s, t) ∈ E* × ∆* is a function a : {1, ... , |t|} → {0,..., |s |}. The particular case a(j) = 0 mea</context>
</contexts>
<marker>Bangalore, Ricardi, 2000</marker>
<rawString>Bangalore, Srinivas and Giuseppe Ricardi. 2000a. Finite-state models for lexical reordering in spoken language translation. In Proceedings of the International Conference on Speech and Language Processing, Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Ricardi</author>
</authors>
<title>Stochastic finite-state models for spoken language machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Embedded Machine Translation Systems, North American Association for Computational Linguistics,</booktitle>
<pages>52--59</pages>
<location>Seattle,</location>
<contexts>
<context position="1790" citStr="Bangalore and Ricardi 2000" startWordPosition="240" endWordPosition="243">ation experiments within the framework of the EuTRANS project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. These transductions are realiz</context>
<context position="3321" citStr="Bangalore and Ricardi 2000" startWordPosition="449" endWordPosition="452">e-state machines for language translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina, * Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 2 Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from finite sets of learning strings which have been used successfully in a number of fields, including automatic speech recognition (Vidal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simple) finitestate grammar lear</context>
<context position="15623" citStr="Bangalore and Ricardi (2000" startWordPosition="2623" endWordPosition="2626">ference technique was presented in Vidal, Garc´ıa, and Segarra (1989) An important drawback of that early proposal was that the methods proposed for building the P* sentences from the training pairs did not adequately cope with the dependencies between the words of the source sentences and the words of the corresponding target sentences. In the following section we show how this drawback can be overcome using statistical alignments (Brown et al. 1993). The resulting methodology is called grammatical inference and alignments for transducer inference (GIATI).5 A related approach was proposed in Bangalore and Ricardi (2000b). In that case, the extended symbols were also built according to previously computed alignments, but the order of target words was not preserved. As a consequence, that approach requires a postprocess to try to restore the target words to a proper order. 4.1 Statistical Alignments The statistical translation models introduced by Brown et al. (1993) are based on the concept of alignment between source and target words (statistical alignment models). Formally, an alignment of a translation pair (s, t) ∈ E* × ∆* is a function a : {1, ... , |t|} → {0,..., |s |}. The particular case a(j) = 0 mea</context>
</contexts>
<marker>Bangalore, Ricardi, 2000</marker>
<rawString>Bangalore, Srinivas and Giuseppe Ricardi. 2000b. Stochastic finite-state models for spoken language machine translation. In Proceedings of the Workshop on Embedded Machine Translation Systems, North American Association for Computational Linguistics, pages 52–59, Seattle, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Ricardi</author>
</authors>
<title>A finite-state approach to machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics</booktitle>
<location>Pittsburgh,</location>
<marker>Bangalore, Ricardi, 2001</marker>
<rawString>Bangalore, Srinivas and Giuseppe Ricardi. 2001. A finite-state approach to machine translation. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics 2001, Pittsburgh, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Berstel</author>
</authors>
<title>Transductions and context-free languages.</title>
<date>1979</date>
<journal>B. G. Teubner,</journal>
<location>Stuttgart.</location>
<contexts>
<context position="2289" citStr="Berstel 1979" startWordPosition="315" endWordPosition="316">ontrol) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. These transductions are realized by the so-called finite-state transducers. Even though other, more powerful transduction models exist, finite-state transducers generally entail much more affordable computational costs, thereby making these simpler models more interesting in practice. One of the main reasons for the interest in finite-state machines for language translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniqu</context>
<context position="6013" citStr="Berstel 1979" startWordPosition="925" endWordPosition="926">of a finite-state transducer is similar to the definition of a regular or finite-state grammar 9. The main difference is that in a finite-state grammar, the set of target symbols ∆ does not exist, and the transitions are defined on Q x Σ x Q. A translation form is the transducer counterpart of a derivation in a finite-state grammar, and the concept of rational translation is reminiscent of the concept of (regular) language, defined as the set of strings associated with the derivations in the grammar 9. Rational translations exhibit many properties similar to those shown for regular languages (Berstel 1979). One of these properties can be stated as follows (Berstel 1979): Theorem 1 T C_ Σ* x ∆* is a rational translation if and only if there exist an alphabet Γ, a regular language L C Γ*, and two morphisms hΣ : Γ* → Σ* and ho : Γ* → ∆*, such that T = {(hΣ(w), ho(w)) | w E L�. 1 By 0∗ and E∗, we denote the set of finite-length strings on 0 and E, respectively. 2 To simplify the notation, we will remove the superscript φ from the components of a translation form if no confusion is induced. 206 Casacuberta and Vidal Translation with Finite-State Transducers As will be discussed later, this theorem d</context>
<context position="12197" citStr="Berstel 1979" startWordPosition="2055" endWordPosition="2056"> will be the approximate result given by equation (7). The computational cost of the iterative version of this algorithm is O( |s |· |Q |· B), where B is the (average) branching factor of the finite-state transducer. Figure 1 shows a simple example in which Viterbi score maximization (7) leads to a suboptimal result. 4. A Method for Inferring Finite-State Transducers Theorems 1 and 2 establish that any (stochastic) rational translation T can be obtained as a homomorphic image of certain (stochastic) regular language L over an adequate alphabet Γ. The proofs of these theorems are constructive (Berstel 1979; Casacuberta, Vidal, and Pico´ 2004) and are based on building a (stochastic) finite-state transducer T for T by applying certain morphisms hr and ho to the symbols of Γ that are associated with the rules of a (stochastic) regular grammar that generates L. This suggests the following general technique for learning a stochastic finite-state transducer, given a finite sample A of string pairs (s, t) E Σ* x ∆* ( a parallel corpus): 1. Each training pair (s, t) from A is transformed into a string z from an extended alphabet Γ (strings of Γ-symbols) yielding a sample S of strings S c Γ�. 2. A (sto</context>
</contexts>
<marker>Berstel, 1979</marker>
<rawString>Berstel, Jean. 1979. Transductions and context-free languages. B. G. Teubner, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="15451" citStr="Brown et al. 1993" startWordPosition="2600" endWordPosition="2603">st allow for a simple implementation of the inverse labeling needed in the third step. A very preliminary, nonstochastic version of this finite-state transducer inference technique was presented in Vidal, Garc´ıa, and Segarra (1989) An important drawback of that early proposal was that the methods proposed for building the P* sentences from the training pairs did not adequately cope with the dependencies between the words of the source sentences and the words of the corresponding target sentences. In the following section we show how this drawback can be overcome using statistical alignments (Brown et al. 1993). The resulting methodology is called grammatical inference and alignments for transducer inference (GIATI).5 A related approach was proposed in Bangalore and Ricardi (2000b). In that case, the extended symbols were also built according to previously computed alignments, but the order of target words was not preserved. As a consequence, that approach requires a postprocess to try to restore the target words to a proper order. 4.1 Statistical Alignments The statistical translation models introduced by Brown et al. (1993) are based on the concept of alignment between source and target words (sta</context>
<context position="16784" citStr="Brown et al. (1993)" startWordPosition="2829" endWordPosition="2832">. , |t|} → {0,..., |s |}. The particular case a(j) = 0 means that the position j in t is not aligned with any position in s. All the possible alignments between t and s are denoted by A(s, t), and the probability of translating a given s into t by an alignment a is Pr(t, a |s). Thus, an optimal alignment between s and t can be computed as aˆ = argmax Pr(t, a |s) (11) a∈A(s,t) 5 In previous work, this idea was often called morphic generator transducer inference. 210 Casacuberta and Vidal Translation with Finite-State Transducers Different approaches for estimating Pr(t, a I s) were proposed in Brown et al. (1993). These approaches are known as models 1 through 5. Adequate software packages are publicly available for training these statistical models and for obtaining good alignments between pairs of sentences (Al-Onaizan et al. 1999; Och and Ney 2000). An example of Spanish-English sentence alignment is given below: Example 1 ¿ Cu´anto cuesta una habitaci´on individual por semana ? how (2) much (2) does (3) a (4) single (6) room (5) cost (3) per (7) week (8) ? (9) Each number within parentheses in the example represents the position in the source sentence that is aligned with the (position of the) pre</context>
<context position="33902" citStr="Brown et al. 1993" startWordPosition="5714" endWordPosition="5717">96 11 471,733 1,432,027 3.1 16.4 0.96 12 492,620 1,485,370 3.1 16.4 0.96 Table 3 Results with the standard corpus EUTRANS-I. The underlying regular models were smoothed n-grams for different values of n. n-grams States Transitions WER % SER % BLEU 2 1,696 17,121 9.0 53.7 0.86 3 8,562 36,763 6.7 38.9 0.90 4 21,338 64,856 6.7 37.9 0.91 5 23,879 72,006 6.6 37.1 0.91 6 25,947 77,531 6.6 37.0 0.91 7 27,336 81,076 6.6 37.0 0.91 217 Computational Linguistics Volume 30, Number 2 for EuTRANS-0 and 6.6% WER for EuTRANS-I. These results were achieved using the statistical alignments provided by model 5 (Brown et al. 1993; Och and Ney 2000) and smoothed 11-grams and 6-grams, respectively. These results were obtained using the first type of transformation described in Section 4.2 (L1). Similar experiments with the second type of transformation (L2) produced slightly worse results. However, L2 is interesting because many of the extended symbols obtained in the experiments involve very good relations between some source word groups and target word groups which could be useful by themselves. Consequently, more research work has to be done with this second type of transformation. The results on the (benchmark) EuTR</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
</authors>
<title>Maximum mutual information and conditional maximum likelihood estimation of stochastic regular syntax-directed translation schemes.</title>
<date>1996</date>
<booktitle>In Grammatical Inference: Learning Syntax from Sentences</booktitle>
<volume>1147</volume>
<pages>282--291</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="10354" citStr="Casacuberta 1996" startWordPosition="1720" endWordPosition="1721"> = (s, t) 3.1 Search with Stochastic Finite-State Transducers The search for an optimal ˆt in Equation (4) has proved to be a difficult computational problem (Casacuberta and de la Higuera 2000). In practice, an approximate solution can be obtained (Casacuberta 2000) on the basis of the following approximation to the probability of a translation pair (Viterbi score of a translation): PTP(s, t) ;: VTP(s, t) = max PTP(φ) (6) ¢∈d(s,t) An approximate translation can now be computed as ˜t = argmax VTP(s, t) = argmax max PTP(φ) (7) t∈∆* t∈∆* ¢∈d(s,t) This computation can be carried out efficiently (Casacuberta 1996) by solving the following recurrence by means of dynamic programming: max VTP(s,t) = max (V(jsj,q) - f(q)) (8) t∈∆∗ q∈Q (V(i − 1,q~) - p(q�,si,w,q)) if i =� 0,q =� q0 (9) V(i,q) = max q/∈Q,w∈∆* V(0,q0) = 1 (10) Finally, the approximate translation ˜t is obtained as the concatenation of the target strings associated with the translation form φ˜ = (q0, s1,¯t1, q1)(q1, s2,¯t2, q2) ... (qI−1, sI−1,¯tI, qI), corresponding to the optimal sequence of states involved in the solution to Equation (8); that is, ˜t = t1t2 ... tI 208 Casacuberta and Vidal Translation with Finite-State Transducers una / a (</context>
</contexts>
<marker>Casacuberta, 1996</marker>
<rawString>Casacuberta, Francisco. 1996. Maximum mutual information and conditional maximum likelihood estimation of stochastic regular syntax-directed translation schemes. In Grammatical Inference: Learning Syntax from Sentences (volume 1147 of Lecture Notes on Computer Science). Springer-Verlag, Berlin and Heidelberg, pages 282–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
</authors>
<title>Inference of finite-state transducers by using regular grammars and morphisms.</title>
<date>2000</date>
<booktitle>In Grammatical Inference: Algorithms and Applications</booktitle>
<volume>1891</volume>
<pages>1--14</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="3340" citStr="Casacuberta 2000" startWordPosition="453" endWordPosition="454"> translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina, * Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 2 Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from finite sets of learning strings which have been used successfully in a number of fields, including automatic speech recognition (Vidal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simple) finitestate grammar learning techniques, su</context>
<context position="10004" citStr="Casacuberta 2000" startWordPosition="1661" endWordPosition="1662">to the stochastic framework (Casacuberta, Vidal, and Pic´o 2004): Theorem 2 A distribution PT : E* x ∆* → [0, 1] is a stochastic rational translation if and only if there exist an alphabet P, two morphisms hΣ : P* → E* and h∆ : P* → ∆*, and a stochastic regular language PL such that, d(s, t) E E* x ∆*, PT(s,t) = 1] PL(W) (5) ω ∈ r* : (hΣ(ω), h∆(ω)) = (s, t) 3.1 Search with Stochastic Finite-State Transducers The search for an optimal ˆt in Equation (4) has proved to be a difficult computational problem (Casacuberta and de la Higuera 2000). In practice, an approximate solution can be obtained (Casacuberta 2000) on the basis of the following approximation to the probability of a translation pair (Viterbi score of a translation): PTP(s, t) ;: VTP(s, t) = max PTP(φ) (6) ¢∈d(s,t) An approximate translation can now be computed as ˜t = argmax VTP(s, t) = argmax max PTP(φ) (7) t∈∆* t∈∆* ¢∈d(s,t) This computation can be carried out efficiently (Casacuberta 1996) by solving the following recurrence by means of dynamic programming: max VTP(s,t) = max (V(jsj,q) - f(q)) (8) t∈∆∗ q∈Q (V(i − 1,q~) - p(q�,si,w,q)) if i =� 0,q =� q0 (9) V(i,q) = max q/∈Q,w∈∆* V(0,q0) = 1 (10) Finally, the approximate translation ˜t</context>
</contexts>
<marker>Casacuberta, 2000</marker>
<rawString>Casacuberta, Francisco. 2000. Inference of finite-state transducers by using regular grammars and morphisms. In Grammatical Inference: Algorithms and Applications (volume 1891 of Lecture Notes in Artificial Intelligence). Springer-Verlag, Berlin and Heidelberg, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Colin de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>In Grammatical Inference: Algorithms and Applications</booktitle>
<volume>1891</volume>
<pages>15--24</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>Casacuberta, Francisco and Colin de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Grammatical Inference: Algorithms and Applications (volume 1891 of Lecture Notes in Artificial Intelligence). Springer-Verlag, Berlin and Heidelberg, pages 15–24.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Francisco Casacuberta</author>
<author>David Llorens</author>
<author>Carlos Martinez</author>
<author>Sirko Molau</author>
<author>Francisco Nevado</author>
<author>Hermann Ney</author>
<author>Moise´es Pastor</author>
<author>David Pic ´o</author>
<author>Alberto Sanchis</author>
<author>Enrique Vidal</author>
<author>Juan-Miguel Vilar</author>
</authors>
<title>Speech-to-speech translation based on finite-state transducers.</title>
<date>2001</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustic, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>613--616</pages>
<publisher>IEEE Press,</publisher>
<location>Piscataway, NJ,</location>
<marker>Casacuberta, Llorens, Martinez, Molau, Nevado, Ney, Pastor, ´o, Sanchis, Vidal, Vilar, 2001</marker>
<rawString>Casacuberta, Francisco, David Llorens, Carlos Martinez, Sirko Molau, Francisco Nevado, Hermann Ney, Moise´es Pastor, David Pic ´o, Alberto Sanchis, Enrique Vidal, and Juan-Miguel Vilar. 2001. Speech-to-speech translation based on finite-state transducers. In Proceedings of the IEEE International Conference on Acoustic, Speech and Signal Processing, volume 1. IEEE Press, Piscataway, NJ, pages 613–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
<author>David Pic ´o</author>
</authors>
<title>Inference of finite-state transducers from regular languages. Pattern Recognition,</title>
<date>2004</date>
<location>forthcoming.</location>
<marker>Casacuberta, Vidal, ´o, 2004</marker>
<rawString>Casacuberta, Francisco, Enrique Vidal, and David Pic ´o. 2004. Inference of finite-state transducers from regular languages. Pattern Recognition, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of EUROSPEECH,</booktitle>
<volume>5</volume>
<pages>2707--2710</pages>
<location>Rhodes,</location>
<contexts>
<context position="22262" citStr="Clarkson and Rosenfeld 1997" startWordPosition="3784" endWordPosition="3787"> (una , a) (camera, room) (la , the) (camera singola , single room) (la , the) (camera, room) Although many other sophisticated transformations can be defined following the above ideas, only the simple L1 will be used in the experiments reported in this article. 4.3 Second Step of the GIATI Methodology: Inferring a Stochastic Regular Grammar from a Set of Strings Many grammatical inference techniques are available to implement the second step of the proposed procedure. In this work, (smoothed) n-grams are used. These models have proven quite successful in many areas such as language modeling (Clarkson and Rosenfeld 1997; Ney, Martin, and Wessel 1997). Figures 4 and 5 show the (nonsmoothed) bigram models inferred from the sample obtained using L1 and L2, respectively, in example 2. Note that the generalization achieved by the first model is greater than that of the second. The probabilities of the n-grams are computed from the corresponding counts in the training set of extended strings. The probability of an extended word zj = (si,¯ti) given the sequence of extended words zi−n+1, . . . , zi−1 = (si−n+1,¯ti−n+1) . . . (si−1,¯ti−1) Figure 4 Bigram model inferred from strings obtained by the transformation L1 i</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU-Cambridge toolkit. In Proceedings of EUROSPEECH, volume 5, pages 2707–2710, Rhodes, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>King-Sun Fu</author>
</authors>
<title>Syntactic pattern recognition and applications. Prentice-Hall,</title>
<date>1982</date>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="1339" citStr="Fu 1982" startWordPosition="179" endWordPosition="180">l relations betweenfinite-state transducers and rational grammars. Given a training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an n-gram) is inferred. This grammar is finally converted into a finite-state transducer. The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTRANS project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recogn</context>
</contexts>
<marker>Fu, 1982</marker>
<rawString>Fu, King-Sun. 1982. Syntactic pattern recognition and applications. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Local languages, the successor method, and a step towards a general methodology for the inference of regular grammars.</title>
<date>1987</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>9</volume>
<issue>6</issue>
<marker>Garcia, Vidal, Casacuberta, 1987</marker>
<rawString>Garcia, Pedro, Enrique Vidal, and Francisco Casacuberta. 1987. Local languages, the successor method, and a step towards a general methodology for the inference of regular grammars. IEEE Transactions on Pattern Analysis and Machine Intelligence, 9(6):841–845.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonz ´alez</author>
<author>Ismael Salvador Jorge</author>
<author>Alejandro Toselli</author>
<author>Alfons Juan</author>
<author>Enrique Vidal</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Offline recognition of syntax-constrained cursive handwritten text.</title>
<date>2000</date>
<booktitle>In Advances in Pattern Recognition (volume 1876 of Lecture Notes in Computer Science).</booktitle>
<pages>143--153</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<marker>´alez, Jorge, Toselli, Juan, Vidal, Casacuberta, 2000</marker>
<rawString>Gonz ´alez, Jorge, Ismael Salvador, Alejandro Toselli, Alfons Juan, Enrique Vidal, and Francisco Casacuberta. 2000. Offline recognition of syntax-constrained cursive handwritten text. In Advances in Pattern Recognition (volume 1876 of Lecture Notes in Computer Science). Springer-Verlag, Berlin and Heidelberg, pages 143–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Hazen</author>
<author>I Lee Hetherington</author>
<author>Alex Park</author>
</authors>
<title>FST-based recognition techniques for multi-lingual and multi-domain spontaneous speech.</title>
<date>2001</date>
<booktitle>In Proceedings of EUROSPEECH2001,</booktitle>
<pages>1591--1594</pages>
<location>Aalborg, Denmark,</location>
<marker>Hazen, Hetherington, Park, 2001</marker>
<rawString>Hazen, Timothy, I. Lee Hetherington, and Alex Park. 2001. FST-based recognition techniques for multi-lingual and multi-domain spontaneous speech. In Proceedings of EUROSPEECH2001, pages 1591–1594, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>FUB ITI</author>
<author>RWTH Aachen</author>
<author>ZERES</author>
</authors>
<title>Example-based language translation systems: Final report.</title>
<date>2000</date>
<tech>Technical Report D0.1c, Instituto Tecnol</tech>
<contexts>
<context position="28467" citStr="ITI et al. 2000" startWordPosition="4859" endWordPosition="4862">consequence, all the search issues discussed in Section 3.1 do apply to GIATI-learned transducers. doppia / double room camera / a, 2 una / a 1 camera / room 3 6 0 la / the 4 camera / a, camera / room 5 singola / single room camera / a, doppia / double room 2 una / a 1 camera / room 0 la / the 4 camera / room camera / a, 3 singola / single room 5 215 Computational Linguistics Volume 30, Number 2 5. Experimental Results Different translation tasks of different levels of difficulty were selected to assess the capabilities of the proposed inference method in the framework of the EuTRANs project (ITI et al. 2000): two Spanish-English tasks (EuTRANs-0 and EuTRANs-I), an Italian-English task (EuTRANs-II) and a Spanish-German task (EuTRANs-Ia). The EuTRANs-0 task, with a large semi-automatically generated training corpus, was used for studying the convergence of transducer learning algorithms for increasingly large training sets (Amengual et al. 2000). In this article it is used to get an estimation of performance limits of the GIATI technique by assuming an unbounded amount of training data. The EuTRANs-I task was similar to EuTRANs-0 but with a more realistically sized training corpus. This corpus was </context>
<context position="30939" citStr="ITI et al. 2000" startWordPosition="5238" endWordPosition="5241">epetitions exhibited the same patterns as those in the corresponding training materials. In all the experiments reported in this article, the approximate optimal translations (equation (7)) of the source test strings were computed and the word error rate (WER), the sentence error rate (SER), and the bilingual evaluation understudy (BLEU) metric for the translations were used as assessment criteria. The WER is the minimum number of substitution, insertion, and deletion operations needed to convert the word string hypothesized by the translation system into a given single reference word string (ITI et al. 2000). The SER is the result of a direct comparison between the hypothesized and reference word strings as a whole. The BLEU metric is based on the n-grams of the hypothesized translation that occur in the reference translations (Papineni et al 2001). The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score). 5.1 The Spanish-English Translation Tasks A Spanish-English corpus was semi-automatically generated in the first phase of the EuTRANs project (Vidal 1997). The domain of the corpus involved typical human-tohuman communication situations at a reception desk of a hotel. A summary of this</context>
<context position="34765" citStr="ITI et al. 2000" startWordPosition="5845" endWordPosition="5848"> worse results. However, L2 is interesting because many of the extended symbols obtained in the experiments involve very good relations between some source word groups and target word groups which could be useful by themselves. Consequently, more research work has to be done with this second type of transformation. The results on the (benchmark) EuTRANS-I corpus can be compared with those obtained using other approaches. GIATI outperforms other finite-state techniques in similar experimental conditions (with a best result of 8.3% WER, using another transducer inference technique called OMEGA [ITI et al. 2000]). On the other hand, the best result achieved by the statistical templates technique (Och and Ney 2000) was 4.4% WER (ITI et al. 2000). However, this result cannot be exactly compared with that achieved by GIATI, because the statistical templates approach used an explicit (automatic) categorization of the source and the target words, while only the raw word forms were used in GIATI. Although GIATI is compatible with different forms of word categorization, the required finite-state expansion is not straightforward, and some work is still needed in order to actually allow this technique to be </context>
<context position="37850" citStr="ITI et al. 2000" startWordPosition="6331" endWordPosition="6334"> different values of n. The training set was (automatically) segmented using a priori knowledge. The statistical alignments were constrained to be within each parallel segment. n-grams States Transitions WER % SER % BLEU 2 6,300 52,385 24.9 93.0 0.62 3 26,194 102,941 25.5 93.3 0.61 4 56,856 164,972 25.5 93.3 0.61 This corpus contained many long sentences, most of which were composed of rather short segments connected by punctuation marks. Typically, these segments can be monotonically aligned with corresponding target segments using a simple dynamic programming procedure (prior segmentation) (ITI et al. 2000). We explored computing the statistical alignments within each pair of segments rather than in the entire sentences. Since the segments were shorter than the whole sentences, the alignment probability distributions were better estimated. In the training phase, extended symbols were built from these alignments, and the strings of extended symbols corresponding to the segments of the same original string pair were concatenated. Test sentences were directly used, without any kind of segmentation. The translation results using prior segmentation are reported in Table 6. These results were clearly </context>
</contexts>
<marker>ITI, Aachen, ZERES, 2000</marker>
<rawString>ITI, FUB, RWTH Aachen, and ZERES. 2000. Example-based language translation systems: Final report. Technical Report D0.1c, Instituto Tecnol ´ogico de Inform ´atica, Fondazione Ugo Bordoni, Rheinisch Westf¨alische Technische Hochschule Aachen Lehrstuhl f¨ur Informatik V and Zeres GmbH Bochum. Information Technology. Long Term Research Domain. Open scheme.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourth. AMTA Conference</booktitle>
<volume>1529</volume>
<pages>421--437</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="3293" citStr="Knight and Al-Onaizan 1998" startWordPosition="445" endWordPosition="448">ns for the interest in finite-state machines for language translation comes from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina, * Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 2 Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from finite sets of learning strings which have been used successfully in a number of fields, including automatic speech recognition (Vidal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simp</context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>Knight, Kevin and Yaser Al-Onaizan. 1998. Translation with finite-state devices. In Proceedings of the Fourth. AMTA Conference (volume 1529 of Lecture Notes in Artificial Intelligence). Springer-Verlag, Berlin and Heidelberg, pages 421–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Llorens</author>
<author>Juna-Miguel Vilar</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Finite state language models smoothed using n-grams.</title>
<date>2002</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>16</volume>
<issue>3</issue>
<marker>Llorens, Vilar, Casacuberta, 2002</marker>
<rawString>Llorens, David, Juna-Miguel Vilar, and Francisco Casacuberta. 2002. Finite state language models smoothed using n-grams. International Journal of Pattern Recognition and Artificial Intelligence, 16(3):275–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erkki M¨akinen</author>
</authors>
<title>Inferring finite transducers.</title>
<date>1999</date>
<tech>Technical Report A-1999-3,</tech>
<institution>University of Tampere,</institution>
<location>Tampere, Finland.</location>
<marker>M¨akinen, 1999</marker>
<rawString>M¨akinen, Erkki. 1999. Inferring finite transducers. Technical Report A-1999-3, University of Tampere, Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="1418" citStr="Mohri 1997" startWordPosition="190" endWordPosition="191">training corpus of source-target pairs of sentences, the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar (e.g., an n-gram) is inferred. This grammar is finally converted into a finite-state transducer. The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTRANS project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mohri, Mehryar. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2):1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaolong Mou</author>
<author>Stephanie Seneff</author>
<author>Victor Zue</author>
</authors>
<title>Context-dependent probabilistic hierarchical sub-lexical modelling using finite-state transducers.</title>
<date>2001</date>
<booktitle>In Proceedings of EUROSPEECH2001,</booktitle>
<pages>451--454</pages>
<location>Aalborg, Denmark,</location>
<marker>Mou, Seneff, Zue, 2001</marker>
<rawString>Mou, Xiaolong, Stephanie Seneff, and Victor Zue. 2001. Context-dependent probabilistic hierarchical sub-lexical modelling using finite-state transducers. In Proceedings of EUROSPEECH2001, pages 451–454, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hermann Ney</author>
</authors>
<location>Sven Martin, and</location>
<marker>Ney, </marker>
<rawString>Ney, Hermann, Sven Martin, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wessel</author>
</authors>
<title>Statistical language modeling using leaving-one-out.</title>
<date>1997</date>
<booktitle>Corpus-Based Statiscal Methods in Speech and Language Processing. Kluwer Academic, Dordrecht, the Netherlands,</booktitle>
<pages>174--207</pages>
<editor>In S. Young and G. Bloothooft, editors,</editor>
<contexts>
<context position="13625" citStr="Wessel 1997" startWordPosition="2307" endWordPosition="2308">osed in Garc´ıa, Vidal, and Casacuberta (1987) for the inference of regular grammars, is illustrated in Figure 2. The first transformation is modeled by the labeling function G : Σ* x ∆* → Γ*, while the last transformation is carried out by an “inverse labeling function” Λ(·), that is, one such that Λ(G(A)) = A. Following Theorems 1 and 2, Λ(·) consists of a couple of morphisms, hr, ho, such that for a string z E Γ*, Λ(z) = (hr(z), ho(z)). Without loss of generality, we assume that the method used in the second step of the proposed method consists of the inference of n-grams (Ney, Martin, and Wessel 1997) with final states, which are particular cases of stochastic regular grammars. This simple method automatically derives, from the strings in S, both the structure of 9 (i.e., the rules—states and transitions) and the associated probabilities. Since Λ is typically the inverse of G, the morphisms hr and ho needed in the third step of the proposed approach are determined by the definition of G. So a key 209 Computational Linguistics Volume 30, Number 2 Figure 2 Basic scheme for the inference of finite-state transducers. A is a finite sample of training pairs. S is the finite sample of strings obt</context>
<context position="22293" citStr="Wessel 1997" startWordPosition="3791" endWordPosition="3792">ingola , single room) (la , the) (camera, room) Although many other sophisticated transformations can be defined following the above ideas, only the simple L1 will be used in the experiments reported in this article. 4.3 Second Step of the GIATI Methodology: Inferring a Stochastic Regular Grammar from a Set of Strings Many grammatical inference techniques are available to implement the second step of the proposed procedure. In this work, (smoothed) n-grams are used. These models have proven quite successful in many areas such as language modeling (Clarkson and Rosenfeld 1997; Ney, Martin, and Wessel 1997). Figures 4 and 5 show the (nonsmoothed) bigram models inferred from the sample obtained using L1 and L2, respectively, in example 2. Note that the generalization achieved by the first model is greater than that of the second. The probabilities of the n-grams are computed from the corresponding counts in the training set of extended strings. The probability of an extended word zj = (si,¯ti) given the sequence of extended words zi−n+1, . . . , zi−1 = (si−n+1,¯ti−n+1) . . . (si−1,¯ti−1) Figure 4 Bigram model inferred from strings obtained by the transformation L1 in example 2. (doppia , double r</context>
</contexts>
<marker>Wessel, 1997</marker>
<rawString>Frank Wessel. 1997. Statistical language modeling using leaving-one-out. In S. Young and G. Bloothooft, editors, Corpus-Based Statiscal Methods in Speech and Language Processing. Kluwer Academic, Dordrecht, the Netherlands, pages 174–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz-Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong,</location>
<contexts>
<context position="17027" citStr="Och and Ney 2000" startWordPosition="2867" endWordPosition="2870">by an alignment a is Pr(t, a |s). Thus, an optimal alignment between s and t can be computed as aˆ = argmax Pr(t, a |s) (11) a∈A(s,t) 5 In previous work, this idea was often called morphic generator transducer inference. 210 Casacuberta and Vidal Translation with Finite-State Transducers Different approaches for estimating Pr(t, a I s) were proposed in Brown et al. (1993). These approaches are known as models 1 through 5. Adequate software packages are publicly available for training these statistical models and for obtaining good alignments between pairs of sentences (Al-Onaizan et al. 1999; Och and Ney 2000). An example of Spanish-English sentence alignment is given below: Example 1 ¿ Cu´anto cuesta una habitaci´on individual por semana ? how (2) much (2) does (3) a (4) single (6) room (5) cost (3) per (7) week (8) ? (9) Each number within parentheses in the example represents the position in the source sentence that is aligned with the (position of the) preceding target word. A graphical representation of this alignment is shown in Figure 3. 4.2 First Step of the GIATI Methodology: Transformation of Training Pairs into Strings The first step of the proposed method consists in a labeling process </context>
<context position="33921" citStr="Och and Ney 2000" startWordPosition="5718" endWordPosition="5721">,027 3.1 16.4 0.96 12 492,620 1,485,370 3.1 16.4 0.96 Table 3 Results with the standard corpus EUTRANS-I. The underlying regular models were smoothed n-grams for different values of n. n-grams States Transitions WER % SER % BLEU 2 1,696 17,121 9.0 53.7 0.86 3 8,562 36,763 6.7 38.9 0.90 4 21,338 64,856 6.7 37.9 0.91 5 23,879 72,006 6.6 37.1 0.91 6 25,947 77,531 6.6 37.0 0.91 7 27,336 81,076 6.6 37.0 0.91 217 Computational Linguistics Volume 30, Number 2 for EuTRANS-0 and 6.6% WER for EuTRANS-I. These results were achieved using the statistical alignments provided by model 5 (Brown et al. 1993; Och and Ney 2000) and smoothed 11-grams and 6-grams, respectively. These results were obtained using the first type of transformation described in Section 4.2 (L1). Similar experiments with the second type of transformation (L2) produced slightly worse results. However, L2 is interesting because many of the extended symbols obtained in the experiments involve very good relations between some source word groups and target word groups which could be useful by themselves. Consequently, more research work has to be done with this second type of transformation. The results on the (benchmark) EuTRANS-I corpus can be</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och, Franz-Josef and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos ´e Oncina</author>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
</authors>
<title>Learning subsequential transducers for pattern recognition interpretation tasks.</title>
<date>1993</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>15</volume>
<issue>5</issue>
<marker>Oncina, Garcia, Vidal, 1993</marker>
<rawString>Oncina, Jos ´e, Pedro Garcia, and Enrique Vidal. 1993. Learning subsequential transducers for pattern recognition interpretation tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(5):448–458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Technical Report RC22176(W0109-022),</tech>
<institution>IBM Research Division,</institution>
<location>Yorktown Heights, NY,</location>
<contexts>
<context position="31184" citStr="Papineni et al 2001" startWordPosition="5279" endWordPosition="5282">rd error rate (WER), the sentence error rate (SER), and the bilingual evaluation understudy (BLEU) metric for the translations were used as assessment criteria. The WER is the minimum number of substitution, insertion, and deletion operations needed to convert the word string hypothesized by the translation system into a given single reference word string (ITI et al. 2000). The SER is the result of a direct comparison between the hypothesized and reference word strings as a whole. The BLEU metric is based on the n-grams of the hypothesized translation that occur in the reference translations (Papineni et al 2001). The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score). 5.1 The Spanish-English Translation Tasks A Spanish-English corpus was semi-automatically generated in the first phase of the EuTRANs project (Vidal 1997). The domain of the corpus involved typical human-tohuman communication situations at a reception desk of a hotel. A summary of this corpus (EuTRANs-0) is given in Table 1 (Amengual et al 2000; Casacuberta et al. 2001). From this (large) corpus, a small subset of ten thousand training sentence pairs (EuTRANs-I) was randomly selected in order to approach more realistic traini</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. Technical Report RC22176(W0109-022), IBM Research Division, Yorktown Heights, NY, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the ARPA Spoken Language Technology Workshop,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="23467" citStr="Rosenfeld 1995" startWordPosition="4000" endWordPosition="4001">ation L1 in example 2. (doppia , double room) 3 (camera , λ) 2 (una , a) 1 6 (camera , room) 0 (la , the) 4 (camera , λ) (singola , single room) (camera , room) 5 213 Computational Linguistics Volume 30, Number 2 Figure 5 Bigram model inferred from strings obtained by the transformation L2 in example 2. is estimated as c(zi−n+1, . . . , zi−1, zi) pn(zi |zi−n+1 ... zi−1) = (12) c(zi−n+1, . . . , zi−1) where c(·) is the number of times that an event occurs in the training set. To deal with unseen n-grams, the back-off smoothing technique from the CMU Statistical Language Modeling (SLM) Toolkit (Rosenfeld 1995) has been used. The (smoothed) n-gram model obtained from the set of extended symbols is represented as a stochastic finite-state automaton (Llorens, Vilar, and Casacuberta 2002). The states of the automaton are the observed (n − 1)-grams. For the n-gram (zi−n+1 ... zi), there is a transition from state (zi−n+1 ... zi−1) to state (zi−n+2 ... zi) with the associated extended word zi and a probability pn(zi |zi−n+1 ... zi−1). The back-off smoothing method supplied by the SLM Toolkit is represented by the states corresponding to kgrams (k &lt; n) and by special transitions between k-gram states and </context>
<context position="36944" citStr="Rosenfeld 1995" startWordPosition="6191" endWordPosition="6192">e used for EuTRANS-II. The results are reported in Table 5. Table 4 The EuTRANS-II corpus. There was a small overlap of seven pairs between the training and test sets, but 107 source words in the test set were not in the (training-set-derived) vocabulary. Italian English Train: Sentence pairs 3,038 Running words 55,302 64,176 Vocabulary 2,459 1,712 Test: Sentences 300 Running words 6,121 7,243 Bigram test perplexity 31 25 218 Casacuberta and Vidal Translation with Finite-State Transducers Table 5 Results with the standard EuTRANS-II corpus. The underlying regular models were smoothed n-grams (Rosenfeld 1995) for different values of n. n-grams States Transitions WER % SER % BLEU 2 5,909 49,701 27.2 96.7 0.56 3 24,852 97,893 27.3 96.0 0.56 4 54,102 157,073 27.4 96.0 0.56 Table 6 Results with the standard EuTRANS-II corpus. The underlying regular models were smoothed n-grams (Rosenfeld 1994) for different values of n. The training set was (automatically) segmented using a priori knowledge. The statistical alignments were constrained to be within each parallel segment. n-grams States Transitions WER % SER % BLEU 2 6,300 52,385 24.9 93.0 0.62 3 26,194 102,941 25.5 93.3 0.61 4 56,856 164,972 25.5 93.3 </context>
</contexts>
<marker>Rosenfeld, 1995</marker>
<rawString>Rosenfeld, Ronald. 1995. The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation. In Proceedings of the ARPA Spoken Language Technology Workshop, Princeton, NJ. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RWTH Aachen</author>
<author>ITI</author>
</authors>
<title>Statistical modeling techniques and results and search techniques and results.</title>
<date>1999</date>
<tech>Technical Report D3.1a and D3.2a,</tech>
<contexts>
<context position="39275" citStr="Aachen and ITI (1999)" startWordPosition="6553" endWordPosition="6556">rformance is obtained with a lower-order n-gram. One obvious reason for this behavior is that this corpus is far more spontaneous than the first one, and consequently, it has a much higher degree of variability. Moreover, the training data set is about three times smaller than the corresponding data of EuTRANS-I, while the vocabularies are three to four times larger. The best result achieved with the proposed technique on EuTRANS-II was 24.9% WER, using prior segmentation of the training pairs and a smoothed bigram model. This result was comparable to the best among all those reported in RWTH Aachen and ITI (1999). The previously mentioned statistical templates technique achieved 25.1% WER in this case. In this application, in which categories are not as important as in EuTRANS-I, statistical templates and GIATI achieved similar results. 219 Computational Linguistics Volume 30, Number 2 Table 7 The Spanish-German corpus. There was no overlap between training and test sets and no out-of-vocabulary words in the test set. Spanish German Train: Sentence pairs 10,000 Distinct pairs 6,636 Running words 96,043 90,099 Vocabulary 6,622 4,890 Test: Sentences 2,862 Running words 33,542 31,103 Bigram test perplexi</context>
</contexts>
<marker>Aachen, ITI, 1999</marker>
<rawString>RWTH Aachen and ITI. 1999. Statistical modeling techniques and results and search techniques and results. Technical Report D3.1a and D3.2a,</rawString>
</citation>
<citation valid="false">
<institution>Rheinisch Westf¨alische Technische Hochschule</institution>
<marker></marker>
<rawString>Rheinisch Westf¨alische Technische Hochschule</rawString>
</citation>
<citation valid="false">
<title>Aachen Lehrstuhl f¨ur Informatik VI and Instituto Tecnol ´ogico de Inform ´atica. Information Technology. Long Term Research Domain. Open scheme.</title>
<marker></marker>
<rawString>Aachen Lehrstuhl f¨ur Informatik VI and Instituto Tecnol ´ogico de Inform ´atica. Information Technology. Long Term Research Domain. Open scheme.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Encarna Segarra</author>
<author>Maria-Isabel Galiano Emilio Sanchis</author>
<author>Fernando Garcia</author>
<author>Luis Hurtado</author>
</authors>
<title>Extracting semantic information through automatic learning techniques.</title>
<date>2001</date>
<booktitle>In Proceedings of the Spanish Symposium on Pattern Recognition and Image Analysis,</booktitle>
<pages>177--182</pages>
<location>Benicasim, Spain,</location>
<contexts>
<context position="1882" citStr="Segarra et al. 2001" startWordPosition="255" endWordPosition="258">ive rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. These transductions are realized by the so-called finite-state transducers. Even though other, more powerful transduction </context>
</contexts>
<marker>Segarra, Sanchis, Garcia, Hurtado, 2001</marker>
<rawString>Segarra, Encarna, Maria-Isabel Galiano Emilio Sanchis, Fernando Garcia, and Luis Hurtado. 2001. Extracting semantic information through automatic learning techniques. In Proceedings of the Spanish Symposium on Pattern Recognition and Image Analysis, pages 177–182, Benicasim, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Seward</author>
</authors>
<title>Transducer optimizations for tight-coupled decoding.</title>
<date>2001</date>
<booktitle>In Proceedings of EUROSPEECH2001,</booktitle>
<pages>1607--1610</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="1896" citStr="Seward 2001" startWordPosition="259" endWordPosition="260">ant framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. These transductions are realized by the so-called finite-state transducers. Even though other, more powerful transduction models exist, </context>
</contexts>
<marker>Seward, 2001</marker>
<rawString>Seward, Alexander. 2001. Transducer optimizations for tight-coupled decoding. In Proceedings of EUROSPEECH2001, pages 1607–1610, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
</authors>
<title>Finite-state speech-to-speech translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Acoustic Speech and Signal Processing,</booktitle>
<pages>111--114</pages>
<publisher>IEEE Press,</publisher>
<location>Munich.</location>
<contexts>
<context position="1762" citStr="Vidal 1997" startWordPosition="238" endWordPosition="239">chine translation experiments within the framework of the EuTRANS project. 1. Introduction Formal transducers give rise to an important framework in syntactic-pattern recognition (Fu 1982; Vidal, Casacuberta, and Garc´ıa 1995) and in language processing (Mohri 1997). Many tasks in automatic speech recognition can be viewed as simple translations from acoustic sequences to sublexical or lexical sequences (acoustic-phonetic decoding) or from acoustic or lexical sequences to query strings (for database access) or (robot control) commands (semantic decoding) (Vidal, Casacuberta, and Garc´ıa 1995; Vidal 1997; Bangalore and Ricardi 2000a, 2000b; Hazen, Hetherington, and Park 2001; Mou, Seneff, and Zue 2001; Segarra et al. 2001; Seward 2001). Another similar application is the recognition of continuous hand-written characters (Gonz´alez et al. 2000). Yet a more complex application of formal transducers is language translation, in which input and output can be text, speech, (continuous) handwritten text, etc. (Mohri 1997; Vidal 1997; Bangalore and Ricardi 2000b, 2001; Amengual et al. 2000). Rational transductions (Berstel 1979) constitute an important class within the field of formal translation. Th</context>
<context position="31407" citStr="Vidal 1997" startWordPosition="5314" endWordPosition="5315">operations needed to convert the word string hypothesized by the translation system into a given single reference word string (ITI et al. 2000). The SER is the result of a direct comparison between the hypothesized and reference word strings as a whole. The BLEU metric is based on the n-grams of the hypothesized translation that occur in the reference translations (Papineni et al 2001). The BLEU metric ranges from 0.0 (worst score) to 1.0 (best score). 5.1 The Spanish-English Translation Tasks A Spanish-English corpus was semi-automatically generated in the first phase of the EuTRANs project (Vidal 1997). The domain of the corpus involved typical human-tohuman communication situations at a reception desk of a hotel. A summary of this corpus (EuTRANs-0) is given in Table 1 (Amengual et al 2000; Casacuberta et al. 2001). From this (large) corpus, a small subset of ten thousand training sentence pairs (EuTRANs-I) was randomly selected in order to approach more realistic training conditions (see also Table 1). From these data, completely disjoint training and test sets were defined. It was guaranteed, however, that all the words in the source test sentences were contained in both training sets (c</context>
</contexts>
<marker>Vidal, 1997</marker>
<rawString>Vidal, Enrique. 1997. Finite-state speech-to-speech translation. In Proceedings of the International Conference on Acoustic Speech and Signal Processing, Munich. IEEE Press, Piscataway, NJ, pages 111–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Francisco Casacuberta</author>
<author>Pedro Garcia</author>
</authors>
<title>Grammatical inference and automatic speech recognition.</title>
<date>1995</date>
<booktitle>New Advances and Trends in Speech Recognition and Coding (volume 147 of NATO-ASI Series F: Computer and Systems Sciences).</booktitle>
<pages>174--191</pages>
<editor>In A. Rubio, editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<marker>Vidal, Casacuberta, Garcia, 1995</marker>
<rawString>Vidal, Enrique, Francisco Casacuberta, and Pedro Garcia. 1995. Grammatical inference and automatic speech recognition. In A. Rubio, editor, New Advances and Trends in Speech Recognition and Coding (volume 147 of NATO-ASI Series F: Computer and Systems Sciences). Springer-Verlag, Berlin and Heidelberg, pages 174–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Vidal</author>
<author>Pedro Garcia</author>
<author>Encarna Segarra</author>
</authors>
<title>Inductive learning of finite-state transducers for the interpretation of unidimensional objects.</title>
<date>1989</date>
<pages>17--35</pages>
<editor>In R. Mohr, T. Pavlidis, and A. Sanfeliu, editors,</editor>
<marker>Vidal, Garcia, Segarra, 1989</marker>
<rawString>Vidal, Enrique, Pedro Garcia, and Encarna Segarra. 1989. Inductive learning of finite-state transducers for the interpretation of unidimensional objects. In R. Mohr, T. Pavlidis, and A. Sanfeliu, editors, Structural Pattern Analysis. World Scientific, Singapore, pages 17–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan-Miguel Vilar</author>
</authors>
<title>Improve the learning of subsequential transducers by using alignments and dictionaries.</title>
<date>2000</date>
<booktitle>In Grammatical Inference: Algorithms and Applications</booktitle>
<volume>1891</volume>
<pages>298--312</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="3353" citStr="Vilar 2000" startWordPosition="455" endWordPosition="456"> from the fact that these machines can be learned automatically from examples (Vidal, Casacuberta, and Garc´ıa 1995). Nowadays, only a few techniques exist for inferring finite-state transducers (Vidal, Garc´ıa, and Segarra 1989; Oncina, * Departamento de Sistemas Inform´aticos y Computaci´on, Instituto Tecnol´ogico de Inform´atica, 46071 Valencia, Spain. E-mail:{fcn, evidal}@iti.upv.es. © 2004 Association for Computational Linguistics Computational Linguistics Volume 30, Number 2 Garcfa, and Vidal 1993; M¨akinen 1999; Knight and Al-Onaizan 1998; Bangalore and Ricardi 2000b; Casacuberta 2000; Vilar 2000). Nevertheless, there are many techniques for inferring regular grammars from finite sets of learning strings which have been used successfully in a number of fields, including automatic speech recognition (Vidal, Casacuberta, and Garcfa 1995). Some of these techniques are based on results from formal language theory. In particular, complex regular grammars can be built by inferring simple grammars that recognize local languages (Garcfa, Vidal, and Casacuberta 1987). Here we explore this idea further and propose methods that use (simple) finitestate grammar learning techniques, such as n-gram </context>
</contexts>
<marker>Vilar, 2000</marker>
<rawString>Vilar, Juan-Miguel. 2000. Improve the learning of subsequential transducers by using alignments and dictionaries. In Grammatical Inference: Algorithms and Applications (volume 1891 of Lecture Notes in Artificial Intelligence). Springer-Verlag, Berlin and Heidelberg, pages 298–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan-Miguel Vilar</author>
<author>Enrique Vidal</author>
<author>Juan-Carlos Amengual</author>
</authors>
<title>Learning extended finite state models for language translation.</title>
<date>1996</date>
<booktitle>Proceedings of the Extended Finite State Models of Language Workshop,</booktitle>
<pages>92--96</pages>
<editor>In Andr´as Kornai, editor,</editor>
<location>Budapest,</location>
<marker>Vilar, Vidal, Amengual, 1996</marker>
<rawString>Vilar, Juan-Miguel, Enrique Vidal, and Juan-Carlos Amengual. 1996. Learning extended finite state models for language translation. In Andr´as Kornai, editor, Proceedings of the Extended Finite State Models of Language Workshop, pages 92–96, Budapest, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>