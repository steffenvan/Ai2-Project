<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99597">
Probabilistic Frame-Semantic Parsing
</title>
<author confidence="0.998369">
Dipanjan Das Nathan Schneider Desai Chen Noah A. Smith
</author>
<affiliation confidence="0.910484">
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.803187">
{dipanjan@cs,nschneid@cs,desaic@andrew,nasmith@cs}.cmu.edu
</email>
<sectionHeader confidence="0.994504" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922230769231">
This paper contributes a formalization of
frame-semantic parsing as a structure predic-
tion problem and describes an implemented
parser that transforms an English sentence
into a frame-semantic representation. It finds
words that evoke FrameNet frames, selects
frames for them, and locates the arguments
for each frame. The system uses two feature-
based, discriminative probabilistic (log-linear)
models, one with latent variables to permit
disambiguation of new predicate words. The
parser is demonstrated to significantly outper-
form previously published results.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901166666667">
FrameNet (Fillmore et al., 2003) is a rich linguistic
resource containing considerable information about
lexical and predicate-argument semantics in En-
glish. Grounded in the theory of frame semantics
(Fillmore, 1982), it suggests—but does not formally
define—a semantic representation that blends word-
sense disambiguation and semantic role labeling.
In this paper, we present a computational and
statistical model for frame-semantic parsing, the
problem of extracting from text semantic predicate-
argument structures such as those shown in Fig. 1.
We aim to predict a frame-semantic representation
as a structure, not as a pipeline of classifiers. We
use a probabilistic framework that cleanly integrates
the FrameNet lexicon and (currently very limited)
available training data. Although our models often
involve strong independence assumptions, the prob-
abilistic framework we adopt is highly amenable to
future extension through new features, relaxed in-
dependence assumptions, and semisupervised learn-
ing. Some novel aspects of our current approach
include a latent-variable model that permits disam-
biguation of words not in the FrameNet lexicon, a
unified model for finding and labeling arguments,
</bodyText>
<figureCaption confidence="0.90011125">
Figure 2. Partial illustration of frames, roles, and LUs
related to the CAUSE TO MAKE NOISE frame, from the
FrameNet lexicon. “Core” roles are filled ovals. 8 addi-
tional roles of CAUSE TO MAKE NOISE are not shown.
</figureCaption>
<bodyText confidence="0.9985285">
and a precision-boosting constraint that forbids ar-
guments of the same predicate to overlap. Our parser
achieves the best published results to date on the
SemEval’07 FrameNet task (Baker et al., 2007).
</bodyText>
<sectionHeader confidence="0.988821" genericHeader="introduction">
2 Resources and Task
</sectionHeader>
<bodyText confidence="0.948472">
We consider frame-semantic parsing resources.
</bodyText>
<subsectionHeader confidence="0.921006">
2.1 FrameNet Lexicon
</subsectionHeader>
<bodyText confidence="0.999977928571429">
The FrameNet lexicon is a taxonomy of manu-
ally identified general-purpose frames for English.1
Listed in the lexicon with each frame are several
lemmas (with part of speech) that can denote the
frame or some aspect of it—these are called lexi-
cal units (LUs). In a sentence, word or phrase to-
kens that evoke a frame are known as targets. The
set of LUs listed for a frame in FrameNet may not
be exhaustive; we may see a target in new data that
does not correspond to an LU for the frame it evokes.
Each frame definition also includes a set of frame el-
ements, or roles, corresponding to different aspects
of the concept represented by the frame, such as par-
ticipants, props, and attributes. We use the term ar-
</bodyText>
<footnote confidence="0.692877">
1Like the SemEval’07 participants, we used FrameNet v. 1.3
(http://framenet.icsi.berkeley.edu).
</footnote>
<figure confidence="0.998760178571429">
TRANSITIVE_
ACTION
Patient
Cause
Agent
Event
Place
Time
blare.v, play.v,
Ñring.v, toot.v, ...
Inheritance relation Causative_of relation
Excludes relation
CAUSE_TO_
MAKE_NOISE
Sound_maker
Purpose
Cause
Agent
Place
Time
cough.v, gobble.v,
ring.v, yodel.v, ...
MAKE_NOISE
Sound_source
Noisy_event
Sound
Place
Time
</figure>
<page confidence="0.945533">
948
</page>
<note confidence="0.788283">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 948–956,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.966030266666667">
But there still are n&apos;t enough ringers to ring more than six of the eight bells . Frame LU
NOISE-MAKERS
N_m
CAUSE-TO-MAKE-NOISE
bell.n
ring.v
Agent
enough.a
SUFFICIENCY
Item
Sound_maker
Enabled_situation
EXISTENCE
there be.v
Entity
</figure>
<figureCaption confidence="0.9997086">
Figure 1. A sentence from PropBank and the SemEval’07 training data, and a partial depiction of gold FrameNet
annotations. Each frame is a row below the sentence (ordered for readability). Thick lines indicate targets that evoke
frames; thin solid/dotted lines with labels indicate arguments. “N m” under bells is short for the Noise maker role of
the NOISE MAKERS frame. The last row indicates that there... are is a discontinuous target. In PropBank, the verb
ring is the only annotated predicate for this sentence, and it is not related to other predicates with similar meanings.
</figureCaption>
<table confidence="0.458122">
FRAMENET LEXICON V. 1.3
lexical exemplars
</table>
<tableCaption confidence="0.845810285714286">
entries counts coverage
8379 LUs 139K sentences, 3.1M words 70% LUs
795 frames 1 frame annotation/ sentence 63% frames
7124 roles 285K overt arguments 56% roles
Table 1. Snapshot of lexicon entries and exemplar sen-
tences. Coverage indicates the fraction of types attested
in at least one exemplar.
</tableCaption>
<bodyText confidence="0.99997947826087">
gument to refer to a sequence of word tokens anno-
tated as filling a frame role. Fig. 1 shows an exam-
ple sentence from the training data with annotated
targets, LUs, frames, and role-argument pairs. The
FrameNet lexicon also provides information about
relations between frames and between roles (e.g.,
INHERITANCE). Fig. 2 shows a subset of the rela-
tions between three frames and their roles.
Accompanying most frame definitions in the
FrameNet lexicon is a set of lexicographic exemplar
sentences (primarily from the British National Cor-
pus) annotated for that frame. Typically chosen to il-
lustrate variation in argument realization patterns for
the frame in question, these sentences only contain
annotations for a single frame. We found that using
exemplar sentences directly to train our models hurt
performance as evaluated on SemEval’07 data, even
though the number of exemplar sentences is an order
of magnitude larger than the number of sentences in
our training set (§2.2). This is presumably because
the exemplars are neither representative as a sample
nor similar to the test data. Instead, we make use of
these exemplars in features (§4.2).
</bodyText>
<subsectionHeader confidence="0.994926">
2.2 Data
</subsectionHeader>
<bodyText confidence="0.999856">
Our training, development, and test sets consist
of documents annotated with frame-semantic struc-
tures for the SemEval’07 task, which we refer to col-
</bodyText>
<equation confidence="0.546008066666667">
FULL-TEXT SemEval’07 data
ANNOTATIONS train dev test
Size (words sentences documents)
all 43.3K1.7K 22 6.3K 251 4 2.8K 120 3
ANC (travel) 3.9K 154 2 .8K 32 1 1.3K 67 1
NTI (bureaucratic) 32.2K1.2K 15 5.5K 219 3 1.5K 53 2
PropBank (news) 7.3K 325 5 0 0 0 0 0 0
Annotations (frames/word overt arguments/word)
all 0.23 0.39 0.22 0.37 0.370.65
Coverage of lexicon (% frames % roles % LUs)
all 64.127.421.0 34.0 10.2 7.3 29.3 7.7 4.9
Out-of-lexicon types (frames roles LUs)
all 14 69 71 2 4 2 39 99 189
Out-of-lexicon tokens (% frames % roles % LUs)
all 0.7 0.9 1.1 1.0 0.4 0.2 9.811.2 25.3
</equation>
<tableCaption confidence="0.98045">
Table 2. Snapshot of the SemEval’07 annotated data.
</tableCaption>
<bodyText confidence="0.999936588235294">
lectively as the SemEval’07 data.2 For the most
part, the frames and roles used in annotating these
documents were defined in the FrameNet lexicon,
but there are some exceptions for which the annota-
tors defined supplementary frames and roles; these
are included in the possible output of our parser.
Table 2 provides a snapshot of the SemEval’07
data. We randomly selected three documents from
the original SemEval training data to create a devel-
opment set for tuning model hyperparameters. No-
tice that the test set contains more annotations per
word, both in terms of frames and arguments. More-
over, there are many more out-of-lexicon frame,
role, and LU types in the test set than in the training
set. This inconsistency in the data results in poor re-
call scores for all models trained on the given data
split, a problem we have not sought to address here.
</bodyText>
<footnote confidence="0.9763495">
2http://framenet.icsi.berkeley.edu/
semeval/FSSE.html
</footnote>
<page confidence="0.998814">
949
</page>
<bodyText confidence="0.9997276">
Preprocessing. We preprocess sentences in our
dataset with a standard set of annotations: POS
tags from MXPOST (Ratnaparkhi, 1996) and depen-
dency parses from the MST parser (McDonald et al.,
2005) since manual syntactic parses are not available
for most of the FrameNet-annotated documents. We
used WordNet (Fellbaum, 1998) for lemmatization.
We also labeled each verb in the data as having AC-
TIVE or PASSIVE voice, using code from the SRL
system described by Johansson and Nugues (2008).
</bodyText>
<subsectionHeader confidence="0.999539">
2.3 Task and Evaluation
</subsectionHeader>
<bodyText confidence="0.999805625">
Automatic annotations of frame-semantic structure
can be broken into three parts: (1) targets, the words
or phrases that evoke frames; (2) the frame type,
defined in the lexicon, evoked by each target; and
(3) the arguments, or spans of words that serve to
fill roles defined by each evoked frame. These cor-
respond to the three subtasks in our parser, each
described and evaluated in turn: target identifica-
tion (§3), frame identification (§4, not unlike word-
sense disambiguation), and argument identification
(§5, not unlike semantic role labeling).
The standard evaluation script from the
SemEval’07 shared task calculates precision,
recall, and FI-measure for frames and arguments;
it also provides a score that gives partial credit
for hypothesizing a frame related to the correct
one. We present precision, recall, and FI-measure
microaveraged across the test documents, report
labels-only matching scores (spans must match
exactly), and do not use named entity labels. More
details can be found in Baker et al. (2007). For our
experiments, statistical significance is measured us-
ing a reimplementation of Dan Bikel’s randomized
parsing evaluation comparator.3
</bodyText>
<subsectionHeader confidence="0.990221">
2.4 Baseline
</subsectionHeader>
<bodyText confidence="0.999975125">
A strong baseline for frame-semantic parsing is
the system presented by Johansson and Nugues
(2007, hereafter J&amp;N’07), the best system in the
SemEval’07 shared task. For frame identifica-
tion, they used an SVM classifier to disambiguate
frames for known frame-evoking words. They used
WordNet synsets to extend the vocabulary of frame-
evoking words to cover unknown words, and then
</bodyText>
<footnote confidence="0.974937">
3http://www.cis.upenn.edu/˜dbikel/
software.html#comparator
</footnote>
<table confidence="0.990932333333333">
TARGET IDENTIFICATION P R Fl
Our technique (§3) 89.92 70.79 79.21
Baseline: J&amp;N’07 87.87 67.11 76.10
</table>
<tableCaption confidence="0.88812">
Table 3. Target identification results for our system and
the baseline. Scores in bold denote significant improve-
ments over the baseline (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.99992375">
used a collection of separate SVM classifiers—one
for each frame—to predict a single evoked frame for
each occurrence of a word in the extended set.
J&amp;N’07 modeled the argument identification
problem by dividing it into two tasks: first, they
classified candidate spans as to whether they were
arguments or not; then they assigned roles to those
that were identified as arguments. Both phases used
SVMs. Thus, their formulation of the problem in-
volves a multitude of classifiers—whereas ours uses
two log-linear models, each with a single set of
weights, to find a full frame-semantic parse.
</bodyText>
<sectionHeader confidence="0.993866" genericHeader="method">
3 Target Identification
</sectionHeader>
<bodyText confidence="0.99999">
Target identification is the problem of deciding
which word tokens (or word token sequences) evoke
frames in a given sentence. In other semantic role
labeling schemes (e.g. PropBank), simple part-of-
speech criteria typically distinguish predicates from
non-predicates. But in frame semantics, verbs,
nouns, adjectives, and even prepositions can evoke
frames under certain conditions. One complication
is that semantically-impoverished support predi-
cates (such as make in make a request) do not
evoke frames in the context of a frame-evoking,
syntactially-dependent noun (request). Further-
more, only temporal, locative, and directional senses
of prepositions evoke frames.
We found that, because the test set is more com-
pletely annotated—that is, it boasts far more frames
per token than the training data (see Table 2)—
learned models did not generalize well and achieved
poor test recall. Instead, we followed J&amp;N’07 in us-
ing a small set of rules to identify targets.
For a span to be a candidate target, it must ap-
pear (up to morphological variation) as a target in the
training data or the lexicon. We consider multiword
targets,4 unlike J&amp;N’07 (though we do not consider
</bodyText>
<footnote confidence="0.941778333333333">
4There are 629 multiword LUs in the lexicon, and they cor-
respond to 4.8% of the targets in the training set; among them
are screw up.V, shoot the breeze.V, and weapon of mass de-
</footnote>
<page confidence="0.982307">
950
</page>
<table confidence="0.9926906">
FRAME IDENTIFICATION exact frame matching partial frame matching
(§4) targets P R Fl P R Fl
Frame identification (oracle targets) � 60.21 60.21 60.21 74.21 74.21 74.21
Frame identification (predicted targets) auto §3 69.75 54.91 61.44 77.51 61.03 68.29
Baseline: J&amp;N’07 auto 66.22 50.57 57.34 73.86 56.41 63.97
</table>
<tableCaption confidence="0.981028">
Table 4. Frame identification results. Precision, recall, and Fl were evaluated under exact and partial frame matching;
see §2.3. Bold indicates statistically significant results with respect to the baseline (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.999871555555556">
discontinuous targets). Using rules from §3.1.1 of
J&amp;N’07, we further prune the list, with two modi-
fications: we prune all prepositions, including loca-
tive, temporal, and directional ones, but do not prune
support verbs. This is a conservative approach; our
automatic target identifier will never propose a target
that was not seen in the training data or FrameNet.
Results. Table 3 shows results on target identifica-
tion; our system gains 3 FI points over the baseline.
</bodyText>
<sectionHeader confidence="0.993836" genericHeader="method">
4 Frame Identification
</sectionHeader>
<bodyText confidence="0.985703">
Given targets, the parser next identifies their frames.
</bodyText>
<subsectionHeader confidence="0.995772">
4.1 Lexical units
</subsectionHeader>
<bodyText confidence="0.999313136363637">
FrameNet specifies a great deal of structural infor-
mation both within and among frames. For frame
identification we make use of frame-evoking lexical
units, the (lemmatized and POS-tagged) words and
phrases listed in the lexicon as referring to specific
frames. For example, listed with the BRAGGING
frame are 10 LUs, including boast.N, boast.V, boast-
ful.A, brag.V, and braggart.N. Of course, due to pol-
ysemy and homonymy, the same LU may be associ-
ated with multiple frames; for example, gobble.V is
listed under both the INGESTION and MAKE NOISE
frames. All targets in the exemplar sentences, and
most in our training and test data, correspond to
known LUs (see Table 2).
To incorporate frame-evoking expressions found
in the training data but not the lexicon—and to avoid
the possibility of lemmatization errors—our frame
identification model will incorporate, via a latent
variable, features based directly on exemplar and
training targets rather than LUs. Let L be the set of
(unlemmatized and automatically POS-tagged) tar-
gets found in the exemplar sentences of the lexi-
con and/or the sentences in our training set. Let
Lf C L be the subset of these targets annotated as
struction.N. In the SemEval’07 training data, there are just 99
discontinuous multiword targets (1% of all targets).
evoking a particular frame f. Let Ll and Llf de-
note the lemmatized versions of L and Lf respec-
tively. Then, we write boasted.VBD E LBRAGGING
and boast.VBD E LlBRAGGING to indicate that this in-
flected verb boasted and its lemma boast have been
seen to evoke the BRAGGING frame. Significantly,
however, another target, such as toot your own horn,
might be used in other data to evoke this frame. We
thus face the additional hurdle of predicting frames
for unknown words.
The SemEval annotators created 47 new frames
not present in the lexicon, out of which 14 belonged
to our training set. We considered these with the 795
frames in the lexicon when parsing new data. Pre-
dicting new frames is a challenge not yet attempted
to our knowledge (including here). Note that the
scoring metric (§2.3) gives partial credit for related
frames (e.g., a more general frame from the lexicon).
</bodyText>
<subsectionHeader confidence="0.967559">
4.2 Model
</subsectionHeader>
<bodyText confidence="0.999959105263158">
For a given sentence x with frame-evoking targets t,
let ti denote the ith target (a word sequence). Let tli
denote its lemma. We seek a list f = (fl, ... , fm)
of frames, one per target. In our model, the set of
candidate frames for ti is defined to include every
frame f such that tli E Ll f—or if tli E� Ll, then every
known frame (the latter condition applies for 4.7%
of the gold targets in the development set). In both
cases, we let Fi be the set of candidate frames for
the ith target in x.
To allow frame identification for targets whose
lemmas were seen in neither the exemplars nor the
training data, our model includes an additional vari-
able, Ei. This variable ranges over the seen targets
in Lf,, which can be thought of as prototypes for
the expression of the frame. Importantly, frames are
predicted, but prototypes are summed over via the
latent variable. The prediction rule requires a prob-
abilistic model over frames for a target:
</bodyText>
<equation confidence="0.8657815">
�
fi � argmaxfcj� tcLf p(f, E  |ti, x) (1)
</equation>
<page confidence="0.865043">
951
</page>
<figure confidence="0.495020222222222">
max N �mj � p�(f(j)
e j=1 i=1 log i , `  |ti
`ELf�j� (j),x(j)) (3)
i
We adopt a conditional log-linear model: for f E Fi
and ` E Lf, po(f, `  |ti, x) =
exp OTg(f, `, ti, x) (2)
� �`&apos;ELf&apos; exp �Tg(f,, `,, ti, x)
f&apos;E.Ti
</figure>
<bodyText confidence="0.999986696969697">
where 0 are the model weights, and g is a vector-
valued feature function. This discriminative formu-
lation is very flexible, allowing for a variety of (pos-
sibly overlapping) features; e.g., a feature might re-
late a frame type to a prototype, represent a lexical-
semantic relationship between a prototype and a tar-
get, or encode part of the syntax of the sentence.
Previous work has exploited WordNet for better
coverage during frame identification (Johansson and
Nugues, 2007; Burchardt et al., 2005, e.g., by ex-
panding the set of targets using synsets), and others
have sought to extend the lexicon itself (see §6). We
differ in our use of a latent variable to incorporate
lexical-semantic features in a discriminative model,
relating known lexical units to unknown words that
may evoke frames. Here we are able to take advan-
tage of the large inventory of partially-annotated ex-
emplar sentences.
Note that this model makes a strong independence
assumption: each frame is predicted independently
of all others in the document. In this way the model
is similar to J&amp;N’07. However, ours is a single
conditional model that shares features and weights
across all targets, frames, and prototypes, whereas
the approach of J&amp;N’07 consists of many separately
trained models. Moreover, our model is unique in
that it uses a latent variable to smooth over frames
for unknown or ambiguous LUs.
Frame identification features depend on the pre-
processed sentence x, the prototype ` and its
WordNet lexical-semantic relationship with the tar-
get ti, and of course the frame f. Our model instan-
tiates 662,020 binary features; see Das et al. (2010).
</bodyText>
<subsectionHeader confidence="0.993572">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.9734515">
Given the training subset of the SemEval’07 data,
which is of the form ((x(j), t(j), f(j), A(j)))N
</bodyText>
<equation confidence="0.526618">
j=1
(N = 1663 is the number of sentences), we dis-
</equation>
<bodyText confidence="0.936682571428571">
criminatively train the frame identification model by
maximizing the following log-likelihood:5
5We found no benefit on development data from using an L2
regularizer (zero-mean Gaussian prior).
Note that the training problem is non-convex be-
cause of the summed-out prototype latent variable
` for each frame. To calculate the objective func-
tion, we need to cope with a sum over frames and
prototypes for each target (see Eq. 2), often an ex-
pensive operation. We locally optimize the function
using a distributed implementation of L-BFGS. This
is the most expensive model that we train: with 100
CPUs, training takes several hours. (Decoding takes
only a few minutes on one CPU for the test set.)
</bodyText>
<sectionHeader confidence="0.553773" genericHeader="method">
4.4 Results
</sectionHeader>
<bodyText confidence="0.998798714285714">
We evaluate the performance of our frame identifi-
cation model given gold-standard targets and auto-
matically identified targets (§3); see Table 4.
Given gold-standard targets, our model is able
to predict frames for lemmas not seen in training,
of which there are 210. The partial-match evalua-
tion gives our model some credit for 190 of these,
4 of which are exactly correct. The hidden vari-
able model, then, is finding related (but rarely exact)
frames for unknown target words. The net effect of
our conservative target identifier on F1 is actually
positive: the frame identifier is far more precise for
targets seen explicitly in training. Together, our tar-
get and frame identification outperform the baseline
by 4 F1 points. To compare the frame identification
stage in isolation with that of J&amp;N’07, we ran our
frame identification model with the targets identified
by their system as input. With partial matching, our
model achieves a relative improvement of 0.6% F1
over J&amp;N’07 (though this is not significant).
While our frame identification model thus per-
forms on par with the current state of the art for
this task, it improves upon J&amp;N’s formulation of
the problem because it requires only a single model,
learns lexical-semantic features as part of that model
rather than requiring a preprocessing step to expand
the vocabulary of frame-evoking words, and is prob-
abilistic, which can facilitate global reasoning.
</bodyText>
<sectionHeader confidence="0.995751" genericHeader="method">
5 Argument Identification
</sectionHeader>
<bodyText confidence="0.9986125">
Given a sentence x = (x1, ... , xn), the set of tar-
gets t = (t1, ... , tm), and a list of evoked frames
</bodyText>
<page confidence="0.994454">
952
</page>
<bodyText confidence="0.996234166666667">
f = (fi, ... , fm) corresponding to each target, ar-
gument identification is the task of choosing which
of each fi’s roles are filled, and by which parts of x.
This task is most similar to the problem of semantic
role labeling, but uses frame-specific labels that are
richer than the PropBank annotations.
</bodyText>
<subsectionHeader confidence="0.986116">
5.1 Model
</subsectionHeader>
<bodyText confidence="0.999870205882353">
Let Rfi = {ri, ... , r|Rfi|} denote frame fi’s roles
(named frame element types) observed in an exem-
plar sentence and/or our training set. A subset of
each frame’s roles are marked as core roles; these
roles are conceptually and/or syntactically necessary
for any given use of the frame, though they need
not be overt in every sentence involving the frame.
These are roughly analogous to the core arguments
A0–A5 and AA in PropBank. Non-core roles—
analogous to the various AMs in PropBank—loosely
correspond to syntactic adjuncts, and carry broadly-
applicable information such as the time, place, or
purpose of an event. The lexicon imposes some
additional structure on roles, including relations to
other roles in the same or related frames, and se-
mantic types with respect to a small ontology (mark-
ing, for instance, that the entity filling the protag-
onist role must be sentient for frames of cogni-
tion). Fig. 2 illustrates some of the structural ele-
ments comprising the frame lexicon by considering
the CAUSE TO MAKE NOISE frame.
We identify a set S of spans that are candidates for
filling any role r E Rfi. In principle, S could con-
tain any subsequence of x, but in this work we only
consider the set of contiguous spans that (a) contain
a single word or (b) comprise a valid subtree of a
word and all its descendants in the dependency parse
produced by the MST parser. This covers 81% of ar-
guments in the development data. The empty span
is also included in S, since some roles are not ex-
plicitly filled; in the development data, the average
number of roles an evoked frame defines is 6.7, but
the average number of overt arguments is only 1.7.6
In training, if a labeled argument is not a valid sub-
</bodyText>
<footnote confidence="0.960525833333333">
6In the annotated data, each core role is filled with one of
three types of null instantiations indicating how the role is con-
veyed implicitly. E.g., the imperative construction implicitly
designates a role as filled by the addressee, and the correspond-
ing filler is thus CNI (constructional null instantiation). In this
work we do not distinguish different types of null instantiations.
</footnote>
<bodyText confidence="0.994691">
tree of the dependency parse, we add its span to S.
Let Ai denote the mapping of roles in Rfi to
spans in S. Our model makes a prediction for each
Ai(rk) (for all roles rk E Rfi) using:
</bodyText>
<equation confidence="0.969173">
Ai(rk) +– argmaxs∈S p(s  |rk, fi, ti, x) (4)
</equation>
<bodyText confidence="0.9997785">
We use a conditional log-linear model over spans for
each role of each evoked frame:
</bodyText>
<equation confidence="0.988133">
p�(Ai(rk) = s  |fi, ti, x) = (5)
</equation>
<bodyText confidence="0.965291864864865">
exp ,0&gt;h(s, rk, fi, ti, x)
Es/∈S exp 0&gt;h(s0, rk, fi, ti, x)
Note that our model chooses the span for each
role separately from the other roles and ignores all
frames except the frame the role belongs to. Our
model departs from the traditional SRL literature by
modeling the argument identification problem in a
single stage, rather than first classifying token spans
as arguments and then labeling them. A constraint
implicit in our formulation restricts each role to have
at most one overt argument, which is consistent with
96.5% of the role instances in the training data.
Out of the overt argument spans in the training
data, 12% are duplicates, having been used by some
previous frame in the sentence (supposing some ar-
bitrary ordering of frames). Our role-filling model,
unlike a sentence-global argument detection-and-
classification approach,7 permits this sort of argu-
ment sharing among frames. The incidence of span
overlap among frames is much higher; Fig. 1 illus-
trates a case with a high degree of overlap. Word
tokens belong to an average of 1.6 argument spans
each, including the quarter of words that do not be-
long to any argument.
Features for our log-linear model (Eq. 5) depend
on the preprocessed sentence x; the target t; a
role r of frame f; and a candidate argument span
s E S. Our model includes lexicalized and unlexi-
calized features considering aspects of the syntactic
parse (most notably the dependency path in the parse
from the target to the argument); voice; word order-
ing/overlap/distance of the argument with respect to
the target; and POS tags within and around the argu-
ment. Many features have a version specific to the
frame and role, plus a smoothed version incorporat-
ing the role name, but not the frame. These features
7J&amp;N’07, like us, identify arguments for each target.
</bodyText>
<page confidence="0.99785">
953
</page>
<bodyText confidence="0.986796">
are fully enumerated in (Das et al., 2010); instanti-
ating them for our data yields 1,297,857 parameters.
</bodyText>
<subsectionHeader confidence="0.99929">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.99981">
We train the argument identification model by:
</bodyText>
<equation confidence="0.9013238">
log pO(A(j)
i (rk)  |�(j)
i , t(j)
i , X(j))
(6)
</equation>
<bodyText confidence="0.9994245">
This objective function is concave, and we globally
optimize it using stochastic gradient ascent (Bottou,
2004). We train this model until the argument iden-
tification F1 score stops increasing on the develop-
ment data. Best results on this dataset were obtained
with a batch size of 2 and 23 passes through the data.
</bodyText>
<subsectionHeader confidence="0.998018">
5.3 Approximate Joint Decoding
</subsectionHeader>
<bodyText confidence="0.999965375">
Naive prediction of roles using Eq. 4 may result
in overlap among arguments filling different roles
of a frame, since the argument identification model
fills each role independently of the others. We want
to enforce the constraint that two roles of a single
frame cannot be filled by overlapping spans. We dis-
allow illegal overlap using a 10000-hypothesis beam
search; the algorithm is given in (Das et al., 2010).
</bodyText>
<subsectionHeader confidence="0.745637">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.9999955">
Performance of the argument identification model
is presented in Table 5. The table shows how per-
formance varies given different types of perfect in-
put: correct targets, correct frames, and the set of
correct spans; correct targets and frames, with the
heuristically-constructed set of candidate spans; cor-
rect targets only, with model frames; and ultimately,
no oracle input (the full frame parsing scenario).
The first four rows of results isolate the argu-
ment identification task from the frame identifica-
tion task. Given gold targets and frames and an ora-
cle set of argument spans, our local model achieves
about 87% precision and 75% recall. Beam search
decoding to eliminate illegal argument assignments
within a frame (§5.3) further improves precision by
about 1.6%, with negligible harm to recall. Note
that 96.5% recall is possible under the constraint that
roles are not multiply-filled (§5.1); there is thus con-
siderable room for improvement with this constraint
in place. Joint prediction of each frame’s arguments
is worth exploring to capture correlations not en-
coded in our local models or joint decoding scheme.
The 15-point drop in recall when the heuristically-
built candidate argument set replaces the set of true
argument spans is unsurprising: an estimated 19% of
correct arguments are excluded because they are nei-
ther single words nor complete subtrees (see §5.1).
Qualitatively, the problem of candidate span recall
seems to be largely due to syntactic parse errors.8
Still, the 10-point decrease in precision when using
the syntactic parse to determine candidate spans sug-
gests that the model has trouble discriminating be-
tween good and bad arguments, and that additional
feature engineering or jointly decoding arguments of
a sentence’s frames may be beneficial in this regard.
The fifth and sixth rows show the effect of auto-
matic frame identification on overall frame parsing
performance. There is a 22% decrease in F1 (18%
when partial credit is given for related frames), sug-
gesting that improved frame identification or joint
prediction of frames and arguments is likely to have
a sizeable impact on overall performance.
The final two rows of the table compare our full
model (target, frame, and argument identification)
with the baseline, showing significant improvement
of more than 4.4 F1 points for both exact and partial
frame matching. As with frame identification, we
compared the argument identification stage with that
of J&amp;N’07 in isolation, using the automatically iden-
tified targets and frames from the latter as input to
our model. With partial frame matching, this gave us
an F1 score of 48.1% on the test set—significantly
better (p &lt; 0.05) than 45.6%, the full parsing re-
sult from J&amp;N’07. This indicates that our argument
identification model—which uses a single discrim-
inative model with a large number of features for
role filling (rather than argument labeling)—is more
powerful than the previous state of the art.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.9813575">
Since Gildea and Jurafsky (2002) pioneered statis-
tical semantic role labeling, a great deal of com-
8Note that, because of our labels-only evaluation scheme
(§2.3), arguments missing a word or containing an extra word
receive no credit. In fact, of the frame roles correctly predicted
as having an overt span, the correct span was predicted 66% of
the time, while 10% of the time the predicted starting and end-
ing boundaries of the span were off by a total of 1 or 2 words.
</bodyText>
<table confidence="0.4879315">
max �N �m31 |7Z�u)
� j=1 i=1 � k=1
</table>
<page confidence="0.927915">
954
</page>
<sectionHeader confidence="0.406" genericHeader="method">
ARGUMENT IDENTIFICATION exact frame matching
</sectionHeader>
<tableCaption confidence="0.796650333333333">
targets frames spans decoding P R F1
Table 5. Argument identification results. * indicates that gold-standard labels were used for a given pipeline stage.
For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05).
</tableCaption>
<table confidence="0.987841071428572">
Argument identifica-
tion (oracle spans)
Argument identifica-
tion (full)
Parsing (oracle targets) * model §4 model §5 beam §5.3
Parsing (full) auto §3 model §4 model §5 beam §5.3
Baseline: J&amp;N’07 auto model model N/A
86.61 75.11 80.45 partial frame matching
88.29 74.77 80.97 P R F1
77.43 60.76 68.09
78.71 60.57 68.46
49.68 42.82 46.00 57.85 49.86 53.56
58.08 38.76 46.49 62.76 41.89 50.24
51.59 35.44 42.01 56.01 38.48 45.62
</table>
<figure confidence="0.770577">
a
* * * n
* * * beam §5.3
* * model §5 naive
* * model §5 beam §5.3
ive
</figure>
<bodyText confidence="0.9995876">
putational work has investigated predicate-argument
structures for semantics. Briefly, we highlight some
relevant work, particularly research that has made
use of FrameNet. (Note that much related research
has focused on PropBank (Kingsbury and Palmer,
2002), a set of shallow predicate-argument annota-
tions for Wall Street Journal articles from the Penn
Treebank (Marcus et al., 1993); a recent issue of CL
(M`arquez et al., 2008) was devoted to the subject.)
Most work on frame-semantic role labeling has
made use of the exemplar sentences in the FrameNet
corpus (see §2.1), each of which is annotated for a
single frame and its arguments. On the probabilis-
tic modeling front, Gildea and Jurafsky (2002) pre-
sented a discriminative model for arguments given
the frame; Thompson et al. (2003) used a gener-
ative model for both the frame and its arguments;
and Fleischman et al. (2003) first used maximum
entropy models to find and label arguments given
the frame. Shi and Mihalcea (2004) developed a
rule-based system to predict frames and their argu-
ments in text, and Erk and Pad´o (2006) introduced
the Shalmaneser tool, which employs Naive Bayes
classifiers to do the same. Other FrameNet SRL
systems (Giuglea and Moschitti, 2006, for instance)
have used SVMs. Most of this work was done on an
older, smaller version of FrameNet.
Recent work on frame-semantic parsing—in
which sentences may contain multiple frames to be
recognized along with their arguments—has used
the SemEval’07 data (Baker et al., 2007). The LTH
system of Johansson and Nugues (2007), our base-
line (§2.4), performed the best in the SemEval’07
task. Matsubayashi et al. (2009) trained a log-
linear model on the SemEval’07 data to evaluate
argument identification features exploiting various
types of taxonomic relations to generalize over roles.
A line of work has sought to extend the coverage
of FrameNet by exploiting VerbNet, WordNet, and
Wikipedia (Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli
and Giuliano, 2009), and projecting entries and an-
notations within and across languages (Boas, 2002;
Fung and Chen, 2004; Pad´o and Lapata, 2005;
F¨urstenau and Lapata, 2009). Others have applied
frame-semantic structures to question answering,
paraphrase/entailment recognition, and information
extraction (Narayanan and Harabagiu, 2004; Shen
and Lapata, 2007; Pad´o and Erk, 2005; Burchardt,
2006; Moschitti et al., 2003; Surdeanu et al., 2003).
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999966555555555">
We have provided a supervised model for rich
frame-semantic parsing, based on a combination
of knowledge from FrameNet, two probabilistic
models trained on SemEval’07 data, and expedi-
ent heuristics. Our system achieves improvements
over the state of the art at each stage of process-
ing and collectively, and is amenable to future ex-
tension. Our parser is available for download at
http://www.ark.cs.cmu.edu/SEMAFOR.
</bodyText>
<sectionHeader confidence="0.998272" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.905938">
We thank Collin Baker, Katrin Erk, Richard Johansson,
and Nils Reiter for software, data, evaluation scripts, and
methodological details. We thank the reviewers, Alan
Black, Ric Crabbe, Michael Ellsworth, Rebecca Hwa,
Dan Klein, Russell Lee-Goldman, Dan Roth, Josef Rup-
penhofer, and members of the ARK group for helpful
comments. This work was supported by DARPA grant
NBCH-1080004, NSF grant IIS-0836431, and computa-
tional resources provided by Yahoo.
</bodyText>
<page confidence="0.99818">
955
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999961663265306">
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: frame semantic structure extraction. In
Proc. of SemEval.
H. C. Boas. 2002. Bilingual FrameNet dictionaries for
machine translation. In Proc. of LREC.
L. Bottou. 2004. Stochastic learning. In Advanced Lec-
tures on Machine Learning. Springer-Verlag.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In B. Fisseni, H.-C. Schmitz,
B. Schr¨oder, and P. Wagner, editors, Sprachtech-
nologie, mobile Kommunikation und linguistische Re-
sourcen, volume 8. Peter Lang.
A. Burchardt. 2006. Approaching textual entailment
with LFG and FrameNet frames. In Proc. of the Sec-
ond PASCAL RTE Challenge Workshop.
D. Das, N. Schneider, D. Chen, and N. A. Smith.
2010. SEMAFOR 1.0: A probabilistic frame-semantic
parser. Technical Report CMU-LTI-10-001, Carnegie
Mellon University.
K. Erk and S. Pad´o. 2006. Shalmaneser - a toolchain for
shallow semantic parsing. In Proc. of LREC.
C. Fellbaum, editor. 1998. WordNet: an electronic lexi-
cal database. MIT Press, Cambridge, MA.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in
the Morning Calm, pages 111–137. Hanshin Publish-
ing Co., Seoul, South Korea.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum
entropy models for FrameNet classification. In Proc.
of EMNLP.
P. Fung and B. Chen. 2004. BiFrameNet: bilin-
gual frame semantics resource construction by cross-
lingual induction. In Proc. of COLING.
H. F¨urstenau and M. Lapata. 2009. Semi-supervised se-
mantic role labeling. In Proc. of EACL.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
A.-M. Giuglea and A. Moschitti. 2006. Shallow
semantic parsing based on FrameNet, VerbNet and
PropBank. In Proc. of ECAI 2006.
R. Johansson and P. Nugues. 2007. LTH: semantic struc-
ture extraction using nonprojective dependency trees.
In Proc. of SemEval.
R. Johansson and P. Nugues. 2008. Dependency-based
semantic role labeling of PropBank. In Proc. of
EMNLP.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. of LREC.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics, 19(2).
L. M`arquez, X. Carreras, K. C. Litkowski, and S. Steven-
son. 2008. Semantic role labeling: an introduction to
the special issue. Computational Linguistics, 34(2).
Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A
comparative study on generalization of semantic roles
in FrameNet. In Proc. of ACL-IJCNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
A. Moschitti, P. Mor˘arescu, and S. M. Harabagiu. 2003.
Open-domain information extraction via automatic se-
mantic labeling. In Proc. of FLAIRS.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proc. of COLING.
S. Pad´o and K. Erk. 2005. To cause or not to cause:
cross-lingual semantic matching for paraphrase mod-
elling. In Proc. of the Cross-Language Knowledge In-
duction Workshop.
S. Pad´o and M. Lapata. 2005. Cross-linguistic projec-
tion of role-semantic information. In Proc. of HLT-
EMNLP.
M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and
M. Roth. 2008. Automatic induction of FrameNet
lexical units. In Proc. of EMNLP.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proc. of EMNLP-
CoNLL.
L. Shi and R. Mihalcea. 2004. An algorithm for open
text semantic parsing. In Proc. of Workshop on Robust
Methods in Analysis of Natural Language Data.
L. Shi and R. Mihalcea. 2005. Putting pieces together:
combining FrameNet, VerbNet and WordNet for ro-
bust semantic parsing. In Computational Linguis-
tics and Intelligent Text Processing: Proc. of CICLing
2005. Springer-Verlag.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using predicate-argument structures for infor-
mation extraction. In Proc. of ACL.
C. A. Thompson, R. Levy, and C. D. Manning. 2003. A
generative model for semantic role labeling. In Proc.
of ECML.
S. Tonelli and C. Giuliano. 2009. Wikipedia as frame
information repository. In Proc. of EMNLP.
</reference>
<page confidence="0.99864">
956
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970505">
<title confidence="0.999976">Probabilistic Frame-Semantic Parsing</title>
<author confidence="0.999789">Dipanjan Das Nathan Schneider Desai Chen Noah A Smith</author>
<affiliation confidence="0.9999575">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999497">Pittsburgh, PA 15213, USA</address>
<abstract confidence="0.997893214285715">This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation. It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame. The system uses two featurebased, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words. The parser is demonstrated to significantly outperform previously published results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>M Ellsworth</author>
<author>K Erk</author>
</authors>
<title>SemEval2007 Task 19: frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval.</booktitle>
<contexts>
<context position="2432" citStr="Baker et al., 2007" startWordPosition="343" endWordPosition="346">d learning. Some novel aspects of our current approach include a latent-variable model that permits disambiguation of words not in the FrameNet lexicon, a unified model for finding and labeling arguments, Figure 2. Partial illustration of frames, roles, and LUs related to the CAUSE TO MAKE NOISE frame, from the FrameNet lexicon. “Core” roles are filled ovals. 8 additional roles of CAUSE TO MAKE NOISE are not shown. and a precision-boosting constraint that forbids arguments of the same predicate to overlap. Our parser achieves the best published results to date on the SemEval’07 FrameNet task (Baker et al., 2007). 2 Resources and Task We consider frame-semantic parsing resources. 2.1 FrameNet Lexicon The FrameNet lexicon is a taxonomy of manually identified general-purpose frames for English.1 Listed in the lexicon with each frame are several lemmas (with part of speech) that can denote the frame or some aspect of it—these are called lexical units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may see a target in new data that does not correspond to an LU for the frame it evokes. Each frame de</context>
<context position="9395" citStr="Baker et al. (2007)" startWordPosition="1463" endWordPosition="1466">ication (§3), frame identification (§4, not unlike wordsense disambiguation), and argument identification (§5, not unlike semantic role labeling). The standard evaluation script from the SemEval’07 shared task calculates precision, recall, and FI-measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one. We present precision, recall, and FI-measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. More details can be found in Baker et al. (2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel’s randomized parsing evaluation comparator.3 2.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and Nugues (2007, hereafter J&amp;N’07), the best system in the SemEval’07 shared task. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frameevoking words to cover unknown words, and then 3http://www.cis.upenn.edu/˜dbikel/ software.html#comparator TARG</context>
<context position="32114" citStr="Baker et al., 2007" startWordPosition="5249" endWordPosition="5252">sed maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, </context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval2007 Task 19: frame semantic structure extraction. In Proc. of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H C Boas</author>
</authors>
<title>Bilingual FrameNet dictionaries for machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="32718" citStr="Boas, 2002" startWordPosition="5344" endWordPosition="5345"> 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over t</context>
</contexts>
<marker>Boas, 2002</marker>
<rawString>H. C. Boas. 2002. Bilingual FrameNet dictionaries for machine translation. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>Stochastic learning.</title>
<date>2004</date>
<booktitle>In Advanced Lectures on Machine Learning.</booktitle>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="25652" citStr="Bottou, 2004" startWordPosition="4193" endWordPosition="4194"> argument with respect to the target; and POS tags within and around the argument. Many features have a version specific to the frame and role, plus a smoothed version incorporating the role name, but not the frame. These features 7J&amp;N’07, like us, identify arguments for each target. 953 are fully enumerated in (Das et al., 2010); instantiating them for our data yields 1,297,857 parameters. 5.2 Training We train the argument identification model by: log pO(A(j) i (rk) |�(j) i , t(j) i , X(j)) (6) This objective function is concave, and we globally optimize it using stochastic gradient ascent (Bottou, 2004). We train this model until the argument identification F1 score stops increasing on the development data. Best results on this dataset were obtained with a batch size of 2 and 23 passes through the data. 5.3 Approximate Joint Decoding Naive prediction of roles using Eq. 4 may result in overlap among arguments filling different roles of a frame, since the argument identification model fills each role independently of the others. We want to enforce the constraint that two roles of a single frame cannot be filled by overlapping spans. We disallow illegal overlap using a 10000-hypothesis beam sea</context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L. Bottou. 2004. Stochastic learning. In Advanced Lectures on Machine Learning. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
<author>K Erk</author>
<author>A Frank</author>
</authors>
<title>A WordNet detour to FrameNet. In</title>
<date>2005</date>
<booktitle>Sprachtechnologie, mobile Kommunikation und linguistische Resourcen,</booktitle>
<volume>8</volume>
<editor>B. Fisseni, H.-C. Schmitz, B. Schr¨oder, and P. Wagner, editors,</editor>
<publisher>Peter Lang.</publisher>
<contexts>
<context position="17199" citStr="Burchardt et al., 2005" startWordPosition="2749" endWordPosition="2752">l log-linear model: for f E Fi and ` E Lf, po(f, ` |ti, x) = exp OTg(f, `, ti, x) (2) � �`&apos;ELf&apos; exp �Tg(f,, `,, ti, x) f&apos;E.Ti where 0 are the model weights, and g is a vectorvalued feature function. This discriminative formulation is very flexible, allowing for a variety of (possibly overlapping) features; e.g., a feature might relate a frame type to a prototype, represent a lexicalsemantic relationship between a prototype and a target, or encode part of the syntax of the sentence. Previous work has exploited WordNet for better coverage during frame identification (Johansson and Nugues, 2007; Burchardt et al., 2005, e.g., by expanding the set of targets using synsets), and others have sought to extend the lexicon itself (see §6). We differ in our use of a latent variable to incorporate lexical-semantic features in a discriminative model, relating known lexical units to unknown words that may evoke frames. Here we are able to take advantage of the large inventory of partially-annotated exemplar sentences. Note that this model makes a strong independence assumption: each frame is predicted independently of all others in the document. In this way the model is similar to J&amp;N’07. However, ours is a single co</context>
</contexts>
<marker>Burchardt, Erk, Frank, 2005</marker>
<rawString>A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet detour to FrameNet. In B. Fisseni, H.-C. Schmitz, B. Schr¨oder, and P. Wagner, editors, Sprachtechnologie, mobile Kommunikation und linguistische Resourcen, volume 8. Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
</authors>
<title>Approaching textual entailment with LFG and FrameNet frames.</title>
<date>2006</date>
<booktitle>In Proc. of the Second PASCAL RTE Challenge Workshop.</booktitle>
<contexts>
<context position="33017" citStr="Burchardt, 2006" startWordPosition="5383" endWordPosition="5384">eralize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reiter for software, data, evaluation scri</context>
</contexts>
<marker>Burchardt, 2006</marker>
<rawString>A. Burchardt. 2006. Approaching textual entailment with LFG and FrameNet frames. In Proc. of the Second PASCAL RTE Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N Schneider</author>
<author>D Chen</author>
<author>N A Smith</author>
</authors>
<title>SEMAFOR 1.0: A probabilistic frame-semantic parser.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI-10-001,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="18330" citStr="Das et al. (2010)" startWordPosition="2935" endWordPosition="2938">the document. In this way the model is similar to J&amp;N’07. However, ours is a single conditional model that shares features and weights across all targets, frames, and prototypes, whereas the approach of J&amp;N’07 consists of many separately trained models. Moreover, our model is unique in that it uses a latent variable to smooth over frames for unknown or ambiguous LUs. Frame identification features depend on the preprocessed sentence x, the prototype ` and its WordNet lexical-semantic relationship with the target ti, and of course the frame f. Our model instantiates 662,020 binary features; see Das et al. (2010). 4.3 Training Given the training subset of the SemEval’07 data, which is of the form ((x(j), t(j), f(j), A(j)))N j=1 (N = 1663 is the number of sentences), we discriminatively train the frame identification model by maximizing the following log-likelihood:5 5We found no benefit on development data from using an L2 regularizer (zero-mean Gaussian prior). Note that the training problem is non-convex because of the summed-out prototype latent variable ` for each frame. To calculate the objective function, we need to cope with a sum over frames and prototypes for each target (see Eq. 2), often an</context>
<context position="25370" citStr="Das et al., 2010" startWordPosition="4145" endWordPosition="4148">; a role r of frame f; and a candidate argument span s E S. Our model includes lexicalized and unlexicalized features considering aspects of the syntactic parse (most notably the dependency path in the parse from the target to the argument); voice; word ordering/overlap/distance of the argument with respect to the target; and POS tags within and around the argument. Many features have a version specific to the frame and role, plus a smoothed version incorporating the role name, but not the frame. These features 7J&amp;N’07, like us, identify arguments for each target. 953 are fully enumerated in (Das et al., 2010); instantiating them for our data yields 1,297,857 parameters. 5.2 Training We train the argument identification model by: log pO(A(j) i (rk) |�(j) i , t(j) i , X(j)) (6) This objective function is concave, and we globally optimize it using stochastic gradient ascent (Bottou, 2004). We train this model until the argument identification F1 score stops increasing on the development data. Best results on this dataset were obtained with a batch size of 2 and 23 passes through the data. 5.3 Approximate Joint Decoding Naive prediction of roles using Eq. 4 may result in overlap among arguments fillin</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010. SEMAFOR 1.0: A probabilistic frame-semantic parser. Technical Report CMU-LTI-10-001, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pad´o</author>
</authors>
<title>Shalmaneser - a toolchain for shallow semantic parsing.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<marker>Erk, Pad´o, 2006</marker>
<rawString>K. Erk and S. Pad´o. 2006. Shalmaneser - a toolchain for shallow semantic parsing. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="850" citStr="Fillmore et al., 2003" startWordPosition="106" endWordPosition="109">du Abstract This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation. It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame. The system uses two featurebased, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words. The parser is demonstrated to significantly outperform previously published results. 1 Introduction FrameNet (Fillmore et al., 2003) is a rich linguistic resource containing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore, 1982), it suggests—but does not formally define—a semantic representation that blends wordsense disambiguation and semantic role labeling. In this paper, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicateargument structures such as those shown in Fig. 1. We aim to predict a frame-semantic representation as a structure, not as a pipeline o</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm,</booktitle>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.,</publisher>
<location>Seoul, South</location>
<contexts>
<context position="1034" citStr="Fillmore, 1982" startWordPosition="133" endWordPosition="134">rame-semantic representation. It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame. The system uses two featurebased, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words. The parser is demonstrated to significantly outperform previously published results. 1 Introduction FrameNet (Fillmore et al., 2003) is a rich linguistic resource containing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore, 1982), it suggests—but does not formally define—a semantic representation that blends wordsense disambiguation and semantic role labeling. In this paper, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicateargument structures such as those shown in Fig. 1. We aim to predict a frame-semantic representation as a structure, not as a pipeline of classifiers. We use a probabilistic framework that cleanly integrates the FrameNet lexicon and (currently very limited) available training data. Although our models often involve str</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>C. J. Fillmore. 1982. Frame semantics. In Linguistics in the Morning Calm, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>N Kwon</author>
<author>E Hovy</author>
</authors>
<title>Maximum entropy models for FrameNet classification.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="31487" citStr="Fleischman et al. (2003)" startWordPosition="5149" endWordPosition="5152"> a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’0</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum entropy models for FrameNet classification. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>B Chen</author>
</authors>
<title>BiFrameNet: bilingual frame semantics resource construction by crosslingual induction.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="32739" citStr="Fung and Chen, 2004" startWordPosition="5346" endWordPosition="5349">LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art a</context>
</contexts>
<marker>Fung, Chen, 2004</marker>
<rawString>P. Fung and B. Chen. 2004. BiFrameNet: bilingual frame semantics resource construction by crosslingual induction. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H F¨urstenau</author>
<author>M Lapata</author>
</authors>
<title>Semi-supervised semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proc. of EACL.</booktitle>
<marker>F¨urstenau, Lapata, 2009</marker>
<rawString>H. F¨urstenau and M. Lapata. 2009. Semi-supervised semantic role labeling. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="29326" citStr="Gildea and Jurafsky (2002)" startWordPosition="4782" endWordPosition="4785"> identification, we compared the argument identification stage with that of J&amp;N’07 in isolation, using the automatically identified targets and frames from the latter as input to our model. With partial frame matching, this gave us an F1 score of 48.1% on the test set—significantly better (p &lt; 0.05) than 45.6%, the full parsing result from J&amp;N’07. This indicates that our argument identification model—which uses a single discriminative model with a large number of features for role filling (rather than argument labeling)—is more powerful than the previous state of the art. 6 Related work Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, a great deal of com8Note that, because of our labels-only evaluation scheme (§2.3), arguments missing a word or containing an extra word receive no credit. In fact, of the frame roles correctly predicted as having an overt span, the correct span was predicted 66% of the time, while 10% of the time the predicted starting and ending boundaries of the span were off by a total of 1 or 2 words. max �N �m31 |7Z�u) � j=1 i=1 � k=1 954 ARGUMENT IDENTIFICATION exact frame matching targets frames spans decoding P R F1 Table 5. Argument identification result</context>
<context position="31309" citStr="Gildea and Jurafsky (2002)" startWordPosition="5118" endWordPosition="5121">riefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, sm</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Giuglea</author>
<author>A Moschitti</author>
</authors>
<title>Shallow semantic parsing based on FrameNet, VerbNet and PropBank.</title>
<date>2006</date>
<booktitle>In Proc. of ECAI</booktitle>
<contexts>
<context position="31835" citStr="Giuglea and Moschitti, 2006" startWordPosition="5205" endWordPosition="5208">for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of wor</context>
</contexts>
<marker>Giuglea, Moschitti, 2006</marker>
<rawString>A.-M. Giuglea and A. Moschitti. 2006. Shallow semantic parsing based on FrameNet, VerbNet and PropBank. In Proc. of ECAI 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>LTH: semantic structure extraction using nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval.</booktitle>
<contexts>
<context position="9649" citStr="Johansson and Nugues (2007" startWordPosition="1498" endWordPosition="1501">measure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one. We present precision, recall, and FI-measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. More details can be found in Baker et al. (2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel’s randomized parsing evaluation comparator.3 2.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and Nugues (2007, hereafter J&amp;N’07), the best system in the SemEval’07 shared task. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frameevoking words to cover unknown words, and then 3http://www.cis.upenn.edu/˜dbikel/ software.html#comparator TARGET IDENTIFICATION P R Fl Our technique (§3) 89.92 70.79 79.21 Baseline: J&amp;N’07 87.87 67.11 76.10 Table 3. Target identification results for our system and the baseline. Scores in bold denote significant improvements over the baseline (p &lt; 0.05). used a c</context>
<context position="17175" citStr="Johansson and Nugues, 2007" startWordPosition="2745" endWordPosition="2748"> (3) i We adopt a conditional log-linear model: for f E Fi and ` E Lf, po(f, ` |ti, x) = exp OTg(f, `, ti, x) (2) � �`&apos;ELf&apos; exp �Tg(f,, `,, ti, x) f&apos;E.Ti where 0 are the model weights, and g is a vectorvalued feature function. This discriminative formulation is very flexible, allowing for a variety of (possibly overlapping) features; e.g., a feature might relate a frame type to a prototype, represent a lexicalsemantic relationship between a prototype and a target, or encode part of the syntax of the sentence. Previous work has exploited WordNet for better coverage during frame identification (Johansson and Nugues, 2007; Burchardt et al., 2005, e.g., by expanding the set of targets using synsets), and others have sought to extend the lexicon itself (see §6). We differ in our use of a latent variable to incorporate lexical-semantic features in a discriminative model, relating known lexical units to unknown words that may evoke frames. Here we are able to take advantage of the large inventory of partially-annotated exemplar sentences. Note that this model makes a strong independence assumption: each frame is predicted independently of all others in the document. In this way the model is similar to J&amp;N’07. Howe</context>
<context position="32161" citStr="Johansson and Nugues (2007)" startWordPosition="5257" endWordPosition="5260">label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 20</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. LTH: semantic structure extraction using nonprojective dependency trees. In Proc. of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="8346" citStr="Johansson and Nugues (2008)" startWordPosition="1300" endWordPosition="1303">ls trained on the given data split, a problem we have not sought to address here. 2http://framenet.icsi.berkeley.edu/ semeval/FSSE.html 949 Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame. These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification (§3), frame identification (§4, not unlike wordsense disambiguation), and argument identification (§5, not unlike semantic role labeling). The standard evaluation</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>R. Johansson and P. Nugues. 2008. Dependency-based semantic role labeling of PropBank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From TreeBank to PropBank.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="30862" citStr="Kingsbury and Palmer, 2002" startWordPosition="5043" endWordPosition="5046">rsing (full) auto §3 model §4 model §5 beam §5.3 Baseline: J&amp;N’07 auto model model N/A 86.61 75.11 80.45 partial frame matching 88.29 74.77 80.97 P R F1 77.43 60.76 68.09 78.71 60.57 68.46 49.68 42.82 46.00 57.85 49.86 53.56 58.08 38.76 46.49 62.76 41.89 50.24 51.59 35.44 42.01 56.01 38.48 45.62 a * * * n * * * beam §5.3 * * model §5 naive * * model §5 beam §5.3 ive putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From TreeBank to PropBank. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="30989" citStr="Marcus et al., 1993" startWordPosition="5063" endWordPosition="5066">74.77 80.97 P R F1 77.43 60.76 68.09 78.71 60.57 68.46 49.68 42.82 46.00 57.85 49.86 53.56 58.08 38.76 46.49 62.76 41.89 50.24 51.59 35.44 42.01 56.01 38.48 45.62 a * * * n * * * beam §5.3 * * model §5 naive * * model §5 beam §5.3 ive putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>X Carreras</author>
<author>K C Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic role labeling: an introduction to the special issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<marker>M`arquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. M`arquez, X. Carreras, K. C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsubayashi</author>
<author>N Okazaki</author>
<author>J Tsujii</author>
</authors>
<title>A comparative study on generalization of semantic roles in FrameNet.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="32253" citStr="Matsubayashi et al. (2009)" startWordPosition="5272" endWordPosition="5275">dict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question </context>
</contexts>
<marker>Matsubayashi, Okazaki, Tsujii, 2009</marker>
<rawString>Y. Matsubayashi, N. Okazaki, and J. Tsujii. 2009. A comparative study on generalization of semantic roles in FrameNet. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="8055" citStr="McDonald et al., 2005" startWordPosition="1253" endWordPosition="1256">Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2http://framenet.icsi.berkeley.edu/ semeval/FSSE.html 949 Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>P Mor˘arescu</author>
<author>S M Harabagiu</author>
</authors>
<title>Open-domain information extraction via automatic semantic labeling.</title>
<date>2003</date>
<booktitle>In Proc. of FLAIRS.</booktitle>
<marker>Moschitti, Mor˘arescu, Harabagiu, 2003</marker>
<rawString>A. Moschitti, P. Mor˘arescu, and S. M. Harabagiu. 2003. Open-domain information extraction via automatic semantic labeling. In Proc. of FLAIRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>S Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="32956" citStr="Narayanan and Harabagiu, 2004" startWordPosition="5371" endWordPosition="5374">ntification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard J</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>S. Narayanan and S. Harabagiu. 2004. Question answering based on semantic structures. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>K Erk</author>
</authors>
<title>To cause or not to cause: cross-lingual semantic matching for paraphrase modelling.</title>
<date>2005</date>
<booktitle>In Proc. of the Cross-Language Knowledge Induction Workshop.</booktitle>
<marker>Pad´o, Erk, 2005</marker>
<rawString>S. Pad´o and K. Erk. 2005. To cause or not to cause: cross-lingual semantic matching for paraphrase modelling. In Proc. of the Cross-Language Knowledge Induction Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Cross-linguistic projection of role-semantic information.</title>
<date>2005</date>
<booktitle>In Proc. of HLTEMNLP.</booktitle>
<marker>Pad´o, Lapata, 2005</marker>
<rawString>S. Pad´o and M. Lapata. 2005. Cross-linguistic projection of role-semantic information. In Proc. of HLTEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>D De Cao</author>
<author>R Basili</author>
<author>D Croce</author>
<author>M Roth</author>
</authors>
<title>Automatic induction of FrameNet lexical units.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Pennacchiotti, De Cao, Basili, Croce, Roth, 2008</marker>
<rawString>M. Pennacchiotti, D. De Cao, R. Basili, D. Croce, and M. Roth. 2008. Automatic induction of FrameNet lexical units. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7989" citStr="Ratnaparkhi, 1996" startWordPosition="1243" endWordPosition="1244">to create a development set for tuning model hyperparameters. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2http://framenet.icsi.berkeley.edu/ semeval/FSSE.html 949 Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having ACTIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arg</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL.</booktitle>
<contexts>
<context position="32979" citStr="Shen and Lapata, 2007" startWordPosition="5375" endWordPosition="5378"> various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reit</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shi</author>
<author>R Mihalcea</author>
</authors>
<title>An algorithm for open text semantic parsing.</title>
<date>2004</date>
<booktitle>In Proc. of Workshop on Robust Methods in Analysis of Natural Language Data.</booktitle>
<contexts>
<context position="31590" citStr="Shi and Mihalcea (2004)" startWordPosition="5166" endWordPosition="5169"> (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), perfor</context>
</contexts>
<marker>Shi, Mihalcea, 2004</marker>
<rawString>L. Shi and R. Mihalcea. 2004. An algorithm for open text semantic parsing. In Proc. of Workshop on Robust Methods in Analysis of Natural Language Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shi</author>
<author>R Mihalcea</author>
</authors>
<title>Putting pieces together: combining FrameNet, VerbNet and WordNet for robust semantic parsing.</title>
<date>2005</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing: Proc. of CICLing</booktitle>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="32552" citStr="Shi and Mihalcea, 2005" startWordPosition="5318" endWordPosition="5321">eNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, bas</context>
</contexts>
<marker>Shi, Mihalcea, 2005</marker>
<rawString>L. Shi and R. Mihalcea. 2005. Putting pieces together: combining FrameNet, VerbNet and WordNet for robust semantic parsing. In Computational Linguistics and Intelligent Text Processing: Proc. of CICLing 2005. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>S Harabagiu</author>
<author>J Williams</author>
<author>P Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="33065" citStr="Surdeanu et al., 2003" startWordPosition="5389" endWordPosition="5392">ught to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reiter for software, data, evaluation scripts, and methodological details. We thank the re</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using predicate-argument structures for information extraction. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>R Levy</author>
<author>C D Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>In Proc. of ECML.</booktitle>
<contexts>
<context position="31396" citStr="Thompson et al. (2003)" startWordPosition="5132" endWordPosition="5135">t. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>C. A. Thompson, R. Levy, and C. D. Manning. 2003. A generative model for semantic role labeling. In Proc. of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tonelli</author>
<author>C Giuliano</author>
</authors>
<title>Wikipedia as frame information repository.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="32638" citStr="Tonelli and Giuliano, 2009" startWordPosition="5330" endWordPosition="5333">le frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on Se</context>
</contexts>
<marker>Tonelli, Giuliano, 2009</marker>
<rawString>S. Tonelli and C. Giuliano. 2009. Wikipedia as frame information repository. In Proc. of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>