<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001215">
<title confidence="0.987497">
Multi-faceted Event Recognition with Bootstrapped Dictionaries
</title>
<author confidence="0.998801">
Ruihong Huang and Ellen Riloff
</author>
<affiliation confidence="0.909136333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.998823">
{huangrh, riloff}@cs.utah.edu
</email>
<sectionHeader confidence="0.995602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999137117647059">
Identifying documents that describe a specific
type of event is challenging due to the high
complexity and variety of event descriptions.
We propose a multi-faceted event recognition
approach, which identifies documents about
an event using event phrases as well as defin-
ing characteristics of the event. Our research
focuses on civil unrest events and learns civil
unrest expressions as well as phrases cor-
responding to potential agents and reasons
for civil unrest. We present a bootstrapping
algorithm that automatically acquires event
phrases, agent terms, and purpose (reason)
phrases from unannotated texts. We use the
bootstrapped dictionaries to identify civil un-
rest documents and show that multi-faceted
event recognition can yield high accuracy.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999456">
Many people are interested in following news re-
ports about events. Government agencies are keenly
interested in news about civil unrest, acts of terror-
ism, and disease outbreaks. Companies want to stay
on top of news about corporate acquisitions, high-
level management changes, and new joint ventures.
The general public is interested in articles about
crime, natural disasters, and plane crashes. We will
refer to the task of identifying documents that de-
scribe a specific type of event as event recognition.
It is tempting to assume that event keywords
are sufficient to identify documents that discuss in-
stances of an event. But event words are rarely reli-
able on their own. For example, consider the chal-
lenge of finding documents about civil unrest. The
</bodyText>
<page confidence="0.991079">
41
</page>
<bodyText confidence="0.999931411764706">
words “strike”, “rally”, and “riot” refer to com-
mon types of civil unrest, but they frequently refer to
other things as well. A strike can refer to a military
event or a sporting event (e.g., “air strike”, “bowl-
ing strike”), a rally can be a race or a spirited ex-
change (e.g.,“car rally”, “tennis rally”), and a riot
can refer to something funny (e.g., “she’s a riot”).
Event keywords also appear in general discussions
that do not mention a specific event (e.g., “37 states
prohibit teacher strikes” or “The fine for inciting a
riot is $1,000”). Furthermore, many relevant docu-
ments are not easy to recognize because events can
be described with complex expressions that do not
include event keywords. For example, “took to the
streets”, “walked off their jobs” and “stormed par-
liament” often describe civil unrest.
The goal of our research is to recognize event de-
scriptions in text by identifying event expressions as
well as defining characteristics of the event. We pro-
pose that agents and purpose are characteristics of
an event that are essential to distinguish one type of
event from another. The agent responsible for an ac-
tion often determines how we categorize the action.
For example, natural disasters, military operations,
and terrorist attacks can all produce human casual-
ties and physical destruction. But the agent of a nat-
ural disaster must be a natural force, the agent of
a military incident must be military personnel, and
the agent of a terrorist attack is never a natural force
and rarely military personnel. There may be other
important factors as well, but the agent is often an
essential part of an event definition.
The purpose of an event is also a crucial factor
in distinguishing between event types. For exam-
</bodyText>
<note confidence="0.448363">
Proceedings of NAACL-HLT 2013, pages 41–51,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999937162790698">
ple, civil unrest events and sporting events both in-
volve large groups of people amassing at a specific
site. But the purpose of civil unrest gatherings is to
protest against socio-political problems, while sport-
ing events are intended as entertainment. As another
example, terrorist events and military incidents can
both cause casualties, but the purpose of terrorism is
to cause widespread fear, while the purpose of mili-
tary actions is to protect national security interests.
Our research explores the idea of multi-faceted
event recognition: using event expressions as well
as facets of the event (agents and purpose) to iden-
tify documents about a specific type of event. We
present a bootstrapping framework to automatically
create event phrase, agent, and purpose dictionaries.
The learning process uses unannotated texts, a few
event keywords, and seed terms for common agents
and purpose phrases associated with the event type.
Our bootstrapping algorithm exploits the obser-
vation that event expressions, agents, and purpose
phrases often appear together in sentences that in-
troduce an event. In the first step, we extract event
expressions based on dependency relations with an
agent and purpose phrase. The harvested event ex-
pressions are added to an event phrase dictionary. In
the second step, new agent terms are extracted from
sentences containing an event phrase and a purpose
phrase, and new purpose phrases are harvested from
sentences containing an event phrase and an agent.
These harvested terms are added to agent and pur-
pose dictionaries. The bootstrapping algorithm rico-
chets back and forth, alternately learning new event
phrases and learning new agent/purpose phrases, in
an iterative process.
We explore several ways of using these boot-
strapped dictionaries. We conclude that finding at
least two different types of event information pro-
duces high accuracy (88% precision) with good re-
call (71%) on documents that contain an event key-
word. We also present experiments with documents
that do not contain event keywords, and obtain 74%
accuracy when matching all three types of event in-
formation.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99974122">
Event recognition has been studied in several dif-
ferent contexts. There has been a lot of research
on event extraction, where the goal is to extract
facts about events from text (e.g., (ACE Evaluations,
2006; Appelt et al., 1993; Riloff, 1996; Yangar-
ber et al., 2000; Chieu and Ng, 2002; Califf and
Mooney, 2003; Sudo et al., 2003; Stevenson and
Greenwood, 2005; Sekine, 2006)). Although our re-
search does not involve extracting facts, event ex-
traction systems can also be used to identify sto-
ries about a specific type of event. For example, the
MUC-4 evaluation (MUC-4 Proceedings, 1992) in-
cluded “text filtering” results that measured the per-
formance of event extraction systems at identifying
event-relevant documents. The best text filtering re-
sults were high (about 90% F score), but relied on
hand-built event extraction systems. More recently,
some research has incorporated event region detec-
tors into event extraction systems to improve extrac-
tion performance (Gu and Cercone, 2006; Patward-
han and Riloff, 2007; Huang and Riloff, 2011).
There has been recent work on event detection
from social media sources (Becker et al., 2011;
Popescu et al., 2011). Some research identifies spe-
cific types of events in tweets, such as earthquakes
(Sakaki et al., 2010) and entertainment events (Ben-
son et al., 2011). There has also been work on event
trend detection (Lampos et al., 2010; Mathioudakis
and Koudas, 2010) and event prediction through so-
cial media, such as predicting elections (Tumasjan
et al., 2010; Conover et al., 2011) or stock mar-
ket indicators (Zhang et al., 2010). (Ritter et al.,
2012) generated a calendar of events mentioned on
twitter. (Metzler et al., 2012) proposed structured
retrieval of historical event information over mi-
croblog archives by distilling high quality event rep-
resentations using a novel temporal query expansion
technique.
Some text classification research has focused on
event categories. (Riloff and Lehnert, 1994) used
an information extraction system to generate rele-
vancy signatures that were indicative of different
event types. This work originally relied on man-
ually labeled patterns and a hand-crafted semantic
dictionary. Later work (Riloff and Lorenzen, 1999)
eliminated the need for the dictionary and labeled
patterns, but still assumed the availability of rele-
vant/irrelevant training texts.
Event recognition is also related to Topic Detec-
tion and Tracking (TDT) (Allan et al., 1998; Allan,
</bodyText>
<page confidence="0.999396">
42
</page>
<figureCaption confidence="0.999844">
Figure 1: Bootstrapped Learning of Event Dictionaries
</figureCaption>
<bodyText confidence="0.9999791875">
2002) which addresses event-based organization of a
stream of news stories. Event recognition is similar
to New Event Detection, also called First Story De-
tection, which is considered the most difficult TDT
task (Allan et al., 2000a). Typical approaches re-
duce documents to a set of features, either as a word
vector (Allan et al., 2000b) or a probability distri-
bution (Jin et al., 1999), and compare the incoming
stories to stories that appeared in the past by com-
puting similarities between their feature representa-
tions. Recently, event paraphrases (Petrovic et al.,
2012) have been explored to deal with the diversity
of event descriptions. However, the New Event De-
tection task differs from our event recognition task
because we want to find all stories describing a cer-
tain type of event, not just new events.
</bodyText>
<sectionHeader confidence="0.9856555" genericHeader="method">
3 Bootstrapped Learning of Event
Dictionaries
</sectionHeader>
<bodyText confidence="0.9979532">
Our bootstrapping approach consists of two stages
of learning as shown in Figure 1. The process be-
gins with a few agent seeds, purpose phrase patterns,
and unannotated articles selected from a broad-
coverage corpus using event keywords. In the first
stage, event expressions are harvested from the sen-
tences that have both an agent and a purpose phrase
in specific syntactic positions. In the second stage,
new purpose phrases are harvested from sentences
that contain both an event phrase and an agent, while
new agent terms are harvested from sentences that
contain both an event phrase and a purpose phrase.
The new terms are added to growing event dictionar-
ies, and the bootstrapping process repeats. Our work
focuses on civil unrest events.
</bodyText>
<subsectionHeader confidence="0.997983">
3.1 Stage 1: Event Phrase Learning
</subsectionHeader>
<bodyText confidence="0.999836375">
We first extract potential civil unrest stories from the
English Gigaword corpus (Parker et al., 2011) using
six civil unrest keywords. As explained in Section 1,
event keywords are not sufficient to obtain relevant
documents with high precision, so the extracted sto-
ries are a mix of relevant and irrelevant articles. Our
algorithm first selects sentences to use for learning,
and then harvests event expressions from them.
</bodyText>
<subsectionHeader confidence="0.913195">
3.1.1 Event Sentence Identification
</subsectionHeader>
<bodyText confidence="0.888758117647059">
The input in stage 1 consists of a few agent terms
and purpose patterns for seeding. The agent seeds
are single nouns, while the purpose patterns are
verbs in infinitive or present participle forms. Table
1 shows the agent terms and purpose phrases used in
our experiments. The agent terms were manually se-
lected by inspecting the most frequent nouns in the
documents with civil unrest keywords. The purpose
patterns are the most common verbs that describe the
reason for a civil unrest event. We identify probable
event sentences by extracting all sentences that con-
tain at least one agent term and one purpose phrase.
Agents protesters, activists, demonstrators,
students, groups, crowd, workers,
palestinians, supporters, women
Purpose demanding, to demand,
Phrases protesting, to protest
</bodyText>
<tableCaption confidence="0.997374">
Table 1: Agent and Purpose Phrases Used for Seeding
</tableCaption>
<page confidence="0.999432">
43
</page>
<subsectionHeader confidence="0.887581">
3.1.2 Harvesting Event Expressions
</subsectionHeader>
<bodyText confidence="0.999950857142857">
To constrain the learning process, we require
event expressions and purpose phrases to match cer-
tain syntactic structures. We apply the Stanford de-
pendency parser (Marneffe et al., 2006) to the prob-
able event sentences to identify verb phrase candi-
dates and to enforce syntactic constraints between
the different types of event information.
</bodyText>
<figureCaption confidence="0.995095">
Figure 2: Phrasal Structure of Event &amp; Purpose Phrases
</figureCaption>
<bodyText confidence="0.99625790625">
Figure 2 shows the two types of verb phrases
that we learn. One type consists of a verb paired
with the head noun of its direct object. For exam-
ple, event phrases can be “stopped work” or “oc-
cupied offices”, and purpose phrases can be “show
support” or “condemn war”. The second type con-
sists of a verb and an attached prepositional phrase,
retaining only the head noun of the embedded noun
phrase. For example, “took to street” and “scuffled
with police” can be event phrases, while “call for
resignation” and “press for wages” can be purpose
phrases. In both types of verb phrases, a particle can
optionally follow the verb.
Event expressions, agents, and purpose phrases
must appear in specific dependency relations, as il-
lustrated in Figure 3. An agent must be the syn-
tactic subject of the event phrase. A purpose phrase
must be a complement of the event phrase, specif-
ically, we require a particular dependency relation,
“xcomp”1, between the two verb phrases. For ex-
ample, in the sentence “Leftist activists took to
the streets in the Nepali capital Wednesday protest-
ing higher fuel prices.”, the dependency relation
1In the dependency parser, “xcomp” denotes a general rela-
tion between a VP or an ADJP and its open clausal complement.
For example, in the sentence “He says that you like to swim.”,
the “xcomp” relation will link “like” (head) and “swim” (de-
pendent). With our constraints on the verb phrase forms, the
dependent verb phrase in this construction tends to describe the
purpose of the verb phrase.
“xcomp” links “took to the streets” with “protest-
ing higher fuel prices”.
</bodyText>
<figureCaption confidence="0.993337">
Figure 3: Syntactic Dependencies between Agents, Event
Phrases, and Purpose Phrases
</figureCaption>
<bodyText confidence="0.999973">
Given the syntactic construction shown in Figure
3, with a known agent and purpose phrase, we ex-
tract the head verb phrase of the “xcomp” depen-
dency relation as an event phrase candidate. The
event phrases that co-occur with at least two unique
agent terms and two unique purposes phrases are
saved in our event phrase dictionary.
</bodyText>
<subsectionHeader confidence="0.9962675">
3.2 Stage 2: Learning Agent and Purpose
Phrases
</subsectionHeader>
<bodyText confidence="0.999995727272727">
In the second stage of bootstrapping, we learn new
agent terms and purpose phrases. Our rationale is
that if a sentence contains an event phrase and one
other important facet of the event (agent or pur-
pose), then the sentence probably describes a rele-
vant event. We can then look for additional facets
of the event in the same sentence. We learn both
agent and purpose phrases simultaneously in paral-
lel learning processes. As before, we first identify
probable event sentences and then harvest agent and
purpose phrases from these sentences.
</bodyText>
<subsectionHeader confidence="0.780624">
3.2.1 Event Sentence Identification
</subsectionHeader>
<bodyText confidence="0.991917857142857">
We identify probable event sentences by extract-
ing sentences that contain at least one event phrase
(based on the dictionary produced in the first stage
of bootstrapping) and an agent term or a purpose
phrase. As before, the event information must oc-
cur in the sentential dependency structures shown in
Figure 3.
</bodyText>
<subsectionHeader confidence="0.963271">
3.2.2 Harvesting Agent and Purpose Phrases
</subsectionHeader>
<bodyText confidence="0.999985571428572">
The sentences that contain an event phrase and
an agent are used to harvest more purpose phrases,
while the sentences that contain an event phrase
and a purpose phrase are used to harvest more
agent terms. Purpose phrases are extracted from the
phrasal structures shown in Figure 2. In the learn-
ing process for agents, if a sentence has an event
</bodyText>
<page confidence="0.995057">
44
</page>
<bodyText confidence="0.99999480952381">
phrase as the head of the “xcomp” dependency re-
lation and a purpose phrase as the dependent clause
of the “xcomp” dependency relation, then the head
noun of the syntactic subject of the event phrase is
harvested as a candidate agent term. We also record
the modifiers appearing in all of the noun phrases
headed by an agent term. Agent candidates that co-
occur with at least two unique event phrases and at
least two different modifiers of known agent terms
are selected as new agent terms.
The learning process for purpose phrases is anal-
ogous. If the syntactic subject of an event phrase
is an agent and the event phrase is the head of
the “xcomp” dependency relation, then the depen-
dent clause of the “xcomp” dependency relation is
harvested as a candidate purpose phrase. Purpose
phrase candidates that co-occur with at least two dif-
ferent event phrases are selected as purpose phrases.
The bootstrapping process then repeats, ricochet-
ing back and forth between learning event phrases
and learning agent and purpose phrases.
</bodyText>
<subsectionHeader confidence="0.999215">
3.3 Domain Relevance Criteria
</subsectionHeader>
<bodyText confidence="0.990010291666667">
To avoid domain drift during bootstrapping, we use
two additional criteria to discard phrases that are not
necessarily associated with the domain.
For each event phrase and purpose phrase, we es-
timate its domain-specificity as the ratio of its preva-
lence in domain-specific texts compared to broad-
coverage texts. The goal is to discard phrases that
are common across many types of documents, and
therefore not specific to the domain. We define the
domain-specificity of phrase p as:
domain-specificity(p) =frequency of p in domain-specific corpus
frequency of p in broad-coverage corpus
We randomly sampled 10% of the Gigaword texts
that contain a civil unrest event keyword to create
the “domain-specific” corpus, and randomly sam-
pled 10% of the remaining Gigaword texts to cre-
ate the “broad-coverage” corpus.2 Keyword-based
sampling is an approximation to domain-relevance,
but gives us a general idea about the prevalance of a
phrase in different types of texts.
For agent terms, our goal is to identify people who
participate as agents of civil unrest events. Other
types of people may be commonly mentioned in
civil unrest stories too, as peripheral characters. For
</bodyText>
<footnote confidence="0.77018">
2The random sampling was simply for efficiency reasons.
</footnote>
<bodyText confidence="0.944294352941177">
example, police may provide security and reporters
may provide media coverage of an event, but they
are not the agents of the event. We estimate the
event-specificity of each agent term as the ratio of
the phrase’s prevalence in event sentences compared
to all the sentences in the domain-specific corpus.
We define an event sentence as one that contains
both a learned event phrase and a purpose phrase,
based on the dictionaries at that point in time. There-
fore, the number of event sentences increases as the
bootstrapped dictionaries grow. We define the event-
specificity of phrase p as:
event-specificity(p) = frequency of p in event sentences
frequency of p in all sentences
In our experiments we required event and purpose
phrases to have domain-specificity &gt; .33 and agent
terms to have event-specificity &gt; .01.3
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.918114">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9999567">
We conducted experiments to evaluate the perfor-
mance of our bootstrapped event dictionaries for rec-
ognizing civil unrest events. Civil unrest is a broad
term typically used by the media or law enforce-
ment to describe a form of public disturbance that
involves a group of people, usually to protest or pro-
mote a cause. Civil unrest events include strikes,
protests, occupations, rallies, and similar forms of
obstructions or riots. We chose six event keywords to
identify potential civil unrest documents: “protest”,
“strike”, “march”, “rally”, “riot” and “occupy”. We
extracted documents from the English Gigaword
corpus (Parker et al., 2011) that contain at least one
of these event keywords, or a morphological variant
of a keyword.4 This process extracted nearly one
million documents, which we will refer to as our
event-keyword corpus.
We randomly sampled 400 documents5 from the
event-keyword corpus and asked two annotators to
determine whether each document mentioned a civil
</bodyText>
<footnote confidence="0.998937875">
3This value is so small because we simply want to filter
phrases that virtually never occur in the event sentences, and
we can recognize very few event sentences in the early stages
of bootstrapping.
4We used “marched” and “marching” as keywords but did
not use “march” because it often refers to a month.
5These 400 documents were excluded from the unannotated
data used for dictionary learning.
</footnote>
<page confidence="0.998911">
45
</page>
<bodyText confidence="0.999758166666667">
unrest event. We defined annotation guidelines and
conducted an inter-annotator agreement study on
100 of these documents. The annotators achieved a
r. score of .82. We used these 100 documents as our
tuning set. Then each annotator annotated 150 more
documents to create our test set of 300 documents.
</bodyText>
<subsectionHeader confidence="0.981947">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999892">
The first row of Table 2 shows event recognition ac-
curacy when only the event keywords are used. All
of our documents were obtained by searching for a
keyword, but only 101 of the 300 documents in our
test set were labeled as relevant by the annotators
(i.e., 101 describe a civil unrest event). This means
that using only the event keywords to identify civil
unrest documents yields about 34% precision. In a
second experiment, KeywordTitle, we required the
event keyword to be in the title (headline) of the doc-
ument. The KeywordTitle approach produced better
precision (66%), but only 33% of the relevant docu-
ments had a keyword in the title.
</bodyText>
<table confidence="0.999677583333333">
Method Recall Precision F
Keyword Accuracy
Keyword - 34 -
KeywordTitle 33 66 44
Supervised Learning
Unigrams 62 66 64
Unigrams+Bigrams 55 71 62
Bootstrapped Dictionary Lookup
Event Phrases (EV) 60 79 69
Agent Phrases (AG) 98 42 59
Purpose Phrases (PU) 59 67 63
All Pairs 71 88 79
</table>
<tableCaption confidence="0.998741">
Table 2: Experimental Results
</tableCaption>
<bodyText confidence="0.999951583333333">
The second section of Table 2 shows the re-
sults of two supervised classifiers based on 10-fold
cross validation with our test set. Both classifiers
were trained using support vector machines (SVMs)
(Joachims, 1999) with a linear kernel (Keerthi and
DeCoste, 2005). The first classifier used unigrams
as features, while the second classifier used both un-
igrams and bigrams. All the features are binary. The
evaluation results show that the unigram classifier
has an F-score of .64. Using both unigram and bi-
gram features increased precision to 71% but recall
fell by 7%, yielding a slightly lower F-score of .62.
</bodyText>
<subsectionHeader confidence="0.801693">
4.3 Event Recognition with Bootstrapped
Dictionaries
</subsectionHeader>
<bodyText confidence="0.9999083">
Next, we used our bootstrapped dictionaries for
event recognition. The bootstrapping process ran
for 8 iterations and then stopped because no more
phrases could be learned. The quality of boot-
strapped data often degrades as bootstrapping pro-
gresses, so we used the tuning set to evaluate the
dictionaries after each iteration. The best perfor-
mance6 on the tuning set resulted from the dictionar-
ies produced after four iterations, so we used these
dictionaries for our experiments. Table 3 shows the
</bodyText>
<table confidence="0.9984825">
Event Agent Purpose
Phrases Terms Phrases
Iter #1 145 67 124
Iter #2 410 106 356
Iter #3 504 130 402
Iter #4 623 139 569
</table>
<tableCaption confidence="0.998917">
Table 3: Dictionary Sizes after Several Iterations
</tableCaption>
<bodyText confidence="0.98681547368421">
number of event phrases, agents and purpose phrases
learned after each iteration. All three lexicons were
significantly enriched after each iteration. The final
bootstrapped dictionaries contain 623 event phrases,
569 purpose phrases and 139 agent terms. Table 4
shows samples from each event dictionary.
Event Phrases: went on strike, took to street,
chanted slogans, gathered in capital, formed chain,
clashed with police, staged rally, held protest,
walked off job, burned flags, set fire, hit streets,
marched in city, blocked roads, carried placards
Agent Terms: employees, miners, muslims, unions,
protestors, journalists, refugees, prisoners, immigrants,
inmates, pilots, farmers, followers, teachers, drivers
Purpose Phrases: accusing government, voice anger,
press for wages, oppose plans, urging end, defying ban,
show solidarity, mark anniversary, calling for right,
condemning act, pressure government, mark death,
push for hike, call attention, celebrating withdrawal
</bodyText>
<tableCaption confidence="0.984111">
Table 4: Examples of Dictionary Entries
</tableCaption>
<bodyText confidence="0.9995668">
The third section of Table 2 shows the results
when using the bootstrapped dictionaries for event
recognition. We used a simple dictionary look-up
approach that searched for dictionary entries in each
document. Our phrases were generated based on
</bodyText>
<footnote confidence="0.965763">
6Based on the performance for the All Pairs approach.
</footnote>
<page confidence="0.99966">
46
</page>
<bodyText confidence="0.999188375">
syntactic analysis and only head words were re-
tained for generality. But we wanted to match dic-
tionary entries without requiring syntactic analysis
of new documents. So we used an approximate
matching scheme that required each word to appear
within 5 words of the previous word. For example,
“held protest” would match “held a large protest”
and “held a very large political protest”. In this way,
we avoid the need for syntactic analysis when using
the dictionaries for event recognition.
First, we labeled a document as relevant if it con-
tained any Event Phrase (EV) in our dictionary. The
event phrases achieved better performance than all
of the baselines, yielding an F-score of 69%. The
best baseline was the unigram classifier, which was
trained with supervised learning. The bootstrapped
event phrase dictionary produced much higher pre-
cision (79% vs. 66%) with only slightly lower recall
(60% vs. 62%), and did not require annotated texts
for training. Statistical significance testing shows
that the Event Phrase lookup approach works signif-
icantly better than the unigram classifier (p &lt; 0.05,
paired bootstrap (Berg-Kirkpatrick et al., 2012)).
For the sake of completeness, we also evaluated
the performance of dictionary look-up using our
bootstrapped Agent (AG) and Purpose (PU) dictio-
naries, individually. The agents terms produced 42%
precision with 98% recall, demonstrating that the
learned agent list has extremely high coverage but
(unsurprisingly) does not achieve high precision on
its own. The purpose phrases achieved a better bal-
ance of recall and precision, producing an F-score
of 63%, which is nearly the same as the supervised
unigram classifier.
Our original hypothesis was that a single type of
event information is not sufficient to accurately iden-
tify event descriptions. Our goal was high-accuracy
event recognition by requiring that a document con-
tain multiple clues pertaining to different facets of an
event (multi-faceted event recognition). The last row
of Table 2 (All Pairs) shows the results when requir-
ing matches from at least two different bootstrapped
dictionaries. Specifically, we labeled a document
as relevant if it contained at least one phrase from
each of two different dictionaries and these phrases
occurred in the same sentence. Table 2 shows that
multi-faceted event recognition achieves 88% preci-
sion with reasonably good recall of 71%, yielding an
F-score of 79%. This multi-faceted approach with
simple dictionary look-up outperformed all of the
baselines, and each dictionary used by itself. Sta-
tistical significance testing shows that the All Pairs
approach works significantly better than the unigram
classifier (p &lt; 0.001, paired bootstrap). The All
Pairs approach is significantly better than the Event
Phrase (EV) lookup approach at the p &lt; 0.1 level.
</bodyText>
<table confidence="0.9994184">
Method Recall Precision F-score
EV + PU 14 100 24
EV + AG 47 94 62
AG + PU 50 85 63
All Pairs 71 88 79
</table>
<tableCaption confidence="0.999715">
Table 5: Analysis of Dictionary Combinations
</tableCaption>
<bodyText confidence="0.997235">
Table 5 takes a closer look at how each pair of
dictionaries performed. The first row shows that re-
quiring a document to have an event phrase and a
purpose phrase produces the best precision (100%)
but with low recall (14%). The second row reveals
that requiring a document to have an event phrase
and an agent term yields better recall (47%) and high
precision (94%). The third row shows that requiring
a document to have a purpose phrase and an agent
term produces the best recall (50%) but with slightly
lower precision (85%). Finally, the last row of Ta-
ble 5 shows that taking the union of these results
(i.e., any combination of dictionary pairs is suffi-
cient) yields the best recall (71%) with high preci-
sion (88%), demonstrating that we get the best cov-
erage by recognizing multiple combinations of event
information.
</bodyText>
<table confidence="0.9992765">
Lexicon Recall Precision F-score
Seeds 13 87 22
Iter #1 50 88 63
Iter #2 63 89 74
Iter #3 68 88 77
Iter #4 71 88 79
</table>
<tableCaption confidence="0.9295005">
Table 6: All Pairs Lookup Results using only Seeds and
the Lexicons Learned after each Iteration, on the Test Set
</tableCaption>
<bodyText confidence="0.998235">
Table 6 shows the performance of the lexicon
lookup approach using the All Pairs criteria dur-
ing the bootstrapping process. The first row shows
the results using only 10 agent seeds and 4 purpose
seeds as shown in Table 1. The following four rows
in the table show the performance of All Pairs using
</bodyText>
<page confidence="0.998435">
47
</page>
<bodyText confidence="0.999210384615385">
the lexicons learned after each bootstrapping itera-
tion. We can see that the recall increases steadily and
that precision is maintained at a high level through-
out the bootstrapping process.
Event recognition can be formulated as an infor-
mation retrieval (IR) problem. As another point of
comparison, we ran an existing IR system, Terrier
(Ounis et al., 2007), on our test set. We used Ter-
rier to rank these 300 documents given our set of
event keywords as the query 7, and then generated a
recall/precision curve (Figure 4) by computing the
precisions at different levels of recall, ranging from
0 to 1 in increments of .10. Terrier was run with the
</bodyText>
<figureCaption confidence="0.995324">
Figure 4: Comparison with the Terrier IR system
</figureCaption>
<bodyText confidence="0.999762636363636">
parameter PL2 which refers to an advanced Diver-
gence From Randomness weighting model (Amati
and Van Rijsbergen, 2002). In addition, Terrier used
automatic query expansion. We can see that Terrier
identified the first 60 documents (20% recall) with
100% precision. But precision dropped sharply after
that. The circle in Figure 4 shows the performance
of our bootstrapped dictionaries using the All Pairs
approach. At comparable level of precision (88%),
Terrier achieved about 45% recall versus 71% recall
produced with the bootstrapped dictionaries.
</bodyText>
<subsectionHeader confidence="0.949961">
4.4 Supervised Classifiers with Bootstrapped
Dictionaries
</subsectionHeader>
<bodyText confidence="0.996569666666667">
We also explored the idea of using the bootstrapped
dictionaries as features for a classifier to see if a su-
pervised learner could make better use of the dic-
</bodyText>
<subsectionHeader confidence="0.526251">
7We gave Terrier one query with all of the event keywords.
</subsectionHeader>
<bodyText confidence="0.7701925">
tionaries. We created five SVM classifiers and per-
formed 10-fold cross validation on the test set.
</bodyText>
<table confidence="0.999555833333333">
Method Recall Precision F-score
TermLex 66 85 74
PairLex 10 91 18
TermSets 59 83 69
PairSets 68 84 75
AllSets 70 84 76
</table>
<tableCaption confidence="0.999581">
Table 7: Supervised classifiers using the dictionaries
</tableCaption>
<bodyText confidence="0.987176722222222">
Table 7 shows the results for the five classifiers.
TermLex encodes a binary feature for every phrase
in any of our dictionaries. PairLex encodes a binary
feature for each pair of phrases from two different
dictionaries and requires them to occur in the same
sentence. The TermLex classifier achieves good per-
formance (74% F-score), but is not as effective as
our All Pairs dictionary look-up approach (79% F-
score). The PairLex classifier yield higher precision
but very low recall, undoubtedly due to sparsity is-
sues in matching specific pairs of phrases.
One of the strengths of our bootstrapping method
is that it creates dictionaries from large volumes of
unannotated documents. A limitation of supervised
learning with lexical features is that the classifier can
not benefit from terms in the bootstrapped dictionar-
ies that do not appear in its training documents. To
address this issue, we also tried encoding the dic-
tionaries as set-based features. The TermSets clas-
sifier encodes three binary features, one for each
dictionary. A feature gets a value of 1 if a docu-
ment contains any word in the corresponding dictio-
nary. The PairSets classifier also encodes three bi-
nary features, but each feature represents a different
pair of dictionaries (EV+AG, EV+PU, or AG+PU).
A feature gets a value of 1 if a document contains at
least one term from each of the two dictionaries in
the same sentence. The AllSets classifier encodes 7
set-based features: the previous six features and one
additional feature that requires a sentence to contain
at least one entry from all three dictionaries.
The All Sets classifier yields the best performance
with an F-score of 76%. However, our straightfor-
ward dictionary look-up approach still performs bet-
ter (79% F-score), and does not require annotated
documents for training.
</bodyText>
<figure confidence="0.998050357142857">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Precision
</figure>
<page confidence="0.993726">
48
</page>
<subsectionHeader confidence="0.980452">
4.5 Finding Articles with no Event Keyword
</subsectionHeader>
<bodyText confidence="0.999708736842105">
The learned event dictionaries have the potential to
recognize event-relevant documents that do not con-
tain any human-selected event keywords. This can
happen in two ways. First, 378 of the 623 learned
event phrases do not contain any of the original event
keywords. Second, we expect that some event de-
scriptions will contain a known agent and purpose
phrase, even if the event phrase is unfamiliar.
We performed an additional set of experiments
with documents in the Gigaword corpus that contain
no human-selected civil unrest keyword. Following
our multi-faceted approach to event recognition, we
collected all documents that contain a sentence that
matches phrases in at least two of our bootstrapped
event dictionaries. This process retrieved 178,197
documents. The first column of Table 8 shows the
number of documents that had phrases found in two
different dictionaries (EV+AG, EV+PU, AG+PU) or
in all three dictionaries (EV+AG+PU).
</bodyText>
<table confidence="0.9981962">
Total Samples Accuracy
EV+AG 67,796 50 44%
EV+PU 2,375 50 54%
AG+PU 101,173 50 18%
EV+AG+PU 6,853 50 74%
</table>
<tableCaption confidence="0.999849">
Table 8: Evaluation of articles with no event keyword
</tableCaption>
<bodyText confidence="0.999993928571429">
We randomly sampled 50 documents from each
category and had them annotated. The accura-
cies are shown in the third column. Finding all
three types of phrases produced the best accuracy,
74%. Furthermore, we found over 6,800 documents
that had all three types of event information us-
ing our learned dictionaries. This result demon-
strates that the bootstrapped dictionaries can recog-
nize many event descriptions that would have been
missed by searching only with manually selected
keywords. This experiment also confirms that multi-
facted event recognition using all three learned dic-
tionaries achieves good accuracy even for docu-
ments that do not contain the civil unrest keywords.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999953">
We proposed a multi-faceted approach to event
recognition and presented a bootstrapping technique
to learn event phrases as well as agent terms and
purpose phrases associated with civil unrest events.
Our results showed that multi-faceted event recog-
nition using the learned dictionaries achieved high
accuracy and performed better than several other
methods. The bootstrapping approach can be eas-
ily trained for new domains since it requires only
a large collection of unannotated texts and a few
event keywords, agent terms, and purpose phrases
for the events of interest. Furthermore, although the
training phase requires syntactic parsing to learn the
event dictionaries, the dictionaries can then be used
for event recognition without needing to parse the
documents.
An open question for future work is to investigate
whether the same multi-faceted approach to event
recognition will work well for other types of events.
Our belief is that many different types of events have
characteristic agent terms, but additional types of
facets will need to be defined to cover a broad array
of event types. The syntactic constructions used to
harvest dictionary items may also vary depending on
the types of event information that must be learned.
In future research, we plan to explore these issues in
more depth to design a more general multi-faceted
event recognition system, and we plan to investigate
new ways to use these event dictionaries for event
extraction as well.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999480384615385">
This research was supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Interior National Business Center (DoI /
NBC) contract number D12PC00285 and by the Na-
tional Science Foundation under grant IIS-1018314.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted as
necessarily representing the official policies or en-
dorsements, either expressed or implied, of IARPA,
DoI/NBE, NSF, or the U.S. Government.
</bodyText>
<page confidence="0.999271">
49
</page>
<sectionHeader confidence="0.986758" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999409475728155">
ACE Evaluations. 2006.
http://www.itl.nist.gov/iad/mig/tests/ace/.
J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. 1998. Topic Detection and Tracking Pilot
Study: Final Report. In Proceedings ofDARPA Broad-
cast News Transcription and Understanding Work-
shop.
J. Allan, V. Lavrenko, and H. Jin. 2000a. First Story
Detection in TDT is Hard. In Proceedings of the 2000
ACM CIKM International Conference on Information
and Knowledge Management.
J. Allan, Victor Lavrenko, Daniella Malin, and Russell
Swan. 2000b. Detections, Bounds, and Timelines:
UMass and TDT-3. In Proceedings of Topic Detection
and Tracking Workshop.
J. Allan, 2002. Topic Detection and Tracking: Event
Based Information Organization. Kluwer Academic
Publishers.
G. Amati and C. J. Van Rijsbergen. 2002. Probabilistic
Models of Information Retrieval based on Measuring
Divergence from Randomness. ACM Transactions on
Information Systems, 20(4):357–389.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
H. Becker, M. Naaman, and L. Gravano. 2011. Be-
yond trending topics: Real-world event identification
on twitter. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds.
T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An
Empirical Investigation of Statistical Significance in
NLP. In Proceedings of the 2012 Conference on Em-
pirical Methods in Natural Language Processing.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177–210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
M. D. Conover, J. Ratkiewicz, M. Francisco,
B. Goncalves, A. Flammini, and F. Menczer. 2011.
Political Polarization on Twitter. In Proceedings of
the Fifth International AAAI Conference on Weblogs
and Social Media.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481–488, Sydney, Australia, July.
R. Huang and E. Riloff. 2011. Peeling Back the Layers:
Detecting Event Role Fillers in Secondary Contexts.
H. Jin, R. Schwartz, S. Sista, and F. Walls. 1999. Topic
Tracking for Radio, TV broadcast, and Newswire. In
EUROSPEECH.
T. Joachims. 1999. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Sch¨olkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal ofMachine Learning Research.
V. Lampos, T. D. Bie, and N. Cristianini. 2010. Flu
Detector - Tracking Epidemics on Twitter. In ECML
PKDD.
M. d. Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the Fifth
Conference on Language Resources and Evaluation
(LREC-2006).
M. Mathioudakis and N. Koudas. 2010. TwitterMonitor:
trend detection over the twitter stream. In Proceedings
of the 2010 international conference on Management
of data, page 11551158. ACM.
D. Metzler, C. Cai, and E. Hovy. 2012. Structured Event
Retrieval over Microblog Archives. In Proceedings of
The 2012 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
I. Ounis, C. Lioma, C. Macdonald, and V. Plachouras.
2007. Research Directions in Terrier. Novat-
ica/UPGRADE Special Issue on Web Information Ac-
cess, Ricardo Baeza-Yates et al. (Eds), Invited Paper.
R. Parker, D. Graff, J. Kong, K. Chen, and Kazuaki M.
2011. English Gigaword. In Linguistic Data Consor-
tium.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Petrovic, M. Osborne, and V. Lavrenko. 2012. Us-
ing Paraphrases for Improving First Story Detection in
</reference>
<page confidence="0.949641">
50
</page>
<reference confidence="0.998404945454546">
News and Twitter. In Proceedings of The 2012 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.
A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe.
2011. Extracting events and event descriptions from
twitter.
E. Riloff and W. Lehnert. 1994. Information Ex-
traction as a Basis for High-Precision Text Classifi-
cation. ACM Transactions on Information Systems,
12(3):296–333, July.
E. Riloff and J. Lorenzen. 1999. Extraction-based text
categorization: Generating domain-specific role rela-
tionships automatically. In Tomek Strzalkowski, edi-
tor, Natural Language Information Retrieval. Kluwer
Academic Publishers.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044–1049. The AAAI Press/MIT Press.
A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open
domain event extraction from twitter. In The Proceed-
ings of The 18th ACMSIGKDD Conference on Knowl-
edge Discovery and Data Mining.
T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes twitter users: real-time event detection
by social sensors.
S. Sekine. 2006. On-demand Information Extrac-
tion. In Proceedings of Joint Conference of the In-
ternational Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL-06).
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379–386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting Elections with Twitter: What
140 Characters Reveal about Political Sentiment. In
Proceedings of the 4th International AAAI Conference
on Weblogs and Social Media.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
X. Zhang, H. Fuehres, and P. A. Gloor. 2010. Predicting
Stock Market Indicators Through Twitter ”I hope it is
not as bad as I fear”. In COINs.
</reference>
<page confidence="0.999078">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793673">
<title confidence="0.999908">Multi-faceted Event Recognition with Bootstrapped Dictionaries</title>
<author confidence="0.999063">Ruihong Huang</author>
<author confidence="0.999063">Ellen</author>
<affiliation confidence="0.9982665">School of University of</affiliation>
<address confidence="0.806513">Salt Lake City, UT</address>
<abstract confidence="0.999265222222222">Identifying documents that describe a specific type of event is challenging due to the high complexity and variety of event descriptions. propose a event recognition approach, which identifies documents about an event using event phrases as well as defining characteristics of the event. Our research focuses on civil unrest events and learns civil unrest expressions as well as phrases corresponding to potential agents and reasons for civil unrest. We present a bootstrapping algorithm that automatically acquires event phrases, agent terms, and purpose (reason) phrases from unannotated texts. We use the bootstrapped dictionaries to identify civil unrest documents and show that multi-faceted event recognition can yield high accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE Evaluations</author>
</authors>
<date>2006</date>
<note>http://www.itl.nist.gov/iad/mig/tests/ace/.</note>
<contexts>
<context position="5940" citStr="Evaluations, 2006" startWordPosition="942" endWordPosition="943">ore several ways of using these bootstrapped dictionaries. We conclude that finding at least two different types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-bu</context>
</contexts>
<marker>Evaluations, 2006</marker>
<rawString>ACE Evaluations. 2006. http://www.itl.nist.gov/iad/mig/tests/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Carbonell</author>
<author>G Doddington</author>
<author>J Yamron</author>
<author>Y Yang</author>
</authors>
<title>Topic Detection and Tracking Pilot Study: Final Report.</title>
<date>1998</date>
<booktitle>In Proceedings ofDARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="8175" citStr="Allan et al., 1998" startWordPosition="1289" endWordPosition="1292">a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, </context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>J. Allan, J. Carbonell, G. Doddington, J. Yamron, and Y. Yang. 1998. Topic Detection and Tracking Pilot Study: Final Report. In Proceedings ofDARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>V Lavrenko</author>
<author>H Jin</author>
</authors>
<title>First Story Detection in TDT is Hard.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 ACM CIKM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="8472" citStr="Allan et al., 2000" startWordPosition="1335" endWordPosition="1338">abeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped </context>
</contexts>
<marker>Allan, Lavrenko, Jin, 2000</marker>
<rawString>J. Allan, V. Lavrenko, and H. Jin. 2000a. First Story Detection in TDT is Hard. In Proceedings of the 2000 ACM CIKM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>Victor Lavrenko</author>
<author>Daniella Malin</author>
<author>Russell Swan</author>
</authors>
<title>Detections, Bounds, and Timelines: UMass and TDT-3.</title>
<date>2000</date>
<booktitle>In Proceedings of Topic Detection and Tracking Workshop.</booktitle>
<contexts>
<context position="8472" citStr="Allan et al., 2000" startWordPosition="1335" endWordPosition="1338">abeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped </context>
</contexts>
<marker>Allan, Lavrenko, Malin, Swan, 2000</marker>
<rawString>J. Allan, Victor Lavrenko, Daniella Malin, and Russell Swan. 2000b. Detections, Bounds, and Timelines: UMass and TDT-3. In Proceedings of Topic Detection and Tracking Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Topic Detection and Tracking: Event Based Information Organization.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Allan, 2002</marker>
<rawString>J. Allan, 2002. Topic Detection and Tracking: Event Based Information Organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Amati</author>
<author>C J Van Rijsbergen</author>
</authors>
<title>Probabilistic Models of Information Retrieval based on Measuring Divergence from Randomness.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Amati, Van Rijsbergen, 2002</marker>
<rawString>G. Amati and C. J. Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval based on Measuring Divergence from Randomness. ACM Transactions on Information Systems, 20(4):357–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Appelt</author>
<author>J Hobbs</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Tyson</author>
</authors>
<title>FASTUS: a finite-state processor for information extraction from real-world text.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5961" citStr="Appelt et al., 1993" startWordPosition="944" endWordPosition="947"> using these bootstrapped dictionaries. We conclude that finding at least two different types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction </context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Tyson, 1993</marker>
<rawString>D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993. FASTUS: a finite-state processor for information extraction from real-world text. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Becker</author>
<author>M Naaman</author>
<author>L Gravano</author>
</authors>
<title>Beyond trending topics: Real-world event identification on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="6872" citStr="Becker et al., 2011" startWordPosition="1089" endWordPosition="1092">of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event inform</context>
</contexts>
<marker>Becker, Naaman, Gravano, 2011</marker>
<rawString>H. Becker, M. Naaman, and L. Gravano. 2011. Beyond trending topics: Real-world event identification on twitter. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Benson</author>
<author>A Haghighi</author>
<author>R Barzilay</author>
</authors>
<title>Event discovery in social media feeds.</title>
<date>2011</date>
<contexts>
<context position="7046" citStr="Benson et al., 2011" startWordPosition="1117" endWordPosition="1121">ing event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused</context>
</contexts>
<marker>Benson, Haghighi, Barzilay, 2011</marker>
<rawString>E. Benson, A. Haghighi, and R. Barzilay. 2011. Event discovery in social media feeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berg-Kirkpatrick</author>
<author>D Burkett</author>
<author>D Klein</author>
</authors>
<title>An Empirical Investigation of Statistical Significance in NLP.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="24559" citStr="Berg-Kirkpatrick et al., 2012" startWordPosition="3938" endWordPosition="3941">elevant if it contained any Event Phrase (EV) in our dictionary. The event phrases achieved better performance than all of the baselines, yielding an F-score of 69%. The best baseline was the unigram classifier, which was trained with supervised learning. The bootstrapped event phrase dictionary produced much higher precision (79% vs. 66%) with only slightly lower recall (60% vs. 62%), and did not require annotated texts for training. Statistical significance testing shows that the Event Phrase lookup approach works significantly better than the unigram classifier (p &lt; 0.05, paired bootstrap (Berg-Kirkpatrick et al., 2012)). For the sake of completeness, we also evaluated the performance of dictionary look-up using our bootstrapped Agent (AG) and Purpose (PU) dictionaries, individually. The agents terms produced 42% precision with 98% recall, demonstrating that the learned agent list has extremely high coverage but (unsurprisingly) does not achieve high precision on its own. The purpose phrases achieved a better balance of recall and precision, producing an F-score of 63%, which is nearly the same as the supervised unigram classifier. Our original hypothesis was that a single type of event information is not su</context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>T. Berg-Kirkpatrick, D. Burkett, and D. Klein. 2012. An Empirical Investigation of Statistical Significance in NLP. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Califf</author>
<author>R Mooney</author>
</authors>
<title>Bottom-up Relational Learning of Pattern Matching rules for Information Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--177</pages>
<contexts>
<context position="6044" citStr="Califf and Mooney, 2003" startWordPosition="959" endWordPosition="962">ifferent types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into </context>
</contexts>
<marker>Califf, Mooney, 2003</marker>
<rawString>M.E. Califf and R. Mooney. 2003. Bottom-up Relational Learning of Pattern Matching rules for Information Extraction. Journal of Machine Learning Research, 4:177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Chieu</author>
<author>H T Ng</author>
</authors>
<title>A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6019" citStr="Chieu and Ng, 2002" startWordPosition="955" endWordPosition="958">nding at least two different types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated eve</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text. In Proceedings of the 18th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Conover</author>
<author>J Ratkiewicz</author>
<author>M Francisco</author>
<author>B Goncalves</author>
<author>A Flammini</author>
<author>F Menczer</author>
</authors>
<title>Political Polarization on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="7268" citStr="Conover et al., 2011" startWordPosition="1154" endWordPosition="1157">nt extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled pattern</context>
</contexts>
<marker>Conover, Ratkiewicz, Francisco, Goncalves, Flammini, Menczer, 2011</marker>
<rawString>M. D. Conover, J. Ratkiewicz, M. Francisco, B. Goncalves, A. Flammini, and F. Menczer. 2011. Political Polarization on Twitter. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gu</author>
<author>N Cercone</author>
</authors>
<title>Segment-Based Hidden Markov Models for Information Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>481--488</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6724" citStr="Gu and Cercone, 2006" startWordPosition="1064" endWordPosition="1067">2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritte</context>
</contexts>
<marker>Gu, Cercone, 2006</marker>
<rawString>Z. Gu and N. Cercone. 2006. Segment-Based Hidden Markov Models for Information Extraction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 481–488, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huang</author>
<author>E Riloff</author>
</authors>
<title>Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts.</title>
<date>2011</date>
<contexts>
<context position="6778" citStr="Huang and Riloff, 2011" startWordPosition="1073" endWordPosition="1076">acting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mention</context>
</contexts>
<marker>Huang, Riloff, 2011</marker>
<rawString>R. Huang and E. Riloff. 2011. Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jin</author>
<author>R Schwartz</author>
<author>S Sista</author>
<author>F Walls</author>
</authors>
<title>Topic Tracking for Radio, TV broadcast, and Newswire.</title>
<date>1999</date>
<booktitle>In EUROSPEECH.</booktitle>
<contexts>
<context position="8628" citStr="Jin et al., 1999" startWordPosition="1363" endWordPosition="1366">ut still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped Learning of Event Dictionaries Our bootstrapping approach consists of two stages of learning as shown in Figure 1. The process begins with a few agent seeds</context>
</contexts>
<marker>Jin, Schwartz, Sista, Walls, 1999</marker>
<rawString>H. Jin, R. Schwartz, S. Sista, and F. Walls. 1999. Topic Tracking for Radio, TV broadcast, and Newswire. In EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale Support Vector Machine Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In A. Smola B. Sch¨olkopf, C. Burges, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20959" citStr="Joachims, 1999" startWordPosition="3384" endWordPosition="3385">better precision (66%), but only 33% of the relevant documents had a keyword in the title. Method Recall Precision F Keyword Accuracy Keyword - 34 - KeywordTitle 33 66 44 Supervised Learning Unigrams 62 66 64 Unigrams+Bigrams 55 71 62 Bootstrapped Dictionary Lookup Event Phrases (EV) 60 79 69 Agent Phrases (AG) 98 42 59 Purpose Phrases (PU) 59 67 63 All Pairs 71 88 79 Table 2: Experimental Results The second section of Table 2 shows the results of two supervised classifiers based on 10-fold cross validation with our test set. Both classifiers were trained using support vector machines (SVMs) (Joachims, 1999) with a linear kernel (Keerthi and DeCoste, 2005). The first classifier used unigrams as features, while the second classifier used both unigrams and bigrams. All the features are binary. The evaluation results show that the unigram classifier has an F-score of .64. Using both unigram and bigram features increased precision to 71% but recall fell by 7%, yielding a slightly lower F-score of .62. 4.3 Event Recognition with Bootstrapped Dictionaries Next, we used our bootstrapped dictionaries for event recognition. The bootstrapping process ran for 8 iterations and then stopped because no more ph</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making Large-Scale Support Vector Machine Learning Practical. In A. Smola B. Sch¨olkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keerthi</author>
<author>D DeCoste</author>
</authors>
<title>A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs.</title>
<date>2005</date>
<journal>Journal ofMachine Learning Research.</journal>
<contexts>
<context position="21008" citStr="Keerthi and DeCoste, 2005" startWordPosition="3390" endWordPosition="3393">f the relevant documents had a keyword in the title. Method Recall Precision F Keyword Accuracy Keyword - 34 - KeywordTitle 33 66 44 Supervised Learning Unigrams 62 66 64 Unigrams+Bigrams 55 71 62 Bootstrapped Dictionary Lookup Event Phrases (EV) 60 79 69 Agent Phrases (AG) 98 42 59 Purpose Phrases (PU) 59 67 63 All Pairs 71 88 79 Table 2: Experimental Results The second section of Table 2 shows the results of two supervised classifiers based on 10-fold cross validation with our test set. Both classifiers were trained using support vector machines (SVMs) (Joachims, 1999) with a linear kernel (Keerthi and DeCoste, 2005). The first classifier used unigrams as features, while the second classifier used both unigrams and bigrams. All the features are binary. The evaluation results show that the unigram classifier has an F-score of .64. Using both unigram and bigram features increased precision to 71% but recall fell by 7%, yielding a slightly lower F-score of .62. 4.3 Event Recognition with Bootstrapped Dictionaries Next, we used our bootstrapped dictionaries for event recognition. The bootstrapping process ran for 8 iterations and then stopped because no more phrases could be learned. The quality of bootstrapp</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>S. Keerthi and D. DeCoste. 2005. A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs. Journal ofMachine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lampos</author>
<author>T D Bie</author>
<author>N Cristianini</author>
</authors>
<title>Flu Detector - Tracking Epidemics on Twitter.</title>
<date>2010</date>
<booktitle>In ECML PKDD.</booktitle>
<contexts>
<context position="7118" citStr="Lampos et al., 2010" startWordPosition="1131" endWordPosition="1134">about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information ext</context>
</contexts>
<marker>Lampos, Bie, Cristianini, 2010</marker>
<rawString>V. Lampos, T. D. Bie, and N. Cristianini. 2010. Flu Detector - Tracking Epidemics on Twitter. In ECML PKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M d Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<contexts>
<context position="11412" citStr="Marneffe et al., 2006" startWordPosition="1806" endWordPosition="1809">t describe the reason for a civil unrest event. We identify probable event sentences by extracting all sentences that contain at least one agent term and one purpose phrase. Agents protesters, activists, demonstrators, students, groups, crowd, workers, palestinians, supporters, women Purpose demanding, to demand, Phrases protesting, to protest Table 1: Agent and Purpose Phrases Used for Seeding 43 3.1.2 Harvesting Event Expressions To constrain the learning process, we require event expressions and purpose phrases to match certain syntactic structures. We apply the Stanford dependency parser (Marneffe et al., 2006) to the probable event sentences to identify verb phrase candidates and to enforce syntactic constraints between the different types of event information. Figure 2: Phrasal Structure of Event &amp; Purpose Phrases Figure 2 shows the two types of verb phrases that we learn. One type consists of a verb paired with the head noun of its direct object. For example, event phrases can be “stopped work” or “occupied offices”, and purpose phrases can be “show support” or “condemn war”. The second type consists of a verb and an attached prepositional phrase, retaining only the head noun of the embedded noun</context>
</contexts>
<marker>Marneffe, MacCartney, Manning, 2006</marker>
<rawString>M. d. Marneffe, B. MacCartney, and C. D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the Fifth Conference on Language Resources and Evaluation (LREC-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mathioudakis</author>
<author>N Koudas</author>
</authors>
<title>TwitterMonitor: trend detection over the twitter stream.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 international conference on Management of data,</booktitle>
<pages>11551158</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7150" citStr="Mathioudakis and Koudas, 2010" startWordPosition="1135" endWordPosition="1138">ut relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relev</context>
</contexts>
<marker>Mathioudakis, Koudas, 2010</marker>
<rawString>M. Mathioudakis and N. Koudas. 2010. TwitterMonitor: trend detection over the twitter stream. In Proceedings of the 2010 international conference on Management of data, page 11551158. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>C Cai</author>
<author>E Hovy</author>
</authors>
<title>Structured Event Retrieval over Microblog Archives. In</title>
<date>2012</date>
<booktitle>Proceedings of The 2012 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="7415" citStr="Metzler et al., 2012" startWordPosition="1179" endWordPosition="1182"> recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but s</context>
</contexts>
<marker>Metzler, Cai, Hovy, 2012</marker>
<rawString>D. Metzler, C. Cai, and E. Hovy. 2012. Structured Event Retrieval over Microblog Archives. In Proceedings of The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-4 Proceedings</author>
</authors>
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6322" citStr="Proceedings, 1992" startWordPosition="1006" endWordPosition="1007"> information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies </context>
</contexts>
<marker>Proceedings, 1992</marker>
<rawString>MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Ounis</author>
<author>C Lioma</author>
<author>C Macdonald</author>
<author>V Plachouras</author>
</authors>
<date>2007</date>
<journal>Research Directions in Terrier. Novatica/UPGRADE Special Issue on Web Information Access, Ricardo Baeza-Yates</journal>
<tech>(Eds), Invited Paper.</tech>
<contexts>
<context position="28088" citStr="Ounis et al., 2007" startWordPosition="4533" endWordPosition="4536">on lookup approach using the All Pairs criteria during the bootstrapping process. The first row shows the results using only 10 agent seeds and 4 purpose seeds as shown in Table 1. The following four rows in the table show the performance of All Pairs using 47 the lexicons learned after each bootstrapping iteration. We can see that the recall increases steadily and that precision is maintained at a high level throughout the bootstrapping process. Event recognition can be formulated as an information retrieval (IR) problem. As another point of comparison, we ran an existing IR system, Terrier (Ounis et al., 2007), on our test set. We used Terrier to rank these 300 documents given our set of event keywords as the query 7, and then generated a recall/precision curve (Figure 4) by computing the precisions at different levels of recall, ranging from 0 to 1 in increments of .10. Terrier was run with the Figure 4: Comparison with the Terrier IR system parameter PL2 which refers to an advanced Divergence From Randomness weighting model (Amati and Van Rijsbergen, 2002). In addition, Terrier used automatic query expansion. We can see that Terrier identified the first 60 documents (20% recall) with 100% precisi</context>
</contexts>
<marker>Ounis, Lioma, Macdonald, Plachouras, 2007</marker>
<rawString>I. Ounis, C. Lioma, C. Macdonald, and V. Plachouras. 2007. Research Directions in Terrier. Novatica/UPGRADE Special Issue on Web Information Access, Ricardo Baeza-Yates et al. (Eds), Invited Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Parker</author>
<author>D Graff</author>
<author>J Kong</author>
<author>K Chen</author>
<author>M Kazuaki</author>
</authors>
<title>English Gigaword. In Linguistic Data Consortium.</title>
<date>2011</date>
<contexts>
<context position="9985" citStr="Parker et al., 2011" startWordPosition="1584" endWordPosition="1587">expressions are harvested from the sentences that have both an agent and a purpose phrase in specific syntactic positions. In the second stage, new purpose phrases are harvested from sentences that contain both an event phrase and an agent, while new agent terms are harvested from sentences that contain both an event phrase and a purpose phrase. The new terms are added to growing event dictionaries, and the bootstrapping process repeats. Our work focuses on civil unrest events. 3.1 Stage 1: Event Phrase Learning We first extract potential civil unrest stories from the English Gigaword corpus (Parker et al., 2011) using six civil unrest keywords. As explained in Section 1, event keywords are not sufficient to obtain relevant documents with high precision, so the extracted stories are a mix of relevant and irrelevant articles. Our algorithm first selects sentences to use for learning, and then harvests event expressions from them. 3.1.1 Event Sentence Identification The input in stage 1 consists of a few agent terms and purpose patterns for seeding. The agent seeds are single nouns, while the purpose patterns are verbs in infinitive or present participle forms. Table 1 shows the agent terms and purpose </context>
<context position="18730" citStr="Parker et al., 2011" startWordPosition="3011" endWordPosition="3014">ts to evaluate the performance of our bootstrapped event dictionaries for recognizing civil unrest events. Civil unrest is a broad term typically used by the media or law enforcement to describe a form of public disturbance that involves a group of people, usually to protest or promote a cause. Civil unrest events include strikes, protests, occupations, rallies, and similar forms of obstructions or riots. We chose six event keywords to identify potential civil unrest documents: “protest”, “strike”, “march”, “rally”, “riot” and “occupy”. We extracted documents from the English Gigaword corpus (Parker et al., 2011) that contain at least one of these event keywords, or a morphological variant of a keyword.4 This process extracted nearly one million documents, which we will refer to as our event-keyword corpus. We randomly sampled 400 documents5 from the event-keyword corpus and asked two annotators to determine whether each document mentioned a civil 3This value is so small because we simply want to filter phrases that virtually never occur in the event sentences, and we can recognize very few event sentences in the early stages of bootstrapping. 4We used “marched” and “marching” as keywords but did not </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Kazuaki, 2011</marker>
<rawString>R. Parker, D. Graff, J. Kong, K. Chen, and Kazuaki M. 2011. English Gigaword. In Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</booktitle>
<contexts>
<context position="6753" citStr="Patwardhan and Riloff, 2007" startWordPosition="1068" endWordPosition="1072">esearch does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a c</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrovic</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>Using Paraphrases for Improving First Story Detection in News and Twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of The 2012 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="8816" citStr="Petrovic et al., 2012" startWordPosition="1391" endWordPosition="1394">1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a word vector (Allan et al., 2000b) or a probability distribution (Jin et al., 1999), and compare the incoming stories to stories that appeared in the past by computing similarities between their feature representations. Recently, event paraphrases (Petrovic et al., 2012) have been explored to deal with the diversity of event descriptions. However, the New Event Detection task differs from our event recognition task because we want to find all stories describing a certain type of event, not just new events. 3 Bootstrapped Learning of Event Dictionaries Our bootstrapping approach consists of two stages of learning as shown in Figure 1. The process begins with a few agent seeds, purpose phrase patterns, and unannotated articles selected from a broadcoverage corpus using event keywords. In the first stage, event expressions are harvested from the sentences that h</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2012</marker>
<rawString>S. Petrovic, M. Osborne, and V. Lavrenko. 2012. Using Paraphrases for Improving First Story Detection in News and Twitter. In Proceedings of The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Popescu</author>
<author>M Pennacchiotti</author>
<author>D A Paranjpe</author>
</authors>
<title>Extracting events and event descriptions from twitter.</title>
<date>2011</date>
<contexts>
<context position="6895" citStr="Popescu et al., 2011" startWordPosition="1093" endWordPosition="1096">, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog ar</context>
</contexts>
<marker>Popescu, Pennacchiotti, Paranjpe, 2011</marker>
<rawString>A.-M. Popescu, M. Pennacchiotti, and D. A. Paranjpe. 2011. Extracting events and event descriptions from twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Lehnert</author>
</authors>
<title>Information Extraction as a Basis for High-Precision Text Classification.</title>
<date>1994</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="7694" citStr="Riloff and Lehnert, 1994" startWordPosition="1217" endWordPosition="1220">ork on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a s</context>
</contexts>
<marker>Riloff, Lehnert, 1994</marker>
<rawString>E. Riloff and W. Lehnert. 1994. Information Extraction as a Basis for High-Precision Text Classification. ACM Transactions on Information Systems, 12(3):296–333, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Lorenzen</author>
</authors>
<title>Extraction-based text categorization: Generating domain-specific role relationships automatically.</title>
<date>1999</date>
<booktitle>Natural Language Information Retrieval.</booktitle>
<editor>In Tomek Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7948" citStr="Riloff and Lorenzen, 1999" startWordPosition="1254" endWordPosition="1257">ter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenzen, 1999) eliminated the need for the dictionary and labeled patterns, but still assumed the availability of relevant/irrelevant training texts. Event recognition is also related to Topic Detection and Tracking (TDT) (Allan et al., 1998; Allan, 42 Figure 1: Bootstrapped Learning of Event Dictionaries 2002) which addresses event-based organization of a stream of news stories. Event recognition is similar to New Event Detection, also called First Story Detection, which is considered the most difficult TDT task (Allan et al., 2000a). Typical approaches reduce documents to a set of features, either as a wo</context>
</contexts>
<marker>Riloff, Lorenzen, 1999</marker>
<rawString>E. Riloff and J. Lorenzen. 1999. Extraction-based text categorization: Generating domain-specific role relationships automatically. In Tomek Strzalkowski, editor, Natural Language Information Retrieval. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="5975" citStr="Riloff, 1996" startWordPosition="948" endWordPosition="949">pped dictionaries. We conclude that finding at least two different types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More </context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044–1049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>O Etzioni Mausam</author>
<author>S Clark</author>
</authors>
<title>Open domain event extraction from twitter.</title>
<date>2012</date>
<booktitle>In The Proceedings of The 18th ACMSIGKDD Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="7339" citStr="Ritter et al., 2012" startWordPosition="1167" endWordPosition="1170"> 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later work (Riloff and Lorenz</context>
</contexts>
<marker>Ritter, Mausam, Clark, 2012</marker>
<rawString>A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open domain event extraction from twitter. In The Proceedings of The 18th ACMSIGKDD Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sakaki</author>
<author>M Okazaki</author>
<author>Y Matsuo</author>
</authors>
<title>Earthquake shakes twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<contexts>
<context position="6999" citStr="Sakaki et al., 2010" startWordPosition="1110" endWordPosition="1113">ormance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique</context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earthquake shakes twitter users: real-time event detection by social sensors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>On-demand Information Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06).</booktitle>
<contexts>
<context position="6109" citStr="Sekine, 2006" startWordPosition="971" endWordPosition="972">with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction performance (Gu an</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>S. Sekine. 2006. On-demand Information Extraction. In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379--386</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="6094" citStr="Stevenson and Greenwood, 2005" startWordPosition="967" endWordPosition="970"> high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction systems to improve extraction per</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 379–386, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sudo</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="6063" citStr="Sudo et al., 2003" startWordPosition="963" endWordPosition="966">nformation produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research has incorporated event region detectors into event extraction sy</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>K. Sudo, S. Sekine, and R. Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="7245" citStr="Tumasjan et al., 2010" startWordPosition="1150" endWordPosition="1153">gion detectors into event extraction systems to improve extraction performance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on m</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M. Welpe. 2010. Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="5999" citStr="Yangarber et al., 2000" startWordPosition="950" endWordPosition="954">ies. We conclude that finding at least two different types of event information produces high accuracy (88% precision) with good recall (71%) on documents that contain an event keyword. We also present experiments with documents that do not contain event keywords, and obtain 74% accuracy when matching all three types of event information. 2 Related Work Event recognition has been studied in several different contexts. There has been a lot of research on event extraction, where the goal is to extract facts about events from text (e.g., (ACE Evaluations, 2006; Appelt et al., 1993; Riloff, 1996; Yangarber et al., 2000; Chieu and Ng, 2002; Califf and Mooney, 2003; Sudo et al., 2003; Stevenson and Greenwood, 2005; Sekine, 2006)). Although our research does not involve extracting facts, event extraction systems can also be used to identify stories about a specific type of event. For example, the MUC-4 evaluation (MUC-4 Proceedings, 1992) included “text filtering” results that measured the performance of event extraction systems at identifying event-relevant documents. The best text filtering results were high (about 90% F score), but relied on hand-built event extraction systems. More recently, some research </context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhang</author>
<author>H Fuehres</author>
<author>P A Gloor</author>
</authors>
<title>Predicting Stock Market Indicators Through Twitter ”I hope it is not as bad as I fear”. In COINs.</title>
<date>2010</date>
<contexts>
<context position="7316" citStr="Zhang et al., 2010" startWordPosition="1163" endWordPosition="1166">mance (Gu and Cercone, 2006; Patwardhan and Riloff, 2007; Huang and Riloff, 2011). There has been recent work on event detection from social media sources (Becker et al., 2011; Popescu et al., 2011). Some research identifies specific types of events in tweets, such as earthquakes (Sakaki et al., 2010) and entertainment events (Benson et al., 2011). There has also been work on event trend detection (Lampos et al., 2010; Mathioudakis and Koudas, 2010) and event prediction through social media, such as predicting elections (Tumasjan et al., 2010; Conover et al., 2011) or stock market indicators (Zhang et al., 2010). (Ritter et al., 2012) generated a calendar of events mentioned on twitter. (Metzler et al., 2012) proposed structured retrieval of historical event information over microblog archives by distilling high quality event representations using a novel temporal query expansion technique. Some text classification research has focused on event categories. (Riloff and Lehnert, 1994) used an information extraction system to generate relevancy signatures that were indicative of different event types. This work originally relied on manually labeled patterns and a hand-crafted semantic dictionary. Later </context>
</contexts>
<marker>Zhang, Fuehres, Gloor, 2010</marker>
<rawString>X. Zhang, H. Fuehres, and P. A. Gloor. 2010. Predicting Stock Market Indicators Through Twitter ”I hope it is not as bad as I fear”. In COINs.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>