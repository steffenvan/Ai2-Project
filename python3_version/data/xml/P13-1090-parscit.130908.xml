<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994292">
Semi-Supervised Semantic Tagging of Conversational Understanding
using Markov Topic Regression
</title>
<figure confidence="0.368683">
Asli Celikyilmaz
Microsoft
Mountain View, CA, USA
asli@ieee.org
Ruhi Sarikaya
Microsoft
</figure>
<address confidence="0.575763">
Redmond, WA, USA
</address>
<email confidence="0.975511">
rusarika@microsoft.com
</email>
<author confidence="0.929874">
Dilek Hakkani-Tur, Gokhan Tur
</author>
<affiliation confidence="0.915608">
Microsoft Research
</affiliation>
<address confidence="0.613593">
Mountain View, CA, USA
</address>
<email confidence="0.978906">
dilek@ieee.org
gokhan.tur@ieee.org
</email>
<sectionHeader confidence="0.994415" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999857551724138">
Finding concepts in natural language ut-
terances is a challenging task, especially
given the scarcity of labeled data for learn-
ing semantic ambiguity. Furthermore,
data mismatch issues, which arise when
the expected test (target) data does not
exactly match the training data, aggra-
vate this scarcity problem. To deal with
these issues, we describe an efficient semi-
supervised learning (SSL) approach which
has two components: (i) Markov Topic
Regression is a new probabilistic model
to cluster words into semantic tags (con-
cepts). It can efficiently handle seman-
tic ambiguity by extending standard topic
models with two new features. First, it en-
codes word n-gram features from labeled
source and unlabeled target data. Sec-
ond, by going beyond a bag-of-words ap-
proach, it takes into account the inherent
sequential nature of utterances to learn se-
mantic classes based on context. (ii) Ret-
rospective Learner is a new learning tech-
nique that adapts to the unlabeled target
data. Our new SSL approach improves
semantic tagging performance by 3% ab-
solute over the baseline models, and also
compares favorably on semi-supervised
syntactic tagging.
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927976744186">
Semantic tagging is used in natural language un-
derstanding (NLU) to recognize words of seman-
tic importance in an utterance, such as entities.
Typically, a semantic tagging model require large
amount of domain specific data to achieve good
performance (Tur and DeMori, 2011). This re-
quires a tedious and time intensive data collection
and labeling process. In the absence of large la-
beled training data, the tagging model can behave
poorly on test data (target domain). This is usually
caused by data mismatch issues and lack of cover-
age that arise when the target data does not match
the training data.
To deal with these issues, we present a new
semi-supervised learning (SSL) approach, which
mainly has two components. It initially starts with
training supervised Conditional Random Fields
(CRF) (Lafferty et al., 2001) on the source train-
ing data which has been semantically tagged. Us-
ing the trained model, it decodes unlabeled dataset
from the target domain. With the data mismatch
issues in mind, to correct errors that the supervised
model make on the target data, the SSL model
leverages the additional information by way of a
new clustering method. Our first contribution is a
new probabilistic topic model, Markov Topic Re-
gression (MTR), which uses rich features to cap-
ture the degree of association between words and
semantic tags. First, it encodes the n-gram context
features from the labeled source data and the unla-
beled target data as prior information to learn se-
mantic classes based on context. Thus, each latent
semantic class corresponds to one of the seman-
tic tags found in labeled data. MTR is not invari-
ant to reshuffling of words due to its Markovian
property; hence, word-topic assignments are also
affected by the topics of the surrounding words.
Because of these properties, MTR is less sensitive
to the errors caused by the semantic ambiguities.
Our SSL uses MTR to smooth the semantic tag pos-
teriors on the unlabeled target data (decoded using
the CRF model) and later obtains the best tag se-
quences. Using the labeled source and automati-
</bodyText>
<page confidence="0.966534">
914
</page>
<note confidence="0.914277">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914–923,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999918244444444">
cally labeled target data, it re-trains a new CRF-
model.
Although our iterative SSL learning model can
deal with the training and test data mismatch, it
neglects the performance effects caused by adapt-
ing the source domain to the target domain. In
fact, most SSL methods used for adaptation, e.g.,
(Zhu, 2005), (Daum´e-III, 2010), (Subramanya et
al., 2010), etc., do not emphasize this issue. With
this in mind, we introduce a new iterative training
algorithm, Retrospective Learning, as our second
contribution. While retrospective learning itera-
tively trains CRF models with the automatically
annotated target data (explained above), it keeps
track of the errors of the previous iterations so as
to carry the properties of both the source and target
domains.
In short, through a series of experiments we
show how MTR clustering provides additional in-
formation to SSL on the target domain utter-
ances, and greatly impacts semantic tagging per-
formance. Specifically, we analyze MTR’s perfor-
mance on two different types of semantic tags:
named-entities and descriptive tags as shown in
Table 1. Our experiments show that it is much
harder to detect descriptive tags compared to
named-entities.
Our SSL approach uses probabilistic clustering
method tailored for tagging natural language utter-
ances. To the best of our knowledge, our work is
the first to explore the unlabeled data to iteratively
adapt the semantic tagging models for target do-
mains, preserving information from the previous
iterations. With the hope of spurring related work
in domains such as entity detection, syntactic tag-
ging, etc., we extend the earlier work on SSL part-
of-speech (POS) tagging and show in the experi-
ments that our approach is not only useful for se-
mantic tagging but also syntactic tagging.
The remainder of this paper is divided as fol-
lows: §2 gives background on SSL and semantic
clustering methods, §3 describes our new cluster-
ing approach, §4 presents the new iterative learn-
ing, §5 presents our experimental results and §6
concludes our paper.
</bodyText>
<sectionHeader confidence="0.978672" genericHeader="introduction">
2 Related Work and Motivation
</sectionHeader>
<listItem confidence="0.786376125">
(I) Semi-Supervised Tagging. Supervised meth-
ods for semantic tagging in NLU require a large
number of in-domain human-labeled utterances
and gazetteers (movie, actor names, etc.), increas-
• Are there any [comedies] with [Ryan Gosling]?
• How about [oscar winning] movies by
[James Cameron]?
• Find [Woody Allen] movies similar to [Manhattan].
</listItem>
<table confidence="0.9496425">
[Named Entities]
director: James Cameron, Woody Allen,...
actor: Ryan Gosling, Woody Allen,...
title: Manhattan, Midnight in Paris,...
[Descriptive Tags]
restriction: similar, suitable, free,rate,...
description: oscar winning, new release, gardening,...
genre: spooky, comedies, feel good, romance,...
</table>
<tableCaption confidence="0.952695666666667">
Table 1: Samples of semantically tagged utter-
ances from movie domain, named-entities and de-
scriptive tags.
</tableCaption>
<bodyText confidence="0.96868725">
ing the need for significant manual labor (Tur and
DeMori, 2011). Recent work on similar tasks
overcome these challenges using SSL methods as
follows:
</bodyText>
<listItem confidence="0.988435818181818">
• (Wang et al., 2009; Li et al., 2009; Li,
2010; Liu et al., 2011) investigate web query
tagging using semi-supervised sequence models.
They extract semantic lexicons from unlabeled
web queries, to use as features. Our work dif-
fers from these, in that, rather than just detecting
named-entities, our utterances include descriptive
tags (see Table 1).
• Typically the source domain has different dis-
tribution than the target domain, due to topic shifts
in time, newly introduced features (e.g., until re-
cently online articles did not include facebook
”like” feature.), etc. Adapting the source domain
using unlabeled data is the key to achieving good
performance across domains. Recent adaptation
methods for SSL use: expectation minimization
(Daum´e-III, 2010) graph-based learning (Chapelle
et al., 2006; Zhu, 2005), etc. In (Subramanya et
al., 2010) an efficient iterative SSL method is de-
scribed for syntactic tagging, using graph-based
learning to smooth POS tag posteriors. However,
(Reisinger and Mooney, 2011) argues that vector
space models, such as graph-learning, may fail to
capture the richness of word meaning, as simi-
larity is not a globally consistent metric. Rather
than graph-learning, we present a new SSL using
a probabilistic model, MTR, to cluster words based
on co-occurrence statistics.
• Most iterative SSL methods, do not keep track
of the errors made, nor consider the divergence
from the original model. (Lavoie et al., 2011) ar-
gues that iterative learning models should mitigate
new errors made by the model at each iteration by
</listItem>
<page confidence="0.998114">
915
</page>
<bodyText confidence="0.985864557692308">
keeping the history of the prior predictions. This
ensures that a penalty is paid for diverging from
the previous model’s predictions, which will be
traded off against the benefit of reducing classi-
fication loss. We present a retrospective SSL for
CRF, in that, the iterative learner keeps track of the
errors of the previous iterations so as to carry the
properties of both the source and target domains.
(II) Semantic Clustering. A common prop-
erty of several context-based word clustering tech-
niques, e.g., Brown clustering (Brown et al.,
1992), Clustering by Committee (Pantel, 2003),
etc., is that they mainly cluster based on local con-
text such as nearby words. Standard topic models,
such as Latent Dirichlet Allocation (LDA) (Blei
et al., 2003), use a bag-of-words approach, which
disregards word order and clusters words together
that appear in a similar global context. Such mod-
els have been effective in discovering lexicons in
many NLP tasks, e.g., named-entity recognition
(Guo et al., 2009), word-sense disambiguation
(Boyd-Graber et al., 2007; Li et al., 2010), syntac-
tic/semantic parsing (Griffiths et al., 2005; Singh
et al., 2010), speaker identification (Nyugen et al.,
2012), etc. Recent topic models consider word
sequence information in documents (Griffiths et
al., 2005; Moon et al., 2010). The Hidden Topic
Markov Model (HTMM) by (Gruber et al., 2005),
for instance, models sentences in documents as
Markov chains, assuming all words in a sentence
have the same topic. While MTR has a similar
Markovian property, we encode features on words
to allow each word in an utterance to sample from
any of the given semantic tags, as in ”what are
[scary] movies b Hitchcock ?&amp;quot;
genre y [ ]director
In LDA, common words tend to dominate all
topics causing related words to end up in differ-
ent topics. In (Petterson et al., 2010), the vector-
based features of words are used as prior informa-
tion in LDA so that the words that are synonyms
end up in same topic. Thus, we build a seman-
tically rich topic model, MTR, using word context
features as side information. Using a smoothing
prior for each word-topic pair (instead of a con-
stant β smoother), MTR assures that the words are
distributed over topics based on how similar they
are. (e.g., ”scary” and ”spooky”, which have sim-
ilar context features, go into the same semantic
tag, ”genre”). Thus, to best of our knowledge,
MTR is the first topic model to incorporate word
features while considering the sequence of words.
</bodyText>
<sectionHeader confidence="0.931116" genericHeader="method">
3 Markov Topic Regression - MTR
</sectionHeader>
<subsectionHeader confidence="0.999301">
3.1 Model and Abstractions
</subsectionHeader>
<bodyText confidence="0.977695857142857">
LDA assumes that the latent topics of documents
are sampled independently from one of K topics.
MTR breaks down this independence assumption
by allowing Markov relations between the hidden
tags to capture the relations between consecutive
words (as sketched in Figure 1 and Algorithm 1).
(I) Semantic Tags (si): Each word wi of a
given utterance with Nj words, uj={wi}Nj
i=1EU,
j=1,..|U|, from a set of utterances U, is associated
with a latent semantic tag (state) variable siES,
where S is the set of semantic tags. We assume a
fixed K topics corresponding to semantic tags of
labeled data. In a similar way to HTMM (Gruber
et al., 2005) described for documents, MTR sam-
ples each si from a Markov chain that is specific
to its utterance uj. Each state si generates a word,
wi, based on the word-state co-occurrences. MTR
allows for sampling of consecutive words from
different tag clusters. The initial probabilities of
the latent states are sampled from a Dirichlet dis-
tribution over state variables, θj, with α hyper-
parameter for each uj.
(II) Tag Transition Indicator (Ov): Given ut-
terance uj, the decision to sample a wi from a
new topic is determined by an indicator variable,
cj,i, that is sampled from a Binomial(Ov=w,) dis-
tribution with a Beta conjugate prior. (There are v
binomials for each vocabulary term.) cj,i=1 sug-
gests that a new state be sampled from K possible
tags for the word wi in uj, and cj,i=0 suggests that
the state si of wi should be the same as the previ-
ous word’s latent state si−1. The first position of
the sequence is sampled from a new state, hence
cj,i=1=1.
</bodyText>
<listItem confidence="0.987643416666667">
(III) Tag Transition Base Measure (η): Prior
probability of a word given a tag should increase
the chances of sampling words from the correct se-
mantic tag. MTR constrains the generation of a tag
si given the previous tag si−1 and the current wi
based on cj,i by using a vocabulary specific Beta
prior, Ov∼Beta(ηv) 1, on each word in vocabulary
wv=1,..V . We inject the prior information on se-
mantic tags to define values of the base measure
ηv using external knowledge from two sources:
(a) Entity Priors (ηS): Prior probability on
named-entities and descriptive tags denoted as
</listItem>
<footnote confidence="0.955422">
1For each beta distribution we use symmetric
Beta(η„)=Beta(α=η„,β=η„).
</footnote>
<page confidence="0.99183">
916
</page>
<figure confidence="0.5341984">
Algorithm 1 Markov Topic Regression
1: for each semantic tag topic sk, k +- 1, ..., K do
2: — draw a topic mixture φk — Dir(,Ok|Ak, x),
3: — let ,Ok=exp(f(x;Ak)); x={xv}Vl
v=1, ,OkE &apos;ZVl
</figure>
<listItem confidence="0.827831333333333">
4: for each word wv in vocabulary v +- 1, ..., V do
5: — draw a tag indicator mixture ψv — Beta(rl),
6: for each utterance j +- 1, ..., |U |do
7: —draw transition distribution θsj — Dir(α)
8: overstates si and set cj1=1.
9: —for words wi in uj, i +- 1, ..., Nj do
</listItem>
<figure confidence="0.992672038461539">
10: o if i &gt;1, toss a coin cj,i — Binomial(ψwi).
†
11: o If cj,i=1, draw si—Multi(θj )
12: otherwise si=si−1.
13: o Sample wi—Multi(φsi).
† Markov assumption over utterance words is used (See Eq.(4)).
C2
CN
C3
indicator for
sampling
semantic tags
Y/
semantic tag
indicator
parameter
V
|U|
w1 w2 w3 ... wn
vocabulary
features
as prior
information
semantic tag
dependent
smoothing coefficient
xv
Ok — Dir(fkv|x;Ak)
Okv fkv
V
Ak
K
distribution over
semantic tags
prior on
per-word
state
transitions
a
S1 S2 S3 ...
&gt;)
ej
SN
semantic tag
distribution
over tags
latent
semantic tag
!k = exp(f(x;Ak))
smoother for
tag-word
pair
</figure>
<figureCaption confidence="0.999922">
Figure 1: The graph representation of the Markov
</figureCaption>
<bodyText confidence="0.971994222222222">
Topic Regression (MTR). To demonstrate hidden
state Markov Chain, the generation of each word
is explicitly shown (inside of the plate).
ηS=p(si|si−1,wi=v,wi−1). We use web sources
(wiki pages on movies and urls such as imdb.com)
and labeled training data to extract entity lists that
correspond to the semantic tags of our domains.
We keep the frequency of each n-gram to convert
into (empirical) prior probability distribution.
(b) Language Model Prior (ηW): Probabilities
on word transitions denoted as ηW =p(wi=v|wi−1).
We built a language model using SRILM (Stol-
cke, 2002) on the domain specific sources such as
top wiki pages and blogs on online movie reviews,
etc., to obtain the probabilities of domain-specific
n-grams, up to 3-grams. The observed priors, ηS
and ηW, are used for calculating the base measure
η for each vocabulary wv as:
</bodyText>
<equation confidence="0.990312833333333">
� ηsi|si−1,wi=v , if ηsi|si−1,wi=v
ηsi|si−1 S exists,
S
v = ηwi=v,wi−1
W
(1)
</equation>
<bodyText confidence="0.980449363636364">
In Eq.(1), we assume that the prior on the se-
mantic tags, ηS, is more indicative of the deci-
sion for sampling a wi from a new tag compared
to language model posteriors on word sequences,
ηW. Here we represent the base-measure (hyper-
parameter) of the semantic tag indicator variable,
which is not to be confused with a probability
measure 2
We update the indicator parameter via mean cri-
teria,ψv=wi=�K i,j=1ηsi|sj
v=wi/(K2). If no prior on
</bodyText>
<footnote confidence="0.953239">
2The base-measure used in Eq.(1) does not relate to a
back-off model in LM sense. Here, instead of using a
constant value for the hyper-parameters, we use probability
scores that we obtain from LM.
</footnote>
<bodyText confidence="0.996483055555556">
a specific word exists, a default value is used for
base measure, ηv=0.01.
(IV) Topic-Word Distribution Priors (βk):
Different from (Mimno et al., 2008), which uses
asymmetric hyper-parameters on document-topic
distributions, in MTR, we learn the asymmetric
hyper-parameters of the semantic tag-word distri-
butions. We use blocked Gibbs sampling, in which
the topic assignments sk and hyper-parameters
{βk}Kk=1 are alternately sampled at each Gibbs
sampling lag period g given all other variables. We
impose the prior knowledge on naturally related
words, such that if two words ”funny” and ”hilar-
ious” indicate the same given ”genre” class, then
their latent tag distributions should also be simi-
lar. We enforce this on smoothing parameter βk,v,
e.g., βk,&apos;funny&apos;∼βk,&apos;hilarious&apos; for a given tag k as
follows:
</bodyText>
<equation confidence="0.854427625">
At each g lag period of the Gibbs sampling, K
log-linear models with parameters, λ(g)
k ∈RM, is
trained to predict β(g)∈βk, for each wv of a tag
kv
β(g)
k = exp(f(xl; λ(g)
k )) (2)
</equation>
<bodyText confidence="0.987217">
where the log-linear function f is:
</bodyText>
<equation confidence="0.5718142">
� λ(g)
nkv = f(xl k,mxl v,m (3)
(g) v; λ(g)
k ) =
m
</equation>
<bodyText confidence="0.999649">
Here x∈RV ×M is the input matrix x, wherein
rows xv∈RM represents M-dimensional scalar
vector of explanatory features on vocabulary
words. We use the word-tag posterior probabili-
ties obtained from a CRF sequence model trained
on labeled utterances as features. The x={xl,xu}
has labeled (l) and unlabeled (u) parts. The labeled
part contains Vl size vocabulary of which we know
the semantic tags, xl={(xl1,s1),...,(xlVl,sVl)}. At
the start of the Gibbs sampling, we designate the
</bodyText>
<equation confidence="0.3958585">
, otherwise
sk:
</equation>
<page confidence="0.960601">
917
</page>
<bodyText confidence="0.904712272727273">
K latent topics to the K semantic tags of our la-
beled data. Therefore, we assign labeled words to
their designated topics. This way we use observed
scalar counts of each labeled word v associated
with its semantic tag k, n(g)
kv , as the output label
of its input vector, xlv; an indication of likelihood
of words getting sampled from the correspond-
ing semantic label sk. Since the impact of the
asymmetric prior is equivalent to adding pseudo-
counts to the sufficient statistics of the semantic
tag to which the word belongs, we predict the
pseudo-counts β(g) using the scalar counts of the
kv
labeled data, n(g)
kv , based on the log-linear model
in Eq. (2). At g=0, we use β(0)
kv =28, if xv∈Xl; oth-
erwise β(0)=2−2, commonly used values for large
kv
and small β. Note that larger β-values indicate
correlation between the word and the topic.
</bodyText>
<subsectionHeader confidence="0.998146">
3.2 Collapsed Sampler
</subsectionHeader>
<bodyText confidence="0.989936">
The goal of MTR is to infer the degree of relation-
ship between a word v and each semantic tag k,
φkv. To perform inference we need two compo-
nents:
</bodyText>
<listItem confidence="0.99461">
• a sampler which can draw from condi-
</listItem>
<bodyText confidence="0.995279166666667">
tional PMTR(sji=k|sji−1, s\ji, α, ψi,βji), when
cj,i=1, where sji and sji−1 are the semantic
tags of the current wi=v of vocabulary v and
previous word wi−1 in utterance uj, and s\ji
are the semantic tag topics of all words except
for wi; and,
</bodyText>
<listItem confidence="0.784949">
• an estimation procedure for (βkv, λk) (see
§3.1).
</listItem>
<bodyText confidence="0.98116152631579">
We integrate out the multinomial and binomial pa-
rameters of the model: utterance-tag distributions
θj, binomial state transition indicator distribution
per each word ψv, and φk for tag-word distribu-
tions. We use collapsed Gibbs sampling to re-
duce random components and model the posterior
distribution by obtaining samples (sji, cj,i) drawn
from this distribution. Under the Markov assump-
tion, for each word wi=v in a given utterance uj,
if cj,i=1, we sample a new tag si=k given the
remaining tags and hyper-parameters Ok, α, and
ηwi=v . Using the following parameters; n(si)
si|si−1 ji ,
which is the number of words assigned to a seman-
tic class si=k excluding case i, and n(si−1)
si is the
number of transitions from class si−1 to si, where
indicator ff(si−1, si)=1 if slot si=si−1, the update
equation is formulated as follows:
</bodyText>
<equation confidence="0.992225625">
si |si
p(sji = k |w, s−ji, α, ηwi −1, Ok) ∝
nji + βkwi
(si) ∗ (n(si−1) + α)∗
n( )) + Pv βkv i
(4)
n(.) + ff(si−1, k) + Kα
(si)
</equation>
<sectionHeader confidence="0.985515" genericHeader="method">
4 Semi-Supervised Semantic Labeling
</sectionHeader>
<subsectionHeader confidence="0.991707">
4.1 Semi Supervised Learning (SSL) with
</subsectionHeader>
<bodyText confidence="0.9708776">
CRF
In (Subramanya et al., 2010), a new SSL method
is described for adapting syntactic POS tagging of
sentences in newswire articles along with search
queries to a target domain of natural language
(NL) questions. They decode unlabeled queries
from target domain (t) using a CRF model trained
on the POS-labeled newswire data (source do-
main (o)). The unlabeled POS tag posteriors are
then smoothed using a graph-based learning algo-
rithm. On graph, the similarities are defined over
sequences by constructing the graph over types,
word 3-grams, where types capture the local con-
text of words. Since CRF tagger only uses lo-
cal features of the input to score tag pairs, they
try to capture all the context with the graph with
additional context features on types. Later, using
viterbi decoding, they select the 1-best POS tag
sequence, s∗j for each utterance uj. Graph-based
SSL defines a new CRF objective function:
</bodyText>
<equation confidence="0.9915755">
Λ(t)
n+1 =argmin
Λ∈RK
( )
− P log p(sj|uj; Λ(t)
n ) + µ�Λ(t)
n �2 −
j=1:l
</equation>
<bodyText confidence="0.9819211">
nτ Pl+i log pn(s∗j  |uj; Λ(t)) o (5)
The first bracket in Eq.(5) is the loss on the la-
beled data and L2 regularization on parameters,
Λ(t)
n , from nth iteration, same as standard CRF.
The last term is the loss on unlabeled data from
target domain with a hyper-parameter τ. They use
a small value for τ to enable the new model to be
as close as possible to the initial model trained on
source data.
</bodyText>
<subsectionHeader confidence="0.979281">
4.2 Retrospective Semi-Supervised CRF
</subsectionHeader>
<bodyText confidence="0.998244">
We describe a Retrospective SSL (R-SSL) train-
ing with CRF (Algorithm 2), using MTR as a
</bodyText>
<equation confidence="0.975274">
(n(si)
si+1 + ff(si−1, si) + ff(si+1, si) + α)
</equation>
<page confidence="0.988948">
918
</page>
<bodyText confidence="0.976446239130435">
smoothing model, instead of a graph-based model,
as follows:
I. DECODING and SMOOTHING. The poste-
rior probability of a tag sji=k given a word wji
in unlabeled utterance uj from target domain (t)
ˆpn(j, i)=ˆpn(sji=k|wji; Λ(t)
n ), is decoded using the
n-th iteration CRF model. MTR uses the decoded
probabilities as semantic tag prior features on vo-
cabulary items. We generate a word-tag matrix of
posteriors, x∈(0,1)V ×K, where K is the number
of semantic tags and V is the vocabulary size from
n-th iteration. Each row is a K dimensional vector
of tag posterior probabilities xv={xv1,... xvK} on
the vocabulary term, wv. The labeled rows xl of
the vocabulary matrix, x={xl,xu}, contain only
{0,1} values, indicating the word’s observed se-
mantic tags in the labeled data. Since a labeled
term wv can have different tags (e.g., ”clint east-
wood” may be tagged as actor-name and director-
name in the training data), PKk xvk≥1 holds. The
x is used as the input matrix of the kth log-linear
model (corresponding to kth semantic tag (topic))
to infer the β hyper-parameter of MTR in Eq. (2).
MTR generates smoothed conditional probabilities
Okv for each vocabulary term v given semantic tag
k.
II. INTERPOLATION. For each word wji=v
in unlabeled utterance uj, we interpolate tag
marginals from CRF and MTR for each semantic
tag sji = k:
III. VITERBI. Using viterbi decoding over
the tag marginals, ˆqn(sji|wij; Λ(t)
n ), and transition
probabilities obtained from the CRF model of n-th
iteration, we get ˆpn(s∗j|uj; Λ(t)
n ), the 1-best decode
s∗j of each unlabeled utterance uj∈Uun.
IV. RETROSPECTIVE SSL (R-SSL). After
we decode the unlabeled data, we re-train a new
CRF model at each iteration. Each iteration makes
predictions on the semantic tags of unlabeled data
with varying posterior probabilities. Motivated by
(Lavoie et al., 2011), we want the loss function to
have a dependency on the prior model predictions.
Thus, R-SSL encodes the history of the prior pre-
</bodyText>
<note confidence="0.671078">
Algorithm 2 Retrospective Semi-Supervised CRF
Input: Labeled Ul, and unlabeled Uu data.
Process: Λ(o)
</note>
<bodyText confidence="0.3940345">
n =crf-train(Ul) at n=0, n=n+1 †.
While not converged
</bodyText>
<equation confidence="0.935419904761905">
ˆp=posterior-decode(Uu n,Λ(o)
n )
O=smooth-posteriors(ˆp) using MTR,
ˆq=interpolate-posteriors(ˆp,O),
Uun=viterbi-decode(ˆq)
Λ(t)
n+1=crf-retrospective(Ul, Uun,. . . ,Uu1 ,Λ(t)
n )
† (n):iteration, (t):target, (o):source domains.
dictions, as follows: (7)
Λ(t)
n+1 =argmin
Λ∈RK
(
− P log p(sj|uj; Λ(t)
n ) + µkΛ(t)
n k2
j=1:l
( )
− P max{0, ˆp∗∗n }
j=1:(l+u)
</equation>
<bodyText confidence="0.984236222222222">
where, ˆp∗∗n =1 − log hn(uj)ˆpn(s∗j|uj; Λ(t)
n ). The
first two terms are same as standard CRF. The
last term ensures that the predictions of the cur-
rent model have the same sign as the predic-
tions of the previous models (using labeled and
unlabeled data), denoted by a maximum margin
Pn−1
hinge weight, hn(uj)= 1 1 ˆpn(s∗ j|uj; Λ(t)
</bodyText>
<equation confidence="0.7313575">
n ).
n−1
</equation>
<bodyText confidence="0.999940625">
It should also be noted that with MTR, the R-SSL
learns the word-tag relations by using features that
describe the words in context, eliminating the need
for additional type representation of graph-based
model. MTR provides a separate probability dis-
tribution θj over tags for each utterance j, implic-
itly allowing for the same word v in separate utter-
ances to differ in tag posteriors Okv.
</bodyText>
<sectionHeader confidence="0.999682" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.823099">
5.1 Datasets and Tagsets
5.1.1 Semantic Tagging Datasets
</subsectionHeader>
<bodyText confidence="0.9971846">
We focus here on audiovisual media in the movie
domain. The user is expected to interact by voice
with a system than can perform a variety of tasks
such as browsing, searching, querying informa-
tion, etc. To build initial NLU models for such
a dialog system, we used crowd-sourcing to col-
lect and annotate utteran
ces, which we consider
our source domain. Given movie domain-specific
tasks, we asked the crowd about how they would
</bodyText>
<table confidence="0.660247142857143">
ˆqn(sji|wij; Λ(t) CRF posterior
n )= 7r z } |{
ˆpn(sji|wij; Λ(t)
n )
+(1 − 7r) MTR
z}|{
Okv (6)
</table>
<page confidence="0.997545">
919
</page>
<bodyText confidence="0.999909105263158">
interact with the media system as if they were talk-
ing to a person.
Our data from target domain is internally col-
lected from real-use scenarios of our spoken dia-
log system. The transcribed text forms of these ut-
terances are obtained from speech recognition en-
gine. Although the crowd-sourced data is similar
to target domain, in terms of pre-defined user in-
tentions, the target domain contains more descrip-
tive vocabulary, which is almost twice as large as
the source domain. This causes data-mismatch is-
sues and hence provides a perfect test-bed for a
domain adaptation task. In total, our corpus has
a 40K semantically tagged utterances from each
source and target domains. There are around 15
named-entity and 10 descriptive tags. We sep-
arated 5K utterances to test the performance of
the semantic tagging models. The most frequent
entities are: movie-director (’James Cameron’),
movie-title (’Die Hard’), etc.; whereas top de-
scriptive tags are: genre (’feel good’), description
(’black and white’, ’pg 13’), review-rate (’epic’,
’not for me’), theater-location (’near me’,’city
center’), etc.
Unlabeled utterances similar to the movie do-
main are pulled from a month old web query logs
and extracted over 2 million search queries from
well-known sites, e.g., IMDB, Netflix, etc. We
filtered queries that are similar to our target set
that start with wh-phrases (’what’, ’who’, etc.) as
well as imperatives ’show’, ’list’, etc. In addition,
we extracted web n-grams and entity lists (see §3)
from movie related web sites, and online blogs and
reviews. We collected around 300K movie review
and blog entries on the entities observed in our
data. We extract prior distributions for entities and
n-grams to calculate entity list η and word-tag β
priors (see §3.1).
</bodyText>
<subsubsectionHeader confidence="0.655779">
5.1.2 Syntactic Tagging Datasets
</subsubsectionHeader>
<bodyText confidence="0.999930913043478">
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source data. Fol-
lowing previous research, we train on sections 00-
18, comprised of 38,219 POS-tagged sentences.
To evaluate the domain adaptation (DA) approach
and to compare with results reported by (Subra-
manya et al., 2010), we use the first and second
half of QuestionBank (Judge et al., 2006) as our
development and test sets (target). The Question-
Bank contains 4000 POS-tagged questions, how-
ever it is difficult to tag with WSJ-trained tag-
gers because the word order is different than WSJ
and contains a test-set vocabulary that is twice
as large as the one in the development set. As
for unlabeled data we crawled the web and col-
lected around 100,000 questions that are similar
in style and length to the ones in QuestionBank,
e.g. ”wh” questions. There are 36 different tag
sets in the Penn dataset which includes tag la-
bels for verbs, nouns, adjectives, adverbs, modal,
determiners, prepositions, etc. More information
about the Penn Tree-bank tag set can be found here
(Marcus et al., 1993).
</bodyText>
<subsectionHeader confidence="0.999634">
5.2 Models
</subsectionHeader>
<bodyText confidence="0.999464">
We evaluated several baseline models on two
tasks:
</bodyText>
<subsubsectionHeader confidence="0.651408">
5.2.1 Semantic Clustering
</subsubsectionHeader>
<bodyText confidence="0.999975647058824">
Since MTR provides a mixture of properties
adapted from earlier models, we present perfor-
mance benchmarks on tag clustering using: (i)
LDA; (ii) Hidden Markov Topic Model HMTM
(Gruber et al., 2005); and, (iii) w-LDA (Petterson
et al., 2010) that uses word features as priors in
LDA. When a uniform β hyper-parameter is used
with no external information on the state transi-
tions in MTR, it reduces to a HMTM model. Sim-
ilarly, if no Markov properties are used (bag-of-
words), MTR reduces to w-LDA. Each topic model
uses Gibbs sampling for inference and parameter
learning. We sample models for 1000 iterations,
with a 500-iteration burn-in and a sampling lag of
10. For testing we iterated the Gibbs sampler us-
ing the trained model for 10 iterations on the test-
ing data.
</bodyText>
<subsectionHeader confidence="0.584453">
5.2.2 SSL for Semantic/Syntactic Tagging
</subsectionHeader>
<bodyText confidence="0.9922095">
We evaluated three different baselines against our
SSL models:
</bodyText>
<listItem confidence="0.978927875">
? CRF: a standard supervised sequence tagging.
? Self-CRF: a wrapper method for SSL using
self-training. First a supervised learning algorithm
is used to build a CRF model based on the labeled
data. A CRF model is used to decode the unla-
beled data to generate more labeled examples for
re-training.
? SSL-Graph: A SSL model presented in (Sub-
</listItem>
<bodyText confidence="0.924499666666667">
ramanya et al., 2010) that uses graph-based learn-
ing as posterior tag smoother for CRF model using
Eq.(5).
In addition to the three baseline, we evaluated
three variations of our SSL method:
? SSL-MTR: Our first version of SSL uses MTR to
</bodyText>
<page confidence="0.983396">
920
</page>
<figure confidence="0.445509">
LDA w-LDA HMTM MTR
</figure>
<figureCaption confidence="0.927449">
Figure 2: F-measure for semantic clustering per-
formance. Performance differences for three dif-
ferent baseline models and our MTR approach by
different semantic tags.
</figureCaption>
<bodyText confidence="0.999832068965517">
smooth the semantic tag posteriors of a unlabeled
data decoded by the CRF model using Eq.(5).
* R-SSL-Graph: Our second version uses
graph-learning to smooth the tag posteriors and re-
train a new CRF model using retrospective SSL in
Eq.(7).
* R-SSL-MTR: Our full model uses MTR as a
Bayesian smoothing model, and retrospective SSL
in Eq.(7) for iterative CRF training.
For all the CRF models, we use lexical fea-
tures consisting of unigrams in a five-word win-
dow around the current word. To include contex-
tual information, we add binary features for all
possible tags. We inject dictionary constraints to
all CRF models, such as features indicating label
prior information. For each model we use sev-
eral named entity features, e.g., movie-title, actor-
name, etc., non-named entity (descriptive) fea-
tures, e.g., movie-description, movie-genre, and
domain independent dictionaries, e.g, time, loca-
tion, etc. For graph-based learning, we imple-
mented the algorithm presented in (Subramanya
et al., 2010) and used the same hyper-parameters
and features. For the rest of the hyper-parameters,
we used: α=0.01 for MTR, π=0.5 for interpolation
mixing. These parameters were chosen based on
the performance of the development set. All CRF
objective functions were optimized using Stochas-
tic Gradient Descent.
</bodyText>
<sectionHeader confidence="0.509704666666667" genericHeader="method">
5.3 Results and Discussions
5.3.1 Experiment 1: Clustering Semantic
Tags.
</sectionHeader>
<bodyText confidence="0.999983586206897">
Here, we want to demonstrate the performance
of MTR model for capturing relationships between
words and semantic tags against baseline topic
models: LDA, HMTM, w-LDA. We take the se-
mantically labeled utterances from the movie tar-
get domain and use the first half for training and
the rest for performance testing. We use all the
collected unlabeled web queries from the movie
domain. For fair comparison, each benchmark
topic model is provided with prior information on
word-semantic tag distributions based on the la-
beled training data, hence, each K latent topic is
assigned to one of K semantic tags at the begin-
ning of Gibbs sampling.
We evaluate the performance separately on de-
scriptive tags, named-entities, and all tags to-
gether. The performance of the four topic models
are reported in Figure 2. LDA shows the worst per-
formance, even though some supervision is pro-
vided by way of labeled semantic tags. Although
w-LDA improves semantic clustering performance
over LDA, the fact that it does not have Markov
properties makes it fall short behind MTR. As for
the effect of word features in MTR, we see a 3%
absolute performance gain over the second best
performing HMTM baseline on named-entity tags,
a 1% absolute gain on descriptive tags and a 2%
absolute overall gain. As expected, we see a drop
in F-measure on all models on descriptive tags.
</bodyText>
<sectionHeader confidence="0.7866475" genericHeader="method">
5.3.2 Experiment 2: Domain Adaptation
Task.
</sectionHeader>
<bodyText confidence="0.99986847826087">
We compare the performance of our SSL model
to that of state-of-the-art models on semantic and
syntactic tagging. Each SSL model is built us-
ing labeled training data from the source do-
main and unlabeled training data from target do-
main. In Table 2 we show the results on Movie
and QuestionBank target test datasets. The re-
sults of SSL-Graph on QuestionBank is taken
from (Subramanya et al., 2010). The self-
training model, Self-CRF adds 3% improve-
ment over supervised CRF models on movie do-
main, but does not improve syntactic tagging. Be-
cause it is always inherently biased towards the
source domain, self-training tends to reinforce
the knowledge that the supervised model already
has. SSL-Graph works much better for both
syntactic and semantic tagging compared to CRF
and Self-CRF models. Our Bayesian MTR ef-
ficiently extracts information from the unlabeled
data for the target domain. Combined with retro-
spective training, R-SSL-MTR demonstrates no-
ticeable improvements, ∼2% on descriptive tags,
and 1% absolute gains in overall semantic tag-
</bodyText>
<figure confidence="0.998328928571429">
77%
74%
82%
78%
• Descriptive Tags
♦ Named-Entities
■ All Tags
82%
84%
79%
F-Measure 0.9
0.8
0.7
0.6
</figure>
<page confidence="0.989237">
921
</page>
<bodyText confidence="0.998945">
ging performance over SSL-Graph. On syntac-
tic tagging, the two retrospective learning models
is comparable, close to 1% improvement over the
SSL-Graph and SSL-MTR.
</bodyText>
<table confidence="0.9992285">
Movie Domain QBank
Model Desc. NE All POS
CRF 75.05 75.84 75.84 83.80
Self-CRF 78.96 79.53 79.19 84.00
SSL-Graph 80.27 81.35 81.23 86.80
SSL-MTR 79.87 79.31 79.19 86.30
R-SSL-Graph 80.58 81.95 81.52 87.12
R-SSL-MTR 82.76 82.27 82.24 87.34
</table>
<tableCaption confidence="0.960043">
Table 2: Domain Adaptation performance
</tableCaption>
<bodyText confidence="0.716337">
in F-measure on Semantic Tagging on
Movie Target domain and POS tagging on
QBank:QuestionBank. Best performing models
are bolded.
</bodyText>
<sectionHeader confidence="0.9034395" genericHeader="method">
5.3.3 Experiment 3: Analysis of Semantic
Disambiguation.
</sectionHeader>
<bodyText confidence="0.99999644">
Here we focus on the accuracy of our models in
tagging semantically ambiguous words. We inves-
tigate words that have more than one observed se-
mantic tag in training data, such as ”are there any
[war]genre movies available.”, ”remove all movies
about [war]description.”). Our corpus contained
30,000 unique vocabulary, 55% of which are con-
tained in one or more semantic categories. Only
6.5% of those are tagged as multiple categories
(polysemous), which are the sources of semantic
ambiguity. Table-3 shows the precision of two best
models for most confused words.
We compare our two best SSL models with dif-
ferent smoothing regularizes: R-SSL-MTR (MTR)
and R-SSL-Graph (GRAPH). We use preci-
sion and recall criterion on semantically confused
words.
In Table 3 we show two most frequent descrip-
tive tags; genre and description, and commonly
misclassified words by the two models. Results
indicate that the R-SSL-MTR, performs better
than the R-SSL-Graph, in activating the correct
meaning of a word. The results indicate that incor-
porating context information with MTR is an effec-
tive option for identifying semantic ambiguity.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999205">
We have presented a novel semi supervised learn-
ing approach using a probabilistic clustering
</bodyText>
<table confidence="0.590957">
genre description
Vocab. GRAPH MTR GRAPH MTR
war 50% 100% 75% 88%
popular 90% 89% 80% 100%
kids 78% 86% − 100%
crime 49% 80% 86% 67%
zombie 67% 89% 67% 86%
</table>
<tableCaption confidence="0.957211">
Table 3: Classification performance in F-measure
</tableCaption>
<bodyText confidence="0.993936222222222">
for semantically ambiguous words on the most fre-
quently confused descriptive tags in the movie do-
main.
method to semantically tag spoken language ut-
terances. Our results show that encoding priors
on words and context information contributes sig-
nificantly to the performance of semantic cluster-
ing. We have also described an efficient iterative
learning model that can handle data inconsisten-
cies that leads to performance increases in seman-
tic and syntactic tagging.
As a future work, we will investigate using ses-
sion data, namely the entire dialog between the
human and the computer. Rather than using sin-
gle turn utterances, we hope to utilize the con-
text information, e.g., information from previous
turns for improving the performance of the seman-
tic tagging of the current turns.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999750454545455">
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A
topic model for word sense disambiguation. Proc.
EMNLP.
P.F. Brown, V.J.D. Pietra, P.V. deSouza, and J.C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479.
O. Chapelle, B. Schlkopf, and Alexander Zien. 2006.
Semi-supervised learning. MIT Press.
H. Daum´e-III. 2010. Frustratingly easy semi-
supervised domain adaptation. Proc. Workshop on
Domain Adaptation for Natural Language Process-
ing at ACL.
T.L Griffiths, M. Steyvers, D.M. Blei, and J.M. Tenen-
baum. 2005. Integrating topics and syntax. Proc. of
NIPS.
A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2005. Hidden
topic markov models. Proc. of ICML.
H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su.
2009. Domain adaptation with latent semantic asso-
ciation for named entity recognition. Proc. NAACL.
</reference>
<page confidence="0.971043">
922
</page>
<reference confidence="0.999533616666667">
J. Judge, A. Cahill, and J.Van Genabith. 2006.
Question-bank: Creating corpus of parse-annotated
questions. Proc. Int. Conf. Computational Linguis-
tics and ACL.
A. Lavoie, M.E. Otey, N. Ratliff, and D. Sculley. 2011.
History dependent domain adaptation. Proc. NIPS
Workshop on Domain Adaptation.
X. Li, Y.-Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. Proc. of SI-
GIR.
L. Li, B. Roth, and C. Sporleder. 2010. Topic mod-
els for word sense disambiguation and token-based
idiom detection. Proc. ACL.
X. Li. 2010. Understanding semantic structure of noun
phrase queries. Proc. ACL.
J Liu, X. Li, A. Acero, and Ye-Yi Wang. 2011. Lex-
icon modeling for query understanding. Proc. of
ICASSP.
M. P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 27:1–30.
D. Mimno, W. Li, and A. McCallum. 2008.
Topic models conditioned on arbitrary features with
dirichlet-multinomial regression. Proc. UAI.
T. Moon, K. Erk, and J. Baldridge. 2010. Crouch-
ing dirichlet, hidden markov model: Unsupervised
pos tagging with context local tag generation. Proc.
ACL.
V.-A. Nyugen, J. Boyd-Graber, and P. Resnik. 2012.
Sits: A hierarchical nonparametric model using
speaker identity for topic segmentation in multiparty
conversations. Proc. ACL.
P. Pantel. 2003. Clustering by committee. Ph.D. The-
sis, University ofAlberta, Edmonton, Alta., Canada.
J. Petterson, A. Smola, T. Caetano, W. Buntine, and
S. Narayanamurthy. 2010. Word features for latent
dirichlet allocation. In Proc. NIPS.
J. Reisinger and R. Mooney. 2011. Cross-cutting mod-
els of lexical semantics. In Proc. of EMNLP.
S. Singh, D. Hillard, and C. Leggetter. 2010.
Minimally-supervised extraction of entities from
text advertisements. Proc. NAACL-HLT.
A. Stolcke. 2002. An extensible language modeling
toolkit. Proc. Interspeech.
A. Subramanya, S. Petrov, and F. Pereira. 2010. Effi-
cient graph-based semi-supervised learning of struc-
tured tagging models. In Proc. EMNLP.
G. Tur and R. DeMori. 2011. Spoken language under-
standing: Systems for extracting semantic informa-
tion from speech. Wiley Press.
Y.-Y. Wang, R. Hoffman, X. Li, and J. Syzmanski.
2009. Semi-supervised learning of semantic classes
for query understanding from the web and for the
web. In The 18th ACM Conference on Information
and Knowledge Management.
X. Zhu. 2005. Semi-supervised learning litera-
ture survey. Technical Report 1530, University of
Wisconsin-Madison.
</reference>
<page confidence="0.998898">
923
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.120484">
<title confidence="0.891946333333333">Semi-Supervised Semantic Tagging of Conversational using Markov Topic Regression Asli</title>
<author confidence="0.889326">Mountain View</author>
<author confidence="0.889326">CA</author>
<email confidence="0.964903">asli@ieee.org</email>
<affiliation confidence="0.290888">Ruhi</affiliation>
<address confidence="0.721966">Redmond, WA,</address>
<email confidence="0.999797">rusarika@microsoft.com</email>
<author confidence="0.903063">Dilek Hakkani-Tur</author>
<author confidence="0.903063">Gokhan</author>
<affiliation confidence="0.866692">Microsoft</affiliation>
<address confidence="0.843074">Mountain View, CA,</address>
<email confidence="0.993641">gokhan.tur@ieee.org</email>
<abstract confidence="0.999210233333333">Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which two components: Topic a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words apit takes into account the nature utterances to learn seclasses based on context. Ret- Learner a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="9034" citStr="Blei et al., 2003" startWordPosition="1410" endWordPosition="1413">, which will be traded off against the benefit of reducing classification loss. We present a retrospective SSL for CRF, in that, the iterative learner keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) b</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boyd-Graber</author>
<author>D Blei</author>
<author>X Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="9339" citStr="Boyd-Graber et al., 2007" startWordPosition="1455" endWordPosition="1458">A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”wha</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>J. Boyd-Graber, D. Blei, and X. Zhu. 2007. A topic model for word sense disambiguation. Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J D Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8829" citStr="Brown et al., 1992" startWordPosition="1377" endWordPosition="1380">ls should mitigate new errors made by the model at each iteration by 915 keeping the history of the prior predictions. This ensures that a penalty is paid for diverging from the previous model’s predictions, which will be traded off against the benefit of reducing classification loss. We present a retrospective SSL for CRF, in that, the iterative learner keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, 1992</marker>
<rawString>P.F. Brown, V.J.D. Pietra, P.V. deSouza, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Chapelle</author>
<author>B Schlkopf</author>
<author>Alexander Zien</author>
</authors>
<title>Semi-supervised learning.</title>
<date>2006</date>
<booktitle>Proc. Workshop on Domain Adaptation for Natural Language Processing at ACL.</booktitle>
<publisher>MIT</publisher>
<contexts>
<context position="7526" citStr="Chapelle et al., 2006" startWordPosition="1166" endWordPosition="1169">eb queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance across domains. Recent adaptation methods for SSL use: expectation minimization (Daum´e-III, 2010) graph-based learning (Chapelle et al., 2006; Zhu, 2005), etc. In (Subramanya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergenc</context>
</contexts>
<marker>Chapelle, Schlkopf, Zien, 2006</marker>
<rawString>O. Chapelle, B. Schlkopf, and Alexander Zien. 2006. Semi-supervised learning. MIT Press. H. Daum´e-III. 2010. Frustratingly easy semisupervised domain adaptation. Proc. Workshop on Domain Adaptation for Natural Language Processing at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
<author>D M Blei</author>
<author>J M Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>Proc. of NIPS.</booktitle>
<contexts>
<context position="9409" citStr="Griffiths et al., 2005" startWordPosition="1466" endWordPosition="1469">.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movies b Hitchcock ?&amp;quot; genre y [ ]director In LDA, common</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>T.L Griffiths, M. Steyvers, D.M. Blei, and J.M. Tenenbaum. 2005. Integrating topics and syntax. Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruber</author>
<author>M Rosen-Zvi</author>
<author>Y Weiss</author>
</authors>
<title>Hidden topic markov models.</title>
<date>2005</date>
<booktitle>Proc. of ICML.</booktitle>
<contexts>
<context position="9657" citStr="Gruber et al., 2005" startWordPosition="1505" endWordPosition="1508">se a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movies b Hitchcock ?&amp;quot; genre y [ ]director In LDA, common words tend to dominate all topics causing related words to end up in different topics. In (Petterson et al., 2010), the vectorbased features of words are used as prior information in LDA so that the words that are synonyms end up in same topic. Th</context>
<context position="11464" citStr="Gruber et al., 2005" startWordPosition="1820" endWordPosition="1823">e latent topics of documents are sampled independently from one of K topics. MTR breaks down this independence assumption by allowing Markov relations between the hidden tags to capture the relations between consecutive words (as sketched in Figure 1 and Algorithm 1). (I) Semantic Tags (si): Each word wi of a given utterance with Nj words, uj={wi}Nj i=1EU, j=1,..|U|, from a set of utterances U, is associated with a latent semantic tag (state) variable siES, where S is the set of semantic tags. We assume a fixed K topics corresponding to semantic tags of labeled data. In a similar way to HTMM (Gruber et al., 2005) described for documents, MTR samples each si from a Markov chain that is specific to its utterance uj. Each state si generates a word, wi, based on the word-state co-occurrences. MTR allows for sampling of consecutive words from different tag clusters. The initial probabilities of the latent states are sampled from a Dirichlet distribution over state variables, θj, with α hyperparameter for each uj. (II) Tag Transition Indicator (Ov): Given utterance uj, the decision to sample a wi from a new topic is determined by an indicator variable, cj,i, that is sampled from a Binomial(Ov=w,) distributi</context>
<context position="28255" citStr="Gruber et al., 2005" startWordPosition="4680" endWordPosition="4683"> that are similar in style and length to the ones in QuestionBank, e.g. ”wh” questions. There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc. More information about the Penn Tree-bank tag set can be found here (Marcus et al., 1993). 5.2 Models We evaluated several baseline models on two tasks: 5.2.1 Semantic Clustering Since MTR provides a mixture of properties adapted from earlier models, we present performance benchmarks on tag clustering using: (i) LDA; (ii) Hidden Markov Topic Model HMTM (Gruber et al., 2005); and, (iii) w-LDA (Petterson et al., 2010) that uses word features as priors in LDA. When a uniform β hyper-parameter is used with no external information on the state transitions in MTR, it reduces to a HMTM model. Similarly, if no Markov properties are used (bag-ofwords), MTR reduces to w-LDA. Each topic model uses Gibbs sampling for inference and parameter learning. We sample models for 1000 iterations, with a 500-iteration burn-in and a sampling lag of 10. For testing we iterated the Gibbs sampler using the trained model for 10 iterations on the testing data. 5.2.2 SSL for Semantic/Syntac</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2005</marker>
<rawString>A. Gruber, M. Rosen-Zvi, and Y. Weiss. 2005. Hidden topic markov models. Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Guo</author>
<author>H Zhu</author>
<author>Z Guo</author>
<author>X Zhang</author>
<author>X Wu</author>
<author>Z Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="9286" citStr="Guo et al., 2009" startWordPosition="1449" endWordPosition="1452">and target domains. (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to s</context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su. 2009. Domain adaptation with latent semantic association for named entity recognition. Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Judge</author>
<author>A Cahill</author>
<author>J Van Genabith</author>
</authors>
<title>Question-bank: Creating corpus of parse-annotated questions.</title>
<date>2006</date>
<booktitle>Proc. Int. Conf. Computational Linguistics and ACL.</booktitle>
<marker>Judge, Cahill, Van Genabith, 2006</marker>
<rawString>J. Judge, A. Cahill, and J.Van Genabith. 2006. Question-bank: Creating corpus of parse-annotated questions. Proc. Int. Conf. Computational Linguistics and ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavoie</author>
<author>M E Otey</author>
<author>N Ratliff</author>
<author>D Sculley</author>
</authors>
<title>History dependent domain adaptation.</title>
<date>2011</date>
<booktitle>Proc. NIPS Workshop on Domain Adaptation.</booktitle>
<contexts>
<context position="8174" citStr="Lavoie et al., 2011" startWordPosition="1269" endWordPosition="1272">manya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergence from the original model. (Lavoie et al., 2011) argues that iterative learning models should mitigate new errors made by the model at each iteration by 915 keeping the history of the prior predictions. This ensures that a penalty is paid for diverging from the previous model’s predictions, which will be traded off against the benefit of reducing classification loss. We present a retrospective SSL for CRF, in that, the iterative learner keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. (II) Semantic Clustering. A common property of several context-based word clustering </context>
<context position="23111" citStr="Lavoie et al., 2011" startWordPosition="3831" endWordPosition="3834">word wji=v in unlabeled utterance uj, we interpolate tag marginals from CRF and MTR for each semantic tag sji = k: III. VITERBI. Using viterbi decoding over the tag marginals, ˆqn(sji|wij; Λ(t) n ), and transition probabilities obtained from the CRF model of n-th iteration, we get ˆpn(s∗j|uj; Λ(t) n ), the 1-best decode s∗j of each unlabeled utterance uj∈Uun. IV. RETROSPECTIVE SSL (R-SSL). After we decode the unlabeled data, we re-train a new CRF model at each iteration. Each iteration makes predictions on the semantic tags of unlabeled data with varying posterior probabilities. Motivated by (Lavoie et al., 2011), we want the loss function to have a dependency on the prior model predictions. Thus, R-SSL encodes the history of the prior preAlgorithm 2 Retrospective Semi-Supervised CRF Input: Labeled Ul, and unlabeled Uu data. Process: Λ(o) n =crf-train(Ul) at n=0, n=n+1 †. While not converged ˆp=posterior-decode(Uu n,Λ(o) n ) O=smooth-posteriors(ˆp) using MTR, ˆq=interpolate-posteriors(ˆp,O), Uun=viterbi-decode(ˆq) Λ(t) n+1=crf-retrospective(Ul, Uun,. . . ,Uu1 ,Λ(t) n ) † (n):iteration, (t):target, (o):source domains. dictions, as follows: (7) Λ(t) n+1 =argmin Λ∈RK ( − P log p(sj|uj; Λ(t) n ) + µkΛ(t) </context>
</contexts>
<marker>Lavoie, Otey, Ratliff, Sculley, 2011</marker>
<rawString>A. Lavoie, M.E. Otey, N. Ratliff, and D. Sculley. 2011. History dependent domain adaptation. Proc. NIPS Workshop on Domain Adaptation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>Y-Y Wang</author>
<author>A Acero</author>
</authors>
<title>Extracting structured information from user queries with semisupervised conditional random fields.</title>
<date>2009</date>
<booktitle>Proc. of SIGIR.</booktitle>
<contexts>
<context position="6759" citStr="Li et al., 2009" startWordPosition="1052" endWordPosition="1055"> to [Manhattan]. [Named Entities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to ac</context>
</contexts>
<marker>Li, Wang, Acero, 2009</marker>
<rawString>X. Li, Y.-Y. Wang, and A. Acero. 2009. Extracting structured information from user queries with semisupervised conditional random fields. Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>B Roth</author>
<author>C Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="9357" citStr="Li et al., 2010" startWordPosition="1459" endWordPosition="1462">al context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movi</context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>L. Li, B. Roth, and C. Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
</authors>
<title>Understanding semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="6769" citStr="Li, 2010" startWordPosition="1056" endWordPosition="1057">[Named Entities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving go</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>X. Li. 2010. Understanding semantic structure of noun phrase queries. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liu</author>
<author>X Li</author>
<author>A Acero</author>
<author>Ye-Yi Wang</author>
</authors>
<title>Lexicon modeling for query understanding.</title>
<date>2011</date>
<booktitle>Proc. of ICASSP.</booktitle>
<contexts>
<context position="6788" citStr="Liu et al., 2011" startWordPosition="1058" endWordPosition="1061">ities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance acro</context>
</contexts>
<marker>Liu, Li, Acero, Wang, 2011</marker>
<rawString>J Liu, X. Li, A. Acero, and Ye-Yi Wang. 2011. Lexicon modeling for query understanding. Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>27--1</pages>
<contexts>
<context position="27968" citStr="Marcus et al., 1993" startWordPosition="4635" endWordPosition="4638">ed questions, however it is difficult to tag with WSJ-trained taggers because the word order is different than WSJ and contains a test-set vocabulary that is twice as large as the one in the development set. As for unlabeled data we crawled the web and collected around 100,000 questions that are similar in style and length to the ones in QuestionBank, e.g. ”wh” questions. There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc. More information about the Penn Tree-bank tag set can be found here (Marcus et al., 1993). 5.2 Models We evaluated several baseline models on two tasks: 5.2.1 Semantic Clustering Since MTR provides a mixture of properties adapted from earlier models, we present performance benchmarks on tag clustering using: (i) LDA; (ii) Hidden Markov Topic Model HMTM (Gruber et al., 2005); and, (iii) w-LDA (Petterson et al., 2010) that uses word features as priors in LDA. When a uniform β hyper-parameter is used with no external information on the state transitions in MTR, it reduces to a HMTM model. Similarly, if no Markov properties are used (bag-ofwords), MTR reduces to w-LDA. Each topic mode</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 27:1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Topic models conditioned on arbitrary features with dirichlet-multinomial regression.</title>
<date>2008</date>
<booktitle>Proc. UAI.</booktitle>
<contexts>
<context position="15907" citStr="Mimno et al., 2008" startWordPosition="2588" endWordPosition="2591"> word sequences, ηW. Here we represent the base-measure (hyperparameter) of the semantic tag indicator variable, which is not to be confused with a probability measure 2 We update the indicator parameter via mean criteria,ψv=wi=�K i,j=1ηsi|sj v=wi/(K2). If no prior on 2The base-measure used in Eq.(1) does not relate to a back-off model in LM sense. Here, instead of using a constant value for the hyper-parameters, we use probability scores that we obtain from LM. a specific word exists, a default value is used for base measure, ηv=0.01. (IV) Topic-Word Distribution Priors (βk): Different from (Mimno et al., 2008), which uses asymmetric hyper-parameters on document-topic distributions, in MTR, we learn the asymmetric hyper-parameters of the semantic tag-word distributions. We use blocked Gibbs sampling, in which the topic assignments sk and hyper-parameters {βk}Kk=1 are alternately sampled at each Gibbs sampling lag period g given all other variables. We impose the prior knowledge on naturally related words, such that if two words ”funny” and ”hilarious” indicate the same given ”genre” class, then their latent tag distributions should also be similar. We enforce this on smoothing parameter βk,v, e.g., </context>
</contexts>
<marker>Mimno, Li, McCallum, 2008</marker>
<rawString>D. Mimno, W. Li, and A. McCallum. 2008. Topic models conditioned on arbitrary features with dirichlet-multinomial regression. Proc. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Moon</author>
<author>K Erk</author>
<author>J Baldridge</author>
</authors>
<title>Crouching dirichlet, hidden markov model: Unsupervised pos tagging with context local tag generation.</title>
<date>2010</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="9594" citStr="Moon et al., 2010" startWordPosition="1494" endWordPosition="1497">h as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movies b Hitchcock ?&amp;quot; genre y [ ]director In LDA, common words tend to dominate all topics causing related words to end up in different topics. In (Petterson et al., 2010), the vectorbased features of words are used as prior information in L</context>
</contexts>
<marker>Moon, Erk, Baldridge, 2010</marker>
<rawString>T. Moon, K. Erk, and J. Baldridge. 2010. Crouching dirichlet, hidden markov model: Unsupervised pos tagging with context local tag generation. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V-A Nyugen</author>
<author>J Boyd-Graber</author>
<author>P Resnik</author>
</authors>
<title>Sits: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations.</title>
<date>2012</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="9476" citStr="Nyugen et al., 2012" startWordPosition="1476" endWordPosition="1479">Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al., 2012), etc. Recent topic models consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movies b Hitchcock ?&amp;quot; genre y [ ]director In LDA, common words tend to dominate all topics causing related words to end up </context>
</contexts>
<marker>Nyugen, Boyd-Graber, Resnik, 2012</marker>
<rawString>V.-A. Nyugen, J. Boyd-Graber, and P. Resnik. 2012. Sits: A hierarchical nonparametric model using speaker identity for topic segmentation in multiparty conversations. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
</authors>
<title>Clustering by committee.</title>
<date>2003</date>
<tech>Ph.D. Thesis,</tech>
<institution>University ofAlberta,</institution>
<location>Edmonton, Alta., Canada.</location>
<contexts>
<context position="8869" citStr="Pantel, 2003" startWordPosition="1384" endWordPosition="1385">l at each iteration by 915 keeping the history of the prior predictions. This ensures that a penalty is paid for diverging from the previous model’s predictions, which will be traded off against the benefit of reducing classification loss. We present a retrospective SSL for CRF, in that, the iterative learner keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. (II) Semantic Clustering. A common property of several context-based word clustering techniques, e.g., Brown clustering (Brown et al., 1992), Clustering by Committee (Pantel, 2003), etc., is that they mainly cluster based on local context such as nearby words. Standard topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), use a bag-of-words approach, which disregards word order and clusters words together that appear in a similar global context. Such models have been effective in discovering lexicons in many NLP tasks, e.g., named-entity recognition (Guo et al., 2009), word-sense disambiguation (Boyd-Graber et al., 2007; Li et al., 2010), syntactic/semantic parsing (Griffiths et al., 2005; Singh et al., 2010), speaker identification (Nyugen et al.</context>
</contexts>
<marker>Pantel, 2003</marker>
<rawString>P. Pantel. 2003. Clustering by committee. Ph.D. Thesis, University ofAlberta, Edmonton, Alta., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Petterson</author>
<author>A Smola</author>
<author>T Caetano</author>
<author>W Buntine</author>
<author>S Narayanamurthy</author>
</authors>
<title>Word features for latent dirichlet allocation. In</title>
<date>2010</date>
<booktitle>Proc. NIPS.</booktitle>
<contexts>
<context position="10124" citStr="Petterson et al., 2010" startWordPosition="1589" endWordPosition="1592">s consider word sequence information in documents (Griffiths et al., 2005; Moon et al., 2010). The Hidden Topic Markov Model (HTMM) by (Gruber et al., 2005), for instance, models sentences in documents as Markov chains, assuming all words in a sentence have the same topic. While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from any of the given semantic tags, as in ”what are [scary] movies b Hitchcock ?&amp;quot; genre y [ ]director In LDA, common words tend to dominate all topics causing related words to end up in different topics. In (Petterson et al., 2010), the vectorbased features of words are used as prior information in LDA so that the words that are synonyms end up in same topic. Thus, we build a semantically rich topic model, MTR, using word context features as side information. Using a smoothing prior for each word-topic pair (instead of a constant β smoother), MTR assures that the words are distributed over topics based on how similar they are. (e.g., ”scary” and ”spooky”, which have similar context features, go into the same semantic tag, ”genre”). Thus, to best of our knowledge, MTR is the first topic model to incorporate word features</context>
<context position="28298" citStr="Petterson et al., 2010" startWordPosition="4687" endWordPosition="4690"> the ones in QuestionBank, e.g. ”wh” questions. There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc. More information about the Penn Tree-bank tag set can be found here (Marcus et al., 1993). 5.2 Models We evaluated several baseline models on two tasks: 5.2.1 Semantic Clustering Since MTR provides a mixture of properties adapted from earlier models, we present performance benchmarks on tag clustering using: (i) LDA; (ii) Hidden Markov Topic Model HMTM (Gruber et al., 2005); and, (iii) w-LDA (Petterson et al., 2010) that uses word features as priors in LDA. When a uniform β hyper-parameter is used with no external information on the state transitions in MTR, it reduces to a HMTM model. Similarly, if no Markov properties are used (bag-ofwords), MTR reduces to w-LDA. Each topic model uses Gibbs sampling for inference and parameter learning. We sample models for 1000 iterations, with a 500-iteration burn-in and a sampling lag of 10. For testing we iterated the Gibbs sampler using the trained model for 10 iterations on the testing data. 5.2.2 SSL for Semantic/Syntactic Tagging We evaluated three different ba</context>
</contexts>
<marker>Petterson, Smola, Caetano, Buntine, Narayanamurthy, 2010</marker>
<rawString>J. Petterson, A. Smola, T. Caetano, W. Buntine, and S. Narayanamurthy. 2010. Word features for latent dirichlet allocation. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>R Mooney</author>
</authors>
<title>Cross-cutting models of lexical semantics.</title>
<date>2011</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="7738" citStr="Reisinger and Mooney, 2011" startWordPosition="1198" endWordPosition="1201">erent distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance across domains. Recent adaptation methods for SSL use: expectation minimization (Daum´e-III, 2010) graph-based learning (Chapelle et al., 2006; Zhu, 2005), etc. In (Subramanya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergence from the original model. (Lavoie et al., 2011) argues that iterative learning models should mitigate new errors made by the model at each iteration by 915 keeping the history of the prior predictions. This ensu</context>
</contexts>
<marker>Reisinger, Mooney, 2011</marker>
<rawString>J. Reisinger and R. Mooney. 2011. Cross-cutting models of lexical semantics. In Proc. of EMNLP. S. Singh, D. Hillard, and C. Leggetter. 2010.</rawString>
</citation>
<citation valid="false">
<title>Minimally-supervised extraction of entities from text advertisements.</title>
<booktitle>Proc. NAACL-HLT.</booktitle>
<marker></marker>
<rawString>Minimally-supervised extraction of entities from text advertisements. Proc. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>Proc. Interspeech.</booktitle>
<contexts>
<context position="14771" citStr="Stolcke, 2002" startWordPosition="2395" endWordPosition="2397">e graph representation of the Markov Topic Regression (MTR). To demonstrate hidden state Markov Chain, the generation of each word is explicitly shown (inside of the plate). ηS=p(si|si−1,wi=v,wi−1). We use web sources (wiki pages on movies and urls such as imdb.com) and labeled training data to extract entity lists that correspond to the semantic tags of our domains. We keep the frequency of each n-gram to convert into (empirical) prior probability distribution. (b) Language Model Prior (ηW): Probabilities on word transitions denoted as ηW =p(wi=v|wi−1). We built a language model using SRILM (Stolcke, 2002) on the domain specific sources such as top wiki pages and blogs on online movie reviews, etc., to obtain the probabilities of domain-specific n-grams, up to 3-grams. The observed priors, ηS and ηW, are used for calculating the base measure η for each vocabulary wv as: � ηsi|si−1,wi=v , if ηsi|si−1,wi=v ηsi|si−1 S exists, S v = ηwi=v,wi−1 W (1) In Eq.(1), we assume that the prior on the semantic tags, ηS, is more indicative of the decision for sampling a wi from a new tag compared to language model posteriors on word sequences, ηW. Here we represent the base-measure (hyperparameter) of the sem</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. An extensible language modeling toolkit. Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Subramanya</author>
<author>S Petrov</author>
<author>F Pereira</author>
</authors>
<title>Efficient graph-based semi-supervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="4110" citStr="Subramanya et al., 2010" startWordPosition="643" endWordPosition="646">nd later obtains the best tag sequences. Using the labeled source and automati914 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914–923, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cally labeled target data, it re-trains a new CRFmodel. Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain. In fact, most SSL methods used for adaptation, e.g., (Zhu, 2005), (Daum´e-III, 2010), (Subramanya et al., 2010), etc., do not emphasize this issue. With this in mind, we introduce a new iterative training algorithm, Retrospective Learning, as our second contribution. While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and greatly impacts semantic tagging performance. Spec</context>
<context position="7573" citStr="Subramanya et al., 2010" startWordPosition="1174" endWordPosition="1177">fers from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance across domains. Recent adaptation methods for SSL use: expectation minimization (Daum´e-III, 2010) graph-based learning (Chapelle et al., 2006; Zhu, 2005), etc. In (Subramanya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergence from the original model. (Lavoie et al., 2011</context>
<context position="19745" citStr="Subramanya et al., 2010" startWordPosition="3254" endWordPosition="3257">,i=1, we sample a new tag si=k given the remaining tags and hyper-parameters Ok, α, and ηwi=v . Using the following parameters; n(si) si|si−1 ji , which is the number of words assigned to a semantic class si=k excluding case i, and n(si−1) si is the number of transitions from class si−1 to si, where indicator ff(si−1, si)=1 if slot si=si−1, the update equation is formulated as follows: si |si p(sji = k |w, s−ji, α, ηwi −1, Ok) ∝ nji + βkwi (si) ∗ (n(si−1) + α)∗ n( )) + Pv βkv i (4) n(.) + ff(si−1, k) + Kα (si) 4 Semi-Supervised Semantic Labeling 4.1 Semi Supervised Learning (SSL) with CRF In (Subramanya et al., 2010), a new SSL method is described for adapting syntactic POS tagging of sentences in newswire articles along with search queries to a target domain of natural language (NL) questions. They decode unlabeled queries from target domain (t) using a CRF model trained on the POS-labeled newswire data (source domain (o)). The unlabeled POS tag posteriors are then smoothed using a graph-based learning algorithm. On graph, the similarities are defined over sequences by constructing the graph over types, word 3-grams, where types capture the local context of words. Since CRF tagger only uses local feature</context>
<context position="27194" citStr="Subramanya et al., 2010" startWordPosition="4502" endWordPosition="4506">ists (see §3) from movie related web sites, and online blogs and reviews. We collected around 300K movie review and blog entries on the entities observed in our data. We extract prior distributions for entities and n-grams to calculate entity list η and word-tag β priors (see §3.1). 5.1.2 Syntactic Tagging Datasets We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source data. Following previous research, we train on sections 00- 18, comprised of 38,219 POS-tagged sentences. To evaluate the domain adaptation (DA) approach and to compare with results reported by (Subramanya et al., 2010), we use the first and second half of QuestionBank (Judge et al., 2006) as our development and test sets (target). The QuestionBank contains 4000 POS-tagged questions, however it is difficult to tag with WSJ-trained taggers because the word order is different than WSJ and contains a test-set vocabulary that is twice as large as the one in the development set. As for unlabeled data we crawled the web and collected around 100,000 questions that are similar in style and length to the ones in QuestionBank, e.g. ”wh” questions. There are 36 different tag sets in the Penn dataset which includes tag </context>
<context position="29292" citStr="Subramanya et al., 2010" startWordPosition="4856" endWordPosition="4860">th a 500-iteration burn-in and a sampling lag of 10. For testing we iterated the Gibbs sampler using the trained model for 10 iterations on the testing data. 5.2.2 SSL for Semantic/Syntactic Tagging We evaluated three different baselines against our SSL models: ? CRF: a standard supervised sequence tagging. ? Self-CRF: a wrapper method for SSL using self-training. First a supervised learning algorithm is used to build a CRF model based on the labeled data. A CRF model is used to decode the unlabeled data to generate more labeled examples for re-training. ? SSL-Graph: A SSL model presented in (Subramanya et al., 2010) that uses graph-based learning as posterior tag smoother for CRF model using Eq.(5). In addition to the three baseline, we evaluated three variations of our SSL method: ? SSL-MTR: Our first version of SSL uses MTR to 920 LDA w-LDA HMTM MTR Figure 2: F-measure for semantic clustering performance. Performance differences for three different baseline models and our MTR approach by different semantic tags. smooth the semantic tag posteriors of a unlabeled data decoded by the CRF model using Eq.(5). * R-SSL-Graph: Our second version uses graph-learning to smooth the tag posteriors and retrain a ne</context>
<context position="30695" citStr="Subramanya et al., 2010" startWordPosition="5081" endWordPosition="5084">all the CRF models, we use lexical features consisting of unigrams in a five-word window around the current word. To include contextual information, we add binary features for all possible tags. We inject dictionary constraints to all CRF models, such as features indicating label prior information. For each model we use several named entity features, e.g., movie-title, actorname, etc., non-named entity (descriptive) features, e.g., movie-description, movie-genre, and domain independent dictionaries, e.g, time, location, etc. For graph-based learning, we implemented the algorithm presented in (Subramanya et al., 2010) and used the same hyper-parameters and features. For the rest of the hyper-parameters, we used: α=0.01 for MTR, π=0.5 for interpolation mixing. These parameters were chosen based on the performance of the development set. All CRF objective functions were optimized using Stochastic Gradient Descent. 5.3 Results and Discussions 5.3.1 Experiment 1: Clustering Semantic Tags. Here, we want to demonstrate the performance of MTR model for capturing relationships between words and semantic tags against baseline topic models: LDA, HMTM, w-LDA. We take the semantically labeled utterances from the movie</context>
<context position="32864" citStr="Subramanya et al., 2010" startWordPosition="5439" endWordPosition="5442">g HMTM baseline on named-entity tags, a 1% absolute gain on descriptive tags and a 2% absolute overall gain. As expected, we see a drop in F-measure on all models on descriptive tags. 5.3.2 Experiment 2: Domain Adaptation Task. We compare the performance of our SSL model to that of state-of-the-art models on semantic and syntactic tagging. Each SSL model is built using labeled training data from the source domain and unlabeled training data from target domain. In Table 2 we show the results on Movie and QuestionBank target test datasets. The results of SSL-Graph on QuestionBank is taken from (Subramanya et al., 2010). The selftraining model, Self-CRF adds 3% improvement over supervised CRF models on movie domain, but does not improve syntactic tagging. Because it is always inherently biased towards the source domain, self-training tends to reinforce the knowledge that the supervised model already has. SSL-Graph works much better for both syntactic and semantic tagging compared to CRF and Self-CRF models. Our Bayesian MTR efficiently extracts information from the unlabeled data for the target domain. Combined with retrospective training, R-SSL-MTR demonstrates noticeable improvements, ∼2% on descriptive ta</context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>A. Subramanya, S. Petrov, and F. Pereira. 2010. Efficient graph-based semi-supervised learning of structured tagging models. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>R DeMori</author>
</authors>
<title>Spoken language understanding: Systems for extracting semantic information from speech.</title>
<date>2011</date>
<publisher>Wiley Press.</publisher>
<contexts>
<context position="1770" citStr="Tur and DeMori, 2011" startWordPosition="259" endWordPosition="262">tial nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging. 1 Introduction Semantic tagging is used in natural language understanding (NLU) to recognize words of semantic importance in an utterance, such as entities. Typically, a semantic tagging model require large amount of domain specific data to achieve good performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection and labeling process. In the absence of large labeled training data, the tagging model can behave poorly on test data (target domain). This is usually caused by data mismatch issues and lack of coverage that arise when the target data does not match the training data. To deal with these issues, we present a new semi-supervised learning (SSL) approach, which mainly has two components. It initially starts with training supervised Conditional Random Fields (CRF) (Lafferty et al., 2001) on the source training data which has been semantic</context>
<context position="6635" citStr="Tur and DeMori, 2011" startWordPosition="1030" endWordPosition="1033">ere any [comedies] with [Ryan Gosling]? • How about [oscar winning] movies by [James Cameron]? • Find [Woody Allen] movies similar to [Manhattan]. [Named Entities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently onli</context>
</contexts>
<marker>Tur, DeMori, 2011</marker>
<rawString>G. Tur and R. DeMori. 2011. Spoken language understanding: Systems for extracting semantic information from speech. Wiley Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
<author>R Hoffman</author>
<author>X Li</author>
<author>J Syzmanski</author>
</authors>
<title>Semi-supervised learning of semantic classes for query understanding from the web and for the web.</title>
<date>2009</date>
<booktitle>In The 18th ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="6742" citStr="Wang et al., 2009" startWordPosition="1048" endWordPosition="1051">len] movies similar to [Manhattan]. [Named Entities] director: James Cameron, Woody Allen,... actor: Ryan Gosling, Woody Allen,... title: Manhattan, Midnight in Paris,... [Descriptive Tags] restriction: similar, suitable, free,rate,... description: oscar winning, new release, gardening,... genre: spooky, comedies, feel good, romance,... Table 1: Samples of semantically tagged utterances from movie domain, named-entities and descriptive tags. ing the need for significant manual labor (Tur and DeMori, 2011). Recent work on similar tasks overcome these challenges using SSL methods as follows: • (Wang et al., 2009; Li et al., 2009; Li, 2010; Liu et al., 2011) investigate web query tagging using semi-supervised sequence models. They extract semantic lexicons from unlabeled web queries, to use as features. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data</context>
</contexts>
<marker>Wang, Hoffman, Li, Syzmanski, 2009</marker>
<rawString>Y.-Y. Wang, R. Hoffman, X. Li, and J. Syzmanski. 2009. Semi-supervised learning of semantic classes for query understanding from the web and for the web. In The 18th ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>University of Wisconsin-Madison.</institution>
<contexts>
<context position="4063" citStr="Zhu, 2005" startWordPosition="639" endWordPosition="640">a (decoded using the CRF model) and later obtains the best tag sequences. Using the labeled source and automati914 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 914–923, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics cally labeled target data, it re-trains a new CRFmodel. Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain. In fact, most SSL methods used for adaptation, e.g., (Zhu, 2005), (Daum´e-III, 2010), (Subramanya et al., 2010), etc., do not emphasize this issue. With this in mind, we introduce a new iterative training algorithm, Retrospective Learning, as our second contribution. While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and gre</context>
<context position="7538" citStr="Zhu, 2005" startWordPosition="1170" endWordPosition="1171">eatures. Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). • Typically the source domain has different distribution than the target domain, due to topic shifts in time, newly introduced features (e.g., until recently online articles did not include facebook ”like” feature.), etc. Adapting the source domain using unlabeled data is the key to achieving good performance across domains. Recent adaptation methods for SSL use: expectation minimization (Daum´e-III, 2010) graph-based learning (Chapelle et al., 2006; Zhu, 2005), etc. In (Subramanya et al., 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. However, (Reisinger and Mooney, 2011) argues that vector space models, such as graph-learning, may fail to capture the richness of word meaning, as similarity is not a globally consistent metric. Rather than graph-learning, we present a new SSL using a probabilistic model, MTR, to cluster words based on co-occurrence statistics. • Most iterative SSL methods, do not keep track of the errors made, nor consider the divergence from the o</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>X. Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>