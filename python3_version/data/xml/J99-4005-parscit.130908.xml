<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.897859666666667">
Squibs and Discussions
Decoding Complexity in Word-Replacement
Translation Models
</title>
<author confidence="0.998225">
Kevin Knight*
</author>
<affiliation confidence="0.996656">
University of Southern California
</affiliation>
<bodyText confidence="0.996319125">
Statistical machine translation is a relatively new approach to the long-standing problem of trans-
lating human languages by computer. Current statistical techniques uncover translation rules
from bilingual training texts and use those rules to translate new texts. The general architecture
is the source-channel model: an English string is statistically generated (source), then statistically
transformed into French (channel). In order to translate (or &amp;quot;decode&amp;quot;) a French string, we look
for the most likely English source. We show that for the simplest form of statistical models, this
problem is NP-complete, i.e., probably exponential in the length of the observed sentence. We
trace this complexity to factors not present in other decoding problems.
</bodyText>
<sectionHeader confidence="0.985794" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999638384615385">
Statistical models are widely used in attacking natural language problems. The source-
channel framework is especially popular, finding applications in part-of-speech tag-
ging, accent restoration, transliteration, speech recognition, and many other areas. In
this framework, we build an underspecified model of how certain structures (such as
strings) are generated and transformed. We then instantiate the model through training
on a database of sample structures and transformations.
Recently, Brown et al. (1993) built a source-channel model of translation between
English and French. They assumed that English strings are produced according to some
stochastic process (source model) and transformed stochastically into French strings
(channel model). To translate French to English, it is necessary to find an English
source string that is likely according to the models. With a nod to its cryptographic
antecedents, this kind of translation is called decoding. This paper looks at decoding
complexity.
</bodyText>
<sectionHeader confidence="0.938661" genericHeader="method">
2. Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.998576285714286">
The prototype source-channel application in natural language is part-of-speech tagging
(Church 1988). We review it here for purposes of comparison with machine translation.
Source strings comprise sequences of part-of-speech tags like noun, verb, etc. A
simple source model assigns a probability to a tag sequence ti tm based on the prob-
abilities of the tag pairs inside it. Target strings are English sentences, e.g., w1 wm.
The channel model assumes each tag is probabilistically replaced by a word (e.g., noun
by dog) without considering context. More concretely, we have:
</bodyText>
<listItem confidence="0.9997475">
• v total tags
• A bigram source model with v2 parameters of the form b(tit), where
</listItem>
<footnote confidence="0.215731">
P(ti tm) h(ti lboundary) • b(t21t0 • . . . • b(tn tm_i) b(boundary I tm)
</footnote>
<note confidence="0.512232333333333">
* Information Sciences Institute, Marina del Rey, CA 90292
© 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 4
</note>
<listItem confidence="0.999455">
• A substitution channel model with parameters of the form s(zvj t), where
P(wi • • • wmiti • • • tm) — s(wi Iti) - s(w2It2) • ... • s(wniltni)
• an m-word text annotated with correct tags
• an m-word unannotated text
</listItem>
<bodyText confidence="0.9909186">
We can assign parts-of-speech to a previously unseen word sequence w1 ... Win
by finding the sequence ti ... 4, that maximizes P(ti ... tm I wi ... Wm). By Bayes&apos; rule,
we can equivalently maximize P(ti ... tm)•P(wi ... Wm&apos; ti . . . G), which we can calculate
directly from the b and s tables above.
Three interesting complexity problems in the source-channel framework are:
</bodyText>
<listItem confidence="0.999920333333333">
• Can parameter values be induced from annotated text efficiently?
• Can optimal decodings be produced efficiently?
• Can parameter values be induced from unannotated text efficiently?
</listItem>
<bodyText confidence="0.9987452">
The first problem is solved in 0(m) time for part-of-speech tagging—we simply
count tag pairs and word/tag pairs, then normalize. The second problem seems to
require enumerating all 0(e) potential source sequences to find the best, but can
actually be solved in 0(mv2) time with dynamic programming. We turn to the third
problem in the context of another application: cryptanalysis.
</bodyText>
<sectionHeader confidence="0.855701" genericHeader="method">
3. Substitution Ciphers
</sectionHeader>
<bodyText confidence="0.945997428571429">
In a substitution cipher, a plaintext message like HELLO WORLD is transformed into
a ciphertext message like EOPPX YXAPF via a fixed letter-substitution table. As with
tagging, we can assume an alphabet of v source tokens, a bigram source model, a
substitution channel model, and an m-token coded text.
If the coded text is annotated with corresponding English, then building source
and channel models is trivially 0(m). Comparing the situation to part-of-speech tag-
ging:
</bodyText>
<listItem confidence="0.99672025">
• (Bad news.) Cryptanalysts rarely get such coded/decoded text pairs and
must employ &amp;quot;ciphertext-only&amp;quot; attacks using unannotated training data.
• (Good news.) It is easy to train a source model separately, on raw
unannotated English text that is unconnected to the ciphertext.
</listItem>
<bodyText confidence="0.998456555555555">
Then the problem becomes one of acquiring a channel model, i.e., a table s(f le) with
an entry for each code-letter/plaintext-letter pair. Starting with an initially uniform
table, we can use the estimation-maximization (EM) algorithm to iteratively revise
s(f. le) so as to increase the probability of the observed corpus P(f). Figure 1 shows a
naive EM implementation that runs in 0(mvin) time. There is an efficient 0(mv2) EM
implementation based on dynamic programming that accomplishes the same thing.
Once the s(f le) table has been learned, there is a similar 0(mv2) algorithm for optimal
decoding. Such methods can break English letter-substitution ciphers of moderate
size.
</bodyText>
<page confidence="0.995806">
608
</page>
<subsectionHeader confidence="0.63691">
Knight Decoding Complexity
</subsectionHeader>
<bodyText confidence="0.94401">
Given coded text f of length m, a plaintext vocabulary of v tokens, and a source model b:
</bodyText>
<listItem confidence="0.898555">
1. set the s(f le) table initially to be uniform
2. for several iterations do:
a. set up a count table c(f le) with zero entries
b. P(f) = 0
c. for all possible source texts el (ei drawn from vocabulary)
compute P(e) = b (el I boundary) • b(boundary em) • 117._2b(eilei_i)
</listItem>
<equation confidence="0.922547">
compute P(fle) = s(f fie j)
P(f) += P(e) • P(fle)
d. for all source texts e of length m
compute P(elf) = P(ekPffle)
for j 1 to m
c(Ie) += P(e[f)
e. normalize c(f le) table to create a revised s(f le)
</equation>
<figureCaption confidence="0.898759">
Figure 1
</figureCaption>
<bodyText confidence="0.747149">
A naive application of the EM algorithm to break a substitution cipher. It runs in 0(men) time.
</bodyText>
<sectionHeader confidence="0.849312" genericHeader="method">
4. Machine Translation
</sectionHeader>
<bodyText confidence="0.999473857142857">
In our discussion of substitution ciphers, we were on relatively sure ground—the
channel model we assumed in decoding is actually the same one used by the cipher
writer for encoding. That is, we know that plaintext is converted to ciphertext, letter by
letter, according to some table. We have no such clear conception about how English
gets converted to French, although many theories exist. Brown et al. (1993) recently cast
some simple theories into a source-channel framework, using the bilingual Canadian
parliament proceedings as training data. We may assume:
</bodyText>
<listItem confidence="0.9999362">
• v total English words.
• A bigram source model with v2 parameters.
• Various substitution/permutation channel models.
• A collection of bilingual sentence pairs (sentence lengths &lt; m).
• A collection of monolingual French sentences (sentence lengths &lt; m).
</listItem>
<bodyText confidence="0.9994245">
Bilingual texts seem to exhibit English words getting substituted with French ones,
though not one-for-one and not without changing their order. These are important
departures from the two applications discussed earlier.
In the main channel model of Brown et al. (1993), each English word token e,
in a source sentence is assigned a &amp;quot;fertility&amp;quot; 0„ which dictates how many French
words it will produce. These assignments are made stochastically according to a table
n(01 e). Then actual French words are produced according to s(f le) and permuted into
new positions according to a distortion table d(j1i, m, I). Here, j and i are absolute tar-
get/source word positions within a sentence, and m and I are target/source sentence
lengths.
Inducing n, s, and d parameter estimates is easy if we are given annotations in the
form of word alignments. An alignment is a set of connections between English and
French words in a sentence pair. In Brown et al. (1993), alignments are asymmetric—
each French word is connected to exactly one English word.
</bodyText>
<page confidence="0.985977">
609
</page>
<figure confidence="0.2481625">
Computational Linguistics Volume 25, Number 4
Given a collection of sentence pairs:
1. collect estimates for the (ml/) table directly from the data
2. set the s(f le) table initially to be uniform
3. for several iterations do:
a. set up a count table c(f le) with zero entries
b. for each given sentence pair e, f with respective lengths 1, m:
for al = 1 to 1
for a2 = 1 to 1 /* select connections for a word alignment */
• • •
for am = 1 to 1
.,mle) U=1 s(file)
compute le, f) =
P(fle)
• • z_.,4=illim=is(filea;)
for j = 1 to m
c(filead += P(ai am le, f)
c. normalize c(fjlei) table to create new s(fi
</figure>
<figureCaption confidence="0.6162635">
Figure 2
Naive EM training for the Model 1 channel model.
</figureCaption>
<bodyText confidence="0.985753466666667">
Word-aligned data is usually not available, but large sets of unaligned bilin-
gual sentence pairs do sometimes exist. A single sentence pair will have rn possible
alignments—for each French word position 1 m, there is a choice of I English po-
sitions to connect to. A naive EM implementation will collect n, s, and d counts by
considering each alignment, but this is expensive. (By contrast, part-of-speech tagging
involves a single alignment, leading to 0(m) training). Lacking a polynomial refor-
mulation, Brown et al. (1993) decided to collect counts only over a subset of likely
alignments. To bootstrap, they required some initial idea of what alignments are rea-
sonable, so they began with several iterations of a simpler channel model (called
Model 1) that has nicer computational properties.
In the following description of Model 1, we represent an alignment formally as a
vector al, . . with values al ranging over English word positions 1 ... 1..
Model 1 Channel
Parameters: c(m 1/) and s(f le).
Given a source sentence e of length 1:
</bodyText>
<listItem confidence="0.9971198">
1. choose a target sentence length m according to c(mil)
2. for j = 1 to m, choose an English word position aj according to the
uniform distribution over 1 /
3. for j = 1 to m, choose a French word f1 according to WI lea)
4. read off fi ...fm as the target sentence
</listItem>
<bodyText confidence="0.971841">
Because the same e may produce the same f by means of many different align-
ments, we must sum over all of them to obtain P(fle):
</bodyText>
<equation confidence="0.814255">
in vkl
s(f. le )
P(fle) = c(mi., L-..a1=1 L-ra2=1 • &apos; • L-a4„,=1 aj
</equation>
<bodyText confidence="0.811637333333333">
Figure 2 illustrates naive EM training for Model 1. If we compute P(fle) once per
iteration, outside the &amp;quot;for a&amp;quot; loops, then the complexity is 0(m/m) per sentence pair,
per iteration.
</bodyText>
<page confidence="0.956894">
610
</page>
<subsectionHeader confidence="0.199693">
Knight Decoding Complexity
</subsectionHeader>
<bodyText confidence="0.685352">
More efficient 0(/m) training was devised by Brown et al. (1993). Instead of pro-
</bodyText>
<equation confidence="0.862238384615385">
cessing each alignment separately, they modified the algorithm in Figure 2 as follows:
b. for each given sentence pair e, f of respective lengths /, m:
for j = 1 to m
sum = 0
for i = 1 to /
sum += s(fjle,)
for i = 1 to 1
c(Ie)+= s(f. le ) / sum
1 I
This works because of the algebraic trick that the portion of POI e) we originally wrote
m .-1
as Ea,=1 &amp;quot; Eal n
tn=i IT i_ 1 s(fjlea,) can be rewritten as 111=1 Li=1 s(f;
</equation>
<bodyText confidence="0.994632333333333">
We next consider decoding. We seek a string e that maximizes P(e If), or equiva-
lently maximizes P(e) • P(fle). A naive algorithm would evaluate all possible source
strings, whose lengths are potentially unbounded. If we limit our search to strings
at most twice the length m of our observed French, then we have a naive 0(m2v2m)
method:
Given a string f of length m
</bodyText>
<listItem confidence="0.993252333333333">
1. for all source strings e of length / 2m:
a. compute P(e) = b(ei boundary) b(boundary el) • 1111=2 1*de/A
b. compute P(fle) = c(m1/)/+„fJ s(fie)
c. compute P(elf) P(e) • P(fle)
d. if P(elf) is the best so far, remember it
2. print best e
</listItem>
<bodyText confidence="0.99942975">
We may now hope to find a way of reorganizing this computation, using tricks like
the ones above. Unfortunately, we are unlikely to succeed, as we now show. For
proof purposes, we define our optimization problem with an associated yes-no decision
problem:
</bodyText>
<sectionHeader confidence="0.563282" genericHeader="method">
Definition: M1-OPTIMIZE
</sectionHeader>
<bodyText confidence="0.976795">
Given a string f of length m and a set of parameter tables (b, E, s), return a string e of
length 1 &lt; 2m that maximizes P(elf), or equivalently maximizes
</bodyText>
<equation confidence="0.698362">
P(e) P(fle) = h(ei boundary) b(boundary I el) • rf=2
coo A fri_i
Definition: Ml-DECIDE
</equation>
<bodyText confidence="0.9996104">
Given a string f of length m, a set of parameter tables (b, €, s), and a real number k,
does there exist a string e of length / &lt; 2m such that P(e) • P(fle) &gt; k?
We will leave the relationship between these two problems somewhat open and
intuitive, noting only that M1-DECIDE&apos;s intractability does not bode well for Ml-
OPTIMIZE.
</bodyText>
<page confidence="0.987658">
611
</page>
<figure confidence="0.54887">
Computational Linguistics Volume 25, Number 4
Theorem
M1-DECIDE is NP-complete.
</figure>
<bodyText confidence="0.99881425">
To show inclusion in NP, we need only nondeterministically choose e for any
problem instance and verify that it has the requisite P(e) • P(fle) in 0(m2) time. Next
we give separate polynomial-time reductions from two NP-complete problems. Each
reduction highlights a different source of complexity.
</bodyText>
<subsectionHeader confidence="0.999508">
4.1 Reduction 1 (from Hamilton Circuit Problem)
</subsectionHeader>
<bodyText confidence="0.998312625">
The Hamilton Circuit Problem asks: given a directed graph G with vertices labeled
0, , n, does G have a path that visits each vertex exactly once and returns to its
starting point? We transform any Hamilton Circuit instance into an M1-DECIDE in-
stance as follows. First, we create a French vocabulary fn, associating word fi
with vertex i in the graph. We create a slightly larger English vocabulary eo, &apos;en,
with eo serving as the &amp;quot;boundary&amp;quot; word for source model scoring. Ultimately, we will
ask Ml-DECIDE to decode the string fi . fn.
We create channel model tables as follows:
</bodyText>
<construct confidence="0.52958675">
if i = j
otherwise
if / = m
otherwise
</construct>
<bodyText confidence="0.960845">
These tables ensure that any decoding e of fi –ft, will contain the n words el, • • • , en
(in some order). We now create a source model. For every pair (i,j) such that 0 &lt; i,j &lt; n:
</bodyText>
<equation confidence="0.818717666666667">
1/n if graph G contains an edge from vertex i to vertex j
b(ejle,) =
0 otherwise
</equation>
<bodyText confidence="0.999885523809524">
Finally, we set k to zero. To solve a Hamilton Circuit Problem, we transform it as
above (in quadratic time), then invoke Ml-DECIDE with inputs b, e, s, k, and fi . •
If M1-DECIDE returns yes, then there must be some string e with both P(e) and
P(fle) nonzero. The channel model lets us conclude that if P(fle) is nonzero, then e
contains the n words el, , en in some order. If P(e) is nonzero, then every bigram in
e (including the two boundary bigrams involving eo) has nonzero probability. Because
each English word in e corresponds to a unique vertex, we can use the order of words
in e to produce an ordering of vertices in G. We append vertex 0 to the beginning
and end of this list to produce a Hamilton Circuit. The source model construction
guarantees an edge between each vertex and the next.
If Ml-DECIDE returns no, then we know that every string e includes at least one
zero value in the computation of either P(e) or P(fle). From any proposed Hamilton
Circuit—i.e., some ordering of vertices in G—we can construct a string e using the
same ordering. This e will have P(fle) = 1 according to the channel model. Therefore,
P(e) = 0. By the source model, this can only happen if the proposed &amp;quot;circuit&amp;quot; is actually
broken somewhere. So no Hamilton Circuit exists.
Figure 3 illustrates the intuitive correspondence between selecting a good word
order and finding a Hamilton Circuit. We note that Brew (1992) discusses the NP-
completeness of a related problem, that of finding some permutation of a string that
is acceptable to a given context-free grammar. Both of these results deal with decision
problems. Returning to optimization, we recall another circuit task called the Traveling
</bodyText>
<page confidence="0.988155">
612
</page>
<figure confidence="0.575555">
Knight Decoding Complexity
</figure>
<figureCaption confidence="0.898513">
Figure 3
</figureCaption>
<bodyText confidence="0.959437038461538">
Selecting a good source word order is like solving the Hamilton Circuit Problem. If we assume
that the channel model offers deterministic, word-for-word translations, then the bigram
source model takes responsibility for ordering them. Some word pairs in the source language
may be illegal. In that case, finding a legal word ordering is like finding a complete circuit in a
graph. (In the graph shown above, a sample circuit is boundary —&gt; this year comma —&gt; my
—4 birthday falls —&gt; on a Thursday —&gt; boundary). If word pairs have probabilities attached
to them, then word ordering resembles the finding the least-cost circuit, also known as the
Traveling Salesman Problem.
Salesman Problem. It introduces edge costs c111 and seeks a minimum-cost circuit. By
viewing edge costs as log probabilities, we can cast the Traveling Salesman Problem
as one of optimizing P(e), that is, of finding the best source word order in Model 1
decoding.
4.2 Reduction 2 (from Minimum Set Cover Problem)
The Minimum Set Cover Problem asks: given a collection C of subsets of finite set S.
and integer n, does C contain a cover for S of size &lt; n, i.e., a subcollection whose
union is S? We now transform any instance of Minimum Set Cover into an instance
of M1-DECIDE, using polynomial time. This time, we assume a rather neutral source
model in which all strings of a given length are equally likely, but we construct a more
complex channel.
We first create a source word e, for each subset in C, and let g, be the size of
that subset. We create a table b(e,jej) with values set uniformly to the reciprocal of the
source vocabulary size (i.e., the number of subsets in C).
Assuming S has m elements, we next create target words ,fin corresponding
to each of those elements, and set up channel model tables as follows:
1/g, if the element in S corresponding to j5 is also in the subset
s(fi le,) = corresponding to e,
</bodyText>
<equation confidence="0.8278274">
0 otherwise
Ii if / &lt; n
6(1111&apos;)i\ 10 otherwise
Ii if / &gt; n
€(7n — 10 otherwise
</equation>
<bodyText confidence="0.942407666666667">
Finally, we set k to zero. This completes the reduction. To solve an instance of
Minimum Set Cover in polynomial time, we transform it as above, then call Ml-
DECIDE with inputs b, E, s, k, and the words • • • n in any order.
</bodyText>
<figure confidence="0.967544833333334">
falls
Thursday
a
boundary
this
on
bir hday
year
.t
613
Computational Linguistics Volume 25, Number 4
obtained however
</figure>
<figureCaption confidence="0.998028">
Figure 4
</figureCaption>
<bodyText confidence="0.996360705882353">
Selecting a concise set of source words is like solving the Minimum Set Cover Problem. A
channel model with overlapping, one-to-many dictionary entries will typically license many
decodings. The source model may prefer short decodings over long ones. Searching for a
decoding of length &lt; n is difficult, resembling the problem of covering a finite set with a small
collection of subsets. In the example shown above, the smallest acceptable set of source words
is {and, cooked, however, left, comma, period} .
If M1-DECIDE returns yes, then some decoding e with P(e) • P(fle) &gt; 0 must exist.
We know that e must contain n or fewer words—otherwise P(fle) = 0 by the E table.
Furthermore, the s table tells us that every word fi is covered by at least one English
word in e. Through the one-to-one correspondence between elements of e and C, we
produce a set cover of size &lt; n for S.
Likewise, if M1-DECIDE returns no, then all decodings have P(e) • P(fle) = 0.
Because there are no zeroes in the source table b, every e has P(fle) = 0. Therefore
either (1) the length of e exceeds n, or (2) some fi is left uncovered by the words in e.
Because source words cover target words in exactly the same fashion as elements of C
cover S, we conclude that there is no set cover of size &lt; n for S. Figure 4 illustrates the
intuitive correspondence between source word selection and minimum set covering.
</bodyText>
<sectionHeader confidence="0.997959" genericHeader="conclusions">
5. Discussion
</sectionHeader>
<bodyText confidence="0.999962">
The two proofs point up separate factors in MT decoding complexity. One is word-
order selection. But even if any word order will do, there is still the problem of picking
a concise decoding in the face of overlapping bilingual dictionary entries. The former
is more closely tied to the source model, and the latter to the channel model, though
the complexity arises from the interaction of the two.
We should note that Model 1 is an intentionally simple translation model, one
whose primary purpose in machine translation has been to allow bootstrapping into
more complex translation models (e.g., IBM Models 2-5). It is easy to show that the
intractability results also apply to stronger &amp;quot;fertility/distortion&amp;quot; models; we assign
zero probability to fertilities other than 1, and we set up uniform distortion tables.
Simple translation models like Model 1 find more direct use in other applications
(e.g., lexicon construction, idiom detection, psychological norms, and cross-language
information retrieval), so their computational properties are of wider interest.
</bodyText>
<page confidence="0.992561">
614
</page>
<subsectionHeader confidence="0.259982">
Knight Decoding Complexity
</subsectionHeader>
<bodyText confidence="0.999969117647059">
The proofs we presented are based on a worst-case analysis. Real s, e, and b ta-
bles may have properties that permit faster optimal decoding than the artificial tables
constructed above. It is also possible to devise approximation algorithms like those de-
vised for other NP-complete problems. To the extent that word ordering is like solving
the Traveling Salesman Problem, it is encouraging substantial progress continues to be
made on Traveling Salesman algorithms. For example, it is often possible to get within
two percent of the optimal tour in practice, and some researchers have demonstrated
an optimal tour of over 13,000 U.S. cities. (The latter experiment relied on things like
distance symmetry and the triangle inequality constraint, however, which do not hold
in word ordering.) So far, statistical translation research has either opted for heuristic
beam-search algorithms or different channel models. For example, some researchers
avoid bag generation by preprocessing bilingual texts to remove word-order differ-
ences, while others adopt channels that eliminate syntactically unlikely alignments.
Finally, expensive decoding also suggests expensive training from unannotated
(monolingual) texts, which presents a challenging bottleneck for extending statistical
machine translation to language pairs and domains where large bilingual corpora do
not exist.
</bodyText>
<sectionHeader confidence="0.977496" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999766647058824">
Brew, Chris. 1992. Letting the cat out of the
bag: Generation for shake-and-bake MT.
In Proceedings of the 14th International
Conference on Computational Linguistics
(COLING), pages 610-616, Nantes, France,
August.
Brown, Peter, Stephen Della-Pietra, Vincent
Della-Pietra, and Robert Mercer. 1993. The
mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263-311.
Church, Kenneth. 1988. A stochastic parts
program and noun phrase parser for
unrestricted text. In Proceedings of the 2nd
Conference on Applied Natural Language
Processing, pages 136-143, Austin, TX,
June.
</reference>
<page confidence="0.998545">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.989898">
<title confidence="0.999691333333333">Squibs and Discussions Decoding Complexity in Word-Replacement Translation Models</title>
<author confidence="0.999979">Kevin Knight</author>
<affiliation confidence="0.999672">University of Southern California</affiliation>
<abstract confidence="0.998870125">Statistical machine translation is a relatively new approach to the long-standing problem of translating human languages by computer. Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts. The general architecture is the source-channel model: an English string is statistically generated (source), then statistically transformed into French (channel). In order to translate (or &amp;quot;decode&amp;quot;) a French string, we look for the most likely English source. We show that for the simplest form of statistical models, this problem is NP-complete, i.e., probably exponential in the length of the observed sentence. We trace this complexity to factors not present in other decoding problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Brew</author>
</authors>
<title>Letting the cat out of the bag: Generation for shake-and-bake MT.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>610--616</pages>
<location>Nantes, France,</location>
<contexts>
<context position="15090" citStr="Brew (1992)" startWordPosition="2579" endWordPosition="2580"> Ml-DECIDE returns no, then we know that every string e includes at least one zero value in the computation of either P(e) or P(fle). From any proposed Hamilton Circuit—i.e., some ordering of vertices in G—we can construct a string e using the same ordering. This e will have P(fle) = 1 according to the channel model. Therefore, P(e) = 0. By the source model, this can only happen if the proposed &amp;quot;circuit&amp;quot; is actually broken somewhere. So no Hamilton Circuit exists. Figure 3 illustrates the intuitive correspondence between selecting a good word order and finding a Hamilton Circuit. We note that Brew (1992) discusses the NPcompleteness of a related problem, that of finding some permutation of a string that is acceptable to a given context-free grammar. Both of these results deal with decision problems. Returning to optimization, we recall another circuit task called the Traveling 612 Knight Decoding Complexity Figure 3 Selecting a good source word order is like solving the Hamilton Circuit Problem. If we assume that the channel model offers deterministic, word-for-word translations, then the bigram source model takes responsibility for ordering them. Some word pairs in the source language may be</context>
</contexts>
<marker>Brew, 1992</marker>
<rawString>Brew, Chris. 1992. Letting the cat out of the bag: Generation for shake-and-bake MT. In Proceedings of the 14th International Conference on Computational Linguistics (COLING), pages 610-616, Nantes, France, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della-Pietra</author>
<author>Vincent Della-Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1413" citStr="Brown et al. (1993)" startWordPosition="194" endWordPosition="197">rved sentence. We trace this complexity to factors not present in other decoding problems. 1. Introduction Statistical models are widely used in attacking natural language problems. The sourcechannel framework is especially popular, finding applications in part-of-speech tagging, accent restoration, transliteration, speech recognition, and many other areas. In this framework, we build an underspecified model of how certain structures (such as strings) are generated and transformed. We then instantiate the model through training on a database of sample structures and transformations. Recently, Brown et al. (1993) built a source-channel model of translation between English and French. They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastically into French strings (channel model). To translate French to English, it is necessary to find an English source string that is likely according to the models. With a nod to its cryptographic antecedents, this kind of translation is called decoding. This paper looks at decoding complexity. 2. Part-of-Speech Tagging The prototype source-channel application in natural language is part-of-speech tag</context>
<context position="6569" citStr="Brown et al. (1993)" startWordPosition="1044" endWordPosition="1047"> 1 to m c(Ie) += P(e[f) e. normalize c(f le) table to create a revised s(f le) Figure 1 A naive application of the EM algorithm to break a substitution cipher. It runs in 0(men) time. 4. Machine Translation In our discussion of substitution ciphers, we were on relatively sure ground—the channel model we assumed in decoding is actually the same one used by the cipher writer for encoding. That is, we know that plaintext is converted to ciphertext, letter by letter, according to some table. We have no such clear conception about how English gets converted to French, although many theories exist. Brown et al. (1993) recently cast some simple theories into a source-channel framework, using the bilingual Canadian parliament proceedings as training data. We may assume: • v total English words. • A bigram source model with v2 parameters. • Various substitution/permutation channel models. • A collection of bilingual sentence pairs (sentence lengths &lt; m). • A collection of monolingual French sentences (sentence lengths &lt; m). Bilingual texts seem to exhibit English words getting substituted with French ones, though not one-for-one and not without changing their order. These are important departures from the two</context>
<context position="7936" citStr="Brown et al. (1993)" startWordPosition="1265" endWordPosition="1268">rtility&amp;quot; 0„ which dictates how many French words it will produce. These assignments are made stochastically according to a table n(01 e). Then actual French words are produced according to s(f le) and permuted into new positions according to a distortion table d(j1i, m, I). Here, j and i are absolute target/source word positions within a sentence, and m and I are target/source sentence lengths. Inducing n, s, and d parameter estimates is easy if we are given annotations in the form of word alignments. An alignment is a set of connections between English and French words in a sentence pair. In Brown et al. (1993), alignments are asymmetric— each French word is connected to exactly one English word. 609 Computational Linguistics Volume 25, Number 4 Given a collection of sentence pairs: 1. collect estimates for the (ml/) table directly from the data 2. set the s(f le) table initially to be uniform 3. for several iterations do: a. set up a count table c(f le) with zero entries b. for each given sentence pair e, f with respective lengths 1, m: for al = 1 to 1 for a2 = 1 to 1 /* select connections for a word alignment */ • • • for am = 1 to 1 .,mle) U=1 s(file) compute le, f) = P(fle) • • z_.,4=illim=is(fi</context>
<context position="9213" citStr="Brown et al. (1993)" startWordPosition="1497" endWordPosition="1500">ze c(fjlei) table to create new s(fi Figure 2 Naive EM training for the Model 1 channel model. Word-aligned data is usually not available, but large sets of unaligned bilingual sentence pairs do sometimes exist. A single sentence pair will have rn possible alignments—for each French word position 1 m, there is a choice of I English positions to connect to. A naive EM implementation will collect n, s, and d counts by considering each alignment, but this is expensive. (By contrast, part-of-speech tagging involves a single alignment, leading to 0(m) training). Lacking a polynomial reformulation, Brown et al. (1993) decided to collect counts only over a subset of likely alignments. To bootstrap, they required some initial idea of what alignments are reasonable, so they began with several iterations of a simpler channel model (called Model 1) that has nicer computational properties. In the following description of Model 1, we represent an alignment formally as a vector al, . . with values al ranging over English word positions 1 ... 1.. Model 1 Channel Parameters: c(m 1/) and s(f le). Given a source sentence e of length 1: 1. choose a target sentence length m according to c(mil) 2. for j = 1 to m, choose </context>
<context position="10471" citStr="Brown et al. (1993)" startWordPosition="1729" endWordPosition="1732">o the uniform distribution over 1 / 3. for j = 1 to m, choose a French word f1 according to WI lea) 4. read off fi ...fm as the target sentence Because the same e may produce the same f by means of many different alignments, we must sum over all of them to obtain P(fle): in vkl s(f. le ) P(fle) = c(mi., L-..a1=1 L-ra2=1 • &apos; • L-a4„,=1 aj Figure 2 illustrates naive EM training for Model 1. If we compute P(fle) once per iteration, outside the &amp;quot;for a&amp;quot; loops, then the complexity is 0(m/m) per sentence pair, per iteration. 610 Knight Decoding Complexity More efficient 0(/m) training was devised by Brown et al. (1993). Instead of processing each alignment separately, they modified the algorithm in Figure 2 as follows: b. for each given sentence pair e, f of respective lengths /, m: for j = 1 to m sum = 0 for i = 1 to / sum += s(fjle,) for i = 1 to 1 c(Ie)+= s(f. le ) / sum 1 I This works because of the algebraic trick that the portion of POI e) we originally wrote m .-1 as Ea,=1 &amp;quot; Eal n tn=i IT i_ 1 s(fjlea,) can be rewritten as 111=1 Li=1 s(f; We next consider decoding. We seek a string e that maximizes P(e If), or equivalently maximizes P(e) • P(fle). A naive algorithm would evaluate all possible source </context>
</contexts>
<marker>Brown, Della-Pietra, Della-Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter, Stephen Della-Pietra, Vincent Della-Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 2nd Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<location>Austin, TX,</location>
<contexts>
<context position="2031" citStr="Church 1988" startWordPosition="284" endWordPosition="285"> a source-channel model of translation between English and French. They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastically into French strings (channel model). To translate French to English, it is necessary to find an English source string that is likely according to the models. With a nod to its cryptographic antecedents, this kind of translation is called decoding. This paper looks at decoding complexity. 2. Part-of-Speech Tagging The prototype source-channel application in natural language is part-of-speech tagging (Church 1988). We review it here for purposes of comparison with machine translation. Source strings comprise sequences of part-of-speech tags like noun, verb, etc. A simple source model assigns a probability to a tag sequence ti tm based on the probabilities of the tag pairs inside it. Target strings are English sentences, e.g., w1 wm. The channel model assumes each tag is probabilistically replaced by a word (e.g., noun by dog) without considering context. More concretely, we have: • v total tags • A bigram source model with v2 parameters of the form b(tit), where P(ti tm) h(ti lboundary) • b(t21t0 • . .</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the 2nd Conference on Applied Natural Language Processing, pages 136-143, Austin, TX, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>