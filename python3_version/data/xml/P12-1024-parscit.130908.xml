<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.742524">
Spectral Learning of Latent-Variable PCFGs
</title>
<author confidence="0.994053">
Shay B. Cohen&apos;, Karl Stratos&apos;, Michael Collins&apos;, Dean P. Foster&apos;, and Lyle Ungar&apos;
</author>
<affiliation confidence="0.9944775">
&apos;Dept. of Computer Science, Columbia University
&apos;Dept. of Statistics/&apos;Dept. of Computer and Information Science, University of Pennsylvania
</affiliation>
<email confidence="0.982732">
{scohen,stratos,mcollinsl@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
</email>
<sectionHeader confidence="0.984704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998786">
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al., 2006).
Under a separability (singular value) condi-
tion, we prove that the method provides con-
sistent parameter estimates.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918547169811">
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other fields. The EM algorithm is
a remarkably successful method for parameter esti-
mation within these models: it is simple, it is often
relatively efficient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of finding the global optimum of
the likelihood function. From a theoretical perspec-
tive, this means that the EM algorithm is not guar-
anteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difficult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation meth-
ods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov mod-
els (Hsu et al., 2009). These algorithms use spec-
tral methods: that is, algorithms based on eigen-
vector decompositions of linear systems, in particu-
lar singular value decomposition (SVD). In the gen-
eral case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods finesse
the problem of intractibility by assuming separabil-
ity conditions. For example, the algorithm of Hsu
et al. (2009) has a sample complexity that is polyno-
mial in 1/Q, where Q is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al., 2006; Matsuzaki et al., 2005). Our
method involves a significant extension of the tech-
niques from Hsu et al. (2009). L-PCFGs have been
shown to be a very effective model for natural lan-
guage parsing. Under a separation (singular value)
condition, our algorithm provides consistent param-
eter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter es-
timation, with the usual problems of local optima.
The parameter estimation algorithm (see figure 4)
is simple and efficient. The first step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a low-
dimensional space. In a second step, empirical av-
erages are calculated on the training example, fol-
lowed by standard matrix operations. On test ex-
amples, simple (tensor-based) variants of the inside-
outside algorithm (figures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
</bodyText>
<listItem confidence="0.704917705882353">
• Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calcu-
lates inside and outside terms correctly.
• Observable representations. Section 6 shows
that under a singular-value condition, there is an ob-
servable form for the tensors required by the inside-
outside algorithm. By an observable form, we fol-
low the terminology of Hsu et al. (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the ob-
servable form satisfy the conditions of theorem 1.
• Estimating the model. Section 7 gives an al-
gorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
</listItem>
<bodyText confidence="0.945833428571428">
sample complexity result, showing that the estimates
�
converge to the true distribution at a rate of 1/ M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-
</bodyText>
<page confidence="0.986631">
223
</page>
<note confidence="0.985729">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223–231,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999795">
veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for exam-
ple alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the inside-
outside algorithm gives a new view of basic calcula-
tions in PCFGs, and may itself lead to new models.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955461538461">
For work on L-PCFGs using the EM algorithm, see
Petrov et al. (2006), Matsuzaki et al. (2005), Pereira
and Schabes (1992). Our work builds on meth-
ods for learning of HMMs (Hsu et al., 2009; Fos-
ter et al., 2012; Jaeger, 2000), but involves sev-
eral extensions: in particular in the tensor form of
the inside-outside algorithm, and observable repre-
sentations for the tensor form. Balle et al. (2011)
consider spectral learning of finite-state transducers;
Lugue et al. (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al.
(2011) consider spectral learning algorithms of tree-
structured directed bayes nets.
</bodyText>
<sectionHeader confidence="0.997504" genericHeader="method">
3 Notation
</sectionHeader>
<bodyText confidence="0.9909309">
Given a matrix A or a vector v, we write AT or vT
for the associated transpose. For any integer n ≥ 1,
we use [n] to denote the set {1, 2,... n}. For any
row or column vector y ∈ Rm, we use diag(y) to
refer to the (m × m) matrix with diagonal elements
equal to yh for h = 1... m, and off-diagonal ele-
ments equal to 0. For any statement Γ, we use [[Γ]]
to refer to the indicator function that is 1 if Γ is true,
and 0 if Γ is false. For a random variable X, we use
E[X] to denote its expected value.
We will make (quite limited) use of tensors:
Definition 1 A tensor C ∈ R(mxmxm) is a set of
m3 parameters Ci,j,k for i, j, k ∈ [m]. Given a ten-
sor C, and a vector y ∈ Rm, we define C(y) to be
the (m × m) matrix with components [C(y)]i,j =
EkE[m] Ci,j,kyk. Hence C can be interpreted as a
function C : Rm → R(mxm) that maps a vector
y ∈ Rm to a matrix C(y) of dimension (m × m).
In addition, we define the tensor C* ∈ R(mxmxm)
for any tensor C ∈ R(mxmxm) to have values
</bodyText>
<equation confidence="0.625652">
[C*]i,j,k = Ck,j,i
</equation>
<bodyText confidence="0.978683333333333">
Finally, for vectors x, y, z ∈ Rm, xyTzT is the
tensor D ∈ Rmxmxm where Dj,k,l = xjykzl (this
is analogous to the outer product: [xyT]j,k = xjyk).
</bodyText>
<sectionHeader confidence="0.942358" genericHeader="method">
4 L-PCFGs: Basic Definitions
</sectionHeader>
<bodyText confidence="0.997547333333333">
This section gives a definition of the L-PCFG for-
malism used in this paper. An L-PCFG is a 5-tuple
(N, I, P, m, n) where:
</bodyText>
<listItem confidence="0.999525727272727">
• N is the set of non-terminal symbols in the
grammar. I ⊂ N is a finite set of in-terminals.
P ⊂ N is a finite set of pre-terminals. We assume
that N = I ∪ P, and I ∩ P = ∅. Hence we have
partitioned the set of non-terminals into two subsets.
• [m] is the set of possible hidden states.
• [n] is the set of possible words.
• For all a ∈ I, b ∈ N, c ∈ N, h1, h2,h3 ∈ [m],
we have a context-free rule a(h1) → b(h2) c(h3).
• For all a ∈ P, h ∈ [m], x ∈ [n], we have a
context-free rule a(h) → x.
</listItem>
<bodyText confidence="0.599157444444444">
Hence each in-terminal a ∈ I is always the left-
hand-side of a binary rule a → b c; and each pre-
terminal a ∈ P is always the left-hand-side of a
rule a → x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We define the set of possible “skeletal rules” as
R = {a → b c : a ∈ I, b ∈ N, c ∈ N}. The
parameters of the model are as follows:
</bodyText>
<listItem confidence="0.958304333333333">
• For each a → b c ∈ R, and h ∈ [m], we have
a parameter q(a → b c|h, a). For each a ∈ P,
x ∈ [n], and h ∈ [m], we have a parameter
q(a → x|h, a). For each a → b c ∈ R, and
h, h′ ∈ [m], we have parameters s(h′|h, a → b c)
and t(h′|h, a → b c).
</listItem>
<bodyText confidence="0.9950425">
These definitions give a PCFG, with rule proba-
bilities
</bodyText>
<equation confidence="0.785002">
p(a(h1) → b(h2) c(h3)|a(h1)) =
q(a → b c|h1, a) × s(h2|h1, a → b c) × t(h3|h1, a → b c)
and p(a(h) → x|a(h)) = q(a → x|h, a).
</equation>
<bodyText confidence="0.999452916666667">
In addition, for each a ∈ I, for each h ∈ [m], we
have a parameter π(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG defines a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 ... rN where each ri is either of the form
a → b c or a → x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See figure 1 for an example.
A full tree consists of an s-tree r1 ... rN, together
with values h1 ... hN. Each hi is the value for
</bodyText>
<page confidence="0.963377">
224
</page>
<equation confidence="0.999841857142857">
r1 = S — *NP VP
r2 =NP—*DN
r3 = D —* the
r4 = N —* dog
r5=VP—*VP
r6 = V —* saw
r7 = P —* him
</equation>
<figureCaption confidence="0.9909735">
Figure 1: An s-tree, and its sequence of rules. (For con-
venience we have numbered the nodes in the tree.)
</figureCaption>
<bodyText confidence="0.9997362">
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
Define ai to be the non-terminal on the left-hand-
side of rule ri. For any i E 12 ... N} define pa(i)
to be the index of the rule above node i in the tree.
Define L C [N] to be the set of nodes in the tree
which are the left-child of some parent, and R C
[N] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
</bodyText>
<equation confidence="0.965228666666667">
p(r1 ... rN, h1 ... hN) = π(a1, h1)
rlq(ri|hi, ai) X s(hi|hpa(i), rpa(i))
iEL
t(hi|hpa(i), rpa(i)) (1)
The PMF over s-trees is p(r1 ... rN) =
Eh1...hN p(r1 ... rN, h1 ... hN).
</equation>
<bodyText confidence="0.990876">
In the remainder of this paper, we make use of ma-
trix form of parameters of an L-PCFG, as follows:
</bodyText>
<listItem confidence="0.8152995">
• For each a —* b c E R, we define Qa,b c E
Rmxm to be the matrix with values q(a —* b c|h, a)
for h = 1, 2,... m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a E P,
x E [n], we define Qa,x E Rmxm to be the matrix
with values q(a —* x|h, a) for h = 1, 2,... m on its
diagonal, and 0 values for its off-diagonal elements.
• For each a —* b c E R, we define Sa,b c E
Rmxm where [Sa,b c]h′,h = s(h′|h, a —* b c).
• For each a —* b c E R, we define Ta,b c E
Rmxm where [Ta,b c]h′,h = t(h′|h, a —* b c).
• For each a E Z, we define the vector πa E Rm
</listItem>
<bodyText confidence="0.596298">
where [πa]h = π(a, h).
</bodyText>
<sectionHeader confidence="0.9894925" genericHeader="method">
5 Tensor Form of the Inside-Outside
Algorithm
</sectionHeader>
<bodyText confidence="0.7783275">
Given an L-PCFG, two calculations are central:
Inputs: s-tree r1 ... rN, L-PCFG (N, Z, P, m, n), parameters
</bodyText>
<listItem confidence="0.990193333333333">
• Ca→b c E R(m×m×m) for all a → b c E R
•
c∞a→x E R(1×m) for all a E P, x E [n]
• c1a E R(m×1) for all a E Z.
Algorithm: (calculate the fi terms bottom-up in the tree)
• For all i E [N] such that ai E P, fi = c∞ri
• For all i E [N] such that ai E Z, fi = fγCri(fβ) where
0 is the index of the left child of node i in the tree, and -y
is the index of the right child.
</listItem>
<figure confidence="0.592637">
Return: f1c1a1 = p(r1 ... rN)
</figure>
<figureCaption confidence="0.992174">
Figure 2: The tensor form for calculation of p(r� ... rN).
</figureCaption>
<listItem confidence="0.9913545">
1. For a given s-tree r1 ... rN, calculate
p(r1 ... rN).
2. For a given input sentence x = x1 ... xN, cal-
culate the marginal probabilities
</listItem>
<equation confidence="0.9938345">
µ(a,i,j) = � p(τ)
τET (x):(a,i,j)Eτ
</equation>
<bodyText confidence="0.990725666666667">
for each non-terminal a E N, for each (i, j)
such that 1 G i G j G N.
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) E τ if non-
terminal a spans words xi ... xj in the parse tree τ.
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 ... xN, the parsing algorithm of Goodman (1996)
can be used to find
</bodyText>
<equation confidence="0.824401">
�arg max µ(a, i, j)
τET (x)
</equation>
<bodyText confidence="0.981759636363637">
This is the parsing algorithm used by Petrov et al.
(2006), for example. In addition, we can calcu-
late the probability for an input sentence, p(x) =
EτET (x) p(τ), as p(x) = EaEZ µ(a,1,N).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the first step in deriving the spectral estimation
method.
The algorithms are shown in figures 2 and 3. Each
algorithm takes the following inputs:
</bodyText>
<listItem confidence="0.990669666666667">
1. A tensor Ca,b c E R(mxmxm) for each rule
a — *b c.
2. A vector c∞a,x E R(1xm) for each rule a —* x.
</listItem>
<figure confidence="0.535637666666667">
S1
NP2 VP5
P7
saw him
D3
the
N4
dog
V6
X
rlN
i=1
rlX
iER
(a,i,j)Eτ
</figure>
<page confidence="0.973485">
225
</page>
<listItem confidence="0.730423">
3. A vector c1a E R(mx1) for each a E Z.
</listItem>
<bodyText confidence="0.9878225">
The following theorem gives conditions under
which the algorithms are correct:
</bodyText>
<construct confidence="0.731736">
Theorem 1 Assume that we have an L-PCFG with
parameters Qa-4x, Qa-4b c, Ta-4b c, Sa-4b c, πa, and
that there exist matrices Ga E R(mxm) for all a E
N such that each Ga is invertible, and such that:
</construct>
<listItem confidence="0.7103325">
1. For all rules a -* b c, Ca-4b c(y) =
GcTa-4b cdiag(yGbSa-4b c)Qa-4b c(Ga)−1
2. For all rules a -* x, cax = 1TQa-4x(Ga)−1
3. For all a E Z, c1a = Gaπa
</listItem>
<construct confidence="0.89343725">
Then: 1) The algorithm in figure 2 correctly com-
putes p(r1 ... rN) under the L-PCFG. 2) The algo-
rithm in figure 3 correctly computes the marginals
µ(a, i, j) under the L-PCFG.
</construct>
<bodyText confidence="0.410796">
Proof: See section 9.1.
</bodyText>
<sectionHeader confidence="0.99278" genericHeader="method">
6 Estimating the Tensor Model
</sectionHeader>
<bodyText confidence="0.999862428571428">
A crucial result is that it is possible to directly esti-
mate parameters Ca-4b c, cax and c1a that satisfy the
conditions in theorem 1, from a training sample con-
sisting of s-trees (i.e., trees where hidden variables
are unobserved). We first describe random variables
underlying the approach, then describe observable
representations based on these random variables.
</bodyText>
<subsectionHeader confidence="0.999092">
6.1 Random Variables Underlying the Approach
</subsectionHeader>
<bodyText confidence="0.999864785714286">
Each s-tree with N rules r1 ... rN has N nodes. We
will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in figure 1 has the rule NP -* D N.
If the rule at a node is of the form a -* b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule pro-
duction, of the form a -* x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
</bodyText>
<figure confidence="0.7946">
S
</figure>
<figureCaption confidence="0.9979875">
Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms µ(a, i, j).
</figureCaption>
<bodyText confidence="0.9997932">
The outside tree contains everything in the s-tree
r1 ... rN, excluding the subtree below node i.
Our random variables are defined as follows.
First, we select a random internal node, from a ran-
dom tree, as follows:
</bodyText>
<listItem confidence="0.974500083333333">
• Sample an s-tree r1 ... rN from the PMF
p(r1 ... rN). Choose a node i uniformly at ran-
dom from [N].
If the rule ri for the node i is of the form a -* b c,
we define random variables as follows:
• R1 is equal to the rule ri (e.g., NP -* D N).
• T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
• H1, H2, H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.
</listItem>
<figure confidence="0.940022931034483">
Inputs: Sentence x1 ... xN, L-PCFG (N, Z, P, m, n), param-
eters Ca→b c E R(m×m×m) for all a → b c E R, c∞a→x E
R(1×m) for all a E P, x E [n], c1a E R(m×1) for all a E Z.
Data structures:
• Each αa,i,j E R1×m for a E N, 1 &lt; i &lt; j &lt; N is a
row vector of inside terms.
• Each βa,i,j E Rm×1 for a E N, 1 &lt; i &lt; j &lt; N is a
column vector of outside terms.
• Each µ(a, i, j) E R for a E N, 1 &lt; i &lt; j &lt; N is a
marginal probability.
Algorithm:
(Inside base case) Va E P, i E [N], αa,i,i = c∞a→xj
(Inside recursion) Va E Z, 1 &lt; i &lt; j &lt; N,
αa,i,j = j−1 X X αc,k+1,jCa→b c(αb,i,k)
k=i a→b c
(Outside base case) Va E Z, βa,1,n = c1a
(Outside recursion) Va E N, 1 &lt; i &lt; j &lt; N,
βa,i,j = Xi − 1 X Cb→c a(αc,k,i−1)βb,k,j
k=1 b→c a
N X Cb→a c
+ X b→a c ∗ (αc,j+1,k)βb,i,k
k=j+1
(Marginals/) Va E N, 1 &lt; i &lt; j &lt; N, αa,i,j
µ(a i, j) = αa,i,jβa,i,j = h βa,i,j
h∈[m] h
NP VP
V
saw him
P
</figure>
<page confidence="0.984404">
226
</page>
<listItem confidence="0.989249833333333">
• A1, A2, A3 are the labels for node i, the left
child of node i, and the right child of node i respec-
tively. (E.g., A1 = NP, A2 = D, A3 = N.)
• O is the outside tree at node i.
• B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
</listItem>
<bodyText confidence="0.99689">
If the rule ri for the selected node i is of
the form a —* x, we have random vari-
ables R1, T1, H1, A1, O, B as defined above, but
H2, H3, T2, T3, A2, and A3 are not defined.
We assume a function ψ that maps outside trees o
to feature vectors ψ(o) E Rd′. For example, the fea-
ture vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function φ
that maps inside trees t to feature vectors φ(t) E Rd.
As one example, the function φ might be an indica-
tor function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good definitions of ψ(o) of φ(t). One
requirement is that d′ &gt; m and d &gt; m.
In tandem with these definitions, we assume pro-
jection matices Ua E R(d×m) and V a E R(d′×m)
for all a E N. We then define additional random
variables Y1, Y2, Y3, Z as
</bodyText>
<equation confidence="0.9997285">
Y1 = (Ua1)⊤φ(T1) Z = (V a1)⊤ψ(O)
Y2 = (Ua2)⊤φ(T2) Y3 = (Ua3)⊤φ(T3)
</equation>
<bodyText confidence="0.9999785">
where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
</bodyText>
<subsectionHeader confidence="0.99879">
6.2 Observable Representations
</subsectionHeader>
<bodyText confidence="0.998147">
Given the definitions in the previous section, our
representation is based on the following matrix, ten-
sor and vector quantities, defined for all a E N, for
all rules of the form a —* b c, and for all rules of the
form a —* x respectively:
</bodyText>
<equation confidence="0.9056212">
Ea = E[Y1Z⊤|A1 = a]
[ ]
Da→b c = E [[R1 = a —*b c]]Y3Z⊤Y 2 ⊤|A1 = a
[ ]
d∞ a→x = E [[R1 = a —* x]]Z⊤|A1 = a
</equation>
<bodyText confidence="0.977021">
Assuming access to functions φ and ψ, and projec-
tion matrices Ua and V a, these quantities can be es-
timated directly from training data consisting of a
set of s-trees (see section 7).
Our observable representation then consists of:
</bodyText>
<equation confidence="0.985856666666667">
Ca→b c(y) = Da→b c(y)(Ea)−1
c∞a→x = d∞a→x(Ea)−1
c1a = E [[[A1 = a]]Y1|B = 1]
</equation>
<bodyText confidence="0.8742368">
We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following definition will be important:
Definition 2 For all a E N, we define the matrices
Ia E R(d×m) and Ja E R(d′×m) as
</bodyText>
<equation confidence="0.9991565">
[Ia]i,h = E[φi(T1)  |H1 = h, A1 = a]
[Ja]i,h = E[ψi(O)  |H1 = h, A1 = a]
</equation>
<bodyText confidence="0.999445357142857">
In addition, for any a E N, we use γa E Rm to
denote the vector with γah = P(H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisfied (these are
parallel to conditions 1 and 2 in Hsu et al. (2009)):
Condition 1 da E N, the matrices Ia and Ja are
offull rank (i.e., they have rank m). For all a E N,
for all h E [m], γah &gt; 0.
Condition 2 da E N, the matrices Ua E R(d×m)
and V a E R(d′×m) are such that the matrices Ga =
(Ua)⊤Ia and Ka = (V a)⊤Ja are invertible.
The following lemma justifies the use of an SVD
calculation as one method for finding values for Ua
and V a that satisfy condition 2:
</bodyText>
<construct confidence="0.953571">
Lemma 1 Assume that condition 1 holds, and for
all a E N define
</construct>
<equation confidence="0.9737335">
Qa = E[φ(T1) (ψ(O))⊤ |A1 = a] (5)
Then if Ua is a matrix of the m left singular vec-
</equation>
<bodyText confidence="0.992788272727273">
tors of Qa corresponding to non-zero singular val-
ues, and V a is a matrix of the m right singular vec-
tors of Qa corresponding to non-zero singular val-
ues, then condition 2 is satisfied.
Proof sketch: It can be shown that Qa =
Iadiag(γa)(Ja)⊤. The remainder is similar to the
proof of lemma 2 in Hsu et al. (2009).
The matrices Qa can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions φ and ψ.
We can now state the following theorem:
</bodyText>
<page confidence="0.988015">
227
</page>
<figure confidence="0.938598666666667">
Theorem 2 Assume conditions 1 and 2 are satisfied.
For all a E N, define Ga = (Ua)⊤Ia. Then under
the definitions in Eqs. 2-4:
1. For all rules a —* b c, Ca→b c(y) =
GcTa→b cdiag(yGbSa→b c)Qa→b c(Ga)−1
a
</figure>
<listItem confidence="0.7765525">
2. For all rules a —* x, c X = 1⊤Qal→x(Ga)−1
3. For all a E N, c1a = Gaπa
</listItem>
<bodyText confidence="0.615967">
Proof: The following identities hold (see sec-
tion 9.2):
</bodyText>
<equation confidence="0.9919212">
Da—b c(y) = (6)
GcTa—b cdiag(yGbSa—b c)Qa—b cdiag(,Ya)(Ka)T
da—x = 1TQa—xdiag(,Ya)(Ka)T (7)
Ea = Gadiag(,Ya)(Ka)T (8)
c�a = Ga7a (9)
</equation>
<bodyText confidence="0.871879666666667">
Under conditions 1 and 2, Ea is invertible, and
(Ea)−1 = ((Ka)⊤)−1(diag(γa))−1(Ga)−1. The
identities in the theorem follow immediately.
</bodyText>
<sectionHeader confidence="0.949255" genericHeader="method">
7 Deriving Empirical Estimates
</sectionHeader>
<bodyText confidence="0.934854666666667">
Figure 4 shows an algorithm that derives esti-
mates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i E [M].
These tuples can be derived from a training set
consisting of s-trees τ1 ... τM as follows:
</bodyText>
<listItem confidence="0.963584">
• di E [M], choose a single node ji uniformly at
random from the nodes in τi. Define r(i,1) to be the
</listItem>
<bodyText confidence="0.71506425">
rule at node ji. t(i,1) is the inside tree rooted at node
ji. If r(i,1) is of the form a —* b c, then t(i,2) is the
inside tree under the left child of node ji, and t(i,3)
is the inside tree under the right child of node ji. If
r(i,1) is of the form a —* x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji. b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
</bodyText>
<equation confidence="0.945691333333333">
τ1 ... τM are i.i.d. draws from the distribution
p(τ) over s-trees under an L-PCFG, the tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d. draws
</equation>
<bodyText confidence="0.991739">
from the joint distribution over the random variables
R1, T1, T2, T3, O, B defined in the previous section.
The algorithm first computes estimates of the pro-
jection matrices Ua and V a: following lemma 1,
this is done by first deriving estimates of Qa,
and then taking SVDs of each Qa. The matrices
are then used to project inside and outside trees
t(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-
tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used to
derive the estimates of Ca→b c, c∞a→x, and c1a.
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of definitions:
</bodyText>
<listItem confidence="0.948816">
• A is the minimum absolute value of any element
of the vectors/matrices/tensors c1a, d∞a→x, Da→b c,
(Ea)−1. (Note that A is a function of the projec-
tion matrices Ua and V a as well as the underlying
L-PCFG.)
• For each a E N, σa is the value of the m’th
</listItem>
<bodyText confidence="0.962111681818182">
largest singular value of Qa. Define σ = mina σa.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in figure 4 are i.i.d. draws from the joint distribution
over the random variables R1, T1, T2, T3, O, B, un-
der an L-PCFG with distribution p(r1 ... rN) over
s-trees. Define m to be the number of latent states
in the L-PCFG. Assume that the algorithm in fig-
ure 4 has projection matrices Ua and V� a derived as
left and right singular vectors of Qa, as defined in
Eq. S. Assume that the L-PCFG, together with Ua
and V�a, has coefficients A &gt; 0 and σ &gt; 0. In addi-
tion, assume that all elements in c1a, d∞a→x, Da→b c,
and Ea are in [−1, +1]. For any s-tree r1 ... rN de-
fine 13(r1 ... rN) to be the value calculated by the
algorithm in figure 3 with inputs c1a, c∞a→x, �Ca→b c
derived from the algorithm in figure 4. Define R to
be the total number of rules in the grammar of the
form a —* b c or a —* x. Define Ma to be the num-
ber of training examples in the input to the algorithm
in figure 4 where ri,1 has non-terminal a on its left-
hand-side. Under these assumptions, iffor all a
</bodyText>
<equation confidence="0.992549">
13(r1 ... rN)
p(r1 ... rN)
</equation>
<bodyText confidence="0.93241">
A similar theorem (omitted for space) states that
��� G 1 + ǫ for the marginals.
The condition that Ua and V� a are derived from
Qa, as opposed to the sample estimate 0a, follows
Foster et al. (2012). As these authors note, similar
techniques to those of Hsu et al. (2009) should be
</bodyText>
<equation confidence="0.429430285714286">
M &gt; 128m2 log C2mR1
Ma ( 2N+�1 + ǫ − 1�2 A2σ4 δ
Then
1 − ǫ G i
i G 1 + ǫ
�� µ(a,i,j)
1 − ǫ G � µ(a,i,j)
</equation>
<page confidence="0.974802">
228
</page>
<bodyText confidence="0.97006975">
applicable in deriving results for the case where Qa
Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i E {1 ... M}, where r(i,1) is a context free rule; t(i,1),
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
φ that maps inside trees t to feature-vectors φ(t) E Rd. A func-
tion ψ that maps outside trees o to feature-vectors ψ(o) E Rd′.
Algorithm:
Define ai to be the non-terminal on the left-hand side of rule
r(i,1). If r(i,1) is of the form a → b c, define bi to be the non-
terminal for the left-child of r(i,1), and ci to be the non-terminal
for the right-child.
</bodyText>
<figure confidence="0.986504727272727">
(Step 0: Singular Value Decompositions)
• Use the algorithm in figure 5 to calculate matrices ˆUa E
R(dxm) and Vˆa E R(d′xm) for each a E N.
(Step 1: Projection)
• For all i E [M], compute y(i,1) = (ˆUai)Tφ(t(i,1)).
• For all i E [M] such that r(i,1) is off the form
a → b c, compute y(i,2) = (ˆUbi )T φ(t(i,2)) and y(i,3) =
(ˆUci)T φ(t(i,3)). —
• For all i E [M], compute z(i) = (Vˆai)Tψ(o(i)).
(Step 2: Calculate Correlations)
• For each a E N, define δa = 1/ PMi=1[[ai = a]]
• For each rule a → b c, compute ˆDa—b c = δa X
PMi=1[[r(i,1) = a → b c]]y(i,3)(z(i))T(y(i,2))T
• For each rule a → x, compute ˆd�a—x = δa X
PMi=1[[r(i,1) = a → x]](z(i))T
• For each a E N, compute ˆΣa = δa X
PMi=1[[ai = a]]y(i,1)(z(i))T
(Step 3: Compute Final Parameters)
• For all a → b c, ˆCa—b c(y) = ˆDa—b c(y)(ˆΣa)−1
• For all a → x, ˆca—x = ˆda—x( ˆΣa)−1
• For all a E Z ˆc1 — FMi=1[[ai=a and b(i)=1]]y(i,1)
a — EMi=1[[b(i)=1]]
</figure>
<figureCaption confidence="0.998905">
Figure 4: The spectral learning algorithm.
</figureCaption>
<figure confidence="0.505160333333333">
Inputs: Identical to algorithm in figure 4.
Algorithm:
• For each a E N, compute ˆΩa E R(d′xd) as
</figure>
<equation confidence="0.836437333333333">
PM i=1[[ai = a]]φ(t(i,1))(ψ(o(i)))�
ˆΩa =
PMi=1[[ai = a]]
</equation>
<bodyText confidence="0.8167068">
and calculate a singular value decomposition of ˆΩa.
• For each a E N, define ˆUa E Rmxd to be a matrix of the left
singular vectors of ˆΩa corresponding to the m largest singular
values. Define Vˆa E Rmxd′ to be a matrix of the right singular
vectors of ˆΩa corresponding to the m largest singular values.
</bodyText>
<figureCaption confidence="0.998083">
Figure 5: Singular value decompositions.
</figureCaption>
<bodyText confidence="0.894662571428571">
is used in place of Qa.
Proofsketch: The proof is similar to that of Foster
et al. (2012). The basic idea is to first show that
under the assumptions of the theorem, the estimates
�1a, �d∞a→x, �Da→b c, Ea are all close to the underlying
values being estimated. The second step is to show
that this ensures that ˆp(r1...rN&apos;) is close to 1.
</bodyText>
<equation confidence="0.909752">
p(r1 ... rN� )
</equation>
<bodyText confidence="0.9956598">
The method described of selecting a single tuple
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-
sures that the samples are i.i.d., and simplifies the
analysis underlying theorem 3. In practice, an im-
plementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p� to p.
</bodyText>
<sectionHeader confidence="0.999491" genericHeader="method">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999960357142857">
There are several potential applications of the
method. The most obvious is parsing with L-
PCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for ex-
ample in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as spe-
cial case of our approach, by converting tagged se-
quences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of Qa; these singular
values are a measure of how well correlated ψ and
φ are with the unobserved hidden variable H1. Ex-
perimental work is required to find a good choice of
values for ψ and φ for parsing.
</bodyText>
<sectionHeader confidence="0.990601" genericHeader="conclusions">
9 Proofs
</sectionHeader>
<bodyText confidence="0.99994975">
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; in-
stead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
</bodyText>
<subsectionHeader confidence="0.998053">
9.1 Proof of Theorem 1
</subsectionHeader>
<bodyText confidence="0.999322">
First, the following lemma leads directly to the cor-
rectness of the algorithm in figure 2:
</bodyText>
<footnote confidence="0.74382275">
1Parameters can be estimated using the algorithm in
figure 4; for a test sentence x1 ... xN we can first
use the algorithm in figure 3 to calculate marginals
µ(a, i, j), then use the algorithm of Goodman (1996) to find
</footnote>
<sectionHeader confidence="0.714051" genericHeader="acknowledgments">
P
</sectionHeader>
<bodyText confidence="0.554205">
arg maxτET (x) (a,i,j)Eτ µ(a, i, j).
</bodyText>
<page confidence="0.99496">
229
</page>
<bodyText confidence="0.8156235">
Lemma 3 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 3 is a sentence x1 ... xN. For any a ∈ N, for
any 1 ≤ i ≤ j ≤ N,define �αa,i,j ∈ R(1xm) to have
</bodyText>
<figure confidence="0.479914692307692">
components �αa,i,j
h = p(xi ... xj|h, a) for h ∈ [m].
In addition, define �βa,i,j ∈ R(mx1) to have compo-
�βa,i,j
h = p(x1 ... xi−1, a(h), xj+1 ... xN) for
h ∈ [m]. Then for all i ∈ [N], αa,i,j = aa,i,j(Ga)−1
and βa,i,j = Ga (ja,i,j. It follows that for all (a, i, j),
µ(a, i, j) = �αa,i,j(Ga)−1Ga�βa,i,j = aa,i,j�βa,i,j
nents
X= �αa,i,j X p(τ)
h h 0a,i,j =
h
τET (x):(a,i,j)Eτ
</figure>
<bodyText confidence="0.997974333333333">
Thus the vectors αa,i,j and βa,i,j are linearly re-
lated to the vectors aa,i,j and �βa,i,j, which are the
inside and outside terms calculated by the conven-
tional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.
</bodyText>
<subsectionHeader confidence="0.545695">
9.2 Proof of the Identity in Eq. 6
</subsectionHeader>
<bodyText confidence="0.99998725">
We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be verified:
</bodyText>
<equation confidence="0.9951354">
P(R1 = a → b c|H1 = h, A1 = a) = q(a → b c|h, a)
E[Y3,j|H1 = h,R1 = a → b c] = Ea-4b c
j,h
E[Zk|H1 = h,R1 = a → b c] = Kak,h
E[Y2,l|H1 = h,R1 = a → b c] = Fa-4b c
</equation>
<bodyText confidence="0.9318528">
l,h
where Ea-4
Y3, Z and Y2 are independent when conditioned
on H1, R1 (this follows from the independence as-
sumptions in the L-PCFG), hence
</bodyText>
<figure confidence="0.906105090909091">
E [[[R1 = a → b c]]Y3,jZkY2,l  |H1 = h, A1 = a]
= q(a → b c|h, a)Ej h b cKak,hFl, h b c
Hence (recall that γah = P(H1 = h|A1 = a)),
Da-4b c
j,k,l = E [[[R1 = a → b c]]Y3,jZkY2,l  |A1 = a]
X= γahE [[[R1 = a → b c]]Y3,jZkY2,l  |H1 = h, A1 = a]
h
X= γahq(a → b c|h, a)Ejah b cKak,hFl h b c (11)
h
from which Eq. 6 follows.
b c = GcTa-4b c, Fa-4b c = GbSa-4b c.
</figure>
<construct confidence="0.826855">
Lemma 2 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
</construct>
<bodyText confidence="0.950741125">
figure 2 is an s-tree r1 ... rN. Define ai for i ∈ [N]
to be the non-terminal on the left-hand-side of rule
ri, and ti for i ∈ [N] to be the s-tree with rule ri
at its root. Finally, for all i ∈ [N], define the row
vector bi ∈ R(1xm) to have components
bih = P(Ti = ti|Hi = h,Ai = ai)
for h ∈ [m]. Then for all i ∈ [N], fi = bi(G(ai))−1.
It follows immediately that
</bodyText>
<equation confidence="0.94429">
f1c1a1 = b1(G(a1))−1Ga1πa1 = p(r1 ... rN)
</equation>
<bodyText confidence="0.987653533333333">
This lemma shows a direct link between the vec-
tors fi calculated in the algorithm, and the terms bih,
which are terms calculated by the conventional in-
side algorithm: each fi is a linear transformation
(through Gai) of the corresponding vector bi.
Proof: The proof is by induction.
First consider the base case. For any leaf—i.e., for
any i such that ai ∈ P—we have bih = q(ri|h, ai),
and it is easily verified that fi = bi(G(ai))−1.
The inductive case is as follows. For all i ∈ [N]
such that ai ∈ I, by the definition in the algorithm,
fi = fγCri(fβ)
= fγGaγTridiag(fβGaβSri)Qri(Gai)−1
Assuming by induction that fγ = bγ(G(aγ))−1 and
fβ = bβ(G(aβ))−1, this simplifies to
</bodyText>
<equation confidence="0.667501">
fi = κrdiag(κl)Qri(Gai)−1 (10)
</equation>
<bodyText confidence="0.9203295">
where κr = bγTri, and κl = bβSri. κr is a row
vector with components κrh = Ph′E[m] bγ h′T ri
h′,h =
Ph′E[m] bγh′t(h′|h, ri). Similarly, κl is a row vector
with components equal to κlh = P h′E[m] bβ h′Sri h′,h =
Ph′E[m] bβh′s(h′|h, ri). It can then be verified that
κrdiag(κl)Qri is a row vector with components
equal to κrhκlhq(ri|h, ai).
</bodyText>
<equation confidence="0.915767">
~P �
But bi h = q(ri|h, ai)× h′E[m] bγ h′t(h′|h, ri) ×
~P �
h′E[m] bβ h′s(h′|h, ri) = q(ri|h, ai)κrhκlh, hence
</equation>
<bodyText confidence="0.99801675">
κrdiag(κl)Qri = bi and the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in figure 3:
</bodyText>
<page confidence="0.995232">
230
</page>
<reference confidence="0.994364846153846">
Acknowledgements: Columbia University gratefully ac-
knowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows Project.
Dean Foster was supported by National Science Founda-
tion grant 1106743.
</reference>
<sectionHeader confidence="0.909907" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987042">
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings ofFOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Associ-
ation for Computational Linguistics, pages 177–183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings ofEACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 75–82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128–135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433–440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algo-
rithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artificial Intelligence, pages
261–268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841–860.
</reference>
<page confidence="0.997928">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.887999">
<title confidence="0.999798">Spectral Learning of Latent-Variable PCFGs</title>
<author confidence="0.996647">B Karl Michael Dean P</author>
<author confidence="0.996647">Lyle</author>
<affiliation confidence="0.9561095">of Computer Science, Columbia University of of Computer and Information Science, University of Pennsylvania</affiliation>
<email confidence="0.999955">foster@wharton.upenn.edu,ungar@cis.upenn.edu</email>
<abstract confidence="0.995668666666667">We introduce a spectral learning algorithm for latent-variable PCFGs (Petrov et al., 2006). Under a separability (singular value) condition, we prove that the method provides consistent parameter estimates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Acknowledgements: Columbia University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government. Shay Cohen was supported by the National Science Foundation under Grant #1136996 to the Computing Research Association for the CIFellows Project. Dean Foster was supported by National Science Foundation grant 1106743.</title>
<marker></marker>
<rawString>Acknowledgements: Columbia University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government. Shay Cohen was supported by the National Science Foundation under Grant #1136996 to the Computing Research Association for the CIFellows Project. Dean Foster was supported by National Science Foundation grant 1106743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Balle</author>
<author>A Quattoni</author>
<author>X Carreras</author>
</authors>
<title>A spectral learning algorithm for finite state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="5323" citStr="Balle et al. (2011)" startWordPosition="830" endWordPosition="833"> for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2,... n}. For any row or column vector y ∈ Rm, we use diag(y) to refer to the (m × m) matrix with diagonal elements equal to yh for h = 1... m, and off-diagonal elements equal to 0. For any statement Γ, we use</context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2011</marker>
<rawString>B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dasgupta</author>
</authors>
<title>Learning mixtures of Gaussians.</title>
<date>1999</date>
<booktitle>In Proceedings ofFOCS.</booktitle>
<contexts>
<context position="1435" citStr="Dasgupta, 1999" startWordPosition="197" endWordPosition="198">t is simple, it is often relatively efficient, and it has well understood formal properties. It does, however, have a major limitation: it has no guarantee of finding the global optimum of the likelihood function. From a theoretical perspective, this means that the EM algorithm is not guaranteed to give consistent parameter estimates. From a practical perspective, problems with local optima can be difficult to deal with. Recent work has introduced polynomial-time learning algorithms (and consistent estimation methods) for two important cases of hidden-variable models: Gaussian mixture models (Dasgupta, 1999; Vempala and Wang, 2004) and hidden Markov models (Hsu et al., 2009). These algorithms use spectral methods: that is, algorithms based on eigenvector decompositions of linear systems, in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods ar</context>
</contexts>
<marker>Dasgupta, 1999</marker>
<rawString>S. Dasgupta. 1999. Learning mixtures of Gaussians. In Proceedings ofFOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dean P Foster</author>
<author>Jordan Rodu</author>
<author>Lyle H Ungar</author>
</authors>
<title>Spectral dimensionality reduction for hmms.</title>
<date>2012</date>
<pages>1203--6130</pages>
<contexts>
<context position="5136" citStr="Foster et al., 2012" startWordPosition="800" endWordPosition="804">012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2,... n}. For any row </context>
<context position="23189" citStr="Foster et al. (2012)" startWordPosition="4546" endWordPosition="4549">o be the value calculated by the algorithm in figure 3 with inputs c1a, c∞a→x, �Ca→b c derived from the algorithm in figure 4. Define R to be the total number of rules in the grammar of the form a —* b c or a —* x. Define Ma to be the number of training examples in the input to the algorithm in figure 4 where ri,1 has non-terminal a on its lefthand-side. Under these assumptions, iffor all a 13(r1 ... rN) p(r1 ... rN) A similar theorem (omitted for space) states that ��� G 1 + ǫ for the marginals. The condition that Ua and V� a are derived from Qa, as opposed to the sample estimate 0a, follows Foster et al. (2012). As these authors note, similar techniques to those of Hsu et al. (2009) should be M &gt; 128m2 log C2mR1 Ma ( 2N+�1 + ǫ − 1�2 A2σ4 δ Then 1 − ǫ G i i G 1 + ǫ �� µ(a,i,j) 1 − ǫ G � µ(a,i,j) 228 applicable in deriving results for the case where Qa Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i E {1 ... M}, where r(i,1) is a context free rule; t(i,1), t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) E Rd. A function ψ that maps outsid</context>
<context position="25608" citStr="Foster et al. (2012)" startWordPosition="5028" endWordPosition="5031">ectral learning algorithm. Inputs: Identical to algorithm in figure 4. Algorithm: • For each a E N, compute ˆΩa E R(d′xd) as PM i=1[[ai = a]]φ(t(i,1))(ψ(o(i)))� ˆΩa = PMi=1[[ai = a]] and calculate a singular value decomposition of ˆΩa. • For each a E N, define ˆUa E Rmxd to be a matrix of the left singular vectors of ˆΩa corresponding to the m largest singular values. Define Vˆa E Rmxd′ to be a matrix of the right singular vectors of ˆΩa corresponding to the m largest singular values. Figure 5: Singular value decompositions. is used in place of Qa. Proofsketch: The proof is similar to that of Foster et al. (2012). The basic idea is to first show that under the assumptions of the theorem, the estimates �1a, �d∞a→x, �Da→b c, Ea are all close to the underlying values being estimated. The second step is to show that this ensures that ˆp(r1...rN&apos;) is close to 1. p(r1 ... rN� ) The method described of selecting a single tuple (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree ensures that the samples are i.i.d., and simplifies the analysis underlying theorem 3. In practice, an implementation should most likely use all nodes in all trees in training data; by Rao-Blackwellization we know such an alg</context>
</contexts>
<marker>Foster, Rodu, Ungar, 2012</marker>
<rawString>Dean P. Foster, Jordan Rodu, and Lyle H. Ungar. 2012. Spectral dimensionality reduction for hmms. arXiv:1203.6130v1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing algorithms and metrics.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>177--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11502" citStr="Goodman (1996)" startWordPosition="2176" endWordPosition="2177"> Figure 2: The tensor form for calculation of p(r� ... rN). 1. For a given s-tree r1 ... rN, calculate p(r1 ... rN). 2. For a given input sentence x = x1 ... xN, calculate the marginal probabilities µ(a,i,j) = � p(τ) τET (x):(a,i,j)Eτ for each non-terminal a E N, for each (i, j) such that 1 G i G j G N. Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) E τ if nonterminal a spans words xi ... xj in the parse tree τ. The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 ... xN, the parsing algorithm of Goodman (1996) can be used to find �arg max µ(a, i, j) τET (x) This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for an input sentence, p(x) = EτET (x) p(τ), as p(x) = EaEZ µ(a,1,N). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 1. A tensor Ca,b c E R(mxmxm) for each rule a — *b c. 2. A </context>
<context position="27599" citStr="Goodman (1996)" startWordPosition="5380" endWordPosition="5381">ntal work is required to find a good choice of values for ψ and φ for parsing. 9 Proofs This section gives proofs of theorems 1 and 2. Due to space limitations we cannot give full proofs; instead we provide proofs of some key lemmas. A long version of this paper will give the full proofs. 9.1 Proof of Theorem 1 First, the following lemma leads directly to the correctness of the algorithm in figure 2: 1Parameters can be estimated using the algorithm in figure 4; for a test sentence x1 ... xN we can first use the algorithm in figure 3 to calculate marginals µ(a, i, j), then use the algorithm of Goodman (1996) to find P arg maxτET (x) (a,i,j)Eτ µ(a, i, j). 229 Lemma 3 Assume that conditions 1-3 of theorem 1 are satisfied, and that the input to the algorithm in figure 3 is a sentence x1 ... xN. For any a ∈ N, for any 1 ≤ i ≤ j ≤ N,define �αa,i,j ∈ R(1xm) to have components �αa,i,j h = p(xi ... xj|h, a) for h ∈ [m]. In addition, define �βa,i,j ∈ R(mx1) to have compo�βa,i,j h = p(x1 ... xi−1, a(h), xj+1 ... xN) for h ∈ [m]. Then for all i ∈ [N], αa,i,j = aa,i,j(Ga)−1 and βa,i,j = Ga (ja,i,j. It follows that for all (a, i, j), µ(a, i, j) = �αa,i,j(Ga)−1Ga�βa,i,j = aa,i,j�βa,i,j nents X= �αa,i,j X p(τ) </context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing algorithms and metrics. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 177–183. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>T Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="1504" citStr="Hsu et al., 2009" startWordPosition="208" endWordPosition="211">erstood formal properties. It does, however, have a major limitation: it has no guarantee of finding the global optimum of the likelihood function. From a theoretical perspective, this means that the EM algorithm is not guaranteed to give consistent parameter estimates. From a practical perspective, problems with local optima can be difficult to deal with. Recent work has introduced polynomial-time learning algorithms (and consistent estimation methods) for two important cases of hidden-variable models: Gaussian mixture models (Dasgupta, 1999; Vempala and Wang, 2004) and hidden Markov models (Hsu et al., 2009). These algorithms use spectral methods: that is, algorithms based on eigenvector decompositions of linear systems, in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent </context>
<context position="3697" citStr="Hsu et al. (2009)" startWordPosition="560" endWordPosition="563">gorithm (figures 2 and 3) can be used to calculate probabilities and marginals of interest. Our method depends on the following results: • Tensor form of the inside-outside algorithm. Section 5 shows that the inside-outside algorithm for L-PCFGs can be written using tensors. Theorem 1 gives conditions under which the tensor form calculates inside and outside terms correctly. • Observable representations. Section 6 shows that under a singular-value condition, there is an observable form for the tensors required by the insideoutside algorithm. By an observable form, we follow the terminology of Hsu et al. (2009) in referring to quantities that can be estimated directly from data where values for latent variables are unobserved. Theorem 2 shows that tensors derived from the observable form satisfy the conditions of theorem 1. • Estimating the model. Section 7 gives an algorithm for estimating parameters of the observable representation from training data. Theorem 3 gives a sample complexity result, showing that the estimates � converge to the true distribution at a rate of 1/ M where M is the number of training examples. The algorithm is strikingly different from the EM algorithm for L-PCFGs, both in </context>
<context position="5115" citStr="Hsu et al., 2009" startWordPosition="796" endWordPosition="799">-14 July 2012. c�2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2</context>
<context position="18300" citStr="Hsu et al. (2009)" startWordPosition="3591" endWordPosition="3594">Ca→b c(y) = Da→b c(y)(Ea)−1 c∞a→x = d∞a→x(Ea)−1 c1a = E [[[A1 = a]]Y1|B = 1] We next introduce conditions under which these quantities satisfy the conditions in theorem 1. The following definition will be important: Definition 2 For all a E N, we define the matrices Ia E R(d×m) and Ja E R(d′×m) as [Ia]i,h = E[φi(T1) |H1 = h, A1 = a] [Ja]i,h = E[ψi(O) |H1 = h, A1 = a] In addition, for any a E N, we use γa E Rm to denote the vector with γah = P(H1 = h|A1 = a). The correctness of the representation will rely on the following conditions being satisfied (these are parallel to conditions 1 and 2 in Hsu et al. (2009)): Condition 1 da E N, the matrices Ia and Ja are offull rank (i.e., they have rank m). For all a E N, for all h E [m], γah &gt; 0. Condition 2 da E N, the matrices Ua E R(d×m) and V a E R(d′×m) are such that the matrices Ga = (Ua)⊤Ia and Ka = (V a)⊤Ja are invertible. The following lemma justifies the use of an SVD calculation as one method for finding values for Ua and V a that satisfy condition 2: Lemma 1 Assume that condition 1 holds, and for all a E N define Qa = E[φ(T1) (ψ(O))⊤ |A1 = a] (5) Then if Ua is a matrix of the m left singular vectors of Qa corresponding to non-zero singular values,</context>
<context position="23262" citStr="Hsu et al. (2009)" startWordPosition="4559" endWordPosition="4562">x, �Ca→b c derived from the algorithm in figure 4. Define R to be the total number of rules in the grammar of the form a —* b c or a —* x. Define Ma to be the number of training examples in the input to the algorithm in figure 4 where ri,1 has non-terminal a on its lefthand-side. Under these assumptions, iffor all a 13(r1 ... rN) p(r1 ... rN) A similar theorem (omitted for space) states that ��� G 1 + ǫ for the marginals. The condition that Ua and V� a are derived from Qa, as opposed to the sample estimate 0a, follows Foster et al. (2012). As these authors note, similar techniques to those of Hsu et al. (2009) should be M &gt; 128m2 log C2mR1 Ma ( 2N+�1 + ǫ − 1�2 A2σ4 δ Then 1 − ǫ G i i G 1 + ǫ �� µ(a,i,j) 1 − ǫ G � µ(a,i,j) 228 applicable in deriving results for the case where Qa Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i E {1 ... M}, where r(i,1) is a context free rule; t(i,1), t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function φ that maps inside trees t to feature-vectors φ(t) E Rd. A function ψ that maps outside trees o to feature-vectors ψ(o) E Rd′. Algorithm: Define ai to be the n</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jaeger</author>
</authors>
<title>Observable operator models for discrete stochastic time series.</title>
<date>2000</date>
<journal>Neural Computation,</journal>
<volume>12</volume>
<issue>6</issue>
<contexts>
<context position="5151" citStr="Jaeger, 2000" startWordPosition="805" endWordPosition="806">omputational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2,... n}. For any row or column vecto</context>
</contexts>
<marker>Jaeger, 2000</marker>
<rawString>H. Jaeger. 2000. Observable operator models for discrete stochastic time series. Neural Computation, 12(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Lugue</author>
<author>A Quattoni</author>
<author>B Balle</author>
<author>X Carreras</author>
</authors>
<title>Spectral learning for non-deterministic dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings ofEACL.</booktitle>
<contexts>
<context position="5399" citStr="Lugue et al. (2012)" startWordPosition="840" endWordPosition="843"> The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2,... n}. For any row or column vector y ∈ Rm, we use diag(y) to refer to the (m × m) matrix with diagonal elements equal to yh for h = 1... m, and off-diagonal elements equal to 0. For any statement Γ, we use [[Γ]] to refer to the indicator function that is 1 if Γ is true, and 0 if Γ</context>
</contexts>
<marker>Lugue, Quattoni, Balle, Carreras, 2012</marker>
<rawString>F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras. 2012. Spectral learning for non-deterministic dependency parsing. In Proceedings ofEACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>75--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2263" citStr="Matsuzaki et al., 2005" startWordPosition="327" endWordPosition="330">ar value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of the training examples down</context>
<context position="5020" citStr="Matsuzaki et al. (2005)" startWordPosition="778" endWordPosition="781">al Meeting of the Association for Computational Linguistics, pages 223–231, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write A</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 75–82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Parikh</author>
<author>L Song</author>
<author>E P Xing</author>
</authors>
<title>A spectral algorithm for latent tree graphical models.</title>
<date>2011</date>
<booktitle>In Proceedings of The 28th International Conference on Machine Learningy (ICML</booktitle>
<contexts>
<context position="5489" citStr="Parikh et al. (2011)" startWordPosition="853" endWordPosition="856"> PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated transpose. For any integer n ≥ 1, we use [n] to denote the set {1, 2,... n}. For any row or column vector y ∈ Rm, we use diag(y) to refer to the (m × m) matrix with diagonal elements equal to yh for h = 1... m, and off-diagonal elements equal to 0. For any statement Γ, we use [[Γ]] to refer to the indicator function that is 1 if Γ is true, and 0 if Γ is false. For a random variable X, we use E[X] to denote its expected value. We will make</context>
</contexts>
<marker>Parikh, Song, Xing, 2011</marker>
<rawString>A. Parikh, L. Song, and E. P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of The 28th International Conference on Machine Learningy (ICML 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Newark, Delaware, USA,</location>
<contexts>
<context position="5048" citStr="Pereira and Schabes (1992)" startWordPosition="782" endWordPosition="785">tion for Computational Linguistics, pages 223–231, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A or a vector v, we write AT or vT for the associated t</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes. 1992. Inside-outside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Newark, Delaware, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="2238" citStr="Petrov et al., 2006" startWordPosition="323" endWordPosition="326"> in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCFGs have been shown to be a very effective model for natural language parsing. Under a separation (singular value) condition, our algorithm provides consistent parameter estimates; this is in contrast with previous work, which has used the EM algorithm for parameter estimation, with the usual problems of local optima. The parameter estimation algorithm (see figure 4) is simple and efficient. The first step is to take an SVD of the training examples, followed by a projection of t</context>
<context position="4995" citStr="Petrov et al. (2006)" startWordPosition="774" endWordPosition="777">dings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223–231, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics veloped in this paper are quite general, and should be relevant to the development of spectral methods for estimation in other models in NLP, for example alignment models for translation, synchronous PCFGs, and so on. The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs, and may itself lead to new models. 2 Related Work For work on L-PCFGs using the EM algorithm, see Petrov et al. (2006), Matsuzaki et al. (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs (Hsu et al., 2009; Foster et al., 2012; Jaeger, 2000), but involves several extensions: in particular in the tensor form of the inside-outside algorithm, and observable representations for the tensor form. Balle et al. (2011) consider spectral learning of finite-state transducers; Lugue et al. (2012) considers spectral learning of head automata for dependency parsing. Parikh et al. (2011) consider spectral learning algorithms of treestructured directed bayes nets. 3 Notation Given a matrix A </context>
<context position="11609" citStr="Petrov et al. (2006)" startWordPosition="2197" endWordPosition="2200">e p(r1 ... rN). 2. For a given input sentence x = x1 ... xN, calculate the marginal probabilities µ(a,i,j) = � p(τ) τET (x):(a,i,j)Eτ for each non-terminal a E N, for each (i, j) such that 1 G i G j G N. Here T (x) denotes the set of all possible s-trees for the sentence x, and we write (a, i, j) E τ if nonterminal a spans words xi ... xj in the parse tree τ. The marginal probabilities have a number of uses. Perhaps most importantly, for a given sentence x = x1 ... xN, the parsing algorithm of Goodman (1996) can be used to find �arg max µ(a, i, j) τET (x) This is the parsing algorithm used by Petrov et al. (2006), for example. In addition, we can calculate the probability for an input sentence, p(x) = EτET (x) p(τ), as p(x) = EaEZ µ(a,1,N). Variants of the inside-outside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms, using tensors. This is the first step in deriving the spectral estimation method. The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs: 1. A tensor Ca,b c E R(mxmxm) for each rule a — *b c. 2. A vector c∞a,x E R(1xm) for each rule a —* x. S1 NP2 VP5 P7 saw him D3 the N4 dog V6 X rlN i=1 rlX iER (a,i,j</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Terwijn</author>
</authors>
<title>On the learnability of hidden markov models.</title>
<date>2002</date>
<booktitle>In Grammatical Inference: Algorithms and Applications</booktitle>
<volume>2484</volume>
<pages>261--268</pages>
<publisher>Springer.</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="1756" citStr="Terwijn, 2002" startWordPosition="250" endWordPosition="251">eter estimates. From a practical perspective, problems with local optima can be difficult to deal with. Recent work has introduced polynomial-time learning algorithms (and consistent estimation methods) for two important cases of hidden-variable models: Gaussian mixture models (Dasgupta, 1999; Vempala and Wang, 2004) and hidden Markov models (Hsu et al., 2009). These algorithms use spectral methods: that is, algorithms based on eigenvector decompositions of linear systems, in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods are not susceptible to problems with local maxima, and give consistent parameter estimates. In this paper we derive a spectral algorithm for learning of latent-variable PCFGs (L-PCFGs) (Petrov et al., 2006; Matsuzaki et al., 2005). Our method involves a significant extension of the techniques from Hsu et al. (2009). L-PCF</context>
</contexts>
<marker>Terwijn, 2002</marker>
<rawString>S. A. Terwijn. 2002. On the learnability of hidden markov models. In Grammatical Inference: Algorithms and Applications (Amsterdam, 2002), volume 2484 of Lecture Notes in Artificial Intelligence, pages 261–268, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vempala</author>
<author>G Wang</author>
</authors>
<title>A spectral algorithm for learning mixtures of distributions.</title>
<date>2004</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>68</volume>
<issue>4</issue>
<contexts>
<context position="1460" citStr="Vempala and Wang, 2004" startWordPosition="199" endWordPosition="202">is often relatively efficient, and it has well understood formal properties. It does, however, have a major limitation: it has no guarantee of finding the global optimum of the likelihood function. From a theoretical perspective, this means that the EM algorithm is not guaranteed to give consistent parameter estimates. From a practical perspective, problems with local optima can be difficult to deal with. Recent work has introduced polynomial-time learning algorithms (and consistent estimation methods) for two important cases of hidden-variable models: Gaussian mixture models (Dasgupta, 1999; Vempala and Wang, 2004) and hidden Markov models (Hsu et al., 2009). These algorithms use spectral methods: that is, algorithms based on eigenvector decompositions of linear systems, in particular singular value decomposition (SVD). In the general case, learning of HMMs or GMMs is intractable (e.g., see Terwijn, 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions. For example, the algorithm of Hsu et al. (2009) has a sample complexity that is polynomial in 1/Q, where Q is the minimum singular value of an underlying decomposition. These methods are not susceptible to prob</context>
</contexts>
<marker>Vempala, Wang, 2004</marker>
<rawString>S. Vempala and G. Wang. 2004. A spectral algorithm for learning mixtures of distributions. Journal of Computer and System Sciences, 68(4):841–860.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>