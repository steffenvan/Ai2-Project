<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016583">
<title confidence="0.975124">
Getting the Most out of Transition-based Dependency Parsing
</title>
<author confidence="0.991119">
Jinho D. Choi
</author>
<affiliation confidence="0.999124">
Department of Computer Science
University of Colorado at Boulder
</affiliation>
<email confidence="0.996467">
choijd@colorado.edu
</email>
<author confidence="0.985453">
Martha Palmer
</author>
<affiliation confidence="0.9984175">
Department of Linguistics
University of Colorado at Boulder
</affiliation>
<email confidence="0.998703">
mpalmer@colorado.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990076">
This paper suggests two ways of improving
transition-based, non-projective dependency
parsing. First, we add a transition to an exist-
ing non-projective parsing algorithm, so it can
perform either projective or non-projective
parsing as needed. Second, we present a boot-
strapping technique that narrows down dis-
crepancies between gold-standard and auto-
matic parses used as features. The new ad-
dition to the algorithm shows a clear advan-
tage in parsing speed. The bootstrapping
technique gives a significant improvement to
parsing accuracy, showing near state-of-the-
art performance with respect to other parsing
approaches evaluated on the same data set.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99321125">
Dependency parsing has recently gained consider-
able interest because it is simple and fast, yet pro-
vides useful information for many NLP tasks (Shen
et al., 2008; Councill et al., 2010). There are two
main dependency parsing approaches (Nivre and
McDonald, 2008). One is a transition-based ap-
proach that greedily searches for local optima (high-
est scoring transitions) and uses parse history as fea-
tures to predict the next transition (Nivre, 2003).
The other is a graph-based approach that searches
for a global optimum (highest scoring tree) from
a complete graph in which vertices represent word
tokens and edges (directed and weighted) represent
dependency relations (McDonald et al., 2005).
Lately, the usefulness of the transition-based ap-
proach has drawn more attention because it gener-
ally performs noticeably faster than the graph-based
687
approach (Cer et al., 2010). The transition-based ap-
proach has a worst-case parsing complexity of O(n)
for projective, and O(n2) for non-projective pars-
ing (Nivre, 2008). The complexity is lower for pro-
jective parsing because it can deterministically drop
certain tokens from the search space whereas that
is not advisable for non-projective parsing. Despite
this fact, it is possible to perform non-projective
parsing in linear time in practice (Nivre, 2009). This
is because the amount of non-projective dependen-
cies is much smaller than the amount of projective
dependencies, so a parser can perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. One other advan-
tage of the transition-based approach is that it can
use parse history as features to make the next pre-
diction. This parse information helps to improve
parsing accuracy without hurting parsing complex-
ity (Nivre, 2006). Most current transition-based ap-
proaches use gold-standard parses as features dur-
ing training; however, this is not necessarily what
parsers encounter during decoding. Thus, it is desir-
able to minimize the gap between gold-standard and
automatic parses for the best results.
This paper improves the engineering of different
aspects of transition-based, non-projective depen-
dency parsing. To reduce the search space, we add a
transition to an existing non-projective parsing algo-
rithm. To narrow down the discrepancies between
gold-standard and automatic parses, we present a
bootstrapping technique. The new addition to the
algorithm shows a clear advantage in parsing speed.
The bootstrapping technique gives a significant im-
provement to parsing accuracy.
</bodyText>
<note confidence="0.7622235">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 687–692,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.9837831">
LEFT-POPL ([λ1|i], λ2, [j|β], E) ⇒ (λ1, λ2, [j|β], E ∪ {i L ← j} )
∃i =6 0, j. i 6→� j ∧ �k ∈ β. i → k
LEFT-ARCL ([λ1|i], λ2 , [j|β], E) ⇒ (λ1 , [i|λ2], [j|β], E ∪ {i L ← j})
∃i =6 0,j. i 6→� j
RIGHT-ARCL ([λ1|i], λ* , [j|β], E) ⇒ (λ1 , [i|λ2], [j|β], E ∪ {i → L j})
∃i, j. i 6←L j
SHIFT ( λ1 , λ2, [j|β], E ) ⇒ ( [λ1 · λ2|j], [ ] , β , E )
DT: λ1 = [ ], NT: �k ∈ λ1. k → j ∨ k ← j
NO-ARC ( [λ1|i], λ2 , [j|β], E ) ⇒ ( λ1 , [i|λ2], [j|β], E )
default transition
</table>
<tableCaption confidence="0.996458">
Table 1: Transitions in our algorithm. For each row, the first line shows a transition and the second line shows
preconditions of the transition.
</tableCaption>
<sectionHeader confidence="0.945282" genericHeader="method">
2 Reducing search space
</sectionHeader>
<bodyText confidence="0.9999753">
Our algorithm is based on Choi-Nicolov’s approach
to Nivre’s list-based algorithm (Nivre, 2008). The
main difference between these two approaches is in
their implementation of the SHIFT transition. Choi-
Nicolov’s approach divides the SHIFT transition into
two, deterministic and non-deterministic SHIFT’s,
and trains the non-deterministic SHIFT with a classi-
fier so it can be predicted during decoding. Choi and
Nicolov (2009) showed that this implementation re-
duces the parsing complexity from O(n2) to linear
time in practice (a worst-case complexity is O(n2)).
We suggest another transition-based parsing ap-
proach that reduces the search space even more.
The idea is to merge transitions in Choi-Nicolov’s
non-projective algorithm with transitions in Nivre’s
projective algorithm (Nivre, 2003). Nivre’s projec-
tive algorithm has a worst-case complexity of O(n),
which is faster than any non-projective parsing al-
gorithm. Since the number of non-projective depen-
dencies is much smaller than the number of projec-
tive dependencies (Nivre and Nilsson, 2005), it is
not efficient to perform non-projective parsing for
all cases. Ideally, it is better to perform projective
parsing for most cases and perform non-projective
parsing only when it is needed. In this algorithm, we
add another transition to Choi-Nicolov’s approach,
LEFT-POP, similar to the LEFT-ARC transition in
Nivre’s projective algorithm. By adding this tran-
sition, an oracle can now choose either projective or
non-projective parsing depending on parsing states.1
</bodyText>
<footnote confidence="0.873883666666667">
1We also tried adding the RIGHT-ARC transition from
Nivre’s projective algorithm, which did not improve parsing
performance for our experiments.
</footnote>
<bodyText confidence="0.999916028571429">
Note that Nivre (2009) has a similar idea of per-
forming projective and non-projective parsing selec-
tively. That algorithm uses a SWAP transition to
reorder tokens related to non-projective dependen-
cies, and runs in linear time in practice (a worst-case
complexity is still O(n2)). Our algorithm is distin-
guished in that it does not require such reordering.
Table 1 shows transitions used in our algorithm.
All parsing states are represented as tuples (λ1, λ2,
β, E), where λ1, λ2, and β are lists of word tokens.
E is a set of labeled edges representing previously
identified dependencies. L is a dependency label and
i, j, k represent indices of their corresponding word
tokens. The initial state is ([0], [ ], [1,...,n], ∅). The
0 identifier corresponds to an initial token, w0, intro-
duced as the root of the sentence. The final state is
(λ1, λ2, [ ], E), i.e., the algorithm terminates when
all tokens in β are consumed.
The algorithm uses five kinds of transitions. All
transitions are performed by comparing the last to-
ken in λ1, wi, and the first token in β, wj. Both
LEFT-POPL and LEFT-ARCL are performed when
wj is the head of wi with a dependency relation L.
The difference is that LEFT-POP removes wi from
λ1 after the transition, assuming that the token is no
longer needed in later parsing states, whereas LEFT-
ARC keeps the token so it can be the head of some
token wj&lt;k&lt;n in β. This wi → wk relation causes
a non-projective dependency. RIGHT-ARCL is per-
formed when wi is the head of wj with a dependency
relation L. SHIFT is performed when λ1 is empty
(DT) or there is no token in λ1 that is either the head
or a dependent of wj (NT). NO-ARC is there to move
tokens around so each token in β can be compared
to all (or some) tokens prior to it.
</bodyText>
<page confidence="0.978408">
688
</page>
<figure confidence="0.942935966666666">
NMOD
IM
OBJ
PRD
SBJ
PMOD
NMOD
ROOT
Root0 It1 wase in3 my4 interests to6 seed you8
Transition x1 x2 0 E
0 [0] [ ] [1|0] ∅
1 SHIFT (NT) [x1|1] [ ] [2|0]
2 LEFT-ARC [0] [1] [2|0] E ∪ {1 ←SBJ− 2}
3 RIGHT-ARC [ ] [0|x2] [2|0] E ∪ {0 −ROOT→ 2}
4 SHIFT (DT) [x1|2] [ ] [3|0]
5 RIGHT-ARC [x1|1] [2] [3|0] E ∪ {2 −PRD→ 3}
6 SHIFT (NT) [x1|3] [ ] [4|0]
7 SHIFT (NT) [x1|4] [ ] [5|0]
8 LEFT-POP [x1|3] [ ] [5|0] E ∪ {4 ←NMOD− 5}
9 RIGHT-ARC [x1|2] [3] [5|0] E ∪ {3 −PMOD→ 5}
10 SHIFT (NT) [x1|5] [ ] [6|0]
11 NO-ARC [x1|3] [5] [6|0]
12 NO-ARC [x1|2] [3|x2] [6|0]
13 NO-ARC [x1|1] [2|x2] [6|0]
14 RIGHT-ARC [0] [1|x2] [6|0] E ∪ {1 −NMOD→ 6}
15 SHIFT (NT) [x1|6] [ ] [7|0]
16 RIGHT-ARC [x1|5] [6] [7|0] E ∪ {6 −IM→ 7}
17 SHIFT (NT) [x1|7] [ ] [8|0]
18 RIGHT-ARC [x1|6] [7] [8|0] E ∪ {7 −OBJ→ 8}
19 SHIFT (NT) [x1|8] [ ] [ ]
</figure>
<tableCaption confidence="0.9117185">
Table 2: Parsing states for the example sentence. After LEFT-POP is performed (#8), [w4 = my] is removed from the
search space and no longer considered in the later parsing states (e.g., between #10 and #11).
</tableCaption>
<bodyText confidence="0.999780384615385">
During training, the algorithm checks for the pre-
conditions of all transitions and generates training
instances with corresponding labels. During decod-
ing, the oracle decides which transition to perform
based on the parsing states. With the addition of
LEFT-POP, the oracle can choose either projective
or non-projective parsing by selecting LEFT-POP or
LEFT-ARC, respectively. Our experiments show that
this additional transition improves both parsing ac-
curacy and speed. The advantage derives from im-
proving the efficiency of the choice mechanism; it is
now simply a transition choice and requires no addi-
tional processing.
</bodyText>
<sectionHeader confidence="0.872089" genericHeader="method">
3 Bootstrapping automatic parses
</sectionHeader>
<bodyText confidence="0.999834428571429">
Transition-based parsing has the advantage of using
parse history as features to make the next prediction.
In our algorithm, when wi and wj are compared,
subtree and head information of these tokens is par-
tially provided by previous parsing states. Graph-
based parsing can also take advantage of using parse
information. This is done by performing ‘higher-
order parsing’, which is shown to improve parsing
accuracy but also increase parsing complexity (Car-
reras, 2007; Koo and Collins, 2010).2 Transition-
based parsing is attractive because it can use parse
information without increasing complexity (Nivre,
2006). The qualification is that parse information
provided by gold-standard trees during training is
not necessarily the same kind of information pro-
vided by automatically parsed trees during decod-
ing. This can confuse a statistical model trained only
on the gold-standard trees.
To reduce the gap between gold-standard and au-
tomatic parses, we use bootstrapping on automatic
parses. First, we train a statistical model using gold-
</bodyText>
<footnote confidence="0.746779">
2Second-order, non-projective, graph-based dependency
parsing is NP-hard without performing approximation.
</footnote>
<page confidence="0.996871">
689
</page>
<bodyText confidence="0.99980355">
standard trees. Then, we parse the training data us-
ing the statistical model. During parsing, we ex-
tract features for each parsing state, consisting of
automatic parse information, and generate a train-
ing instance by joining the features with the gold-
standard label. The gold-standard label is achieved
by comparing the dependency relation between wz
and wj in the gold-standard tree. When the parsing
is done, we train a different model using the training
instances induced by the previous model. We repeat
the procedure until a stopping criteria is met.
The stopping criteria is determined by performing
cross-validation. For each stage, we perform cross-
validation to check if the average parsing accuracy
on the current cross-validation set is higher than the
one from the previous stage. We stop the procedure
when the parsing accuracy on cross-validation sets
starts decreasing. Our experiments show that this
simple bootstrapping technique gives a significant
improvement to parsing accuracy.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999983">
Daum´e et al. (2009) presented an algorithm, called
SEARN, for integrating search and learning to solve
complex structured prediction problems. Our boot-
strapping technique can be viewed as a simplified
version of SEARN. During training, SEARN itera-
tively creates a set of new cost-sensitive examples
using a known policy. In our case, the new examples
are instances containing automatic parses induced
by the previous model. Our technique is simpli-
fied because the new examples are not cost-sensitive.
Furthermore, SEARN interpolates the current policy
with the previous policy whereas we do not per-
form such interpolation. During decoding, SEARN
generates a sequence of decisions and makes a fi-
nal prediction. In our case, the decisions are pre-
dicted dependency relations and the final prediction
is a dependency tree. SEARN has been successfully
adapted to several NLP tasks such as named entity
recognition, syntactic chunking, and POS tagging.
To the best of our knowledge, this is the first time
that this idea has been applied to transition-based
parsing and shown promising results.
Zhang and Clark (2008) suggested a transition-
based projective parsing algorithm that keeps B dif-
ferent sequences of parsing states and chooses the
one with the best score. They use beam search and
show a worst-case parsing complexity of O(n) given
a fixed beam size. Similarly to ours, their learn-
ing mechanism using the structured perceptron al-
gorithm involves training on automatically derived
parsing states that closely resemble potential states
encountered during decoding.
</bodyText>
<sectionHeader confidence="0.999828" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998741">
5.1 Corpora and learning algorithm
</subsectionHeader>
<bodyText confidence="0.999941625">
All models are trained and tested on English and
Czech data using automatic lemmas, POS tags,
and feats, as distributed by the CoNLL’09 shared
task (Hajiˇc et al., 2009). We use Liblinear L2-L1
SVM for learning (L2 regularization, L1 loss; Hsieh
et al. (2008)). For our experiments, we use the fol-
lowing learning parameters: c = 0.1 (cost), e = 0.1
(termination criterion), B = 0 (bias).
</bodyText>
<subsectionHeader confidence="0.999315">
5.2 Accuracy comparisons
</subsectionHeader>
<bodyText confidence="0.999961933333334">
First, we evaluate the impact of the LEFT-POP tran-
sition we add to Choi-Nicolov’s approach. To make
a fair comparison, we implemented both approaches
and built models using the exact same feature set.
The ‘CN’ and ‘Our’ rows in Table 3 show accuracies
achieved by Choi-Nicolov’s and our approaches, re-
spectively. Our approach shows higher accuracies
for all categories. Next, we evaluate the impact of
our bootstrapping technique. The ‘Our+’ row shows
accuracies achieved by our algorithm using the boot-
strapping technique. The improvement from ‘Our’
to ‘Our+’ is statistically significant for all categories
(McNemar, p &lt; .0001). The improvment is even
more significant in a language like Czech for which
parsers generally perform more poorly.
</bodyText>
<table confidence="0.994885714285714">
English Czech
LAS UAS LAS UAS
CN 88.54 90.57 78.12 83.29
Our 88.62 90.66 78.30 83.47
Our+ 89.15* 91.18* 80.24* 85.24*
Merlo 88.79 (3) - 80.38 (1) -
Bohnet 89.88 (1) - 80.11 (2) -
</table>
<tableCaption confidence="0.98152025">
Table 3: Accuracy comparisons between different pars-
ing approaches (LAS/UAS: labeled/unlabeled attachment
score). * indicates a statistically significant improvement.
(#) indicates an overall rank of the system in CoNLL’09.
</tableCaption>
<page confidence="0.997696">
690
</page>
<bodyText confidence="0.999992625">
Finally, we compare our work against other state-of-
the-art systems. For the CoNLL’09 shared task, Ges-
mundo et al. (2009) introduced the best transition-
based system using synchronous syntactic-semantic
parsing (‘Merlo’), and Bohnet (2009) introduced the
best graph-based system using a maximum span-
ning tree algorithm (‘Bohnet’). Our approach shows
quite comparable results with these systems.3
</bodyText>
<subsectionHeader confidence="0.999108">
5.3 Speed comparisons
</subsectionHeader>
<bodyText confidence="0.9999609">
Figure 1 shows average parsing speeds for each
sentence group in both English and Czech eval-
uation sets (Table 4). ‘Nivre’ is Nivre’s swap
algorithm (Nivre, 2009), of which we use the
implementation from MaltParser (maltparser.
org). The other approaches are implemented in
our open source project, called ClearParser (code.
google.com/p/clearparser). Note that fea-
tures used in MaltParser have not been optimized
for these evaluation sets. All experiments are tested
on an Intel Xeon 2.57GHz machine. For general-
ization, we run five trials for each parser, cut off
the top and bottom speeds, and average the middle
three. The loading times for machine learning mod-
els are excluded because they are independent from
the parsing algorithms. The average parsing speeds
are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre,
CN, and Our+, respectively. Our approach shows
linear growth all along, even for the sentence groups
where some approaches start showing curves.
</bodyText>
<subsectionHeader confidence="0.313373">
Sentence length
</subsectionHeader>
<figureCaption confidence="0.9933895">
Figure 1: Average parsing speeds with respect to sentence
groups in Table 4.
</figureCaption>
<footnote confidence="0.535909">
3Later, ‘Merlo’ and ‘Bohnet” introduced more advanced
systems, showing some improvements over their previous ap-
proaches (Titov et al., 2009; Bohnet, 2010).
</footnote>
<table confidence="0.979659">
&lt; 10 &lt; 20 &lt; 30 &lt; 40 &lt; 50 &lt; 60 &lt; 70
1,415 2,289 1,714 815 285 72 18
</table>
<tableCaption confidence="0.997999">
Table 4: # of sentences in each group, extracted from both
English/Czech evaluation sets. ‘&lt; n’ implies a group
containing sentences whose lengths are less than n.
</tableCaption>
<bodyText confidence="0.9999602">
We also measured average parsing speeds for ‘Our’,
which showed a very similar growth to ‘Our+’. The
average parsing speed of ‘Our’ was 2.20 ms; it per-
formed slightly faster than ‘Our+’ because it skipped
more nodes by performing more non-deterministic
SHIFT’s, which may or may not have been correct
decisions for the corresponding parsing states.
It is worth mentioning that the curve shown by
‘Nivre’ might be caused by implementation details
regarding feature extraction, which we included as
part of parsing. To abstract away from these im-
plementation details and focus purely on the algo-
rithms, we would need to compare the actual num-
ber of transitions performed by each parser, which
will be explored in future work.
</bodyText>
<sectionHeader confidence="0.947619" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.9999785">
We present two ways of improving transition-based,
non-projective dependency parsing. The additional
transition gives improvements to both parsing speed
and accuracy, showing a linear time parsing speed
with respect to sentence length. The bootstrapping
technique gives a significant improvement to parsing
accuracy, showing near state-of-the-art performance
with respect to other parsing approaches. In the fu-
ture, we will test the robustness of these approaches
in more languages.
</bodyText>
<sectionHeader confidence="0.984421" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987890071428572">
We gratefully acknowledge the support of the Na-
tional Science Foundation Grants CISE-IIS-RI-0910992,
Richer Representations for Machine Translation, a sub-
contract from the Mayo Clinic and Harvard Children’s
Hospital based on a grant from the ONC, 90TR0002/01,
Strategic Health Advanced Research Project Area 4: Nat-
ural Language Processing, and a grant from the Defense
Advanced Research Projects Agency (DARPA/IPTO) un-
der the GALE program, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc. Any
opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reflect the views of the National Science
Foundation.
</bodyText>
<figure confidence="0.991882">
0 10 20 30 40 50 60 70
Nivre
22
18
14
CN
10
6
Our+
2
Parsing speed (in ms)
</figure>
<page confidence="0.989023">
691
</page>
<sectionHeader confidence="0.988904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999698759615385">
Bernd Bohnet. 2009. Efficient parsing of syntactic and
semantic dependency structures. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task (CoNLL’09), pages 67–72.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In The 23rd In-
ternational Conference on Computational Linguistics
(COLING’10).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
the CoNLL Shared Task Session of EMNLP-CoNLL’07
(CoNLL’07), pages 957–961.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing
to stanford dependencies: Trade-offs between speed
and accuracy. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC’10).
Jinho D. Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing us-
ing robust risk minimization. In Recent Advances in
Natural Language Processing V, pages 205–216. John
Benjamins.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s great and what’s not: Learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing (NeSp-NLP’10), pages 51–59.
Hal Daum´e, Iii, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75(3):297–325.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning: Shared
Task (CoNLL’09), pages 37–42.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL’09): Shared Task, pages 1–18.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In Proceed-
ings of the 25th international conference on Machine
learning (ICML’08), pages 408–415.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL’10).
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP’05), pages 523–530.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT’08), pages 950–958.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05), pages 99–106.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT’03), pages 23–25.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513–553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP’09),
pages 351–359.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL:HLT’08), pages 577–585.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic depen-
dencies. In Proceedings of the 21st International
Joint Conference on Artificial Intelligence (IJCAI’09),
pages 1562–1567.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing using
beam-search. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP’08), pages 562–571.
</reference>
<page confidence="0.99791">
692
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933690">
<title confidence="0.999945">Getting the Most out of Transition-based Dependency Parsing</title>
<author confidence="0.990175">D Jinho</author>
<affiliation confidence="0.9986465">Department of Computer University of Colorado at</affiliation>
<email confidence="0.962389">choijd@colorado.edu</email>
<author confidence="0.993317">Martha</author>
<affiliation confidence="0.9993195">Department of University of Colorado at</affiliation>
<email confidence="0.997696">mpalmer@colorado.edu</email>
<abstract confidence="0.9994685625">This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Efficient parsing of syntactic and semantic dependency structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task (CoNLL’09),</booktitle>
<pages>67--72</pages>
<contexts>
<context position="14996" citStr="Bohnet (2009)" startWordPosition="2440" endWordPosition="2441">N 88.54 90.57 78.12 83.29 Our 88.62 90.66 78.30 83.47 Our+ 89.15* 91.18* 80.24* 85.24* Merlo 88.79 (3) - 80.38 (1) - Bohnet 89.88 (1) - 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (LAS/UAS: labeled/unlabeled attachment score). * indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. 690 Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open source project, called ClearParser (code. google.com/p/clearparser). Note that features used in MaltParser have not been optimized for the</context>
</contexts>
<marker>Bohnet, 2009</marker>
<rawString>Bernd Bohnet. 2009. Efficient parsing of syntactic and semantic dependency structures. In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task (CoNLL’09), pages 67–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In The 23rd International Conference on Computational Linguistics (COLING’10).</booktitle>
<contexts>
<context position="16388" citStr="Bohnet, 2010" startWordPosition="2653" endWordPosition="2654">rage the middle three. The loading times for machine learning models are excluded because they are independent from the parsing algorithms. The average parsing speeds are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre, CN, and Our+, respectively. Our approach shows linear growth all along, even for the sentence groups where some approaches start showing curves. Sentence length Figure 1: Average parsing speeds with respect to sentence groups in Table 4. 3Later, ‘Merlo’ and ‘Bohnet” introduced more advanced systems, showing some improvements over their previous approaches (Titov et al., 2009; Bohnet, 2010). &lt; 10 &lt; 20 &lt; 30 &lt; 40 &lt; 50 &lt; 60 &lt; 70 1,415 2,289 1,714 815 285 72 18 Table 4: # of sentences in each group, extracted from both English/Czech evaluation sets. ‘&lt; n’ implies a group containing sentences whose lengths are less than n. We also measured average parsing speeds for ‘Our’, which showed a very similar growth to ‘Our+’. The average parsing speed of ‘Our’ was 2.20 ms; it performed slightly faster than ‘Our+’ because it skipped more nodes by performing more non-deterministic SHIFT’s, which may or may not have been correct decisions for the corresponding parsing states. It is worth mentio</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In The 23rd International Conference on Computational Linguistics (COLING’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL’07 (CoNLL’07),</booktitle>
<pages>957--961</pages>
<contexts>
<context position="9880" citStr="Carreras, 2007" startWordPosition="1651" endWordPosition="1653">the efficiency of the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses Transition-based parsing has the advantage of using parse history as features to make the next prediction. In our algorithm, when wi and wj are compared, subtree and head information of these tokens is partially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using gold2Second-order, non-projective</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL’07 (CoNLL’07), pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to stanford dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to stanford dependencies: Trade-offs between speed and accuracy. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Nicolas Nicolov</author>
</authors>
<title>K-best, locally pruned, transition-based dependency parsing using robust risk minimization.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing V,</booktitle>
<pages>205--216</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="4726" citStr="Choi and Nicolov (2009)" startWordPosition="757" endWordPosition="760">|β], E ) default transition Table 1: Transitions in our algorithm. For each row, the first line shows a transition and the second line shows preconditions of the transition. 2 Reducing search space Our algorithm is based on Choi-Nicolov’s approach to Nivre’s list-based algorithm (Nivre, 2008). The main difference between these two approaches is in their implementation of the SHIFT transition. ChoiNicolov’s approach divides the SHIFT transition into two, deterministic and non-deterministic SHIFT’s, and trains the non-deterministic SHIFT with a classifier so it can be predicted during decoding. Choi and Nicolov (2009) showed that this implementation reduces the parsing complexity from O(n2) to linear time in practice (a worst-case complexity is O(n2)). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Choi-Nicolov’s non-projective algorithm with transitions in Nivre’s projective algorithm (Nivre, 2003). Nivre’s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm. Since the number of non-projective dependencies is much smaller than the number of projective dependenc</context>
</contexts>
<marker>Choi, Nicolov, 2009</marker>
<rawString>Jinho D. Choi and Nicolas Nicolov. 2009. K-best, locally pruned, transition-based dependency parsing using robust risk minimization. In Recent Advances in Natural Language Processing V, pages 205–216. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s great and what’s not: Learning to classify the scope of negation for improved sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing (NeSp-NLP’10),</booktitle>
<pages>51--59</pages>
<contexts>
<context position="1116" citStr="Councill et al., 2010" startWordPosition="159" endWordPosition="162">g as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because </context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac G. Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s great and what’s not: Learning to classify the scope of negation for improved sentiment analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing (NeSp-NLP’10), pages 51–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>John Langford Iii</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>75</volume>
<issue>3</issue>
<marker>Daum´e, Iii, Marcu, 2009</marker>
<rawString>Hal Daum´e, Iii, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75(3):297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Ivan Titov</author>
</authors>
<title>A latent variable model of synchronous syntactic-semantic parsing for multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task (CoNLL’09),</booktitle>
<pages>37--42</pages>
<contexts>
<context position="14879" citStr="Gesmundo et al. (2009)" startWordPosition="2423" endWordPosition="2427">ven more significant in a language like Czech for which parsers generally perform more poorly. English Czech LAS UAS LAS UAS CN 88.54 90.57 78.12 83.29 Our 88.62 90.66 78.30 83.47 Our+ 89.15* 91.18* 80.24* 85.24* Merlo 88.79 (3) - 80.38 (1) - Bohnet 89.88 (1) - 80.11 (2) - Table 3: Accuracy comparisons between different parsing approaches (LAS/UAS: labeled/unlabeled attachment score). * indicates a statistically significant improvement. (#) indicates an overall rank of the system in CoNLL’09. 690 Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open source project, calle</context>
</contexts>
<marker>Gesmundo, Henderson, Merlo, Titov, 2009</marker>
<rawString>Andrea Gesmundo, James Henderson, Paola Merlo, and Ivan Titov. 2009. A latent variable model of synchronous syntactic-semantic parsing for multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task (CoNLL’09), pages 37–42.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL’09): Shared Task,</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL’09): Shared Task, pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cho-Jui Hsieh</author>
<author>Kai-Wei Chang</author>
<author>Chih-Jen Lin</author>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear svm.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning (ICML’08),</booktitle>
<pages>408--415</pages>
<contexts>
<context position="13453" citStr="Hsieh et al. (2008)" startWordPosition="2197" endWordPosition="2200">re. They use beam search and show a worst-case parsing complexity of O(n) given a fixed beam size. Similarly to ours, their learning mechanism using the structured perceptron algorithm involves training on automatically derived parsing states that closely resemble potential states encountered during decoding. 5 Experiments 5.1 Corpora and learning algorithm All models are trained and tested on English and Czech data using automatic lemmas, POS tags, and feats, as distributed by the CoNLL’09 shared task (Hajiˇc et al., 2009). We use Liblinear L2-L1 SVM for learning (L2 regularization, L1 loss; Hsieh et al. (2008)). For our experiments, we use the following learning parameters: c = 0.1 (cost), e = 0.1 (termination criterion), B = 0 (bias). 5.2 Accuracy comparisons First, we evaluate the impact of the LEFT-POP transition we add to Choi-Nicolov’s approach. To make a fair comparison, we implemented both approaches and built models using the exact same feature set. The ‘CN’ and ‘Our’ rows in Table 3 show accuracies achieved by Choi-Nicolov’s and our approaches, respectively. Our approach shows higher accuracies for all categories. Next, we evaluate the impact of our bootstrapping technique. The ‘Our+’ row </context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear svm. In Proceedings of the 25th international conference on Machine learning (ICML’08), pages 408–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10).</booktitle>
<contexts>
<context position="9904" citStr="Koo and Collins, 2010" startWordPosition="1654" endWordPosition="1657">f the choice mechanism; it is now simply a transition choice and requires no additional processing. 3 Bootstrapping automatic parses Transition-based parsing has the advantage of using parse history as features to make the next prediction. In our algorithm, when wi and wj are compared, subtree and head information of these tokens is partially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using gold2Second-order, non-projective, graph-based dependency</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP’05),</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP’05), pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL:HLT’08),</booktitle>
<pages>950--958</pages>
<contexts>
<context position="1193" citStr="Nivre and McDonald, 2008" startWordPosition="170" endWordPosition="173">n discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 approach (Ce</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL:HLT’08), pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>99--106</pages>
<contexts>
<context position="5355" citStr="Nivre and Nilsson, 2005" startWordPosition="850" endWordPosition="853">ed that this implementation reduces the parsing complexity from O(n2) to linear time in practice (a worst-case complexity is O(n2)). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Choi-Nicolov’s non-projective algorithm with transitions in Nivre’s projective algorithm (Nivre, 2003). Nivre’s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm. Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed. In this algorithm, we add another transition to Choi-Nicolov’s approach, LEFT-POP, similar to the LEFT-ARC transition in Nivre’s projective algorithm. By adding this transition, an oracle can now choose either projective or non-projective parsing depending on parsing states.1 1We also tried adding the RIGHT-ARC transition from Nivre’s projective algorithm, which did not improve parsing performance fo</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 99–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT’03),</booktitle>
<pages>23--25</pages>
<contexts>
<context position="1379" citStr="Nivre, 2003" startWordPosition="202" endWordPosition="203">t improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2) for non-projective parsing (Nivre, 2008). The complexity is lower for</context>
<context position="5096" citStr="Nivre, 2003" startWordPosition="812" endWordPosition="813">FT transition. ChoiNicolov’s approach divides the SHIFT transition into two, deterministic and non-deterministic SHIFT’s, and trains the non-deterministic SHIFT with a classifier so it can be predicted during decoding. Choi and Nicolov (2009) showed that this implementation reduces the parsing complexity from O(n2) to linear time in practice (a worst-case complexity is O(n2)). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Choi-Nicolov’s non-projective algorithm with transitions in Nivre’s projective algorithm (Nivre, 2003). Nivre’s projective algorithm has a worst-case complexity of O(n), which is faster than any non-projective parsing algorithm. Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed. In this algorithm, we add another transition to Choi-Nicolov’s approach, LEFT-POP, similar to the LEFT-ARC transition in Nivre’s projective algo</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT’03), pages 23–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2709" citStr="Nivre, 2006" startWordPosition="407" endWordPosition="408">isable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use parse history as features to make the next prediction. This parse information helps to improve parsing accuracy without hurting parsing complexity (Nivre, 2006). Most current transition-based approaches use gold-standard parses as features during training; however, this is not necessarily what parsers encounter during decoding. Thus, it is desirable to minimize the gap between gold-standard and automatic parses for the best results. This paper improves the engineering of different aspects of transition-based, non-projective dependency parsing. To reduce the search space, we add a transition to an existing non-projective parsing algorithm. To narrow down the discrepancies between gold-standard and automatic parses, we present a bootstrapping technique</context>
<context position="10025" citStr="Nivre, 2006" startWordPosition="1672" endWordPosition="1673">s Transition-based parsing has the advantage of using parse history as features to make the next prediction. In our algorithm, when wi and wj are compared, subtree and head information of these tokens is partially provided by previous parsing states. Graphbased parsing can also take advantage of using parse information. This is done by performing ‘higherorder parsing’, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transitionbased parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). The qualification is that parse information provided by gold-standard trees during training is not necessarily the same kind of information provided by automatically parsed trees during decoding. This can confuse a statistical model trained only on the gold-standard trees. To reduce the gap between gold-standard and automatic parses, we use bootstrapping on automatic parses. First, we train a statistical model using gold2Second-order, non-projective, graph-based dependency parsing is NP-hard without performing approximation. 689 standard trees. Then, we parse the training data using the stat</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1950" citStr="Nivre, 2008" startWordPosition="288" endWordPosition="289">predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use p</context>
<context position="4396" citStr="Nivre, 2008" startWordPosition="711" endWordPosition="712">λ1|i], λ2 , [j|β], E) ⇒ (λ1 , [i|λ2], [j|β], E ∪ {i L ← j}) ∃i =6 0,j. i 6→� j RIGHT-ARCL ([λ1|i], λ* , [j|β], E) ⇒ (λ1 , [i|λ2], [j|β], E ∪ {i → L j}) ∃i, j. i 6←L j SHIFT ( λ1 , λ2, [j|β], E ) ⇒ ( [λ1 · λ2|j], [ ] , β , E ) DT: λ1 = [ ], NT: �k ∈ λ1. k → j ∨ k ← j NO-ARC ( [λ1|i], λ2 , [j|β], E ) ⇒ ( λ1 , [i|λ2], [j|β], E ) default transition Table 1: Transitions in our algorithm. For each row, the first line shows a transition and the second line shows preconditions of the transition. 2 Reducing search space Our algorithm is based on Choi-Nicolov’s approach to Nivre’s list-based algorithm (Nivre, 2008). The main difference between these two approaches is in their implementation of the SHIFT transition. ChoiNicolov’s approach divides the SHIFT transition into two, deterministic and non-deterministic SHIFT’s, and trains the non-deterministic SHIFT with a classifier so it can be predicted during decoding. Choi and Nicolov (2009) showed that this implementation reduces the parsing complexity from O(n2) to linear time in practice (a worst-case complexity is O(n2)). We suggest another transition-based parsing approach that reduces the search space even more. The idea is to merge transitions in Ch</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP’09),</booktitle>
<pages>351--359</pages>
<contexts>
<context position="2240" citStr="Nivre, 2009" startWordPosition="332" endWordPosition="333">ately, the usefulness of the transition-based approach has drawn more attention because it generally performs noticeably faster than the graph-based 687 approach (Cer et al., 2010). The transition-based approach has a worst-case parsing complexity of O(n) for projective, and O(n2) for non-projective parsing (Nivre, 2008). The complexity is lower for projective parsing because it can deterministically drop certain tokens from the search space whereas that is not advisable for non-projective parsing. Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). This is because the amount of non-projective dependencies is much smaller than the amount of projective dependencies, so a parser can perform projective parsing for most cases and perform non-projective parsing only when it is needed. One other advantage of the transition-based approach is that it can use parse history as features to make the next prediction. This parse information helps to improve parsing accuracy without hurting parsing complexity (Nivre, 2006). Most current transition-based approaches use gold-standard parses as features during training; however, this is not necessarily w</context>
<context position="5996" citStr="Nivre (2009)" startWordPosition="946" endWordPosition="947">orm non-projective parsing for all cases. Ideally, it is better to perform projective parsing for most cases and perform non-projective parsing only when it is needed. In this algorithm, we add another transition to Choi-Nicolov’s approach, LEFT-POP, similar to the LEFT-ARC transition in Nivre’s projective algorithm. By adding this transition, an oracle can now choose either projective or non-projective parsing depending on parsing states.1 1We also tried adding the RIGHT-ARC transition from Nivre’s projective algorithm, which did not improve parsing performance for our experiments. Note that Nivre (2009) has a similar idea of performing projective and non-projective parsing selectively. That algorithm uses a SWAP transition to reorder tokens related to non-projective dependencies, and runs in linear time in practice (a worst-case complexity is still O(n2)). Our algorithm is distinguished in that it does not require such reordering. Table 1 shows transitions used in our algorithm. All parsing states are represented as tuples (λ1, λ2, β, E), where λ1, λ2, and β are lists of word tokens. E is a set of labeled edges representing previously identified dependencies. L is a dependency label and i, j</context>
<context position="15337" citStr="Nivre, 2009" startWordPosition="2492" endWordPosition="2493">nk of the system in CoNLL’09. 690 Finally, we compare our work against other state-ofthe-art systems. For the CoNLL’09 shared task, Gesmundo et al. (2009) introduced the best transitionbased system using synchronous syntactic-semantic parsing (‘Merlo’), and Bohnet (2009) introduced the best graph-based system using a maximum spanning tree algorithm (‘Bohnet’). Our approach shows quite comparable results with these systems.3 5.3 Speed comparisons Figure 1 shows average parsing speeds for each sentence group in both English and Czech evaluation sets (Table 4). ‘Nivre’ is Nivre’s swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser. org). The other approaches are implemented in our open source project, called ClearParser (code. google.com/p/clearparser). Note that features used in MaltParser have not been optimized for these evaluation sets. All experiments are tested on an Intel Xeon 2.57GHz machine. For generalization, we run five trials for each parser, cut off the top and bottom speeds, and average the middle three. The loading times for machine learning models are excluded because they are independent from the parsing algorithms. The average parsing sp</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP’09), pages 351–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL:HLT’08),</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1092" citStr="Shen et al., 2008" startWordPosition="155" endWordPosition="158">n-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set. 1 Introduction Dependency parsing has recently gained considerable interest because it is simple and fast, yet provides useful information for many NLP tasks (Shen et al., 2008; Councill et al., 2010). There are two main dependency parsing approaches (Nivre and McDonald, 2008). One is a transition-based approach that greedily searches for local optima (highest scoring transitions) and uses parse history as features to predict the next transition (Nivre, 2003). The other is a graph-based approach that searches for a global optimum (highest scoring tree) from a complete graph in which vertices represent word tokens and edges (directed and weighted) represent dependency relations (McDonald et al., 2005). Lately, the usefulness of the transition-based approach has drawn</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL:HLT’08), pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI’09),</booktitle>
<pages>1562--1567</pages>
<contexts>
<context position="16373" citStr="Titov et al., 2009" startWordPosition="2649" endWordPosition="2652">ttom speeds, and average the middle three. The loading times for machine learning models are excluded because they are independent from the parsing algorithms. The average parsing speeds are 2.86, 2.69, and 2.29 (in milliseconds) for Nivre, CN, and Our+, respectively. Our approach shows linear growth all along, even for the sentence groups where some approaches start showing curves. Sentence length Figure 1: Average parsing speeds with respect to sentence groups in Table 4. 3Later, ‘Merlo’ and ‘Bohnet” introduced more advanced systems, showing some improvements over their previous approaches (Titov et al., 2009; Bohnet, 2010). &lt; 10 &lt; 20 &lt; 30 &lt; 40 &lt; 50 &lt; 60 &lt; 70 1,415 2,289 1,714 815 285 72 18 Table 4: # of sentences in each group, extracted from both English/Czech evaluation sets. ‘&lt; n’ implies a group containing sentences whose lengths are less than n. We also measured average parsing speeds for ‘Our’, which showed a very similar growth to ‘Our+’. The average parsing speed of ‘Our’ was 2.20 ms; it performed slightly faster than ‘Our+’ because it skipped more nodes by performing more non-deterministic SHIFT’s, which may or may not have been correct decisions for the corresponding parsing states. It </context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI’09), pages 1562–1567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<pages>562--571</pages>
<contexts>
<context position="12688" citStr="Zhang and Clark (2008)" startWordPosition="2077" endWordPosition="2080">-sensitive. Furthermore, SEARN interpolates the current policy with the previous policy whereas we do not perform such interpolation. During decoding, SEARN generates a sequence of decisions and makes a final prediction. In our case, the decisions are predicted dependency relations and the final prediction is a dependency tree. SEARN has been successfully adapted to several NLP tasks such as named entity recognition, syntactic chunking, and POS tagging. To the best of our knowledge, this is the first time that this idea has been applied to transition-based parsing and shown promising results. Zhang and Clark (2008) suggested a transitionbased projective parsing algorithm that keeps B different sequences of parsing states and chooses the one with the best score. They use beam search and show a worst-case parsing complexity of O(n) given a fixed beam size. Similarly to ours, their learning mechanism using the structured perceptron algorithm involves training on automatically derived parsing states that closely resemble potential states encountered during decoding. 5 Experiments 5.1 Corpora and learning algorithm All models are trained and tested on English and Czech data using automatic lemmas, POS tags, </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’08), pages 562–571.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>