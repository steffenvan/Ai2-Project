<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036136">
<title confidence="0.999643">
A Connectionist Approach to Prepositional Phrase Attachment
for Real World Texts
</title>
<author confidence="0.973646">
Josep M. Sopena and Agusti LLoberas and Joan L. Moliner
</author>
<affiliation confidence="0.995076">
Laboratory of Neurocomputing
University of Barcelona
</affiliation>
<address confidence="0.968684">
Pg. Vall d&apos;Hebron, 171
08035 Barcelona (Spain)
</address>
<email confidence="0.978622">
e-mail: {pep , agusti , j oan}Qaxon . psi . ub . es
</email>
<sectionHeader confidence="0.997068" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999103153846154">
In this paper we describe a neural network-based
approach to prepositional phrase attachment disam-
biguation for real world texts. Although the use of
semantic classes in this task seems intuitively to be
adequate, methods employed to date have not used
them very effectively. Causes of their poor results
are discussed. Our model, which uses only classes,
scores appreciably better than the other class-based
methods which have been tested on the Wall Street
Journal corpus. To date, the best result obtained
using only classes was a score of 79.1%; we obtained
an accuracy score of 86.8%. This score is among the
best reported in the literature using this corpus.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994044395833333">
Structural ambiguity is one of the most serious prob-
lems faced by Natural Language Processing (NLP)
systems. It occurs when the syntactic information
does not suffice to make an assignment decision.
Prepositional phrase (PP) attachment is, perhaps,
the canonical case of structural ambiguity. What
kind of information should we use in order to solve
this ambiguity? In most cases, the information
needed comes from a local context, and the attach-
ment decision is based essentially on the relation-
ships existing between predicates and arguments,
what Katz y Fodor (1963) called selectional restric-
tions. For example, in the expression: (V accommo-
date) (NP Johnson&apos;s election) (PP as a director),
the PP is attached to the NP. However, in the ex-
pression: (V taking) (NP that news) (PP as a sign
to be cautious), the PP is attached to the verb. In
both expressions, the attachment site is decided on
the basis of verb and noun selectional restrictions.
In other cases, the information determining the PP
attachment comes from a global context. In this pa-
per we will focus on the disambiguation mechanism
based on selectional restrictions.
Previous work has shown that it is extremely diffi-
cult to build hand-made rule-based systems able to
deal with this kind of problem. Since such hand-
made systems proved unsuccessful, in recent years
two main methods have appeared capable of auto-
matic learning from tagged corpora: automatic rule
based methods and statistical methods. In this pa-
per we will show that, providing that the problem is
correctly approached, an NN can obtain better re-
sults than any of the methods used to date for PP
attachment disambiguation.
Statistical methods consider how a local context
can disambiguate PP attachment estimating the
probability from a corpus:
p(verb attachjv N P1 prep N P2)
Since an NP can be arbitrarily complex, the prob-
lem can be simplified by considering that only the
heads of the respective phrases are relevant when de-
ciding PP attachment. Therefore, ambiguity is re-
solved by means of a model that takes into account
only phrasal heads: p(verb attachlverb n1 prep n2).
There are two distinct methods for establishing the
relationships between the verb and its arguments:
methods using words (lexical preferences) and meth-
ods using semantic classes (selectional restrictions).
</bodyText>
<sectionHeader confidence="0.926756" genericHeader="method">
2 Using Words
</sectionHeader>
<bodyText confidence="0.996574238095238">
The attachment probability
p(verb attachlverb n1 prep n2)
should be computed. Due to the use of word co-
occurrence, this approach comes up against the se-
rious problem of data sparseness: the same 4-tuple
(v n1 prep n2) is hardly ever repeated across the
corpus even when the corpus is very large. Collins
and Brooks (1995) showed how serious this problem
can be: almost 95% of the 3097 4-tuples of their
test set do not appear in their 20801 training set 4-
tuples. In order to reduce data sparseness, Hindle
and Rooth (1993) simplified the context, by consid-
ering only verb-preposition (p(preplverb)), and nl-
preposition (p(prepin1)) co- occurrences. n2 was ig-
nored in spite of the fact that it may play an im-
portant role. In the test, attachment to verb was
decided if p(preplverb) &gt; p(prepinoun); otherwise
attachment to n1 is decided. Despite these limita-
tions, 80% of PP were correctly assigned.
Another method for reducing data sparseness has
been introduced recently by Collins and Brooks
</bodyText>
<page confidence="0.971766">
1233
</page>
<bodyText confidence="0.999838714285714">
(1995). These authors showed that the problem of
PP attachment ambiguity is analogous to n-gram
language models used in speech recognition, and
that one of the most common methods for language
modelling, the backed-off estimate, is also applica-
ble here. Using this method they obtained 84.5%
accuracy on WSJ data.
</bodyText>
<sectionHeader confidence="0.965136" genericHeader="method">
3 Using Classes
</sectionHeader>
<bodyText confidence="0.9999531875">
Working with words implies generating huge param-
eter spaces for which a vast amount of memory space
is required. NNs (probably like people) cannot deal
with such spaces. NNs are able to approximate
very complex functions, but they cannot memorize
huge probability look-up tables. The use of seman-
tic classes has been suggested as an alternative to
word co-occurrence. If we accept the idea that all
the words included in a given class milst have simi-
lar (attachment) behaviour, and that there are fewer
semantic classes than there are words, the problem
of data sparseness and memory space can be consid-
erably reduced.
Some of the class-based methods have used Word-
Net (Miller et al., 1993) to extract word classes.
WordNet is a semantic net in which each node
stands for a set of synonyms (synset), and domi-
nation stands for set inclusion (IS-A links). Each
synset represents an underlying concept. Table 1
shows three of the senses for the noun bank. Ta-
ble 2 shows the accuracy of the results reported
in previous work. The worst results were obtained
when only classes were used. It is reasonable to
assume a major source of knowledge humans use
to make attachment decisions is the semantic class
for the words involved and consequently there must
be a class-based method that provides better re-
sults. One possible reason for low performance using
classes is that WordNet is not an adequate hierarchy
since it is hand-crafted. Ratnaparkhi et al. (1994),
instead of using hand-crafted semantic classes, uses
word classes obtained via Mutual Information Clus-
tering (MIC) in a training corpus. Table 2 shows
that, again, worse results are obtained with classes.
A complementary explanation for the poor results
using classes would be that current methods do not
use class information very effectively for sev-
eral reasons: 1.-In WordNet, a particular sense be-
longs to several classes (a word belongs to a class if
it falls within the IS-A tree below that class), and so
determining an adequate level of abstraction is diffi-
cult. 2.- Most words have more than one sense. As
a result, before deciding attachment, it is first nec-
essary to determine the correct sense for each word.
3.- None of the preceding methods used classes for
verbs. 4.- For reasons of complexity, the complete
4-tuple has not been considered simultaneously ex-
cept in Ratnaparkhi et al.(1994). 5.- Classes of a
</bodyText>
<page confidence="0.870154">
1234
</page>
<bodyText confidence="0.984101946428572">
given sense and classes of different senses of different
words can have complex interactions and the pre-
ceding methods cannot take such interactions into
account.
4 Encoding and Network
Architecture.
Semantic classes were extracted from Wordnet 1.5.
In order to encode each word we did not use Word-
Net directly, but constructed a new hierarchy (a sub-
set of WordNet) including only the classes that cor-
responded to the words that belonged to the training
and test sets. We counted the number of times the
different semantic classes appear in the training and
test sets. The hierarchy was pruned taking these
statistics into account. Given a threshold h, classes
which appear less than h% were not included. In
this way we avoided having an excessive number of
classes in the definition of each word which may have
been insufficiently trained due to a lack of examples
in the training set. We call the new hierarchy ob-
tained after the cut WordNet&apos;. Due to the large
number of verb hierarchies, we made each verb lex-
icographical file into a tree by adding a root node
corresponding to the file name. According to Miller
et al. (1993), verb synsets are divided into 15 lex-
icographical files on the basis of semantic criteria.
Each root node of a verb hierarchy belongs to only
one lexicographical file. We made each old root node
hang from a new root node, the label of which was
the name of its lexicographical file. In addition, we
codified the name of the lexicographical file of the
verb itself.
There are essentially two alternative procedures
for using class information. The first one consists of
the simultaneous presentation of all the classes of all
the senses of all the words in the 4-tuple. The in-
put was divided into four slots representing the verb,
nl, prep, and n2 respectively. In slots n1 and n2,
each sense of the corresponding noun was encoded
using all the classes within the IS-A branch of the
WordNet&apos; hierarchy, from the corresponding hierar-
chy root node to its bottom-most node. In the verb
slot, the verb was encoded using the IS_A_WAY_OF
branches. There was a unit in the input for each
node of the WordNet subset. This unit was on if
it represented a semantic class to which one of the
senses of the word to be encoded belonged. As for
the output, there were only two units representing
whether the PP attached to the verb or not.
The second procedure consists of presenting all the
classes of each sense of each word serially. However,
the parallel procedure have the advantage that the
network can detect which classes are related with
which ones in the same slot and between slots. We
observed this advantage in preliminary studies.
Feedforward networks with one hidden layer and
</bodyText>
<tableCaption confidence="0.999566">
Table 1: WordNet information for the noun &apos;bank&apos;.
</tableCaption>
<table confidence="0.556780666666667">
Sense 1 group people —4 organization institution —+ financial_institut.
Sense 2 entity —4 object —4 artifact —&gt; facility —4 depository
Sense 3 entity —4 object —4 natural_object geological_formation —■ slope
</table>
<tableCaption confidence="0.966498">
Table 2: Test size and accuracy results reported in previous works. &apos;W&apos; denotes words only, &apos;C&apos; class only and
words-Fclasses.
</tableCaption>
<table confidence="0.998465375">
Author W C W-FC Classes Test size
Hindle and Rooth (93) 80 - - - 880
Resnik and Hearst(93) 81.6 79.3 83.9 WordNet 172
Resnik and Hearst (93) - - 75° WordNet 500
Ratnaparkhi et al. (94) 81.2 79.1 81.6 MIC 3097
Brill and Resnik (94) 80.8 - 81.8 WordNet 500
Collins and Brooks (95) 84.5 - - - 3097
Li and Abe (95) - 85.86 84.9 WordNet 172
</table>
<tableCaption confidence="0.8486485">
&apos;Accuracy obtained by Bill and Resnik (94) using Resnik&apos;s method on a larger test.
bThis accuracy is based on 66% coverage.
</tableCaption>
<bodyText confidence="0.997349181818182">
a full interconnectivity between layers were used in
all the experiments. The networks were trained with
backpropagation learning algorithm. The activation
function was the logistic function. The number of
hidden units ranged from 70 to 150. This network
was used for solving our classification problem: at-
tached to noun or attached to verb. The output
activation of this network represented the bayesian
posterior probability that the PP of the encoded sen-
tence attaches to the verb or not (Richard and Lipp-
mann (1991)).
</bodyText>
<sectionHeader confidence="0.9985735" genericHeader="method">
5 Training and Experimental
Results.
</sectionHeader>
<bodyText confidence="0.999869">
21418 examples of structures of the kind `VB Ni
PREP N2&apos; were extracted from the Penn-TreeBank
Wall Street Journal (Marcus et al. 1993). Word-
Net did not cover 100% of this material. Proper
names of people were substituted by the WordNet
class someone, company names by the class busi-
ness_organization, and prefixed nouns for their stem
(co-chairman chairman). 788 4-tuples were dis-
carded because of some of their words were not in
WordNet and could not be substituted. 20630 codi-
fied patterns were finally obtained: 12016 (58.25%)
with the PP attached to Ni, and 8614 (41.75%) to
VB.
We used the cross-validation method as a mea-
sure of a correct generalization. After encoding,
the 20630 patterns were divided into three subsets:
training set (18630 patterns), set A (1000 patterns),
and set B (1000 patterns). This method evaluated
performance (the number of attachment errors) on a
</bodyText>
<page confidence="0.794458">
1235
</page>
<bodyText confidence="0.999890693548388">
pattern set (validation set) after each complete pass
through the training data (epoch). Series of three
runs were performed that systematically varied the
random starting weights. In each run the networks
were trained for 40 epochs. In each run the weights
of the epoch having the smallest error with respect
to the validation set were stored. The weights corre-
sponding to the best result obtained on the valida-
tion test in the three runs were selected and used to
evaluate the performance in the test set. First, we
used set A as validation set and set B as test, and
afterwards we used set B as validation and set A as
test. This experiment was replicated with two new
partitions of the pattern set: two new training sets
(18630 patterns) and 4 new validation/test sets of
1000 patterns each.
Results showed in table 3 are the average accu-
racy over the six test sets (1000 patterns each) used.
We performed three series of runs that varied the in-
put encoding. In all these encodings, three tree cut
thresholds were used: 10%, 6% and 2%. The num-
ber of semantic classes in the input encoding ranged
from 139 (10% cut) to 475 (2%) In the first encod-
ing, the 4-tuple without extra information was used.
The results for this case are shown in the 4-tuple
column entry of table 3. In the second encoding,
we added the prepositions the verbs select for their
internal arguments, since English verbs with seman-
tic similarity could select different prepositions (for
example, accuse and blame). Verbs can be classi-
fied on the basis of the kind of prepositions they
select. Adding this classification to the WordNet&apos;
classes in the input encoding improved the results
(4-tuple+ column entry of table 3).
The 2% cut results were significantly better (p &lt;
0.02) than those of the 6% cut for 4-tuple and 4-
tuple+ encodings. Also, the results for the 4-tuple+
condition were significanly better (p &lt; 0.01).
For all simulations the momentum was 0.8, initial
weight range 0.1. No exhaustive parameter explo-
ration was carried out, so the results can still be
improved.
Some of the errors committed by the network can
be attributed to an inadequate class assignment by
WordNet. For instance, names of countries have
only one sense, that of location. This sense is not ap-
propriate in sentences like: Italy increased its sales
to Spain; locations do not sell or buy anything, and
the correct sense is social_group. Other mistakes
come from what are known as reporting and aspec-
tual verbs. For example in expressions like reported
injuries to employees or initiated talks with the Sovi-
ets the n1 has an argumental structure, and it is the
element that imposes selectional restrictions on the
PP. There is no good classification for these kinds
of verbs in WordNet. Finally, collocations or id-
ioms, which are very frequent, (e.g. take a look, pay
attention), are not considered lexical units in the
WSJ corpus. Their idiosyncratic behaviour intro-
duces noise in the selectional restrictions acquisition
process. Word-based models offer a clear advantage
over class-based methods in these cases.
</bodyText>
<sectionHeader confidence="0.99864" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999965923076923">
When sentences with PP attachment ambiguities
were presented to two human expert judges the mean
accuracy obtained was 93.2% using the whole sen-
tence and 88.2% using only the 4-tuple (Ratnaparkhi
et al., 1994). Our best result is 86.8%. This accu-
racy is close to human performance using the 4-tuple
alone. Collins and Brooks (1995) reported an accu-
racy of 84.5% using words alone, a better score than
those obtained with other methods tested on the
WSJ corpus. We used the same corpus as Collins
and Brooks (WSJ) and a similar sized training set.
They used a test set size of 3097 patterns, whereas
we used 6000. Due to this size, the differences be-
tween both results (84.5% and 86.81%) were proba-
bly significant. Note that our results were obtained
using only class information. Ratnaparkhi et al.
(1994)&apos;s results are the best reported so far using
only classes (for 100% coverage): 79.1%. From these
results we can conclude that improvements in the
syntactic disambiguation problem will come not only
from the availability of better hierarchies of classes
but also from methods that use them better. NNs
seem especially well designed to use them effectively.
How do we account for the improved results?
First, we used verb class information. Given the
set of words in the 4-tuple and a way to repre-
</bodyText>
<page confidence="0.939208">
1236
</page>
<bodyText confidence="0.9999730625">
sent senses and semantic class information, a syn-
tactic disambiguation system (SDS) must find some
regularities between the co-occurrence of classes
and the attachment point. Presenting all of the
classes of all the senses of the complete 4-tuple
simultaneously, assuming that the training set is
adequate, the network can detect which classes
(and consequently which senses) are related with
which others. As we have said, due to its com-
plexity, current methods do not consider the com-
plete 4-tuple simultaneously. For example, Li
and Abe (1995) use p(verb attachlv prep n2) or
p(verb attachlv n1 prep)). The task of selecting
which of the senses contributes to making the cor-
rect attachment could be difficult if the whole 4-
tuple is not simultaneously present. A verb has
many senses, and each one could have a different
argumental structure. In the selection of the cor-
rect sense of the verb, the role of the object (n1)
is very important. Deciding the attachment site by
computing p(verb attach Iv prep n2) would be inad-
equate. It is also inadequate to omit n2. Rule based
approaches also come up against this problem. In
Brill and Resnik (1994), for instance, for reasons of
run-time efficiency and complexity, rules regarding
the classes of both n1 and n2 were not permitted.
Using a parallel presentation it is also possible to
detect complex interactions between the classes of
a particular sense (for example, exceptions) or the
classes of different senses that cannot be detected
in the case of current statistical methods. We have
detected these interactions in studies on word sense
disambiguation we are currently carrying out. For
example, the behavior of verbs which have the senses
of process and state differs from that of verbs which
have the sense of process but not of state, and vicev-
ersa.
A parallel presentation (of classes as well of senses)
gives rise to a highly complex input. A very impor-
tant characteristic of neural networks is their capa-
bility of dealing with multidimensional inputs (Bar-
ron, 1993). They can compute very complex statis-
tical functions and they are model free. Compared
to the current methods used by the statistical or
rule-based approaches to natural language process-
ing, NNs offer the possibility of dealing with a much
more complex approach (non-linear and high dimen-
sional).
</bodyText>
<sectionHeader confidence="0.961245" genericHeader="references">
References.
</sectionHeader>
<reference confidence="0.95609275">
Barron, A. (1993). Universal Approximation Bounds for
Superposition of a Sigmoidal Function. IEEE Transac-
tions on Information Theory, 39:930-945.
Brill, E. &amp; Resnik, P. (1994). A Rule-Based Approach
to Prepositional Phrase Attachment Disambiguation. In
Proceedings of the Fifteenth International Conferences
on Computational Linguistics (COLING-94).
Collins, M. &amp; Brooks, J. (1995). Prepositional Phrase
</reference>
<tableCaption confidence="0.989118">
Table 3: Accuracy results for different input encoding and tree cuts.
</tableCaption>
<table confidence="0.90170925">
Cut 4-tuple 4-tuple+
10% 83.17 ±0.9 85.15 ±0.8
6% 84.07 ±0.7 85.32 ±0.9
2% 85.12 ±1.0 86.81 ±0.9
</table>
<reference confidence="0.99819928">
attachment. In Proceedings of the 3rd Workshop on Very
Large Corpora.
Hindle, D. &amp; Rooth, M. (1993). Structural Ambigu-
ity and Lexical Relations. Computational Linguistics,
19:103-120.
Katz, J. &amp; Fodor, J. (1963). The Structure of Seman-
tic Theory. Language, 39: 170-210.
Li, H. &amp; Abe, N. (1995). Generalizing Case Frames us-
ing a Thesaurus and the MDL Principle. In Proceedings
of the International Workshop on Parsing Technology.
Marcus, M., Santorini, B. &amp; Marcinkiewicz, M.
(1993). Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics, 19:313-
330.
Miller, G., Beckwith, R., Felbaum, C., Gross, D. &amp;
Miller, K. (1993). Introduction to WordNet: An On-
line Lexical Database. Anonymous FTP, internet: clar-
ity.princeton.edu.
Ratnaparkhi, A., Reynar, J. &amp; Roukos, S. (1994). A
Maximum Entropy Model for Prepositional Phrase At-
tachment. In Proceedings of the A RPA Workshop on
Human Language Technology.
Resnik, P. &amp; Hearst, M. (1993). Syntactic Ambiguity
and Conceptual Relations. In Proceedings of the ACL
Workshop on Very Large Corpora.
</reference>
<page confidence="0.992294">
1237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919515">
<title confidence="0.999812">A Connectionist Approach to Prepositional Phrase Attachment for Real World Texts</title>
<author confidence="0.999688">M Sopena LLoberas L Moliner</author>
<affiliation confidence="0.9995555">Laboratory of Neurocomputing University of Barcelona</affiliation>
<address confidence="0.9960775">Pg. Vall d&apos;Hebron, 171 08035 Barcelona (Spain)</address>
<email confidence="0.945569">,agusti,joan}Qaxon.psi.ub.es</email>
<abstract confidence="0.997994642857143">In this paper we describe a neural network-based approach to prepositional phrase attachment disambiguation for real world texts. Although the use of semantic classes in this task seems intuitively to be adequate, methods employed to date have not used them very effectively. Causes of their poor results are discussed. Our model, which uses only classes, scores appreciably better than the other class-based methods which have been tested on the Wall Street Journal corpus. To date, the best result obtained using only classes was a score of 79.1%; we obtained an accuracy score of 86.8%. This score is among the best reported in the literature using this corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Barron</author>
</authors>
<title>Universal Approximation Bounds for Superposition of a Sigmoidal Function.</title>
<date>1993</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>39--930</pages>
<marker>Barron, 1993</marker>
<rawString>Barron, A. (1993). Universal Approximation Bounds for Superposition of a Sigmoidal Function. IEEE Transactions on Information Theory, 39:930-945.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conferences on Computational Linguistics (COLING-94).</booktitle>
<contexts>
<context position="17574" citStr="Brill and Resnik (1994)" startWordPosition="2943" endWordPosition="2946"> Li and Abe (1995) use p(verb attachlv prep n2) or p(verb attachlv n1 prep)). The task of selecting which of the senses contributes to making the correct attachment could be difficult if the whole 4- tuple is not simultaneously present. A verb has many senses, and each one could have a different argumental structure. In the selection of the correct sense of the verb, the role of the object (n1) is very important. Deciding the attachment site by computing p(verb attach Iv prep n2) would be inadequate. It is also inadequate to omit n2. Rule based approaches also come up against this problem. In Brill and Resnik (1994), for instance, for reasons of run-time efficiency and complexity, rules regarding the classes of both n1 and n2 were not permitted. Using a parallel presentation it is also possible to detect complex interactions between the classes of a particular sense (for example, exceptions) or the classes of different senses that cannot be detected in the case of current statistical methods. We have detected these interactions in studies on word sense disambiguation we are currently carrying out. For example, the behavior of verbs which have the senses of process and state differs from that of verbs whi</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Brill, E. &amp; Resnik, P. (1994). A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation. In Proceedings of the Fifteenth International Conferences on Computational Linguistics (COLING-94).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional Phrase attachment.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="3634" citStr="Collins and Brooks (1995)" startWordPosition="589" endWordPosition="592"> model that takes into account only phrasal heads: p(verb attachlverb n1 prep n2). There are two distinct methods for establishing the relationships between the verb and its arguments: methods using words (lexical preferences) and methods using semantic classes (selectional restrictions). 2 Using Words The attachment probability p(verb attachlverb n1 prep n2) should be computed. Due to the use of word cooccurrence, this approach comes up against the serious problem of data sparseness: the same 4-tuple (v n1 prep n2) is hardly ever repeated across the corpus even when the corpus is very large. Collins and Brooks (1995) showed how serious this problem can be: almost 95% of the 3097 4-tuples of their test set do not appear in their 20801 training set 4- tuples. In order to reduce data sparseness, Hindle and Rooth (1993) simplified the context, by considering only verb-preposition (p(preplverb)), and nlpreposition (p(prepin1)) co- occurrences. n2 was ignored in spite of the fact that it may play an important role. In the test, attachment to verb was decided if p(preplverb) &gt; p(prepinoun); otherwise attachment to n1 is decided. Despite these limitations, 80% of PP were correctly assigned. Another method for red</context>
<context position="15448" citStr="Collins and Brooks (1995)" startWordPosition="2583" endWordPosition="2586"> frequent, (e.g. take a look, pay attention), are not considered lexical units in the WSJ corpus. Their idiosyncratic behaviour introduces noise in the selectional restrictions acquisition process. Word-based models offer a clear advantage over class-based methods in these cases. 6 Discussion When sentences with PP attachment ambiguities were presented to two human expert judges the mean accuracy obtained was 93.2% using the whole sentence and 88.2% using only the 4-tuple (Ratnaparkhi et al., 1994). Our best result is 86.8%. This accuracy is close to human performance using the 4-tuple alone. Collins and Brooks (1995) reported an accuracy of 84.5% using words alone, a better score than those obtained with other methods tested on the WSJ corpus. We used the same corpus as Collins and Brooks (WSJ) and a similar sized training set. They used a test set size of 3097 patterns, whereas we used 6000. Due to this size, the differences between both results (84.5% and 86.81%) were probably significant. Note that our results were obtained using only class information. Ratnaparkhi et al. (1994)&apos;s results are the best reported so far using only classes (for 100% coverage): 79.1%. From these results we can conclude that</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Collins, M. &amp; Brooks, J. (1995). Prepositional Phrase attachment. In Proceedings of the 3rd Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--103</pages>
<contexts>
<context position="3837" citStr="Hindle and Rooth (1993)" startWordPosition="626" endWordPosition="629">(lexical preferences) and methods using semantic classes (selectional restrictions). 2 Using Words The attachment probability p(verb attachlverb n1 prep n2) should be computed. Due to the use of word cooccurrence, this approach comes up against the serious problem of data sparseness: the same 4-tuple (v n1 prep n2) is hardly ever repeated across the corpus even when the corpus is very large. Collins and Brooks (1995) showed how serious this problem can be: almost 95% of the 3097 4-tuples of their test set do not appear in their 20801 training set 4- tuples. In order to reduce data sparseness, Hindle and Rooth (1993) simplified the context, by considering only verb-preposition (p(preplverb)), and nlpreposition (p(prepin1)) co- occurrences. n2 was ignored in spite of the fact that it may play an important role. In the test, attachment to verb was decided if p(preplverb) &gt; p(prepinoun); otherwise attachment to n1 is decided. Despite these limitations, 80% of PP were correctly assigned. Another method for reducing data sparseness has been introduced recently by Collins and Brooks 1233 (1995). These authors showed that the problem of PP attachment ambiguity is analogous to n-gram language models used in speec</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Hindle, D. &amp; Rooth, M. (1993). Structural Ambiguity and Lexical Relations. Computational Linguistics, 19:103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Katz</author>
<author>J Fodor</author>
</authors>
<title>The Structure of Semantic Theory.</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<pages>170--210</pages>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, J. &amp; Fodor, J. (1963). The Structure of Semantic Theory. Language, 39: 170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Generalizing Case Frames using a Thesaurus and the MDL Principle.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology.</booktitle>
<contexts>
<context position="16969" citStr="Li and Abe (1995)" startWordPosition="2836" endWordPosition="2839">ormation. Given the set of words in the 4-tuple and a way to repre1236 sent senses and semantic class information, a syntactic disambiguation system (SDS) must find some regularities between the co-occurrence of classes and the attachment point. Presenting all of the classes of all the senses of the complete 4-tuple simultaneously, assuming that the training set is adequate, the network can detect which classes (and consequently which senses) are related with which others. As we have said, due to its complexity, current methods do not consider the complete 4-tuple simultaneously. For example, Li and Abe (1995) use p(verb attachlv prep n2) or p(verb attachlv n1 prep)). The task of selecting which of the senses contributes to making the correct attachment could be difficult if the whole 4- tuple is not simultaneously present. A verb has many senses, and each one could have a different argumental structure. In the selection of the correct sense of the verb, the role of the object (n1) is very important. Deciding the attachment site by computing p(verb attach Iv prep n2) would be inadequate. It is also inadequate to omit n2. Rule based approaches also come up against this problem. In Brill and Resnik (</context>
</contexts>
<marker>Li, Abe, 1995</marker>
<rawString>Li, H. &amp; Abe, N. (1995). Generalizing Case Frames using a Thesaurus and the MDL Principle. In Proceedings of the International Workshop on Parsing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--313</pages>
<contexts>
<context position="11274" citStr="Marcus et al. 1993" startWordPosition="1882" endWordPosition="1885">he networks were trained with backpropagation learning algorithm. The activation function was the logistic function. The number of hidden units ranged from 70 to 150. This network was used for solving our classification problem: attached to noun or attached to verb. The output activation of this network represented the bayesian posterior probability that the PP of the encoded sentence attaches to the verb or not (Richard and Lippmann (1991)). 5 Training and Experimental Results. 21418 examples of structures of the kind `VB Ni PREP N2&apos; were extracted from the Penn-TreeBank Wall Street Journal (Marcus et al. 1993). WordNet did not cover 100% of this material. Proper names of people were substituted by the WordNet class someone, company names by the class business_organization, and prefixed nouns for their stem (co-chairman chairman). 788 4-tuples were discarded because of some of their words were not in WordNet and could not be substituted. 20630 codified patterns were finally obtained: 12016 (58.25%) with the PP attached to Ni, and 8614 (41.75%) to VB. We used the cross-validation method as a measure of a correct generalization. After encoding, the 20630 patterns were divided into three subsets: train</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B. &amp; Marcinkiewicz, M. (1993). Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Felbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An Online Lexical Database. Anonymous FTP, internet: clarity.princeton.edu.</title>
<date>1993</date>
<contexts>
<context position="5332" citStr="Miller et al., 1993" startWordPosition="870" endWordPosition="873">a vast amount of memory space is required. NNs (probably like people) cannot deal with such spaces. NNs are able to approximate very complex functions, but they cannot memorize huge probability look-up tables. The use of semantic classes has been suggested as an alternative to word co-occurrence. If we accept the idea that all the words included in a given class milst have similar (attachment) behaviour, and that there are fewer semantic classes than there are words, the problem of data sparseness and memory space can be considerably reduced. Some of the class-based methods have used WordNet (Miller et al., 1993) to extract word classes. WordNet is a semantic net in which each node stands for a set of synonyms (synset), and domination stands for set inclusion (IS-A links). Each synset represents an underlying concept. Table 1 shows three of the senses for the noun bank. Table 2 shows the accuracy of the results reported in previous work. The worst results were obtained when only classes were used. It is reasonable to assume a major source of knowledge humans use to make attachment decisions is the semantic class for the words involved and consequently there must be a class-based method that provides b</context>
<context position="8157" citStr="Miller et al. (1993)" startWordPosition="1349" endWordPosition="1352">ent semantic classes appear in the training and test sets. The hierarchy was pruned taking these statistics into account. Given a threshold h, classes which appear less than h% were not included. In this way we avoided having an excessive number of classes in the definition of each word which may have been insufficiently trained due to a lack of examples in the training set. We call the new hierarchy obtained after the cut WordNet&apos;. Due to the large number of verb hierarchies, we made each verb lexicographical file into a tree by adding a root node corresponding to the file name. According to Miller et al. (1993), verb synsets are divided into 15 lexicographical files on the basis of semantic criteria. Each root node of a verb hierarchy belongs to only one lexicographical file. We made each old root node hang from a new root node, the label of which was the name of its lexicographical file. In addition, we codified the name of the lexicographical file of the verb itself. There are essentially two alternative procedures for using class information. The first one consists of the simultaneous presentation of all the classes of all the senses of all the words in the 4-tuple. The input was divided into fou</context>
</contexts>
<marker>Miller, Beckwith, Felbaum, Gross, Miller, 1993</marker>
<rawString>Miller, G., Beckwith, R., Felbaum, C., Gross, D. &amp; Miller, K. (1993). Introduction to WordNet: An Online Lexical Database. Anonymous FTP, internet: clarity.princeton.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of the A RPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="6097" citStr="Ratnaparkhi et al. (1994)" startWordPosition="1000" endWordPosition="1003">inclusion (IS-A links). Each synset represents an underlying concept. Table 1 shows three of the senses for the noun bank. Table 2 shows the accuracy of the results reported in previous work. The worst results were obtained when only classes were used. It is reasonable to assume a major source of knowledge humans use to make attachment decisions is the semantic class for the words involved and consequently there must be a class-based method that provides better results. One possible reason for low performance using classes is that WordNet is not an adequate hierarchy since it is hand-crafted. Ratnaparkhi et al. (1994), instead of using hand-crafted semantic classes, uses word classes obtained via Mutual Information Clustering (MIC) in a training corpus. Table 2 shows that, again, worse results are obtained with classes. A complementary explanation for the poor results using classes would be that current methods do not use class information very effectively for several reasons: 1.-In WordNet, a particular sense belongs to several classes (a word belongs to a class if it falls within the IS-A tree below that class), and so determining an adequate level of abstraction is difficult. 2.- Most words have more th</context>
<context position="15326" citStr="Ratnaparkhi et al., 1994" startWordPosition="2562" endWordPosition="2565">e PP. There is no good classification for these kinds of verbs in WordNet. Finally, collocations or idioms, which are very frequent, (e.g. take a look, pay attention), are not considered lexical units in the WSJ corpus. Their idiosyncratic behaviour introduces noise in the selectional restrictions acquisition process. Word-based models offer a clear advantage over class-based methods in these cases. 6 Discussion When sentences with PP attachment ambiguities were presented to two human expert judges the mean accuracy obtained was 93.2% using the whole sentence and 88.2% using only the 4-tuple (Ratnaparkhi et al., 1994). Our best result is 86.8%. This accuracy is close to human performance using the 4-tuple alone. Collins and Brooks (1995) reported an accuracy of 84.5% using words alone, a better score than those obtained with other methods tested on the WSJ corpus. We used the same corpus as Collins and Brooks (WSJ) and a similar sized training set. They used a test set size of 3097 patterns, whereas we used 6000. Due to this size, the differences between both results (84.5% and 86.81%) were probably significant. Note that our results were obtained using only class information. Ratnaparkhi et al. (1994)&apos;s r</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Ratnaparkhi, A., Reynar, J. &amp; Roukos, S. (1994). A Maximum Entropy Model for Prepositional Phrase Attachment. In Proceedings of the A RPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
<author>M Hearst</author>
</authors>
<title>Syntactic Ambiguity and Conceptual Relations.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL Workshop on Very Large Corpora.</booktitle>
<marker>Resnik, Hearst, 1993</marker>
<rawString>Resnik, P. &amp; Hearst, M. (1993). Syntactic Ambiguity and Conceptual Relations. In Proceedings of the ACL Workshop on Very Large Corpora.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>