<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.340242">
<title confidence="0.996013">
UWM: A Simple Baseline Method for Identifying Attributes of Disease
and Disorder Mentions in Clinical Text
</title>
<author confidence="0.997593">
Omid Ghiasvand Rohit J. Kate
</author>
<affiliation confidence="0.979739">
University of Wisconsin-Milwaukee University of Wisconsin-Milwaukee
Milwaukee, Wisconsin Milwaukee, Wisconsin
</affiliation>
<email confidence="0.996444">
ghiasva2@uwm.edu katerj@uwm.edu
</email>
<sectionHeader confidence="0.995584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995304">
In this paper the system that was developed
by Team UWM for the Task 14 of SemEval
2015 competition is described. Task 14
included two tasks: Task 1 was identification
of disorder mentions and their normalization,
and Task 2 was identification of the following
attributes for disorder mentions: the CUI of
the disorder, negation indicator, subject,
uncertainty indicator, course, severity,
conditional, generic indicator, and body
location. For Task 1, an earlier system was
applied that uses Conditional Random Fields
(CRFs) for disorder recognition and learned
edit distance patterns for normalization. Task
2 was implemented by a simple method that
finds the attribute terms around the disease
mentions by matching them in the training
data. Among all participants Team UWM was
ranked fourth in Task 1, fourth in Task 2A
(over gold-standard mentions) and third in
Task 2B (over extracted mentions).
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932756097561">
Automated extraction tools are crucial for
managing huge amount of clinical texts. These
tools have the potential to enable many automated
applications in healthcare. The Task 14 of
SemEval 2015 was designed to serve as a platform
for evaluating one such extraction tool. Its Task 1
involved extracting and normalizing disorder
mentions from clinical text and its Task 2 involved
assertion identification for the mentions.
Task 1 is challenging because there is a lot of
variability in which diseases and disorders are
mentioned in clinical text and hence a pre-defined
list of mentions is not sufficient to extract them.
The task also required normalizing the extracted
mentions by mapping them to UMLS CUIs if they
exist in the SNOMED-CT part of UMLS and are
marked as disease/disorder, otherwise they were to
be declared as “CUI-less.” This normalization
process is also challenging because disorder names
are frequently mentioned in modified forms which
prevents them from exactly matching the concepts
in UMLS. Task 2 required finding certain
attributes for the mentions and finding the spans of
these attributes in text. This task is also
challenging due to the variability in which
attributes are attributed to disease and disorder
mentions in clinical text.
Our team, UWM, participated in both Task 1
and Task 2. For Task 1, we used the same system
that we had previously developed for the Task 7 of
SemEval 2014 (Ghiasvand and Kate 2014). For
Task 2, we used a simple method that finds
attributes of mentions by first collecting lists of
attribute terms from the training data and then
matching in this list. The nearest attribute terms to
a mention are assigned to that mention. The
attribute terms are normalized by finding their
normalized values in the training data. Despite
being simple, this method gave competitive results.
The methods used in this paper are described in
more details in the next section.
</bodyText>
<page confidence="0.988258">
385
</page>
<note confidence="0.608455">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 385–388,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.989587" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.961963">
2.1 Task 1
</subsectionHeader>
<bodyText confidence="0.999978318181818">
We briefly describe the system we had developed
for Task 7 of SemEval 2014 (Ghiasvand and Kate
2014) which we used for Task 1. We treated
disorder mention extraction as a standard sequence
labeling task with “BIO” (Begin, Inside, Outside)
labeling scheme. The model was trained using
Conditional Random Fields (Lafferty et al., 2001)
with various types of lexical and semantic features
that included MetaMap (Aronson and Lang 2010)
matches. These features are fully described in
(Ghiasvand, 2014). This model was also inherently
capable of extracting discontinuous disorder
mentions. To normalize disorder mentions, our
system first looked for exact matches with disorder
mentions in the training data and then in the
UMLS. If no exact match was found, then suitable
variations of the disorder mentions were generated
based on possible variations of disorder mentions
learned from UMLS synonyms. These variations
were learned in the form of edit distance patterns
(Levenshtein 1966) using a novel method
described in (Ghiasvand and Kate 2014).
</bodyText>
<subsectionHeader confidence="0.985689">
2.2 Task 2
</subsectionHeader>
<bodyText confidence="0.999793724137931">
In this task, attributes related to disease or disorder
mentions were to be identified along with their
normalized values and spans in the text
(Bodenreider, 2003). There were nine attributes
related to each disorder mention for this task which
were: the CUI of the disorder (same as Task 1),
negation indicator, subject, uncertainty indicator,
course, severity, conditional, generic indicator, and
body location.
For identifying CUI attribute, we used the same
normalization method that we had used for Task 1.
For identifying the rest of the attributes, we used a
simple matching method based on the training data
for Task 2. The method first collects a list of
attribute terms from the training data for each
attribute type. For example, if “likely arising
from”, “lower suspicion of”, and “possibly
secondary” are marked as uncertainty terms in the
training data then they will be included in our list
of attribute terms for uncertainty. Table 1 lists the
number of attribute terms thus collected from the
training data for each of the attribute type. The
only attribute that has many more values than other
attributes is body location. For this attribute we
used not only training data but also UMLS matches
of body locations. Our training dataset consisted of
combined training and development dataset parts,
but when we collected these terms from only the
training part, we found that a majority of these
match in the development part. Thus we
determined that only a small list of terms are
frequently used to indicate most of the attributes of
disease and disorder mentions and decided to use
the simple matching method.
Our method identifies attributes of disease and
disorder mentions as follows. Using the list of
attribute terms, it first identifies attribute terms in
the same sentence in which the mention is
included. For each attribute type, the nearest
attribute term (if present) is associated with the
mention. The normalized value of the attribute is
then simply obtained from the training data. For
example the term “increasingly” in the course
attribute type has normalized value “increased” in
the training data, and the term “maternal aunt” in
the subject attribute type has the normalized value
“family_member”. Hence if “increasingly” is the
course attribute term found nearest to a disease
mention in the test data then its course attribute
will be assigned the value “increased”. Similarly if
“maternal aunt” is found as the nearest subject
attribute term then its value will be assigned as
“family_member”.
Task 2 had two subtasks. In Subtask 2A, gold-
standard disease and disorder mentions were
provided and in Subtask 2B the mentions were to
be first extracted by the system, hence it combined
Task 1 and Subtask 2A.
</bodyText>
<table confidence="0.9990927">
Attribute Number of attribute
terms in training data
Conditional (CND) 154
Course (COU) 168
Generic (GEN) 45
Negation (NEG) 139
Severity (SEV) 92
Subject (SUB) 33
Uncertainty (UNC) 295
Body Location (BL) 1108
</table>
<tableCaption confidence="0.9940785">
Table 1: Number of attribute terms for each attribute
type in the training data.
</tableCaption>
<page confidence="0.99889">
386
</page>
<sectionHeader confidence="0.999897" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9995816875">
The training, development and test datasets of
SemEval 2015 Task 14 had 298, 133 and 100
clinical notes respectively. We formed our training
dataset by combining training and development
datasets. The clinical notes contained different
types of notes including de-identified discharge
summaries, electrocardiogram, echocardiogram
and radiology reports (Pradhan et al., 2013). The
extraction and normalization performance in Task
1 was evaluated in terms of precision, recall and F-
measure for strict (exact boundaries) and relaxed
(overlapping boundaries) settings. Table 2 shows
the results of this task. In this task, based on
relaxed F-score, we got second rank, and based on
strict F-score we got fourth rank considering only
the best run of each participating team.
</bodyText>
<table confidence="0.999573">
Precision Recall F-score
Strict 0.773 0.699 0.734
Relaxed 0.809 0.731 0.768
</table>
<tableCaption confidence="0.9746325">
Table 2: Results of Task1 (mention extraction and
normalization).
</tableCaption>
<bodyText confidence="0.999898875">
For the Task 2A, unweighted and weighted
accuracies were used as evaluation measures. For
each disorder, a per-disorder, unweighted accuracy
is computed, which represents the ability to
identify all the slots for that disorder. The
unweighted accuracy is the average of the per-
disorder unweighted accuracy over all the
disorders in the test set. For each disorder, a
weighted per-disorder accuracy is computed,
which represents the ability to identify all the slots
for that disorder.
For Task 2B, the following evaluation measures
were used: F-score for span identification,
unweighted accuracy (which is same as the
unweighted accuracy described in Task 2A
computed over the true-positive identified
disorders), and weighted accuracy (which is same
as the weighted accuracy described in Task 2A
computed over the true-positive identified
disorders).
In Table 3 and 4, the results of these two
subtasks are shown. Table 5 shows the results
separately for each attribute type for both the
subtasks. In Task 2A, except for the body location
attribute our method got above eighty percent
accuracy on all other attributes and above ninety
percent on three of them. We also want point out
that for the attribute type CUI we got 0.911
accuracy in Task 2A which is only slightly behind
the best accuracy of 0.918 got by another team.
The reason our system got a very low accuracy
for the body location attribute is because we forgot
to include the CUI values for this attribute during
the competition. This then also adversely affected
our overall performance scores. Overall, in Task
2A our team ranked fourth and in Task 2B our
team ranked third considering the best run of each
participating team.
Our method for Task 2 was found to be
competitive despite being very simple. For
example, this simple matching scheme got 92.4%
accuracy for negation attribute while the best team
got 97.5% accuracy in Task 2A. Hence this method
forms a very good baseline for comparing more
sophisticated methods. It can also serve as a
method that provides potential attributes which
then can be tested and filtered by machine learning
methods.
</bodyText>
<table confidence="0.977468333333333">
F-Score Accuracy F*A Weighted- F*WA
Accuracy
1.00 0.859 0.859 0818 0.818
</table>
<tableCaption confidence="0.990707">
Table 3: Results of Task 2A.
</tableCaption>
<table confidence="0.999458666666667">
F-Score Accuracy F*A Weighted- F*WA
Accuracy
0.893 0.852 0.761 0.798 0.713
</table>
<tableCaption confidence="0.993137">
Table 4: Results of Task 2B.
</tableCaption>
<table confidence="0.999854272727273">
Attribute Accuracy Accuracy
(Task 2A) (Task 2B)
BL 0.531 0.551
CUI 0.911 0.858
CND 0838 0.839
COU 0.802 0.793
GEN 0.836 0.845
NEG 0.924 0.931
SEV 0.895 0.905
SUB 0.933 0.929
UNC 0.831 0.837
</table>
<tableCaption confidence="0.967076">
Table 5: Accuracy for each attribute type in Task 2A
and Task 2B.
</tableCaption>
<sectionHeader confidence="0.984113" genericHeader="evaluation">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.99982875">
We participated in Task 14 of SemEval 2015
which involved disorder mention extraction,
normalization, and attribute identification. Our
system used conditional random fields to extract
</bodyText>
<page confidence="0.993801">
387
</page>
<bodyText confidence="0.99990825">
disorder mentions and edit distance patterns for
normalization of the extracted mentions. For
identifying attributes, we used a simple matching
based method using the training data. Our team
preformed competitively on all the subtasks. In
future, we plan to combine machine learning
methods with our simple matching method for
attribute identification.
</bodyText>
<sectionHeader confidence="0.936023" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9958236">
This work was supported by grant UL1RR031973
from the Clinical and Translational Science Award
(CTSA) program of the National Center for
Research Resources and the National Center for
Advancing Translational Sciences.
</bodyText>
<sectionHeader confidence="0.999282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786666666667">
Aronson A. R., and Lang F. M. An overview of
MetaMap: historical perspectives and recent
advances. Journal of American Medical
Informatics Association. 2010;17(3):229–36.
Bodenreider, O. and McCray, A. 2003. Exploring
semantic groups through visual approaches.
Journal of Biomedical Informatics, 36(2203): pp.
414-432.
Omid Ghiasvand, 2014. Disease Name Extraction
from Clinical Text Using Conditional Random
Fields, Thesis and Dissertation, University of
Wisconsin-Milwaukee, Milwaukee, USA.
Omid Ghiasvand and Rohit J. Kate, 2014. UWM:
Disorder Mention Extraction from Clinical Text
Using CRFs and Normalization Using Learned
Edit Distance Patterns, in Proceeding of the
Eight International Workshop on Semantic
Evaluations (SemEval 2014), pages 828-832,
Dublin, Ireland.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and
labeling sequence data. In Proceedings of 18th
International Conference on Machine Learning
(ICML-2001), pages 282–289, Williamstown,
MA.
Vladimir I Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and
reversals. In Soviet physics doklady, volume 10,
page 707.
Sameer Pradhan, Noemie Elhadad, B South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, W Chapman, and Guergana Savova.
2013. Task 1: ShARe/CLEF eHealth Evaluation
Lab 2013. Online Working Notes of CLEF,
CLEF, 230.
</reference>
<page confidence="0.998244">
388
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.627618">
<title confidence="0.9966605">UWM: A Simple Baseline Method for Identifying Attributes of and Disorder Mentions in Clinical Text</title>
<author confidence="0.999408">Omid Ghiasvand Rohit J Kate</author>
<affiliation confidence="0.999906">University of Wisconsin-Milwaukee University of Wisconsin-Milwaukee</affiliation>
<address confidence="0.951515">Milwaukee, Wisconsin Milwaukee, Wisconsin</address>
<email confidence="0.999842">katerj@uwm.edu</email>
<abstract confidence="0.982539727272727">In this paper the system that was developed by Team UWM for the Task 14 of SemEval 2015 competition is described. Task 14 included two tasks: Task 1 was identification of disorder mentions and their normalization, and Task 2 was identification of the following attributes for disorder mentions: the CUI of the disorder, negation indicator, subject, uncertainty indicator, course, severity, conditional, generic indicator, and body location. For Task 1, an earlier system was applied that uses Conditional Random Fields (CRFs) for disorder recognition and learned edit distance patterns for normalization. Task 2 was implemented by a simple method that finds the attribute terms around the disease mentions by matching them in the training data. Among all participants Team UWM was ranked fourth in Task 1, fourth in Task 2A (over gold-standard mentions) and third in Task 2B (over extracted mentions).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>A R Aronson</author>
<author>F M Lang</author>
</authors>
<title>An overview of MetaMap: historical perspectives and recent advances.</title>
<journal>Journal of American Medical Informatics Association.</journal>
<volume>17</volume>
<issue>3</issue>
<marker>Aronson, Lang, </marker>
<rawString>Aronson A. R., and Lang F. M. An overview of MetaMap: historical perspectives and recent advances. Journal of American Medical Informatics Association. 2010;17(3):229–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bodenreider</author>
<author>A McCray</author>
</authors>
<title>Exploring semantic groups through visual approaches.</title>
<date>2003</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>36</volume>
<issue>2203</issue>
<pages>414--432</pages>
<marker>Bodenreider, McCray, 2003</marker>
<rawString>Bodenreider, O. and McCray, A. 2003. Exploring semantic groups through visual approaches. Journal of Biomedical Informatics, 36(2203): pp. 414-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omid Ghiasvand</author>
</authors>
<title>Disease Name Extraction from Clinical Text Using Conditional Random Fields, Thesis and Dissertation,</title>
<date>2014</date>
<institution>University of Wisconsin-Milwaukee,</institution>
<location>Milwaukee, USA.</location>
<contexts>
<context position="3831" citStr="Ghiasvand, 2014" startWordPosition="595" endWordPosition="596"> pages 385–388, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Methods 2.1 Task 1 We briefly describe the system we had developed for Task 7 of SemEval 2014 (Ghiasvand and Kate 2014) which we used for Task 1. We treated disorder mention extraction as a standard sequence labeling task with “BIO” (Begin, Inside, Outside) labeling scheme. The model was trained using Conditional Random Fields (Lafferty et al., 2001) with various types of lexical and semantic features that included MetaMap (Aronson and Lang 2010) matches. These features are fully described in (Ghiasvand, 2014). This model was also inherently capable of extracting discontinuous disorder mentions. To normalize disorder mentions, our system first looked for exact matches with disorder mentions in the training data and then in the UMLS. If no exact match was found, then suitable variations of the disorder mentions were generated based on possible variations of disorder mentions learned from UMLS synonyms. These variations were learned in the form of edit distance patterns (Levenshtein 1966) using a novel method described in (Ghiasvand and Kate 2014). 2.2 Task 2 In this task, attributes related to disea</context>
</contexts>
<marker>Ghiasvand, 2014</marker>
<rawString>Omid Ghiasvand, 2014. Disease Name Extraction from Clinical Text Using Conditional Random Fields, Thesis and Dissertation, University of Wisconsin-Milwaukee, Milwaukee, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omid Ghiasvand</author>
<author>Rohit J Kate</author>
</authors>
<title>UWM: Disorder Mention Extraction from Clinical Text Using CRFs and Normalization Using Learned Edit Distance Patterns,</title>
<date>2014</date>
<booktitle>in Proceeding of the Eight International Workshop on Semantic Evaluations (SemEval</booktitle>
<pages>828--832</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2650" citStr="Ghiasvand and Kate 2014" startWordPosition="407" endWordPosition="410">d as “CUI-less.” This normalization process is also challenging because disorder names are frequently mentioned in modified forms which prevents them from exactly matching the concepts in UMLS. Task 2 required finding certain attributes for the mentions and finding the spans of these attributes in text. This task is also challenging due to the variability in which attributes are attributed to disease and disorder mentions in clinical text. Our team, UWM, participated in both Task 1 and Task 2. For Task 1, we used the same system that we had previously developed for the Task 7 of SemEval 2014 (Ghiasvand and Kate 2014). For Task 2, we used a simple method that finds attributes of mentions by first collecting lists of attribute terms from the training data and then matching in this list. The nearest attribute terms to a mention are assigned to that mention. The attribute terms are normalized by finding their normalized values in the training data. Despite being simple, this method gave competitive results. The methods used in this paper are described in more details in the next section. 385 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 385–388, Denver, Colorado, J</context>
<context position="4377" citStr="Ghiasvand and Kate 2014" startWordPosition="676" endWordPosition="679">nd Lang 2010) matches. These features are fully described in (Ghiasvand, 2014). This model was also inherently capable of extracting discontinuous disorder mentions. To normalize disorder mentions, our system first looked for exact matches with disorder mentions in the training data and then in the UMLS. If no exact match was found, then suitable variations of the disorder mentions were generated based on possible variations of disorder mentions learned from UMLS synonyms. These variations were learned in the form of edit distance patterns (Levenshtein 1966) using a novel method described in (Ghiasvand and Kate 2014). 2.2 Task 2 In this task, attributes related to disease or disorder mentions were to be identified along with their normalized values and spans in the text (Bodenreider, 2003). There were nine attributes related to each disorder mention for this task which were: the CUI of the disorder (same as Task 1), negation indicator, subject, uncertainty indicator, course, severity, conditional, generic indicator, and body location. For identifying CUI attribute, we used the same normalization method that we had used for Task 1. For identifying the rest of the attributes, we used a simple matching metho</context>
</contexts>
<marker>Ghiasvand, Kate, 2014</marker>
<rawString>Omid Ghiasvand and Rohit J. Kate, 2014. UWM: Disorder Mention Extraction from Clinical Text Using CRFs and Normalization Using Learned Edit Distance Patterns, in Proceeding of the Eight International Workshop on Semantic Evaluations (SemEval 2014), pages 828-832, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>282--289</pages>
<location>Williamstown, MA.</location>
<contexts>
<context position="3668" citStr="Lafferty et al., 2001" startWordPosition="569" endWordPosition="572">he methods used in this paper are described in more details in the next section. 385 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 385–388, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 Methods 2.1 Task 1 We briefly describe the system we had developed for Task 7 of SemEval 2014 (Ghiasvand and Kate 2014) which we used for Task 1. We treated disorder mention extraction as a standard sequence labeling task with “BIO” (Begin, Inside, Outside) labeling scheme. The model was trained using Conditional Random Fields (Lafferty et al., 2001) with various types of lexical and semantic features that included MetaMap (Aronson and Lang 2010) matches. These features are fully described in (Ghiasvand, 2014). This model was also inherently capable of extracting discontinuous disorder mentions. To normalize disorder mentions, our system first looked for exact matches with disorder mentions in the training data and then in the UMLS. If no exact match was found, then suitable variations of the disorder mentions were generated based on possible variations of disorder mentions learned from UMLS synonyms. These variations were learned in the </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 282–289, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Soviet physics doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="4317" citStr="Levenshtein 1966" startWordPosition="668" endWordPosition="669">nd semantic features that included MetaMap (Aronson and Lang 2010) matches. These features are fully described in (Ghiasvand, 2014). This model was also inherently capable of extracting discontinuous disorder mentions. To normalize disorder mentions, our system first looked for exact matches with disorder mentions in the training data and then in the UMLS. If no exact match was found, then suitable variations of the disorder mentions were generated based on possible variations of disorder mentions learned from UMLS synonyms. These variations were learned in the form of edit distance patterns (Levenshtein 1966) using a novel method described in (Ghiasvand and Kate 2014). 2.2 Task 2 In this task, attributes related to disease or disorder mentions were to be identified along with their normalized values and spans in the text (Bodenreider, 2003). There were nine attributes related to each disorder mention for this task which were: the CUI of the disorder (same as Task 1), negation indicator, subject, uncertainty indicator, course, severity, conditional, generic indicator, and body location. For identifying CUI attribute, we used the same normalization method that we had used for Task 1. For identifying</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet physics doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Noemie Elhadad</author>
<author>B South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>W Chapman</author>
<author>Guergana Savova</author>
</authors>
<date>2013</date>
<booktitle>Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of CLEF, CLEF,</booktitle>
<pages>230</pages>
<contexts>
<context position="7815" citStr="Pradhan et al., 2013" startWordPosition="1227" endWordPosition="1230">itional (CND) 154 Course (COU) 168 Generic (GEN) 45 Negation (NEG) 139 Severity (SEV) 92 Subject (SUB) 33 Uncertainty (UNC) 295 Body Location (BL) 1108 Table 1: Number of attribute terms for each attribute type in the training data. 386 3 Results The training, development and test datasets of SemEval 2015 Task 14 had 298, 133 and 100 clinical notes respectively. We formed our training dataset by combining training and development datasets. The clinical notes contained different types of notes including de-identified discharge summaries, electrocardiogram, echocardiogram and radiology reports (Pradhan et al., 2013). The extraction and normalization performance in Task 1 was evaluated in terms of precision, recall and Fmeasure for strict (exact boundaries) and relaxed (overlapping boundaries) settings. Table 2 shows the results of this task. In this task, based on relaxed F-score, we got second rank, and based on strict F-score we got fourth rank considering only the best run of each participating team. Precision Recall F-score Strict 0.773 0.699 0.734 Relaxed 0.809 0.731 0.768 Table 2: Results of Task1 (mention extraction and normalization). For the Task 2A, unweighted and weighted accuracies were used </context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2013</marker>
<rawString>Sameer Pradhan, Noemie Elhadad, B South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, W Chapman, and Guergana Savova. 2013. Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of CLEF, CLEF, 230.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>