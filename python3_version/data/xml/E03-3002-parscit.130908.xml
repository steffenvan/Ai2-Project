<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006946">
<title confidence="0.900121">
Integrating cohesion and coherence for Automatic Summarization
</title>
<author confidence="0.561962">
Laura Alonso i Alemany Maria Fuentes Fort
</author>
<affiliation confidence="0.380041">
GRIAL Departament d&apos;Informatica i Matematica Aplicada
Departament de Lingilistica General Universitat de Girona
</affiliation>
<address confidence="0.312338">
Universitat de Barcelona maria.fuentes@udg.es
</address>
<email confidence="0.984887">
lalonso@lingua.fil.ub.es
</email>
<sectionHeader confidence="0.997096" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999710058823529">
This paper presents the integration of
cohesive properties of text with co-
herence relations, to obtain an ade-
quate representation of text for auto-
matic summarization. A summarizer
based on Lexical Chains is enchanced
with rhetorical and argumentative struc-
ture obtained via Discourse Markers.
When evaluated with newspaper corpus,
this integration yields only slight im-
provement in the resulting summaries
and cannot beat a dummy baseline con-
sisting of the first sentence in the doc-
ument. Nevertheless, we argue that
this approach relies on basic linguis-
tic mechanisms and is therefore genre-
independent.
</bodyText>
<sectionHeader confidence="0.993091" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999743245283019">
Text Summarization (TS) can be decomposed into
three phases: analysing the input text to obtain
text representation, transforming it into a sum-
mary representation, and synthesizing an appro-
priate output form to generate the summary text.
Much of the early work in summarization has
been concerned with detecting relevant elements
of text and presenting them in the &amp;quot;shortest possi-
ble form&amp;quot;. More recently, an increasing attention
has been devoted to the adequacy of the resulting
texts to a human user. Well-formedness, cohesion
and coherence are currently under inspection, not
only because they improve the quality of a sum-
mary as a text, but also because they can reduce
the final summary by reducing the reading time
and cost that is needed to process it.
TS systems that performed best in last DUC
contest (DUC, 2002) apply template-driven sum-
marization, by information-extraction procedures
in the line of (Schank and Abelson, 1977). This
approach yields very good results in assessing rel-
evance and keeping well-formedness, but it is de-
pendent on a clearly defined representation of the
information need to be fulfilled and, in most cases,
also on some regularities of the kind of texts to be
summarized.
In more generic TS, genre-dependent regular-
ities are not always found, and template-driven
analysis cannot capture the variety of texts. In ad-
dition, the information need is usually very fuzzy.
In these circumstances, the most reliable source
of information on relevance and coherence prop-
erties of a text is the source text itself. An ad-
equate representation of that text should account
not only for relevant elements, but also for the re-
lations holding between them, in the diverse tex-
tual levels. Exploiting the discursive properties of
text seems to accomplish both these requirements,
since they have language-wide validity can be suc-
cessfully combined with information at superficial
or semantic level.
In this paper, we present an integration of two
kinds of discursive information, cohesion and co-
herence, to obtain an adequate representation of
text for the task of TS. Our starting point is an
extractive informative summarization system that
exploits the cohesive properties of text by building
and ranking lexical chains (see Section 3). This
system is enhanced with discourse coherence in-
formation (Section 5.3). Experiments were carried
out on the combination of these two kinds of infor-
mation, and results were evaluated on a Spanish
news agency corpus (Section 5).
</bodyText>
<page confidence="0.964879">
1
</page>
<note confidence="0.7226355">
2 Previous Work on Combining 3 Summarizing with Lexical Chains
Cohesion and Coherence
</note>
<bodyText confidence="0.999878666666667">
Traditionally, two main components have been
distinguished in the discursive structure of a text:
cohesion and coherence. As defined by (Halliday
and Hasan, 1976), cohesion tries to account for
relationships among the elements of a text. Four
broad categories of cohesion are identified: refer-
ence, ellipsis, conjunction, and lexical cohesion.
On the other hand, coherence is represented in
terms of relations between text segments, such as
elaboration, cause or explanation. (Mani, 2001)
argues that an integration of these two kinds of
discursive information would yield significant im-
provements in the task of text summarization.
(Corston-Oliver and Dolan, 1999) showed that
eliminating discursive satellites as defined by the
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), yields an improvement in the
task of Information Retrieval. Precision is im-
proved because only words in discursively relevant
text locations are taken into account as indexing
terms, while traditional methods treat texts as un-
structured bags of words.
Some analogous experiments have been carried
out in the area of TS. (Brunn et al., 2001; Alonso
and Fuentes, 2002) claim that the performance of
summarizers based on lexical chains can be im-
proved by ignoring possible chain members if they
occur in irrelevant locations such as subordinate
clauses, and therefore only consider chain candi-
dates in main clauses. However, syntactical sub-
ordination does not always map discursive rele-
vance. For example, in clauses expressing finality
or dominated by a verb of cognition, like Y said
that X, the syntactically subordinate clause X is
discursively nuclear, while the main clause is less
relevant (Verhagen, 2001).
In (Alonso and Fuentes, 2002), we showed that
identifying and removing discursively motivated
satellites yields an improvement in the task of text
summarization. Nevertheless, we will show that
a more adequate representation of the source text
can be obtained by ranking chain members in ac-
cordance to their position in the discourse struc-
ture, instead of simply eliminating them.
The lexical chain summarizer follows the work of
(Morris and Hirst, 1991) and (Barzilay, 1997).
As can be seen in Figure 1 (left) the text is first
segmented, at different granularity levels (para-
graph, sentence, clause) depending on the appli-
cation. To detect chain candidates, the text is mor-
phologically analysed, and the lemma and POS of
each word are obtained. Then, Named Entities are
identified and classified in a gazzetteer. For Span-
ish, a simplified version of (Palomar et al., 2001)
extracts co-referenece links for some types of pro-
nouns, dropping off the constraints and rules in-
volving syntactic information.
Semantic tagging of common nouns is been per-
formed with is-a relations by attaching Euro Word-
Net (Vossen, 1998) synsets to them. Named Enti-
ties are been semantically tagged with instance re-
lations by a set of trigger words, like former pres-
ident, queen, etc., associated to each of them in a
gazzetteer. Semantic relations between common
nouns and Named Entities can be established via
the EWN synset of the trigger words associated to
a each entity.
Chain candidates are common nouns, Named
Entities, definite noun phrases and pronouns, with
no word sense disambiguation. For each chain
candidate, three kinds of relations are considered,
as defined by (Barzilay, 1997):
</bodyText>
<listItem confidence="0.995079">
• Extra-strong between repetitions of a word.
• Strong between two words connected by a
direct EuroWordNet relation.
• Medium-strong if the path length between
the Euro WordNet synsets of the words is
longer than one.
</listItem>
<bodyText confidence="0.999228545454546">
Being based on general resources and princi-
ples, the system is highly parametrisable. It has a
relative independence because it may obtain sum-
maries for texts in any language for which there is
a version of WordNet an tools for POS tagging and
Named Entity recognition and classification. It
can also be parametrised for obtaining summaries
of various lengths and at granularity levels.
As for relevance assessment, some constraints
can be set on chain building, like determining the
maximum distance between WN synsets of chain
</bodyText>
<page confidence="0.988291">
2
</page>
<bodyText confidence="0.999959225806452">
candidates for building medium-strong chains, or
the type of chain merging when using gazetteer
information. Once lexical chains are built, they
are scored according to a number of heuristics that
consider characteristics such as their length, the
kind of relation between their words and the point
of text where they start. Textual Units (TUs) are
ranked according to the number and type of chains
crossing them, and the TUs which are ranked high-
est are extracted as a summary. This ranking of
TUs can be parametrised so that a TU can be as-
signed a different relative scoring if it is crossed
by a strong chain, by a Named Entity Chain or by
a co-reference chain. For a better adaptation to
textual genres, heuristics schemata can be applied.
However, linguistic structure is not taken into
account for scoring the relevance lexical chains
or TUs, since the relevance of chain elements is
calculated irrespective of other discourse informa-
tion. Consequently, the strength of lexical chains
is exclusively based on lexic. This partial repre-
sentation can be even misleading to discover the
relevant elements of a text. For example, a Named
Entity that is nominally conveying a piece of news
in a document can present a very tight pattern of
occurrence, without being actually relevant to the
aim of the text. The same applies to other linguis-
tic structures, such as recurring parallelisms, ex-
amples or adjuncts. Nevertheless, the relative rel-
evance of these elements is usually marked struc-
turally, either by sentential or discursive syntax.
</bodyText>
<sectionHeader confidence="0.8846775" genericHeader="introduction">
4 Incorporating Rhetorical and
Argumentative Relations
</sectionHeader>
<bodyText confidence="0.96547425">
The lexical chain summarizer was enhanced with
discourse structural information as can be seen in
Figure 1 (right).
Following the approach of (Marcu, 1997), a par-
tial representation of discourse structre was ob-
tained by means of the information associated to
a Discourse Marker (DM) lexicon. DMs are de-
scribed in four dimensions:
</bodyText>
<listItem confidence="0.985637066666667">
• matter: following (Asher and Lascarides,
2002), three different kinds of subject-matter
meaning are distinguished, namely causality,
parallelism and context.
• argumentation: in the line of (Anscom-
bre and Ducrot, 1983), three argumentative
moves are distinguished: progression, elabo-
ration and revision.
• structure: following the notion of right fron-
tier (Polanyi, 1988), symmetric and asymmet-
ric relations are distinguished.
• syntax: describes the relation of the DM with
the rest of the elements at the discourse level,
in the line of (Forbes et al., 2003), mainly
used for discourse segmentation.
</listItem>
<bodyText confidence="0.999969333333333">
The information stored in this DM lexicon was
used for identifying inter- and intra-sentential dis-
course segments (Alonso and Castellon, 2001)
and the discursive relations holding between them.
Discourse segments were taken as Textual Units
by the Lexical Chain summarizer, thus allowing a
finer granularity level than sentences.
Two combinations of DM descriptive features
were used, in order to account for the interaction
of different structural information with the lexical
information of lexical chains. On the one hand,
nucleus-satellite relations were identified by the
combination of matter and structure dimensions of
DMs. This rhetorical information yielded a hier-
archical structure of text, so that satellites are sub-
ordinate to nucleus and they are accordingly con-
sidered less relevant. On the other hand, the ar-
gumentative line of text was traced via the argu-
mentation and also structure DM dimensions, so
that segments were tagged with their contribution
to the progression of the argumentation.
These two kinds of structural analyses are com-
plementary. Rhetorical information is mainly
effective at discovering local coherence struc-
tures, but it is unreliable when analyzing macro-
structure. As (Knott et al., 2001) argue, a differ-
ent kind of analysis is needed to track coherence
throughout a whole text; in their case the alter-
native information used is focus, we have opted
for argumentative orientation. Argumentative in-
formation accounts for a higher-level structure, al-
though it doesn&apos;t provide much detail about it.
This lexicon has been developed for Spanish
(Alonso et al., 2002a). Nevertheless, the struc-
ture of the DM lexicon and the discourse parsing
tools based on it is highly portable, and versions
</bodyText>
<page confidence="0.953321">
3
</page>
<figure confidence="0.956220879310345">
cleaning up
.01
semantic
Lagging
ci IC
co—reference
rules
trigger—words
TEXT
Textual Unit
segmentation
morphological
analysis
Lexical Unit
segmentation
co—reference
resolution
morphosyntactical
analysis
PN rules discourse
segmentation
discourse
markers
rhetorical relation
interpretation
;
heuristics
EuroWN
; !
i RIIETORICAL .
i
I INFORMATION i
i , !
.—
,..• i
..•&amp;quot;* ;
i
;
;
i
;
PRE—PROCESSED
TEXT
LEXICAL CHAINER
CHAINS
OUTPUT
textual
units
;
SENTENCE ;
!. COMPRESSION I
Lexical Chain
Summary
Lemcal Chain and
S4ntence Compression
—.1Parameters
RANKING &amp;
SELECTION
</figure>
<figureCaption confidence="0.999997">
Figure 1: Integration of discursive information: lexical chains (left) and discourse structural (right)
</figureCaption>
<bodyText confidence="0.999519">
for English and Catalan are being developed by
bootstraping techniques (Alonso et al., 2002b).
</bodyText>
<sectionHeader confidence="0.998805" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999877857142857">
A number of experiments were carried out in or-
der to test whether taking into account the struc-
tural status of the textual unit where a chain mem-
ber occurs can improve the relevance assessment
of lexical chains (see Figure 2). Since the DM
lexicon and the evaluation corpus were available
only for Spanish, the experiments were limited to
that language. Linguistic pre-processing was per-
formed with the CLiC-TALP system (Carmona et
al., 1998; Arevalo et al., 2002).
For the evaluation of the different experiments,
the evaluation software MEADeval (MEA, 2002)
was used, to compare the obtained summaries with
a golden standard (see Section 5.1). From this
package, the usual precision and recall measures
were selected, as well as the simple cosine. Sim-
ple cosine (simply cosine from now on) was cho-
sen because it provides a measure of similarity be-
tween the golden standard and the obtained ex-
tracts, overcoming the limitations of measures de-
pending on concrete textual units.
</bodyText>
<subsectionHeader confidence="0.984969">
5.1 Golden Standard
</subsectionHeader>
<bodyText confidence="0.9993555">
The corpus used for evaluation was created within
Hermes projeal , to evaluate automatic summariz-
</bodyText>
<footnote confidence="0.6328645">
&apos;Information about this project available in
http://terral.ieec.uned.es/hermes/
</footnote>
<page confidence="0.976217">
4
</page>
<figureCaption confidence="0.9930225">
Figure 2: Experiments to assess the impact of discourse
structure on lexical chain members
</figureCaption>
<bodyText confidence="0.999957818181818">
ers for Spanish, by comparison to human summa-
rizers. It consists of 1202 news agency stories of
various topics, ranging from 2 to 28 sentences and
from 28 to 734 words in length, with an average
length of 275 words per story.
To avoid the variability of human generated ab-
stracts, human summarizers built an extract-based
golden standard. Paragraphs were chosen as the
basic textual unit because they are self-contained
meaning units. In most of the cases, paragraphs
contained a single sentence. Every paragraph in
a story was ranked from 0 to 2, according to its
relevance. 31 human judges summarized the cor-
pus, so that at least 5 different evaluations were
obtained for each story.
Golden standards were obtained coming as
close as possible to the 10% of the length of the
original text (19% compression average).
The two main shortcomings of this corpus are
its small size and the fact that it belongs to the
journalistic genre. However, we know of no other
corpus for summary evaluation in Spanish.
</bodyText>
<subsectionHeader confidence="0.998905">
5.2 Performance of the Lexical Chain System
</subsectionHeader>
<bodyText confidence="0.999903090909091">
The performance of the Lexical Chain System
with no discourse structural information was taken
as the base to improve. (Fuentes and Rodriguez,
2002) report on a number of experiments to evalu-
ate the effect of different parameters on the results
of lexical chains. To keep comparability with the
golden standard, and to adequately calculate pre-
cision and recall measures, paragraph-sized TUs
were extracted at 10% compression rate.
Some parameters were left unaltered for the
whole of the experiment set: only strong or extra-
</bodyText>
<footnote confidence="0.9981355">
2For the experiments reported here, one-paragraph news
were dropped, resulting in a final set of Ill news stories.
</footnote>
<table confidence="0.99964944">
Precision Recall Cosine
Lead .95 .85 .90
SweSum .90 .81 .87
HEURISTIC 1
Lex. Chains .82 .81 .85
Lex. Chains .85 .85 .88
+ PN Chains
Lex. Chains .83 .83 .87
+ PN Chains
+ coRef Chains
Lex. Chains .88 .88 .90
+ PN Chains
+ coRef Chains
+ 1st TU
HEURISTIC 2
Lex. Chains .71 .72 .79
Lex. Chains .73 .74 .81
+ PN Chains
Lex. Chains .70 .71 .78
+ PN Chains
+ coRef Chains
Lex. Chains .82 .82 .86
+ PN Chains
+ coRef Chains
+ 1st TU
</table>
<tableCaption confidence="0.999943">
Table 1: Performance of the lexical chain Summarizer
</tableCaption>
<bodyText confidence="0.999960620689655">
strong chains were built, no information from de-
fined noun phrases or trigger words could be used
and only short co-reference chains were built. Re-
sults are presented in Table I.
The first column in the table shows the main
parameters governing each trial: simple lexi-
cal chains, lexical chains successively augmented
with proper noun and co-Reference chains, and fi-
nally giving special weighting to the 1st TU be-
cause of global document structure appliable to the
journalistic genre.
Two heuristics schemata were experimented:
heuristic 1 ranks as most relevant the first TU
crossed by a strong chain, while heuristic 2 ranks
highest the TU crossed by the maximum of strong
chains. An evaluation of SweSum (SweSum,
2002), a summarization system available for Span-
ish, is also provided as a comparison ground. Tri-
als with SweSum were carried out with the default
parameters of the system. In addition, the first
paragraph of every text, the so-called lead sum-
mary, was taken as a dummy baseline.
As can be seen in Table 1, the lead achieves the
best results, with almost the best possible score.
This is due to the pyramidal organisation of the
journalistic genre, that causes most relevant infor-
mation to be placed at the beginning of the text.
Consequently, any heuristic assigning more rele-
vance to the beginning of the text will achieve bet-
</bodyText>
<figure confidence="0.9996903125">
Lexical Chains
Removing Satellites
Lexical Clanina
Rhetorical Information
Lexical Chains
Rhetorical &amp; Argurnentati,
&apos;sr]
Lexical Chains rt,rie
prectston
VI
cosinus
L
ts,&apos;&amp;quot;
t-tt-
L
Named Eat NEI
</figure>
<page confidence="0.945037">
5
</page>
<bodyText confidence="0.999912391304348">
ter results in this kind of genre. This is the case for
the default parameters of SweSum and heuristic 1.
However, it must be noted that lexical chain
summarizer produces results with high cosine and
low precision, while SweSum yields high pre-
cision and low cosine. This means that, while
the textual units extracted by the summarizer are
not identical to the ones in the golden standard,
their content is not dissimilar. This seems to
indicate that the summarizer successfully cap-
tures content-based relevance, which is genre-
independent. Consequently, the lexical chain sum-
marizer should be able to capture relevance when
applied to non-journalistic texts. This seems to be
supported by the fact that heuristic 2 improves co-
sine over precision four points higher than heuris-
tic 1, which seems more genre-dependent.
Unexpectedly, co-reference chains cause a de-
crease in the performance of the system. This may
be due to their limited length, and also to the fact
that both full forms and pronouns are given the
same score, which does not capture the difference
in relevance signalled by the difference in form.
</bodyText>
<subsectionHeader confidence="0.904549">
5.3 Results of the Integration of
Heterogenous Discursive Informations
</subsectionHeader>
<bodyText confidence="0.9998535">
Structural discursive information was integrated
with only those parameters of the lexical chain
summarizer that exploited general discursive in-
formation. Heuristic I was not considered because
it is too genre-dependent. No co-reference infor-
mation was taken into account, since it does not
seem to yield any improvement.
The results of integrating lexical chains with
discourse structural information can be seen in Ta-
ble 2. Following the design sketched in Figure
5, the performance of the lexical chains summa-
rizer was first evaluated on a text where satellites
had been removed. As stated by (Brunn et al.,
2001; Alonso and Fuentes, 2002), removing satel-
lites slightly improves the relevance assessment of
the lexical chainer (by one point).
Secondly, discourse coherence information was
incorporated. Rhetorical and argumentative infor-
mations were distinguished, since the first iden-
tifies mainly unimportant parts of text and the
second identifies both important and unimportant.
Identifying satellites instead of removing them
</bodyText>
<table confidence="0.999979580645161">
1 Precision I Recall 1 Cosine
Sentence Compression
+ Lexical Chains
Sentence Compression .74 .75 .70
+ Lexical Chains
+ PN Chains
Sentence Compression .86 .85 .76
+ Lexical Chains
+ PN Chains
+ 1st TU
Rhetoecal Information
+ Lexical Chains
Rhetorical Information .74 .76 .82
+ Lex. Chains
+ PN Chains
Rhetorical Information .83 .84 .86
+ Lex. Chains
+ PN Chains
+ 1st TU
Rhetorical
+ Argumentative
+ Lexical Chains
Rhetorical Information .79 .80 .84
+ Argumentative
+ Lex. Chains
+ PN Chains
Rhetorical Information .84 .85 .87
+ Argumentative
+ Lex. Chains
+ PN Chains
+ 1st TU
</table>
<tableCaption confidence="0.9833485">
Table 2: Results of the integration of lexical chains and
discourse structural information
</tableCaption>
<bodyText confidence="0.99993948">
yields only a slight improvement on recall (from
.75 to .76), but significantly improves cosine (from
.70 to .82).
When argumentative information is provided,
an improvement of .5 in performance is observed
in all three metrics in comparison to removing
satellites. As can be expected, ranking the first
TU higher results in better measures, because of
the nature of the genre. When this parameter is
set, removing satellites outperforms the results ob-
tained by taking into account discourse structural
information in precision. However, this can also
be due to the fact that when the text is compressed,
TUs are shorter, and a higher number of them can
be extracted within the fixed compression rate. It
must be noted, though, that recall does not drop
for these summaries.
Lastly, intra-sentential and sentential satellites
of the best summary obtained by lexical chains
were removed, increasing compression of the re-
sulting summaries from an average 18.84% for
lexical chain summaries to a 14.43% for sum-
maries which were sentence-compressed. More-
over, since sentences were shortened, readability
was increased, which can be considered as a fur-
</bodyText>
<page confidence="0.996173">
6
</page>
<bodyText confidence="0.999807875">
ther factor of compression. However, these sum-
maries have not been evaluated with the MEADe-
val package because no golden standard was avail-
able for textual units smaller than paragraphs. Pre-
cision and recall measures could not be calcu-
lated for summaries that removed satellites, be-
cause they could not be compared with the golden
standard, consisting only full sentences.
</bodyText>
<subsectionHeader confidence="0.563554">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999971545454545">
The presented evaluation successfully shows the
improvements of integrating cohesion and coher-
ence, but it has two weak points. First, the small
size of the corpus and the fact that it represents a
single genre, which does not allow for safe gener-
alisations. Second, the fact that evaluation metrics
fall short in assessing the improvements yielded
by the combination of these two discursive infor-
mations, since they cannot account for quantitative
improvements at granularity levels different from
the unit used in the golden standard, and therefore
a full evaluation of summaries involving sentence
compression is precluded. Moreover, qualitative
improvements on general text coherence cannot be
captured, nor their impact on summary readability.
As stated by (Goldstein et al., 1999), &amp;quot;one of the
unresolved problems in summarization evaluation
is how to penalize extraneous non-useful informa-
tion contained in a summary&amp;quot;. We have tried to
address this problem by identifying text segments
which carry non-useful information, but the pre-
sented metrics do not capture this improvement.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992897959184">
We have shown that the collaborative integration
of heterogeneous discursive information yields an
improvement on the reperesentation of source text,
as can be seen by improvements in resulting sum-
maries. Although this enriched representation
does not outperform a dummy baseline consisting
of taking the first paragraph of the text, we have
argued that the resulting representation of text is
genre-independent and succeeds in capturing con-
tent relevance, as shown by cosine measures.
Since the properties exploited by the presented
system are text-bound and follow general princi-
ples of text organization, they can be considered
to have language-wide validity. This means that
the system is domain-independent, though it can
be easily tuned to different genres.
Moreover, the system presents portability to a
variety of languages, as long as it has the knowl-
edge sources required, basically, shallow tools for
morpho-syntactical analysis, a version of WordNet
for building and ranking lexical chains, and a lex-
icon of discourse markers for obtaining a certain
discourse structure.
Future work concerning the lexical chain sum-
marizer will be focussed in building longer lexical
chains, exploiting other relations in EWN, merg-
ing chains and even merging heterogeneous infor-
mation. Improvements in the analysis of struc-
tural discursive information include enhancing the
scope to paragraph and global document level,
integrating heterogeneous discursive information
and proving language-wide validity of Discourse
Marker information.
To provide an adequate assessment of the
achieved improvements, the evaluation procedure
is currently being changed. Given the enormous
cost of building a comprehensive corpus for sum-
mary evaluation, the system has been partially
adapted to English, so that it can be evaluated with
the data and procedures of (DUC, 2002).
Nevertheless, our future efforts will also be di-
rected to gathering a corpus of Spanish texts with
abstracts from which to automatically obtain a cor-
pus of extracts with their corresponding texts, as
proposed by (Marcu, 1999). Concerning quali-
tative evaluation, we will try to apply evaluation
metrics that are able to capture content and coher-
ence aspects of summaries, such as more complex
content similarity or readability measures.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.946650333333333">
This research has been conducted thanks to a grant asso-
ciated to the X-TRACT project, PB98-1226 of the Span-
ish Research Department. It has also been partially
funded by projects HERMES (TIC2000-0335-0O3-02), PE-
TRA (TIC2000-1735-0O2-02), and by CLiC (Centre de Ll-
lengutatge i Computacio).
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9972706">
Laura Alonso and Irene Castellon. 2001. Towards a delimita-
tion of discursive segment for natural language processing
applications. In First International Workshop on Seman-
tics, Pragmatics and Rhetoric, Donostia - San Sebastian,
November.
</reference>
<page confidence="0.990125">
7
</page>
<reference confidence="0.999601433333333">
Laura Alonso and Maria Fuentes. 2002. Collaborating dis-
course for text summarisation. In Proceedings of the Sev-
enth ESSLLI Student Session.
Laura Alonso, Irene Castellon, and Lluis Padre,. 2002a. De-
sign and implementation of a spanish discourse marker
lexicon. In SEPLIV, Valladolid.
Laura Alonso, Irene Castellon, and Lillis PadrO. 2002b.
X-tractor: A tool for extracting discourse markers. In
LREC 2002 workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated Lan-
guage Data, Las Palmas.
J. C. Anscombre and 0. Ducrot. 1983. L&apos;argumentation clans
la langue. Mardaga.
Montse Arevalo, Xavi Carreras, Lids Marquez, M.Antonia
Marti, Liu&amp; Padre&apos;, and M.Jose Simon. 2002. A pro-
posal for wide-coverage spanish named entity recognition.
Procesamiento del Len guaje Natural, 1(3).
Nicholas Asher and Alex Lascarides. 2002. The Logic of
Conversation. Cambridge University Press.
Regina Barzilay. 1997. Lexical Chains for Summarization.
Ph.D. thesis, Ben-Gurion University of the Negev.
Meru Brunn, Yllias Chali, and Christopher J. Pinchak. 2001.
Text Summarization using lexical chains. In Workshop on
Text Summarization in conjunction with the ACM SIGIR
Conference 2001, New Orleans, Louisiana.
Josep Carmona, Sergi Cervell, Lluis Marquez, M. Antonia
Mart, Lluis Padro, Roberto Placer, Horacio Rodriguez,
Mariona Taule, and Jordi Turmo. 1998. An environ-
ment for morphosyntactic processing of unrestricted span-
ish text. In First International Conference on Language
Resources and Evaluation (LREC&apos;98), Granada, Spain.
Simon H. Corston-Oliver and W. Dolan. 1999. Less is more:
Eliminating index terms from subordinate clauses. In 37th
Annual Meeting of the Association for Computational Lin-
guistics (ACL&apos;99), pages 348 - 356.
DUC. 2002. DUC-document understanding conference.
http://duc.nist.gov/.
K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar, A. Joshi,
and B. Webber. 2003. D-LTAG system - discourse pars-
ing with a lexicalized tree-adjoining grammar. Journal of
Language, Logic and Information. to appear.
Maria Fuentes and Horacio Rodriguez. 2002. Using cohe-
sive properties of text for automatic summarization. In
JOTRI&apos;02.
Jade Goldstein, Vibhu Mittal, Mark Kantrowitz, and Jaime
Carbonell. 1999. Summarizing text documents: Sentence
selection and evaluation metrics. In SIGIR-99.
M. A. K. Halliday and R. Hasan. 1976. Cohesion in English.
English Language Series. Longman Group Ltd.
Alistair Knott, Jon Oberlander, Mick O&apos;Donnell, and Chris
Mellish. 2001. Beyond elaboration: The interaction of
relations and focus in coherent text. In Ted Sanders, Joust
Schilperoord, and Wilbert Spooren, editors, Text represen-
tation: linguistic and psycholinguistic aspects, pages 181-
196. Benjamins.
Inderjeet Mani. 2001. Automatic Summarization. Nautral
Language Processing. John Benjamins Publishing Com-
pany.
William C. Mann and Sandra A. Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of text
organisation. Text, 3(8):234-281.
Daniel Marcu. 1997. The Rhetorical Parsing, Summariza-
tion and Generation of Natural Language Texts. Ph.D.
thesis, Department of Computer Science, University of
Toronto, Toronto, Canada.
Daniel Marcu. 1999. The automatic construction of large-
scale corpora for summarization research. In SIGIR-99.
2002. MEADeval. http://perun.si.umich.edu/clair/meadeval/.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion, the
thesaurus, and the structure of text. Computational lin-
guistics, 17(1):21-48.
M. Palomar, A. Ferrandez, L. Moreno, P. Martinez-Barco,
J. Peral, M. Saiz-Noeda, and R. Mu noz. 2001. An algo-
rithm for anaphora resolution in spanish texts. Computa-
tional Linguistics, 27(4).
Livia Polanyi. 1988. A formal model of the structure of
discourse. Journal of Pragmatics, 12:601-638.
R. Schank and R. Abelson. 1977. Scripts, Plans, Goals, and
Understanding. Lawrence Erlbaum, Hillsdale, NJ.
SweSum. 2002. http://www.nada.kth.set xmartin/
swesum/index-eng.html.
Arie Verhagen. 2001. Subordination and discourse segmen-
tation revisited, or: Why matrix clauses may be more
dependent than complements. In Ted Sanders, Joost
Schilperoord, and Wilbert Spooren, editors, Text Rep-
resentation. Linguistic and psychological aspects, pages
337-357. John Benjamins.
Piek Vossen, editor. 1998. Euro WordNet: a multilingual
database with lexical semantic networks. Kluwer Aca-
demic Publishers.
</reference>
<page confidence="0.998448">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999898">Integrating cohesion and coherence for Automatic Summarization</title>
<author confidence="0.995135">Laura Alonso i Alemany Maria Fuentes Fort</author>
<affiliation confidence="0.835833666666667">GRIAL Departament d&apos;Informatica i Matematica Aplicada Departament de Lingilistica General Universitat de Girona de Barcelona maria.fuentes@udg.es</affiliation>
<email confidence="0.806563">lalonso@lingua.fil.ub.es</email>
<abstract confidence="0.976112392105263">This paper presents the integration of of text with coto obtain an adequate representation of text for automatic summarization. A summarizer based on Lexical Chains is enchanced with rhetorical and argumentative structure obtained via Discourse Markers. When evaluated with newspaper corpus, this integration yields only slight improvement in the resulting summaries and cannot beat a dummy baseline consisting of the first sentence in the document. Nevertheless, we argue that this approach relies on basic linguistic mechanisms and is therefore genreindependent. 1 Motivation Text Summarization (TS) can be decomposed into three phases: analysing the input text to obtain text representation, transforming it into a summary representation, and synthesizing an appropriate output form to generate the summary text. Much of the early work in summarization has been concerned with detecting relevant elements of text and presenting them in the &amp;quot;shortest possible form&amp;quot;. More recently, an increasing attention has been devoted to the adequacy of the resulting texts to a human user. Well-formedness, cohesion and coherence are currently under inspection, not only because they improve the quality of a summary as a text, but also because they can reduce the final summary by reducing the reading time and cost that is needed to process it. TS systems that performed best in last DUC contest (DUC, 2002) apply template-driven summarization, by information-extraction procedures in the line of (Schank and Abelson, 1977). This approach yields very good results in assessing relevance and keeping well-formedness, but it is dependent on a clearly defined representation of the information need to be fulfilled and, in most cases, also on some regularities of the kind of texts to be summarized. In more generic TS, genre-dependent regularities are not always found, and template-driven analysis cannot capture the variety of texts. In addition, the information need is usually very fuzzy. In these circumstances, the most reliable source of information on relevance and coherence properties of a text is the source text itself. An adequate representation of that text should account not only for relevant elements, but also for the relations holding between them, in the diverse textual levels. Exploiting the discursive properties of text seems to accomplish both these requirements, since they have language-wide validity can be successfully combined with information at superficial or semantic level. In this paper, we present an integration of two of discursive information, coobtain an adequate representation of text for the task of TS. Our starting point is an extractive informative summarization system that exploits the cohesive properties of text by building and ranking lexical chains (see Section 3). This system is enhanced with discourse coherence information (Section 5.3). Experiments were carried out on the combination of these two kinds of information, and results were evaluated on a Spanish news agency corpus (Section 5). 1 2 Previous Work on Combining 3 Summarizing with Lexical Chains Cohesion and Coherence Traditionally, two main components have been distinguished in the discursive structure of a text: cohesion and coherence. As defined by (Halliday Hasan, 1976), to account for relationships among the elements of a text. Four categories of cohesion are identified: referellipsis, conjunction, cohesion. the other hand, represented in terms of relations between text segments, such as cause 2001) argues that an integration of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chain members if they occur in irrelevant locations such as subordinate clauses, and therefore only consider chain candidates in main clauses. However, syntactical subordination does not always map discursive relevance. For example, in clauses expressing finality dominated by a verb of cognition, like Y X, syntactically subordinate clause discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 2002), we showed that identifying and removing discursively motivated satellites yields an improvement in the task of text summarization. Nevertheless, we will show that a more adequate representation of the source text can be obtained by ranking chain members in accordance to their position in the discourse structure, instead of simply eliminating them. The lexical chain summarizer follows the work of (Morris and Hirst, 1991) and (Barzilay, 1997). As can be seen in Figure 1 (left) the text is first segmented, at different granularity levels (paragraph, sentence, clause) depending on the application. To detect chain candidates, the text is morphologically analysed, and the lemma and POS of each word are obtained. Then, Named Entities are identified and classified in a gazzetteer. For Spanish, a simplified version of (Palomar et al., 2001) extracts co-referenece links for some types of pronouns, dropping off the constraints and rules involving syntactic information. Semantic tagging of common nouns is been perwith by attaching Euro Word- Net (Vossen, 1998) synsets to them. Named Entiare been semantically tagged with reby a set of words, presqueen, associated to each of them in a gazzetteer. Semantic relations between common nouns and Named Entities can be established via the EWN synset of the trigger words associated to a each entity. Chain candidates are common nouns, Named Entities, definite noun phrases and pronouns, with no word sense disambiguation. For each chain candidate, three kinds of relations are considered, as defined by (Barzilay, 1997): Extra-strong repetitions of a word. Strong two words connected by a direct EuroWordNet relation. Medium-strong the path length between the Euro WordNet synsets of the words is longer than one. Being based on general resources and principles, the system is highly parametrisable. It has a relative independence because it may obtain summaries for texts in any language for which there is a version of WordNet an tools for POS tagging and Named Entity recognition and classification. It can also be parametrised for obtaining summaries of various lengths and at granularity levels. As for relevance assessment, some constraints can be set on chain building, like determining the maximum distance between WN synsets of chain 2 candidates for building medium-strong chains, or the type of chain merging when using gazetteer information. Once lexical chains are built, they are scored according to a number of heuristics that consider characteristics such as their length, the kind of relation between their words and the point of text where they start. Textual Units (TUs) are ranked according to the number and type of chains crossing them, and the TUs which are ranked highest are extracted as a summary. This ranking of TUs can be parametrised so that a TU can be assigned a different relative scoring if it is crossed by a strong chain, by a Named Entity Chain or by a co-reference chain. For a better adaptation to textual genres, heuristics schemata can be applied. However, linguistic structure is not taken into account for scoring the relevance lexical chains or TUs, since the relevance of chain elements is calculated irrespective of other discourse information. Consequently, the strength of lexical chains is exclusively based on lexic. This partial representation can be even misleading to discover the relevant elements of a text. For example, a Named Entity that is nominally conveying a piece of news in a document can present a very tight pattern of occurrence, without being actually relevant to the aim of the text. The same applies to other linguistic structures, such as recurring parallelisms, examples or adjuncts. Nevertheless, the relative relevance of these elements is usually marked structurally, either by sentential or discursive syntax. 4 Incorporating Rhetorical and Argumentative Relations The lexical chain summarizer was enhanced with discourse structural information as can be seen in Figure 1 (right). Following the approach of (Marcu, 1997), a partial representation of discourse structre was obtained by means of the information associated to a Discourse Marker (DM) lexicon. DMs are described in four dimensions: matter: (Asher and Lascarides, 2002), three different kinds of subject-matter are distinguished, namely argumentation: the line of (Anscombre and Ducrot, 1983), three argumentative are distinguished: elabostructure: the notion of right fron- (Polanyi, 1988), asymmetare distinguished. syntax: describes the relation of the DM the rest of the elements at the discourse level, in the line of (Forbes et al., 2003), mainly used for discourse segmentation. The information stored in this DM lexicon was used for identifying interand intra-sentential discourse segments (Alonso and Castellon, 2001) and the discursive relations holding between them. Discourse segments were taken as Textual Units by the Lexical Chain summarizer, thus allowing a finer granularity level than sentences. Two combinations of DM descriptive features were used, in order to account for the interaction of different structural information with the lexical information of lexical chains. On the one hand, were identified by the of of This yielded a hierarchical structure of text, so that satellites are subordinate to nucleus and they are accordingly conless relevant. On the other hand, the arof text was traced via the argualso dimensions, so that segments were tagged with their contribution to the progression of the argumentation. These two kinds of structural analyses are complementary. Rhetorical information is mainly effective at discovering local coherence structures, but it is unreliable when analyzing macrostructure. As (Knott et al., 2001) argue, a different kind of analysis is needed to track coherence throughout a whole text; in their case the alternative information used is focus, we have opted for argumentative orientation. Argumentative information accounts for a higher-level structure, although it doesn&apos;t provide much detail about it. This lexicon has been developed for Spanish (Alonso et al., 2002a). Nevertheless, the structure of the DM lexicon and the discourse parsing tools based on it is highly portable, and versions 3 cleaning up .01 semantic Lagging co—reference rules trigger—words TEXT Textual Unit segmentation morphological analysis Lexical Unit segmentation co—reference resolution morphosyntactical analysis PN rules discourse segmentation discourse markers rhetorical relation interpretation ; heuristics EuroWN ; ! i RIIETORICAL . i I INFORMATION i i , ! .— ,..• i ..•&amp;quot;* ; i ; ; i ; PRE—PROCESSED TEXT LEXICAL CHAINER CHAINS OUTPUT textual units ; SENTENCE ; !. COMPRESSION I Lexical Chain Summary Lemcal Chain and S4ntence Compression RANKING &amp; SELECTION 1: of discursive information: lexical chains (left) and discourse structural (right) for English and Catalan are being developed by bootstraping techniques (Alonso et al., 2002b). A number of experiments were carried out in order to test whether taking into account the structural status of the textual unit where a chain member occurs can improve the relevance assessment of lexical chains (see Figure 2). Since the DM lexicon and the evaluation corpus were available only for Spanish, the experiments were limited to that language. Linguistic pre-processing was performed with the CLiC-TALP system (Carmona et al., 1998; Arevalo et al., 2002). For the evaluation of the different experiments, the evaluation software MEADeval (MEA, 2002) was used, to compare the obtained summaries with a golden standard (see Section 5.1). From this package, the usual precision and recall measures were selected, as well as the simple cosine. Simcosine (simply now on) was chosen because it provides a measure of similarity between the golden standard and the obtained extracts, overcoming the limitations of measures depending on concrete textual units. 5.1 Golden Standard The corpus used for evaluation was created within , evaluate automatic summariz- &apos;Information about this project available in http://terral.ieec.uned.es/hermes/ 4 2: to assess the impact of discourse structure on lexical chain members ers for Spanish, by comparison to human summa- It consists of news agency stories of various topics, ranging from 2 to 28 sentences and from 28 to 734 words in length, with an average length of 275 words per story. To avoid the variability of human generated abstracts, human summarizers built an extract-based golden standard. Paragraphs were chosen as the basic textual unit because they are self-contained meaning units. In most of the cases, paragraphs contained a single sentence. Every paragraph in a story was ranked from 0 to 2, according to its relevance. 31 human judges summarized the corpus, so that at least 5 different evaluations were obtained for each story. Golden standards were obtained coming as close as possible to the 10% of the length of the original text (19% compression average). The two main shortcomings of this corpus are its small size and the fact that it belongs to the journalistic genre. However, we know of no other corpus for summary evaluation in Spanish. 5.2 Performance of the Lexical Chain System The performance of the Lexical Chain System with no discourse structural information was taken as the base to improve. (Fuentes and Rodriguez, 2002) report on a number of experiments to evaluate the effect of different parameters on the results of lexical chains. To keep comparability with the golden standard, and to adequately calculate precision and recall measures, paragraph-sized TUs were extracted at 10% compression rate. Some parameters were left unaltered for the of the experiment set: only extrathe experiments reported here, one-paragraph news were dropped, resulting in a final set of Ill news stories.</abstract>
<note confidence="0.906716875">Precision Recall Cosine Lead .95 .85 .90 SweSum .90 .81 .87 HEURISTIC 1 Lex. Chains .82 .81 .85 Lex. Chains .85 .85 .88 + PN Chains Lex. Chains .83 .83 .87 + PN Chains + coRef Chains Lex. Chains .88 .88 .90 + PN Chains + coRef Chains + 1st TU HEURISTIC 2 Lex. Chains .71 .72 .79 Lex. Chains .73 .74 .81 + PN Chains Lex. Chains .70 .71 .78 + PN Chains + coRef Chains Lex. Chains .82 .82 .86 + PN Chains + coRef Chains</note>
<abstract confidence="0.981500516129032">1st TU 1: of the lexical chain Summarizer were built, no information from defined noun phrases or trigger words could be used and only short co-reference chains were built. Results are presented in Table I. The first column in the table shows the main parameters governing each trial: simple lexical chains, lexical chains successively augmented with proper noun and co-Reference chains, and finally giving special weighting to the 1st TU because of global document structure appliable to the journalistic genre. Two heuristics schemata were experimented: 1 as most relevant the first TU by a strong chain, while 2 highest the TU crossed by the maximum of strong chains. An evaluation of SweSum (SweSum, 2002), a summarization system available for Spanish, is also provided as a comparison ground. Trials with SweSum were carried out with the default parameters of the system. In addition, the first paragraph of every text, the so-called lead summary, was taken as a dummy baseline. As can be seen in Table 1, the lead achieves the best results, with almost the best possible score. This is due to the pyramidal organisation of the journalistic genre, that causes most relevant information to be placed at the beginning of the text. Consequently, any heuristic assigning more releto the beginning of the text will achieve bet-</abstract>
<title confidence="0.91001925">Lexical Chains Removing Satellites Rhetorical Information Lexical Chains</title>
<author confidence="0.30822">Rhetorical</author>
<author confidence="0.30822">Argurnentati</author>
<abstract confidence="0.971766561403509">Lexical Chains rt,rie prectston VI cosinus L t-tt- L Named Eat NEI 5 ter results in this kind of genre. This is the case for default parameters of SweSum and 1. However, it must be noted that lexical chain summarizer produces results with high cosine and low precision, while SweSum yields high precision and low cosine. This means that, while the textual units extracted by the summarizer are not identical to the ones in the golden standard, their content is not dissimilar. This seems to indicate that the summarizer successfully captures content-based relevance, which is genreindependent. Consequently, the lexical chain summarizer should be able to capture relevance when applied to non-journalistic texts. This seems to be by the fact that improves coover precision four points higher than heuris- 1, seems more genre-dependent. Unexpectedly, co-reference chains cause a decrease in the performance of the system. This may be due to their limited length, and also to the fact that both full forms and pronouns are given the same score, which does not capture the difference in relevance signalled by the difference in form. 5.3 Results of the Integration of Heterogenous Discursive Informations Structural discursive information was integrated with only those parameters of the lexical chain summarizer that exploited general discursive in- I not considered because it is too genre-dependent. No co-reference information was taken into account, since it does not seem to yield any improvement. The results of integrating lexical chains with discourse structural information can be seen in Table 2. Following the design sketched in Figure 5, the performance of the lexical chains summarizer was first evaluated on a text where satellites had been removed. As stated by (Brunn et al., 2001; Alonso and Fuentes, 2002), removing satellites slightly improves the relevance assessment of the lexical chainer (by one point). Secondly, discourse coherence information was incorporated. Rhetorical and argumentative informations were distinguished, since the first identifies mainly unimportant parts of text and the second identifies both important and unimportant. Identifying satellites instead of removing them 1 Precision I Recall 1 Cosine</abstract>
<title confidence="0.658656818181818">Sentence Compression + Lexical Chains Sentence Compression .74 .75 .70 + Lexical Chains + PN Chains Sentence Compression .86 .85 .76 + Lexical Chains + PN Chains + 1st TU Rhetoecal Information + Lexical Chains</title>
<note confidence="0.8348422">Rhetorical Information .74 .76 .82 + Lex. Chains + PN Chains Rhetorical Information .83 .84 .86 + Lex. Chains + PN Chains + 1st TU Rhetorical + Argumentative + Lexical Chains Rhetorical Information .79 .80 .84 + Argumentative + Lex. Chains + PN Chains Rhetorical Information .84 .85 .87</note>
<title confidence="0.67017">Argumentative</title>
<author confidence="0.702364">Chains</author>
<abstract confidence="0.993170018018018">PN Chains + 1st TU 2: of the integration of lexical chains and discourse structural information yields only a slight improvement on recall (from .75 to .76), but significantly improves cosine (from .70 to .82). When argumentative information is provided, an improvement of .5 in performance is observed in all three metrics in comparison to removing satellites. As can be expected, ranking the first TU higher results in better measures, because of the nature of the genre. When this parameter is set, removing satellites outperforms the results obtained by taking into account discourse structural information in precision. However, this can also be due to the fact that when the text is compressed, TUs are shorter, and a higher number of them can be extracted within the fixed compression rate. It must be noted, though, that recall does not drop for these summaries. Lastly, intra-sentential and sentential satellites of the best summary obtained by lexical chains were removed, increasing compression of the resulting summaries from an average 18.84% for lexical chain summaries to a 14.43% for summaries which were sentence-compressed. Moreover, since sentences were shortened, readability increased, which can be considered as a fur- 6 ther factor of compression. However, these summaries have not been evaluated with the MEADeval package because no golden standard was available for textual units smaller than paragraphs. Precision and recall measures could not be calculated for summaries that removed satellites, because they could not be compared with the golden standard, consisting only full sentences. 5.4 Discussion The presented evaluation successfully shows the improvements of integrating cohesion and coherence, but it has two weak points. First, the small size of the corpus and the fact that it represents a single genre, which does not allow for safe generalisations. Second, the fact that evaluation metrics fall short in assessing the improvements yielded by the combination of these two discursive informations, since they cannot account for quantitative improvements at granularity levels different from the unit used in the golden standard, and therefore a full evaluation of summaries involving sentence compression is precluded. Moreover, qualitative improvements on general text coherence cannot be captured, nor their impact on summary readability. stated by (Goldstein et al., 1999), of the unresolved problems in summarization evaluation is how to penalize extraneous non-useful informacontained in a summary&amp;quot;. have tried to address this problem by identifying text segments which carry non-useful information, but the presented metrics do not capture this improvement. 6 Conclusions and Future Work We have shown that the collaborative integration of heterogeneous discursive information yields an improvement on the reperesentation of source text, as can be seen by improvements in resulting summaries. Although this enriched representation does not outperform a dummy baseline consisting of taking the first paragraph of the text, we have argued that the resulting representation of text is genre-independent and succeeds in capturing content relevance, as shown by cosine measures. Since the properties exploited by the presented system are text-bound and follow general principles of text organization, they can be considered to have language-wide validity. This means that the system is domain-independent, though it can be easily tuned to different genres. Moreover, the system presents portability to a variety of languages, as long as it has the knowledge sources required, basically, shallow tools for morpho-syntactical analysis, a version of WordNet for building and ranking lexical chains, and a lexicon of discourse markers for obtaining a certain discourse structure. Future work concerning the lexical chain summarizer will be focussed in building longer lexical chains, exploiting other relations in EWN, merging chains and even merging heterogeneous information. Improvements in the analysis of structural discursive information include enhancing the scope to paragraph and global document level, integrating heterogeneous discursive information and proving language-wide validity of Discourse Marker information. To provide an adequate assessment of the achieved improvements, the evaluation procedure is currently being changed. Given the enormous cost of building a comprehensive corpus for summary evaluation, the system has been partially adapted to English, so that it can be evaluated with the data and procedures of (DUC, 2002). Nevertheless, our future efforts will also be directed to gathering a corpus of Spanish texts with abstracts from which to automatically obtain a corpus of extracts with their corresponding texts, as proposed by (Marcu, 1999). Concerning qualitative evaluation, we will try to apply evaluation metrics that are able to capture content and coherence aspects of summaries, such as more complex content similarity or readability measures.</abstract>
<note confidence="0.875904324324324">7 Acknowledgements This research has been conducted thanks to a grant assoto the X-TRACT project, PB98-1226 of the Spanish Research Department. It has also been partially funded by projects HERMES (TIC2000-0335-0O3-02), PE- TRA (TIC2000-1735-0O2-02), and by CLiC (Centre de Lllengutatge i Computacio). References Laura Alonso and Irene Castellon. 2001. Towards a delimitation of discursive segment for natural language processing In International Workshop on Seman- Pragmatics and Rhetoric, - San Sebastian, November. 7 Laura Alonso and Maria Fuentes. 2002. Collaborating disfor text summarisation. In of the Seventh ESSLLI Student Session. Laura Alonso, Irene Castellon, and Lluis Padre,. 2002a. Design and implementation of a spanish discourse marker In Laura Alonso, Irene Castellon, and Lillis PadrO. 2002b. X-tractor: A tool for extracting discourse markers. In LREC 2002 workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Lan- Data, Palmas. C. Anscombre and 0. Ducrot. 1983. clans langue. Montse Arevalo, Xavi Carreras, Lids Marquez, M.Antonia Marti, Liu&amp; Padre&apos;, and M.Jose Simon. 2002. A proposal for wide-coverage spanish named entity recognition. del Len guaje Natural, Asher and Alex Lascarides. 2002. Logic of University Press. Barzilay. 1997. Chains for Summarization. Ph.D. thesis, Ben-Gurion University of the Negev. Meru Brunn, Yllias Chali, and Christopher J. Pinchak. 2001. Summarization using lexical chains. In on</note>
<affiliation confidence="0.672982">Text Summarization in conjunction with the ACM SIGIR</affiliation>
<address confidence="0.959915">2001, Orleans, Louisiana.</address>
<author confidence="0.849637">An environ-</author>
<abstract confidence="0.835296333333333">ment for morphosyntactic processing of unrestricted spantext. In International Conference on Language and Evaluation (LREC&apos;98), Spain.</abstract>
<note confidence="0.8931006">Simon H. Corston-Oliver and W. Dolan. 1999. Less is more: index terms from subordinate clauses. In Annual Meeting of the Association for Computational Lin- (ACL&apos;99), 348 - 356. DUC. 2002. DUC-document understanding conference.</note>
<web confidence="0.964326">http://duc.nist.gov/.</web>
<author confidence="0.921708">K Forbes</author>
<author confidence="0.921708">E Miltsakaki</author>
<author confidence="0.921708">R Prasad</author>
<author confidence="0.921708">A Sarkar</author>
<author confidence="0.921708">A Joshi</author>
<abstract confidence="0.909779">and B. Webber. 2003. D-LTAG system discourse parswith a lexicalized tree-adjoining grammar. of Logic and Information. appear. Maria Fuentes and Horacio Rodriguez. 2002. Using cohesive properties of text for automatic summarization. In</abstract>
<note confidence="0.711148882352941">JOTRI&apos;02. Jade Goldstein, Vibhu Mittal, Mark Kantrowitz, and Jaime Carbonell. 1999. Summarizing text documents: Sentence and evaluation metrics. In A. K. Halliday and R. Hasan. 1976. in English. English Language Series. Longman Group Ltd. Alistair Knott, Jon Oberlander, Mick O&apos;Donnell, and Chris Mellish. 2001. Beyond elaboration: The interaction of relations and focus in coherent text. In Ted Sanders, Joust and Wilbert Spooren, editors, represenlinguistic and psycholinguistic aspects, 181- 196. Benjamins. Mani. 2001. Summarization. Language Processing. John Benjamins Publishing Company. William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text</note>
<author confidence="0.44814">Rhetorical Parsing</author>
<author confidence="0.44814">Summariza-</author>
<affiliation confidence="0.7042565">and Generation of Natural Language Texts. thesis, Department of Computer Science, University of</affiliation>
<address confidence="0.770447">Toronto, Toronto, Canada.</address>
<note confidence="0.825642857142857">Daniel Marcu. 1999. The automatic construction of largecorpora for summarization research. In 2002. MEADeval. http://perun.si.umich.edu/clair/meadeval/. Jane Morris and Graeme Hirst. 1991. Lexical cohesion, the and the structure of text. lin- M. Palomar, A. Ferrandez, L. Moreno, P. Martinez-Barco, J. Peral, M. Saiz-Noeda, and R. Mu noz. 2001. An algofor anaphora resolution in spanish texts. Computa- Linguistics, Livia Polanyi. 1988. A formal model of the structure of of Pragmatics, Schank and R. Abelson. 1977. Plans, Goals, and Erlbaum, Hillsdale, NJ. SweSum. 2002. http://www.nada.kth.set xmartin/</note>
<abstract confidence="0.9838155">swesum/index-eng.html. Arie Verhagen. 2001. Subordination and discourse segmentation revisited, or: Why matrix clauses may be more dependent than complements. In Ted Sanders, Joost and Wilbert Spooren, editors, Rep- Linguistic and psychological aspects, 337-357. John Benjamins. Vossen, editor. 1998. WordNet: a multilingual with lexical semantic networks. Academic Publishers.</abstract>
<intro confidence="0.726279">8</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Alonso</author>
<author>Irene Castellon</author>
</authors>
<title>Towards a delimitation of discursive segment for natural language processing applications.</title>
<date>2001</date>
<booktitle>In First International Workshop on Semantics, Pragmatics and Rhetoric,</booktitle>
<location>Donostia - San Sebastian,</location>
<contexts>
<context position="10295" citStr="Alonso and Castellon, 2001" startWordPosition="1610" endWordPosition="1613">mely causality, parallelism and context. • argumentation: in the line of (Anscombre and Ducrot, 1983), three argumentative moves are distinguished: progression, elaboration and revision. • structure: following the notion of right frontier (Polanyi, 1988), symmetric and asymmetric relations are distinguished. • syntax: describes the relation of the DM with the rest of the elements at the discourse level, in the line of (Forbes et al., 2003), mainly used for discourse segmentation. The information stored in this DM lexicon was used for identifying inter- and intra-sentential discourse segments (Alonso and Castellon, 2001) and the discursive relations holding between them. Discourse segments were taken as Textual Units by the Lexical Chain summarizer, thus allowing a finer granularity level than sentences. Two combinations of DM descriptive features were used, in order to account for the interaction of different structural information with the lexical information of lexical chains. On the one hand, nucleus-satellite relations were identified by the combination of matter and structure dimensions of DMs. This rhetorical information yielded a hierarchical structure of text, so that satellites are subordinate to nu</context>
</contexts>
<marker>Alonso, Castellon, 2001</marker>
<rawString>Laura Alonso and Irene Castellon. 2001. Towards a delimitation of discursive segment for natural language processing applications. In First International Workshop on Semantics, Pragmatics and Rhetoric, Donostia - San Sebastian, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Alonso</author>
<author>Maria Fuentes</author>
</authors>
<title>Collaborating discourse for text summarisation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh</booktitle>
<publisher>ESSLLI Student Session.</publisher>
<contexts>
<context position="4652" citStr="Alonso and Fuentes, 2002" startWordPosition="710" endWordPosition="713">ds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chain members if they occur in irrelevant locations such as subordinate clauses, and therefore only consider chain candidates in main clauses. However, syntactical subordination does not always map discursive relevance. For example, in clauses expressing finality or dominated by a verb of cognition, like Y said that X, the syntactically subordinate clause X is discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 2002), we showed that identify</context>
<context position="19460" citStr="Alonso and Fuentes, 2002" startWordPosition="3095" endWordPosition="3098">sive information was integrated with only those parameters of the lexical chain summarizer that exploited general discursive information. Heuristic I was not considered because it is too genre-dependent. No co-reference information was taken into account, since it does not seem to yield any improvement. The results of integrating lexical chains with discourse structural information can be seen in Table 2. Following the design sketched in Figure 5, the performance of the lexical chains summarizer was first evaluated on a text where satellites had been removed. As stated by (Brunn et al., 2001; Alonso and Fuentes, 2002), removing satellites slightly improves the relevance assessment of the lexical chainer (by one point). Secondly, discourse coherence information was incorporated. Rhetorical and argumentative informations were distinguished, since the first identifies mainly unimportant parts of text and the second identifies both important and unimportant. Identifying satellites instead of removing them 1 Precision I Recall 1 Cosine Sentence Compression + Lexical Chains Sentence Compression .74 .75 .70 + Lexical Chains + PN Chains Sentence Compression .86 .85 .76 + Lexical Chains + PN Chains + 1st TU Rhetoec</context>
</contexts>
<marker>Alonso, Fuentes, 2002</marker>
<rawString>Laura Alonso and Maria Fuentes. 2002. Collaborating discourse for text summarisation. In Proceedings of the Seventh ESSLLI Student Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Alonso</author>
<author>Irene Castellon</author>
<author>Lluis Padre</author>
</authors>
<title>Design and implementation of a spanish discourse marker lexicon.</title>
<date>2002</date>
<booktitle>In SEPLIV,</booktitle>
<location>Valladolid.</location>
<contexts>
<context position="11754" citStr="Alonso et al., 2002" startWordPosition="1835" endWordPosition="1838">ssion of the argumentation. These two kinds of structural analyses are complementary. Rhetorical information is mainly effective at discovering local coherence structures, but it is unreliable when analyzing macrostructure. As (Knott et al., 2001) argue, a different kind of analysis is needed to track coherence throughout a whole text; in their case the alternative information used is focus, we have opted for argumentative orientation. Argumentative information accounts for a higher-level structure, although it doesn&apos;t provide much detail about it. This lexicon has been developed for Spanish (Alonso et al., 2002a). Nevertheless, the structure of the DM lexicon and the discourse parsing tools based on it is highly portable, and versions 3 cleaning up .01 semantic Lagging ci IC co—reference rules trigger—words TEXT Textual Unit segmentation morphological analysis Lexical Unit segmentation co—reference resolution morphosyntactical analysis PN rules discourse segmentation discourse markers rhetorical relation interpretation ; heuristics EuroWN ; ! i RIIETORICAL . i I INFORMATION i i , ! .— ,..• i ..•&amp;quot;* ; i ; ; i ; PRE—PROCESSED TEXT LEXICAL CHAINER CHAINS OUTPUT textual units ; SENTENCE ; !. COMPRESSION </context>
</contexts>
<marker>Alonso, Castellon, Padre, 2002</marker>
<rawString>Laura Alonso, Irene Castellon, and Lluis Padre,. 2002a. Design and implementation of a spanish discourse marker lexicon. In SEPLIV, Valladolid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Alonso</author>
<author>Irene Castellon</author>
<author>Lillis PadrO</author>
</authors>
<title>X-tractor: A tool for extracting discourse markers.</title>
<date>2002</date>
<booktitle>In LREC</booktitle>
<location>Las Palmas.</location>
<contexts>
<context position="11754" citStr="Alonso et al., 2002" startWordPosition="1835" endWordPosition="1838">ssion of the argumentation. These two kinds of structural analyses are complementary. Rhetorical information is mainly effective at discovering local coherence structures, but it is unreliable when analyzing macrostructure. As (Knott et al., 2001) argue, a different kind of analysis is needed to track coherence throughout a whole text; in their case the alternative information used is focus, we have opted for argumentative orientation. Argumentative information accounts for a higher-level structure, although it doesn&apos;t provide much detail about it. This lexicon has been developed for Spanish (Alonso et al., 2002a). Nevertheless, the structure of the DM lexicon and the discourse parsing tools based on it is highly portable, and versions 3 cleaning up .01 semantic Lagging ci IC co—reference rules trigger—words TEXT Textual Unit segmentation morphological analysis Lexical Unit segmentation co—reference resolution morphosyntactical analysis PN rules discourse segmentation discourse markers rhetorical relation interpretation ; heuristics EuroWN ; ! i RIIETORICAL . i I INFORMATION i i , ! .— ,..• i ..•&amp;quot;* ; i ; ; i ; PRE—PROCESSED TEXT LEXICAL CHAINER CHAINS OUTPUT textual units ; SENTENCE ; !. COMPRESSION </context>
</contexts>
<marker>Alonso, Castellon, PadrO, 2002</marker>
<rawString>Laura Alonso, Irene Castellon, and Lillis PadrO. 2002b. X-tractor: A tool for extracting discourse markers. In LREC 2002 workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Anscombre</author>
</authors>
<title>L&apos;argumentation clans la langue.</title>
<date>1983</date>
<journal>Mardaga.</journal>
<marker>Anscombre, 1983</marker>
<rawString>J. C. Anscombre and 0. Ducrot. 1983. L&apos;argumentation clans la langue. Mardaga.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Montse Arevalo</author>
<author>Xavi Carreras</author>
<author>Lids Marquez</author>
<author>M Antonia Marti</author>
<author>Liu&amp; Padre&apos;</author>
<author>M Jose Simon</author>
</authors>
<title>A proposal for wide-coverage spanish named entity recognition.</title>
<date>2002</date>
<booktitle>Procesamiento del Len guaje Natural,</booktitle>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="13127" citStr="Arevalo et al., 2002" startWordPosition="2047" endWordPosition="2050"> chains (left) and discourse structural (right) for English and Catalan are being developed by bootstraping techniques (Alonso et al., 2002b). 5 Experiments A number of experiments were carried out in order to test whether taking into account the structural status of the textual unit where a chain member occurs can improve the relevance assessment of lexical chains (see Figure 2). Since the DM lexicon and the evaluation corpus were available only for Spanish, the experiments were limited to that language. Linguistic pre-processing was performed with the CLiC-TALP system (Carmona et al., 1998; Arevalo et al., 2002). For the evaluation of the different experiments, the evaluation software MEADeval (MEA, 2002) was used, to compare the obtained summaries with a golden standard (see Section 5.1). From this package, the usual precision and recall measures were selected, as well as the simple cosine. Simple cosine (simply cosine from now on) was chosen because it provides a measure of similarity between the golden standard and the obtained extracts, overcoming the limitations of measures depending on concrete textual units. 5.1 Golden Standard The corpus used for evaluation was created within Hermes projeal ,</context>
</contexts>
<marker>Arevalo, Carreras, Marquez, Marti, Padre&apos;, Simon, 2002</marker>
<rawString>Montse Arevalo, Xavi Carreras, Lids Marquez, M.Antonia Marti, Liu&amp; Padre&apos;, and M.Jose Simon. 2002. A proposal for wide-coverage spanish named entity recognition. Procesamiento del Len guaje Natural, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>The Logic of Conversation.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9597" citStr="Asher and Lascarides, 2002" startWordPosition="1506" endWordPosition="1509">ructures, such as recurring parallelisms, examples or adjuncts. Nevertheless, the relative relevance of these elements is usually marked structurally, either by sentential or discursive syntax. 4 Incorporating Rhetorical and Argumentative Relations The lexical chain summarizer was enhanced with discourse structural information as can be seen in Figure 1 (right). Following the approach of (Marcu, 1997), a partial representation of discourse structre was obtained by means of the information associated to a Discourse Marker (DM) lexicon. DMs are described in four dimensions: • matter: following (Asher and Lascarides, 2002), three different kinds of subject-matter meaning are distinguished, namely causality, parallelism and context. • argumentation: in the line of (Anscombre and Ducrot, 1983), three argumentative moves are distinguished: progression, elaboration and revision. • structure: following the notion of right frontier (Polanyi, 1988), symmetric and asymmetric relations are distinguished. • syntax: describes the relation of the DM with the rest of the elements at the discourse level, in the line of (Forbes et al., 2003), mainly used for discourse segmentation. The information stored in this DM lexicon wa</context>
</contexts>
<marker>Asher, Lascarides, 2002</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2002. The Logic of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Lexical Chains for Summarization.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Ben-Gurion University of the Negev.</institution>
<contexts>
<context position="5673" citStr="Barzilay, 1997" startWordPosition="871" endWordPosition="872">ike Y said that X, the syntactically subordinate clause X is discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 2002), we showed that identifying and removing discursively motivated satellites yields an improvement in the task of text summarization. Nevertheless, we will show that a more adequate representation of the source text can be obtained by ranking chain members in accordance to their position in the discourse structure, instead of simply eliminating them. The lexical chain summarizer follows the work of (Morris and Hirst, 1991) and (Barzilay, 1997). As can be seen in Figure 1 (left) the text is first segmented, at different granularity levels (paragraph, sentence, clause) depending on the application. To detect chain candidates, the text is morphologically analysed, and the lemma and POS of each word are obtained. Then, Named Entities are identified and classified in a gazzetteer. For Spanish, a simplified version of (Palomar et al., 2001) extracts co-referenece links for some types of pronouns, dropping off the constraints and rules involving syntactic information. Semantic tagging of common nouns is been performed with is-a relations </context>
</contexts>
<marker>Barzilay, 1997</marker>
<rawString>Regina Barzilay. 1997. Lexical Chains for Summarization. Ph.D. thesis, Ben-Gurion University of the Negev.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meru Brunn</author>
<author>Yllias Chali</author>
<author>Christopher J Pinchak</author>
</authors>
<title>Text Summarization using lexical chains.</title>
<date>2001</date>
<booktitle>In Workshop on Text Summarization in conjunction with the ACM SIGIR Conference</booktitle>
<location>New Orleans, Louisiana.</location>
<contexts>
<context position="4625" citStr="Brunn et al., 2001" startWordPosition="706" endWordPosition="709">ion of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chain members if they occur in irrelevant locations such as subordinate clauses, and therefore only consider chain candidates in main clauses. However, syntactical subordination does not always map discursive relevance. For example, in clauses expressing finality or dominated by a verb of cognition, like Y said that X, the syntactically subordinate clause X is discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 200</context>
<context position="19433" citStr="Brunn et al., 2001" startWordPosition="3091" endWordPosition="3094">ns Structural discursive information was integrated with only those parameters of the lexical chain summarizer that exploited general discursive information. Heuristic I was not considered because it is too genre-dependent. No co-reference information was taken into account, since it does not seem to yield any improvement. The results of integrating lexical chains with discourse structural information can be seen in Table 2. Following the design sketched in Figure 5, the performance of the lexical chains summarizer was first evaluated on a text where satellites had been removed. As stated by (Brunn et al., 2001; Alonso and Fuentes, 2002), removing satellites slightly improves the relevance assessment of the lexical chainer (by one point). Secondly, discourse coherence information was incorporated. Rhetorical and argumentative informations were distinguished, since the first identifies mainly unimportant parts of text and the second identifies both important and unimportant. Identifying satellites instead of removing them 1 Precision I Recall 1 Cosine Sentence Compression + Lexical Chains Sentence Compression .74 .75 .70 + Lexical Chains + PN Chains Sentence Compression .86 .85 .76 + Lexical Chains +</context>
</contexts>
<marker>Brunn, Chali, Pinchak, 2001</marker>
<rawString>Meru Brunn, Yllias Chali, and Christopher J. Pinchak. 2001. Text Summarization using lexical chains. In Workshop on Text Summarization in conjunction with the ACM SIGIR Conference 2001, New Orleans, Louisiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Carmona</author>
<author>Sergi Cervell</author>
<author>Lluis Marquez</author>
<author>M Antonia Mart</author>
<author>Lluis Padro</author>
<author>Roberto Placer</author>
<author>Horacio Rodriguez</author>
<author>Mariona Taule</author>
<author>Jordi Turmo</author>
</authors>
<title>An environment for morphosyntactic processing of unrestricted spanish text.</title>
<date>1998</date>
<booktitle>In First International Conference on Language Resources and Evaluation (LREC&apos;98),</booktitle>
<location>Granada,</location>
<contexts>
<context position="13104" citStr="Carmona et al., 1998" startWordPosition="2043" endWordPosition="2046">e information: lexical chains (left) and discourse structural (right) for English and Catalan are being developed by bootstraping techniques (Alonso et al., 2002b). 5 Experiments A number of experiments were carried out in order to test whether taking into account the structural status of the textual unit where a chain member occurs can improve the relevance assessment of lexical chains (see Figure 2). Since the DM lexicon and the evaluation corpus were available only for Spanish, the experiments were limited to that language. Linguistic pre-processing was performed with the CLiC-TALP system (Carmona et al., 1998; Arevalo et al., 2002). For the evaluation of the different experiments, the evaluation software MEADeval (MEA, 2002) was used, to compare the obtained summaries with a golden standard (see Section 5.1). From this package, the usual precision and recall measures were selected, as well as the simple cosine. Simple cosine (simply cosine from now on) was chosen because it provides a measure of similarity between the golden standard and the obtained extracts, overcoming the limitations of measures depending on concrete textual units. 5.1 Golden Standard The corpus used for evaluation was created </context>
</contexts>
<marker>Carmona, Cervell, Marquez, Mart, Padro, Placer, Rodriguez, Taule, Turmo, 1998</marker>
<rawString>Josep Carmona, Sergi Cervell, Lluis Marquez, M. Antonia Mart, Lluis Padro, Roberto Placer, Horacio Rodriguez, Mariona Taule, and Jordi Turmo. 1998. An environment for morphosyntactic processing of unrestricted spanish text. In First International Conference on Language Resources and Evaluation (LREC&apos;98), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon H Corston-Oliver</author>
<author>W Dolan</author>
</authors>
<title>Less is more: Eliminating index terms from subordinate clauses.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99),</booktitle>
<pages>348--356</pages>
<contexts>
<context position="4160" citStr="Corston-Oliver and Dolan, 1999" startWordPosition="634" endWordPosition="637"> have been distinguished in the discursive structure of a text: cohesion and coherence. As defined by (Halliday and Hasan, 1976), cohesion tries to account for relationships among the elements of a text. Four broad categories of cohesion are identified: reference, ellipsis, conjunction, and lexical cohesion. On the other hand, coherence is represented in terms of relations between text segments, such as elaboration, cause or explanation. (Mani, 2001) argues that an integration of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chai</context>
</contexts>
<marker>Corston-Oliver, Dolan, 1999</marker>
<rawString>Simon H. Corston-Oliver and W. Dolan. 1999. Less is more: Eliminating index terms from subordinate clauses. In 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99), pages 348 - 356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>DUC-document understanding conference.</title>
<date>2002</date>
<note>http://duc.nist.gov/.</note>
<contexts>
<context position="1736" citStr="DUC, 2002" startWordPosition="261" endWordPosition="262">enerate the summary text. Much of the early work in summarization has been concerned with detecting relevant elements of text and presenting them in the &amp;quot;shortest possible form&amp;quot;. More recently, an increasing attention has been devoted to the adequacy of the resulting texts to a human user. Well-formedness, cohesion and coherence are currently under inspection, not only because they improve the quality of a summary as a text, but also because they can reduce the final summary by reducing the reading time and cost that is needed to process it. TS systems that performed best in last DUC contest (DUC, 2002) apply template-driven summarization, by information-extraction procedures in the line of (Schank and Abelson, 1977). This approach yields very good results in assessing relevance and keeping well-formedness, but it is dependent on a clearly defined representation of the information need to be fulfilled and, in most cases, also on some regularities of the kind of texts to be summarized. In more generic TS, genre-dependent regularities are not always found, and template-driven analysis cannot capture the variety of texts. In addition, the information need is usually very fuzzy. In these circums</context>
<context position="25029" citStr="DUC, 2002" startWordPosition="3952" endWordPosition="3953">en merging heterogeneous information. Improvements in the analysis of structural discursive information include enhancing the scope to paragraph and global document level, integrating heterogeneous discursive information and proving language-wide validity of Discourse Marker information. To provide an adequate assessment of the achieved improvements, the evaluation procedure is currently being changed. Given the enormous cost of building a comprehensive corpus for summary evaluation, the system has been partially adapted to English, so that it can be evaluated with the data and procedures of (DUC, 2002). Nevertheless, our future efforts will also be directed to gathering a corpus of Spanish texts with abstracts from which to automatically obtain a corpus of extracts with their corresponding texts, as proposed by (Marcu, 1999). Concerning qualitative evaluation, we will try to apply evaluation metrics that are able to capture content and coherence aspects of summaries, such as more complex content similarity or readability measures. 7 Acknowledgements This research has been conducted thanks to a grant associated to the X-TRACT project, PB98-1226 of the Spanish Research Department. It has also</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. DUC-document understanding conference. http://duc.nist.gov/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes</author>
<author>E Miltsakaki</author>
<author>R Prasad</author>
<author>A Sarkar</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>D-LTAG system - discourse parsing with a lexicalized tree-adjoining grammar.</title>
<date>2003</date>
<journal>Journal of Language, Logic and Information.</journal>
<note>to appear.</note>
<contexts>
<context position="10111" citStr="Forbes et al., 2003" startWordPosition="1584" endWordPosition="1587">arker (DM) lexicon. DMs are described in four dimensions: • matter: following (Asher and Lascarides, 2002), three different kinds of subject-matter meaning are distinguished, namely causality, parallelism and context. • argumentation: in the line of (Anscombre and Ducrot, 1983), three argumentative moves are distinguished: progression, elaboration and revision. • structure: following the notion of right frontier (Polanyi, 1988), symmetric and asymmetric relations are distinguished. • syntax: describes the relation of the DM with the rest of the elements at the discourse level, in the line of (Forbes et al., 2003), mainly used for discourse segmentation. The information stored in this DM lexicon was used for identifying inter- and intra-sentential discourse segments (Alonso and Castellon, 2001) and the discursive relations holding between them. Discourse segments were taken as Textual Units by the Lexical Chain summarizer, thus allowing a finer granularity level than sentences. Two combinations of DM descriptive features were used, in order to account for the interaction of different structural information with the lexical information of lexical chains. On the one hand, nucleus-satellite relations were</context>
</contexts>
<marker>Forbes, Miltsakaki, Prasad, Sarkar, Joshi, Webber, 2003</marker>
<rawString>K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar, A. Joshi, and B. Webber. 2003. D-LTAG system - discourse parsing with a lexicalized tree-adjoining grammar. Journal of Language, Logic and Information. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Fuentes</author>
<author>Horacio Rodriguez</author>
</authors>
<title>Using cohesive properties of text for automatic summarization.</title>
<date>2002</date>
<booktitle>In JOTRI&apos;02.</booktitle>
<contexts>
<context position="15128" citStr="Fuentes and Rodriguez, 2002" startWordPosition="2371" endWordPosition="2374">vance. 31 human judges summarized the corpus, so that at least 5 different evaluations were obtained for each story. Golden standards were obtained coming as close as possible to the 10% of the length of the original text (19% compression average). The two main shortcomings of this corpus are its small size and the fact that it belongs to the journalistic genre. However, we know of no other corpus for summary evaluation in Spanish. 5.2 Performance of the Lexical Chain System The performance of the Lexical Chain System with no discourse structural information was taken as the base to improve. (Fuentes and Rodriguez, 2002) report on a number of experiments to evaluate the effect of different parameters on the results of lexical chains. To keep comparability with the golden standard, and to adequately calculate precision and recall measures, paragraph-sized TUs were extracted at 10% compression rate. Some parameters were left unaltered for the whole of the experiment set: only strong or extra2For the experiments reported here, one-paragraph news were dropped, resulting in a final set of Ill news stories. Precision Recall Cosine Lead .95 .85 .90 SweSum .90 .81 .87 HEURISTIC 1 Lex. Chains .82 .81 .85 Lex. Chains .</context>
</contexts>
<marker>Fuentes, Rodriguez, 2002</marker>
<rawString>Maria Fuentes and Horacio Rodriguez. 2002. Using cohesive properties of text for automatic summarization. In JOTRI&apos;02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Mark Kantrowitz</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In SIGIR-99.</booktitle>
<contexts>
<context position="22844" citStr="Goldstein et al., 1999" startWordPosition="3625" endWordPosition="3628"> and the fact that it represents a single genre, which does not allow for safe generalisations. Second, the fact that evaluation metrics fall short in assessing the improvements yielded by the combination of these two discursive informations, since they cannot account for quantitative improvements at granularity levels different from the unit used in the golden standard, and therefore a full evaluation of summaries involving sentence compression is precluded. Moreover, qualitative improvements on general text coherence cannot be captured, nor their impact on summary readability. As stated by (Goldstein et al., 1999), &amp;quot;one of the unresolved problems in summarization evaluation is how to penalize extraneous non-useful information contained in a summary&amp;quot;. We have tried to address this problem by identifying text segments which carry non-useful information, but the presented metrics do not capture this improvement. 6 Conclusions and Future Work We have shown that the collaborative integration of heterogeneous discursive information yields an improvement on the reperesentation of source text, as can be seen by improvements in resulting summaries. Although this enriched representation does not outperform a dum</context>
</contexts>
<marker>Goldstein, Mittal, Kantrowitz, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Mark Kantrowitz, and Jaime Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In SIGIR-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English. English Language Series.</title>
<date>1976</date>
<publisher>Longman Group Ltd.</publisher>
<contexts>
<context position="3657" citStr="Halliday and Hasan, 1976" startWordPosition="560" endWordPosition="563">tractive informative summarization system that exploits the cohesive properties of text by building and ranking lexical chains (see Section 3). This system is enhanced with discourse coherence information (Section 5.3). Experiments were carried out on the combination of these two kinds of information, and results were evaluated on a Spanish news agency corpus (Section 5). 1 2 Previous Work on Combining 3 Summarizing with Lexical Chains Cohesion and Coherence Traditionally, two main components have been distinguished in the discursive structure of a text: cohesion and coherence. As defined by (Halliday and Hasan, 1976), cohesion tries to account for relationships among the elements of a text. Four broad categories of cohesion are identified: reference, ellipsis, conjunction, and lexical cohesion. On the other hand, coherence is represented in terms of relations between text segments, such as elaboration, cause or explanation. (Mani, 2001) argues that an integration of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. A. K. Halliday and R. Hasan. 1976. Cohesion in English. English Language Series. Longman Group Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O&apos;Donnell</author>
<author>Chris Mellish</author>
</authors>
<title>Beyond elaboration: The interaction of relations and focus in coherent text.</title>
<date>2001</date>
<pages>181--196</pages>
<editor>In Ted Sanders, Joust Schilperoord, and Wilbert Spooren, editors,</editor>
<publisher>Benjamins.</publisher>
<contexts>
<context position="11382" citStr="Knott et al., 2001" startWordPosition="1775" endWordPosition="1778">re dimensions of DMs. This rhetorical information yielded a hierarchical structure of text, so that satellites are subordinate to nucleus and they are accordingly considered less relevant. On the other hand, the argumentative line of text was traced via the argumentation and also structure DM dimensions, so that segments were tagged with their contribution to the progression of the argumentation. These two kinds of structural analyses are complementary. Rhetorical information is mainly effective at discovering local coherence structures, but it is unreliable when analyzing macrostructure. As (Knott et al., 2001) argue, a different kind of analysis is needed to track coherence throughout a whole text; in their case the alternative information used is focus, we have opted for argumentative orientation. Argumentative information accounts for a higher-level structure, although it doesn&apos;t provide much detail about it. This lexicon has been developed for Spanish (Alonso et al., 2002a). Nevertheless, the structure of the DM lexicon and the discourse parsing tools based on it is highly portable, and versions 3 cleaning up .01 semantic Lagging ci IC co—reference rules trigger—words TEXT Textual Unit segmentat</context>
</contexts>
<marker>Knott, Oberlander, O&apos;Donnell, Mellish, 2001</marker>
<rawString>Alistair Knott, Jon Oberlander, Mick O&apos;Donnell, and Chris Mellish. 2001. Beyond elaboration: The interaction of relations and focus in coherent text. In Ted Sanders, Joust Schilperoord, and Wilbert Spooren, editors, Text representation: linguistic and psycholinguistic aspects, pages 181-196. Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization. Nautral Language Processing.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="3983" citStr="Mani, 2001" startWordPosition="610" endWordPosition="611">sh news agency corpus (Section 5). 1 2 Previous Work on Combining 3 Summarizing with Lexical Chains Cohesion and Coherence Traditionally, two main components have been distinguished in the discursive structure of a text: cohesion and coherence. As defined by (Halliday and Hasan, 1976), cohesion tries to account for relationships among the elements of a text. Four broad categories of cohesion are identified: reference, ellipsis, conjunction, and lexical cohesion. On the other hand, coherence is represented in terms of relations between text segments, such as elaboration, cause or explanation. (Mani, 2001) argues that an integration of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried </context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Summarization. Nautral Language Processing. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organisation.</title>
<date>1988</date>
<tech>Text,</tech>
<pages>3--8</pages>
<contexts>
<context position="4284" citStr="Mann and Thompson, 1988" startWordPosition="651" endWordPosition="654">ohesion tries to account for relationships among the elements of a text. Four broad categories of cohesion are identified: reference, ellipsis, conjunction, and lexical cohesion. On the other hand, coherence is represented in terms of relations between text segments, such as elaboration, cause or explanation. (Mani, 2001) argues that an integration of these two kinds of discursive information would yield significant improvements in the task of text summarization. (Corston-Oliver and Dolan, 1999) showed that eliminating discursive satellites as defined by the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), yields an improvement in the task of Information Retrieval. Precision is improved because only words in discursively relevant text locations are taken into account as indexing terms, while traditional methods treat texts as unstructured bags of words. Some analogous experiments have been carried out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chain members if they occur in irrelevant locations such as subordinate clauses, and therefore only consider chain candidates in</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organisation. Text, 3(8):234-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Rhetorical Parsing, Summarization and Generation of Natural Language Texts.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada.</location>
<contexts>
<context position="9374" citStr="Marcu, 1997" startWordPosition="1472" endWordPosition="1473">Entity that is nominally conveying a piece of news in a document can present a very tight pattern of occurrence, without being actually relevant to the aim of the text. The same applies to other linguistic structures, such as recurring parallelisms, examples or adjuncts. Nevertheless, the relative relevance of these elements is usually marked structurally, either by sentential or discursive syntax. 4 Incorporating Rhetorical and Argumentative Relations The lexical chain summarizer was enhanced with discourse structural information as can be seen in Figure 1 (right). Following the approach of (Marcu, 1997), a partial representation of discourse structre was obtained by means of the information associated to a Discourse Marker (DM) lexicon. DMs are described in four dimensions: • matter: following (Asher and Lascarides, 2002), three different kinds of subject-matter meaning are distinguished, namely causality, parallelism and context. • argumentation: in the line of (Anscombre and Ducrot, 1983), three argumentative moves are distinguished: progression, elaboration and revision. • structure: following the notion of right frontier (Polanyi, 1988), symmetric and asymmetric relations are distinguish</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. The Rhetorical Parsing, Summarization and Generation of Natural Language Texts. Ph.D. thesis, Department of Computer Science, University of Toronto, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of largescale corpora for summarization research.</title>
<date>1999</date>
<booktitle>In SIGIR-99.</booktitle>
<note>MEADeval. http://perun.si.umich.edu/clair/meadeval/.</note>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. The automatic construction of largescale corpora for summarization research. In SIGIR-99. 2002. MEADeval. http://perun.si.umich.edu/clair/meadeval/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text.</title>
<date>1991</date>
<journal>Computational linguistics,</journal>
<pages>17--1</pages>
<contexts>
<context position="5652" citStr="Morris and Hirst, 1991" startWordPosition="866" endWordPosition="869">ted by a verb of cognition, like Y said that X, the syntactically subordinate clause X is discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 2002), we showed that identifying and removing discursively motivated satellites yields an improvement in the task of text summarization. Nevertheless, we will show that a more adequate representation of the source text can be obtained by ranking chain members in accordance to their position in the discourse structure, instead of simply eliminating them. The lexical chain summarizer follows the work of (Morris and Hirst, 1991) and (Barzilay, 1997). As can be seen in Figure 1 (left) the text is first segmented, at different granularity levels (paragraph, sentence, clause) depending on the application. To detect chain candidates, the text is morphologically analysed, and the lemma and POS of each word are obtained. Then, Named Entities are identified and classified in a gazzetteer. For Spanish, a simplified version of (Palomar et al., 2001) extracts co-referenece links for some types of pronouns, dropping off the constraints and rules involving syntactic information. Semantic tagging of common nouns is been performed</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion, the thesaurus, and the structure of text. Computational linguistics, 17(1):21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palomar</author>
<author>A Ferrandez</author>
<author>L Moreno</author>
<author>P Martinez-Barco</author>
<author>J Peral</author>
<author>M Saiz-Noeda</author>
<author>R Mu noz</author>
</authors>
<title>An algorithm for anaphora resolution in spanish texts.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="6072" citStr="Palomar et al., 2001" startWordPosition="936" endWordPosition="939">tained by ranking chain members in accordance to their position in the discourse structure, instead of simply eliminating them. The lexical chain summarizer follows the work of (Morris and Hirst, 1991) and (Barzilay, 1997). As can be seen in Figure 1 (left) the text is first segmented, at different granularity levels (paragraph, sentence, clause) depending on the application. To detect chain candidates, the text is morphologically analysed, and the lemma and POS of each word are obtained. Then, Named Entities are identified and classified in a gazzetteer. For Spanish, a simplified version of (Palomar et al., 2001) extracts co-referenece links for some types of pronouns, dropping off the constraints and rules involving syntactic information. Semantic tagging of common nouns is been performed with is-a relations by attaching Euro WordNet (Vossen, 1998) synsets to them. Named Entities are been semantically tagged with instance relations by a set of trigger words, like former president, queen, etc., associated to each of them in a gazzetteer. Semantic relations between common nouns and Named Entities can be established via the EWN synset of the trigger words associated to a each entity. Chain candidates ar</context>
</contexts>
<marker>Palomar, Ferrandez, Moreno, Martinez-Barco, Peral, Saiz-Noeda, noz, 2001</marker>
<rawString>M. Palomar, A. Ferrandez, L. Moreno, P. Martinez-Barco, J. Peral, M. Saiz-Noeda, and R. Mu noz. 2001. An algorithm for anaphora resolution in spanish texts. Computational Linguistics, 27(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>A formal model of the structure of discourse.</title>
<date>1988</date>
<journal>Journal of Pragmatics,</journal>
<pages>12--601</pages>
<contexts>
<context position="9922" citStr="Polanyi, 1988" startWordPosition="1553" endWordPosition="1554"> seen in Figure 1 (right). Following the approach of (Marcu, 1997), a partial representation of discourse structre was obtained by means of the information associated to a Discourse Marker (DM) lexicon. DMs are described in four dimensions: • matter: following (Asher and Lascarides, 2002), three different kinds of subject-matter meaning are distinguished, namely causality, parallelism and context. • argumentation: in the line of (Anscombre and Ducrot, 1983), three argumentative moves are distinguished: progression, elaboration and revision. • structure: following the notion of right frontier (Polanyi, 1988), symmetric and asymmetric relations are distinguished. • syntax: describes the relation of the DM with the rest of the elements at the discourse level, in the line of (Forbes et al., 2003), mainly used for discourse segmentation. The information stored in this DM lexicon was used for identifying inter- and intra-sentential discourse segments (Alonso and Castellon, 2001) and the discursive relations holding between them. Discourse segments were taken as Textual Units by the Lexical Chain summarizer, thus allowing a finer granularity level than sentences. Two combinations of DM descriptive feat</context>
</contexts>
<marker>Polanyi, 1988</marker>
<rawString>Livia Polanyi. 1988. A formal model of the structure of discourse. Journal of Pragmatics, 12:601-638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schank</author>
<author>R Abelson</author>
</authors>
<title>Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum,</title>
<date>1977</date>
<location>Hillsdale, NJ. SweSum.</location>
<note>http://www.nada.kth.set xmartin/ swesum/index-eng.html.</note>
<contexts>
<context position="1852" citStr="Schank and Abelson, 1977" startWordPosition="274" endWordPosition="277">elevant elements of text and presenting them in the &amp;quot;shortest possible form&amp;quot;. More recently, an increasing attention has been devoted to the adequacy of the resulting texts to a human user. Well-formedness, cohesion and coherence are currently under inspection, not only because they improve the quality of a summary as a text, but also because they can reduce the final summary by reducing the reading time and cost that is needed to process it. TS systems that performed best in last DUC contest (DUC, 2002) apply template-driven summarization, by information-extraction procedures in the line of (Schank and Abelson, 1977). This approach yields very good results in assessing relevance and keeping well-formedness, but it is dependent on a clearly defined representation of the information need to be fulfilled and, in most cases, also on some regularities of the kind of texts to be summarized. In more generic TS, genre-dependent regularities are not always found, and template-driven analysis cannot capture the variety of texts. In addition, the information need is usually very fuzzy. In these circumstances, the most reliable source of information on relevance and coherence properties of a text is the source text i</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>R. Schank and R. Abelson. 1977. Scripts, Plans, Goals, and Understanding. Lawrence Erlbaum, Hillsdale, NJ. SweSum. 2002. http://www.nada.kth.set xmartin/ swesum/index-eng.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arie Verhagen</author>
</authors>
<title>Subordination and discourse segmentation revisited, or: Why matrix clauses may be more dependent than complements.</title>
<date>2001</date>
<booktitle>Text Representation. Linguistic and psychological aspects,</booktitle>
<pages>337--357</pages>
<editor>In Ted Sanders, Joost Schilperoord, and Wilbert Spooren, editors,</editor>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="5196" citStr="Verhagen, 2001" startWordPosition="797" endWordPosition="798">d out in the area of TS. (Brunn et al., 2001; Alonso and Fuentes, 2002) claim that the performance of summarizers based on lexical chains can be improved by ignoring possible chain members if they occur in irrelevant locations such as subordinate clauses, and therefore only consider chain candidates in main clauses. However, syntactical subordination does not always map discursive relevance. For example, in clauses expressing finality or dominated by a verb of cognition, like Y said that X, the syntactically subordinate clause X is discursively nuclear, while the main clause is less relevant (Verhagen, 2001). In (Alonso and Fuentes, 2002), we showed that identifying and removing discursively motivated satellites yields an improvement in the task of text summarization. Nevertheless, we will show that a more adequate representation of the source text can be obtained by ranking chain members in accordance to their position in the discourse structure, instead of simply eliminating them. The lexical chain summarizer follows the work of (Morris and Hirst, 1991) and (Barzilay, 1997). As can be seen in Figure 1 (left) the text is first segmented, at different granularity levels (paragraph, sentence, clau</context>
</contexts>
<marker>Verhagen, 2001</marker>
<rawString>Arie Verhagen. 2001. Subordination and discourse segmentation revisited, or: Why matrix clauses may be more dependent than complements. In Ted Sanders, Joost Schilperoord, and Wilbert Spooren, editors, Text Representation. Linguistic and psychological aspects, pages 337-357. John Benjamins.</rawString>
</citation>
<citation valid="true">
<title>Euro WordNet: a multilingual database with lexical semantic networks.</title>
<date>1998</date>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. Euro WordNet: a multilingual database with lexical semantic networks. Kluwer Academic Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>