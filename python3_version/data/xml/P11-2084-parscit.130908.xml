<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.997599">
Identifying Word Translations from Comparable Corpora Using Latent
Topic Models
</title>
<author confidence="0.998274">
Ivan Vuli´c, Wim De Smet and Marie-Francine Moens
</author>
<affiliation confidence="0.8561965">
Department of Computer Science
K.U. Leuven
</affiliation>
<address confidence="0.528789">
Celestijnenlaan 200A
Leuven, Belgium
</address>
<email confidence="0.994413">
{ivan.vulic,wim.desmet,sien.moens}@cs.kuleuven.be
</email>
<sectionHeader confidence="0.998559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998517625">
A topic model outputs a set of multinomial
distributions over words for each topic. In
this paper, we investigate the value of bilin-
gual topic models, i.e., a bilingual Latent
Dirichlet Allocation model for finding trans-
lations of terms in comparable corpora with-
out using any linguistic resources. Experi-
ments on a document-aligned English-Italian
Wikipedia corpus confirm that the developed
methods which only use knowledge from
word-topic distributions outperform methods
based on similarity measures in the original
word-document space. The best results, ob-
tained by combining knowledge from word-
topic distributions with similarity measures in
the original space, are also reported.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973166666667">
Generative models for documents such as Latent
Dirichlet Allocation (LDA) (Blei et al., 2003) are
based upon the idea that latent variables exist which
determine how words in documents might be gener-
ated. Fitting a generative model means finding the
best set of those latent variables in order to explain
the observed data. Within that setting, documents
are observed as mixtures of latent topics, where top-
ics are probability distributions over words.
Our goal is to model and test the capability of
probabilistic topic models to identify potential trans-
lations from document-aligned text collections. A
representative example of such a comparable text
collection is Wikipedia, where one may observe arti-
cles discussing the same topic, but strongly varying
in style, length and even vocabulary, while still shar-
ing a certain amount of main concepts (or topics).
We try to establish a connection between such latent
topics and an idea known as the distributional hy-
pothesis (Harris, 1954) - words with a similar mean-
ing are often used in similar contexts.
Besides the obvious context of direct co-
occurrence, we believe that topic models are an ad-
ditional source of knowledge which might be used
to improve results in the quest for translation can-
didates extracted without the availability of a trans-
lation dictionary and linguistic knowledge. We de-
signed several methods, all derived from the core
idea of using word distributions over topics as an
extra source of contextual knowledge. Two words
are potential translation candidates if they are often
present in the same cross-lingual topics and not ob-
served in other cross-lingual topics. In other words,
a word w2 from a target language is a potential trans-
lation candidate for a word w1 from a source lan-
guage, if the distribution of w2 over the target lan-
guage topics is similar to the distribution of w1 over
the source language topics.
The remainder of this paper is structured as fol-
lows. Section 2 describes related work, focusing on
previous attempts to use topic models to recognize
potential translations. Section 3 provides a short
summary of the BiLDA model used in the experi-
ments, presents all main ideas behind our work and
gives an overview and a theoretical background of
the methods. Section 4 evaluates and discusses ini-
tial results. Finally, section 5 proposes several ex-
tensions and gives a summary of the current work.
</bodyText>
<page confidence="0.988613">
479
</page>
<note confidence="0.7977665">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999605" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999390861111111">
The idea to acquire translation candidates based
on comparable and unrelated corpora comes from
(Rapp, 1995). Similar approaches are described in
(Diab and Finch, 2000), (Koehn and Knight, 2002)
and (Gaussier et al., 2004). These methods need
an initial lexicon of translations, cognates or simi-
lar words which are then used to acquire additional
translations of the context words. In contrast, our
method does not bootstrap on language pairs that
share morphology, cognates or similar words.
Some attempts of obtaining translations using
cross-lingual topic models have been made in the
last few years, but they are model-dependent and do
not provide a general environment to adapt and ap-
ply other topic models for the task of finding trans-
lation correspondences. (Ni et al., 2009) have de-
signed a probabilistic topic model that fits Wikipedia
data, but they did not use their models to obtain po-
tential translations. (Mimno et al., 2009) retrieve
a list of potential translations simply by selecting
a small number N of the most probable words in
both languages and then add the Cartesian product
of these sets for every topic to a set of candidate
translations. This approach is straightforward, but it
does not catch the structure of the latent topic space
completely.
Another model proposed in (Boyd-Graber and
Blei, 2009) builds topics as distributions over bilin-
gual matchings where matching priors may come
from different initial evidences such as a machine
readable dictionary, edit distance, or the Point-
wise Mutual Information (PMI) statistic scores from
available parallel corpora. The main shortcoming is
that it introduces external knowledge for matching
priors, suffers from overfitting and uses a restricted
vocabulary.
</bodyText>
<sectionHeader confidence="0.998066" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9999325">
In this section we present the topic model we used
in our experiments and outline the formal framework
within which three different approaches for acquir-
ing potential word translations were built.
</bodyText>
<subsectionHeader confidence="0.998589">
3.1 Bilingual LDA
</subsectionHeader>
<bodyText confidence="0.999992789473684">
The topic model we use is a bilingual extension
of a standard LDA model, called bilingual LDA
(BiLDA), which has been presented in (Ni et al.,
2009; Mimno et al., 2009; De Smet and Moens,
2009). As the name suggests, it is an extension
of the basic LDA model, taking into account bilin-
guality and designed for parallel document pairs.
We test its performance on a collection of compara-
ble texts which are document-aligned and therefore
share their topics. BiLDA takes advantage of the
document alignment by using a single variable that
contains the topic distribution 0, that is language-
independent by assumption and shared by the paired
bilingual comparable documents. Topics for each
document are sampled from 0, from which the words
are sampled in conjugation with the vocabulary dis-
tribution 0 (for language S) and 0 (for language
T). Algorithm 3.1 summarizes the generative story,
while figure 1 shows the plate model.
</bodyText>
<sectionHeader confidence="0.613901" genericHeader="method">
Algorithm
3.1: GENERATIVE STORY FOR BILDA()
</sectionHeader>
<bodyText confidence="0.982969">
for each document pair dj
{ for each word position i E djS
</bodyText>
<figure confidence="0.434182285714286">
(sample zSjiti Mult(B)
do {
sample wSji ti Mult(o,zS ji)
for each word position i E djT
�sample zT ji ti Mult(�)
do
sample wT jiti Mult(0, zT ji)
</figure>
<figureCaption confidence="0.999797">
Figure 1: The standard bilingual LDA model
</figureCaption>
<bodyText confidence="0.99984925">
Having one common 0 for both of the related doc-
uments implies parallelism between the texts. This
observation does not completely hold for compara-
ble corpora with topically aligned texts. To train the
</bodyText>
<figure confidence="0.99849725">
wS ji
zS ji
N M
a
e
wTji
zTji
D
0
a
1P
do
</figure>
<page confidence="0.9927">
480
</page>
<bodyText confidence="0.9999807">
model we use Gibbs sampling, similar to the sam-
pling method for monolingual LDA, with param-
eters α and β set to 50/K and 0.01 respectively,
where K denotes the number of topics. After the
training we end up with a set of φ and ψ word-topic
probability distributions that are used for the calcu-
lations of the word associations.
If we are given a source vocabulary WS, then the
distribution φ of sampling a new token as word wi ∈
WS from a topic zk can be obtained as follows:
</bodyText>
<equation confidence="0.986585666666667">
n(wi) + β
P(wi|zk) = φk,i = |WS |k (wA5 (1)
Ej=1 nk + W Q
</equation>
<bodyText confidence="0.9989824">
where, for a word wi and a topic zk, n(wi)
k denotes
the total number of times that the topic zk is assigned
to the word wi from the vocabulary WS, β is a sym-
metric Dirichlet prior, �|WS|
</bodyText>
<equation confidence="0.760632">
j=1 n(wj)
</equation>
<bodyText confidence="0.999906166666667">
k is the total num-
ber of words assigned to the topic zk, and |WS |is
the total number of distinct words in the vocabulary.
The formula for a set of ψ word-topic probability
distributions for the target side of a corpus is com-
puted in an analogical manner.
</bodyText>
<subsectionHeader confidence="0.998218">
3.2 Main Framework
</subsectionHeader>
<bodyText confidence="0.9999305">
Once we derive a shared set of topics along with
language-specific distributions of words over topics,
it is possible to use them for the computation of the
similarity between words in different languages.
</bodyText>
<subsectionHeader confidence="0.341809">
3.2.1 KL Method
</subsectionHeader>
<bodyText confidence="0.9855736">
The similarity between a source word w1 and a tar-
get word w2 is measured by the extent to which
they share the same topics, i.e., by the extent that
their conditional topic distributions are similar. One
way of expressing similarity is the Kullback-Leibler
(KL) divergence, already used in a monolingual set-
ting in (Steyvers and Griffiths, 2007). The simi-
larity between two words is based on the similar-
ity between χ(1) and χ(2), the similarity of con-
ditional topic distributions for words w1 and w2,
where χ(1) = P(Z|w1)1 and χ(2) = P(Z|w2). We
have to calculate the probabilities P(zj|wi), which
describe a probability that a given word is assigned
to a particular topic. If we apply Bayes’ rule, we
get P(Z|w) = P(w|Z)P (Z)
</bodyText>
<equation confidence="0.993113">
P (w) , where P(Z) and P(w)
1P(Z|w1) refers to a set of all conditional topic distributions
P(zj|w1)
</equation>
<bodyText confidence="0.997432923076923">
are prior distributions for topics and words respec-
tively. P(Z) is a uniform distribution for the BiLDA
model, whereas this assumption clearly does not
hold for topic models with a non-uniform topic prior.
P(w) is given by P(w) = P(w|Z)P(Z). If the
assumption of uniformity for P(Z) holds, we can
write:
for a French word wi, where Normφ denotes the
normalization factor EKj=1 P(wi|zj), i.e., the sum
of all probabilities φ (or probabilities ψ for Normψ)
for the currently observed word wi.
We can then calculate the KL divergence as fol-
lows:
</bodyText>
<equation confidence="0.437320666666667">
φj,1 lo Oj 1/Normφ
Normφ g ψj,2/Normψ
(4)
</equation>
<subsectionHeader confidence="0.421561">
3.2.2 Cue Method
</subsectionHeader>
<bodyText confidence="0.999929214285714">
An alternative, more straightforward approach
(called the Cue method) tries to express similarity
between two words emphasizing the associative re-
lation between two words in a more natural way. It
models the probability P(w2|w1), i.e., the probabil-
ity that a target word w2 will be generated as a re-
sponse to a cue source word w1. For the BiLDA
model we can write:
This conditioning automatically compromises be-
tween word frequency and semantic relatedness
(Griffiths et al., 2007), since higher frequency words
tend to have higher probabilities across all topics,
but the distribution over topics P(zj|w1) ensures
that semantically related topics dominate the sum.
</bodyText>
<equation confidence="0.9868354">
for an English word wi, and:
P(zj  |wi) ∝
P(wi  |zj) j,i
Norm
ψ
(3) ormψ
P(wi|zj) φj,i
P(zj|wi) ∝
Normφ Normφ (2)
K
KL(χ(1), χ(2)) ∝
j=1
K
P(w2|w1) = P(w2|zj)P(zj|w1)
j=1
K
j=1
φj,1
ψj,2
Normφ (5)
</equation>
<page confidence="0.99272">
481
</page>
<subsectionHeader confidence="0.339629">
3.2.3 TI Method
</subsectionHeader>
<bodyText confidence="0.999983181818182">
The last approach borrows an idea from information
retrieval and constructs word vectors over a shared
latent topic space. Values within vectors are the
TF-ITF (term frequency - inverse topic frequency)
scores which are calculated in a completely ana-
logical manner as the TF-IDF scores for the orig-
inal word-document space (Manning and Sch¨utze,
1999). If we are given a source word wi, n(wi) k,Sde-
notes the number of times the word wi is associated
with a source topic zk. Term frequency (TF) of the
source word wi for the source topic zk is given as:
</bodyText>
<equation confidence="0.9669612">
TFi,k = (wi) (6)
nk,S
� n(wj)
k,S
wjEWS
</equation>
<bodyText confidence="0.991405666666667">
Inverse topical frequency (ITF) measures the gen-
eral importance of the source word wi across all
source topics. Rare words are given a higher im-
portance and thus they tend to be more descriptive
for a specific topic. The inverse topical frequency
for the source word wi is calculated as2:
</bodyText>
<equation confidence="0.999547">
K
ITFi = log (7)
1 + |k : n(wi)
k,S &gt; 0|
</equation>
<bodyText confidence="0.999604066666667">
The final TF-ITF score for the source word wi and
the topic zk is given by TF−ITFi,k = TFi,k·ITFi.
We calculate the TF-ITF scores for target words as-
sociated with target topics in an analogical man-
ner. Source and target words share the same K-
dimensional topical space, where K-dimensional
vectors consisting of the TF-ITF scores are built
for all words. The standard cosine similarity met-
ric is then used to find the most similar word vectors
from the target vocabulary for a source word vec-
tor. We name this method the TI method. For in-
stance, given a source word w1 represented by a K-
dimensional vector S1 and a target word w2 repre-
sented by a K-dimensional vector T2, the similarity
between the two words is calculated as follows:
</bodyText>
<footnote confidence="0.72982075">
2Stronger association with a topic is modeled by setting a
higher threshold value in n�w�
k,S &gt; threshold, where we have
chosen 0.
</footnote>
<sectionHeader confidence="0.9986" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999928794871795">
As our training corpus, we use the English-Italian
Wikipedia corpus of 18,898 document pairs, where
each aligned pair discusses the same subject. In or-
der to reduce data sparsity, we keep only lemmatized
noun forms for further analysis. Our Italian vocabu-
lary consists of 7,160 nouns, while our English vo-
cabulary contains 9,166 nouns. The subset of the
650 most frequent terms was used for testing. We
have used the Google Translate tool for evaluations.
As our baseline system, we use the cosine similar-
ity between Italian word vectors and English word
vectors with TF-IDF scores in the original word-
document space (Cos), with aligned documents.
Table 1 shows the Precision@1 scores (the per-
centage of words where the first word from the list
of translations is the correct one) for all three ap-
proaches (KL, Cue and TI), for different number
of topics K. Although KL is designed specifically
to measure the similarity of two distributions, its re-
sults are significantly below those of the Cue and TI,
whose performances are comparable. Whereas the
latter two methods yield the highest results around
the 2, 000 topics mark, the performance of KL in-
creases linearly with the number of topics. This is
an undesirable result as good results are computa-
tionally hard to get.
We have also detected that we are able to boost
overall scores if we combine two methods. We have
opted for the two best methods (TI+Cue), where
overall score is calculated by Score =A·ScoreCue+
ScoreTI.3 We also provide the results obtained by
linearly combining (with equal weights) the cosine
similarity between TF-ITF vectors with that between
TF-IDF vector (TI+Cos).
In a more lenient evaluation setting we employ the
mean reciprocal rank (MRR) (Voorhees, 1999). For
a source word w, rankw denotes the rank of its cor-
rect translation within the retrieved list of potential
translations. MRR is then defined as follows:
</bodyText>
<footnote confidence="0.85089">
3The value of A is empirically set to 10
</footnote>
<equation confidence="0.995182">
(8)
_ EKk =1 Sk1
· Tk2
coS(w1, w2)
��K ��K
k=1 (S1 k)2 · k=1 (Tk 2 )2
</equation>
<page confidence="0.99679">
482
</page>
<table confidence="0.9993235">
K KL Cue TI TI+Cue TI+Cos
200 0.3015 0.1800 0.3169 0.2862 0.5369
500 0.2846 0.3338 0.3754 0.4000 0.5308
800 0.2969 0.4215 0.4523 0.4877 0.5631
1200 0.3246 0.5138 0.4969 0.5708 0.5985
1500 0.3323 0.5123 0.4938 0.5723 0.5908
1800 0.3569 0.5246 0.5154 0.5985 0.6123
2000 0.3954 0.5246 0.5385 0.6077 0.6046
2200 0.4185 0.5323 0.5169 0.5908 0.6015
2600 0.4292 0.4938 0.5185 0.5662 0.5907
3000 0.4354 0.4554 0.4923 0.5631 0.5953
3500 0.4585 0.4492 0.4785 0.5738 0.5785
</table>
<tableCaption confidence="0.8964515">
Table 1: Precision@1 scores for the test subset of the IT-
EN Wikipedia corpus (baseline precision score: 0.5031)
</tableCaption>
<table confidence="0.773972">
1 � 1 (9)
MRR = �V rankw
wEV
</table>
<bodyText confidence="0.99953175">
where V denotes the set of words used for evalu-
ation. We kept only the top 20 candidates from the
ranked list. Table 2 shows the MRR scores for the
same set of experiments.
</bodyText>
<table confidence="0.999323333333333">
K KL Cue TI TI+Cue TI+Cos
200 0.3569 0.2990 0.3868 0.4189 0.5899
500 0.3349 0.4331 0.4431 0.4965 0.5808
800 0.3490 0.5093 0.5215 0.5733 0.6173
1200 0.3773 0.5751 0.5618 0.6372 0.6514
1500 0.3865 0.5756 0.5562 0.6320 0.6435
1800 0.4169 0.5858 0.5802 0.6581 0.6583
2000 0.4561 0.5841 0.5914 0.6616 0.6548
2200 0.4686 0.5898 0.5753 0.6471 0.6523
2600 0.4763 0.5550 0.5710 0.6268 0.6416
3000 0.4848 0.5272 0.5572 0.6257 0.6465
3500 0.5022 0.5199 0.5450 0.6238 0.6310
</table>
<tableCaption confidence="0.9726605">
Table 2: MRR scores for the test subset of the IT-EN
Wikipedia corpus (baseline MRR score: 0.5890)
</tableCaption>
<bodyText confidence="0.99971856">
Topic models have the ability to build clusters of
words which might not always co-occur together in
the same textual units and therefore add extra infor-
mation of potential relatedness. Although we have
presented results for a document-aligned corpus, the
framework is completely generic and applicable to
other topically related corpora.
Again, the KL method has the weakest perfor-
mance among the three methods based on the word-
topic distributions, while the other two methods
seem very useful when combined together or when
combined with the similarity measure used in the
original word-document space. We believe that the
results are in reality even higher than presented in
the paper, due to errors in the evaluation tool (e.g.,
the Italian word raggio is correctly translated as ray,
but Google Translate returns radius as the first trans-
lation candidate).
All proposed methods retrieve lists of semanti-
cally related words, where synonymy is not the only
semantic relation observed. Such lists provide com-
prehensible and useful contextual information in the
target language for the source word, even when the
correct translation candidate is missing, as might be
seen in table 3.
</bodyText>
<table confidence="0.742270583333333">
(1) romanzo (2) paesaggio (3) cavallo
(novel) (landscape) (horse)
writer tourist horse
novella painting stud
novellette landscape horseback
humorist local hoof
novelist visitor breed
essayist hut stamina
penchant draftsman luggage
formative tourism mare
foreword attraction riding
author vegetation pony
</table>
<tableCaption confidence="0.8415415">
Table 3: Lists of the top 10 translation candidates, where
the correct translation is not found (column 1), lies hidden
lower in the list (2), and is retrieved as the first candidate
(3); K=2000; TI+Cue.
</tableCaption>
<sectionHeader confidence="0.998765" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969083333333">
We have presented a generic, language-independent
framework for mining translations of words from
latent topic models. We have proven that topical
knowledge is useful and improves the quality of
word translations. The quality of translations de-
pends only on the quality of a topic model and its
ability to find latent relations between words. Our
next steps involve experiments with other topic mod-
els and other corpora, and combining this unsuper-
vised approach with other tools for lexicon extrac-
tion and synonymy detection from unrelated and
comparable corpora.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9853484">
The research has been carried out in the frame-
work of the TermWise Knowledge Platform (IOF-
KP/09/001) funded by the Industrial Research Fund
K.U. Leuven, Belgium, and the Flemish SBO-IWT
project AMASS++ (SBO-IWT 0060051).
</bodyText>
<page confidence="0.99913">
483
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824886792453">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ’09, pages 75–82.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the web using
interlingual topic modelling. In Proceedings of the
CIKM 2009 Workshop on Social Web Search and Min-
ing, pages 57–64.
Mona T. Diab and Steve Finch. 2000. A statistical trans-
lation model using comparable corpora. In Proceed-
ings of the 2000 Conference on Content-Based Multi-
media Information Access (RIAO), pages 1500–1508.
´Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, pages
526–533.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211–244.
Zellig S. Harris. 1954. Distributional structure. In Word
10 (23), pages 146–162.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition - Volume 9, ULA ’02, pages 9–16.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA, USA.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 880–889.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia. In
Proceedings of the 18th International World Wide Web
Conference, pages 1155–1156.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd Annual
Meeting of the Association for Computational Linguis-
tics, ACL ’95, pages 320–322.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of Latent Semantic Analysis,
427(7):424–440.
Ellen M. Voorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of the Eighth TExt
Retrieval Conference (TREC-8).
</reference>
<page confidence="0.998948">
484
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503147">
<title confidence="0.9905905">Identifying Word Translations from Comparable Corpora Using Latent Topic Models</title>
<author confidence="0.964387">Wim De_Smet Vuli´c</author>
<affiliation confidence="0.949369333333333">Department of Computer K.U. Celestijnenlaan</affiliation>
<address confidence="0.589499">Leuven,</address>
<abstract confidence="0.997682294117647">A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1064" citStr="Blei et al., 2003" startWordPosition="143" endWordPosition="146">irichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported. 1 Introduction Generative models for documents such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) are based upon the idea that latent variables exist which determine how words in documents might be generated. Fitting a generative model means finding the best set of those latent variables in order to explain the observed data. Within that setting, documents are observed as mixtures of latent topics, where topics are probability distributions over words. Our goal is to model and test the capability of probabilistic topic models to identify potential translations from document-aligned text collections. A representative example of such a comparable text collection is Wikipedia, where one may </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David M Blei</author>
</authors>
<title>Multilingual topic models for unaligned text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="4911" citStr="Boyd-Graber and Blei, 2009" startWordPosition="767" endWordPosition="770">opic models for the task of finding translation correspondences. (Ni et al., 2009) have designed a probabilistic topic model that fits Wikipedia data, but they did not use their models to obtain potential translations. (Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. This approach is straightforward, but it does not catch the structure of the latent topic space completely. Another model proposed in (Boyd-Graber and Blei, 2009) builds topics as distributions over bilingual matchings where matching priors may come from different initial evidences such as a machine readable dictionary, edit distance, or the Pointwise Mutual Information (PMI) statistic scores from available parallel corpora. The main shortcoming is that it introduces external knowledge for matching priors, suffers from overfitting and uses a restricted vocabulary. 3 Methodology In this section we present the topic model we used in our experiments and outline the formal framework within which three different approaches for acquiring potential word trans</context>
</contexts>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David M. Blei. 2009. Multilingual topic models for unaligned text. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslanguage linking of news stories on the web using interlingual topic modelling.</title>
<date>2009</date>
<booktitle>In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining,</booktitle>
<pages>57--64</pages>
<marker>De Smet, Moens, 2009</marker>
<rawString>Wim De Smet and Marie-Francine Moens. 2009. Crosslanguage linking of news stories on the web using interlingual topic modelling. In Proceedings of the CIKM 2009 Workshop on Social Web Search and Mining, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Steve Finch</author>
</authors>
<title>A statistical translation model using comparable corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Conference on Content-Based Multimedia Information Access (RIAO),</booktitle>
<pages>1500--1508</pages>
<contexts>
<context position="3752" citStr="Diab and Finch, 2000" startWordPosition="577" endWordPosition="580">main ideas behind our work and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Mona T. Diab and Steve Finch. 2000. A statistical translation model using comparable corpora. In Proceedings of the 2000 Conference on Content-Based Multimedia Information Access (RIAO), pages 1500–1508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="10151" citStr="Griffiths et al., 2007" startWordPosition="1680" endWordPosition="1683">he currently observed word wi. We can then calculate the KL divergence as follows: φj,1 lo Oj 1/Normφ Normφ g ψj,2/Normψ (4) 3.2.2 Cue Method An alternative, more straightforward approach (called the Cue method) tries to express similarity between two words emphasizing the associative relation between two words in a more natural way. It models the probability P(w2|w1), i.e., the probability that a target word w2 will be generated as a response to a cue source word w1. For the BiLDA model we can write: This conditioning automatically compromises between word frequency and semantic relatedness (Griffiths et al., 2007), since higher frequency words tend to have higher probabilities across all topics, but the distribution over topics P(zj|w1) ensures that semantically related topics dominate the sum. for an English word wi, and: P(zj |wi) ∝ P(wi |zj) j,i Norm ψ (3) ormψ P(wi|zj) φj,i P(zj|wi) ∝ Normφ Normφ (2) K KL(χ(1), χ(2)) ∝ j=1 K P(w2|w1) = P(w2|zj)P(zj|w1) j=1 K j=1 φj,1 ψj,2 Normφ (5) 481 3.2.3 TI Method The last approach borrows an idea from information retrieval and constructs word vectors over a shared latent topic space. Values within vectors are the TF-ITF (term frequency - inverse topic frequenc</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>In Word</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="1959" citStr="Harris, 1954" startWordPosition="288" endWordPosition="289">ures of latent topics, where topics are probability distributions over words. Our goal is to model and test the capability of probabilistic topic models to identify potential translations from document-aligned text collections. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the same topic, but strongly varying in style, length and even vocabulary, while still sharing a certain amount of main concepts (or topics). We try to establish a connection between such latent topics and an idea known as the distributional hypothesis (Harris, 1954) - words with a similar meaning are often used in similar contexts. Besides the obvious context of direct cooccurrence, we believe that topic models are an additional source of knowledge which might be used to improve results in the quest for translation candidates extracted without the availability of a translation dictionary and linguistic knowledge. We designed several methods, all derived from the core idea of using word distributions over topics as an extra source of contextual knowledge. Two words are potential translation candidates if they are often present in the same cross-lingual to</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. In Word 10 (23), pages 146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition - Volume 9, ULA ’02,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="3778" citStr="Koehn and Knight, 2002" startWordPosition="581" endWordPosition="584">rk and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have design</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition - Volume 9, ULA ’02, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>880--889</pages>
<contexts>
<context position="4523" citStr="Mimno et al., 2009" startWordPosition="703" endWordPosition="706">used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have designed a probabilistic topic model that fits Wikipedia data, but they did not use their models to obtain potential translations. (Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. This approach is straightforward, but it does not catch the structure of the latent topic space completely. Another model proposed in (Boyd-Graber and Blei, 2009) builds topics as distributions over bilingual matchings where matching priors may come from different initial evidences such as a machine readable dictionary, edit distance, or the Pointwise Mutual Information (</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Jian Hu</author>
<author>Zheng Chen</author>
</authors>
<title>Mining multilingual topics from Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International World Wide Web Conference,</booktitle>
<pages>1155--1156</pages>
<contexts>
<context position="4366" citStr="Ni et al., 2009" startWordPosition="676" endWordPosition="679">0), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have designed a probabilistic topic model that fits Wikipedia data, but they did not use their models to obtain potential translations. (Mimno et al., 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. This approach is straightforward, but it does not catch the structure of the latent topic space completely. Another model proposed in (Boyd-Graber and Blei, 2009) builds topics as distributions over bilingual matching</context>
<context position="5696" citStr="Ni et al., 2009" startWordPosition="888" endWordPosition="891">nce, or the Pointwise Mutual Information (PMI) statistic scores from available parallel corpora. The main shortcoming is that it introduces external knowledge for matching priors, suffers from overfitting and uses a restricted vocabulary. 3 Methodology In this section we present the topic model we used in our experiments and outline the formal framework within which three different approaches for acquiring potential word translations were built. 3.1 Bilingual LDA The topic model we use is a bilingual extension of a standard LDA model, called bilingual LDA (BiLDA), which has been presented in (Ni et al., 2009; Mimno et al., 2009; De Smet and Moens, 2009). As the name suggests, it is an extension of the basic LDA model, taking into account bilinguality and designed for parallel document pairs. We test its performance on a collection of comparable texts which are document-aligned and therefore share their topics. BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution 0, that is languageindependent by assumption and shared by the paired bilingual comparable documents. Topics for each document are sampled from 0, from which the words are sampled</context>
</contexts>
<marker>Ni, Sun, Hu, Chen, 2009</marker>
<rawString>Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen. 2009. Mining multilingual topics from Wikipedia. In Proceedings of the 18th International World Wide Web Conference, pages 1155–1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95,</booktitle>
<pages>320--322</pages>
<contexts>
<context position="3692" citStr="Rapp, 1995" startWordPosition="570" endWordPosition="571">BiLDA model used in the experiments, presents all main ideas behind our work and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic mod</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, ACL ’95, pages 320–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<date>2007</date>
<booktitle>Probabilistic topic models. Handbook of Latent Semantic Analysis,</booktitle>
<pages>427--7</pages>
<contexts>
<context position="8585" citStr="Steyvers and Griffiths, 2007" startWordPosition="1413" endWordPosition="1416">orpus is computed in an analogical manner. 3.2 Main Framework Once we derive a shared set of topics along with language-specific distributions of words over topics, it is possible to use them for the computation of the similarity between words in different languages. 3.2.1 KL Method The similarity between a source word w1 and a target word w2 is measured by the extent to which they share the same topics, i.e., by the extent that their conditional topic distributions are similar. One way of expressing similarity is the Kullback-Leibler (KL) divergence, already used in a monolingual setting in (Steyvers and Griffiths, 2007). The similarity between two words is based on the similarity between χ(1) and χ(2), the similarity of conditional topic distributions for words w1 and w2, where χ(1) = P(Z|w1)1 and χ(2) = P(Z|w2). We have to calculate the probabilities P(zj|wi), which describe a probability that a given word is assigned to a particular topic. If we apply Bayes’ rule, we get P(Z|w) = P(w|Z)P (Z) P (w) , where P(Z) and P(w) 1P(Z|w1) refers to a set of all conditional topic distributions P(zj|w1) are prior distributions for topics and words respectively. P(Z) is a uniform distribution for the BiLDA model, wherea</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilistic topic models. Handbook of Latent Semantic Analysis, 427(7):424–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC-8 question answering track report.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighth TExt Retrieval Conference (TREC-8).</booktitle>
<contexts>
<context position="14104" citStr="Voorhees, 1999" startWordPosition="2365" endWordPosition="2366">, the performance of KL increases linearly with the number of topics. This is an undesirable result as good results are computationally hard to get. We have also detected that we are able to boost overall scores if we combine two methods. We have opted for the two best methods (TI+Cue), where overall score is calculated by Score =A·ScoreCue+ ScoreTI.3 We also provide the results obtained by linearly combining (with equal weights) the cosine similarity between TF-ITF vectors with that between TF-IDF vector (TI+Cos). In a more lenient evaluation setting we employ the mean reciprocal rank (MRR) (Voorhees, 1999). For a source word w, rankw denotes the rank of its correct translation within the retrieved list of potential translations. MRR is then defined as follows: 3The value of A is empirically set to 10 (8) _ EKk =1 Sk1 · Tk2 coS(w1, w2) ��K ��K k=1 (S1 k)2 · k=1 (Tk 2 )2 482 K KL Cue TI TI+Cue TI+Cos 200 0.3015 0.1800 0.3169 0.2862 0.5369 500 0.2846 0.3338 0.3754 0.4000 0.5308 800 0.2969 0.4215 0.4523 0.4877 0.5631 1200 0.3246 0.5138 0.4969 0.5708 0.5985 1500 0.3323 0.5123 0.4938 0.5723 0.5908 1800 0.3569 0.5246 0.5154 0.5985 0.6123 2000 0.3954 0.5246 0.5385 0.6077 0.6046 2200 0.4185 0.5323 0.516</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Ellen M. Voorhees. 1999. The TREC-8 question answering track report. In Proceedings of the Eighth TExt Retrieval Conference (TREC-8).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>