<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9942925">
The Sentimental Factor: Improving Review Classification via
Human-Provided Information
</title>
<author confidence="0.996263">
Philip Beineke*and Trevor Hastie Shivakumar Vaithyanathan
</author>
<affiliation confidence="0.943993">
Dept. of Statistics IBM Almaden Research Center
Stanford University 650 Harry Rd.
</affiliation>
<address confidence="0.628493">
Stanford, CA 94305 San Jose, CA 95120-6099
</address>
<sectionHeader confidence="0.976065" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924210526316">
Sentiment classification is the task of labeling a re-
view document according to the polarity of its pre-
vailing opinion (favorable or unfavorable). In ap-
proaching this problem, a model builder often has
three sources of information available: a small col-
lection of labeled documents, a large collection of
unlabeled documents, and human understanding of
language. Ideally, a learning method will utilize all
three sources. To accomplish this goal, we general-
ize an existing procedure that uses the latter two.
We extend this procedure by re-interpreting it
as a Naive Bayes model for document sentiment.
Viewed as such, it can also be seen to extract a
pair of derived features that are linearly combined
to predict sentiment. This perspective allows us to
improve upon previous methods, primarily through
two strategies: incorporating additional derived fea-
tures into the model and, where possible, using la-
beled data to estimate their relative influence.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956137931035">
Text documents are available in ever-increasing
numbers, making automated techniques for infor-
mation extraction increasingly useful. Traditionally,
most research effort has been directed towards “ob-
jective” information, such as classification accord-
ing to topic; however, interest is growing in produc-
ing information about the opinions that a document
contains; for instance, Morinaga et al. (2002). In
March, 2004, the American Association for Artifi-
cial Intelligence held a symposium in this area, en-
titled “Exploring Affect and Attitude in Text.”
One task in opinion extraction is to label a re-
view document d according to its prevailing senti-
ment s E {−1, 1} (unfavorable or favorable). Sev-
eral previous papers have addressed this problem
by building models that rely exclusively upon la-
beled documents, e.g. Pang et al. (2002), Dave
et al. (2003). By learning models from labeled
data, one can apply familiar, powerful techniques
directly; however, in practice it may be difficult to
obtain enough labeled reviews to learn model pa-
rameters accurately.
A contrasting approach (Turney, 2002) relies only
upon documents whose labels are unknown. This
makes it possible to use a large underlying corpus –
in this case, the entire Internet as seen through the
AltaVista search engine. As a result, estimates for
model parameters are subject to a relatively small
amount of random variation. The corresponding
drawback to such an approach is that its predictions
are not validated on actual documents.
In machine learning, it has often been effec-
tive to use labeled and unlabeled examples in tan-
dem, e.g. Nigam et al. (2000). Turney’s model
introduces the further consideration of incorporat-
ing human-provided knowledge about language. In
this paper we build models that utilize all three
sources: labeled documents, unlabeled documents,
and human-provided information.
The basic concept behind Turney’s model is quite
simple. The “sentiment orientation” (Hatzivas-
siloglou and McKeown, 1997) of a pair of words
is taken to be known. These words serve as “an-
chors” for positive and negative sentiment. Words
that co-occur more frequently with one anchor than
the other are themselves taken to be predictive of
sentiment. As a result, information about a pair of
words is generalized to many words, and then to
documents.
In the following section, we relate this model
with Naive Bayes classification, showing that Tur-
ney’s classifier is a “pseudo-supervised” approach:
it effectively generates a new corpus of labeled doc-
uments, upon which it fits a Naive Bayes classifier.
This insight allows the procedure to be represented
as a probability model that is linear on the logistic
scale, which in turn suggests generalizations that are
developed in subsequent sections.
</bodyText>
<sectionHeader confidence="0.971589" genericHeader="method">
2 A Logistic Model for Sentiment
</sectionHeader>
<subsectionHeader confidence="0.992799">
2.1 Turney’s Sentiment Classifier
</subsectionHeader>
<bodyText confidence="0.9817355">
In Turney’s model, the “sentiment orientation”  of
word w is estimated as follows.
</bodyText>
<equation confidence="0.9900315">
ˆ(w) = log N(w,excellent)/Nexcellent (1)
N(w,poor)/Npoor
</equation>
<bodyText confidence="0.999627857142857">
Here, Na is the total number of sites on the Internet
that contain an occurrence of a – a feature that can
be a word type or a phrase. N(w,a) is the number of
sites in which features w and a appear “near” each
other, i.e. in the same passage of text, within a span
of ten words. Both numbers are obtained from the
hit count that results from a query of the AltaVista
search engine. The rationale for this estimate is that
words that express similar sentiment often co-occur,
while words that express conflicting sentiment co-
occur more rarely. Thus, a word that co-occurs more
frequently with excellent than poor is estimated to
have a positive sentiment orientation.
To extrapolate from words to documents, the esti-
mated sentiment sˆ E {−1, 11 of a review document
d is the sign of the average sentiment orientation of
its constituent features.&apos; To represent this estimate
formally, we introduce the following notation: W
is a “dictionary” of features: (w1, ... , wp). Each
feature’s respective sentiment orientation is repre-
sented as an entry in the vector ˆ of length p:
</bodyText>
<equation confidence="0.977968">
ˆj = ˆ(wj) (2)
</equation>
<bodyText confidence="0.9589996">
Given a collection of n review documents, the i-th
each di is also represented as a vector of length p,
with dij equal to the number of times that feature wj
number of features, |di |= p
occurs in di. The length of a document is its total
</bodyText>
<equation confidence="0.372385">
j=1 dij.
</equation>
<bodyText confidence="0.976543384615385">
Turney’s classifier for the i-th document’s senti-
ment si can now be written:
Using a carefully chosen collection of features,
this classifier produces correct results on 65.8% of
a collection of 120 movie reviews, where 60 are
labeled positive and 60 negative. Although this is
not a particularly encouraging result, movie reviews
tend to be a difficult domain. Accuracy on senti-
ment classification in other domains exceeds 80%
(Turney, 2002).
&apos;Note that not all words or phrases need to be considered as
features. In Turney (2002), features are selected according to
part-of-speech labels.
</bodyText>
<subsectionHeader confidence="0.996629">
2.2 Naive Bayes Classification
</subsectionHeader>
<bodyText confidence="0.999452666666667">
Bayes’ Theorem provides a convenient framework
for predicting a binary response s E {−1, 11 from a
feature vector x:
</bodyText>
<equation confidence="0.9996165">
Pr(s = 1 x _ Pr(x|s = 1)1 ( 4)
( I ) kCI−1,11 Pr(x|s = k)k
</equation>
<bodyText confidence="0.999781888888889">
For a labeled sample of data (xi, si), i = 1, ..., n,
a class’s marginal probability k can be estimated
trivially as the proportion of training samples be-
longing to the class. Thus the critical aspect of clas-
sification by Bayes’ Theorem is to estimate the con-
ditional distribution of x given s. Naive Bayes sim-
plifies this problem by making a “naive” assump-
tion: within a class, the different feature values are
taken to be independent of one another.
</bodyText>
<equation confidence="0.98107">
Pr(x|s) =  Pr(xj|s) (5)
j
</equation>
<bodyText confidence="0.98794">
As a result, the estimation problem is reduced to
univariate distributions.
</bodyText>
<listItem confidence="0.643306">
• Naive Bayes for a Multinomial Distribution
</listItem>
<bodyText confidence="0.994115444444444">
We consider a “bag of words” model for a docu-
ment that belongs to class k, where features are as-
sumed to result from a sequence of |di |independent
multinomial draws with outcome probability vector
ilk = (qk1,...,qkp).
Given a collection of documents with labels,
(di, si), i = 1, ... , n, a natural estimate for qkj is
the fraction of all features in documents of class k
that equal wj:
</bodyText>
<equation confidence="0.9978875">
ˆqkj = i:si=k |di|
i:si=k dij (6)
</equation>
<bodyText confidence="0.981225375">
In the two-class case, the logit transformation
provides a revealing representation of the class pos-
terior probabilities of the Naive Bayes model.
logit(s|d) °A Pr(s = 1|d) (9)
where ˆ0 =
ˆj =
=
=
log
Pr(s = −1|d)
p
ˆ1 ˆq1j
log + dj log
ˆ−1 ˆq−1j
j=1
 p
ˆ0 + dj ˆj
j=1
ˆ1
log
ˆ−1
ˆq1j
log
ˆq−1j
</bodyText>
<equation confidence="0.9956888">
3 ..
j =1 Q j d7
z
ˆsi = sign
|di  | (3)
</equation>
<bodyText confidence="0.9371595">
Observe that the estimate for the logit in Equation
9 has a simple structure: it is a linear function of
d. Models that take this form are commonplace in
classification.
</bodyText>
<subsectionHeader confidence="0.997432">
2.3 Turney’s Classifier as Naive Bayes
</subsectionHeader>
<bodyText confidence="0.9995935">
Although Naive Bayes classification requires a la-
beled corpus of documents, we show in this sec-
tion that Turney’s approach corresponds to a Naive
Bayes model. The necessary documents and their
corresponding labels are built from the spans of text
that surround the anchor words excellent and poor.
More formally, a labeled corpus may be produced
by the following procedure:
</bodyText>
<listItem confidence="0.990490111111111">
1. For a particular anchor ak, locate all of the sites
on the Internet where it occurs.
2. From all of the pages within a site, gather the
features that occur within ten words of an oc-
currence of ak, with any particular feature in-
cluded at most once. This list comprises a new
“document,” representing that site.2
3. Label this document +1 if ak = excellent, -1
if ak = poor.
</listItem>
<bodyText confidence="0.950921947368421">
When a Naive Bayes model is fit to the corpus
described above, it results in a vector a of length
p, consisting of coefficient estimates for all fea-
tures. In Propositions 1 and 2 below, we show that
Turney’s estimates of sentiment orientation Q are
closely related to ˆ�, and that both estimates produce
identical classifiers.
Proposition 1
Proposition 2 Turney’s classifier is identical to a
Naive Bayes classifier fit on this corpus, with 7r1 =
7r−1 = 0.5.
Proof: A Naive Bayes classifier typically assigns an
observation to its most probable class. This is equiv-
alent to classifying according to the sign of the es-
timated logit. So for any document, we must show
that both the logit estimate and the average senti-
ment orientation are identical in sign.
When 7r1 = 0.5, a0 = 0. Thus the estimated logit
is
</bodyText>
<equation confidence="0.996464333333333">
logit(s|d) =  p ˆ�jdj (18)
j=1
ˆ�jdj (19)
</equation>
<bodyText confidence="0.835666">
This is a positive multiple of Turney’s classifier
(Equation 3), so they clearly match in sign. ❑
</bodyText>
<sectionHeader confidence="0.993311" genericHeader="method">
3 A More Versatile Model
</sectionHeader>
<subsectionHeader confidence="0.996863">
3.1 Desired Extensions
</subsectionHeader>
<bodyText confidence="0.999993222222222">
By understanding Turney’s model within a Naive
Bayes framework, we are able to interpret its out-
put as a probability model for document classes. In
the presence of labeled examples, this insight also
makes it possible to estimate the intercept term a0.
Further, we are able to view this model as a mem-
ber of a broad class: linear estimates for the logit.
This understanding facilitates further extensions, in
particular, utilizing the following:
</bodyText>
<equation confidence="0.986969666666667">
 p
j=1
= C1
a = C1ˆ� (12)
Nexc./i:si=1 |di|
Npoor/ i:si=−1 |di |(13)
</equation>
<table confidence="0.64195025">
where C1 =
1. Labeled documents
2. More anchor words
Proof: Because a feature is restricted to at most one
occurrence in a document,
 dij = N(w,ak) (14)
i:si=k
El
</table>
<footnote confidence="0.9545015">
2If both anchors occur on a site, then there will actually be
two documents, one for each sentiment
</footnote>
<bodyText confidence="0.986926333333333">
The reason for using labeled documents is
straightforward; labels offer validation for any cho-
sen model. Using additional anchors is desirable
in part because it is inexpensive to produce lists of
words that are believed to reflect positive sentiment,
perhaps by reference to a thesaurus. In addition, a
single anchor may be at once too general and too
specific.
An anchor may be too general in the sense that
many common words have multiple meanings, and
not all of them reflect a chosen sentiment orien-
tation. For example, poor can refer to an objec-
tive economic state that does not necessarily express
negative sentiment. As a result, a word such as
income appears 4.18 times as frequently with poor
as excellent, even though it does not convey nega-
tive sentiment. Similarly, excellent has a technical
Then from Equations 6 and 11:
</bodyText>
<equation confidence="0.969799166666667">
ˆ�j = log ˆq1j (15)
ˆq−1j
N(w,poor)/  i:si=−1|di|
N(w,exc.)/ i:si=1 |di |(16)
= C1ˆ�j (17)
= log
</equation>
<bodyText confidence="0.999870818181818">
meaning in antiquity trading, which causes it to ap-
pear 3.34 times as frequently with furniture.
An anchor may also be too specific, in the sense
that there are a variety of different ways to express
sentiment, and a single anchor may not capture them
all. So a word like pretentious carries a strong
negative sentiment but co-occurs only slightly more
frequently (1.23 times) with excellent than poor.
Likewise, fascination generally reflects a positive
sentiment, yet it appears slightly more frequently
(1.06 times) with poor than excellent.
</bodyText>
<subsectionHeader confidence="0.996749">
3.2 Other Sources of Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.999631076923077">
The use of additional anchors has a drawback in
terms of being resource-intensive. A feature set may
contain many words and phrases, and each of them
requires a separate AltaVista query for every chosen
anchor word. In the case of 30,000 features and ten
queries per minute, downloads for a single anchor
word require over two days of data collection.
An alternative approach is to access a large
collection of documents directly. Then all co-
occurrences can be counted in a single pass.
Although this approach dramatically reduces the
amount of data available, it does offer several ad-
vantages.
</bodyText>
<listItem confidence="0.982599333333333">
• Increased Query Options Search engine
queries of the form phrase NEAR anchor
may not produce all of the desired co-
occurrence counts. For instance, one may wish
to run queries that use stemmed words, hy-
phenated words, or punctuation marks. One
may also wish to modify the definition of
NEAR, or to count individual co-occurrences,
rather than counting sites that contain at least
one co-occurrence.
• Topic Matching Across the Internet as a
whole, features may not exhibit the same cor-
relation structure as they do within a specific
domain. By restricting attention to documents
within a domain, one may hope to avoid co-
occurrences that are primarily relevant to other
subjects.
• Reproducibility On a fixed corpus, counts of
word occurrences produce consistent results.
Due to the dynamic nature of the Internet,
numbers may fluctuate.
</listItem>
<subsectionHeader confidence="0.981529">
3.3 Co-Occurrences and Derived Features
</subsectionHeader>
<bodyText confidence="0.997199">
The Naive Bayes coefficient estimate ˆ�j may itself
be interpreted as an intercept term plus a linear com-
bination of features of the form log N(wj,ak).
</bodyText>
<figure confidence="0.711624571428571">
Num. of Labeled Occurrences Correlation
1 - 5 0.022
6 - 10 0.082
11 - 25 0.113
26 - 50 0.183
51 - 75 0.283
76 - 100 0.316
</figure>
<figureCaption confidence="0.9924785">
Figure 1: Correlation between Supervised and Un-
supervised Coefficient Estimates
</figureCaption>
<equation confidence="0.99886925">
N(j,exc.)/  i:si=1 |di |(20)
N(j,pr.)/  i:si=−1 |di|
= log C1 + log N(j,exc.) − log N(j,pr.)
(21)
</equation>
<bodyText confidence="0.999125">
We generalize this estimate as follows: for a col-
lection of K different anchor words, we consider a
general linear combination of logged co-occurrence
counts.
</bodyText>
<equation confidence="0.998111">
ˆ�j = K &apos;Yk log N(wj,ak) (22)
k=1
</equation>
<bodyText confidence="0.9997886">
In the special case of a Naive Bayes model, &apos;Yk =
1 when the k-th anchor word ak conveys positive
sentiment, −1 when it conveys negative sentiment.
Replacing the logit estimate in Equation 9 with
an estimate of this form, the model becomes:
</bodyText>
<equation confidence="0.351412">
dj log N(wj,ak)
</equation>
<bodyText confidence="0.965357111111111">
This model has only K + 1 parameters:
&apos;Y0,&apos;Y1, . . . , &apos;YK. These can be learned straightfor-
wardly from labeled documents by a method such
as logistic regression.
anchor word p
Observe that a document receives a score for each
j=1 dj log N(wj,ak). Effectively, the
predictor variables in this model are no longer
counts of the original features dj. Rather, they are
</bodyText>
<equation confidence="0.945933333333333">
ˆ�j = log
logit(s|d) = ˆ�0 +
 p dj ˆ�j (23)
j=1
dj&apos;Yk log N(wj,ak)
(24)
K
k=1
ˆ�0 +
 p
j=1
&apos;Yk
= &apos;Y0 +
K
k=1
</equation>
<figure confidence="0.9559025">
 p
j=1
Positive Negative
best awful
brilliant bad
excellent pathetic
spectacular poor
wonderful worst
</figure>
<figureCaption confidence="0.877114">
Figure 3: Selected Anchor Words
</figureCaption>
<figure confidence="0.9631862">
Unsupervised vs. Supervised Coefficients
−2.0 −1.5−1.0 −0.5 0.0 0.5 1.0 1.5
−3 −2 −1 0 1 2 3 4
Turney Naive Bayes Coefs.
Traditional Naive Bayes Coefs.
</figure>
<figureCaption confidence="0.978133">
Figure 2: Unsupervised versus Supervised Coeffi-
cient Estimates
</figureCaption>
<bodyText confidence="0.99805075">
inner products between the entire feature vector d
and the logged co-occurence vector N(w,ak). In this
respect, the vector of logged co-occurrences is used
to produce derived feature.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="method">
4 Data Analysis
</sectionHeader>
<subsectionHeader confidence="0.999948">
4.1 Accuracy of Unsupervised Coefficients
</subsectionHeader>
<bodyText confidence="0.999965266666667">
By means of a Perl script that uses the Lynx
browser, Version 2.8.3rel.1, we download AltaVista
hit counts for queries of the form “target NEAR
anchor.” The initial list of targets consists of
44,321 word types extracted from the Pang cor-
pus of 1400 labeled movie reviews. After pre-
processing, this number is reduced to 28,629.3
In Figure 1, we compare estimates produced by
two Naive Bayes procedures. For each feature wj,
we estimate aj by using Turney’s procedure, and
by fitting a traditional Naive Bayes model to the
labeled documents. The traditional estimates are
smoothed by assuming a Beta prior distribution that
is equivalent to having four previous observations of
wj in documents of each class.
</bodyText>
<equation confidence="0.978888">
4 + i:sz=1 dij (27)
4 + i:sz=−1dij
4p + �i:sz=1 |di|
where C2 = (28)
4p + �i:sz=−1 |di
</equation>
<bodyText confidence="0.374338">
Here, dij is used to indicate feature presence:
</bodyText>
<footnote confidence="0.991395857142857">
�1 if wj appears in di dij = (29)
0 otherwise
3We eliminate extremely rare words by requiring each target
to co-occur at least once with each anchor. In addition, certain
types, such as words containing hyphens, apostrophes, or other
punctuation marks, do not appear to produce valid counts, so
they are discarded.
</footnote>
<bodyText confidence="0.999828428571429">
We choose this fitting procedure among several can-
didates because it performs well in classifying test
documents.
In Figure 1, each entry in the right-hand col-
umn is the observed correlation between these two
estimates over a subset of features. For features
that occur in five documents or fewer, the corre-
lation is very weak (0.022). This is not surpris-
ing, as it is difficult to estimate a coefficient from
such a small number of labeled examples. Corre-
lations are stronger for more common features, but
never strong. As a baseline for comparison, Naive
Bayes coefficients can be estimated using a subset
of their labeled occurrences. With two independent
sets of 51-75 occurrences, Naive Bayes coefficient
estimates had a correlation of 0.475.
Figure 2 is a scatterplot of the same coefficient
estimates for word types that appear in 51 to 100
documents. The great majority of features do not
have large coefficients, but even for the ones that
do, there is not a tight correlation.
</bodyText>
<subsectionHeader confidence="0.995892">
4.2 Additional Anchors
</subsectionHeader>
<bodyText confidence="0.999860380952381">
We wish to learn how our model performance de-
pends on the choice and number of anchor words.
Selecting from WordNet synonym lists (Fellbaum,
1998), we choose five positive anchor words and
five negative (Figure 3). This produces a total of
25 different possible pairs for use in producing co-
efficient estimates.
Figure 4 shows the classification performance
of unsupervised procedures using the 1400 labeled
Pang documents as test data. Coefficients ˆ�j are es-
timated as described in Equation 22. Several differ-
ent experimental conditions are applied. The meth-
ods labeled ”Count” use the original un-normalized
coefficients, while those labeled “Norm.” have been
normalized so that the number of co-occurrences
with each anchor have identical variance. Results
are shown when rare words (with three or fewer oc-
currences in the labeled corpus) are included and
omitted. The methods “pair” and “10” describe
whether all ten anchor coefficients are used at once,
or just the ones that correspond to a single pair of
</bodyText>
<equation confidence="0.359840666666667">
ˆq1j
ˆq−1j
= C2
</equation>
<table confidence="0.998551333333333">
Method Feat. Misclass. St.Dev
Count Pair &gt;3 39.6% 2.9%
Norm. Pair &gt;3 38.4% 3.0%
Count Pair all 37.4% 3.1%
Norm. Pair all 37.3% 3.0%
Count 10 &gt; 3 36.4% –
Norm. 10 &gt; 3 35.4% –
Count 10 all 34.6% –
Norm. 10 all 34.1% –
</table>
<figureCaption confidence="0.9791505">
Figure 4: Classification Error Rates for Different
Unsupervised Approaches
</figureCaption>
<bodyText confidence="0.982597923076923">
anchor words. For anchor pairs, the mean error
across all 25 pairs is reported, along with its stan-
dard deviation.
Patterns are consistent across the different condi-
tions. A relatively large improvement comes from
using all ten anchor words. Smaller benefits arise
from including rare words and from normalizing
model coefficients.
Models that use the original pair of anchor words,
excellent and poor, perform slightly better than the
average pair. Whereas mean performance ranges
from 37.3% to 39.6%, misclassification rates for
this pair of anchors ranges from 37.4% to 38.1%.
</bodyText>
<subsectionHeader confidence="0.998166">
4.3 A Smaller Unlabeled Corpus
</subsectionHeader>
<bodyText confidence="0.999926956521739">
As described in Section 3.2, there are several rea-
sons to explore the use of a smaller unlabeled cor-
pus, rather than the entire Internet. In our experi-
ments, we use additional movie reviews as our doc-
uments. For this domain, Pang makes available
27,886 reviews.4
Because this corpus offers dramatically fewer in-
stances of anchor words, we modify our estimation
procedure. Rather than discarding words that rarely
co-occur with anchors, we use the same feature set
as before and regularize estimates by the same pro-
cedure used in the Naive Bayes procedure described
earlier.
Using all features, and ten anchor words with nor-
malized scores, test error is 35.0%. This suggests
that comparable results can be attained while re-
ferring to a considerably smaller unlabeled corpus.
Rather than requiring several days of downloads,
the count of nearby co-occurrences was completed
in under ten minutes.
Because this procedure enables fast access to
counts, we explore the possibility of dramatically
enlarging our collection of anchor words. We col-
</bodyText>
<footnote confidence="0.950718">
4This corpus is freely available on the following website:
</footnote>
<subsectionHeader confidence="0.2425075">
Misclassification versus Sample Size
Num. of Labeled Documents
</subsectionHeader>
<bodyText confidence="0.937532578947368">
Figure 5: Misclassification with Labeled Docu-
ments. The solid curve represents a latent fac-
tor model with estimated coefficients. The dashed
curve uses a Naive Bayes classifier. The two hor-
izontal lines represent unsupervised estimates; the
upper one is for the original unsupervised classifier,
and the lower is for the most successful unsuper-
vised method.
lect data for the complete set of WordNet syn-
onyms for the words good, best, bad, boring, and
dreadful. This yields a total of 83 anchor words,
35 positive and 48 negative. When all of these an-
chors are used in conjunction, test error increases to
38.3%. One possible difficulty in using this auto-
mated procedure is that some synonyms for a word
do not carry the same sentiment orientation. For in-
stance, intense is listed as a synonym for bad, even
though its presence in a movie review is a strongly
positive indication.5
</bodyText>
<subsectionHeader confidence="0.999218">
4.4 Methods with Supervision
</subsectionHeader>
<bodyText confidence="0.999395352941177">
As demonstrated in Section 3.3, each anchor word
ak is associated with a coefficient yk. In unsu-
pervised models, these coefficients are assumed to
be known. However, when labeled documents are
available, it may be advantageous to estimate them.
Figure 5 compares the performance of a model
with estimated coefficient vector y, as opposed to
unsupervised models and a traditional supervised
approach. When a moderate number of labeled doc-
uments are available, it offers a noticeable improve-
ment.
The supervised method used for reference in this
case is the Naive Bayes model that is described in
section 4.1. Naive Bayes classification is of partic-
ular interest here because it converges faster to its
asymptotic optimum than do discriminative meth-
ods (Ng, A. Y. and Jordan, M., 2002). Further, with
</bodyText>
<footnote confidence="0.951101">
5In the labeled Pang corpus, intense appears in 38 positive
</footnote>
<figure confidence="0.5239655">
100 200 300 400 500 600
0.30 0.32 0.34 0.36 0.38 0.40
Classif. Error
http://www.cs.cornell.edu/people/pabo/movie-reiew-data/.
reviews and only
6 negative ones.
</figure>
<bodyText confidence="0.996559">
a larger number of labeled documents, its perfor-
mance on this corpus is comparable to that of Sup-
port Vector Machines and Maximum Entropy mod-
els (Pang et al., 2002).
The coefficient vector  is estimated by regular-
ized logistic regression. This method has been used
in other text classification problems, as in Zhang
and Yang (2003). In our case, the regularization6
is introduced in order to enforce the beliefs that:
</bodyText>
<equation confidence="0.9971295">
1 pz� 2, if a1, a2 synonyms (30)
1 pz� −2, if a1, a2 antonyms (31)
</equation>
<bodyText confidence="0.9994175">
For further information on regularized model fitting,
see for instance, Hastie et al. (2001).
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985928571429">
In business settings, there is growing interest in
learning product reputations from the Internet. For
such problems, it is often difficult or expensive to
obtain labeled data. As a result, a change in mod-
eling strategies is needed, towards approaches that
require less supervision. In this paper we pro-
vide a framework for allowing human-provided in-
formation to be combined with unlabeled docu-
ments and labeled documents. We have found that
this framework enables improvements over existing
techniques, both in terms of the speed of model es-
timation and in classification accuracy. As a result,
we believe that this is a promising new approach to
problems of practical importance.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99499152631579">
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews.
C. Fellbaum. 1998. Wordnet an electronic lexical
database.
T. Hastie, R. Tibshirani, and J. Friedman. 2001.
The Elements of Statistical Learning: Data Min-
ing, Inference, and Prediction. Springer-Verlag.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1997. Predicting the semantic orientation
of adjectives. In Philip R. Cohen and Wolfgang
Wahlster, editors, Proceedings of the Thirty-Fifth
Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 174–181, Somerset,
New Jersey. Association for Computational Lin-
guistics.
6By cross-validation, we choose the regularization term A =
1.5/sqrt(n), where n is the number of labeled documents.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web.
Ng, A. Y. and Jordan, M. 2002. On discriminative
vs. generative classifiers: A comparison of logis-
tic regression and naive bayes. Advances in Neu-
ral Information Processing Systems, 14.
Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text clas-
sification from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103–134.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? senti-
ment classification using machine learning
techniques. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised
classification of reviews. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL’02), pages 417–
424, Philadelphia, Pennsylvania. Association for
Computational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proc. 17th National Con-
ference on Artificial Intelligence (AAAI-2000),
Austin, Texas.
Jian Zhang and Yiming Yang. 2003. ”robustness of
regularized linear classification methods in text
categorization”. In Proceedings of the 26th An-
nual International ACM SIGIR Conference (SI-
GIR 2003).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616850">
<title confidence="0.9995585">The Sentimental Factor: Improving Review Classification via Human-Provided Information</title>
<author confidence="0.999698">Trevor Hastie Shivakumar Vaithyanathan</author>
<affiliation confidence="0.999972">Dept. of Statistics IBM Almaden Research Center</affiliation>
<address confidence="0.8172275">Stanford University 650 Harry Rd. Stanford, CA 94305 San Jose, CA 95120-6099</address>
<abstract confidence="0.99856575">Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable). In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language. Ideally, a learning method will utilize all three sources. To accomplish this goal, we generalize an existing procedure that uses the latter two. We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment. Viewed as such, it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment. This perspective allows us to improve upon previous methods, primarily through two strategies: incorporating additional derived features into the model and, where possible, using labeled data to estimate their relative influence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<contexts>
<context position="2101" citStr="Dave et al. (2003)" startWordPosition="315" endWordPosition="318">ification according to topic; however, interest is growing in producing information about the opinions that a document contains; for instance, Morinaga et al. (2002). In March, 2004, the American Association for Artificial Intelligence held a symposium in this area, entitled “Exploring Affect and Attitude in Text.” One task in opinion extraction is to label a review document d according to its prevailing sentiment s E {−1, 1} (unfavorable or favorable). Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al. (2002), Dave et al. (2003). By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately. A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown. This makes it possible to use a large underlying corpus – in this case, the entire Internet as seen through the AltaVista search engine. As a result, estimates for model parameters are subject to a relatively small amount of random variation. The corresponding drawback to such an approach is that its p</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>Wordnet an electronic lexical database.</title>
<date>1998</date>
<contexts>
<context position="17759" citStr="Fellbaum, 1998" startWordPosition="2997" endWordPosition="2998">mparison, Naive Bayes coefficients can be estimated using a subset of their labeled occurrences. With two independent sets of 51-75 occurrences, Naive Bayes coefficient estimates had a correlation of 0.475. Figure 2 is a scatterplot of the same coefficient estimates for word types that appear in 51 to 100 documents. The great majority of features do not have large coefficients, but even for the ones that do, there is not a tight correlation. 4.2 Additional Anchors We wish to learn how our model performance depends on the choice and number of anchor words. Selecting from WordNet synonym lists (Fellbaum, 1998), we choose five positive anchor words and five negative (Figure 3). This produces a total of 25 different possible pairs for use in producing coefficient estimates. Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data. Coefficients ˆ�j are estimated as described in Equation 22. Several different experimental conditions are applied. The methods labeled ”Count” use the original un-normalized coefficients, while those labeled “Norm.” have been normalized so that the number of co-occurrences with each anchor have identical var</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. Wordnet an electronic lexical database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
<author>J Friedman</author>
</authors>
<title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</title>
<date>2001</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="23217" citStr="Hastie et al. (2001)" startWordPosition="3884" endWordPosition="3887">a/. reviews and only 6 negative ones. a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al., 2002). The coefficient vector  is estimated by regularized logistic regression. This method has been used in other text classification problems, as in Zhang and Yang (2003). In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 pz� 2, if a1, a2 synonyms (30) 1 pz� −2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al. (2001). 5 Conclusion In business settings, there is growing interest in learning product reputations from the Internet. For such problems, it is often difficult or expensive to obtain labeled data. As a result, a change in modeling strategies is needed, towards approaches that require less supervision. In this paper we provide a framework for allowing human-provided information to be combined with unlabeled documents and labeled documents. We have found that this framework enables improvements over existing techniques, both in terms of the speed of model estimation and in classification accuracy. As</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2001</marker>
<rawString>T. Hastie, R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer-Verlag.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<editor>In Philip R. Cohen and Wolfgang Wahlster, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="3240" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="487" endWordPosition="491">y small amount of random variation. The corresponding drawback to such an approach is that its predictions are not validated on actual documents. In machine learning, it has often been effective to use labeled and unlabeled examples in tandem, e.g. Nigam et al. (2000). Turney’s model introduces the further consideration of incorporating human-provided knowledge about language. In this paper we build models that utilize all three sources: labeled documents, unlabeled documents, and human-provided information. The basic concept behind Turney’s model is quite simple. The “sentiment orientation” (Hatzivassiloglou and McKeown, 1997) of a pair of words is taken to be known. These words serve as “anchors” for positive and negative sentiment. Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment. As a result, information about a pair of words is generalized to many words, and then to documents. In the following section, we relate this model with Naive Bayes classification, showing that Turney’s classifier is a “pseudo-supervised” approach: it effectively generates a new corpus of labeled documents, upon which it fits a Naive Bayes classifier. This insight allow</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<title>6By cross-validation, we choose the regularization term A = 1.5/sqrt(n), where n is the number of labeled documents.</title>
<marker></marker>
<rawString>6By cross-validation, we choose the regularization term A = 1.5/sqrt(n), where n is the number of labeled documents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Morinaga</author>
</authors>
<title>Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima.</title>
<date>2002</date>
<marker>Morinaga, 2002</marker>
<rawString>Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and Toshikazu Fukushima. 2002. Mining product reputations on the web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Y Ng</author>
<author>M Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>14</pages>
<marker>Ng, Jordan, 2002</marker>
<rawString>Ng, A. Y. and Jordan, M. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in Neural Information Processing Systems, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew K McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="2873" citStr="Nigam et al. (2000)" startWordPosition="440" endWordPosition="443">abeled reviews to learn model parameters accurately. A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown. This makes it possible to use a large underlying corpus – in this case, the entire Internet as seen through the AltaVista search engine. As a result, estimates for model parameters are subject to a relatively small amount of random variation. The corresponding drawback to such an approach is that its predictions are not validated on actual documents. In machine learning, it has often been effective to use labeled and unlabeled examples in tandem, e.g. Nigam et al. (2000). Turney’s model introduces the further consideration of incorporating human-provided knowledge about language. In this paper we build models that utilize all three sources: labeled documents, unlabeled documents, and human-provided information. The basic concept behind Turney’s model is quite simple. The “sentiment orientation” (Hatzivassiloglou and McKeown, 1997) of a pair of words is taken to be known. These words serve as “anchors” for positive and negative sentiment. Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment. As a</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2081" citStr="Pang et al. (2002)" startWordPosition="311" endWordPosition="314">ation, such as classification according to topic; however, interest is growing in producing information about the opinions that a document contains; for instance, Morinaga et al. (2002). In March, 2004, the American Association for Artificial Intelligence held a symposium in this area, entitled “Exploring Affect and Attitude in Text.” One task in opinion extraction is to label a review document d according to its prevailing sentiment s E {−1, 1} (unfavorable or favorable). Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al. (2002), Dave et al. (2003). By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately. A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown. This makes it possible to use a large underlying corpus – in this case, the entire Internet as seen through the AltaVista search engine. As a result, estimates for model parameters are subject to a relatively small amount of random variation. The corresponding drawback to such an ap</context>
<context position="22799" citStr="Pang et al., 2002" startWordPosition="3813" endWordPosition="3816">es model that is described in section 4.1. Naive Bayes classification is of particular interest here because it converges faster to its asymptotic optimum than do discriminative methods (Ng, A. Y. and Jordan, M., 2002). Further, with 5In the labeled Pang corpus, intense appears in 38 positive 100 200 300 400 500 600 0.30 0.32 0.34 0.36 0.38 0.40 Classif. Error http://www.cs.cornell.edu/people/pabo/movie-reiew-data/. reviews and only 6 negative ones. a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al., 2002). The coefficient vector  is estimated by regularized logistic regression. This method has been used in other text classification problems, as in Zhang and Yang (2003). In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 pz� 2, if a1, a2 synonyms (30) 1 pz� −2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al. (2001). 5 Conclusion In business settings, there is growing interest in learning product reputations from the Internet. For such problems, it is often difficult or expensive to obtain labe</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>M L Littman</author>
</authors>
<title>Unsupervised learning of semantic orientation from a hundredbillion-word corpus.</title>
<date>2002</date>
<marker>Turney, Littman, 2002</marker>
<rawString>P.D. Turney and M.L. Littman. 2002. Unsupervised learning of semantic orientation from a hundredbillion-word corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>417--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="2344" citStr="Turney, 2002" startWordPosition="353" endWordPosition="354">sium in this area, entitled “Exploring Affect and Attitude in Text.” One task in opinion extraction is to label a review document d according to its prevailing sentiment s E {−1, 1} (unfavorable or favorable). Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al. (2002), Dave et al. (2003). By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately. A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown. This makes it possible to use a large underlying corpus – in this case, the entire Internet as seen through the AltaVista search engine. As a result, estimates for model parameters are subject to a relatively small amount of random variation. The corresponding drawback to such an approach is that its predictions are not validated on actual documents. In machine learning, it has often been effective to use labeled and unlabeled examples in tandem, e.g. Nigam et al. (2000). Turney’s model introduces the further consideration of incorporating </context>
<context position="6008" citStr="Turney, 2002" startWordPosition="959" endWordPosition="960"> as a vector of length p, with dij equal to the number of times that feature wj number of features, |di |= p occurs in di. The length of a document is its total j=1 dij. Turney’s classifier for the i-th document’s sentiment si can now be written: Using a carefully chosen collection of features, this classifier produces correct results on 65.8% of a collection of 120 movie reviews, where 60 are labeled positive and 60 negative. Although this is not a particularly encouraging result, movie reviews tend to be a difficult domain. Accuracy on sentiment classification in other domains exceeds 80% (Turney, 2002). &apos;Note that not all words or phrases need to be considered as features. In Turney (2002), features are selected according to part-of-speech labels. 2.2 Naive Bayes Classification Bayes’ Theorem provides a convenient framework for predicting a binary response s E {−1, 11 from a feature vector x: Pr(s = 1 x _ Pr(x|s = 1)1 ( 4) ( I ) kCI−1,11 Pr(x|s = k)k For a labeled sample of data (xi, si), i = 1, ..., n, a class’s marginal probability k can be estimated trivially as the proportion of training samples belonging to the class. Thus the critical aspect of classification by Bayes’ Theorem is </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 417– 424, Philadelphia, Pennsylvania. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000),</booktitle>
<location>Austin, Texas.</location>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proc. 17th National Conference on Artificial Intelligence (AAAI-2000), Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Zhang</author>
<author>Yiming Yang</author>
</authors>
<title>robustness of regularized linear classification methods in text categorization”.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference (SIGIR</booktitle>
<contexts>
<context position="22967" citStr="Zhang and Yang (2003)" startWordPosition="3840" endWordPosition="3843">scriminative methods (Ng, A. Y. and Jordan, M., 2002). Further, with 5In the labeled Pang corpus, intense appears in 38 positive 100 200 300 400 500 600 0.30 0.32 0.34 0.36 0.38 0.40 Classif. Error http://www.cs.cornell.edu/people/pabo/movie-reiew-data/. reviews and only 6 negative ones. a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al., 2002). The coefficient vector  is estimated by regularized logistic regression. This method has been used in other text classification problems, as in Zhang and Yang (2003). In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 pz� 2, if a1, a2 synonyms (30) 1 pz� −2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al. (2001). 5 Conclusion In business settings, there is growing interest in learning product reputations from the Internet. For such problems, it is often difficult or expensive to obtain labeled data. As a result, a change in modeling strategies is needed, towards approaches that require less supervision. In this paper we provide a framework for allowing hu</context>
</contexts>
<marker>Zhang, Yang, 2003</marker>
<rawString>Jian Zhang and Yiming Yang. 2003. ”robustness of regularized linear classification methods in text categorization”. In Proceedings of the 26th Annual International ACM SIGIR Conference (SIGIR 2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>