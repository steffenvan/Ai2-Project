<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012037">
<title confidence="0.987623">
Multimodal Grammar Implementation
</title>
<author confidence="0.994433">
Katya Alahverdzhieva
</author>
<affiliation confidence="0.998588">
University of Edinburgh
</affiliation>
<address confidence="0.985506">
10 Crichton Street
Edinburgh EH8 9AB
</address>
<email confidence="0.998631">
K.Alahverdzhieva@sms.ed.ac.uk
</email>
<author confidence="0.9818">
Dan Flickinger
</author>
<affiliation confidence="0.976685">
Stanford University
</affiliation>
<address confidence="0.876743">
Stanford, CA 94305-2150
</address>
<email confidence="0.991508">
danf@stanford.edu
</email>
<author confidence="0.992825">
Alex Lascarides
</author>
<affiliation confidence="0.998433">
University of Edinburgh
</affiliation>
<address confidence="0.985566">
10 Crichton Street
Edinburgh EH8 9AB
</address>
<email confidence="0.998776">
alex@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999246130434783">
This paper reports on an implementation of a
multimodal grammar of speech and co-speech
gesture within the LKB/PET grammar engi-
neering environment. The implementation ex-
tends the English Resource Grammar (ERG,
Flickinger (2000)) with HPSG types and rules
that capture the form of the linguistic signal,
the form of the gestural signal and their rel-
ative timing to constrain the meaning of the
multimodal action. The grammar yields a sin-
gle parse tree that integrates the spoken and
gestural modality thereby drawing on stan-
dard semantic composition techniques to de-
rive the multimodal meaning representation.
Using the current machinery, the main chal-
lenge for the grammar engineer is the non-
linear input: the modalities can overlap tem-
porally. We capture this by identical speech
and gesture token edges. Further, the semantic
contribution of gestures is encoded by lexical
rules transforming a speech phrase into a mul-
timodal entity of conjoined spoken and gestu-
ral semantics.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999677">
Our aim is to regiment the form-meaning mapping
of multimodal actions consisting of speech and co-
speech gestures. The language of study is English,
and the gestures of interest are depicting—the hand
depicts the referent—and deictic—the hand points
at the referent’s spatial coordinates.
Motivation for encoding the form-meaning map-
ping in the grammar stems from the fact that form
effects judgments of multimodal grammaticality:
e.g., in (1)1 the gesture performance along with
</bodyText>
<footnote confidence="0.957543">
1The speech item where the gesture is performed is marked
by underlining, and the accented item is given in uppercase.
</footnote>
<bodyText confidence="0.998430666666667">
the unaccented “called” in a single prosodic phrase
seems ill-formed despite the gesture depicting an as-
pect of the referent—the act of calling.
</bodyText>
<equation confidence="0.523207">
(1) * Your MOTHER called ...
Hand lifts to the ear to imitate holding a receiver.
</equation>
<bodyText confidence="0.999941933333334">
This intuitive judgment is in line with the em-
pirical findings of Giorgolo and Verstraten (2008)
who observed that prosody influences the perception
of temporally misaligned speech-and-gesture sig-
nals as ill-formed. Further, Alahverdzhieva and Las-
carides (2010) established empirically that the ges-
ture performance can be predicted from the prosodic
prominence in speech and that gestures not overlap-
ping subject NPs cannot be semantically related with
that subject NP. The fact that speech-and-gesture in-
tegration is informed by the form of the linguistic
signal suggests formalising the integration within
the grammar. Alternatively, integrating the gestu-
ral contribution by discourse update would involve
pragmatic reasoning accessing information about
linguistic form, disrupting the transition between
syntax/semantics and pragmatics.
The work is set within HPSG — a constraint-based
grammar framework with the different types and
rules organised in a hierarchy. The semantic infor-
mation, derived in parallel with syntax, is expressed
in Minimal Recursion Semantics (MRS) which sup-
ports a high level of underspecifiability (Copestake
et al., 2005). This is useful for computing gesture
meaning since even through discourse processing
not all semantic information resolves to a specific
interpretation.
The rest of the paper is structured as follows: §2
provides theoretical background, §3 details the im-
plementation and §4 discusses the evaluation.
</bodyText>
<page confidence="0.942261666666667">
582
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 582–586,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<sectionHeader confidence="0.993981" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999768">
2.1 Attachment Ambiguity
</subsectionHeader>
<bodyText confidence="0.994866925925926">
We view the integration of gesture and the syn-
chronous, semantically related speech phrase as an
attachment in a single parse tree constrained by the
form of the speech signal—its prosodic prominence.
With standard methods for semantic composition,
we map this multimodal tree to an Underspecified
Logical Form (ULF) which supports the possible in-
terpretations of the speech and gesture in their con-
text. The choices of attachment are not unique. Sim-
ilarly to “John saw the man with the telescope”,
there is ambiguity as to which linguistic phrase a
gesture is semantically related to, and hence likewise
ambiguity as to which linguistic phrase it attaches to
in syntax; e.g., in (2) the open vertical hand shape
can denote a container containing books or a con-
tainee of books. This interpretation is supported by
a gesture attachment to the N “books”. A higher
attachment to the root node of the tree supports an-
other, metaphoric interpretation where the forward
movement is the conduit metaphor of giving.
(2) I can give you other BOOKS ...
Hands are parallel with palms open vertical. They
perform a short forward move to the frontal centre.
We address this ambiguity by grammar rules
that allow for multiple attachments in the syntactic
tree constrained by the prosodic prominence of the
speech signal. The two basic rules are as follows:
</bodyText>
<listItem confidence="0.950839555555556">
1. Prosodic Word Constraint. Gesture can at-
tach to a prosodically prominent spoken word
if there is an overlap between the timing of the
gesture and the timing of the speech word.
2. Head-Argument Constraint. Gesture can at-
tach to a syntactic head partially or fully sat-
urated with its arguments and/or modifiers if
there is a temporal overlap between the syntac-
tic constituent and the gesture.
</listItem>
<bodyText confidence="0.999742875">
Applied to (2), these rules would attach the ges-
ture to “books” (a prosodically prominent item),
also to “other books”, “give you other books”, “can
give you other books” and even to “I can give you
other books” (heads saturated with their arguments).
However, nothing licenses attachments to “I” or
“give”. These distinct attachments would support
the interpretations proposed above.
</bodyText>
<subsectionHeader confidence="0.999774">
2.2 Representing Gesture Form and Meaning
</subsectionHeader>
<bodyText confidence="0.999996333333333">
It is now commonplace to represent gesture form
with Typed Feature Structures (TFS) where each fea-
ture captures an aspect of the gesture’s meaning;
e.g., the gesture in (2) maps to the TFS in (3). Note
that the TFS is typed as depicting so as to differen-
tiate between, say, a hand shape of depicting ges-
ture and a hand shape of deixis. This distinction ef-
fects the gestural interpretation: a depicting gesture
provides non-spatial aspects of the referent’s deno-
tation, and so form bears resemblance to meaning.
Conversely, deixis identifies the spatial coordinates
of the referent in the physical space.
</bodyText>
<equation confidence="0.982395142857143">
�
depicting
HAND-SHAPE open-flat � �
PALM-ORIENT towards-centre � �
FINGER-ORIENT away-body � �
HAND-LOCATION centre-low
HAND-MOVEMENT away-body-straight
</equation>
<bodyText confidence="0.999986789473684">
Each feature introduces an underspecified ele-
mentary predication (EP) into LF; e.g., the hand
shape introduces li : hand shape open flat(i1)
where li is a unique label that underspecifies the
scope of the EP relative to other EPs in the ges-
ture’s LF, ii is a unique metavariable that under-
specifies the main argument’ sort (e.g., in (2) it can
resolve to an individual if the gesture denotes the
books or an event if it denotes the giving act) and
hand shape open flat underspecifies reference to
a property that the entity ii has and that can be de-
picted through the gesture’s open flat hand shape.
In the grammar, we introduce underspecified se-
mantic relations vis rel(s,g) between speech s and
depicting gesture g, and deictic rel(s,d) between
speech s and deixis d. The resolution of these un-
derspecified predicates is a matter of commonsense
reasoning (Lascarides and Stone, 2009) and it there-
fore lies outside the scope of the grammar.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999898857142857">
The grammar was implemented in the LKB grammar
engineering platform (Copestake, 2002) which was
designed for TFS grammars such as HPSG. Since
the LKB parser accepts as input linearly ordered
strings and we represent gesture form with TFSs,
we used the PET engine (Callmeier, 2000) which al-
lows for injecting an arbitrary XML-based FS into
</bodyText>
<equation confidence="0.9863035">
(3) �
� � � � � �
</equation>
<page confidence="0.980726">
583
</page>
<bodyText confidence="0.999955238095238">
the input tokens. The input to our grammar is a lat-
tice of FSs where the spoken tokens are augmented
with prosodic information and the gesture tokens are
feature-value pairs such as (3).
The main challenge for the multimodal grammar
implementation stems from the non-linear multi-
modal input. The HPSG-based parsing platforms—
LKB, PET and TRALE—can parse linearly ordered
strings, and so they do not handle multimodal sig-
nals whose input comes from separate channels con-
nected through temporal relations. Also, these pars-
ing platforms do not support quantitative compari-
son operations over the time stamps of the input to-
kens. This is essential for our grammar since the
multimodal integration is constrained by temporal
overlap between speech and gesture (recall §2.1).
To solve this, we pre-processed the XML-based
FS input so that overlapping TIME START and
TIME END values were “translated” into identical
start and end edges of the speech token and the ges-
ture token as follows:
</bodyText>
<equation confidence="0.55705125">
&lt;edge source=&amp;quot;v0&amp;quot; target=&amp;quot;v1&amp;quot;&gt;
&lt;fs type=&amp;quot;speech_token&amp;quot;&gt;
&lt;edge source=&amp;quot;v0&amp;quot; target=&amp;quot;v1&amp;quot;&gt;
&lt;fs type=&amp;quot;gesture_token&amp;quot;&gt;
</equation>
<bodyText confidence="0.999963266666667">
This robust pre-processing step is sufficient since
the only temporal relation required by the grammar
is overlap, an abstraction over more fined-grained
relations between speech (S) and gesture (G) such
as (precedence(start(S), start(G)) n identity (end(S),
end(G))).
The linking of gesture to its temporally over-
lapping speech segment happens prior to parsing
via chart-mapping rules (Adolphs et al., 2008)
which involve re-writing chart items into FSs. The
gesture-unary-rule (see Fig.1) rewrites an in-
put (I) speech token in the context (C) of a gesture
token into a combined speech+gesture token where
the +GEST and +PROS values of the speech and ges-
ture tokens are copied onto the output (O).
</bodyText>
<construct confidence="0.836504">
gesture-unary-rule := cm_rule &amp;
[+CONTEXT &lt;gesture_token &amp; [+GEST #gest]&gt;,
+INPUT &lt;speech_token &amp; [+PROS #pros]&gt;,
+OUTPUT &lt;speech+gesture_token &amp;
[+GEST #gest, +PROS #pros]&gt;,
+POSITION &amp;quot;O1@I1, I1@C1&amp;quot; ].
</construct>
<figureCaption confidence="0.999936">
Figure 1: Definition of gesture-unary-rule
</figureCaption>
<bodyText confidence="0.9994916">
The +PROS attribute contains prosodic informa-
tion and the +GEST attribute is a feature-structure
representation as shown in (3). The +POSITION con-
straint restricts the position of the I, O and C items to
an overlap (@), i.e., the edge markers of the gesture
token should be identical to those of the speech to-
ken, and also identical to the speech+gesture token.
This chart-mapping rule recognises the gesture to-
ken overlapping the speech token and it records this
by “augmenting” the speech token with the gesture
feature-values.
In the grammar, we extended the ERG word and
phrase rules with prosodic and gestural information
where the +PROS and +GEST features of the input
token are identified with the PROS and GEST of the
word and/or lexical phrase in the grammar. We then
added a lexical rule (see Fig. 2) which projects a ges-
ture daughter to a complex gesture-marked entity of
a single argument for which both the PROS and GEST
features are appropriate.
</bodyText>
<figure confidence="0.60171075">
gesture_lexrule := phrase_or_lexrule &amp;
[ ORTH [ PROS #pros ],
ARGS &lt;[ ORTH [ GEST gesture-form,
PROS p-word &amp; #pros ]]&gt;].
</figure>
<figureCaption confidence="0.999734">
Figure 2: Definition of gesture lexrule
</figureCaption>
<bodyText confidence="0.9984242">
This rule constrains PROS to a prosodically promi-
nent word of type p-word thereby preventing a ges-
ture from plugging into a prosodically unmarked
word. The gesture-form value is a supertype over the
distinct gesture types—depicting and deictic. The
gesture lexrule is inherited by a lexical rule
specific to depicting gestures, and by a lexical rule
specific to deictic gestures. In this way, we can en-
code the semantic contribution of depicting gestures
which is different from the semantic contribution of
deixis. For the sake of space, Fig. 3 presents only the
depicting lexrule. The semantic information
contributed by the rule is encoded within C-CONT.
Following §2.2, the rule introduces an underspec-
ified vis rel between the main label #dltop of the
spoken sign (via the HCONS constraints) and the
main label #glbl of the gesture semantics (via the
HCONS constraints). Note that these two arguments
are in a geq (greater or equal) constraint. This means
that vis rel can operate over any projection of the
speech word; e.g., attaching the gesture to “book” in
(2) means that the relation is not restricted to the EPs
contributed by “books” but it can be also over the
EPs of a higher projection. The gesture’s semantics
is a bag of EPs (see §2.2), all of which are outscoped
</bodyText>
<page confidence="0.99547">
584
</page>
<table confidence="0.998744222222222">
‘gesture/12-04-02/pet’ Coverage Profile
Aggregate total positive word lexical distinct total overall
items items string items analyses results coverage
d d 0 0 0 d %
90 &lt; i-length &lt; 95 126 92 93.00 26.46 1.67 92 100.0
70 &lt; i-length &lt; 75 78 54 71.00 12.00 1.00 54 100.0
60 &lt; i-length &lt; 65 249 179 60.00 9.42 1.00 179 100.0
45 &lt; i-length &lt; 50 18 14 49.00 7.00 1.00 14 100.0
Total 471 339 70.25 14.35 1.18 339 100.0
</table>
<tableCaption confidence="0.996305">
Table 1: Coverage Profile of Test Items generated by [incr tsdb()]
</tableCaption>
<table confidence="0.939163277777778">
depicting_lexrule := gesture_lexrule &amp;
[ARGS &lt;[ SYNSEM.LOCAL.CONT.HOOK.LTOP
#dltop,
ORTH [ GEST depicting] &gt;,
C-CONT [ RELS &lt;![ PRED vis_rel,
S-ARG #arg1,
G-ARG #arg2 ],
[ PRED G_mod,
LBL #glbl,
ARG1 #harg ],
[ LBL #larg1 ],...!&gt;,
HCONS &lt;!geq&amp;[ HARG #arg1,
LARG #dltop ],
qeq&amp;[ HARG #arg2,
LARG #glbl ],
qeq&amp;[ HARG #harg,
LARG #larg1 ],
...!&gt;]].
</table>
<figureCaption confidence="0.999241">
Figure 3: Definition of depicting lexrule
</figureCaption>
<bodyText confidence="0.99983275">
by the gestural modality [!9]. The rule therefore in-
troduces in RELS a label (here #larg1) for an EP
which is in qeq constraints with [!9]. The instanti-
ation of the particular EPs comes from the gestural
lexical entry. In the real implementation, the num-
ber of these labels corresponds to the number of fea-
tures. They are designed in the same way and we
thus forego any details about the rest.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9990034375">
The evaluation was performed against a test suite
designed in analogy to the traditional phenomenon-
based test-suites (Lehmann et al., 1996): manually-
crafted to ensure coverage of well-formed and ill-
formed data, but inspired by an examination of natu-
ral data. We systematically tested syntactic phenom-
ena (intransitivity, transitivity, complex NPs, coordi-
nation, negation and modification) over well-formed
and ill-formed examples where the ill-formed items
were derived by means of the following operations:
prosodic permutation (varying the prosodic marked-
ness, e.g., from (4a) we derive (4b) to reflect in-
tuitions of native speakers); gesture variation (test-
ing distinct gesture types) and temporal permutation
(moving the gestural performance over the distinct
speech items).
</bodyText>
<figure confidence="0.83786625">
(4) a. ANNA ate ...
Depicting gesture along with “Anna”.
b. *anna ATE ...
Depicting gesture along with “Anna”.
</figure>
<bodyText confidence="0.999953888888889">
The test set contained 471 multimodal items (72%
well-formed) covering the full range of prosodic
(prosodic markedness and unmarkedness) and ges-
ture (the span of depicting/deictic gesture and its
temporal relation to the prosodically marked ele-
ments) permutations. The gestural vocabulary was
limited since a larger gesture lexicon has no effects
on the performance. To test the grammar, we used
the [incr tsdb()]2 competence and performance tool
which enables batch processing of test items and
which creates a coverage profile of the test set (see
Table 1). The values are as follows: the left col-
umn separates the items per aggregation criterion
(the length of test items); the next column shows the
number of test items per aggregate; then we have
the number of grammatical items; average length of
test item; average number of lexical items; average
number of distinct analyses and total coverage.
</bodyText>
<sectionHeader confidence="0.998268" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999888363636364">
This paper reported on an implementation of a mul-
timodal grammar combining spoken and gestural in-
put. The main challenge for the current parsing
platforms was the non-linear input which we solved
by extending the spoken sign with the synchronous
gestural sign semantics where synchrony was estab-
lished by means of identical token edges. In the fu-
ture, we shall extend the lexical coverage so that the
grammar can handle various gestures and we also
intend to evaluate the grammar with naturally occur-
ring examples in XML format.
</bodyText>
<footnote confidence="0.976222">
2http://www.delph-in.net/itsdb/
</footnote>
<page confidence="0.998234">
585
</page>
<sectionHeader confidence="0.995835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999781078947368">
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Daniel Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural lan-
guage parsing. In Proceedings of the Sixth Interna-
tional Language Resources and Evaluation. ELRA.
Katya Alahverdzhieva and Alex Lascarides. 2010.
Analysing speech and co-speech gesture in constraint-
based grammars. In Stefan M¨uller, editor, The Pro-
ceedings of the 17th International Conference on
Head-Driven Phrase Structure Grammar, pages 6–26,
Stanford. CSLI Publications.
Ulrich Callmeier. 2000. PET — A platform for experi-
mentation with efficient HPSG processing techniques.
Natural Language Engineering, 6 (1) (Special Issue on
Efficient Processing with HPSG):99 –108.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 2005. Minimal recursion semantics: An intro-
duction. Journal of Research on Language and Com-
putation, 3(2–3):281–332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering.
Gianluca Giorgolo and Frans Verstraten. 2008. Per-
ception of speech-and-gesture integration. In Pro-
ceedings of the International Conference on Auditory-
Visual Speech Processing 2008, pages 31–36.
Alex Lascarides and Matthew Stone. 2009. A formal
semantic analysis of gesture. Journal of Semantics.
Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost,
Klaus Netter, Veronika Lux, Judith Klein, Kirsten
Falkedal, Frederik Fouvry, Dominique Estival, Eva
Dauphin, Herve Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. Tsnlp - test suites
for natural language processing. In COLING, pages
711–716.
</reference>
<page confidence="0.998692">
586
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327301">
<title confidence="0.999253">Multimodal Grammar Implementation</title>
<author confidence="0.991251">Katya</author>
<affiliation confidence="0.996063">University of</affiliation>
<address confidence="0.825401">10 Crichton Edinburgh EH8 9AB</address>
<email confidence="0.993403">K.Alahverdzhieva@sms.ed.ac.uk</email>
<author confidence="0.800755">Dan</author>
<affiliation confidence="0.925597">Stanford</affiliation>
<address confidence="0.998925">Stanford, CA 94305-2150</address>
<email confidence="0.999431">danf@stanford.edu</email>
<author confidence="0.987401">Alex</author>
<affiliation confidence="0.995534">University of</affiliation>
<address confidence="0.828522">10 Crichton Edinburgh EH8 9AB</address>
<email confidence="0.992714">alex@inf.ed.ac.uk</email>
<abstract confidence="0.995913125">This paper reports on an implementation of a multimodal grammar of speech and co-speech within the engineering environment. The implementation exthe English Resource Grammar (2000)) with and rules that capture the form of the linguistic signal, the form of the gestural signal and their relative timing to constrain the meaning of the multimodal action. The grammar yields a single parse tree that integrates the spoken and gestural modality thereby drawing on standard semantic composition techniques to derive the multimodal meaning representation. Using the current machinery, the main challenge for the grammar engineer is the nonlinear input: the modalities can overlap temporally. We capture this by identical speech and gesture token edges. Further, the semantic contribution of gestures is encoded by lexical rules transforming a speech phrase into a multimodal entity of conjoined spoken and gestural semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Adolphs</author>
<author>Stephan Oepen</author>
<author>Ulrich Callmeier</author>
<author>Berthold Crysmann</author>
<author>Daniel Flickinger</author>
<author>Bernd Kiefer</author>
</authors>
<title>Some fine points of hybrid natural language parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation. ELRA.</booktitle>
<contexts>
<context position="9614" citStr="Adolphs et al., 2008" startWordPosition="1495" endWordPosition="1498">ranslated” into identical start and end edges of the speech token and the gesture token as follows: &lt;edge source=&amp;quot;v0&amp;quot; target=&amp;quot;v1&amp;quot;&gt; &lt;fs type=&amp;quot;speech_token&amp;quot;&gt; &lt;edge source=&amp;quot;v0&amp;quot; target=&amp;quot;v1&amp;quot;&gt; &lt;fs type=&amp;quot;gesture_token&amp;quot;&gt; This robust pre-processing step is sufficient since the only temporal relation required by the grammar is overlap, an abstraction over more fined-grained relations between speech (S) and gesture (G) such as (precedence(start(S), start(G)) n identity (end(S), end(G))). The linking of gesture to its temporally overlapping speech segment happens prior to parsing via chart-mapping rules (Adolphs et al., 2008) which involve re-writing chart items into FSs. The gesture-unary-rule (see Fig.1) rewrites an input (I) speech token in the context (C) of a gesture token into a combined speech+gesture token where the +GEST and +PROS values of the speech and gesture tokens are copied onto the output (O). gesture-unary-rule := cm_rule &amp; [+CONTEXT &lt;gesture_token &amp; [+GEST #gest]&gt;, +INPUT &lt;speech_token &amp; [+PROS #pros]&gt;, +OUTPUT &lt;speech+gesture_token &amp; [+GEST #gest, +PROS #pros]&gt;, +POSITION &amp;quot;O1@I1, I1@C1&amp;quot; ]. Figure 1: Definition of gesture-unary-rule The +PROS attribute contains prosodic information and the +GEST</context>
</contexts>
<marker>Adolphs, Oepen, Callmeier, Crysmann, Flickinger, Kiefer, 2008</marker>
<rawString>Peter Adolphs, Stephan Oepen, Ulrich Callmeier, Berthold Crysmann, Daniel Flickinger, and Bernd Kiefer. 2008. Some fine points of hybrid natural language parsing. In Proceedings of the Sixth International Language Resources and Evaluation. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katya Alahverdzhieva</author>
<author>Alex Lascarides</author>
</authors>
<title>Analysing speech and co-speech gesture in constraintbased grammars.</title>
<date>2010</date>
<booktitle>The Proceedings of the 17th International Conference on Head-Driven Phrase Structure Grammar,</booktitle>
<pages>6--26</pages>
<editor>In Stefan M¨uller, editor,</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford.</location>
<contexts>
<context position="2407" citStr="Alahverdzhieva and Lascarides (2010)" startWordPosition="356" endWordPosition="360">sture performance along with 1The speech item where the gesture is performed is marked by underlining, and the accented item is given in uppercase. the unaccented “called” in a single prosodic phrase seems ill-formed despite the gesture depicting an aspect of the referent—the act of calling. (1) * Your MOTHER called ... Hand lifts to the ear to imitate holding a receiver. This intuitive judgment is in line with the empirical findings of Giorgolo and Verstraten (2008) who observed that prosody influences the perception of temporally misaligned speech-and-gesture signals as ill-formed. Further, Alahverdzhieva and Lascarides (2010) established empirically that the gesture performance can be predicted from the prosodic prominence in speech and that gestures not overlapping subject NPs cannot be semantically related with that subject NP. The fact that speech-and-gesture integration is informed by the form of the linguistic signal suggests formalising the integration within the grammar. Alternatively, integrating the gestural contribution by discourse update would involve pragmatic reasoning accessing information about linguistic form, disrupting the transition between syntax/semantics and pragmatics. The work is set withi</context>
</contexts>
<marker>Alahverdzhieva, Lascarides, 2010</marker>
<rawString>Katya Alahverdzhieva and Alex Lascarides. 2010. Analysing speech and co-speech gesture in constraintbased grammars. In Stefan M¨uller, editor, The Proceedings of the 17th International Conference on Head-Driven Phrase Structure Grammar, pages 6–26, Stanford. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Callmeier</author>
</authors>
<title>PET — A platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG):99</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>108</pages>
<contexts>
<context position="8028" citStr="Callmeier, 2000" startWordPosition="1250" endWordPosition="1251">oduce underspecified semantic relations vis rel(s,g) between speech s and depicting gesture g, and deictic rel(s,d) between speech s and deixis d. The resolution of these underspecified predicates is a matter of commonsense reasoning (Lascarides and Stone, 2009) and it therefore lies outside the scope of the grammar. 3 Implementation The grammar was implemented in the LKB grammar engineering platform (Copestake, 2002) which was designed for TFS grammars such as HPSG. Since the LKB parser accepts as input linearly ordered strings and we represent gesture form with TFSs, we used the PET engine (Callmeier, 2000) which allows for injecting an arbitrary XML-based FS into (3) � � � � � � � 583 the input tokens. The input to our grammar is a lattice of FSs where the spoken tokens are augmented with prosodic information and the gesture tokens are feature-value pairs such as (3). The main challenge for the multimodal grammar implementation stems from the non-linear multimodal input. The HPSG-based parsing platforms— LKB, PET and TRALE—can parse linearly ordered strings, and so they do not handle multimodal signals whose input comes from separate channels connected through temporal relations. Also, these pa</context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>Ulrich Callmeier. 2000. PET — A platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG):99 –108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Journal of Research on Language and Computation,</journal>
<pages>3--2</pages>
<contexts>
<context position="3298" citStr="Copestake et al., 2005" startWordPosition="485" endWordPosition="488"> the form of the linguistic signal suggests formalising the integration within the grammar. Alternatively, integrating the gestural contribution by discourse update would involve pragmatic reasoning accessing information about linguistic form, disrupting the transition between syntax/semantics and pragmatics. The work is set within HPSG — a constraint-based grammar framework with the different types and rules organised in a hierarchy. The semantic information, derived in parallel with syntax, is expressed in Minimal Recursion Semantics (MRS) which supports a high level of underspecifiability (Copestake et al., 2005). This is useful for computing gesture meaning since even through discourse processing not all semantic information resolves to a specific interpretation. The rest of the paper is structured as follows: §2 provides theoretical background, §3 details the implementation and §4 discusses the evaluation. 582 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 582–586, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics 2 Background 2.1 Attachment Ambiguity We view the integration of gestu</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard. 2005. Minimal recursion semantics: An introduction. Journal of Research on Language and Computation, 3(2–3):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="7833" citStr="Copestake, 2002" startWordPosition="1217" endWordPosition="1218">the giving act) and hand shape open flat underspecifies reference to a property that the entity ii has and that can be depicted through the gesture’s open flat hand shape. In the grammar, we introduce underspecified semantic relations vis rel(s,g) between speech s and depicting gesture g, and deictic rel(s,d) between speech s and deixis d. The resolution of these underspecified predicates is a matter of commonsense reasoning (Lascarides and Stone, 2009) and it therefore lies outside the scope of the grammar. 3 Implementation The grammar was implemented in the LKB grammar engineering platform (Copestake, 2002) which was designed for TFS grammars such as HPSG. Since the LKB parser accepts as input linearly ordered strings and we represent gesture form with TFSs, we used the PET engine (Callmeier, 2000) which allows for injecting an arbitrary XML-based FS into (3) � � � � � � � 583 the input tokens. The input to our grammar is a lattice of FSs where the spoken tokens are augmented with prosodic information and the gesture tokens are feature-value pairs such as (3). The main challenge for the multimodal grammar implementation stems from the non-linear multimodal input. The HPSG-based parsing platforms</context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing Typed Feature Structure Grammars. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering.</journal>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianluca Giorgolo</author>
<author>Frans Verstraten</author>
</authors>
<title>Perception of speech-and-gesture integration.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on AuditoryVisual Speech Processing</booktitle>
<pages>31--36</pages>
<contexts>
<context position="2242" citStr="Giorgolo and Verstraten (2008)" startWordPosition="336" endWordPosition="339">ivation for encoding the form-meaning mapping in the grammar stems from the fact that form effects judgments of multimodal grammaticality: e.g., in (1)1 the gesture performance along with 1The speech item where the gesture is performed is marked by underlining, and the accented item is given in uppercase. the unaccented “called” in a single prosodic phrase seems ill-formed despite the gesture depicting an aspect of the referent—the act of calling. (1) * Your MOTHER called ... Hand lifts to the ear to imitate holding a receiver. This intuitive judgment is in line with the empirical findings of Giorgolo and Verstraten (2008) who observed that prosody influences the perception of temporally misaligned speech-and-gesture signals as ill-formed. Further, Alahverdzhieva and Lascarides (2010) established empirically that the gesture performance can be predicted from the prosodic prominence in speech and that gestures not overlapping subject NPs cannot be semantically related with that subject NP. The fact that speech-and-gesture integration is informed by the form of the linguistic signal suggests formalising the integration within the grammar. Alternatively, integrating the gestural contribution by discourse update wo</context>
</contexts>
<marker>Giorgolo, Verstraten, 2008</marker>
<rawString>Gianluca Giorgolo and Frans Verstraten. 2008. Perception of speech-and-gesture integration. In Proceedings of the International Conference on AuditoryVisual Speech Processing 2008, pages 31–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Matthew Stone</author>
</authors>
<title>A formal semantic analysis of gesture.</title>
<date>2009</date>
<journal>Journal of Semantics.</journal>
<contexts>
<context position="7674" citStr="Lascarides and Stone, 2009" startWordPosition="1190" endWordPosition="1193"> unique metavariable that underspecifies the main argument’ sort (e.g., in (2) it can resolve to an individual if the gesture denotes the books or an event if it denotes the giving act) and hand shape open flat underspecifies reference to a property that the entity ii has and that can be depicted through the gesture’s open flat hand shape. In the grammar, we introduce underspecified semantic relations vis rel(s,g) between speech s and depicting gesture g, and deictic rel(s,d) between speech s and deixis d. The resolution of these underspecified predicates is a matter of commonsense reasoning (Lascarides and Stone, 2009) and it therefore lies outside the scope of the grammar. 3 Implementation The grammar was implemented in the LKB grammar engineering platform (Copestake, 2002) which was designed for TFS grammars such as HPSG. Since the LKB parser accepts as input linearly ordered strings and we represent gesture form with TFSs, we used the PET engine (Callmeier, 2000) which allows for injecting an arbitrary XML-based FS into (3) � � � � � � � 583 the input tokens. The input to our grammar is a lattice of FSs where the spoken tokens are augmented with prosodic information and the gesture tokens are feature-val</context>
</contexts>
<marker>Lascarides, Stone, 2009</marker>
<rawString>Alex Lascarides and Matthew Stone. 2009. A formal semantic analysis of gesture. Journal of Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Lehmann</author>
<author>Stephan Oepen</author>
<author>Sylvie Regnier-Prost</author>
<author>Klaus Netter</author>
<author>Veronika Lux</author>
<author>Judith Klein</author>
<author>Kirsten Falkedal</author>
</authors>
<title>Tsnlp - test suites for natural language processing.</title>
<date>1996</date>
<booktitle>In COLING,</booktitle>
<pages>711--716</pages>
<institution>Frederik Fouvry, Dominique Estival, Eva Dauphin, Herve Compagnion, Judith Baur, Lorna Balkan, and</institution>
<contexts>
<context position="13973" citStr="Lehmann et al., 1996" startWordPosition="2231" endWordPosition="2234">G #harg, LARG #larg1 ], ...!&gt;]]. Figure 3: Definition of depicting lexrule by the gestural modality [!9]. The rule therefore introduces in RELS a label (here #larg1) for an EP which is in qeq constraints with [!9]. The instantiation of the particular EPs comes from the gestural lexical entry. In the real implementation, the number of these labels corresponds to the number of features. They are designed in the same way and we thus forego any details about the rest. 4 Evaluation The evaluation was performed against a test suite designed in analogy to the traditional phenomenonbased test-suites (Lehmann et al., 1996): manuallycrafted to ensure coverage of well-formed and illformed data, but inspired by an examination of natural data. We systematically tested syntactic phenomena (intransitivity, transitivity, complex NPs, coordination, negation and modification) over well-formed and ill-formed examples where the ill-formed items were derived by means of the following operations: prosodic permutation (varying the prosodic markedness, e.g., from (4a) we derive (4b) to reflect intuitions of native speakers); gesture variation (testing distinct gesture types) and temporal permutation (moving the gestural perfo</context>
</contexts>
<marker>Lehmann, Oepen, Regnier-Prost, Netter, Lux, Klein, Falkedal, 1996</marker>
<rawString>Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost, Klaus Netter, Veronika Lux, Judith Klein, Kirsten Falkedal, Frederik Fouvry, Dominique Estival, Eva Dauphin, Herve Compagnion, Judith Baur, Lorna Balkan, and Doug Arnold. 1996. Tsnlp - test suites for natural language processing. In COLING, pages 711–716.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>