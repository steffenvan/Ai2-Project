<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.997924">
Semi-supervised Training for the Averaged Perceptron POS Tagger
</title>
<author confidence="0.971871">
Drahomira “johanka” Spoustov´a Jan Hajiˇc Jan Raab Miroslav Spousta
</author>
<affiliation confidence="0.989734333333333">
Institute of Formal and Applied Linguistics
Faculty of Mathematics and Physics,
Charles University Prague, Czech Republic
</affiliation>
<email confidence="0.602433">
{johanka,hajic,raab,spousta}@
ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.983391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907823529412">
This paper describes POS tagging exper-
iments with semi-supervised training as
an extension to the (supervised) averaged
perceptron algorithm, first introduced for
this task by (Collins, 2002). Experiments
with an iterative training on standard-sized
supervised (manually annotated) dataset
(106 tokens) combined with a relatively
modest (in the order of 108 tokens) un-
supervised (plain) data in a bagging-like
fashion showed significant improvement
of the POS classification task on typo-
logically different languages, yielding bet-
ter than state-of-the-art results for English
and Czech (4.12 % and 4.86 % relative er-
ror reduction, respectively; absolute accu-
racies being 97.44 % and 95.89 %).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994754673076923">
Since 2002, we have seen a renewed interest in
improving POS tagging results for English, and
an inflow of results (initial or improved) for many
other languages. For English, after a relatively big
jump achieved by (Collins, 2002), we have seen
two significant improvements: (Toutanova et al.,
2003) and (Shen et al., 2007) pushed the results
by a significant amount each time.1
1In our final comparison, we have also included the re-
sults of (Gim´enez and M`arquez, 2004), because it has sur-
passed (Collins, 2002) as well and we have used this tag-
ger in the data preparation phase. See more details below.
Most recently, (Suzuki and Isozaki, 2008) published their
Semi-supervised sequential labelling method, whose results
on POS tagging seem to be optically better than (Shen et al.,
2007), but no significance tests were given and the tool is not
available for download, i.e. for repeating the results and sig-
nificance testing. Thus, we compare our results only to the
tools listed above.
Even though an improvement in POS tagging
might be a questionable enterprise (given that its
effects on other tasks, such as parsing or other
NLP problems are less than clear—at least for En-
glish), it is still an interesting problem. Moreover,
the “ideal”2 situation of having a single algorithm
(and its implementation) for many (if not all) lan-
guages has not been reached yet. We have cho-
sen Collins’ perceptron algorithm because of its
simplicity, short training times, and an apparent
room for improvement with (substantially) grow-
ing data sizes (see Figure 1). However, it is clear
that there is usually little chance to get (substan-
tially) more manually annotated data. Thus, we
have been examining the effect of adding a large
monolingual corpus to Collins’ perceptron, appro-
priately extended, for two typologically different
languages: English and Czech. It is clear however
that the features (feature templates) that the tag-
gers use are still language-dependent.
One of the goals is also to have a fast im-
plementation for tagging large amounts of data
quickly. We have experimented with various clas-
sifier combination methods, such as those de-
scribed in (Brill and Wu, 1998) or (van Halteren et
al., 2001), and got improved results, as expected.
However, we view this only as a side effect (yet, a
positive one)—our goal was to stay on the turf of
single taggers, which are both the common ground
for competing on tagger accuracy today and also
significantly faster at runtime.3 Nevertheless, we
have found that it is advantageous to use them to
(pre-)tag the large amounts of plain text data dur-
</bodyText>
<footnote confidence="0.9986392">
2We mean easy to use for further research on problems
requiring POS tagging, especially multilingual ones.
3And much easier to (re)implement as libraries in proto-
type systems, which is often difficult if not impossible with
other people’s code.
</footnote>
<note confidence="0.9622345">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 763–771,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995283">
763
</page>
<figure confidence="0.641239">
Training data size (thousands of tokens)
</figure>
<figureCaption confidence="0.91876">
Figure 1: Accuracy of the original averaged per-
ceptron, supervised training on PTB/WSJ (En-
glish)
</figureCaption>
<bodyText confidence="0.98900715">
ing the training phase.
Apart from feeding the perceptron by various
mixtures of manually tagged (“supervised”) and
auto-tagged (“unsupervised”)4 data, we have also
used various feature templates extensively; for ex-
ample, we use lexicalization (with the added twist
of lemmatization, useful especially for Czech, an
inflectionally rich language), “manual” tag clas-
sification into large classes (again, useful espe-
cially for Czech to avoid the huge, still-to-be-
overcome data sparseness for such a language5),
and sub-lexical features mainly targeted at OOV
words. Inspired i.a. by (Toutanova et al., 2003)
and (Hajiˇc and Vidov´a-Hladk´a, 1998), we also use
“lookahead” features (however, we still remain
in the left-to-right HMM world – in this respect
our solution is closer to the older work of (Hajiˇc
and Vidov´a-Hladk´a, 1998) than to (Toutanova et
al., 2003), who uses bidirectional dependencies
to include the right-hand side disambiguated tags,
</bodyText>
<footnote confidence="0.704420571428572">
4For brevity, we will use the terms “supervised” and “un-
supervised” data for “manually annotated” and “(automat-
ically annotated) plain (raw) text” data, respectively, even
though these adjectives are meant to describe the process of
learning, not the data themselves.
5As (Hajiˇc, 2004) writes, Czech has 4400 plausible tags,
of which we have observed almost 2000 in the 100M cor-
pus we have used in our experiments. However, only 1100
of them have been found in the manually annotated PDT 2.0
corpus (the corpus on which we have based the supervised
experiments). The situation with word forms (tokens) is even
worse: Czech has about 20M different word forms, and the
OOV rate based on the 1.5M PDT 2.0 data and measured
against the 100M raw corpus is almost 10 %.
</footnote>
<bodyText confidence="0.999494666666667">
which we cannot.)
To summarize, we can describe our system as
follows: it is based on (Votrubec, 2006)’s imple-
mentation of (Collins, 2002), which has been fed
at each iteration by a different dataset consisting
of the supervised and unsupervised part: precisely,
by a concatenation of the manually tagged training
data (WSJ portion of the PTB 3 for English, mor-
phologically disambiguated data from PDT 2.0 for
Czech) and a chunk of automatically tagged unsu-
pervised data. The “parameters” of the training
process (feature templates, the size of the unsu-
pervised chunks added to the trainer at each itera-
tion, number of iterations, the combination of tag-
gers that should be used in the auto-tagging of the
unsupervised chunk, etc.) have been determined
empirically in a number of experiments on a de-
velopment data set. We should also note that as a
result of these development-data-based optimiza-
tions, no feature pruning has been employed (see
Section 4 for details); adding (even lexical) fea-
tures from the auto-tagged data did not give signif-
icant accuracy improvements (and only made the
training very slow).
The final taggers have surpassed the current
state-of-the-art taggers by significant margins (we
have achieved 4.12 % relative error reduction for
English and 4.86 % for Czech over the best pre-
viously published results, single or combined),
using a single tagger. However, the best En-
glish tagger combining some of the previous state-
of-the-art ones is still “optically” better (yet not
significantly—see Section 6).
</bodyText>
<sectionHeader confidence="0.785012" genericHeader="introduction">
2 The perceptron algorithm
</sectionHeader>
<bodyText confidence="0.999577333333333">
We have used the Morˇce6 tagger (Votrubec, 2006)
as a main component in our experiments. It is a
reimplementation of the averaged perceptron de-
scribed in (Collins, 2002), which uses such fea-
tures that it behaves like an HMM tagger and thus
the standard Viterbi decoding is possible. Collins’
GEN(x) set (a set of possible tags at any given
position) is generated, in our case, using a mor-
phological analyzer for the given language (essen-
</bodyText>
<footnote confidence="0.996884857142857">
6The name “Morˇce” stands for “MORfologie ˇCEˇstiny”
(“Czech morphology”, see (Votrubec, 2006)), since it
has been originally developed for Czech. We keep this
name in this paper as the generic name of the aver-
aged perceptron tagger for the English-language experi-
ments as well. We have used the version available at
http://ufal.mff.cuni.cz/morce/.
</footnote>
<table confidence="0.865822666666667">
100 200 300 400 500 600 700 800 900
96.0 96.5 97.0 97.5 98.0
Accuracy on development data
</table>
<page confidence="0.998291">
764
</page>
<bodyText confidence="0.99984528">
tially, a dictionary that returns all possible tags7
for an input word form). The transition and out-
put scores for the candidate tags are based on a
large number of binary-valued features and their
weights, which are determined during iterative
training by the averaged perceptron algorithm.
The binary features describe the tag being pre-
dicted and its context. They can be derived from
any information we already have about the text at
the point of decision (respecting the HMM-based
overall setting). Every feature can be true or false
in a given context, so we can consider the true fea-
tures at the current position to be the description
of a tag and its context.
For every feature, the perceptron keeps its
weight coefficient, which is (in its basic version)
an integer number, (possibly) changed at every
training sentence. After its final update, this in-
teger value is stored with the feature to be later
retrieved and used at runtime. Then, the task of
the perceptron algorithm is to sum up all the co-
efficients of true features in a given context. The
result is passed to the Viterbi algorithm as a tran-
sition and output weight for the current state.8 We
can express it as
</bodyText>
<equation confidence="0.997073666666667">
n
w(C, T) _ αi.Oi(C,T) (1)
i=1
</equation>
<bodyText confidence="0.9999655">
where w(C, T) is the transition weight for tag T
in context C, n is the number of features, αi is the
weight coefficient of the ith feature and Oi(C, T)
is the evaluation of the ith feature for context C
and tag T. In the averaged perceptron, the val-
ues of every coefficient are added up at each up-
date, which happens (possibly) at each training
sentence, and their arithmetic average is used in-
stead.9 This trick makes the algorithm more re-
sistant to weight oscillations during training (or,
more precisely, at the end of it) and as a result, it
substantially improves its performance.10
</bodyText>
<footnote confidence="0.8770022">
7And lemmas, which are then used in some of the fea-
tures. A (high recall, low precision) “guesser” is used for
OOV words.
8Which identifies unambiguously the corresponding tag.
9Implementation note: care must be taken to avoid inte-
</footnote>
<bodyText confidence="0.882115961538462">
ger overflows, which (at 100 iterations through millions of
sentences) can happen for 32bit integers easily.
10Our experiments have shown that using averaging helps
tremendously, confirming both the theoretical and practical
results of (Collins, 2002). On Czech, using the best feature
set, the difference on the development data set is 95.96 % vs.
95.02 %. Therefore, all the results presented in the following
text use averaging.
The supervised training described in (Collins,
2002) uses manually annotated data for the esti-
mation of the weight coefficients α. The train-
ing algorithm is very simple—only integer num-
bers (counts and their sums for the averaging) are
updated for each feature at each sentence with
imperfect match(es) found against the gold stan-
dard. Therefore, it can be relatively quickly re-
trained and thus many different feature sets and
other training parameters, such as the number of
iterations, feature thresholds etc. can be con-
sidered and tested. As a result of this tuning,
our (fully supervised) version of the Morˇce tag-
ger gives the best accuracy among all single tag-
gers for Czech and also very good results for En-
glish, being beaten only by the tagger (Shen et al.,
2007) (by 0.10 % absolute) and (not significantly)
by (Toutanova et al., 2003).
</bodyText>
<sectionHeader confidence="0.996792" genericHeader="method">
3 The data
</sectionHeader>
<subsectionHeader confidence="0.999947">
3.1 The “supervised” data
</subsectionHeader>
<bodyText confidence="0.999883">
For English, we use the same data division of Penn
Treebank (PTB) parsed section (Marcus et al.,
1994) as all of (Collins, 2002), (Toutanova et al.,
2003), (Gim´enez and M`arquez, 2004) and (Shen
et al., 2007) do; for details, see Table 1.
</bodyText>
<table confidence="0.56139425">
data set tokens sentences
train (0-18) 912,344 38,220
dev-test (19-21) 131,768 5,528
eval-test (22-24) 129,654 5,463
</table>
<tableCaption confidence="0.775154">
Table 1: English supervised data set — WSJ part
of Penn Treebank 3
</tableCaption>
<bodyText confidence="0.749170333333333">
For Czech, we use the current standard Prague
Dependency Treebank (PDT 2.0) data sets (Hajiˇc
et al., 2006); for details, see Table 2.
</bodyText>
<table confidence="0.63679375">
data set tokens sentences
train 1,539,241 91,049
dev-test 201,651 11,880
eval-test 219,765 13,136
</table>
<tableCaption confidence="0.8951495">
Table 2: Czech supervised data set — Prague De-
pendency Treebank 2.0
</tableCaption>
<subsectionHeader confidence="0.999712">
3.2 The “unsupervised” data
</subsectionHeader>
<bodyText confidence="0.997844">
For English, we have processed the North Amer-
ican News Text corpus (Graff, 1995) (without the
</bodyText>
<page confidence="0.988368">
765
</page>
<bodyText confidence="0.9999354">
WSJ section) with the Stanford segmenter and to-
kenizer (Toutanova et al., 2003). For Czech, we
have used the SYN2005 part of Czech National
Corpus (CNC, 2005) (with the original segmenta-
tion and tokenization).
</bodyText>
<subsectionHeader confidence="0.984023">
3.3 GEN(x): The morphological analyzers
</subsectionHeader>
<bodyText confidence="0.999807176470588">
For English, we perform a very simple morpholog-
ical analysis, which reduces the full PTB tagset to
a small list of tags for each token on input. The re-
sulting list is larger than such a list derived solely
from the PTB/WSJ, but much smaller than a full
list of tags found in the PTB/WSJ.11 The English
morphological analyzer is thus (empirically) opti-
mized for precision while keeping as high recall
as possible (it still overgenerates). It consists of a
small dictionary of exceptions and a small set of
general rules, thus covering also a lot of OOV to-
kens.12
For Czech, the separate morphological analyzer
(Hajiˇc, 2004) usually precedes the tagger. We use
the version from April 2006 (the same as (Spous-
tov´a et al., 2007), who reported the best previous
result on Czech tagging).
</bodyText>
<sectionHeader confidence="0.941339" genericHeader="method">
4 The perceptron feature sets
</sectionHeader>
<bodyText confidence="0.999828157894737">
The averaged perceptron’s accuracy is determined
(to a large extent) by the set of features used. A
feature set is based on feature templates, i.e. gen-
eral patterns, which are filled in with concrete val-
ues from the training data. Czech and English
are morphosyntactically very different languages,
therefore each of them needs a different set of
feature templates. We have empirically tested
hundreds of feature templates on both languages,
taken over from previous works for direct compar-
ison, inspired by them, or based on a combination
of previous experience, error analysis and linguis-
tic intuition.
In the following sections, we present the best
performing set of feature templates as determined
on the development data set using only the super-
vised training setting; our feature templates have
thus not been influenced nor extended by the un-
supervised data.13
</bodyText>
<footnote confidence="0.9974185">
11The full list of tags, as used by (Shen et al., 2007), also
makes the underlying Viterbi algorithm unbearably slow.
12The English morphology tool is also downloadable as a
separate module on the paper’s accompanying website.
13Another set of experiments has shown that there is not,
perhaps surprisingly, a significant gain in doing so.
</footnote>
<subsectionHeader confidence="0.956719">
4.1 English feature templates
</subsectionHeader>
<bodyText confidence="0.999192">
The best feature set for English consists of 30 fea-
ture templates. All templates predict the current
tag as a whole. A detailed description of the En-
glish feature templates can be found in Table 3.
</bodyText>
<table confidence="0.999753066666667">
Context predicting whole tag
Tags Previous tag
Previous two tags
First letter of previous tag
Word forms Current word form
Previous word form
Previous two word forms
Following word form
Following two word forms
Last but one word form
Current word affixes Prefixes of length 1-9
Suffixes of length 1-9
Current word features Contains number
Contains dash
Contains upper case letter
</table>
<tableCaption confidence="0.999586">
Table 3: Feature templates for English
</tableCaption>
<bodyText confidence="0.991558666666667">
A total of 1,953,463 features has been extracted
from the supervised training data using the tem-
plates from Table 3.
</bodyText>
<subsectionHeader confidence="0.996745">
4.2 Czech feature templates
</subsectionHeader>
<bodyText confidence="0.999825703703704">
The best feature set for Czech consists of 63 fea-
ture templates. 26 of them predict current tag as
a whole, whereas the rest predicts only some parts
of the current tag separately (e.g., detailed POS,
gender, case) to avoid data sparseness. Such a fea-
ture is true, in an identical context, for several dif-
ferent tags belonging to the same class (e.g., shar-
ing a locative case). The individual grammatical
categories used for such classing have been cho-
sen on both linguistic grounds (POS, detailed fine-
grained POS) and also such categories have been
used which contribute most to the elimination of
the tagger errors (based on an extensive error anal-
ysis of previous results, the detailed description of
which can be found in (Votrubec, 2006)).
Several features can look ahead (to the right
of the current position) - apart from the obvious
word form, which is unambiguous, we have used
(in case of ambiguity) a random tag and lemma of
the first position to the right from the current po-
sition which might be occupied with a verb (based
on dictionary and the associated morphological
guesser restrictions).
A total of 8,440,467 features has been extracted
from the supervised training data set. A detailed
description is included in the distribution down-
loadable from the Morˇce website.
</bodyText>
<page confidence="0.995455">
766
</page>
<sectionHeader confidence="0.964886" genericHeader="method">
5 The (un)supervised training setup
</sectionHeader>
<bodyText confidence="0.993984461538461">
We have extended the averaged perceptron setup
in the following way: the training algorithm is
fed, in each iteration, by a concatenation of the
supervised data (the manually tagged corpus) and
the automatically pre-tagged unsupervised data,
different for each iteration (in this order). In
other words, the training algorithm proper does
not change at all: it is the data and their selection
(including the selection of the way they are auto-
matically tagged) that makes all the difference.
The following “parameters” of the (unsuper-
vised part of the) data selection had to be deter-
mined experimentally:
</bodyText>
<listItem confidence="0.993805285714286">
• the tagging process for tagging the selected
data
• the selection mechanism (sequential or ran-
dom with/without replacement)
• the size to use for each iteration
• and the use and order of concatenation with
the manually tagged data.
</listItem>
<bodyText confidence="0.999957">
We have experimented with various settings to
arrive at the best performing configuration, de-
scribed below. In each subsection, we compare
the result of our ,,winning“ configuration with re-
sults of the experiments which have the selected
attributes omitted or changed; everything is mea-
sured on the development data set.
</bodyText>
<subsectionHeader confidence="0.996163">
5.1 Tagging the plain data
</subsectionHeader>
<bodyText confidence="0.999974444444444">
In order to simulate the labeled training events,
we have tagged the unsupervised data simply by
a combination of the best available taggers. For
practical reasons (to avoid prohibitive training
times), we have tagged all the data in advance, i.e.
no re-tagging is performed between iterations.
The setup for the combination is as follows (the
idea is simplified from (Spoustov´a et al., 2007)
where it has been used in a more complex setting):
</bodyText>
<listItem confidence="0.996168125">
1. run N different taggers independently;
2. join the results on each position in the data
from the previous step — each token thus
ends up with between 1 and N tags, a union
of the tags output by the taggers at that posi-
tion;
3. do final disambiguation (by a single tag-
ger14).
</listItem>
<table confidence="0.97893775">
Tagger Accuracy
Morˇce 97.21
Shen 97.33
Combination 97.44
</table>
<tableCaption confidence="0.983118333333333">
Table 4: Dependence on the tagger(s) used to tag
the additional plain text data (English)16
Table 4 illustrates why it is advantageous to go
</tableCaption>
<bodyText confidence="0.933874">
through this (still)16 complicated setup against a
single-tagger bootstrapping mechanism, which al-
ways uses the same tagger for tagging the unsu-
pervised data.
For both English and Czech, the selection of
taggers, the best combination and the best over-
all setup has been optimized on the development
data set. A bit surprisingly, the final setup is very
similar for both languages (two taggers to tag the
data in Step 1, and a third one to finish it up).
For English, we use three state-of-the-art tag-
gers: the taggers of (Toutanova et al., 2003) and
(Shen et al., 2007) in Step 1, and the SVM tag-
ger (Gim´enez and M`arquez, 2004) in Step 3. We
run the taggers with the parameters which were
shown to be the best in the corresponding papers.
The SVM tagger needed to be adapted to accept
the (reduced) list of possible tags.17
For Czech, we use the Feature-based tagger
(Hajiˇc, 2004) and the Morˇce tagger (with the new
feature set as described in section 4) in Step 1, and
an HMM tagger (Krbec, 2005) in Step 3. This
combination outperforms the results in (Spoustov´a
et al., 2007) by a small margin.
</bodyText>
<subsectionHeader confidence="0.999699">
5.2 Selection mechanism for the plain data
</subsectionHeader>
<bodyText confidence="0.995124285714286">
We have found that it is better to feed the training
with different chunks of the unsupervised data at
each iteration. We have then experimented with
14This tagger (possibly different from any of the N taggers
from Step 1) runs as usual, but it is given a minimal list of (at
most N) tags that come from Step 2 only.
15”Accuracy” means accuracy of the semi-supervised
method using this tagger for pre-tagging the unsupervised
data, not the accuracy of the tagger itself.
16In fact, we have experimented with other tagger
combinations and configurations as well—with the TnT
(Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTag-
ger (Schmid, 1994), with or without the Morˇce tagger in the
pack; see below for the winning combination.
</bodyText>
<footnote confidence="0.775814">
17This patch is available on the paper’s website (see Sec-
tion 7).
</footnote>
<page confidence="0.994975">
767
</page>
<bodyText confidence="0.998054222222222">
three methods of unsupervised data selection, i.e.
generating the unsupervised data chunks for each
training iteration from the ,,pool“ of sentences.
These methods are: simple sequential chopping,
randomized data selection with replacement and
randomized selection without replacement. Ta-
ble 5 demonstrates that there is practically no dif-
ference in the results. Thus, we use the sequential
chopping mechanism, mainly for its simplicity.
</bodyText>
<table confidence="0.9988005">
Method of data selection English Czech
Sequential chopping 97.44 96.21
Random without replacement 97.44 96.20
Random with replacement 97.44 96.21
</table>
<tableCaption confidence="0.999046">
Table 5: Unsupervised data selection
</tableCaption>
<subsectionHeader confidence="0.99907">
5.3 Joining the data
</subsectionHeader>
<bodyText confidence="0.995683125">
We have experimented with various sizes of the
unsupervised parts (from 500k tokens to 5M) and
also with various numbers of iterations. The best
results (on the development data set) have been
achieved with the unsupervised chunks containing
approx. 4 million tokens for English and 1 million
tokens for Czech. Each training process consists
of (at most) 100 iterations (Czech) or 50 iterations
(English); therefore, for the 50 (100) iterations we
needed only about 200,000,000 (100,000,000) to-
kens of raw texts. The best development data set
results have been (with the current setup) achieved
on the 44th (English) and 33th (Czech) iteration.
The development data set has been also used to
determine the best way to “merge” the manually
labeled data (the PTB/WSJ and the PDT 2.0 train-
ing data) and the unsupervised parts of the data.
Given the properties of the perceptron algorithm,
it is not too surprising that the best solution is to
put (the full size of) the manually labeled data first,
followed by the (four) million-token chunk of the
automatically tagged data (different data in each
chunk but of the same size for each iteration). It
corresponds to the situation when the trainer is pe-
riodically “returned to the right track” by giving it
the gold standard data time to time.
Figure 2 (English) and especially Figure 3
(Czech) demonstrate the perceptron behavior in
cases where the supervised data precede the un-
supervised data only in selected iterations. A sub-
set of these development results is also present in
Table 6.
</bodyText>
<figure confidence="0.544189">
Iteration
</figure>
<figureCaption confidence="0.997339">
Figure 2: Dependence on the inclusion of the su-
pervised training data (English)
</figureCaption>
<table confidence="0.9519155">
English Czech
No supervised data 97.37 95.88
Once at the beginning 97.40 96.00
Every training iteration 97.44 96.21
</table>
<tableCaption confidence="0.959619">
Table 6: Dependence on the inclusion of the su-
pervised training data
</tableCaption>
<subsectionHeader confidence="0.583883">
5.4 The morphological analyzers and the
perceptron feature templates
</subsectionHeader>
<bodyText confidence="0.999818590909091">
The whole experiment can be performed with
the original perceptron feature set described in
(Collins, 2002) instead of the feature set described
in this article. The results are compared in Table 7
(for English only).
Also, for English it is not necessary to use our
morphological analyzer described in section 3.3
(other variants are to use the list of tags derived
solely from the WSJ training data or to give each
token the full list of tags found in WSJ). It is
practically impossible to perform the unsupervised
training with the full list of tags (it would take sev-
eral years instead of several days with the default
setup), thus we compare only the results with mor-
phological analyzer to the results with the list of
tags derived from the training data, see Table 8.
It can be expected (some approximated exper-
iments were performed) that the results with the
full list of tags would be very similar to the results
with the morphological analyzer, i.e. the morpho-
logical analyzer is used mainly for technical rea-
sons. Our expectations are based mainly (but not
</bodyText>
<table confidence="0.625502571428571">
0 10 20 30 40 50
Accuracy on development data
97.20 97.25 97.30 97.35 97.40
Every iteration
Every 4th iteration
Every 8th iteration
Every 16th iteration
Once at the beginning
No supervised data
768
GEN(x) Accuracy
List of tags derived from train 95.89
Our morphological analyzer 97.17
Full tagset 97.15
</table>
<tableCaption confidence="0.796057">
Table 9: Supervised training results: dependence
on the GEN(x)
</tableCaption>
<figure confidence="0.760818">
Iteration
</figure>
<figureCaption confidence="0.987056">
Figure 3: Dependence on the inclusion of the su-
pervised training data (Czech)
</figureCaption>
<table confidence="0.999406">
Tagger accuracy
Collins 97.07 %
SVM 97.16 %
Stanford 97.24 %
Shen 97.33 %
Morˇce supervised 97.23 %
combination 97.48 %
Morˇce semi-supervised 97.44 %
</table>
<tableCaption confidence="0.999288">
Table 10: Evaluation of the English taggers
</tableCaption>
<figure confidence="0.7913035">
0 10 20 30 40 50
Accuracy on development data
95.6 95.7 95.8 95.9 96.0 96.1 96.2
Every iteration
</figure>
<figureCaption confidence="0.717453">
Every 4th iteration
Every 8th iteration
Every 16th iteration
Once at the beginning
No supervised data
</figureCaption>
<bodyText confidence="0.99923225">
only) on the supervised training results, where the
performance of the taggers using the morpholog-
ical analyzer output and using the full list of tags
are nearly the same, see Table 9.
</bodyText>
<table confidence="0.95175">
Feature set Accuracy
Collins’ 97.38
Our’s 97.44
</table>
<tableCaption confidence="0.9511545">
Table 7: Dependence on the feature set used by the
perceptron algorithm (English)
</tableCaption>
<table confidence="0.989621333333333">
GEN(x) Accuracy
List of tags derived from train 97.13
Our morphological analyzer 97.44
</table>
<tableCaption confidence="0.996257">
Table 8: Dependence on the GEN(x)
</tableCaption>
<sectionHeader confidence="0.999573" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.948600636363636">
In Tables 10 and 11, the main results (on the eval-
test data sets) are summarized. The state-of-the
art taggers are using feature sets discribed in the
corresponding articles ((Collins, 2002), (Gim´enez
and M`arquez, 2004), (Toutanova et al., 2003) and
(Shen et al., 2007)), Morˇce supervised and Morˇce
semi-supervised are using feature set desribed in
section 4.
For significance tests, we have used the paired
Wilcoxon signed rank test as implemented in the
R package (R Development Core Team, 2008)
</bodyText>
<table confidence="0.998434666666667">
Tagger accuracy
Feature-based 94.04 %
HMM 94.82 %
Morˇce supervised 95.67 %
combination 95.70 %
Morˇce semi-supervised 95.89 %
</table>
<tableCaption confidence="0.999796">
Table 11: Evaluation of the Czech taggers
</tableCaption>
<bodyText confidence="0.996284">
in wilcox.test(), dividing the data into 100
chunks (data pairs).
</bodyText>
<subsectionHeader confidence="0.991374">
6.1 English
</subsectionHeader>
<bodyText confidence="0.999992692307692">
The combination of the three existing English tag-
gers seems to be best, but it is not significantly
better than our semi-supervised approach.
The combination is significantly better than
(Shen et al., 2007) at a very high level, but more
importantly, Shen’s results (currently represent-
ing the replicable state-of-the-art in POS tagging)
have been significantly surpassed also by the semi-
supervised Morˇce (at the 99 % confidence level).
In addition, the semi-supervised Morˇce per-
forms (on single CPU and development data set)
77 times faster than the combination and 23 times
faster than (Shen et al., 2007).
</bodyText>
<subsectionHeader confidence="0.993454">
6.2 Czech
</subsectionHeader>
<bodyText confidence="0.997336666666667">
The best results (Table 11) are statistically signif-
icantly better than the previous results: the semi-
supervised Morˇce is significantly better than both
</bodyText>
<page confidence="0.99317">
769
</page>
<bodyText confidence="0.9974555">
the combination and the supervised (original) vari-
ant at a very high level.
</bodyText>
<sectionHeader confidence="0.997262" genericHeader="method">
7 Download
</sectionHeader>
<bodyText confidence="0.999958428571429">
We decided to publish our system for wide use un-
der the name COMPOST (Common POS Tagger).
All the programs, patches and data files are avail-
able at the website http://ufal.mff.cuni.cz/compost
under either the original data provider license, or
under the usual GNU General Public License, un-
less they are available from the widely-known and
easily obtainable sources (such as the LDC, in
which case pointers are provided on the download
website).
The Compost website also contains easy-to-run
Linux binaries of the best English and Czech sin-
gle taggers (based on the Morˇce technology) as de-
scribed in Section 6.
</bodyText>
<sectionHeader confidence="0.984413" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9962494">
We have shown that the “right”18 mixture of su-
pervised and unsupervised (auto-tagged) data can
significantly improve tagging accuracy of the av-
eraged perceptron on two typologically different
languages (English and Czech), achieving the best
known accuracy to date.
To determine what is the contribution of the in-
dividual ”dimensions” of the system setting, as
described in Sect. 5, we have performed exper-
iments fixing all but one of the dimensions, and
compared their contribution (or rather, their loss
when compared to the best ”mix” overall). For
English, we found that excluding the state-of-the-
art-tagger (in fact, a carefully selected combina-
tion of taggers yielding significantly higher qual-
ity than any of them has) drops the resulting ac-
curacy the most (0.2 absolute). Significant yet
smaller drop (less than 0.1 percent) appears when
the manually tagged portion of the data is not used
or used only once (or infrequently) in the input
to the perceptron’s learner. The difference in us-
ing various feature templates (yet all largely sim-
ilar to what state-of-the-art taggers currently use)
is not significant. Similarly, the way the unsuper-
vised data is selected plays no role, either; this dif-
fers from the bagging technique (Breiman, 1996)
where it is significant. For Czech, the drop in ac-
curacy appears in all dimensions, except the unsu-
pervised data selection one. We have used novel
features inspired by previous work but not used in
18As empirically determined on the development data set.
the standard perceptron setting yet (linguistically
motivated tag classes in features, lookahead fea-
tures). Interestingly, the resulting tagger is better
than even a combination of the previous state-of-
the-art taggers (for English, this comparison is in-
conclusive).
We are working now on parallelization of the
perceptron training, which seems to be possible
(based i.a. on small-scale preliminary experiments
with only a handful of parallel processes and
specific data sharing arrangements among them).
This would further speed up the training phase, not
just as a nice bonus per se, but it would also allow
for a semi-automated feature template selection,
avoiding the (still manual) feature template prepa-
ration for individual languages. This would in turn
facilitate one of our goals to (publicly) provide
single-implementation, easy-to-maintain state-of-
the-art tagging tools for as many languages as pos-
sible (we are currently preparing Dutch, Slovak
and several other languages).19
Another area of possible future work is more
principled tag classing for languages with large
tagsets (in the order of 103), and/or adding
syntactically-motivated features; it has helped
Czech tagging accuracy even when only the “in-
trospectively” defined classes have been added. It
is an open question if a similar approach helps
English as well (certain grammatical categories
can be generalized from the current WSJ tagset as
well, such as number, degree of comparison, 3rd
person present tense).
Finally, it would be nice to merge some of the
approaches by (Toutanova et al., 2003) and (Shen
et al., 2007) with the ideas of semi-supervised
learning introduced here, since they seem orthog-
onal in at least some aspects (e.g., to replace the
rudimentary lookahead features with full bidirec-
tionality).
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999735">
The research described here was supported by the
projects MSM0021620838 and LC536 of Ministry
of Education, Youth and Sports of the Czech Re-
public, GA405/09/0278 of the Grant Agency of the
Czech Republic and 1ET101120503 of Academy
of Sciences of the Czech Republic.
</bodyText>
<footnote confidence="0.758335">
19Available soon also on the website.
</footnote>
<page confidence="0.995824">
770
</page>
<sectionHeader confidence="0.995827" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938765306122">
Thorsten Brants. 2000. TnT - a Statistical Part-of-
Speech Tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference, pages
224–231, Seattle, WA. ACL.
Leo Breiman. 1996. Bagging predictors. Mach.
Learn., 24(2):123–140.
Eric Brill and Jun Wu. 1998. Classifier Combination
for Improved Lexical Disambiguation. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics, pages 191–195, Montreal, Que-
bec, Canada. Association for Computational Lin-
guistics.
CNC, 2005. Czech National Corpus – SYN2005. In-
stitute of Czech National Corpus, Faculty of Arts,
Charles University, Prague, Czech Republic.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In EMNLP ’02:
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing, volume 10,
pages 1–8, Philadelphia, PA.
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool:
A General POS Tagger Generator Based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation, pages 43–46, Lisbon, Portugal.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium, Cat. LDC95T21,
Philadelphia, PA.
Jan Hajiˇc and Barbora Vidov´a-Hladk´a. 1998. Tag-
ging Inflective Languages: Prediction of Morpho-
logical Categories for a Rich, Structured Tagset.
In Proceedings of the 17th international conference
on Computational linguistics, pages 483–490. Mon-
treal, Quebec, Canada.
Jan Hajiˇc. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv´ı Karolinum, Prague.
Jan Hajiˇc, Eva Hajiˇcov´a, Jarmila Panevov´a, Petr Sgall,
Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, and Marie
Mikulov´a. 2006. Prague Dependency Treebank
v2.0, CDROM, LDC Cat. No. LDC2006T01. Lin-
guistic Data Consortium, Philadelphia, PA.
Pavel Krbec. 2005. Language Modelling for Speech
Recognition of Czech. Ph.D. thesis, UK MFF,
Prague, Malostransk´e n´amˇest´ı 25, 118 00 Praha 1.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1994. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.
R Development Core Team, 2008. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the 1st EMNLP, pages 133–142, New Brunswick,
NJ. ACL.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, page 9pp., Manchester, GB.
Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007.
Guided Learning for Bidirectional Sequence Classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760–767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Drahom´ıra ”johanka” Spoustov´a, Jan Hajiˇc, Jan
Votrubec, Pavel Krbec, and Pavel Kvˇetoˇn. 2007.
The Best of Two Worlds: Cooperation of Statistical
and Rule-Based Taggers for Czech. In Proceedings
of the Workshop on Balto-Slavonic Natural Lan-
guage Processing 2007, pages 67–74, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL-
08: HLT, pages 665–673, Columbus, Ohio, June.
Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In NAACL ’03: Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology, pages 173–180, Edmonton,
Canada. Association for Computational Linguistics.
Hans van Halteren, Walter Daelemans, and Jakub Za-
vrel. 2001. Improving accuracy in word class
tagging through the combination of machine learn-
ing systems. Computational Linguistics, 27(2):199–
229.
Jan Votrubec. 2006. Morphological Tagging Based
on Averaged Perceptron. In WDS’06 Proceedings of
Contributed Papers, pages 191–195, Prague, Czech
Republic. Matfyzpress, Charles University.
</reference>
<page confidence="0.997955">
771
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527539">
<title confidence="0.996588">Semi-supervised Training for the Averaged Perceptron POS Tagger</title>
<author confidence="0.964539">Drahomira “johanka” Spoustov´a Jan Hajiˇc Jan Raab Miroslav Spousta</author>
<affiliation confidence="0.9986475">Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics,</affiliation>
<address confidence="0.77904">Charles University Prague, Czech Republic</address>
<email confidence="0.9223455">johanka@ufal.mff.cuni.cz</email>
<email confidence="0.9223455">hajic@ufal.mff.cuni.cz</email>
<email confidence="0.9223455">raab@ufal.mff.cuni.cz</email>
<email confidence="0.9223455">spousta@ufal.mff.cuni.cz</email>
<abstract confidence="0.988439555555556">This paper describes POS tagging experiments with semi-supervised training as an extension to the (supervised) averaged perceptron algorithm, first introduced for this task by (Collins, 2002). Experiments with an iterative training on standard-sized supervised (manually annotated) dataset tokens) combined with a relatively (in the order of tokens) unsupervised (plain) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT - a Statistical Part-ofSpeech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference,</booktitle>
<pages>224--231</pages>
<publisher>ACL.</publisher>
<location>Seattle, WA.</location>
<contexts>
<context position="20763" citStr="Brants, 2000" startWordPosition="3411" endWordPosition="3412">anism for the plain data We have found that it is better to feed the training with different chunks of the unsupervised data at each iteration. We have then experimented with 14This tagger (possibly different from any of the N taggers from Step 1) runs as usual, but it is given a minimal list of (at most N) tags that come from Step 2 only. 15”Accuracy” means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself. 16In fact, we have experimented with other tagger combinations and configurations as well—with the TnT (Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTagger (Schmid, 1994), with or without the Morˇce tagger in the pack; see below for the winning combination. 17This patch is available on the paper’s website (see Section 7). 767 three methods of unsupervised data selection, i.e. generating the unsupervised data chunks for each training iteration from the ,,pool“ of sentences. These methods are: simple sequential chopping, randomized data selection with replacement and randomized selection without replacement. Table 5 demonstrates that there is practically no difference in the results. Thus, we use the sequ</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT - a Statistical Part-ofSpeech Tagger. In Proceedings of the 6th Applied Natural Language Processing Conference, pages 224–231, Seattle, WA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<location>Mach. Learn.,</location>
<contexts>
<context position="29368" citStr="Breiman, 1996" startWordPosition="4804" endWordPosition="4805">lected combination of taggers yielding significantly higher quality than any of them has) drops the resulting accuracy the most (0.2 absolute). Significant yet smaller drop (less than 0.1 percent) appears when the manually tagged portion of the data is not used or used only once (or infrequently) in the input to the perceptron’s learner. The difference in using various feature templates (yet all largely similar to what state-of-the-art taggers currently use) is not significant. Similarly, the way the unsupervised data is selected plays no role, either; this differs from the bagging technique (Breiman, 1996) where it is significant. For Czech, the drop in accuracy appears in all dimensions, except the unsupervised data selection one. We have used novel features inspired by previous work but not used in 18As empirically determined on the development data set. the standard perceptron setting yet (linguistically motivated tag classes in features, lookahead features). Interestingly, the resulting tagger is better than even a combination of the previous state-ofthe-art taggers (for English, this comparison is inconclusive). We are working now on parallelization of the perceptron training, which seems </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Mach. Learn., 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classifier Combination for Improved Lexical Disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>191--195</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="3193" citStr="Brill and Wu, 1998" startWordPosition="492" endWordPosition="495">. However, it is clear that there is usually little chance to get (substantially) more manually annotated data. Thus, we have been examining the effect of adding a large monolingual corpus to Collins’ perceptron, appropriately extended, for two typologically different languages: English and Czech. It is clear however that the features (feature templates) that the taggers use are still language-dependent. One of the goals is also to have a fast implementation for tagging large amounts of data quickly. We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. However, we view this only as a side effect (yet, a positive one)—our goal was to stay on the turf of single taggers, which are both the common ground for competing on tagger accuracy today and also significantly faster at runtime.3 Nevertheless, we have found that it is advantageous to use them to (pre-)tag the large amounts of plain text data dur2We mean easy to use for further research on problems requiring POS tagging, especially multilingual ones. 3And much easier to (re)implement as libraries in prototype systems, wh</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classifier Combination for Improved Lexical Disambiguation. In Proceedings of the 17th international conference on Computational linguistics, pages 191–195, Montreal, Quebec, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CNC</author>
</authors>
<title>Czech National Corpus – SYN2005.</title>
<date>2005</date>
<institution>Institute of Czech National Corpus, Faculty of Arts, Charles University,</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12688" citStr="CNC, 2005" startWordPosition="2061" endWordPosition="2062"> part of Penn Treebank 3 For Czech, we use the current standard Prague Dependency Treebank (PDT 2.0) data sets (Hajiˇc et al., 2006); for details, see Table 2. data set tokens sentences train 1,539,241 91,049 dev-test 201,651 11,880 eval-test 219,765 13,136 Table 2: Czech supervised data set — Prague Dependency Treebank 2.0 3.2 The “unsupervised” data For English, we have processed the North American News Text corpus (Graff, 1995) (without the 765 WSJ section) with the Stanford segmenter and tokenizer (Toutanova et al., 2003). For Czech, we have used the SYN2005 part of Czech National Corpus (CNC, 2005) (with the original segmentation and tokenization). 3.3 GEN(x): The morphological analyzers For English, we perform a very simple morphological analysis, which reduces the full PTB tagset to a small list of tags for each token on input. The resulting list is larger than such a list derived solely from the PTB/WSJ, but much smaller than a full list of tags found in the PTB/WSJ.11 The English morphological analyzer is thus (empirically) optimized for precision while keeping as high recall as possible (it still overgenerates). It consists of a small dictionary of exceptions and a small set of gen</context>
</contexts>
<marker>CNC, 2005</marker>
<rawString>CNC, 2005. Czech National Corpus – SYN2005. Institute of Czech National Corpus, Faculty of Arts, Charles University, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<volume>10</volume>
<pages>1--8</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1249" citStr="Collins, 2002" startWordPosition="175" endWordPosition="176">vely modest (in the order of 108 tokens) unsupervised (plain) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %). 1 Introduction Since 2002, we have seen a renewed interest in improving POS tagging results for English, and an inflow of results (initial or improved) for many other languages. For English, after a relatively big jump achieved by (Collins, 2002), we have seen two significant improvements: (Toutanova et al., 2003) and (Shen et al., 2007) pushed the results by a significant amount each time.1 1In our final comparison, we have also included the results of (Gim´enez and M`arquez, 2004), because it has surpassed (Collins, 2002) as well and we have used this tagger in the data preparation phase. See more details below. Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given a</context>
<context position="6034" citStr="Collins, 2002" startWordPosition="949" endWordPosition="950">ites, Czech has 4400 plausible tags, of which we have observed almost 2000 in the 100M corpus we have used in our experiments. However, only 1100 of them have been found in the manually annotated PDT 2.0 corpus (the corpus on which we have based the supervised experiments). The situation with word forms (tokens) is even worse: Czech has about 20M different word forms, and the OOV rate based on the 1.5M PDT 2.0 data and measured against the 100M raw corpus is almost 10 %. which we cannot.) To summarize, we can describe our system as follows: it is based on (Votrubec, 2006)’s implementation of (Collins, 2002), which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part: precisely, by a concatenation of the manually tagged training data (WSJ portion of the PTB 3 for English, morphologically disambiguated data from PDT 2.0 for Czech) and a chunk of automatically tagged unsupervised data. The “parameters” of the training process (feature templates, the size of the unsupervised chunks added to the trainer at each iteration, number of iterations, the combination of taggers that should be used in the auto-tagging of the unsupervised chunk, etc.) have be</context>
<context position="7620" citStr="Collins, 2002" startWordPosition="1204" endWordPosition="1205">. The final taggers have surpassed the current state-of-the-art taggers by significant margins (we have achieved 4.12 % relative error reduction for English and 4.86 % for Czech over the best previously published results, single or combined), using a single tagger. However, the best English tagger combining some of the previous stateof-the-art ones is still “optically” better (yet not significantly—see Section 6). 2 The perceptron algorithm We have used the Morˇce6 tagger (Votrubec, 2006) as a main component in our experiments. It is a reimplementation of the averaged perceptron described in (Collins, 2002), which uses such features that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible. Collins’ GEN(x) set (a set of possible tags at any given position) is generated, in our case, using a morphological analyzer for the given language (essen6The name “Morˇce” stands for “MORfologie ˇCEˇstiny” (“Czech morphology”, see (Votrubec, 2006)), since it has been originally developed for Czech. We keep this name in this paper as the generic name of the averaged perceptron tagger for the English-language experiments as well. We have used the version available at http://ufal.mff</context>
<context position="10614" citStr="Collins, 2002" startWordPosition="1713" endWordPosition="1714">t oscillations during training (or, more precisely, at the end of it) and as a result, it substantially improves its performance.10 7And lemmas, which are then used in some of the features. A (high recall, low precision) “guesser” is used for OOV words. 8Which identifies unambiguously the corresponding tag. 9Implementation note: care must be taken to avoid integer overflows, which (at 100 iterations through millions of sentences) can happen for 32bit integers easily. 10Our experiments have shown that using averaging helps tremendously, confirming both the theoretical and practical results of (Collins, 2002). On Czech, using the best feature set, the difference on the development data set is 95.96 % vs. 95.02 %. Therefore, all the results presented in the following text use averaging. The supervised training described in (Collins, 2002) uses manually annotated data for the estimation of the weight coefficients α. The training algorithm is very simple—only integer numbers (counts and their sums for the averaging) are updated for each feature at each sentence with imperfect match(es) found against the gold standard. Therefore, it can be relatively quickly retrained and thus many different feature s</context>
<context position="23608" citStr="Collins, 2002" startWordPosition="3862" endWordPosition="3863">or in cases where the supervised data precede the unsupervised data only in selected iterations. A subset of these development results is also present in Table 6. Iteration Figure 2: Dependence on the inclusion of the supervised training data (English) English Czech No supervised data 97.37 95.88 Once at the beginning 97.40 96.00 Every training iteration 97.44 96.21 Table 6: Dependence on the inclusion of the supervised training data 5.4 The morphological analyzers and the perceptron feature templates The whole experiment can be performed with the original perceptron feature set described in (Collins, 2002) instead of the feature set described in this article. The results are compared in Table 7 (for English only). Also, for English it is not necessary to use our morphological analyzer described in section 3.3 (other variants are to use the list of tags derived solely from the WSJ training data or to give each token the full list of tags found in WSJ). It is practically impossible to perform the unsupervised training with the full list of tags (it would take several years instead of several days with the default setup), thus we compare only the results with morphological analyzer to the results </context>
<context position="26052" citStr="Collins, 2002" startWordPosition="4274" endWordPosition="4275">d training results, where the performance of the taggers using the morphological analyzer output and using the full list of tags are nearly the same, see Table 9. Feature set Accuracy Collins’ 97.38 Our’s 97.44 Table 7: Dependence on the feature set used by the perceptron algorithm (English) GEN(x) Accuracy List of tags derived from train 97.13 Our morphological analyzer 97.44 Table 8: Dependence on the GEN(x) 6 Results In Tables 10 and 11, the main results (on the evaltest data sets) are summarized. The state-of-the art taggers are using feature sets discribed in the corresponding articles ((Collins, 2002), (Gim´enez and M`arquez, 2004), (Toutanova et al., 2003) and (Shen et al., 2007)), Morˇce supervised and Morˇce semi-supervised are using feature set desribed in section 4. For significance tests, we have used the paired Wilcoxon signed rank test as implemented in the R package (R Development Core Team, 2008) Tagger accuracy Feature-based 94.04 % HMM 94.82 % Morˇce supervised 95.67 % combination 95.70 % Morˇce semi-supervised 95.89 % Table 11: Evaluation of the Czech taggers in wilcox.test(), dividing the data into 100 chunks (data pairs). 6.1 English The combination of the three existing Eng</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In EMNLP ’02: Proceedings of the ACL-02 conference on Empirical methods in natural language processing, volume 10, pages 1–8, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>SVMTool: A General POS Tagger Generator Based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>43--46</pages>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A General POS Tagger Generator Based on Support Vector Machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation, pages 43–46, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>North American News Text Corpus.</title>
<date>1995</date>
<booktitle>Linguistic Data Consortium, Cat. LDC95T21,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="12512" citStr="Graff, 1995" startWordPosition="2031" endWordPosition="2032">ails, see Table 1. data set tokens sentences train (0-18) 912,344 38,220 dev-test (19-21) 131,768 5,528 eval-test (22-24) 129,654 5,463 Table 1: English supervised data set — WSJ part of Penn Treebank 3 For Czech, we use the current standard Prague Dependency Treebank (PDT 2.0) data sets (Hajiˇc et al., 2006); for details, see Table 2. data set tokens sentences train 1,539,241 91,049 dev-test 201,651 11,880 eval-test 219,765 13,136 Table 2: Czech supervised data set — Prague Dependency Treebank 2.0 3.2 The “unsupervised” data For English, we have processed the North American News Text corpus (Graff, 1995) (without the 765 WSJ section) with the Stanford segmenter and tokenizer (Toutanova et al., 2003). For Czech, we have used the SYN2005 part of Czech National Corpus (CNC, 2005) (with the original segmentation and tokenization). 3.3 GEN(x): The morphological analyzers For English, we perform a very simple morphological analysis, which reduces the full PTB tagset to a small list of tags for each token on input. The resulting list is larger than such a list derived solely from the PTB/WSJ, but much smaller than a full list of tags found in the PTB/WSJ.11 The English morphological analyzer is thus</context>
</contexts>
<marker>Graff, 1995</marker>
<rawString>David Graff, 1995. North American News Text Corpus. Linguistic Data Consortium, Cat. LDC95T21, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Barbora Vidov´a-Hladk´a</author>
</authors>
<title>Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>483--490</pages>
<location>Montreal, Quebec, Canada.</location>
<marker>Hajiˇc, Vidov´a-Hladk´a, 1998</marker>
<rawString>Jan Hajiˇc and Barbora Vidov´a-Hladk´a. 1998. Tagging Inflective Languages: Prediction of Morphological Categories for a Rich, Structured Tagset. In Proceedings of the 17th international conference on Computational linguistics, pages 483–490. Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv´ı Karolinum,</title>
<date>2004</date>
<location>Prague.</location>
<marker>Hajiˇc, 2004</marker>
<rawString>Jan Hajiˇc. 2004. Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv´ı Karolinum, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie Mikulov´a</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank v2.0, CDROM, LDC Cat. No. LDC2006T01. Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<marker>Hajiˇc, Hajiˇcov´a, Panevov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, 2006</marker>
<rawString>Jan Hajiˇc, Eva Hajiˇcov´a, Jarmila Panevov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, and Marie Mikulov´a. 2006. Prague Dependency Treebank v2.0, CDROM, LDC Cat. No. LDC2006T01. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Krbec</author>
</authors>
<title>Language Modelling for Speech Recognition of Czech.</title>
<date>2005</date>
<booktitle>Ph.D. thesis, UK MFF, Prague, Malostransk´e n´amˇest´ı 25, 118 00</booktitle>
<location>Praha</location>
<contexts>
<context position="20031" citStr="Krbec, 2005" startWordPosition="3288" endWordPosition="3289">ers to tag the data in Step 1, and a third one to finish it up). For English, we use three state-of-the-art taggers: the taggers of (Toutanova et al., 2003) and (Shen et al., 2007) in Step 1, and the SVM tagger (Gim´enez and M`arquez, 2004) in Step 3. We run the taggers with the parameters which were shown to be the best in the corresponding papers. The SVM tagger needed to be adapted to accept the (reduced) list of possible tags.17 For Czech, we use the Feature-based tagger (Hajiˇc, 2004) and the Morˇce tagger (with the new feature set as described in section 4) in Step 1, and an HMM tagger (Krbec, 2005) in Step 3. This combination outperforms the results in (Spoustov´a et al., 2007) by a small margin. 5.2 Selection mechanism for the plain data We have found that it is better to feed the training with different chunks of the unsupervised data at each iteration. We have then experimented with 14This tagger (possibly different from any of the N taggers from Step 1) runs as usual, but it is given a minimal list of (at most N) tags that come from Step 2 only. 15”Accuracy” means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the </context>
</contexts>
<marker>Krbec, 2005</marker>
<rawString>Pavel Krbec. 2005. Language Modelling for Speech Recognition of Czech. Ph.D. thesis, UK MFF, Prague, Malostransk´e n´amˇest´ı 25, 118 00 Praha 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="11781" citStr="Marcus et al., 1994" startWordPosition="1910" endWordPosition="1913">y quickly retrained and thus many different feature sets and other training parameters, such as the number of iterations, feature thresholds etc. can be considered and tested. As a result of this tuning, our (fully supervised) version of the Morˇce tagger gives the best accuracy among all single taggers for Czech and also very good results for English, being beaten only by the tagger (Shen et al., 2007) (by 0.10 % absolute) and (not significantly) by (Toutanova et al., 2003). 3 The data 3.1 The “supervised” data For English, we use the same data division of Penn Treebank (PTB) parsed section (Marcus et al., 1994) as all of (Collins, 2002), (Toutanova et al., 2003), (Gim´enez and M`arquez, 2004) and (Shen et al., 2007) do; for details, see Table 1. data set tokens sentences train (0-18) 912,344 38,220 dev-test (19-21) 131,768 5,528 eval-test (22-24) 129,654 5,463 Table 1: English supervised data set — WSJ part of Penn Treebank 3 For Czech, we use the current standard Prague Dependency Treebank (PDT 2.0) data sets (Hajiˇc et al., 2006); for details, see Table 2. data set tokens sentences train 1,539,241 91,049 dev-test 201,651 11,880 eval-test 219,765 13,136 Table 2: Czech supervised data set — Prague D</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1994</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1994. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Development Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2008</date>
<journal>ISBN</journal>
<pages>3--900051</pages>
<location>Vienna,</location>
<contexts>
<context position="26363" citStr="Team, 2008" startWordPosition="4323" endWordPosition="4324">of tags derived from train 97.13 Our morphological analyzer 97.44 Table 8: Dependence on the GEN(x) 6 Results In Tables 10 and 11, the main results (on the evaltest data sets) are summarized. The state-of-the art taggers are using feature sets discribed in the corresponding articles ((Collins, 2002), (Gim´enez and M`arquez, 2004), (Toutanova et al., 2003) and (Shen et al., 2007)), Morˇce supervised and Morˇce semi-supervised are using feature set desribed in section 4. For significance tests, we have used the paired Wilcoxon signed rank test as implemented in the R package (R Development Core Team, 2008) Tagger accuracy Feature-based 94.04 % HMM 94.82 % Morˇce supervised 95.67 % combination 95.70 % Morˇce semi-supervised 95.89 % Table 11: Evaluation of the Czech taggers in wilcox.test(), dividing the data into 100 chunks (data pairs). 6.1 English The combination of the three existing English taggers seems to be best, but it is not significantly better than our semi-supervised approach. The combination is significantly better than (Shen et al., 2007) at a very high level, but more importantly, Shen’s results (currently representing the replicable state-of-the-art in POS tagging) have been sign</context>
</contexts>
<marker>Team, 2008</marker>
<rawString>R Development Core Team, 2008. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1st EMNLP,</booktitle>
<pages>133--142</pages>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="20791" citStr="Ratnaparkhi, 1996" startWordPosition="3414" endWordPosition="3415">a We have found that it is better to feed the training with different chunks of the unsupervised data at each iteration. We have then experimented with 14This tagger (possibly different from any of the N taggers from Step 1) runs as usual, but it is given a minimal list of (at most N) tags that come from Step 2 only. 15”Accuracy” means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself. 16In fact, we have experimented with other tagger combinations and configurations as well—with the TnT (Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTagger (Schmid, 1994), with or without the Morˇce tagger in the pack; see below for the winning combination. 17This patch is available on the paper’s website (see Section 7). 767 three methods of unsupervised data selection, i.e. generating the unsupervised data chunks for each training iteration from the ,,pool“ of sentences. These methods are: simple sequential chopping, randomized data selection with replacement and randomized selection without replacement. Table 5 demonstrates that there is practically no difference in the results. Thus, we use the sequential chopping mechanism, m</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the 1st EMNLP, pages 133–142, New Brunswick, NJ. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing, page 9pp.,</booktitle>
<location>Manchester, GB.</location>
<contexts>
<context position="20821" citStr="Schmid, 1994" startWordPosition="3419" endWordPosition="3420">o feed the training with different chunks of the unsupervised data at each iteration. We have then experimented with 14This tagger (possibly different from any of the N taggers from Step 1) runs as usual, but it is given a minimal list of (at most N) tags that come from Step 2 only. 15”Accuracy” means accuracy of the semi-supervised method using this tagger for pre-tagging the unsupervised data, not the accuracy of the tagger itself. 16In fact, we have experimented with other tagger combinations and configurations as well—with the TnT (Brants, 2000), MaxEnt (Ratnaparkhi, 1996) and TreeTagger (Schmid, 1994), with or without the Morˇce tagger in the pack; see below for the winning combination. 17This patch is available on the paper’s website (see Section 7). 767 three methods of unsupervised data selection, i.e. generating the unsupervised data chunks for each training iteration from the ,,pool“ of sentences. These methods are: simple sequential chopping, randomized data selection with replacement and randomized selection without replacement. Table 5 demonstrates that there is practically no difference in the results. Thus, we use the sequential chopping mechanism, mainly for its simplicity. Meth</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, page 9pp., Manchester, GB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind K Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>760--767</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1342" citStr="Shen et al., 2007" startWordPosition="188" endWordPosition="191">ion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %). 1 Introduction Since 2002, we have seen a renewed interest in improving POS tagging results for English, and an inflow of results (initial or improved) for many other languages. For English, after a relatively big jump achieved by (Collins, 2002), we have seen two significant improvements: (Toutanova et al., 2003) and (Shen et al., 2007) pushed the results by a significant amount each time.1 1In our final comparison, we have also included the results of (Gim´enez and M`arquez, 2004), because it has surpassed (Collins, 2002) as well and we have used this tagger in the data preparation phase. See more details below. Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance te</context>
<context position="11567" citStr="Shen et al., 2007" startWordPosition="1873" endWordPosition="1876">is very simple—only integer numbers (counts and their sums for the averaging) are updated for each feature at each sentence with imperfect match(es) found against the gold standard. Therefore, it can be relatively quickly retrained and thus many different feature sets and other training parameters, such as the number of iterations, feature thresholds etc. can be considered and tested. As a result of this tuning, our (fully supervised) version of the Morˇce tagger gives the best accuracy among all single taggers for Czech and also very good results for English, being beaten only by the tagger (Shen et al., 2007) (by 0.10 % absolute) and (not significantly) by (Toutanova et al., 2003). 3 The data 3.1 The “supervised” data For English, we use the same data division of Penn Treebank (PTB) parsed section (Marcus et al., 1994) as all of (Collins, 2002), (Toutanova et al., 2003), (Gim´enez and M`arquez, 2004) and (Shen et al., 2007) do; for details, see Table 1. data set tokens sentences train (0-18) 912,344 38,220 dev-test (19-21) 131,768 5,528 eval-test (22-24) 129,654 5,463 Table 1: English supervised data set — WSJ part of Penn Treebank 3 For Czech, we use the current standard Prague Dependency Treeban</context>
<context position="14517" citStr="Shen et al., 2007" startWordPosition="2362" endWordPosition="2365">fore each of them needs a different set of feature templates. We have empirically tested hundreds of feature templates on both languages, taken over from previous works for direct comparison, inspired by them, or based on a combination of previous experience, error analysis and linguistic intuition. In the following sections, we present the best performing set of feature templates as determined on the development data set using only the supervised training setting; our feature templates have thus not been influenced nor extended by the unsupervised data.13 11The full list of tags, as used by (Shen et al., 2007), also makes the underlying Viterbi algorithm unbearably slow. 12The English morphology tool is also downloadable as a separate module on the paper’s accompanying website. 13Another set of experiments has shown that there is not, perhaps surprisingly, a significant gain in doing so. 4.1 English feature templates The best feature set for English consists of 30 feature templates. All templates predict the current tag as a whole. A detailed description of the English feature templates can be found in Table 3. Context predicting whole tag Tags Previous tag Previous two tags First letter of previou</context>
<context position="19599" citStr="Shen et al., 2007" startWordPosition="3206" endWordPosition="3209">Table 4 illustrates why it is advantageous to go through this (still)16 complicated setup against a single-tagger bootstrapping mechanism, which always uses the same tagger for tagging the unsupervised data. For both English and Czech, the selection of taggers, the best combination and the best overall setup has been optimized on the development data set. A bit surprisingly, the final setup is very similar for both languages (two taggers to tag the data in Step 1, and a third one to finish it up). For English, we use three state-of-the-art taggers: the taggers of (Toutanova et al., 2003) and (Shen et al., 2007) in Step 1, and the SVM tagger (Gim´enez and M`arquez, 2004) in Step 3. We run the taggers with the parameters which were shown to be the best in the corresponding papers. The SVM tagger needed to be adapted to accept the (reduced) list of possible tags.17 For Czech, we use the Feature-based tagger (Hajiˇc, 2004) and the Morˇce tagger (with the new feature set as described in section 4) in Step 1, and an HMM tagger (Krbec, 2005) in Step 3. This combination outperforms the results in (Spoustov´a et al., 2007) by a small margin. 5.2 Selection mechanism for the plain data We have found that it is</context>
<context position="26133" citStr="Shen et al., 2007" startWordPosition="4285" endWordPosition="4288">cal analyzer output and using the full list of tags are nearly the same, see Table 9. Feature set Accuracy Collins’ 97.38 Our’s 97.44 Table 7: Dependence on the feature set used by the perceptron algorithm (English) GEN(x) Accuracy List of tags derived from train 97.13 Our morphological analyzer 97.44 Table 8: Dependence on the GEN(x) 6 Results In Tables 10 and 11, the main results (on the evaltest data sets) are summarized. The state-of-the art taggers are using feature sets discribed in the corresponding articles ((Collins, 2002), (Gim´enez and M`arquez, 2004), (Toutanova et al., 2003) and (Shen et al., 2007)), Morˇce supervised and Morˇce semi-supervised are using feature set desribed in section 4. For significance tests, we have used the paired Wilcoxon signed rank test as implemented in the R package (R Development Core Team, 2008) Tagger accuracy Feature-based 94.04 % HMM 94.82 % Morˇce supervised 95.67 % combination 95.70 % Morˇce semi-supervised 95.89 % Table 11: Evaluation of the Czech taggers in wilcox.test(), dividing the data into 100 chunks (data pairs). 6.1 English The combination of the three existing English taggers seems to be best, but it is not significantly better than our semi-s</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind K. Joshi. 2007. Guided Learning for Bidirectional Sequence Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahom´ıra ”johanka” Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kvˇetoˇn</author>
</authors>
<title>The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing</booktitle>
<pages>67--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Spoustov´a, Hajiˇc, Votrubec, Krbec, Kvˇetoˇn, 2007</marker>
<rawString>Drahom´ıra ”johanka” Spoustov´a, Jan Hajiˇc, Jan Votrubec, Pavel Krbec, and Pavel Kvˇetoˇn. 2007. The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing 2007, pages 67–74, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1666" citStr="Suzuki and Isozaki, 2008" startWordPosition="244" endWordPosition="247">n a renewed interest in improving POS tagging results for English, and an inflow of results (initial or improved) for many other languages. For English, after a relatively big jump achieved by (Collins, 2002), we have seen two significant improvements: (Toutanova et al., 2003) and (Shen et al., 2007) pushed the results by a significant amount each time.1 1In our final comparison, we have also included the results of (Gim´enez and M`arquez, 2004), because it has surpassed (Collins, 2002) as well and we have used this tagger in the data preparation phase. See more details below. Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given and the tool is not available for download, i.e. for repeating the results and significance testing. Thus, we compare our results only to the tools listed above. Even though an improvement in POS tagging might be a questionable enterprise (given that its effects on other tasks, such as parsing or other NLP problems are less than clear—at least for English), it is still an interesting problem. Moreover, the “ideal”2</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In Proceedings of ACL08: HLT, pages 665–673, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network. In</title>
<date>2003</date>
<booktitle>NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1318" citStr="Toutanova et al., 2003" startWordPosition="183" endWordPosition="186">) data in a bagging-like fashion showed significant improvement of the POS classification task on typologically different languages, yielding better than state-of-the-art results for English and Czech (4.12 % and 4.86 % relative error reduction, respectively; absolute accuracies being 97.44 % and 95.89 %). 1 Introduction Since 2002, we have seen a renewed interest in improving POS tagging results for English, and an inflow of results (initial or improved) for many other languages. For English, after a relatively big jump achieved by (Collins, 2002), we have seen two significant improvements: (Toutanova et al., 2003) and (Shen et al., 2007) pushed the results by a significant amount each time.1 1In our final comparison, we have also included the results of (Gim´enez and M`arquez, 2004), because it has surpassed (Collins, 2002) as well and we have used this tagger in the data preparation phase. See more details below. Most recently, (Suzuki and Isozaki, 2008) published their Semi-supervised sequential labelling method, whose results on POS tagging seem to be optically better than (Shen et al., 2007), but no significance tests were given and the tool is not available for download, i.e. for repeating the res</context>
<context position="4782" citStr="Toutanova et al., 2003" startWordPosition="741" endWordPosition="744"> PTB/WSJ (English) ing the training phase. Apart from feeding the perceptron by various mixtures of manually tagged (“supervised”) and auto-tagged (“unsupervised”)4 data, we have also used various feature templates extensively; for example, we use lexicalization (with the added twist of lemmatization, useful especially for Czech, an inflectionally rich language), “manual” tag classification into large classes (again, useful especially for Czech to avoid the huge, still-to-beovercome data sparseness for such a language5), and sub-lexical features mainly targeted at OOV words. Inspired i.a. by (Toutanova et al., 2003) and (Hajiˇc and Vidov´a-Hladk´a, 1998), we also use “lookahead” features (however, we still remain in the left-to-right HMM world – in this respect our solution is closer to the older work of (Hajiˇc and Vidov´a-Hladk´a, 1998) than to (Toutanova et al., 2003), who uses bidirectional dependencies to include the right-hand side disambiguated tags, 4For brevity, we will use the terms “supervised” and “unsupervised” data for “manually annotated” and “(automatically annotated) plain (raw) text” data, respectively, even though these adjectives are meant to describe the process of learning, not the </context>
<context position="11640" citStr="Toutanova et al., 2003" startWordPosition="1885" endWordPosition="1888">veraging) are updated for each feature at each sentence with imperfect match(es) found against the gold standard. Therefore, it can be relatively quickly retrained and thus many different feature sets and other training parameters, such as the number of iterations, feature thresholds etc. can be considered and tested. As a result of this tuning, our (fully supervised) version of the Morˇce tagger gives the best accuracy among all single taggers for Czech and also very good results for English, being beaten only by the tagger (Shen et al., 2007) (by 0.10 % absolute) and (not significantly) by (Toutanova et al., 2003). 3 The data 3.1 The “supervised” data For English, we use the same data division of Penn Treebank (PTB) parsed section (Marcus et al., 1994) as all of (Collins, 2002), (Toutanova et al., 2003), (Gim´enez and M`arquez, 2004) and (Shen et al., 2007) do; for details, see Table 1. data set tokens sentences train (0-18) 912,344 38,220 dev-test (19-21) 131,768 5,528 eval-test (22-24) 129,654 5,463 Table 1: English supervised data set — WSJ part of Penn Treebank 3 For Czech, we use the current standard Prague Dependency Treebank (PDT 2.0) data sets (Hajiˇc et al., 2006); for details, see Table 2. da</context>
<context position="19575" citStr="Toutanova et al., 2003" startWordPosition="3201" endWordPosition="3204"> plain text data (English)16 Table 4 illustrates why it is advantageous to go through this (still)16 complicated setup against a single-tagger bootstrapping mechanism, which always uses the same tagger for tagging the unsupervised data. For both English and Czech, the selection of taggers, the best combination and the best overall setup has been optimized on the development data set. A bit surprisingly, the final setup is very similar for both languages (two taggers to tag the data in Step 1, and a third one to finish it up). For English, we use three state-of-the-art taggers: the taggers of (Toutanova et al., 2003) and (Shen et al., 2007) in Step 1, and the SVM tagger (Gim´enez and M`arquez, 2004) in Step 3. We run the taggers with the parameters which were shown to be the best in the corresponding papers. The SVM tagger needed to be adapted to accept the (reduced) list of possible tags.17 For Czech, we use the Feature-based tagger (Hajiˇc, 2004) and the Morˇce tagger (with the new feature set as described in section 4) in Step 1, and an HMM tagger (Krbec, 2005) in Step 3. This combination outperforms the results in (Spoustov´a et al., 2007) by a small margin. 5.2 Selection mechanism for the plain data </context>
<context position="26109" citStr="Toutanova et al., 2003" startWordPosition="4280" endWordPosition="4283"> taggers using the morphological analyzer output and using the full list of tags are nearly the same, see Table 9. Feature set Accuracy Collins’ 97.38 Our’s 97.44 Table 7: Dependence on the feature set used by the perceptron algorithm (English) GEN(x) Accuracy List of tags derived from train 97.13 Our morphological analyzer 97.44 Table 8: Dependence on the GEN(x) 6 Results In Tables 10 and 11, the main results (on the evaltest data sets) are summarized. The state-of-the art taggers are using feature sets discribed in the corresponding articles ((Collins, 2002), (Gim´enez and M`arquez, 2004), (Toutanova et al., 2003) and (Shen et al., 2007)), Morˇce supervised and Morˇce semi-supervised are using feature set desribed in section 4. For significance tests, we have used the paired Wilcoxon signed rank test as implemented in the R package (R Development Core Team, 2008) Tagger accuracy Feature-based 94.04 % HMM 94.82 % Morˇce supervised 95.67 % combination 95.70 % Morˇce semi-supervised 95.89 % Table 11: Evaluation of the Czech taggers in wilcox.test(), dividing the data into 100 chunks (data pairs). 6.1 English The combination of the three existing English taggers seems to be best, but it is not significantl</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-Rich Partof-Speech Tagging with a Cyclic Dependency Network. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
</authors>
<title>Improving accuracy in word class tagging through the combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<pages>229</pages>
<marker>van Halteren, Daelemans, Zavrel, 2001</marker>
<rawString>Hans van Halteren, Walter Daelemans, and Jakub Zavrel. 2001. Improving accuracy in word class tagging through the combination of machine learning systems. Computational Linguistics, 27(2):199– 229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Votrubec</author>
</authors>
<title>Morphological Tagging Based on Averaged Perceptron.</title>
<date>2006</date>
<booktitle>In WDS’06 Proceedings of Contributed Papers,</booktitle>
<pages>191--195</pages>
<location>Prague, Czech Republic. Matfyzpress, Charles University.</location>
<contexts>
<context position="5998" citStr="Votrubec, 2006" startWordPosition="944" endWordPosition="945">ata themselves. 5As (Hajiˇc, 2004) writes, Czech has 4400 plausible tags, of which we have observed almost 2000 in the 100M corpus we have used in our experiments. However, only 1100 of them have been found in the manually annotated PDT 2.0 corpus (the corpus on which we have based the supervised experiments). The situation with word forms (tokens) is even worse: Czech has about 20M different word forms, and the OOV rate based on the 1.5M PDT 2.0 data and measured against the 100M raw corpus is almost 10 %. which we cannot.) To summarize, we can describe our system as follows: it is based on (Votrubec, 2006)’s implementation of (Collins, 2002), which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part: precisely, by a concatenation of the manually tagged training data (WSJ portion of the PTB 3 for English, morphologically disambiguated data from PDT 2.0 for Czech) and a chunk of automatically tagged unsupervised data. The “parameters” of the training process (feature templates, the size of the unsupervised chunks added to the trainer at each iteration, number of iterations, the combination of taggers that should be used in the auto-tagging of t</context>
<context position="7499" citStr="Votrubec, 2006" startWordPosition="1184" endWordPosition="1185">) features from the auto-tagged data did not give significant accuracy improvements (and only made the training very slow). The final taggers have surpassed the current state-of-the-art taggers by significant margins (we have achieved 4.12 % relative error reduction for English and 4.86 % for Czech over the best previously published results, single or combined), using a single tagger. However, the best English tagger combining some of the previous stateof-the-art ones is still “optically” better (yet not significantly—see Section 6). 2 The perceptron algorithm We have used the Morˇce6 tagger (Votrubec, 2006) as a main component in our experiments. It is a reimplementation of the averaged perceptron described in (Collins, 2002), which uses such features that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible. Collins’ GEN(x) set (a set of possible tags at any given position) is generated, in our case, using a morphological analyzer for the given language (essen6The name “Morˇce” stands for “MORfologie ˇCEˇstiny” (“Czech morphology”, see (Votrubec, 2006)), since it has been originally developed for Czech. We keep this name in this paper as the generic name of the avera</context>
<context position="16335" citStr="Votrubec, 2006" startWordPosition="2664" endWordPosition="2665">edicts only some parts of the current tag separately (e.g., detailed POS, gender, case) to avoid data sparseness. Such a feature is true, in an identical context, for several different tags belonging to the same class (e.g., sharing a locative case). The individual grammatical categories used for such classing have been chosen on both linguistic grounds (POS, detailed finegrained POS) and also such categories have been used which contribute most to the elimination of the tagger errors (based on an extensive error analysis of previous results, the detailed description of which can be found in (Votrubec, 2006)). Several features can look ahead (to the right of the current position) - apart from the obvious word form, which is unambiguous, we have used (in case of ambiguity) a random tag and lemma of the first position to the right from the current position which might be occupied with a verb (based on dictionary and the associated morphological guesser restrictions). A total of 8,440,467 features has been extracted from the supervised training data set. A detailed description is included in the distribution downloadable from the Morˇce website. 766 5 The (un)supervised training setup We have extend</context>
</contexts>
<marker>Votrubec, 2006</marker>
<rawString>Jan Votrubec. 2006. Morphological Tagging Based on Averaged Perceptron. In WDS’06 Proceedings of Contributed Papers, pages 191–195, Prague, Czech Republic. Matfyzpress, Charles University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>