<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000206">
<title confidence="0.999282">
A New Feature Selection Score for Multinomial Naive
Bayes Text Classification Based on KL-Divergence
</title>
<author confidence="0.991655">
Karl-Michael Schneider
</author>
<affiliation confidence="0.826955666666667">
Department of General Linguistics
University of Passau
94032 Passau, Germany
</affiliation>
<email confidence="0.984282">
schneide@phil.uni-passau.de
</email>
<sectionHeader confidence="0.982433" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981333333333">
We define a new feature selection score for text
classification based on the KL-divergence between
the distribution of words in training documents and
their classes. The score favors words that have a
similar distribution in documents of the same class
but different distributions in documents of different
classes. Experiments on two standard data sets in-
dicate that the new method outperforms mutual in-
formation, especially for smaller categories.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999728380952381">
Text classification is the assignment of predefined
categories to text documents. Text classification
has many applications in natural language process-
ing tasks such as E-mail filtering, prediction of user
preferences and organization of web content.
The Naive Bayes classifier is a popular machine
learning technique for text classification because it
performs well in many domains, despite its simplic-
ity (Domingos and Pazzani, 1997). Naive Bayes as-
sumes a stochastic model of document generation.
Using Bayes’ rule, the model is inverted in order to
predict the most likely class for a new document.
We assume that documents are generated accord-
ing to a multinomial event model (McCallum and
Nigam, 1998). Thus a document is represented as
a vector di = (xi1 ... xi|V |) of word counts where
V is the vocabulary and each xit  {0, 1, 2,... } in-
dicates how often wt occurs in di. Given model pa-
rameters p(wt|cj) and class prior probabilities p(cj)
and assuming independence of the words, the most
likely class for a document di is computed as
</bodyText>
<equation confidence="0.9944335">
c*(di) = argmax p(cj)p(d|cj)
j
(1)
p(wt|cj)n(wt,di)
</equation>
<bodyText confidence="0.999746333333333">
where n(wt, di) is the number of occurrences of wt
in di. p(wt|cj) and p(cj) are estimated from train-
ing documents with known classes, using maximum
</bodyText>
<equation confidence="0.984195857142857">
likelihood estimation with a Laplacean prior:
1 + Edicc, n(wt,di) p(wt|cj) = |V  |&apos; (2)
|V  |+ Et=1 Ediccj n(wt, di)
p(cj) = |Icj|
(3)
ECI
j�=1 |cj�|
</equation>
<bodyText confidence="0.987820095238096">
It is common practice to use only a subset of
the words in the training documents for classifi-
cation to avoid overfitting and make classification
more efficient. This is usually done by assigning
each word a score f(wt) that measures its useful-
ness for classification and selecting the N highest
scored words. One of the best performing scoring
functions for feature selection in text classification
is mutual information (Yang and Pedersen, 1997).
The mutual information between two random vari-
ables, MI(X; Y ), measures the amount of informa-
tion that the value of one variable gives about the
value of the other (Cover and Thomas, 1991).
Note that in the multinomial model, the word
variable W takes on values from the vocabulary V .
In order to use mutual information with a multi-
nomial model, one defines new random variables
Wt  {0, 1} with p(Wt = 1) = p(W = wt) (Mc-
Callum and Nigam, 1998; Rennie, 2001). Then the
mutual information between a word wt and the class
variable C is
</bodyText>
<equation confidence="0.954466">
L p(x, cj)log p(x, cj)
p(x)p(cj)
(4)
</equation>
<bodyText confidence="0.999913">
where p(x, cj) and p(x) are short for p(Wt = x, cj)
and p(Wt = x). p(x,cj), p(x) and p(cj) are esti-
mated from the training documents by counting how
often wt occurs in each class.
</bodyText>
<sectionHeader confidence="0.74977" genericHeader="method">
2 Naive Bayes and KL-Divergence
</sectionHeader>
<bodyText confidence="0.999739666666667">
There is a strong connection between Naive Bayes
and KL-divergence (Kullback-Leibler divergence,
relative entropy). KL-divergence measures how
</bodyText>
<equation confidence="0.9949974">
= argmax
j
|V |
H
t=1
p(cj)
|C|
MI(Wt; C) = L
j=1
x=0,1
</equation>
<bodyText confidence="0.998484277777778">
much one probability distribution is different from
another (Cover and Thomas, 1991). It is defined (for
discrete distributions) by
a higher score. By removing words with a lower
score from the vocabulary, the training documents
of each class become more similar to each other,
and therefore, also to the class, in terms of word dis-
tribution. This leads to more homogeneous classes.
Assuming that the test documents and training doc-
uments come from the same distribution, the simi-
larity between the test documents and their respec-
tive classes will be increased as well, thus resulting
in higher classification accuracy.
We now make this more precise. Let S =
{d1, ... , d|S|} be the set of training documents, and
denote the class of di with c(di). The average KL-
divergence for a word wt between the training doc-
uments and their classes is given by
</bodyText>
<equation confidence="0.9488896">

KL(p, q) =
x
p(x) log p(x) (5)
q(x)
</equation>
<bodyText confidence="0.9997615">
By viewing a document as a probability distribu-
tion over words, Naive Bayes can be interpreted in
an information-theoretic framework (Dhillon et al.,
2002). Let p(wt|d) = n(wt,d)/|d|. Taking loga-
rithms and dividing by the length of d, (1) can be
rewritten as
</bodyText>
<equation confidence="0.9887651875">
c*(d)
= argmax log p(cj) + |V  |n(wt, d) log p(wt|cj)
j 
t=1
= argmax 1  |V  |p(wt|d) log p(wt|cj)
j |d |log p(cj) + t=1 (6)
1 
KLt(S) = KL(p(wt|di),p(wt|c(di))).
|S|
Adding the entropy of p(W |d), we get
c*(d)
p(wt|d) log p(wt|d)
p(wt|cj)
1
KL(p(W|d),p(W |cj)) − |d |log p(cj)
(7)
</equation>
<bodyText confidence="0.9999208">
This means that Naive Bayes assigns to a document
d the class which is “most similar” to d in terms
of the distribution of words. Note also that the
prior probabilities are usually dominated by docu-
ment probabilities except for very short documents.
</bodyText>
<sectionHeader confidence="0.981212" genericHeader="method">
3 Feature Selection using KL-Divergence
</sectionHeader>
<bodyText confidence="0.965748392857143">
We define a new scoring function for feature selec-
tion based on the following considerations. In the
previous section we have seen that Naive Bayes as-
signs a document d the class c* such that the “dis-
tance” between d and c* is minimized. A classifi-
cation error occurs when a test document is closer
to some other class than to its true class, in terms of
KL-divergence.
We seek to define a scoring function such that
words whose distribution in the individual training
documents of a class is much different from the dis-
tribution in the class (according to (2)) receive a
lower score, while words with a similar distribution
in all training documents of the same class receive
dzES
(8)
One problem with (8) is that in addition to the con-
ditional probabilities p(wt|cj) for each word and
each class, the computation considers each individ-
ual document, thus resulting in a time requirement
of O(|S|).1 In order to avoid this additional com-
plexity, instead of KLt(S) we use an approxima-
tion KLt(S), which is based on the following two
assumptions: (i) the number of occurrences of wt
is the same in all documents that contain wt, (ii)
all documents in the same class cj have the same
length. Let Njt be the number of documents in cj
that contain wt, and let
</bodyText>
<equation confidence="0.9695825">
˜pd(wt|cj) = p(wt|cj) |cj |(9)
Njt
</equation>
<bodyText confidence="0.899756666666667">
be the average probability of wt in those documents
in cj that contain wt (if wt does not occur in cj, set
˜pd(wt|cj) = 0). Then KLt(S) reduces to
</bodyText>
<equation confidence="0.877823375">
1 |C |pd (wt  |cj )
KLt(S) = |S |Njtpd (wt |cj)log p(wt|cj) j=1
(10)
Plugging in (9) and (3) and defining q(wt|cj) =
Njt/|cj|, we get
|C|
KLt(S) = −  p(cj)p(wt|cj)log q(wt|cj). (11)
j=1
</equation>
<bodyText confidence="0.997255166666667">
Note that computing KLt(S) only requires a statis-
tics of the number of words and documents for each
&apos;Note that KLt(S) cannot be computed simultaneously with
p(wticj) in one pass over the documents in (2): KLt(S) re-
quires p(wt|cj) when each document is considered, but com-
puting the latter needs iterating over all documents itself.
</bodyText>
<figure confidence="0.946787461538461">
= argmax 1  |V |
j |d |log p(cj) − t=1
= argmin
j
Classification Accuracy
0.8
0.6
0.4
0.2
1
dKL
MI
KL
</figure>
<bodyText confidence="0.999320363636364">
class, not per document. Thus KLt(S) can be com-
puted in O(|C|). Typically, |C |is much smaller
than |S|.
Another important thing to note is the following.
By removing words with an uneven distribution in
the documents of the same class, not only the doc-
uments in the class, but also the classes themselves
may become more similar, which reduces the ability
to distinguish between different classes. Let p(wt)
be the number of occurrences of wt in all training
documents, divided by the total number of words,
</bodyText>
<equation confidence="0.99924475">
q(wt) = �|C|
j=1 Njt/|S |and define
010 100 1000 10000 100000
�Kt(S) = −p(wt) log q(wt). (12)
</equation>
<bodyText confidence="0.988445285714286">
�Kt(S) can be interpreted as an approximation of the
average divergence of the distribution of wt in the
individual training documents from the global dis-
tribution (averaged over all training documents in
all classes). If wt is independent of the class, then
�Kt(S) = �KLt(S). The difference between the two
is a measure of the increase in homogeneity of the
training documents, in terms of the distribution of
wt, when the documents are clustered in their true
classes. It is large if the distribution of wt is similar
in the training documents of the same class but dis-
similar in documents of different classes. In analogy
to mutual information, we define our new scoring
function as the difference
</bodyText>
<equation confidence="0.8422565">
KL(wt) = �Kt(S) − �KLt(S). (13)
We also use a variant of KL, denoted dKL, where
p(wt) is estimated according to (14):
|C|
p�(wt) = L p(cj)p(wt|cj) (14)
j=1
</equation>
<bodyText confidence="0.89214">
and p(wt|cj) is estimated as in (2).
</bodyText>
<sectionHeader confidence="0.99763" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999979875">
We compare KL and dKL to mutual information,
using two standard data sets: 20 Newsgroups2 and
Reuters 21578.3 In tokenizing the data, only words
consisting of alphabetic characters are used after
conversion to lower case. In addition, all numbers
are mapped to a special token NUM. For 20 News-
groups we remove the newsgroup headers and use a
stoplist consisting of the 100 most frequent words of
</bodyText>
<footnote confidence="0.7741655">
2http://www.ai.mit.edu/—jrennie/20Newsgroups/
3http://www.daviddlewis.com/resources/testcollections/
reuters21578/
Vocabulary Size
</footnote>
<figureCaption confidence="0.882085">
Figure 1: Classification accuracy for 20 News-
groups. The curves have small error bars.
</figureCaption>
<bodyText confidence="0.999957147058823">
the British National Corpus.4 We use the ModApte
split of Reuters 21578 (Apt´e et al., 1994) and use
only the 10 largest classes. The vocabulary size is
111868 words for 20 Newsgroups and 22430 words
for Reuters.
Experiments with 20 Newsgroups are performed
with 5-fold cross-validation, using 80% of the data
for training and 20% for testing. We build a sin-
gle classifier for the 20 classes and vary the num-
ber of selected words from 20 to 20000. Figure 1
compares classification accuracy for the three scor-
ing functions. dKL slightly outperforms mutual in-
formation, especially for smaller vocabulary sizes.
The difference is statistically significant for 20 to
200 words at the 99% confidence level, and for 20
to 2000 words at the 95% confidence level, using a
one-tailed paired t-test.
For the Reuters dataset we build a binary classi-
fier for each of the ten topics and set the number of
positively classified documents such that precision
equals recall. Precision is the percentage of posi-
tive documents among all positively classified doc-
uments. Recall is the percentage of positive docu-
ments that are classified as positive.
In Figures 2 and 3 we report microaveraged and
macroaveraged recall for each number of selected
words. Microaveraged recall is the percentage of all
positive documents (in all topics) that are classified
as positive. Macroaveraged recall is the average of
the recall values of the individual topics. Microav-
eraged recall gives equal weight to the documents
and thus emphasizes larger topics, while macroav-
eraged recall gives equal weight to the topics and
thus emphasizes smaller topics more than microav-
</bodyText>
<figure confidence="0.3747465">
4http://www.itri.brighton.ac.uk/—Adam.Kilgarriff/bnc-
readme.html
</figure>
<figureCaption confidence="0.987156">
Figure 2: Microaveraged recall on Reuters at break-
even point.
</figureCaption>
<figure confidence="0.898692">
Vocabulary Size
</figure>
<figureCaption confidence="0.9898415">
Figure 3: Macroaveraged recall on Reuters at break-
even point.
</figureCaption>
<bodyText confidence="0.99203165">
eraged recall.
Both KL and dKL achieve slightly higher values
for microaveraged recall than mutual information,
for most vocabulary sizes (Fig. 2). KL performs best
at 20000 words with 90.1% microaveraged recall,
compared to 89.3% for mutual information. The
largest improvement is found for dKL at 100 words
with 88.0%, compared to 86.5% for mutual infor-
mation.
For smaller categories, the difference between
the KL-divergence based scores and mutual infor-
mation is larger, as indicated by the curves for
macroaveraged recall (Fig. 3). KL yields the high-
est recall at 20000 words with 82.2%, an increase of
3.9% compared to mutual information with 78.3%,
whereas dKL has its largest value at 100 words with
78.8%, compared to 76.1% for mutual information.
We find the largest improvement at 5000 words with
5.6% for KL and 2.9% for dKL, compared to mutual
information.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999368">
By interpreting Naive Bayes in an information the-
oretic framework, we derive a new scoring method
for feature selection in text classification, based on
the KL-divergence between training documents and
their classes. Our experiments show that it out-
performs mutual information, which was one of
the best performing methods in previous studies
(Yang and Pedersen, 1997). The KL-divergence
based scores are especially effective for smaller cat-
egories, but additional experiments are certainly re-
quired.
In order to keep the computational cost low,
we use an approximation instead of the exact KL-
divergence. Assessing the error introduced by this
approximation is a topic for future work.
</bodyText>
<sectionHeader confidence="0.989448" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99256071875">
Chidanand Apt´e, Fred Damerau, and Sholom M.
Weiss. 1994. Towards language independent au-
tomated learning of text categorization models.
In Proc. 17th ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR ’94), pages 23–30.
Thomas M. Cover and Joy A. Thomas. 1991. El-
ements of Information Theory. John Wiley, New
York.
Inderjit S. Dhillon, Subramanyam Mallela, and Ra-
hul Kumar. 2002. Enhanced word clustering for
hierarchical text classification. In Proc. 8th ACM
SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 191–
200.
Pedro Domingos and Michael Pazzani. 1997. On
the optimality of the simple bayesian classifier
under zero-one loss. Machine Learning, 29:103–
130.
Andrew McCallum and Kamal Nigam. 1998. A
comparison of event models for Naive Bayes text
classification. In Learning for Text Categoriza-
tion: Papers from the AAAI Workshop, pages 41–
48. AAAI Press. Technical Report WS-98-05.
Jason D. M. Rennie. 2001. Improving multi-class
text classification with Naive Bayes. Master’s
thesis, Massachusetts Institute of Technology.
Yiming Yang and Jan O. Pedersen. 1997. A com-
parative study on feature selection in text catego-
rization. In Proc. 14th International Conference
on Machine Learning (ICML-97), pages 412–
420.
</reference>
<figure confidence="0.999171612903226">
0.710
100
1000
Vocabulary Size
10000
100000
Precision/Recall Breakeven Point
0.95
0.85
0.75
0.9
0.8
1
MI
KL
dKL
Precision/Recall Breakeven Point
0.95
0.85
0.75
0.9
0.8
0.710
1
100
1000
10000
dKL
KL
MI
100000
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.557362">
<title confidence="0.803529">A New Feature Selection Score for Multinomial Naive Bayes Text Classification Based on KL-Divergence</title>
<author confidence="0.999176">Karl-Michael Schneider</author>
<affiliation confidence="0.999634">Department of General Linguistics University of Passau</affiliation>
<address confidence="0.999977">94032 Passau, Germany</address>
<email confidence="0.998904">schneide@phil.uni-passau.de</email>
<abstract confidence="0.9919264">We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes. The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes. Experiments on two standard data sets indicate that the new method outperforms mutual information, especially for smaller categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chidanand Apt´e</author>
<author>Fred Damerau</author>
<author>Sholom M Weiss</author>
</authors>
<title>Towards language independent automated learning of text categorization models.</title>
<date>1994</date>
<booktitle>In Proc. 17th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’94),</booktitle>
<pages>23--30</pages>
<marker>Apt´e, Damerau, Weiss, 1994</marker>
<rawString>Chidanand Apt´e, Fred Damerau, and Sholom M. Weiss. 1994. Towards language independent automated learning of text categorization models. In Proc. 17th ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’94), pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="2735" citStr="Cover and Thomas, 1991" startWordPosition="435" endWordPosition="438">actice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient. This is usually done by assigning each word a score f(wt) that measures its usefulness for classification and selecting the N highest scored words. One of the best performing scoring functions for feature selection in text classification is mutual information (Yang and Pedersen, 1997). The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other (Cover and Thomas, 1991). Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables Wt  {0, 1} with p(Wt = 1) = p(W = wt) (McCallum and Nigam, 1998; Rennie, 2001). Then the mutual information between a word wt and the class variable C is L p(x, cj)log p(x, cj) p(x)p(cj) (4) where p(x, cj) and p(x) are short for p(Wt = x, cj) and p(Wt = x). p(x,cj), p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class. 2 Naive Bayes and KL-Divergence Ther</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Subramanyam Mallela</author>
<author>Rahul Kumar</author>
</authors>
<title>Enhanced word clustering for hierarchical text classification.</title>
<date>2002</date>
<booktitle>In Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>191--200</pages>
<contexts>
<context position="4572" citStr="Dhillon et al., 2002" startWordPosition="761" endWordPosition="764">raining documents come from the same distribution, the similarity between the test documents and their respective classes will be increased as well, thus resulting in higher classification accuracy. We now make this more precise. Let S = {d1, ... , d|S|} be the set of training documents, and denote the class of di with c(di). The average KLdivergence for a word wt between the training documents and their classes is given by  KL(p, q) = x p(x) log p(x) (5) q(x) By viewing a document as a probability distribution over words, Naive Bayes can be interpreted in an information-theoretic framework (Dhillon et al., 2002). Let p(wt|d) = n(wt,d)/|d|. Taking logarithms and dividing by the length of d, (1) can be rewritten as c*(d) = argmax log p(cj) + |V |n(wt, d) log p(wt|cj) j  t=1 = argmax 1  |V |p(wt|d) log p(wt|cj) j |d |log p(cj) + t=1 (6) 1  KLt(S) = KL(p(wt|di),p(wt|c(di))). |S| Adding the entropy of p(W |d), we get c*(d) p(wt|d) log p(wt|d) p(wt|cj) 1 KL(p(W|d),p(W |cj)) − |d |log p(cj) (7) This means that Naive Bayes assigns to a document d the class which is “most similar” to d in terms of the distribution of words. Note also that the prior probabilities are usually dominated by document probabilit</context>
</contexts>
<marker>Dhillon, Mallela, Kumar, 2002</marker>
<rawString>Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. 2002. Enhanced word clustering for hierarchical text classification. In Proc. 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 191– 200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Michael Pazzani</author>
</authors>
<title>On the optimality of the simple bayesian classifier under zero-one loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--103</pages>
<contexts>
<context position="1138" citStr="Domingos and Pazzani, 1997" startWordPosition="157" endWordPosition="160">butions in documents of different classes. Experiments on two standard data sets indicate that the new method outperforms mutual information, especially for smaller categories. 1 Introduction Text classification is the assignment of predefined categories to text documents. Text classification has many applications in natural language processing tasks such as E-mail filtering, prediction of user preferences and organization of web content. The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains, despite its simplicity (Domingos and Pazzani, 1997). Naive Bayes assumes a stochastic model of document generation. Using Bayes’ rule, the model is inverted in order to predict the most likely class for a new document. We assume that documents are generated according to a multinomial event model (McCallum and Nigam, 1998). Thus a document is represented as a vector di = (xi1 ... xi|V |) of word counts where V is the vocabulary and each xit  {0, 1, 2,... } indicates how often wt occurs in di. Given model parameters p(wt|cj) and class prior probabilities p(cj) and assuming independence of the words, the most likely class for a document di is co</context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>Pedro Domingos and Michael Pazzani. 1997. On the optimality of the simple bayesian classifier under zero-one loss. Machine Learning, 29:103– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text classification.</title>
<date>1998</date>
<booktitle>In Learning for Text Categorization: Papers from the AAAI Workshop,</booktitle>
<tech>Technical Report WS-98-05.</tech>
<pages>41--48</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1410" citStr="McCallum and Nigam, 1998" startWordPosition="203" endWordPosition="206">Text classification has many applications in natural language processing tasks such as E-mail filtering, prediction of user preferences and organization of web content. The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains, despite its simplicity (Domingos and Pazzani, 1997). Naive Bayes assumes a stochastic model of document generation. Using Bayes’ rule, the model is inverted in order to predict the most likely class for a new document. We assume that documents are generated according to a multinomial event model (McCallum and Nigam, 1998). Thus a document is represented as a vector di = (xi1 ... xi|V |) of word counts where V is the vocabulary and each xit  {0, 1, 2,... } indicates how often wt occurs in di. Given model parameters p(wt|cj) and class prior probabilities p(cj) and assuming independence of the words, the most likely class for a document di is computed as c*(di) = argmax p(cj)p(d|cj) j (1) p(wt|cj)n(wt,di) where n(wt, di) is the number of occurrences of wt in di. p(wt|cj) and p(cj) are estimated from training documents with known classes, using maximum likelihood estimation with a Laplacean prior: 1 + Edicc, n(wt</context>
<context position="2991" citStr="McCallum and Nigam, 1998" startWordPosition="485" endWordPosition="489">d selecting the N highest scored words. One of the best performing scoring functions for feature selection in text classification is mutual information (Yang and Pedersen, 1997). The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other (Cover and Thomas, 1991). Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables Wt  {0, 1} with p(Wt = 1) = p(W = wt) (McCallum and Nigam, 1998; Rennie, 2001). Then the mutual information between a word wt and the class variable C is L p(x, cj)log p(x, cj) p(x)p(cj) (4) where p(x, cj) and p(x) are short for p(Wt = x, cj) and p(Wt = x). p(x,cj), p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class. 2 Naive Bayes and KL-Divergence There is a strong connection between Naive Bayes and KL-divergence (Kullback-Leibler divergence, relative entropy). KL-divergence measures how = argmax j |V | H t=1 p(cj) |C| MI(Wt; C) = L j=1 x=0,1 much one probability distribution is different from another (</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. A comparison of event models for Naive Bayes text classification. In Learning for Text Categorization: Papers from the AAAI Workshop, pages 41– 48. AAAI Press. Technical Report WS-98-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D M Rennie</author>
</authors>
<title>Improving multi-class text classification with Naive Bayes. Master’s thesis,</title>
<date>2001</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="3006" citStr="Rennie, 2001" startWordPosition="490" endWordPosition="491">scored words. One of the best performing scoring functions for feature selection in text classification is mutual information (Yang and Pedersen, 1997). The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other (Cover and Thomas, 1991). Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables Wt  {0, 1} with p(Wt = 1) = p(W = wt) (McCallum and Nigam, 1998; Rennie, 2001). Then the mutual information between a word wt and the class variable C is L p(x, cj)log p(x, cj) p(x)p(cj) (4) where p(x, cj) and p(x) are short for p(Wt = x, cj) and p(Wt = x). p(x,cj), p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class. 2 Naive Bayes and KL-Divergence There is a strong connection between Naive Bayes and KL-divergence (Kullback-Leibler divergence, relative entropy). KL-divergence measures how = argmax j |V | H t=1 p(cj) |C| MI(Wt; C) = L j=1 x=0,1 much one probability distribution is different from another (Cover and Thoma</context>
</contexts>
<marker>Rennie, 2001</marker>
<rawString>Jason D. M. Rennie. 2001. Improving multi-class text classification with Naive Bayes. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Jan O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proc. 14th International Conference on Machine Learning (ICML-97),</booktitle>
<pages>412--420</pages>
<contexts>
<context position="2544" citStr="Yang and Pedersen, 1997" startWordPosition="401" endWordPosition="404">known classes, using maximum likelihood estimation with a Laplacean prior: 1 + Edicc, n(wt,di) p(wt|cj) = |V |&apos; (2) |V |+ Et=1 Ediccj n(wt, di) p(cj) = |Icj| (3) ECI j�=1 |cj�| It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient. This is usually done by assigning each word a score f(wt) that measures its usefulness for classification and selecting the N highest scored words. One of the best performing scoring functions for feature selection in text classification is mutual information (Yang and Pedersen, 1997). The mutual information between two random variables, MI(X; Y ), measures the amount of information that the value of one variable gives about the value of the other (Cover and Thomas, 1991). Note that in the multinomial model, the word variable W takes on values from the vocabulary V . In order to use mutual information with a multinomial model, one defines new random variables Wt  {0, 1} with p(Wt = 1) = p(W = wt) (McCallum and Nigam, 1998; Rennie, 2001). Then the mutual information between a word wt and the class variable C is L p(x, cj)log p(x, cj) p(x)p(cj) (4) where p(x, cj) and p(x) a</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yiming Yang and Jan O. Pedersen. 1997. A comparative study on feature selection in text categorization. In Proc. 14th International Conference on Machine Learning (ICML-97), pages 412– 420.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>