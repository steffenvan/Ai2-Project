<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.998968">
Unsupervised Multilingual Learning for Morphological Segmentation
</title>
<author confidence="0.996565">
Benjamin Snyder and Regina Barzilay
</author>
<affiliation confidence="0.998294">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.998605">
{bsnyder,regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999921285714286">
For centuries, the deep connection between
languages has brought about major discover-
ies about human communication. In this pa-
per we investigate how this powerful source
of information can be exploited for unsuper-
vised language learning. In particular, we
study the task of morphological segmentation
of multiple languages. We present a non-
parametric Bayesian model that jointly in-
duces morpheme segmentations of each lan-
guage under consideration and at the same
time identifies cross-lingual morpheme pat-
terns, or abstract morphemes. We apply our
model to three Semitic languages: Arabic, He-
brew, Aramaic, as well as to English. Our
results demonstrate that learning morpholog-
ical models in tandem reduces error by up
to 24% relative to monolingual models. Fur-
thermore, we provide evidence that our joint
model achieves better performance when ap-
plied to languages from the same family.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911225806452">
For centuries, the deep connection between human
languages has fascinated linguists, anthropologists
and historians (Eco, 1995). The study of this con-
nection has made possible major discoveries about
human communication: it has revealed the evolu-
tion of languages, facilitated the reconstruction of
proto-languages, and led to understanding language
universals.
The connection between languages should be a
powerful source of information for automatic lin-
guistic analysis as well. In this paper we investi-
gate two questions: (i) Can we exploit cross-lingual
correspondences to improve unsupervised language
learning? (ii) Will this joint analysis provide more or
less benefit when the languages belong to the same
family?
We study these two questions in the context of
unsupervised morphological segmentation, the auto-
matic division of a word into morphemes (the basic
units of meaning). For example, the English word
misunderstanding would be segmented into mis -
understand - ing. This task is an informative testbed
for our exploration, as strong correspondences at the
morphological level across various languages have
been well-documented (Campbell, 2004).
The model presented in this paper automatically
induces a segmentation and morpheme alignment
from a multilingual corpus of short parallel phrases.1
For example, given parallel phrases meaning in my
land in English, Arabic, Hebrew, and Aramaic, we
wish to segment and align morphemes as follows:
</bodyText>
<note confidence="0.663737">
English: in my land
</note>
<sectionHeader confidence="0.565565666666667" genericHeader="introduction">
Arabic:
Hebrew:
Aramaic:
</sectionHeader>
<bodyText confidence="0.999894857142857">
This example illustrates the potential benefits
of unsupervised multilingual learning. The three
Semitic languages use cognates (words derived from
a common ancestor) to represent the word land.
They also use an identical suffix (-y) to represent the
first person possessive pronoun (my). These similar-
ities in form should guide the model by constraining
</bodyText>
<footnote confidence="0.995167333333333">
1In this paper, we focus on bilingual models. The model can
be extended to handle several languages simultaneously as in
this example.
</footnote>
<figure confidence="0.968356666666667">
fy arḍ - y
b - arṣ - y
b - arʿ - y
</figure>
<page confidence="0.937761">
737
</page>
<note confidence="0.696036">
Proceedings of ACL-08: HLT, pages 737–745,
</note>
<page confidence="0.537018">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999931193548387">
the space of joint segmentations. The corresponding
English phrase lacks this resemblance to its Semitic
counterparts. However, in this as in many cases, no
segmentation is required for English as all the mor-
phemes are expressed as individual words. For this
reason, English should provide a strong source of
disambiguation for highly inflected languages, such
as Arabic and Hebrew.
In general, we pose the following question. In
which scenario will multilingual learning be most
effective? Will it be for related languages, which
share a common core of linguistic features, or for
distant languages, whose linguistic divergence can
provide strong sources of disambiguation?
As a first step towards answering this question,
we propose a model which can take advantage of
both similarities and differences across languages.
This joint bilingual model identifies optimal mor-
phemes for two languages and at the same time finds
compact multilingual representations. For each lan-
guage in the pair, the model favors segmentations
which yield high frequency morphemes. More-
over, bilingual morpheme pairs which consistently
share a common semantic or syntactic function are
treated as abstract morphemes, generated by a sin-
gle language-independent process. These abstract
morphemes are induced automatically by the model
from recurring bilingual patterns. For example, in
the case above, the tuple (in, fy, b-, b-) would consti-
tute one of three abstract morphemes in the phrase.
When a morpheme occurs in one language with-
out a direct counterpart in the other language, our
model can explain away the stray morpheme as aris-
ing through a language-specific process.
To achieve this effect in a probabilistic frame-
work, we formulate a hierarchical Bayesian model
with Dirichlet Process priors. This framework al-
lows us to define priors over the infinite set of pos-
sible morphemes in each language. In addition,
we define a prior over abstract morphemes. This
prior can incorporate knowledge of the phonetic re-
lationship between the two alphabets, giving poten-
tial cognates greater prior likelihood. The resulting
posterior distributions concentrate their probability
mass on a small group of recurring and stable pat-
terns within and between languages.
We test our model on a multilingual corpus of
short parallel phrases drawn from the Hebrew Bible
and Arabic, Aramaic, and English translations. The
Semitic language family, of which Hebrew, Arabic,
and Aramaic are members, is known for a highly
productive morphology (Bravmann, 1977). Our re-
sults indicate that cross-lingual patterns can indeed
be exploited successfully for the task of unsuper-
vised morphological segmentation. When modeled
in tandem, gains are observed for all language pairs,
reducing relative error by as much as 24%. Further-
more, our experiments show that both related and
unrelated language pairs benefit from multilingual
learning. However, when common structures such
as phonetic correspondences are explicitly modeled,
related languages provide the most benefit.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="method">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999437387096774">
Multilingual Language Learning Recently, the
availability of parallel corpora has spurred research
on multilingual analysis for a variety of tasks
ranging from morphology to semantic role label-
ing (Yarowsky et al., 2000; Diab and Resnik, 2002;
Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of
this research assumes that one language has annota-
tions for the task of interest. Given a parallel cor-
pus, the annotations are projected from this source
language to its counterpart, and the resulting anno-
tations are used for supervised training in the target
language. In fact, Rogati et al., (2003) employ this
method to learn arabic morphology assuming anno-
tations provided by an English stemmer.
An alternative approach has been proposed by
Feldman, Hana and Brew (2004; 2006). While their
approach does not require a parallel corpus it does
assume the availability of annotations in one lan-
guage. Rather than being fully projected, the source
annotations provide co-occurrence statistics used by
a model in the resource-poor target language. The
key assumption here is that certain distributional
properties are invariant across languages from the
same language families. An example of such a prop-
erty is the distribution of part-of-speech bigrams.
Hana et al., (2004) demonstrate that adding such
statistics from an annotated Czech corpus improves
the performance of a Russian part-of-speech tagger
over a fully unsupervised version.
The approach presented here differs from previ-
ous work in two significant ways. First, we do
</bodyText>
<page confidence="0.996201">
738
</page>
<bodyText confidence="0.999864666666667">
not assume supervised data in any of the languages.
Second, we learn a single multilingual model, rather
than asymmetrically handling one language at a
time. This design allows us to capitalize on struc-
tural regularities across languages for the mutual
benefit of each language.
</bodyText>
<subsectionHeader confidence="0.579202">
Unsupervised Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.924678875">
Unsupervised morphology is an active area of
research (Schone and Jurafsky, 2000; Goldsmith,
2001; Adler and Elhadad, 2006; Creutz and Lagus,
2007; Dasgupta and Ng, 2007).
Most existing algorithms derive morpheme lexi-
cons by identifying recurring patterns in string dis-
tribution. The goal is to optimize the compactness
of the data representation by finding a small lexicon
of highly frequent strings. Our work builds on prob-
abilistic segmentation approaches such as Morfes-
sor (Creutz and Lagus, 2007). In these approaches,
models with short description length are preferred.
Probabilities are computed for both the morpheme
lexicon and the representation of the corpus condi-
tioned on the lexicon. A locally optimal segmenta-
tion is identified using a task-specific greedy search.
In contrast to previous approaches, our model
induces morphological segmentation for multiple
related languages simultaneously. By represent-
ing morphemes abstractly through the simultane-
ous alignment and segmentation of data in two lan-
guages, our algorithm capitalizes on deep connec-
tions between morpheme usage across different lan-
guages.
</bodyText>
<sectionHeader confidence="0.943199" genericHeader="method">
3 Multilingual Morphological
Segmentation
</sectionHeader>
<bodyText confidence="0.99991505">
The underlying assumption of our work is that struc-
tural commonality across different languages is a
powerful source of information for morphological
analysis. In this section, we provide several exam-
ples that motivate this assumption.
The main benefit of joint multilingual analysis is
that morphological structure ambiguous in one lan-
guage is sometimes explicitly marked in another lan-
guage. For example, in Hebrew, the preposition
meaning “in”, b-, is always prefixed to its nomi-
nal argument. On the other hand, in Arabic, the
most common corresponding particle is fy, which
appears as a separate word. By modeling cross-
lingual morpheme alignments while simultaneously
segmenting, the model effectively propagates infor-
mation between languages and in this case would be
encouraged to segment the Hebrew prefix b-.
Cognates are another important means of disam-
biguation in the multilingual setting. Consider trans-
lations of the phrase “...and they wrote it...”:
</bodyText>
<listItem confidence="0.999973">
• Hebrew: w-ktb-w ath
• Arabic: f-ktb-w-ha
</listItem>
<bodyText confidence="0.9997093">
In both languages, the triliteral root ktb is used to
express the act of writing. By considering the two
phrases simultaneously, the model can be encour-
aged to split off the respective Hebrew and Arabic
prefixes w- and f- in order to properly align the cog-
nate root ktb.
In the following section, we describe a model that
can model both generic cross-lingual patterns (fy and
b-), as well as cognates between related languages
(ktb for Hebrew and Arabic).
</bodyText>
<sectionHeader confidence="0.994461" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.999908454545455">
Overview In order to simultaneously model prob-
abilistic dependencies across languages as well as
morpheme distributions within each language, we
employ a hierarchical Bayesian model.2
Our segmentation model is based on the notion
that stable recurring string patterns within words
are indicative of morphemes. In addition to learn-
ing independent morpheme patterns for each lan-
guage, the model will prefer, when possible, to join
together frequently occurring bilingual morpheme
pairs into single abstract morphemes. The model is
fully unsupervised and is driven by a preference for
stable and high frequency cross-lingual morpheme
patterns. In addition the model can incorporate
character-to-character phonetic correspondences be-
tween alphabets as prior information, thus allowing
the implicit modeling of cognates.
Our aim is to induce a model which concentrates
probability on highly frequent patterns while still
allowing for the possibility of those previously un-
seen. Dirichlet processes are particularly suitable for
such conditions. In this framework, we can encode
</bodyText>
<footnote confidence="0.971802666666667">
2In (Snyder and Barzilay, 2008) we consider the use of this
model in the case where supervised data in one or more lan-
guages is available.
</footnote>
<page confidence="0.997406">
739
</page>
<bodyText confidence="0.999349303797468">
prior knowledge over the infinite sets of possible
morpheme strings as well as abstract morphemes.
Distributions drawn from a Dirichlet process nev-
ertheless produce sparse representations with most
probability mass concentrated on a small number of
observed and predicted patterns. Our model utilizes
a Dirichlet process prior for each language, as well
as for the cross-lingual links (abstract morphemes).
Thus, a distribution over morphemes and morpheme
alignments is first drawn from the set of Dirichlet
processes and then produces the observed data. In
practice, we never deal with such distributions di-
rectly, but rather integrate over them during Gibbs
sampling.
In the next section we describe our model’s “gen-
erative story” for producing the data we observe. We
formalize our model in the context of two languages
£ and F. However, the formulation can be extended
to accommodate evidence from multiple languages
as well. We provide an example of parallel phrase
generation in Figure 1.
High-level Generative Story We have a parallel
corpus of several thousand short phrases in the two
languages £ and F. Our model provides a genera-
tive story explaining how these parallel phrases were
probabilistically created. The core of the model
consists of three components: a distribution A over
bilingual morpheme pairs (abstract morphemes), a
distribution E over stray morphemes in language £
occurring without a counterpart in language F, and
a similar distribution F for stray morphemes in lan-
guage F.
As usual for hierarchical Bayesian models, the
generative story begins by drawing the model pa-
rameters themselves – in our case the three distri-
butions A, E, and F. These three distributions are
drawn from three separate Dirichlet processes, each
with appropriately defined base distributions. The
Dirichlet processes ensure that the resulting distri-
butions concentrate their probability mass on a small
number of morphemes while holding out reasonable
probability for unseen possibilities.
Once A, E, and F have been drawn, we model
our parallel corpus of short phrases as a series of
independent draws from a phrase-pair generation
model. For each new phrase-pair, the model first
chooses the number and type of morphemes to be
generated. In particular, it must choose how many
unaligned stray morphemes from language £, un-
aligned stray morphemes from language F, and
abstract morphemes are to compose the parallel
phrases. These three numbers, respectively denoted
as m, n, and k, are drawn from a Poisson distribu-
tion. This step is illustrated in Figure 1 part (a).
The model then proceeds to independently draw
m language £ morphemes from distribution E, n
language-F morphemes from distribution F, and k
abstract morphemes from distribution A. This step
is illustrated in part (b) of Figure 1.
The m + k resulting language-£ morphemes are
then ordered and fused to form a phrase in language
£, and likewise for the n + k resulting language-
F morphemes. The ordering and fusing decisions
are modeled as draws from a uniform distribution
over the set of all possible orderings and fusings for
sizes m, n, and k. These final steps are illustrated in
parts (c)-(d) of Figure 1. Now we describe the model
more formally.
Stray Morpheme Distributions Sometimes a
morpheme occurs in a phrase in one language with-
out a corresponding foreign language morpheme
in the parallel phrase. We call these “stray mor-
phemes,” and we employ language-specific mor-
pheme distributions to model their generation.
For each language, we draw a distribution over
all possible morphemes (finite-length strings com-
posed of characters in the appropriate alphabet) from
a Dirichlet process with concentration parameter α
and base distribution Pe or Pf respectively:
</bodyText>
<equation confidence="0.420748">
E|α, Pe — DP(α, Pe)
F|α, Pf — DP(α, Pf)
</equation>
<bodyText confidence="0.999424777777778">
The base distributions Pe and Pf can encode prior
knowledge about the properties of morphemes in
each of the two languages, such as length and char-
acter n-grams. For simplicity, we use a geometric
distribution over the length of the string with a final
end-morpheme character. The distributions E and F
which result from the respective Dirichlet processes
place most of their probability mass on a small num-
ber of morphemes with the degree of concentration
</bodyText>
<page confidence="0.93728">
740
</page>
<figure confidence="0.6873768">
(a) (b) E kn`n w A h (c) at h kn n y `w (d) -ACC the-canaan-of
m =1 at F kn`n w y al w al kn`n y yn and -at h-kn`n-y
n = 1 yn y w and w-al-kn`n-y -yn
k= 4 -the-canaan-of-PLURAL
13y3� -n nxi &amp;quot;...and the Canaanites&amp;quot; t-r�- او
</figure>
<figureCaption confidence="0.998962">
Figure 1: Generation process for a parallel bilingual phrase, with Hebrew shown on top and Arabic on bottom. (a)
First the numbers of stray (m and n) and abstract (k) morphemes are drawn from a Poisson distribution. (b) Stray
morphemes are then drawn from E and F (language-specific distributions) and abstract morphemes are drawn from
A. (c) The resulting morphemes are ordered. (d) Finally, some of the contiguous morphemes are fused into words.
</figureCaption>
<equation confidence="0.971185">
ti DP
A|α0,P0
(α0, P0)
(e, f) ti A
</equation>
<bodyText confidence="0.964325516129032">
As before, the resulting distribution A will
give
non-zero probability to all abstract morphemes
(e, f). The base distribution
acts as a prior on
such pairs. To define
we can simply use a mix-
ture of geometric distributions in the lengths of the
component morphemes. However, if the languages
£ and
are related and the regular phonetic corre-
spondences between the letter in the two alphabets
are known, then we can use
to assign higher like-
lihood to potential cognates. In particular we define
the prior
f) to be the probabilistic string-edit
distance (Ristad and Yianilos, 1998) between
and
the known phonetic correspondences to pa-
rameterize the string-edit model. In particular, in-
sertion and deletion probabilities are held constant
for all characters, and substitution probabilities are
determined based on the known sound correspon-
dences.
We report results for both the simple geometric
prior as well as the string-edit prior.
Phrase Generation To generate a bilingual paral-
lel phrase, we first draw m, n, and k independently
from a Poisson distribution. These three integers
represent the number and type of the morphemes
</bodyText>
<equation confidence="0.990426625">
P0
P0,
F
P0
P0(e,
e
f, using
F
</equation>
<bodyText confidence="0.977355774193548">
d
the number of coupled bilingual morpheme pairs, re-
spectively.
controlled by the prior
Nevertheless, some non-
zero probability is reserved for every possible string.
We note that these single-language morpheme
distributions also serve as monolingual segmenta-
tion models, and similar models have been success-
fully applied to the task of word boundary detection
(Goldwater et al., 2006).
Abstract Morpheme Distribution To model the
connections between morphemes across languages,
we further define a model for bilingual morpheme
pairs, or abstract morphemes. This model assigns
probabilities to all pairs of morphemes
is, all
pairs of finite strings from the respective alphabets
(e, f). Intuitively, we wish to assign high proba-
bility to pairs of morphemes that play similar syn-
tactic or semantic roles (e.g. (fy, b-) for
in Ara-
bic and Hebrew). These morpheme pairs can thus
be viewed as representing abstract morphemes. As
with the stray morpheme models, we wish to define
a distribution which concentrates probability mass
on a small number of highly co-occurring morpheme
pairs while still holding out some probability for all
other pairs.
We define this abstract morpheme model A as a
draw from an
</bodyText>
<equation confidence="0.2679856">
α.
– that
–
“in”
other Dirichlet process:
</equation>
<bodyText confidence="0.946877333333333">
that compose the parallel phrase, giving the number
of stray morphemes in each language £ and
an
m, n, k ti
Given these values, we now draw the appropriate
number of stray an
</bodyText>
<equation confidence="0.56197">
Poisson(�)
</equation>
<bodyText confidence="0.879081">
d abstract morphemes from the
corresponding distributions:
</bodyText>
<page confidence="0.947999">
741
</page>
<bodyText confidence="0.793323333333333">
e1, ..., em ∼ E
f1, ..., fn ∼ F
(ei, fi), ..., (ek, f�k) ∼ A
The sets of morphemes drawn for each language
are then ordered:
˜e1, ..., ˜em+k ∼ ORDER|e1, ..., em, ei, ..., ek
˜f1, ..., ˜fn+k ∼ ORDER|f1, ..., fn, f1, ..., fk
Finally the ordered morphemes are fused into the
words that form the parallel phrases:
</bodyText>
<equation confidence="0.925609">
w1, ..., ws ∼ FUSE|˜e1, ..., ˜em+k
v1,...,vt ∼ FUSE |˜f1, ..., ˜fn+k
</equation>
<bodyText confidence="0.994047028169014">
To keep the model as simple as possible, we em-
ploy uniform distributions over the sets of orderings
and fusings. In other words, given a set of r mor-
phemes (for each language), we define the distribu-
tion over permutations of the morphemes to simply
be ORDER(·|r) = 1��. Then, given a fixed mor-
pheme order, we consider fusing each adjacent mor-
pheme into a single word. Again, we simply model
the distribution over the r − 1 fusing decisions uni-
formly as FUSE(·|r) = 1
2&apos;&apos;−1 .
Implicit Alignments Note that nowhere do we ex-
plicitly assign probabilities to morpheme alignments
between parallel phrases. However, our model al-
lows morphemes to be generated in precisely one of
two ways: as a lone stray morpheme or as part of a
bilingual abstract morpheme pair. Thus, our model
implicitly assumes that each morpheme is either un-
aligned, or aligned to exactly one morpheme in the
opposing language.
If we are given a parallel phrase with already seg-
mented morphemes we can easily induce the distri-
bution over alignments implied by our model. As we
will describe in the next section, drawing from these
induced alignment distributions plays a crucial role
in our inference procedure.
Inference Given our corpus of short parallel bilin-
gual phrases, we wish to make segmentation de-
cisions which yield a set of morphemes with high
joint probability. To assess the probability of a po-
tential morpheme set, we need to marginalize over
all possible alignments (i.e. possible abstract mor-
pheme pairings and stray morpheme assignments).
We also need to marginalize over all possible draws
of the distributions A, E, and F from their respec-
tive Dirichlet process priors. We achieve these aims
by performing Gibbs sampling.
Sampling We follow (Neal, 1998) in the deriva-
tion of our blocked and collapsed Gibbs sampler.
Gibbs sampling starts by initializing all random vari-
ables to arbitrary starting values. At each iteration,
the sampler selects a random variable Xi, and draws
a new value for Xi from the conditional distribution
of Xi given the current value of the other variables:
P(Xi|X_i). The stationary distribution of variables
derived through this procedure is guaranteed to con-
verge to the true joint distribution of the random
variables. However, if some variables can be jointly
sampled, then it may be beneficial to perform block
sampling of these variables to speed convergence. In
addition, if a random variable is not of direct inter-
est, we can avoid sampling it directly by marginal-
izing it out, yielding a collapsed sampler. We uti-
lize variable blocking by jointly sampling multiple
segmentation and alignment decisions. We also col-
lapse our Gibbs sampler in the standard way, by us-
ing predictive posteriors marginalized over all possi-
ble draws from the Dirichlet processes (resulting in
Chinese Restaurant Processes).
Resampling For each bilingual phrase, we resam-
ple each word in the phrase in turn. For word w
in language E, we consider at once all possible seg-
mentations, and for each segmentation all possible
alignments. We keep fixed the previously sampled
segmentation decisions for all other words in the
phrase as well as sampled alignments involving mor-
phemes in other words. We are thus considering at
once: all possible segmentations of w along with
all possible alignments involving morphemes in w
with some subset of previously sampled language-
F morphemes.3
</bodyText>
<footnote confidence="0.9429375">
3We retain morpheme identities during resampling of the
morpheme alignments. This procedure is technically justi-
</footnote>
<page confidence="0.980642">
742
</page>
<table confidence="0.9998294">
Arabic Hebrew
precision recall F-score precision recall F-score
RANDOM 18.28 19.24 18.75 24.95 24.66 24.80
MORFESSOR 71.10 60.51 65.38 65.38 57.69 61.29
MONOLINGUAL 52.95 78.46 63.22 55.76 64.44 59.78
+ ARABIC/HEBREW 60.40 78.64 68.32 59.08 66.50 62.57
+ ARAMAIC 61.33 77.83 68.60 54.63 65.68 59.64
+ ENGLISH 63.19 74.79 68.49 60.20 64.42 62.23
+ ARAMAIC+PH 66.74 75.46 70.83 60.87 59.73 60.29
+ ARABIC/HEBREW+PH 67.75 77.29 72.20 64.90 62.87 63.87
</table>
<tableCaption confidence="0.759897">
Table 1: Precision, recall and F-score evaluated on Arabic and Hebrew. The first three rows provide baselines (random
selection, an alternative state-of-the-art system, and the monolingual version of our model). The next three rows show
the result of our bilingual model when one of Arabic, Hebrew, Aramaic, or English is added. The final two rows
show the result of the bilingual model when character-to-character phonetic correspondences are used in the abstract
morpheme prior.
</tableCaption>
<bodyText confidence="0.9999305">
The sampling formulas are easily derived as prod-
ucts of the relevant Chinese Restaurant Processes
(with a minor adjustment to take into account the
number of stray and abstract morphemes resulting
from each decision). See (Neal, 1998) for general
formulas for Gibbs sampling from distributions with
Dirichlet process priors. All results reported are av-
eraged over five runs using simulated annealing.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="method">
5 Experimental Set-Up
</sectionHeader>
<bodyText confidence="0.991566170212766">
Morpheme Definition For the purpose of these
experiments, we define morphemes to include con-
junctions, prepositional and pronominal affixes, plu-
ral and dual suffixes, particles, definite articles, and
roots. We do not model cases of infixed morpheme
transformations, as those cannot be modeled by lin-
ear segmentation.
Dataset As a source of parallel data, we use the
Hebrew Bible and translations. For the Hebrew ver-
sion, we use an edition distributed by Westminster
Hebrew Institute (Groves and Lowery, 2006). This
Bible edition is augmented by gold standard mor-
phological analysis (including segmentation) per-
formed by biblical scholars.
For the Arabic, Aramaic, and English versions,
fied by augmenting the model with a pair of “morpheme-
identity” variables deterministically drawn from each abstract
morpheme. Thus the identity of the drawn morphemes can be
retained even while resampling their generation mechanism.
we use the Van Dyke Arabic translation,4 Targum
Onkelos,5 and the Revised Standard Version (Nel-
son, 1952), respectively. We obtained gold stan-
dard segmentations of the Arabic translation with a
hand-crafted Arabic morphological analyzer which
utilizes manually constructed word lists and compat-
ibility rules and is further trained on a large corpus
of hand-annotated Arabic data (Habash and Ram-
bow, 2005). The accuracy of this analyzer is re-
ported to be 94% for full morphological analyses,
and 98%-99% when part-of-speech tag accuracy is
not included. We don’t have gold standard segmen-
tations for the English and Aramaic portions of the
data, and thus restrict our evaluation to Hebrew and
Arabic.
To obtain our corpus of short parallel phrases, we
preprocessed each language pair using the Giza++
alignment toolkit.6 Given word alignments for each
language pair, we extract a list of phrase pairs that
form independent sets in the bipartite alignment
graph. This process allows us to group together
phrases like fy s.bah. in Arabic and bbqr in He-
brew while being reasonably certain that all the rele-
vant morphemes are contained in the short extracted
phrases. The number of words in such phrases
ranges from one to four words in the Semitic lan-
guages and up to six words in English. Before per-
forming any experiments, a manual inspection of
</bodyText>
<footnote confidence="0.999975333333333">
4http://www.arabicbible.com/bible/vandyke.htm
5http://www.mechon-mamre.org/i/t/u/u0.htm
6http://www.fjoch.com/GIZA++.html
</footnote>
<page confidence="0.997808">
743
</page>
<bodyText confidence="0.999695936170213">
the generated parallel phrases revealed that many
infrequent phrase pairs occurred merely as a result
of noisy translation and alignment. Therefore, we
eliminated all parallel phrases that occur fewer than
five times. As a result of this process, we obtain
6,139 parallel short phrases in Arabic, Hebrew, Ara-
maic, and English. The average number of mor-
phemes per word in the Hebrew data is 1.8 and is
1.7 in Arabic.
For the bilingual models which employs prob-
abilistic string-edit distance as a prior on abstract
morphemes, we parameterize the string-edit model
with the chart of Semitic consonant relationships
listed on page xxiv of (Thackston, 1999). All pairs
of corresponding letters are given equal substitution
probability, while all other letter pairs are given sub-
stitution probability of zero.
Evaluation Methods Following previous work,
we evaluate the performance of our automatic seg-
mentation algorithm using F-score. This measure is
the harmonic mean of recall and precision, which are
calculated on the basis of all possible segmentation
points. The evaluation is performed on a random set
of 1/5 of the parallel phrases which is unseen dur-
ing the training phase. During testing, we do not
allow the models to consider any multilingual evi-
dence. This restriction allows us to simulate future
performance on purely monolingual data.
Baselines Our primary purpose is to compare the
performance of our bilingual model with its fully
monolingual counterpart. However, to demonstrate
the competitiveness of this baseline model, we also
provide results using MORFESSOR (Creutz and La-
gus, 2007), a state-of-the-art unsupervised system
for morphological segmentation. While developed
originally for Finnish, this system has been success-
fully applied to a range of languages including Ger-
man, Turkish and English. The probabilistic formu-
lation of this model is close to our monolingual seg-
mentation model, but it uses a greedy search specif-
ically designed for the segmentation task. We use
the publicly available implementation of this system.
To provide some idea of the inherent difficulty of
this segmentation task, we also provide results from
a random baseline which makes segmentation deci-
sions based on a coin weighted with the true seg-
mentation frequency.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999957863636364">
Table 1 shows the performance of the various auto-
matic segmentation methods. The first three rows
provide baselines, as mentioned in the previous sec-
tion. Our primary baseline is MONOLINGUAL,
which is the monolingual counterpart to our model
and only uses the language-specific distributions E
or F. The next three rows shows the performance of
various bilingual models that don’t use character-to-
character phonetic correspondences to capture cog-
nate information. We find that with the excep-
tion of the HEBREW(+ARAMAIC) pair, the bilingual
models show marked improvement over MONOLIN-
GUAL. We notice that in general, adding English –
which has comparatively little morphological ambi-
guity – is about as useful as adding a more closely
related Semitic language. However, once character-
to-character phonetic correspondences are added as
an abstract morpheme prior (final two rows), we
find the performance of related language pairs out-
strips English, reducing relative error over MONO-
LINGUAL by 10% and 24% for the Hebrew/Arabic
pair.
</bodyText>
<sectionHeader confidence="0.998001" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999850352941176">
We started out by posing two questions: (i) Can we
exploit cross-lingual patterns to improve unsuper-
vised analysis? (ii) Will this joint analysis provide
more or less benefit when the languages belong to
the same family? The model and results presented in
this paper answer the first question in the affirmative,
at least for the task of morphological segmentation.
We also provided some evidence that considering
closely related languages may be more beneficial
than distant pairs if the model is able to explicitly
represent shared language structure (the character-
to-character phonetic correspondences in our case).
In the future, we hope to apply similar multilingual
models to other core unsupervised analysis tasks, in-
cluding part-of-speech tagging and grammar induc-
tion, and to further investigate the role that language
relatedness plays in such models. 7
</bodyText>
<footnote confidence="0.9371795">
7We acknowledge the support of the National Science Foun-
dation (CAREER grant IIS-0448168 and grant IIS-0415865)
and the Microsoft Research Faculty Fellowship. Thanks to
members of the MIT NLP group for enlightening discussion.
</footnote>
<page confidence="0.997421">
744
</page>
<sectionHeader confidence="0.998316" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597040540541">
Meni Adler and Michael Elhadad. 2006. An un-
supervised morpheme-based hmm for hebrew mor-
phological disambiguation. In Proceedings of the
ACL/CONLL, pages 665–672.
M. M. Bravmann. 1977. Studies in Semitic Philology.
Leiden:Brill.
Lyle Campbell. 2004. Historical Linguistics: An Intro-
duction. Cambridge: MIT Press.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
Sajib Dasgupta and Vincent Ng. 2007. Unsuper-
vised part-of-speech acquisition for resource-scarce
languages. In Proceedings of the EMNLP-CoNLL,
pages 218–227.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the ACL, pages 255–262.
Umberto Eco. 1995. The Search for the Perfect Lan-
guage. Wiley-Blackwell.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings of LREC.
John A. Goldsmith. 2001. Unsupervised learning of
the morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the ACL, pages
673–680.
Alan Groves and Kirk Lowery, editors. 2006. The West-
minster Hebrew Bible Morphology Database. West-
minster Hebrew Institute, Philadelphia, PA, USA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambig uation in one fell swoop. In Proceedings of
the ACL, pages 573–580.
Jiri Hana, Anna Feldman, and Chris Brew. 2004. A
resource-light approach to russian morphology: Tag-
ging russian using czech resources. In Proceedings of
EMNLP, pages 222–229.
Radford M. Neal. 1998. Markov chain sampling meth-
ods for dirichlet process mixture models. Technical
Report 9815, Dept. of Statistics and Dept. of Computer
Science, University of Toronto, September.
Thomas Nelson, editor. 1952. The Holy Bible Revised
Standard Version. Thomas Nelson &amp; Sons.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings ofACL, pages 1161 – 1168.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning
string-edit distance. IEEE Trans. Pattern Anal. Mach.
Intell., 20(5):522–532.
Monica Rogati, J. Scott McCarley, and Yiming Yang.
2003. Unsupervised learning of arabic stemming us-
ing a parallel corpus. In Proceedings of the ACL, pages
391–398.
Patrick Schone and Daniel Jurafsky. 2000. Knowledge-
free induction of morphology using latent semantic
analysis. In Proceedings of the CoNLL, pages 67–72.
Benjamin Snyder and Regina Barzilay. 2008. Cross-
lingual propagation for morphological analysis. In
Proceedings of AAAI.
Wheeler M. Thackston. 1999. Introduction to Syriac.
Ibex Publishers.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of HLT/EMNLP, pages 851 – 858.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161–168.
</reference>
<page confidence="0.998543">
745
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828510">
<title confidence="0.999831">Unsupervised Multilingual Learning for Morphological Segmentation</title>
<author confidence="0.999699">Benjamin Snyder</author>
<author confidence="0.999699">Regina Barzilay</author>
<affiliation confidence="0.999962">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<abstract confidence="0.9918825">For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>An unsupervised morpheme-based hmm for hebrew morphological disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/CONLL,</booktitle>
<pages>665--672</pages>
<contexts>
<context position="8312" citStr="Adler and Elhadad, 2006" startWordPosition="1253" endWordPosition="1256">he performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmen</context>
</contexts>
<marker>Adler, Elhadad, 2006</marker>
<rawString>Meni Adler and Michael Elhadad. 2006. An unsupervised morpheme-based hmm for hebrew morphological disambiguation. In Proceedings of the ACL/CONLL, pages 665–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Bravmann</author>
</authors>
<date>1977</date>
<note>Studies in Semitic Philology. Leiden:Brill.</note>
<contexts>
<context position="5819" citStr="Bravmann, 1977" startWordPosition="876" endWordPosition="877"> a prior over abstract morphemes. This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood. The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages. We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations. The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977). Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation. When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%. Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning. However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora ha</context>
</contexts>
<marker>Bravmann, 1977</marker>
<rawString>M. M. Bravmann. 1977. Studies in Semitic Philology. Leiden:Brill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>Historical Linguistics: An Introduction.</title>
<date>2004</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="2299" citStr="Campbell, 2004" startWordPosition="330" endWordPosition="331">ross-lingual correspondences to improve unsupervised language learning? (ii) Will this joint analysis provide more or less benefit when the languages belong to the same family? We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning). For example, the English word misunderstanding would be segmented into mis - understand - ing. This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004). The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows: English: in my land Arabic: Hebrew: Aramaic: This example illustrates the potential benefits of unsupervised multilingual learning. The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land. They also use an identical suffix (-y) to represent the first </context>
</contexts>
<marker>Campbell, 2004</marker>
<rawString>Lyle Campbell. 2004. Historical Linguistics: An Introduction. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="8336" citStr="Creutz and Lagus, 2007" startWordPosition="1257" endWordPosition="1260">an part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmentation is identified usi</context>
<context position="28856" citStr="Creutz and Lagus, 2007" startWordPosition="4535" endWordPosition="4539">precision, which are calculated on the basis of all possible segmentation points. The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase. During testing, we do not allow the models to consider any multilingual evidence. This restriction allows us to simulate future performance on purely monolingual data. Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart. However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation. While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English. The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task. We use the publicly available implementation of this system. To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decis</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised part-of-speech acquisition for resource-scarce languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL,</booktitle>
<pages>218--227</pages>
<contexts>
<context position="8360" citStr="Dasgupta and Ng, 2007" startWordPosition="1261" endWordPosition="1264"> over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon. A locally optimal segmentation is identified using a task-specific greed</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng. 2007. Unsupervised part-of-speech acquisition for resource-scarce languages. In Proceedings of the EMNLP-CoNLL, pages 218–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="6581" citStr="Diab and Resnik, 2002" startWordPosition="985" endWordPosition="988">on. When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%. Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning. However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does a</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the ACL, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umberto Eco</author>
</authors>
<title>The Search for the Perfect Language.</title>
<date>1995</date>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="1266" citStr="Eco, 1995" startWordPosition="177" endWordPosition="178">nguage under consideration and at the same time identifies cross-lingual morpheme patterns, or abstract morphemes. We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family. 1 Introduction For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995). The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals. The connection between languages should be a powerful source of information for automatic linguistic analysis as well. In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning? (ii) Will this joint analysis provide more or less benefit when the languages belong to the same family? We st</context>
</contexts>
<marker>Eco, 1995</marker>
<rawString>Umberto Eco. 1995. The Search for the Perfect Language. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Feldman</author>
<author>Jirka Hana</author>
<author>Chris Brew</author>
</authors>
<title>A cross-language approach to rapid creation of new morpho-syntactically annotated resources.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Feldman, Hana, Brew, 2006</marker>
<rawString>Anna Feldman, Jirka Hana, and Chris Brew. 2006. A cross-language approach to rapid creation of new morpho-syntactically annotated resources. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="8287" citStr="Goldsmith, 2001" startWordPosition="1251" endWordPosition="1252">corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon.</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John A. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>673--680</pages>
<contexts>
<context position="18533" citStr="Goldwater et al., 2006" startWordPosition="2887" endWordPosition="2890">edit prior. Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution. These three integers represent the number and type of the morphemes P0 P0, F P0 P0(e, e f, using F d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string. We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006). Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes. This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f). Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g. (fy, b-) for in Arabic and Hebrew). These morpheme pairs can thus be viewed as representing abstract morphemes. As with the stray morpheme models, we wish to define a distribut</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the ACL, pages 673–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Groves</author>
<author>Kirk Lowery</author>
<author>editors</author>
</authors>
<title>The Westminster Hebrew Bible Morphology Database. Westminster Hebrew Institute,</title>
<date>2006</date>
<location>Philadelphia, PA, USA.</location>
<marker>Groves, Lowery, editors, 2006</marker>
<rawString>Alan Groves and Kirk Lowery, editors. 2006. The Westminster Hebrew Bible Morphology Database. Westminster Hebrew Institute, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic tokenization, part-of-speech tagging and morphological disambig uation in one fell swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>573--580</pages>
<contexts>
<context position="26188" citStr="Habash and Rambow, 2005" startWordPosition="4121" endWordPosition="4125">nting the model with a pair of “morphemeidentity” variables deterministically drawn from each abstract morpheme. Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively. We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005). The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included. We don’t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic. To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph. This process allows us to group together ph</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic tokenization, part-of-speech tagging and morphological disambig uation in one fell swoop. In Proceedings of the ACL, pages 573–580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Hana</author>
<author>Anna Feldman</author>
<author>Chris Brew</author>
</authors>
<title>A resource-light approach to russian morphology: Tagging russian using czech resources.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>222--229</pages>
<contexts>
<context position="7607" citStr="Hana et al., (2004)" startWordPosition="1148" endWordPosition="1151">g annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availability of annotations in one language. Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language. The key assumption here is that certain distributional properties are invariant across languages from the same language families. An example of such a property is the distribution of part-of-speech bigrams. Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morp</context>
</contexts>
<marker>Hana, Feldman, Brew, 2004</marker>
<rawString>Jiri Hana, Anna Feldman, and Chris Brew. 2004. A resource-light approach to russian morphology: Tagging russian using czech resources. In Proceedings of EMNLP, pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Markov chain sampling methods for dirichlet process mixture models.</title>
<date>1998</date>
<tech>Technical Report 9815,</tech>
<institution>Dept. of Statistics and Dept. of Computer Science, University of Toronto,</institution>
<contexts>
<context position="21758" citStr="Neal, 1998" startWordPosition="3439" endWordPosition="3440"> plays a crucial role in our inference procedure. Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability. To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments). We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors. We achieve these aims by performing Gibbs sampling. Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler. Gibbs sampling starts by initializing all random variables to arbitrary starting values. At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i). The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables. However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of</context>
<context position="24675" citStr="Neal, 1998" startWordPosition="3898" endWordPosition="3899">lines (random selection, an alternative state-of-the-art system, and the monolingual version of our model). The next three rows show the result of our bilingual model when one of Arabic, Hebrew, Aramaic, or English is added. The final two rows show the result of the bilingual model when character-to-character phonetic correspondences are used in the abstract morpheme prior. The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision). See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors. All results reported are averaged over five runs using simulated annealing. 5 Experimental Set-Up Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots. We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation. Dataset As a source of parallel data, we use the Hebrew Bible and translations. For the Hebre</context>
</contexts>
<marker>Neal, 1998</marker>
<rawString>Radford M. Neal. 1998. Markov chain sampling methods for dirichlet process mixture models. Technical Report 9815, Dept. of Statistics and Dept. of Computer Science, University of Toronto, September.</rawString>
</citation>
<citation valid="true">
<title>The Holy Bible Revised Standard Version.</title>
<date>1952</date>
<editor>Thomas Nelson, editor.</editor>
<publisher>Thomas Nelson &amp; Sons.</publisher>
<marker>1952</marker>
<rawString>Thomas Nelson, editor. 1952. The Holy Bible Revised Standard Version. Thomas Nelson &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1161--1168</pages>
<marker>Pad´o, Lapata, 2006</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proceedings ofACL, pages 1161 – 1168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="17569" citStr="Ristad and Yianilos, 1998" startWordPosition="2736" endWordPosition="2739"> into words. ti DP A|α0,P0 (α0, P0) (e, f) ti A As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f). The base distribution acts as a prior on such pairs. To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes. However, if the languages £ and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates. In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model. In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences. We report results for both the simple geometric prior as well as the string-edit prior. Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution. These three integers represent the number and type of the morphemes P0 P0, F P0 P0(e, e f, using F d the number of coup</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string-edit distance. IEEE Trans. Pattern Anal. Mach. Intell., 20(5):522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monica Rogati</author>
<author>J Scott McCarley</author>
<author>Yiming Yang</author>
</authors>
<title>Unsupervised learning of arabic stemming using a parallel corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>391--398</pages>
<contexts>
<context position="6934" citStr="Rogati et al., (2003)" startWordPosition="1045" endWordPosition="1048">t benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availability of annotations in one language. Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language. The key assumption here is that certain distributional properties are invariant across languages from the same language families. An example of such a pr</context>
</contexts>
<marker>Rogati, McCarley, Yang, 2003</marker>
<rawString>Monica Rogati, J. Scott McCarley, and Yiming Yang. 2003. Unsupervised learning of arabic stemming using a parallel corpus. In Proceedings of the ACL, pages 391–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledgefree induction of morphology using latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the CoNLL,</booktitle>
<pages>67--72</pages>
<contexts>
<context position="8270" citStr="Schone and Jurafsky, 2000" startWordPosition="1247" endWordPosition="1250">cs from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version. The approach presented here differs from previous work in two significant ways. First, we do 738 not assume supervised data in any of the languages. Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time. This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language. Unsupervised Morphological Segmentation Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007). Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings. Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007). In these approaches, models with short description length are preferred. Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditione</context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2000. Knowledgefree induction of morphology using latent semantic analysis. In Proceedings of the CoNLL, pages 67–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="11927" citStr="Snyder and Barzilay, 2008" startWordPosition="1798" endWordPosition="1801">pairs into single abstract morphemes. The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns. In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates. Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen. Dirichlet processes are particularly suitable for such conditions. In this framework, we can encode 2In (Snyder and Barzilay, 2008) we consider the use of this model in the case where supervised data in one or more languages is available. 739 prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes. Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns. Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes). Thus, a distribution over morphemes and morpheme alignments is first drawn from t</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Crosslingual propagation for morphological analysis. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wheeler M Thackston</author>
</authors>
<title>Introduction to Syriac.</title>
<date>1999</date>
<publisher>Ibex Publishers.</publisher>
<contexts>
<context position="27906" citStr="Thackston, 1999" startWordPosition="4392" endWordPosition="4393">led that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment. Therefore, we eliminated all parallel phrases that occur fewer than five times. As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English. The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic. For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999). All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero. Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score. This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points. The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase. During testing, we do not allow the models to consider any multilingual e</context>
</contexts>
<marker>Thackston, 1999</marker>
<rawString>Wheeler M. Thackston. 1999. Introduction to Syriac. Ibex Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhai Xi</author>
<author>Rebecca Hwa</author>
</authors>
<title>A backoff model for bootstrapping resources for non-english languages.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>851--858</pages>
<contexts>
<context position="6599" citStr="Xi and Hwa, 2005" startWordPosition="989" endWordPosition="992">dem, gains are observed for all language pairs, reducing relative error by as much as 24%. Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning. However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a parallel corpus it does assume the availabi</context>
</contexts>
<marker>Xi, Hwa, 2005</marker>
<rawString>Chenhai Xi and Rebecca Hwa. 2005. A backoff model for bootstrapping resources for non-english languages. In Proceedings of HLT/EMNLP, pages 851 – 858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="6558" citStr="Yarowsky et al., 2000" startWordPosition="981" endWordPosition="984">orphological segmentation. When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%. Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning. However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit. 2 Related Work Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006). Most of this research assumes that one language has annotations for the task of interest. Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language. In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer. An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006). While their approach does not require a pa</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2000</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2000. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of HLT, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>