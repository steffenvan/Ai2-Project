<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000591">
<title confidence="0.996599">
Exploiting N-best Hypotheses for SMT Self-Enhancement
</title>
<author confidence="0.998114">
Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li
</author>
<affiliation confidence="0.9913775">
Department of Human Language Technology
Institute for Infocomm Research
</affiliation>
<address confidence="0.907068">
21 Heng Mui Keng Terrace, 119613, Singapore
</address>
<email confidence="0.973486">
{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.996936" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998106888888889">
Word and n-gram posterior probabilities esti-
mated on N-best hypotheses have been used to
improve the performance of statistical ma-
chine translation (SMT) in a rescoring frame-
work. In this paper, we extend the idea to
estimate the posterior probabilities on N-best
hypotheses for translation phrase-pairs, target
language n-grams, and source word re-
orderings. The SMT system is self-enhanced
with the posterior knowledge learned from N-
best hypotheses in a re-decoding framework.
Experiments on NIST Chinese-to-English task
show performance improvements for all the
strategies. Moreover, the combination of the
three strategies achieves further improvements
and outperforms the baseline by 0.67 BLEU
score on NIST-2003 set, and 0.64 on NIST-
2005 set, respectively.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999431348837209">
State-of-the-art Statistical Machine Translation
(SMT) systems usually adopt a two-pass search
strategy. In the first pass, a decoding algorithm is
applied to generate an N-best list of translation
hypotheses; while in the second pass, the final
translation is selected by rescoring and re-ranking
the N-best hypotheses through additional feature
functions. In this framework, the N-best hypothe-
ses serve as the candidates for the final translation
selection in the second pass.
These N-best hypotheses can also provide useful
feedback to the MT system as the first decoding
has discarded many undesirable translation candi-
dates. Thus, the knowledge captured in the N-best
hypotheses, such as posterior probabilities for
words, n-grams, phrase-pairs, and source word re-
orderings, etc. is more compatible with the source
sentences and thus could potentially be used to
improve the translation performance.
Word posterior probabilities estimated from the
N-best hypotheses have been widely used for con-
fidence measure in automatic speech recognition
(Wessel, 2002) and have also been adopted into
machine translation. Blatz et al. (2003) and Uef-
fing et al. (2003) used word posterior probabilities
to estimate the confidence of machine translation.
Chen et al. (2005), Zens and Ney (2006) reported
performance improvements by computing target n-
grams posterior probabilities estimated on the N-
best hypotheses in a rescoring framework. Trans-
ductive learning method (Ueffing et al., 2007)
which repeatedly re-trains the generated source-
target N-best hypotheses with the original training
data again showed translation performance im-
provement and demonstrated that the translation
model can be reinforced from N-best hypotheses.
In this paper, we further exploit the potential of
the N-best hypotheses and propose several
schemes to derive the posterior knowledge from
the N-best hypotheses, in an effort to enhance the
language model, translation model, and source
word reordering under a re-decoding framework of
any phrase-based SMT system.
</bodyText>
<sectionHeader confidence="0.7249675" genericHeader="method">
2 Self-Enhancement with Posterior
Knowledge
</sectionHeader>
<bodyText confidence="0.998529375">
The self-enhancement system structure is shown in
Figure 1. Our baseline system is set up using
Moses (Koehn et al., 2007), a state-of-the-art
phrase-base SMT open source package. In the fol-
lowings, we detail the approaches to exploiting the
three different kinds of posterior knowledge,
namely, language model, translation model and
word reordering.
</bodyText>
<page confidence="0.979756">
157
</page>
<reference confidence="0.2084645">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<figureCaption confidence="0.997385666666667">
Figure 1: Self-enhancement system structure, where
TM is translation model, LM is language model, and
RM is reordering model.
</figureCaption>
<subsectionHeader confidence="0.886605">
2.1 Language Model
</subsectionHeader>
<bodyText confidence="0.994950266666667">
We consider self-enhancement of language model
as a language model adaptation problem similar to
(Nakajima et al., 2002). The original monolingual
target training data is regarded as general-domain
data while the test data as a domain-specific data.
Obviously, the real domain-specific target data
(test data) is unavailable for training. In this work,
the N-best hypotheses of the test set are used as a
quasi-corpus to train a language model. This new
language model trained on the quasi-corpus is then
used together with the language model trained on
the general-domain data (original training data) to
produce a new list of N-best hypotheses under our
self-enhancement framework. The feature function
of the language model (1 , 1 )
</bodyText>
<equation confidence="0.4160895">
hLM f e is a mixture
J I
</equation>
<bodyText confidence="0.934031">
model of the two language models as in Equation 1.
</bodyText>
<equation confidence="0.970875">
hLM f e = λ hTLM e + λ hQLM e (1)
( 1 , 1 ) 1 ( 1 ) 2 ( 1 )
J I I I
</equation>
<bodyText confidence="0.984964">
where f 1 is the source language words string,
</bodyText>
<equation confidence="0.978288">
J
e is the target language words string, TLM is the
I
1
</equation>
<bodyText confidence="0.998992285714286">
language model trained on target training data, and
QLM is on the quasi-corpus of N-best hypotheses.
The mixture model exploits multiple language
models with weights λ1 and λ2 being optimized
together with other feature functions. The proce-
dure for self-enhancement of the language model is
as follows.
</bodyText>
<listItem confidence="0.999736">
1. Run decoding and extract N-best hypotheses.
2. Train a new language model (QLM) on the N-
best hypotheses.
3. Optimize the weights of the decoder which uses
both original LM (TLM) and the new LM
(QLM).
4. Repeat step 1-3 for a fixed number of iterations.
</listItem>
<subsectionHeader confidence="0.566094">
2.2 Translation Model
</subsectionHeader>
<bodyText confidence="0.92715929032258">
In general, we can safely assume that for a given
source input, phrase-pairs that appeared in the N-
best hypotheses are better than those that did not.
We call the former “good phrase-pairs” and the
later “bad phrase-pairs” for the given source input.
Hypothetically, we can reinforce the translation
model by appending the “good phrase-pairs” to the
original phrase table and changing the probability
space of the translation model, as phrase-based
translation probabilities are estimated using rela-
tive frequencies. The new direct phrase-based
translation probabilities are computed as follows:
% %
where f is the source language phrase, e% is the
target language phrase, Ntrain (.) is the frequencies
observed in the training data, and Nnbest (.) is the
frequencies observed in the N-best hypotheses. For
those phrase-pairs that did not appear in the N-best
hypotheses list (“bad phrase-pairs”),
Nnbest (f % , e % )
%
equals 0, but the marginal count of f is increased
%
by Nnbest (f ) , in this way the phrase-based transla-
tion probabilities of “bad phrase-pairs” degraded
when compared with the corresponding probabili-
ties in the original translation model, and that of
“good phrase-pairs” increased, hence improve the
translation model.
The procedure for translation model self-
enhancement can be summarized as follows.
</bodyText>
<listItem confidence="0.9994134">
1. Run decoding and extract N-best hypotheses.
2. Extract “good phrase-pairs” according to the
hypotheses’ phrase-alignment information and
append them to the original phrase table to gen-
erate a new phrase table.
3. Score the new phrase table to create a new
translation model.
4. Optimize the weights of the decoder with the
above new translation model.
5. Repeat step 1-4 for a fixed number of iterations.
</listItem>
<subsectionHeader confidence="0.998085">
2.3 Word Reordering
</subsectionHeader>
<bodyText confidence="0.99688">
Some previous work (Costa-jussà and Fonollosa,
2006; Li et al., 2007) have shown that reordering a
source sentence to match the word order in its cor-
</bodyText>
<figure confidence="0.850323230769231">
p
j  |f) = Ntrain N f N f
( )
% + ( )
%
train nbest
%
t
f
(f,e)+Nnbes
(
,e)
(2)
</figure>
<page confidence="0.975563">
158
</page>
<bodyText confidence="0.9988181875">
responding target sentence can produce better
translations for a phrase-based SMT system. We
bring this idea forward to our word reordering self-
enhancement framework, which similarly trans-
lates a source sentence (S) to target sentence (T) in
two stages: S → S′ → T , where S′ is the reor-
dered source sentence.
The phrase-alignment information in each hy-
pothesis indicates the word reordering for source
sentence. We select the word reordering with the
highest posterior probability as the best word reor-
dering for a given source sentence. Word re-
orderings from different phrase segmentation but
with same word surface order are merged. The
posterior probabilities of the word re-orderings are
computed as in Equation 3.
</bodyText>
<equation confidence="0.996122076923077">
Nr( )
J
1
p r f = (3)
(  |)
J J
1 1
N
hyp
where (1 )
N r is the count of word reordering 1
J r ,
J
</equation>
<bodyText confidence="0.9497116">
and Nhyp is the number of N-best hypotheses.
The words of the source sentence are then reor-
dered according to their indices in the best selected
word reordering r1J . The procedure for self-
enhancement of word reordering is as follows.
</bodyText>
<listItem confidence="0.99918825">
1. Run decoding and extract N-best hypotheses.
2. Select the best word re-orderings according to
the phrase-alignment information.
3. Reorder the source sentences according to the
selected word reordering.
4. Optimize the weights of the decoder with the
reordered source sentences.
5. Repeat step 1-4 for a fixed number of iterations.
</listItem>
<sectionHeader confidence="0.900483" genericHeader="method">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999705727272727">
Experiments on Chinese-to-English NIST transla-
tion tasks were carried out on the FBIS1 corpus.
We used NIST 2002 MT evaluation test set as our
development set, and the NIST 2003, 2005 test sets
as our test sets as shown in Table 1.
We determine the number of iteration empiri-
cally by setting it to 10. We then observe the
BLEU score on the development set for each itera-
tion. The iteration number which achieved the best
BLEU score on development set is selected as the
iteration number of iterations for the test set.
</bodyText>
<table confidence="0.95233625">
1 LDC2003E14
Data set type #Running words
Chinese English
train parallel 7.0M 8.9M
monolingual - 61.5M
NIST 02 dev 23.2K 108.6K
NIST 03 test 25.8K 116.5K
NIST 05 test 30.5K 141.9K
</table>
<tableCaption confidence="0.993079333333333">
Table 1: Statistics of training, dev and test sets. Evalua-
tion sets of NIST campaigns include 4 references: total
numbers of running words are provided in the table.
</tableCaption>
<table confidence="0.999655666666667">
System #iter. NIST 02 NIST 03 NIST 05
Base - 27.67 26.68 24.82
TM 4 27.87 26.95 25.05
LM 6 27.96 27.06 25.07
WR 6 27.99 27.04 25.11
Comb 7 28.45 27.35 25.46
</table>
<tableCaption confidence="0.93106825">
Table 2: BLEU% scores of five systems: decoder (Base),
self-enhancement on translation model (TM), language
model (LM), word reordering (WR) and the combina-
tion of TM, LM and WR (Comb).
</tableCaption>
<bodyText confidence="0.9996896">
Further experiments also suggested that, in this
experiment scenario, setting the size of N-best list
to 3,000 arrives at the greatest performance im-
provements. Our evaluation metric is BLEU (Pap-
ineni et al., 2002). The translation performance is
reported in Table 2, where the column “#iter.” re-
fers to the iteration number where the system
achieved the best BLEU score on development set.
Compared with the baseline (“Base” in Table 2),
all three self-enhancement methods (“TM”, “LM”,
and “WR” in Table 2) consistently improved the
performance. In general, absolute gains of 0.23-
0.38 BLEU score were obtained for each method
on two test sets. While comparing the performance
among all three methods, we can see that they
achieved very similar improvement. Combining
the three methods showed further gains in BLEU
score. Totally, the combined system outperformed
the baseline by 0.67 BLEU score on NIST’03, and
0.64 on NIST’05 test set, respectively.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.987897">
As posterior knowledge applied in our models are
posterior probabilities, the main difference be-
tween our work and all previous work is the use of
knowledge source, where we derive knowledge
from the N-best hypotheses generated from previ-
ous iteration.
</bodyText>
<page confidence="0.997594">
159
</page>
<bodyText confidence="0.999196571428571">
Comparing the work of (Nakajima et al., 2002),
there is a slight difference between the two models.
Nakajima et al. used only 1-best hypothesis, while
we use N-best hypotheses of test set as the quasi-
corpus to train the language model.
In the work of (Costa-jussà and Fonollosa, 2006;
Li et al., 2007) which similarly translates a source
sentence (S) to target sentence (T) in two stages:
S → S′ → T , they derive S′ from training data;
while we obtain S′ based on the occurrence fre-
quency, i.e. posterior probability of each source
word reordering in the N-best hypotheses list.
An alternative solution for enhancing the trans-
lation model is through self-training (Ueffing,
2006; Ueffing et al., 2007) which re-trains the
source-target N-best hypotheses together with the
original training data, and thus differs from ours in
the way of new phrase pairs extraction. We only
supplement those phrase-pairs appeared in the N-
best hypotheses to the original phrase table. Fur-
ther experiment showed that improvement ob-
tained by self-training method is not as consistent
on both development and test sets as that by our
method. One possible reason is that in self-training,
the entire translation model is adjusted with the
addition of new phrase-pairs extracted from the
source-target N-best hypotheses, and hence the
effect is less predictable.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999992578947368">
To take advantage of the N-best hypotheses, we
proposed schemes in a re-decoding framework and
made use of the posterior knowledge learned from
the N-best hypotheses to improve a phrase-based
SMT system. The posterior knowledge include
posterior probabilities for target n-grams, transla-
tion phrase-pairs and source word re-orderings,
which in turn improve the language model, transla-
tion model, and word reordering respectively.
Experiments were based on the state-of-the-art
phrase-based decoder and carried out on NIST
Chinese-to-English task. It has been shown that all
three methods improved the performance. More-
over, the combination of all three strategies outper-
forms each individual method and significantly
outperforms the baseline. We demonstrated that
the SMT system can be self-enhanced by exploit-
ing useful feedback from the N-best hypotheses
which are generated by itself.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789173913044">
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003.
Confidence estimation for machine translation. Final
report, JHU/CLSP Summer Workshop.
B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M.
Federico. 2005. The ITC-irst SMT System for
IWSLT-2005. In Proceeding of IWSLT-2005, pp.98-
104, Pittsburgh, USA, October.
M. R. Costa-jussà, J. A. R. Fonollosa. 2006. Statistical
Machine Reordering. In Proceeding of EMNLP 2006.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.
Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E.
Herbst. 2007. Moses: Open Source Toolkit for Statis-
tical Machine Translation. In Proceedings of ACL-
2007, pp. 177-180, Prague, Czech Republic.
C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan.
2007. A Probabilistic Approach to Syntax-based Re-
ordering for Statistical Machine Translation. In Pro-
ceedings of ACL-2007. Prague, Czech Republic.
H. Nakajima, H. Yamamoto, T. Watanabe. 2002. Lan-
guage model adaptation with additional text gener-
ated by machine translation. In Proceedings of
COLING-2002. Volume 1, Pages: 1-7. Taipei.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu, 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceeding of ACL-2002, pp.
311-318.
N. Ueffing. 2006. Using Monolingual Source-Language
Data to Improve MT Performance. In Proceedings of
IWSLT 2006. Kyoto, Japan. November 27-28.
N. Ueffing, K. Macherey, and H. Ney. 2003. Confi-
dence Measures for Statistical Machine Translation.
In Proceeding of MT Summit IX, pages 394–401,
New Orleans, LA, September.
N. Ueffing, G. Haffari, A. Sarkar. 2007. Transductive
learning for statistical machine translation. In Pro-
ceedings of ACL-2007, Prague.
F. Wessel. 2002. Word Posterior Probabilities for Large
Vocabulary Continuous Speech Recognition. Ph.D.
thesis, RWTH Aachen University. Aachen, Germany,
January.
R. Zens and H. Ney. 2006. N-gram Posterior Probabili-
ties for Statistical Machine Translation. In Proceed-
ings of the HLT-NAACL Workshop on SMT, pp. 72-
77, NY.
</reference>
<page confidence="0.997455">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468184">
<title confidence="0.999769">Exploiting N-best Hypotheses for SMT Self-Enhancement</title>
<author confidence="0.999032">Min Zhang Chen</author>
<author confidence="0.999032">Aiti Aw Li</author>
<affiliation confidence="0.99986">Department of Human Language Technology Institute for Infocomm Research</affiliation>
<address confidence="0.997271">21 Heng Mui Keng Terrace, 119613, Singapore</address>
<email confidence="0.982013">bxchen@i2r.a-star.edu.sg</email>
<email confidence="0.982013">mzhang@i2r.a-star.edu.sg</email>
<email confidence="0.982013">aaiti@i2r.a-star.edu.sg</email>
<email confidence="0.982013">hli@i2r.a-star.edu.sg</email>
<abstract confidence="0.971496842105263">Word and n-gram posterior probabilities estimated on N-best hypotheses have been used to improve the performance of statistical machine translation (SMT) in a rescoring framework. In this paper, we extend the idea to estimate the posterior probabilities on N-best hypotheses for translation phrase-pairs, target language n-grams, and source word reorderings. The SMT system is self-enhanced with the posterior knowledge learned from Nbest hypotheses in a re-decoding framework. Experiments on NIST Chinese-to-English task show performance improvements for all the strategies. Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 BLEU score on NIST-2003 set, and 0.64 on NIST- 2005 set, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>157--160</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157–160,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for machine translation. Final report, JHU/CLSP Summer Workshop.</title>
<date>2003</date>
<contexts>
<context position="2185" citStr="Blatz et al. (2003)" startWordPosition="313" endWordPosition="316">ide useful feedback to the MT system as the first decoding has discarded many undesirable translation candidates. Thus, the knowledge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this pape</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Confidence estimation for machine translation. Final report, JHU/CLSP Summer Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>R Cattoni</author>
<author>N Bertoldi</author>
<author>M Cettolo</author>
<author>M Federico</author>
</authors>
<date>2005</date>
<booktitle>The ITC-irst SMT System for IWSLT-2005. In Proceeding of IWSLT-2005,</booktitle>
<pages>98--104</pages>
<location>Pittsburgh, USA,</location>
<contexts>
<context position="2315" citStr="Chen et al. (2005)" startWordPosition="334" endWordPosition="337">ge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from th</context>
</contexts>
<marker>Chen, Cattoni, Bertoldi, Cettolo, Federico, 2005</marker>
<rawString>B. Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M. Federico. 2005. The ITC-irst SMT System for IWSLT-2005. In Proceeding of IWSLT-2005, pp.98-104, Pittsburgh, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<date>2006</date>
<booktitle>Statistical Machine Reordering. In Proceeding of EMNLP</booktitle>
<marker>Costa-jussà, Fonollosa, 2006</marker>
<rawString>M. R. Costa-jussà, J. A. R. Fonollosa. 2006. Statistical Machine Reordering. In Proceeding of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL2007,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of ACL2007, pp. 177-180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-H Li</author>
<author>M Li</author>
<author>D Zhang</author>
<author>M Li</author>
<author>M Zhou</author>
<author>Y Guan</author>
</authors>
<title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007.</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan. 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In Proceedings of ACL-2007. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakajima</author>
<author>H Yamamoto</author>
<author>T Watanabe</author>
</authors>
<title>Language model adaptation with additional text generated by machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-2002.</booktitle>
<volume>1</volume>
<pages>1--7</pages>
<location>Taipei.</location>
<marker>Nakajima, Yamamoto, Watanabe, 2002</marker>
<rawString>H. Nakajima, H. Yamamoto, T. Watanabe. 2002. Language model adaptation with additional text generated by machine translation. In Proceedings of COLING-2002. Volume 1, Pages: 1-7. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceeding of ACL-2002,</booktitle>
<pages>311--318</pages>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu, 2002. BLEU: a method for automatic evaluation of machine translation. In Proceeding of ACL-2002, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
</authors>
<title>Using Monolingual Source-Language Data to Improve MT Performance.</title>
<date>2006</date>
<booktitle>In Proceedings of IWSLT 2006. Kyoto,</booktitle>
<location>Japan.</location>
<marker>Ueffing, 2006</marker>
<rawString>N. Ueffing. 2006. Using Monolingual Source-Language Data to Improve MT Performance. In Proceedings of IWSLT 2006. Kyoto, Japan. November 27-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>K Macherey</author>
<author>H Ney</author>
</authors>
<title>Confidence Measures for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceeding of MT Summit IX,</booktitle>
<pages>394--401</pages>
<location>New Orleans, LA,</location>
<contexts>
<context position="2211" citStr="Ueffing et al. (2003)" startWordPosition="318" endWordPosition="322">he MT system as the first decoding has discarded many undesirable translation candidates. Thus, the knowledge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the </context>
</contexts>
<marker>Ueffing, Macherey, Ney, 2003</marker>
<rawString>N. Ueffing, K. Macherey, and H. Ney. 2003. Confidence Measures for Statistical Machine Translation. In Proceeding of MT Summit IX, pages 394–401, New Orleans, LA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>G Haffari</author>
<author>A Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-2007,</booktitle>
<location>Prague.</location>
<contexts>
<context position="2533" citStr="Ueffing et al., 2007" startWordPosition="366" endWordPosition="369">ed to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, in an effort to enhance the language model, translation model, and source word reordering under a re-decoding framework of any phrase-based SMT system. 2 Self-Enhancement with Posterior Knowledge T</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>N. Ueffing, G. Haffari, A. Sarkar. 2007. Transductive learning for statistical machine translation. In Proceedings of ACL-2007, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wessel</author>
</authors>
<title>Word Posterior Probabilities for Large Vocabulary Continuous Speech Recognition.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen University.</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="2112" citStr="Wessel, 2002" startWordPosition="303" endWordPosition="304">selection in the second pass. These N-best hypotheses can also provide useful feedback to the MT system as the first decoding has discarded many undesirable translation candidates. Thus, the knowledge captured in the N-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the</context>
</contexts>
<marker>Wessel, 2002</marker>
<rawString>F. Wessel. 2002. Word Posterior Probabilities for Large Vocabulary Continuous Speech Recognition. Ph.D. thesis, RWTH Aachen University. Aachen, Germany, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-gram Posterior Probabilities for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on SMT,</booktitle>
<pages>72--77</pages>
<contexts>
<context position="2336" citStr="Zens and Ney (2006)" startWordPosition="338" endWordPosition="341">-best hypotheses, such as posterior probabilities for words, n-grams, phrase-pairs, and source word reorderings, etc. is more compatible with the source sentences and thus could potentially be used to improve the translation performance. Word posterior probabilities estimated from the N-best hypotheses have been widely used for confidence measure in automatic speech recognition (Wessel, 2002) and have also been adopted into machine translation. Blatz et al. (2003) and Ueffing et al. (2003) used word posterior probabilities to estimate the confidence of machine translation. Chen et al. (2005), Zens and Ney (2006) reported performance improvements by computing target ngrams posterior probabilities estimated on the Nbest hypotheses in a rescoring framework. Transductive learning method (Ueffing et al., 2007) which repeatedly re-trains the generated sourcetarget N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In this paper, we further exploit the potential of the N-best hypotheses and propose several schemes to derive the posterior knowledge from the N-best hypotheses, </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. N-gram Posterior Probabilities for Statistical Machine Translation. In Proceedings of the HLT-NAACL Workshop on SMT, pp. 72-77, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>