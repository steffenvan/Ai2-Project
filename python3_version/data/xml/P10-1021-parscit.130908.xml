<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.9976785">
Syntactic and Semantic Factors in Processing Difficulty:
An Integrated Measure
</title>
<author confidence="0.9993">
Jeff Mitchell, Mirella Lapata, Vera Demberg and Frank Keller
</author>
<affiliation confidence="0.9281025">
University of Edinburgh
Edinburgh, United Kingdom
</affiliation>
<email confidence="0.844095">
jeff.mitchell@ed.ac.uk, mlap@inf.ed.ac.uk,
v.demberg@ed.ac.uk, keller@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.996442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999891875">
The analysis of reading times can pro-
vide insights into the processes that under-
lie language comprehension, with longer
reading times indicating greater cognitive
load. There is evidence that the language
processor is highly predictive, such that
prior context allows upcoming linguistic
material to be anticipated. Previous work
has investigated the contributions of se-
mantic and syntactic contexts in isolation,
essentially treating them as independent
factors. In this paper we analyze reading
times in terms of a single predictive mea-
sure which integrates a model of seman-
tic composition with an incremental parser
and a language model.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932902777778">
Psycholinguists have long realized that language
comprehension is highly incremental, with readers
and listeners continuously extracting the meaning
of utterances on a word-by-word basis. As soon
as they encounter a word in a sentence, they inte-
grate it as fully as possible into a representation
of the sentence thus far (Marslen-Wilson 1973;
Konieczny 2000; Tanenhaus et al. 1995; Sturt and
Lombardo 2005). Recent research suggests that
language comprehension can also be highly pre-
dictive, i.e., comprehenders are able to anticipate
upcoming linguistic material. This is beneficial as
it gives them more time to keep up with the in-
put, and predictions can be used to compensate for
problems with noise or ambiguity.
Two types of prediction have been observed in
the literature. The first type is semantic predic-
tion, as evidenced in semantic priming: a word
that is preceded by a semantically related prime
or a semantically congruous sentence fragment is
processed faster (Stanovich and West 1981; van
Berkum et al. 1999; Clifton et al. 2007). Another
example is argument prediction: listeners are able
to launch eye-movements to the predicted argu-
ment of a verb before having encountered it, e.g.,
they will fixate an edible object as soon as they
hear the word eat (Altmann and Kamide 1999).
The second type of prediction is syntactic predic-
tion. Comprehenders are faster at naming words
that are syntactically compatible with prior con-
text, even when they bear no semantic relationship
to the context (Wright and Garrett 1984). Another
instance of syntactic prediction has been reported
by Staub and Clifton (2006): following the word
either, readers predict or and the complement that
follows it, and process it faster compared to a con-
trol condition without either.
Thus, human language processing takes advan-
tage of the constraints imposed by the preceding
semantic and syntactic context to derive expecta-
tions about the upcoming input. Much recent work
has focused on developing computational mea-
sures of these constraints and expectations. Again,
the literature is split into syntactic and semantic
models. Probably the best known measure of syn-
tactic expectation is surprisal (Hale 2001) which
can be coarsely defined as the negative log proba-
bility of word wt given the preceding words, typ-
ically computed using a probabilistic context-free
grammar.
Modeling work on semantic constraint focuses
on the degree to which a word is related to its
preceding context. Pynte et al. (2008) use La-
tent Semantic Analysis (LSA, Landauer and Du-
mais 1997) to assess the degree of contextual con-
straint exerted on a word by its context. In this
framework, word meanings are represented as vec-
tors in a high dimensional space and distance in
this space is interpreted as an index of process-
ing difficulty. Other work (McDonald and Brew
2004) models contextual constraint in information
theoretic terms. The assumption is that words
carry prior semantic expectations which are up-
dated upon seeing the next word. Expectations are
represented by a vector of probabilities which re-
flects the likely location in semantic space of the
upcoming word.
The measures discussed above are typically
computed automatically on real-language corpora
using data-driven methods and their predictions
are verified through analysis of eye-movements
that people make while reading. Ample evidence
</bodyText>
<page confidence="0.984319">
196
</page>
<note confidence="0.9435745">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196–206,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999817696969697">
(Rayner 1998) demonstrates that eye-movements
are related to the moment-to-moment cognitive ac-
tivities of readers. They also provide an accurate
temporal record of the on-line processing of nat-
ural language, and through the analysis of eye-
movement measurements (e.g., the amount of time
spent looking at a word) can give insight into the
processing difficulty involved in reading.
In this paper, we investigate a model of predic-
tion that is incremental and takes into account syn-
tactic as well as semantic constraint. The model
essentially integrates the predictions of an incre-
mental parser (Roark 2001) together with those
of a semantic space model (Mitchell and Lap-
ata 2009). The latter creates meaning representa-
tions compositionally, and therefore builds seman-
tic expectations for word sequences (e.g., phrases,
sentences, even documents) rather than isolated
words. Some existing models of sentence process-
ing integrate semantic information into a prob-
abilistic parser (Narayanan and Jurafsky 2002;
Pad´o et al. 2009); however, the semantic compo-
nent of these models is limited to semantic role in-
formation, rather than attempting to build a full se-
mantic representation for a sentence. Furthermore,
the models of Narayanan and Jurafsky (2002) and
Pad´o et al. (2009) do not explicitly model pre-
diction, but rather focus on accounting for garden
path effects. The proposed model simultaneously
captures semantic and syntactic effects in a sin-
gle measure which we empirically show is predic-
tive of processing difficulty as manifested in eye-
movements.
</bodyText>
<sectionHeader confidence="0.836964" genericHeader="method">
2 Models of Processing Difficulty
</sectionHeader>
<bodyText confidence="0.999991862068966">
As described in Section 1, reading times provide
an insight into the various cognitive activities that
contribute to the overall processing difficulty in-
volved in comprehending a written text. To quan-
tify and understand the overall cognitive load asso-
ciated with processing a word in context, we will
break that load down into a sum of terms repre-
senting distinct computational costs (semantic and
syntactic). For example, surprisal can be thought
of as measuring the cost of dealing with unex-
pected input. When a word conforms to the lan-
guage processor’s expectations, surprisal is low,
and the cognitive load associated with processing
that input will also be low. In contrast, unexpected
words will have a high surprisal and a high cogni-
tive cost.
However, high-level syntactic and semantic fac-
tors are only one source of cognitive costs. A siz-
able proportion of the variance in reading times is
accounted for by costs associated with low-level
features of the stimuli, e.g.. relating to orthography
and eye-movement control (Rayner 1998). In ad-
dition, there may also be costs associated with the
integration of new input into an incremental rep-
resentation. Dependency Locality Theory (DLT,
Gibson 2000) is essentially a distance-based mea-
sure of the amount of processing effort required
when the head of a phrase is integrated with its
syntactic dependents. We do not consider integra-
tion costs here (as they have not been shown to
correlate reliably with reading times; see Demberg
and Keller 2008 for details) and instead focus on
the costs associated with semantic and syntactic
constraint and low-level features, which appear to
make the most substantial contributions.
In the following subsections we describe the
various features which contribute to the process-
ing costs of a word in context. We begin by look-
ing at the low-level costs and move on to con-
sider the costs associated with syntactic and se-
mantic constraint. For readers unfamiliar with the
methodology involved in modeling eye-tracking
data, we note that regression analysis (or the more
general mixed effects models) is typically used to
study the relationship between dependent and in-
dependent variables. The independent variables
are the various costs of processing effort and
the dependent variables are measurements of eye-
movements, three of which are routinely used in
the literature: first fixation duration (the duration
of the first fixation on a word regardless of whether
it is the first fixation on a word or the first of mul-
tiple fixations on the same word), first pass dura-
tion, also known as gaze duration, (the sum of all
fixations made on a word prior to looking at an-
other word), and total reading time (the sum of
all fixations on a word including refixations after
moving on to other words).
</bodyText>
<subsectionHeader confidence="0.974302">
2.1 Low-level Costs
</subsectionHeader>
<bodyText confidence="0.987602785714286">
Low-level features include word frequency (more
frequent words are read faster), word length
(shorter words are read faster), and the position
of the word in the sentence (later words are read
faster). Oculomotor variables have also been
found to influence reading times. These include
previous fixation (indicating whether or not the
previous word has been fixated), launch distance
(how many characters intervene between the cur-
rent fixation and the previous fixation), and land-
ing position (which letter in the word the fixation
landed on).
Information about the sequential context of a
word can also influence reading times. Mc-
</bodyText>
<page confidence="0.997152">
197
</page>
<bodyText confidence="0.9998415">
Donald and Shillcock (2003) show that forward
and backward transitional probabilities are pre-
dictive of first fixation and first pass durations:
the higher the transitional probability, the shorter
the fixation time. Backward transitional prob-
ability is essentially the conditional probabil-
ity of a word given its immediately preceding
word, P(wk|wk−1). Analogously, forward proba-
bility is the conditional probability of the current
word given the next word, P(wk|wk+1).
</bodyText>
<subsectionHeader confidence="0.999363">
2.2 Syntactic Constraint
</subsectionHeader>
<bodyText confidence="0.999758409090909">
As mentioned earlier, surprisal (Hale 2001; Levy
2008) is one of the best known models of process-
ing difficulty associated with syntactic constraint,
and has been previously applied to the modeling of
reading times (Demberg and Keller 2008; Ferrara
Boston et al. 2008; Roark et al. 2009; Frank 2009).
The basic idea is that the processing costs relating
to the expectations of the language processor can
be expressed in terms of the probabilities assigned
by some form of language model to the input.
These processing costs are assumed to arise from
the change in the expectations of the language pro-
cessor as new input arrives. If we express these ex-
pectations in terms of a distribution over all possi-
ble continuations of the input seen so far, then we
can measure the magnitude of this change in terms
of the Kullback-Leibler divergence of the old dis-
tribution to the updated distribution. This measure
of processing cost for an input word, wk+1, given
the previous context, w1...wk, can be expressed
straightforwardly in terms of its conditional prob-
ability as:
</bodyText>
<equation confidence="0.997944">
S = −logP(wk+1|w1 ...wk) (1)
</equation>
<bodyText confidence="0.950668607142857">
That is, the processing cost for a word decreases as
its probability increases, with zero processing cost
incurred for words which must appear in a given
context, as these do not result in any change in the
expectations of the language processor.
The original formulation of surprisal (Hale
2001) used a probabilistic parser to calculate these
probabilities, as the emphasis was on the process-
ing costs incurred when parsing structurally am-
biguous garden path sentences.1 Several variants
of calculating surprisal have been developed in the
literature since using different parsing strategies
1While hearing a sentence like The horse raced past the
barn fell (Bever 1970), English speakers are inclined to in-
terpreted horse as the subject of raced expecting the sentence
to end at the word barn. So upon hearing the word fell they
are forced to revise their analysis of the sentence thus far and
adopt a reduced relative reading.
(e.g., left-to-right vs. top-down, PCFGs vs de-
pendency parsing) and different degrees of lexi-
calization (see Roark et al. 2009 for an overview) .
For instance, unlexicalized surprisal can be easily
derived by substituting the words in Equation (1)
with parts of speech (Demberg and Keller 2008).
Surprisal could be also defined using a vanilla
language model that does not take any structural
or grammatical information into account (Frank
2009).
</bodyText>
<subsectionHeader confidence="0.999485">
2.3 Semantic Constraint
</subsectionHeader>
<bodyText confidence="0.999963953488372">
Distributional models of meaning have been com-
monly used to quantify the semantic relation be-
tween a word and its context in computational
studies of lexical processing. These models are
based on the idea that words with similar mean-
ings will be found in similar contexts. In putting
this idea into practice, the meaning of a word is
then represented as a vector in a high dimensional
space, with the vector components relating to the
strength on occurrence of that word in various
types of context. Semantic similarities are then
modeled in terms of geometric similarities within
the space.
To give a concrete example, Latent Semantic
Analysis (LSA, Landauer and Dumais 1997) cre-
ates a meaning representation for words by con-
structing a word-document co-occurrence matrix
from a large collection of documents. Each row in
the matrix represents a word, each column a doc-
ument, and each entry the frequency with which
the word appeared within that document. Because
this matrix tends to be quite large it is often trans-
formed via a singular value decomposition (Berry
et al. 1995) into three component matrices: a ma-
trix of word vectors, a matrix of document vectors,
and a diagonal matrix containing singular values.
Re-multiplying these matrices together using only
the initial portions of each (corresponding to the
use of a lower dimensional spatial representation)
produces a tractable approximation to the original
matrix. In this framework, the similarity between
two words can be easily quantified, e.g., by mea-
suring the cosine of the angle of the vectors repre-
senting them.
As LSA is one the best known semantic space
models it comes as no surprise that it has been
used to analyze semantic constraint. Pynte et al.
(2008) measure the similarity between the next
word and its preceding context under the assump-
tion that high similarity indicates high semantic
constraint (i.e., the word was expected) and analo-
gously low similarity indicates low semantic con-
straint (i.e., the word was unexpected). They oper-
</bodyText>
<page confidence="0.993293">
198
</page>
<bodyText confidence="0.999984844827587">
ationalize preceding contexts in two ways, either
as the word immediately preceding the next word
as the sentence fragment preceding it. Sentence
fragments are represented as the average of the
words they contain independently of their order.
The model takes into account only content words,
function words are of little interest here as they can
be found in any context.
Pynte et al. (2008) analyze reading times on the
French part of the Dundee corpus (Kennedy and
Pynte 2005) and find that word-level LSA similar-
ities are predictive of first fixation and first pass
durations, whereas sentence-level LSA is only
predictive of first pass duration (i.e., for a mea-
sure that includes refixation). This latter finding
is somewhat counterintuitive, one would expect
longer contexts to have an immediate effect as
they are presumably more constraining. One rea-
son why sentence-level influences are only visible
on first pass duration may be due to LSA itself,
which is syntax-blind. Another reason relates to
the way sentential context was modeled as vec-
tor addition (or averaging). The idea of averag-
ing is not very attractive from a linguistic perspec-
tive as it blends the meanings of individual words
together. Ideally, the combination of simple el-
ements onto more complex ones must allow the
construction of novel meanings which go beyond
those of the individual elements (Pinker 1994).
The only other model of semantic constraint we
are aware of is Incremental Contextual Distinc-
tiveness (ICD, McDonald 2000; McDonald and
Brew 2004). ICD assumes that words carry prior
semantic expectations which are updated upon
seeing the next word. Context is represented by
a vector of probabilities which reflects the likely
location in semantic space of the upcoming word.
When the latter is observed, the prior expecta-
tion is updated using a Bayesian inference mecha-
nism to reflect the newly arrived information. Like
LSA, ICD is based on word co-occurrence vectors,
however it does not employ singular value decom-
position, and constructs a word-word rather than a
word-document co-occurrence matrix. Although
this model has been shown to successfully simu-
late single- and multiple-word priming (McDon-
ald and Brew 2004), it failed to predict processing
costs in the Embra eye-tracking corpus (McDon-
ald and Shillcock 2003).
In this work we model semantic constraint us-
ing the representational framework put forward in
Mitchell and Lapata (2008). Their aim is not so
much to model processing difficulty, but to con-
struct vector-based meaning representations that
go beyond individual words. They introduce a
general framework for studying vector composi-
tion, which they formulate as a function f of two
vectors u and v:
</bodyText>
<equation confidence="0.97906">
h = f (u,v) (2)
</equation>
<bodyText confidence="0.998545857142857">
where h denotes the composition of u and v. Dif-
ferent composition models arise, depending on
how f is chosen. Assuming that h is a linear func-
tion of the Cartesian product of u and v allows to
specify additive models which are by far the most
common method of vector combination in the lit-
erature:
</bodyText>
<equation confidence="0.997374">
hi = ui + vi (3)
</equation>
<bodyText confidence="0.999861666666667">
Alternatively, we can assume that h is a linear
function of the tensor product of u and v, and thus
derive models based on multiplication:
</bodyText>
<equation confidence="0.997336">
hi = ui · vi (4)
</equation>
<bodyText confidence="0.999605705882353">
Mitchell and Lapata (2008) show that several ad-
ditive and multiplicative models can be formu-
lated under this framework, including the well-
known tensor products (Smolensky 1990) and cir-
cular convolution (Plate 1995). Importantly, com-
position models are not defined with a specific se-
mantic space in mind, they could easily be adapted
to LSA, or simple co-occurrence vectors, or more
sophisticated semantic representations (e.g., Grif-
fiths et al. 2007), although admittedly some com-
position functions may be better suited for partic-
ular semantic spaces.
Composition models can be straightforwardly
used as predictors of processing difficulty, again
via measuring the cosine of the angle between a
vector w representing the upcoming word and a
vector h representing the words preceding it:
</bodyText>
<equation confidence="0.996027666666667">
w · h
sim(w,h) = (5)
|w||h|
</equation>
<bodyText confidence="0.9997684">
where h is created compositionally, via some (ad-
ditive or multiplicative) function f.
In this paper we evaluate additive and compo-
sitional models in their ability to capture seman-
tic prediction. We also examine the influence of
the underlying meaning representations by com-
paring a simple semantic space similar to Mc-
Donald (2000) against Latent Dirichlet Allocation
(Blei et al. 2003; Griffiths et al. 2007). Specif-
ically, the simpler space is based on word co-
occurrence counts; it constructs the vector repre-
senting a given target word, t, by identifying all the
tokens of t in a corpus and recording the counts of
context words, ci (within a specific window). The
context words, ci, are limited to a set of the n most
</bodyText>
<page confidence="0.994304">
199
</page>
<bodyText confidence="0.999927">
common content words and each vector compo-
nent is given by the ratio of the probability of a ci
given t to the overall probability of ci.
</bodyText>
<equation confidence="0.9389875">
vi = p(ci|t) (6)
p(ci)
</equation>
<bodyText confidence="0.999776393939394">
Despite its simplicity, the above semantic space
(and variants thereof) has been used to success-
fully simulate lexical priming (e.g., McDonald
2000), human judgments of semantic similarity
(Bullinaria and Levy 2007), and synonymy tests
(Pad´o and Lapata 2007) such as those included in
the Test of English as Foreign Language (TOEFL).
LDA is a probabilistic topic model offering an
alternative to spatial semantic representations. It
is similar in spirit to LSA, it also operates on a
word-document co-occurrence matrix and derives
a reduced dimensionality description of words and
documents. Whereas in LSA words are repre-
sented as points in a multi-dimensional space,
LDA represents words using topics. Specifically,
each document in a corpus is modeled as a distri-
bution over K topics, which are themselves char-
acterized as distribution over words. The individ-
ual words in a document are generated by repeat-
edly sampling a topic according to the topic distri-
bution and then sampling a single word from the
chosen topic. Under this framework, word mean-
ing is represented as a probability distribution over
a set of latent topics, essentially a vector whose
dimensions correspond to topics and values to the
probability of the word given these topics. Topic
models have been recently gaining ground as a
more structured representation of word meaning
(Griffiths et al. 2007; Steyvers and Griffiths 2007).
In contrast to more standard semantic space mod-
els where word senses are conflated into a single
representation, topics have an intuitive correspon-
dence to coarse-grained sense distinctions.
</bodyText>
<sectionHeader confidence="0.961319" genericHeader="method">
3 Integrating Semantic Constraint into
Surprisal
</sectionHeader>
<bodyText confidence="0.981248741935484">
The treatment of semantic and syntactic constraint
in models of processing difficulty has been some-
what inconsistent. While surprisal is a theo-
retically well-motivated measure, formalizing the
idea of linguistic processing being highly predic-
tive in terms of probabilistic language models, the
measurement of semantic constraint in terms of
vector similarities lacks a clear motivation. More-
over, the two approaches, surprisal and similarity,
produce mathematically different types of mea-
sures. Formally, it would be preferable to have
a single approach to capturing constraint and the
obvious solution is to derive some form of seman-
tic surprisal rather than sticking with similarity.
This can be achieved by turning a vector model
of semantic similarity into a probabilistic language
model.
There are in fact a number of approaches to de-
riving language models from distributional mod-
els of semantics (e.g., Bellegarda 2000; Coccaro
and Jurafsky 1998; Gildea and Hofmann 1999).
We focus here on the model of Mitchell and La-
pata (2009) which tackles the issue of the compo-
sition of semantic vectors and also integrates the
output of an incremental parser. The core of their
model is based on the product of a trigram model
p(wn|wn−1
n−2) and a semantic component Δ(wn,h)
which determines the factor by which this proba-
bility should be scaled up or down given the prior
semantic context h:
</bodyText>
<equation confidence="0.9997665">
p(wn) = p(wn|wn−1
n−2) · Δ(wn,h) (7)
</equation>
<bodyText confidence="0.998923923076923">
The factor Δ(wn,h) is essentially based on a com-
parison between the vector representing the cur-
rent word wn and the vector representing the prior
history h. Varying the method for constructing
word vectors (e.g., using LDA or a simpler seman-
tic space model) and for combining them into a
representation of the prior context h (e.g., using
additive or multiplicative functions) produces dis-
tinct models of semantic composition.
The calculation of Δ is then based on a weighted
dot product of the vector representing the upcom-
ing word w, with the vector representing the prior
context h:
</bodyText>
<equation confidence="0.9964365">
Δ(w,h) = ∑ wihip(ci) (8)
i
</equation>
<bodyText confidence="0.999930333333333">
As shown in Equation (7) this semantic factor then
modulates the trigram probabilities, to take ac-
count of the effect of the semantic content outside
the n-gram window.
Mitchell and Lapata (2009) show that a com-
bined semantic-trigram language model derived
from this approach and trained on the Wall Street
Journal outperforms a baseline trigram model in
terms of perplexity on a held out set. They also
linearly interpolate this semantic language model
with the output of an incremental parser, which
computes the following probability:
</bodyText>
<equation confidence="0.999609">
p(w|h) = λp1(w|h) + (1− λ)p2(w|h) (9)
</equation>
<bodyText confidence="0.99242025">
where p1(w|h) is computed as in Equation (7)
and p2(w|h) is computed by the parser. Their im-
plementation uses Roark’s (2001) top-down incre-
mental parser which estimates the probability of
</bodyText>
<page confidence="0.970505">
200
</page>
<bodyText confidence="0.999969388888889">
the next word based upon the previous words of
the sentence. These prefix probabilities are calcu-
lated from a grammar, by considering the likeli-
hood of seeing the next word given the possible
grammatical relations representing the prior con-
text.
Equation (9) essentially defines a language
model which combines semantic, syntactic and
n-gram structure, and Mitchell and Lapata (2009)
demonstrate that it improves further upon a se-
mantic language model in terms of perplexity. We
argue that the probabilities from this model give
us a means to model the incrementally and predic-
tivity of the language processor in a manner that
integrates both syntactic and semantic constraints.
Converting these probabilities to surprisal should
result in a single measure of the processing cost as-
sociated with semantic and syntactic expectations.
</bodyText>
<sectionHeader confidence="0.997662" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.992249576086957">
Data The models discussed in the previous sec-
tion were evaluated against an eye-tracking cor-
pus. Specifically, we used the English portion
of the Dundee Corpus (Kennedy and Pynte 2005)
which contains 20 texts taken from The Indepen-
dent newspaper. The corpus consists of 51,502
tokens and 9,776 types in total. It is annotated
with the eye-movement records of 10 English na-
tive speakers, who each read the whole corpus.
The eye-tracking data was preprocessed following
the methodology described in Demberg and Keller
(2008). From this data, we computed total reading
time for each word in the corpus. Our statistical
analyses were based on actual reading times, and
so we only included words that were not skipped.
We also excluded words for which the previous
word had been skipped, and words on which the
normal left-to-right movement of gaze had been
interrupted, i.e., by blinks, regressions, etc. Fi-
nally, because our focus is the influence of seman-
tic context, we selected only content words whose
prior sentential context contained at least two fur-
ther content words. The resulting data set con-
sisted of 53,704 data points, which is about 10%
of the theoretically possible total.2
2The total of all words read by all subjects is 515,020.
The pre-processing recommended by Demberg and Keller’s
(2008) results in a data sets containing 436,000 data points.
Removing non-content words leaves 205,922 data points. It
only makes sense to consider words that were actually fixated
(the eye-tracking measures used are not defined on skipped
words), which leaves 162,129 data points. Following Pynte
et al. (2008), we require that the previous word was fixated,
with 70,051 data points remaining. We exclude words on
which the normal left to right movement of gaze had been
interrupted, e.g., by blinks and regressions, which results in
the final total to 53,704 data points.
Model Implementation All elements of our
model were trained on the BLLIP corpus, a col-
lection of texts from the Wall Street Journal
(years 1987–89). The training corpus consisted of
38,521,346 words. We used a development cor-
pus of 50,006 words and a test corpus of similar
size. All words were converted to lowercase and
numbers were replaced with the symbol (num). A
vocabulary of 20,000 words was chosen and the
remaining tokens were replaced with (unk).
Following Mitchell and Lapata (2009), we con-
structed a simple semantic space based on co-
occurrence statistics from the BLLIP training set.
We used the 2,000 most frequent word types as
contexts and a symmetric five word window. Vec-
tor components were defined as in Equation (6).
We also trained the LDA model on BLLIP, using
the Gibb’s sampling procedure discussed in Grif-
fiths et al. (2007). We experimented with different
numbers of topics on the development set (from 10
to 1,000) and report results on the test set with 100
topics. In our experiments, the hyperparameter a
was initialized to .5, and the b word probabilities
were initialized randomly.
We integrated our compositional models with a
trigram model which we also trained on BLLIP.
The model was built using the SRILM toolkit
(Stolcke 2002) with backoff and Kneser-Ney
smoothing. As our incremental parser we used
Roark’s (2001) parser trained on sections 2–21 of
the Penn Treebank containing 936,017 words. The
parser produces prefix probabilities for each word
of a sentence which we converted to conditional
probabilities by dividing each current probability
by the previous one.
Statistical Analysis The statistical analyses in
this paper were carried out using linear mixed
effects models (LME, Pinheiro and Bates 2000).
The latter can be thought of as generalization of
linear regression that allows the inclusion of ran-
dom factors (such as participants or items) as well
as fixed factors (e.g., word frequency). In our
analyses, we treat participant as a random factor,
which means that our models contain an intercept
term for each participant, representing the individ-
ual differences in the rates at which they read.3
We evaluated the effect of adding a factor to a
model by comparing the likelihoods of the mod-
els with and without that factor. If a c2 test on the
3Other random factors that are appropriate for our anal-
yses are word and sentence; however, due to the large num-
ber of instances for these factors (given that the Dundee cor-
pus contains 51,502 tokens), we were not able to include
them: the model fitting algorithm we used (implemented in
the R package lme4) does not converge for such large models.
</bodyText>
<page confidence="0.993511">
201
</page>
<table confidence="0.8292922">
Model Composition Coefficient
SSS Additive −.03820* **
Multiplicative −.00895***
LDA Additive −.02500***
Multiplicative −.00262***
Table 2: Coefficients of LME models including
simple semantic space (SSS) or Latent Dirichlet
Allocation (LDA) as factors; ***p &lt; .001
Factor Coefficient
Intercept −.011
Word Length .264
Launch Distance .109
Landing Position .612
Word Frequency −.010
Reading Time of Last Word .151
</table>
<tableCaption confidence="0.997883">
Table 1: Coefficients of the baseline LME model
</tableCaption>
<bodyText confidence="0.9837858">
for total reading time
likelihood ratio is significant, then this indicates
that the new factor significantly improves model
fit. We also experimented with adding random
slopes for participant to the model (in addition to
the random intercept); however, this either led to
non-convergence of the model fitting procedure, or
failed to result in an increase in model fit accord-
ing to the likelihood ratio test. Therefore, all mod-
els reported in the rest of this paper contain ran-
dom intercept of participants as the sole random
factor.
Rather than model raw reading times, we model
times on the log scale. This is desirable for a
number of reasons. Firstly, the raw reading times
tend to have a skew distribution and taking logs
produces something closer to normal, which is
preferable for modeling. Secondly, the regres-
sion equation makes more sense on the log scale
as the contribution of each term to raw reading
time is multiplicative rather than additive. That is,
log(t) = Ei Pixi implies t = r1i ePixi. In particular,
the intercept term for each participant now repre-
sents a multiplicative factor by which that partici-
pant is slower or faster.
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999973721311476">
We computed separate mixed effects models for
three dependent variables, namely first fixation du-
ration, first pass duration, and total reading time.
We report results for total times throughout, as
the results of the other two dependent variables
are broadly similar. Our strategy was to first con-
struct a baseline model of low-level factors influ-
encing reading time, and then to take the resid-
uals from that model as the dependent variable
in subsequent analyses. In this way we removed
the effects of low-level factors before investigating
the factors associated with syntactic and semantic
constraint. This avoids problems with collinear-
ity between low-level factors and the factors we
are interested in (e.g., trigram probability is highly
correlated with word frequency). The baseline
model contained the factors word length, word fre-
quency, launch distance, landing position, and the
reading time for the last fixated word, and its pa-
rameter estimates are given in Table 1. To further
reduce collinearity, we also centered all fixed fac-
tors, both in the baseline model, and in the models
fitted on the residuals that we report in the follow-
ing. Note that some intercorrelations remain be-
tween the factors, which we will discuss at the end
of Section 5.
Before investigating whether an integrated
model of semantic and syntactic constraint im-
proves the goodness of fit over the baseline, we ex-
amined the influence of semantic constraint alone.
This was necessary as compositional models have
not been previously used to model processing
difficulty. Besides, replicating Pynte et al.’s
(2008) finding, we were also interested in assess-
ing whether the underlying semantic representa-
tion (simple semantic space or LDA) and com-
position function (additive versus multiplicative)
modulate reading times differentially.
We built an LME model that predicted the resid-
ual reading times of the baseline model using the
similarity scores from our composition models as
factors. We then carried out a x2 test on the like-
lihood ratio of a model only containing the ran-
dom factor and the intercept, and a model also
containing the semantic factor (cosine similarity).
The addition of the semantic factor significantly
improves model fit for both the simple semantic
space and LDA. This result is observed for both
additive and multiplicative composition functions.
Our results are summarized in Table 2 which re-
ports the coefficients of the four LME models fit-
ted against the residuals of the baseline model, to-
gether with the p-values of the x2 test.
Before evaluating our integrated surprisal mea-
sure, we evaluated its components individually in
order to tease their contributions apart. For ex-
ample, it may be the case that syntactic surprisal
is an overwhelmingly better predictor of reading
time than semantic surprisal, however we would
not be able to detect this by simply adding a factor
based on Equation (9) to the baseline model. The
</bodyText>
<page confidence="0.993293">
202
</page>
<table confidence="0.7655825">
Model Composition Coefficient
SSS Additive .00804* **
Multiplicative .00819***
Factor SSS Coef LDA Coef
</table>
<equation confidence="0.981391571428571">
−log(p) .00760*** .00760***
−log(Δ) .03810*** .00622***
log(λ+ (1 − λ) p2
p1) .00953*** .00943***
Add
−log(Δ) .01110*** −.00033
log(λ+ (1− λ) p2
</equation>
<bodyText confidence="0.742656166666667">
p1 ) .00882*** .00133
Table 3: Coefficients of nested LME models with
the components of SSS or LDA surprisal as fac-
tors; only the coefficient of the additional factor at
each step are shown
integrated surprisal measure can be written as:
</bodyText>
<equation confidence="0.925128">
S = −log(λp1 +(1−λ)p2) (10)
</equation>
<bodyText confidence="0.96654075">
Where p2 is the incremental parser probability and
p1 is the product of the semantic component, Δ,
and the trigram probability, p. This can be broken
down into the sum of two terms:
</bodyText>
<equation confidence="0.9922255">
S = −log(p1)−log(λ +(1−λ) p2 ) (11)
p1
</equation>
<bodyText confidence="0.798024">
Since the first term,−log(p1) is itself a product it
can also be broken down further:
</bodyText>
<equation confidence="0.989029">
S = −log(p)−log(Δ)−log(λ+(1−λ) p2) (12)
p1
</equation>
<bodyText confidence="0.95211892">
Thus, to evaluate the contribution of the three
components to the integrated surprisal measure we
fitted nested LME models, i.e., we entered these
terms one at a time into a mixed effects model
and tested the significance of the improvement in
model fit for each additional term.
We again start with an LME model that only
contains the random factor and the intercept, with
the residuals of the baseline models as the depen-
dent variable. Considering the trigram model first,
we find that adding this factor to the model gives a
significant improvement in fit. Also adding the se-
mantic component (−log(Δ)) improves fit further,
both for additive and multiplicative composition
functions using a simple semantic space. Finally,
the addition of the parser probabilities (log(λ +
(1−λ) p2
p1 )) again improves model fit significantly.
As far as LDA is concerned, the additive model
significantly improves model fit, whereas the mul-
tiplicative one does not. These results mirror
the findings of Mitchell and Lapata (2009), who
report that a multiplicative composition function
produced the lowest perplexity for the simple se-
mantic space model, whereas an additive function
gave the best perplexity for the LDA space. Ta-
ble 3 lists the coefficients for the nested models for
LDA Additive .00817***
Multiplicative .00640***
Table 4: Coefficients of LME models with inte-
grated surprisal measure (based on SSS or LDA)
as factor
all four variants of our semantic constraint mea-
sure.
Finally, we built a separate LME model where
we added the integrated surprisal measure (see
Equation (9)) to the model only containing the ran-
dom factor and the intercept (see Table 4). We
did this separately for all four versions of the in-
tegrated surprisal measure (SSS, LDA; additive,
multiplicative). We find that model fit improved
significantly all versions of integrated surprisal.
One technical issue that remains to be discussed
is collinearity, i.e., intercorrelations between the
factors in a model. The presence of collinearity
is problematic, as it can render the model fitting
procedure unstable; it can also affect the signifi-
cance of individual factors. As mentioned in Sec-
tion 4 we used two techniques to reduce collinear-
ity: residualizing and centering. Table 5 gives
an overview of the correlation coefficients for all
pairs of factors. It becomes clear that collinear-
ity has mostly been removed; there is a remaining
relationship between word length and word fre-
quency, which is expected as shorter words tend to
be more frequent. This correlation is not a prob-
lem for our analysis, as it is confined to the base-
line model. Furthermore, word frequency and tri-
gram probability are highly correlated. Again this
is expected, given that the frequencies of unigrams
and higher-level n-grams tend to be related. This
correlation is taken care of by residualizing, which
isolates the two factors: word frequency is part
of the baseline model, while trigram probability is
part of the separate models that we fit on the resid-
uals. All other correlations are small (with coeffi-
cients of .27 or less), with one exception: there is
a high correlation between the−log(Δ) term and
the log(λ + (1 − λ) p2
p1) term in the multiplicative
LDA model. This collinearity issue may explain
the absence of a significant improvement in model
fit when these two terms are added to the baseline
(see Table 3).
Mult
</bodyText>
<page confidence="0.991296">
203
</page>
<table confidence="0.989711772727273">
SSS
Add
SSS
Mult
LDA
Add
LDA
Mult
Factor Len Freq −l(p) −l(Δ)
Frequency −.310
−log(p) .230 −.700
−log(Δ) .016 −.120 .025 .065
log(λ + (1− λ) p2 −.270
p1 ) .024 .036
−log(Δ) −.015 −.110 .035 .160
log(λ+ (1− λ) p2 −.260
p1 ) .020 .028
−log(Δ) −.024 −.130 .046 .030
log(λ+ (1− λ) p2 −.250
p1 ) .005 .014
−log(Δ) −.120 .006 −.046
log(λ+ (1 −λ)p2 p1)−.089 −.005 −.180 .740
</table>
<tableCaption confidence="0.999755">
Table 5: Intercorrelations between model factors
</tableCaption>
<sectionHeader confidence="0.99686" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999859376344086">
In this paper we investigated the contributions of
syntactic and semantic constraint in modeling pro-
cessing difficulty. Our work departs from previ-
ous approaches in that we propose a single mea-
sure which integrates syntactic and semantic fac-
tors. Evaluation on an eye-tracking corpus shows
that our measure predicts reading time better than
a baseline model that captures low-level factors
in reading (word length, landing position, etc.).
Crucially, we were able to show that the semantic
component of our measure improves reading time
predictions over and above a model that includes
syntactic measures (based on a trigram model and
incremental parser). This means that semantic
costs are a significant predictor of reading time in
addition to the well-known syntactic surprisal.
An open issue is whether a single, integrated
measure (as evaluated in Table 4) fits the eye-
movement data significantly better than separate
measures for trigram, syntactic, and semantic sur-
prisal (as evaluated in Table 3. However, we are
not able to investigate this hypothesis: our ap-
proach to testing the significance of factors re-
quires nested models; the log-likelihood test (see
Section 4) is only able to establish whether adding
a factor to a model improves its fit; it cannot com-
pare models with disjunct sets of factors (such as
a model containing the integrated surprisal mea-
sure and one containing the three separate ones).
However, we would argue that a single, integrated
measure that captures human predictive process-
ing is preferable over a collection of separate mea-
sures. It is conceptually simpler (as it is more par-
simonious), and is also easier to use in applica-
tions (such as readability prediction). Finally, an
integrated measure requires less parameters; our
definition of surprisal in 12 is simply the sum of
the trigram, syntactic, and semantic components.
An LME model containing separate factors, on the
other hand, requires a coefficient for each of them,
and thus has more parameters.
In evaluating our model, we adopted a broad
coverage approach using the reading time data
from a naturalistic corpus rather than artificially
constructed experimental materials. In doing so,
we were able to compare different syntactic and
semantic costs on the same footing. Previous
analyses of semantic constraint have been con-
ducted on different eye-tracking corpora (Dundee
and Embra Corpus) and on different languages
(English, French). Moreover, comparisons of the
individual contributions of syntactic and semantic
factors were generally absent from the literature.
Our analysis showed that both of these factors can
be captured by our integrated surprisal measure
which is uniformly probabilistic and thus prefer-
able to modeling semantic and syntactic costs dis-
jointly using a mixture of probabilistic and non-
probabilistic measures.
An interesting question is which aspects of se-
mantics our model is able to capture, i.e., why
does the combination of LSA or LDA representa-
tions with an incremental parser yield a better fit of
the behavioral data. In the psycholinguistic liter-
ature, various types of semantic information have
been investigated: lexical semantics (word senses,
selectional restrictions, thematic roles), senten-
tial semantics (scope, binding), and discourse se-
mantics (coreference and coherence); see Keller
(2010) of a detailed discussion. We conjecture that
our model is mainly capturing lexical semantics
(through the vector space representation of words)
and sentential semantics (through the multiplica-
tion or addition of words). However, discourse
coreference effects (such as the ones reported by
Altmann and Steedman (1988) and much subse-
quent work) are probably not amenable to a treat-
ment in terms of vector space semantics; an ex-
plicit representation of discourse entities and co-
reference relations is required (see Dubey 2010
for a model of human sentence processing that can
handle coreference).
A key objective for future work will be to in-
vestigate models that integrate semantic constraint
with syntactic predictions more tightly. For ex-
ample, we could envisage a parser that uses se-
mantic representations to guide its search, e.g., by
pruning syntactic analyses that have a low seman-
tic probability. At the same time, the semantic
model should have access to syntactic informa-
tion, i.e., the composition of word representations
should take their syntactic relationships into ac-
count, rather than just linear order.
</bodyText>
<page confidence="0.997854">
204
</page>
<sectionHeader confidence="0.988867" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.84405680952381">
ACL. 2010. Proceedings of the 48th Annual Meet-
ing of the Association for Computational Lin-
guistics. Uppsala.
Altmann, Gerry T. M. and Yuki Kamide. 1999.
Incremental interpretation at verbs: Restricting
the domain of subsequent reference. Cognition
73:247–264.
Altmann, Gerry T. M. and Mark J. Steedman.
1988. Interaction with context during human
sentence processing. Cognition 30(3):191–238.
Bellegarda, Jerome R. 2000. Exploiting latent se-
mantic information in statistical language mod-
eling. Proceedings of the IEEE 88(8):1279–
1296.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O’Brien. 1995. Using linear algebra
for intelligent information retrieval. SIAM re-
view 37(4):573–595.
Bever, Thomas G. 1970. The cognitive basis for
linguistic strutures. In J. R. Hayes, editor, Cog-
nition and the Development of Language, Wi-
</bodyText>
<reference confidence="0.990591938271605">
ley, New York, pages 279–362.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet allocation. Journal
of Machine Learning Research 3:993–1022.
Bullinaria, John A. and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study.
Behavior Research Methods 39:510–526.
Clifton, Charles, Adrian Staub, and Keith Rayner.
2007. Eye movement in reading words and sen-
tences. In R V Gompel, M Fisher, W Murray,
and R L Hill, editors, Eye Movements: A Win-
dow in Mind and Brain, Elsevier, pages 341–
372.
Coccaro, Noah and Daniel Jurafsky. 1998. To-
wards better integration of semantic predictors
in satistical language modeling. In Proceedings
of the 5th International Conference on Spoken
Language Processing. Sydney, Australia, pages
2403–2406.
Demberg, Vera and Frank Keller. 2008. Data from
eye-tracking corpora as evidence for theories
of syntactic processing complexity. Cognition
101(2):193–210.
Dubey, Amit. 2010. The influence of discourse on
syntax: A psycholinguistic model of sentence
processing. In ACL.
Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth.
2008. Parsing costs as predictors of reading dif-
ficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Re-
search 2(1):1–12.
Frank, Stefan L. 2009. Surprisal-based compar-
ison between a symbolic and a connectionist
model of sentence processing. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society. Austin, TX, pages 139–1144.
Gibson, Edward. 2000. Dependency locality the-
ory: A distance-dased theory of linguistic com-
plexity. In Alec Marantz, Yasushi Miyashita,
and Wayne O’Neil, editors, Image, Language,
Brain: Papers from the First Mind Articulation
Project Symposium, MIT Press, Cambridge,
MA, pages 95–126.
Gildea, Daniel and Thomas Hofmann. 1999.
Topic-based language models using EM. In
Proceedings of the 6th European Conference
on Speech Communiation and Technology. Bu-
dapest, Hungary, pages 2167–2170.
Griffiths, Thomas L., Mark Steyvers, and
Joshua B. Tenenbaum. 2007. Topics in se-
mantic representation. Psychological Review
114(2):211–244.
Hale, John. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chap-
ter of the Association. Association for Compu-
tational Linguistics, Pittsburgh, PA, volume 2,
pages 159–166.
Keller, Frank. 2010. Cognitively plausible models
of human language processing. In ACL.
Kennedy, Alan and Joel Pynte. 2005. Parafoveal-
on-foveal effects in normal reading. Vision Re-
search 45:153–168.
Konieczny, Lars. 2000. Locality and parsing com-
plexity. Journal of Psycholinguistic Research
29(6):627–645.
Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato’s problem: the latent seman-
tic analysis theory of acquisition, induction and
representation of knowledge. Psychological Re-
view 104(2):211–240.
Levy, Roger. 2008. Expectation-based syntactic
comprehension. Cognition 106(3):1126–1177.
Marslen-Wilson, William D. 1973. Linguistic
structure and speech shadowing at very short la-
tencies. Nature 244:522–523.
McDonald, Scott. 2000. Environmental Determi-
nants of Lexical Processing Effort. Ph.D. thesis,
University of Edinburgh.
</reference>
<page confidence="0.979512">
205
</page>
<reference confidence="0.99988468627451">
McDonald, Scott and Chris Brew. 2004. A dis-
tributional model of semantic context effects in
lexical processing. In Proceedings of the 42th
Annual Meeting of the Association for Com-
putational Linguistics. Barcelona, Spain, pages
17–24.
McDonald, Scott A. and Richard C. Shillcock.
2003. Low-level predictive inference in read-
ing: The influence of transitional probabilities
on eye movements. Vision Research 43:1735–
1751.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-
based models of semantic composition. In Pro-
ceedings of ACL-08: HLT. Columbus, OH,
pages 236–244.
Mitchell, Jeff and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing. Sin-
gapore, pages 430–439.
Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-
ence and reading time in sentence processing. In
Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-
formation Processing Systems 14. MIT Press,
Cambridge, MA, pages 59–65.
Pad´o, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Computational Linguistics
33(2):161–199.
Pad´o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic
plausibility in sentence processing. Cognitive
Science 33(5):794–838.
Pinheiro, Jose C. and Douglas M. Bates.
2000. Mixed-effects Models in S and S-PLUS.
Springer, New York.
Pinker, Steven. 1994. The Language Instinct: How
the Mind Creates Language. HarperCollins,
New York.
Plate, Tony A. 1995. Holographic reduced repre-
sentations. IEEE Transactions on Neural Net-
works 6(3):623–641.
Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading
normal text: A multiple-regression analysis. Vi-
sion Research 48:2172–2183.
Rayner, Keith. 1998. Eye movements in read-
ing and information processing: 20 years of re-
search. Psychological Bulletin 124(3):372–422.
Roark, Brian. 2001. Probabilistic top-down pars-
ing and language modeling. Computational
Linguistics 27(2):249–276.
Roark, Brian, Asaf Bachrach, Carlos Cardenas,
and Christophe Pallier. 2009. Deriving lex-
ical and syntactic expectation-based measures
for psycholinguistic modeling via incremental
top-down parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural
Language Processing. Association for Compu-
tational Linguistics, Singapore, pages 324–333.
Smolensky, Paul. 1990. Tensor product vari-
able binding and the representation of symbolic
structures in connectionist systems. Artificial
Intelligence 46:159–216.
Stanovich, Kieth E. and Richard F. West. 1981.
The effect of sentence context on ongoing word
recognition: Tests of a two-pricess theory. Jour-
nal of Experimental Psychology: Human Per-
ception and Performance 7:658–672.
Staub, Adrian and Charles Clifton. 2006. Syntac-
tic prediction in language comprehension: Evi-
dence from either ...or. Journal of Experimen-
tal Psychology: Learning, Memory, and Cogni-
tion 32:425–436.
Steyvers, Mark and Tom Griffiths. 2007. Proba-
bilistic topic models. In T. Landauer, D. Mc-
Namara, S Dennis, and W Kintsch, editors, A
Handbook of Latent Semantic Analysis, Psy-
chology Press.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the
Internatinal Conference on Spoken Language
Processing. Denver, Colorado.
Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-
tality and connectedness. Cognitive Science
29(2):291–305.
Tanenhaus, Michael K., Michael J. Spivey-
Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehen-
sion. Science 268:1632–1634.
van Berkum, Jos J. A., Colin M. Brown, and Peter
Hagoort. 1999. Early referential context effects
in sentence processing: Evidence from event-
related brain potentials. Journal of Memory and
Language 41:147–182.
Wright, Barton and Merrill F. Garrett. 1984. Lex-
ical decision in sentences: Effects of syntactic
structure. Memory and Cognition 12:31–45.
</reference>
<page confidence="0.998894">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864469">
<title confidence="0.999699">Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</title>
<author confidence="0.999843">Jeff Mitchell</author>
<author confidence="0.999843">Mirella Lapata</author>
<author confidence="0.999843">Vera Demberg</author>
<author confidence="0.999843">Frank Keller</author>
<affiliation confidence="0.999996">University of Edinburgh</affiliation>
<address confidence="0.939297">Edinburgh, United Kingdom</address>
<email confidence="0.954198">jeff.mitchell@ed.ac.uk,mlap@inf.ed.ac.uk,v.demberg@ed.ac.uk,keller@inf.ed.ac.uk</email>
<abstract confidence="0.999865411764706">The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ley</author>
</authors>
<pages>279--362</pages>
<location>New York,</location>
<marker>ley, </marker>
<rawString>ley, New York, pages 279–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<pages>3--993</pages>
<contexts>
<context position="18945" citStr="Blei et al. 2003" startWordPosition="3016" endWordPosition="3019">ghtforwardly used as predictors of processing difficulty, again via measuring the cosine of the angle between a vector w representing the upcoming word and a vector h representing the words preceding it: w · h sim(w,h) = (5) |w||h| where h is created compositionally, via some (additive or multiplicative) function f. In this paper we evaluate additive and compositional models in their ability to capture semantic prediction. We also examine the influence of the underlying meaning representations by comparing a simple semantic space similar to McDonald (2000) against Latent Dirichlet Allocation (Blei et al. 2003; Griffiths et al. 2007). Specifically, the simpler space is based on word cooccurrence counts; it constructs the vector representing a given target word, t, by identifying all the tokens of t in a corpus and recording the counts of context words, ci (within a specific window). The context words, ci, are limited to a set of the n most 199 common content words and each vector component is given by the ratio of the probability of a ci given t to the overall probability of ci. vi = p(ci|t) (6) p(ci) Despite its simplicity, the above semantic space (and variants thereof) has been used to successfu</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods</journal>
<pages>39--510</pages>
<contexts>
<context position="19662" citStr="Bullinaria and Levy 2007" startWordPosition="3142" endWordPosition="3145">ts; it constructs the vector representing a given target word, t, by identifying all the tokens of t in a corpus and recording the counts of context words, ci (within a specific window). The context words, ci, are limited to a set of the n most 199 common content words and each vector component is given by the ratio of the probability of a ci given t to the overall probability of ci. vi = p(ci|t) (6) p(ci) Despite its simplicity, the above semantic space (and variants thereof) has been used to successfully simulate lexical priming (e.g., McDonald 2000), human judgments of semantic similarity (Bullinaria and Levy 2007), and synonymy tests (Pad´o and Lapata 2007) such as those included in the Test of English as Foreign Language (TOEFL). LDA is a probabilistic topic model offering an alternative to spatial semantic representations. It is similar in spirit to LSA, it also operates on a word-document co-occurrence matrix and derives a reduced dimensionality description of words and documents. Whereas in LSA words are represented as points in a multi-dimensional space, LDA represents words using topics. Specifically, each document in a corpus is modeled as a distribution over K topics, which are themselves chara</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>Bullinaria, John A. and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Clifton</author>
<author>Adrian Staub</author>
<author>Keith Rayner</author>
</authors>
<title>Eye movement in reading words and sentences. In</title>
<date>2007</date>
<booktitle>Eye Movements: A Window in Mind and Brain, Elsevier,</booktitle>
<pages>341--372</pages>
<editor>R V Gompel, M Fisher, W Murray, and R L Hill, editors,</editor>
<contexts>
<context position="1984" citStr="Clifton et al. 2007" startWordPosition="291" endWordPosition="294">uggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster (Stanovich and West 1981; van Berkum et al. 1999; Clifton et al. 2007). Another example is argument prediction: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide 1999). The second type of prediction is syntactic prediction. Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (Wright and Garrett 1984). Another instance of syntactic prediction has been reported by Staub and Clifton (2006): following the word eith</context>
</contexts>
<marker>Clifton, Staub, Rayner, 2007</marker>
<rawString>Clifton, Charles, Adrian Staub, and Keith Rayner. 2007. Eye movement in reading words and sentences. In R V Gompel, M Fisher, W Murray, and R L Hill, editors, Eye Movements: A Window in Mind and Brain, Elsevier, pages 341– 372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Towards better integration of semantic predictors in satistical language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing.</booktitle>
<pages>2403--2406</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="22044" citStr="Coccaro and Jurafsky 1998" startWordPosition="3511" endWordPosition="3514">nt in terms of vector similarities lacks a clear motivation. Moreover, the two approaches, surprisal and similarity, produce mathematically different types of measures. Formally, it would be preferable to have a single approach to capturing constraint and the obvious solution is to derive some form of semantic surprisal rather than sticking with similarity. This can be achieved by turning a vector model of semantic similarity into a probabilistic language model. There are in fact a number of approaches to deriving language models from distributional models of semantics (e.g., Bellegarda 2000; Coccaro and Jurafsky 1998; Gildea and Hofmann 1999). We focus here on the model of Mitchell and Lapata (2009) which tackles the issue of the composition of semantic vectors and also integrates the output of an incremental parser. The core of their model is based on the product of a trigram model p(wn|wn−1 n−2) and a semantic component Δ(wn,h) which determines the factor by which this probability should be scaled up or down given the prior semantic context h: p(wn) = p(wn|wn−1 n−2) · Δ(wn,h) (7) The factor Δ(wn,h) is essentially based on a comparison between the vector representing the current word wn and the vector re</context>
</contexts>
<marker>Coccaro, Jurafsky, 1998</marker>
<rawString>Coccaro, Noah and Daniel Jurafsky. 1998. Towards better integration of semantic predictors in satistical language modeling. In Proceedings of the 5th International Conference on Spoken Language Processing. Sydney, Australia, pages 2403–2406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eye-tracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition</journal>
<volume>101</volume>
<issue>2</issue>
<contexts>
<context position="7597" citStr="Demberg and Keller 2008" startWordPosition="1173" endWordPosition="1176">in reading times is accounted for by costs associated with low-level features of the stimuli, e.g.. relating to orthography and eye-movement control (Rayner 1998). In addition, there may also be costs associated with the integration of new input into an incremental representation. Dependency Locality Theory (DLT, Gibson 2000) is essentially a distance-based measure of the amount of processing effort required when the head of a phrase is integrated with its syntactic dependents. We do not consider integration costs here (as they have not been shown to correlate reliably with reading times; see Demberg and Keller 2008 for details) and instead focus on the costs associated with semantic and syntactic constraint and low-level features, which appear to make the most substantial contributions. In the following subsections we describe the various features which contribute to the processing costs of a word in context. We begin by looking at the low-level costs and move on to consider the costs associated with syntactic and semantic constraint. For readers unfamiliar with the methodology involved in modeling eye-tracking data, we note that regression analysis (or the more general mixed effects models) is typicall</context>
<context position="10270" citStr="Demberg and Keller 2008" startWordPosition="1596" endWordPosition="1599">xation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express these expectations in terms of a distribution over all possible continuations of the input seen so far, then we can measure the magnitude of this change in terms of the Kullback-Leibler divergence</context>
<context position="12350" citStr="Demberg and Keller 2008" startWordPosition="1939" endWordPosition="1942">hearing a sentence like The horse raced past the barn fell (Bever 1970), English speakers are inclined to interpreted horse as the subject of raced expecting the sentence to end at the word barn. So upon hearing the word fell they are forced to revise their analysis of the sentence thus far and adopt a reduced relative reading. (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.3 Semantic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be found in similar contexts. In putting this idea into practice, the meaning of a word is then represented as a vector in a high dimensional space, with the vector components rela</context>
<context position="25264" citStr="Demberg and Keller (2008)" startWordPosition="4038" endWordPosition="4041"> a single measure of the processing cost associated with semantic and syntactic expectations. 4 Method Data The models discussed in the previous section were evaluated against an eye-tracking corpus. Specifically, we used the English portion of the Dundee Corpus (Kennedy and Pynte 2005) which contains 20 texts taken from The Independent newspaper. The corpus consists of 51,502 tokens and 9,776 types in total. It is annotated with the eye-movement records of 10 English native speakers, who each read the whole corpus. The eye-tracking data was preprocessed following the methodology described in Demberg and Keller (2008). From this data, we computed total reading time for each word in the corpus. Our statistical analyses were based on actual reading times, and so we only included words that were not skipped. We also excluded words for which the previous word had been skipped, and words on which the normal left-to-right movement of gaze had been interrupted, i.e., by blinks, regressions, etc. Finally, because our focus is the influence of semantic context, we selected only content words whose prior sentential context contained at least two further content words. The resulting data set consisted of 53,704 data </context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Demberg, Vera and Frank Keller. 2008. Data from eye-tracking corpora as evidence for theories of syntactic processing complexity. Cognition 101(2):193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Dubey</author>
</authors>
<title>The influence of discourse on syntax: A psycholinguistic model of sentence processing.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="42371" citStr="Dubey 2010" startWordPosition="6813" endWordPosition="6814">scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. For example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability. At the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order. 204 R</context>
</contexts>
<marker>Dubey, 2010</marker>
<rawString>Dubey, Amit. 2010. The influence of discourse on syntax: A psycholinguistic model of sentence processing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferrara Boston</author>
<author>John Hale Marisa</author>
<author>Reinhold Kliegl</author>
<author>Umesh Patil</author>
<author>Shravan Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="10298" citStr="Boston et al. 2008" startWordPosition="1601" endWordPosition="1604">the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express these expectations in terms of a distribution over all possible continuations of the input seen so far, then we can measure the magnitude of this change in terms of the Kullback-Leibler divergence of the old distribution to </context>
</contexts>
<marker>Boston, Marisa, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>Ferrara Boston, Marisa, John Hale, Reinhold Kliegl, Umesh Patil, and Shravan Vasishth. 2008. Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement Research 2(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society.</booktitle>
<pages>139--1144</pages>
<location>Austin, TX,</location>
<contexts>
<context position="10330" citStr="Frank 2009" startWordPosition="1609" endWordPosition="1610">, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express these expectations in terms of a distribution over all possible continuations of the input seen so far, then we can measure the magnitude of this change in terms of the Kullback-Leibler divergence of the old distribution to the updated distribution. This m</context>
<context position="12501" citStr="Frank 2009" startWordPosition="1964" endWordPosition="1965">ntence to end at the word barn. So upon hearing the word fell they are forced to revise their analysis of the sentence thus far and adopt a reduced relative reading. (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.3 Semantic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be found in similar contexts. In putting this idea into practice, the meaning of a word is then represented as a vector in a high dimensional space, with the vector components relating to the strength on occurrence of that word in various types of context. Semantic similarities are then modeled in terms of geometric similarities </context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Frank, Stefan L. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proceedings of the 31st Annual Conference of the Cognitive Science Society. Austin, TX, pages 139–1144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Dependency locality theory: A distance-dased theory of linguistic complexity.</title>
<date>2000</date>
<booktitle>Papers from the First Mind Articulation Project Symposium,</booktitle>
<pages>95--126</pages>
<editor>In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, Brain:</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="7301" citStr="Gibson 2000" startWordPosition="1125" endWordPosition="1126">e cognitive load associated with processing that input will also be low. In contrast, unexpected words will have a high surprisal and a high cognitive cost. However, high-level syntactic and semantic factors are only one source of cognitive costs. A sizable proportion of the variance in reading times is accounted for by costs associated with low-level features of the stimuli, e.g.. relating to orthography and eye-movement control (Rayner 1998). In addition, there may also be costs associated with the integration of new input into an incremental representation. Dependency Locality Theory (DLT, Gibson 2000) is essentially a distance-based measure of the amount of processing effort required when the head of a phrase is integrated with its syntactic dependents. We do not consider integration costs here (as they have not been shown to correlate reliably with reading times; see Demberg and Keller 2008 for details) and instead focus on the costs associated with semantic and syntactic constraint and low-level features, which appear to make the most substantial contributions. In the following subsections we describe the various features which contribute to the processing costs of a word in context. We </context>
</contexts>
<marker>Gibson, 2000</marker>
<rawString>Gibson, Edward. 2000. Dependency locality theory: A distance-dased theory of linguistic complexity. In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, Brain: Papers from the First Mind Articulation Project Symposium, MIT Press, Cambridge, MA, pages 95–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Thomas Hofmann</author>
</authors>
<title>Topic-based language models using EM.</title>
<date>1999</date>
<booktitle>In Proceedings of the 6th European Conference on Speech Communiation and Technology.</booktitle>
<pages>2167--2170</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="22070" citStr="Gildea and Hofmann 1999" startWordPosition="3515" endWordPosition="3518">arities lacks a clear motivation. Moreover, the two approaches, surprisal and similarity, produce mathematically different types of measures. Formally, it would be preferable to have a single approach to capturing constraint and the obvious solution is to derive some form of semantic surprisal rather than sticking with similarity. This can be achieved by turning a vector model of semantic similarity into a probabilistic language model. There are in fact a number of approaches to deriving language models from distributional models of semantics (e.g., Bellegarda 2000; Coccaro and Jurafsky 1998; Gildea and Hofmann 1999). We focus here on the model of Mitchell and Lapata (2009) which tackles the issue of the composition of semantic vectors and also integrates the output of an incremental parser. The core of their model is based on the product of a trigram model p(wn|wn−1 n−2) and a semantic component Δ(wn,h) which determines the factor by which this probability should be scaled up or down given the prior semantic context h: p(wn) = p(wn|wn−1 n−2) · Δ(wn,h) (7) The factor Δ(wn,h) is essentially based on a comparison between the vector representing the current word wn and the vector representing the prior histo</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>Gildea, Daniel and Thomas Hofmann. 1999. Topic-based language models using EM. In Proceedings of the 6th European Conference on Speech Communiation and Technology. Budapest, Hungary, pages 2167–2170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="18196" citStr="Griffiths et al. 2007" startWordPosition="2896" endWordPosition="2900">i = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui · vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the wellknown tensor products (Smolensky 1990) and circular convolution (Plate 1995). Importantly, composition models are not defined with a specific semantic space in mind, they could easily be adapted to LSA, or simple co-occurrence vectors, or more sophisticated semantic representations (e.g., Griffiths et al. 2007), although admittedly some composition functions may be better suited for particular semantic spaces. Composition models can be straightforwardly used as predictors of processing difficulty, again via measuring the cosine of the angle between a vector w representing the upcoming word and a vector h representing the words preceding it: w · h sim(w,h) = (5) |w||h| where h is created compositionally, via some (additive or multiplicative) function f. In this paper we evaluate additive and compositional models in their ability to capture semantic prediction. We also examine the influence of the und</context>
<context position="20821" citStr="Griffiths et al. 2007" startWordPosition="3329" endWordPosition="3332">ed as a distribution over K topics, which are themselves characterized as distribution over words. The individual words in a document are generated by repeatedly sampling a topic according to the topic distribution and then sampling a single word from the chosen topic. Under this framework, word meaning is represented as a probability distribution over a set of latent topics, essentially a vector whose dimensions correspond to topics and values to the probability of the word given these topics. Topic models have been recently gaining ground as a more structured representation of word meaning (Griffiths et al. 2007; Steyvers and Griffiths 2007). In contrast to more standard semantic space models where word senses are conflated into a single representation, topics have an intuitive correspondence to coarse-grained sense distinctions. 3 Integrating Semantic Constraint into Surprisal The treatment of semantic and syntactic constraint in models of processing difficulty has been somewhat inconsistent. While surprisal is a theoretically well-motivated measure, formalizing the idea of linguistic processing being highly predictive in terms of probabilistic language models, the measurement of semantic constraint</context>
<context position="27465" citStr="Griffiths et al. (2007)" startWordPosition="4402" endWordPosition="4406">us of 50,006 words and a test corpus of similar size. All words were converted to lowercase and numbers were replaced with the symbol (num). A vocabulary of 20,000 words was chosen and the remaining tokens were replaced with (unk). Following Mitchell and Lapata (2009), we constructed a simple semantic space based on cooccurrence statistics from the BLLIP training set. We used the 2,000 most frequent word types as contexts and a symmetric five word window. Vector components were defined as in Equation (6). We also trained the LDA model on BLLIP, using the Gibb’s sampling procedure discussed in Griffiths et al. (2007). We experimented with different numbers of topics on the development set (from 10 to 1,000) and report results on the test set with 100 topics. In our experiments, the hyperparameter a was initialized to .5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke 2002) with backoff and Kneser-Ney smoothing. As our incremental parser we used Roark’s (2001) parser trained on sections 2–21 of the Penn Treebank containing 936,017 words. The parser produc</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Griffiths, Thomas L., Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association. Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="3138" citStr="Hale 2001" startWordPosition="474" endWordPosition="475">ted by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficu</context>
<context position="10073" citStr="Hale 2001" startWordPosition="1566" endWordPosition="1567">e sequential context of a word can also influence reading times. Mc197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express</context>
<context position="11422" citStr="Hale 2001" startWordPosition="1792" endWordPosition="1793">of this change in terms of the Kullback-Leibler divergence of the old distribution to the updated distribution. This measure of processing cost for an input word, wk+1, given the previous context, w1...wk, can be expressed straightforwardly in terms of its conditional probability as: S = −logP(wk+1|w1 ...wk) (1) That is, the processing cost for a word decreases as its probability increases, with zero processing cost incurred for words which must appear in a given context, as these do not result in any change in the expectations of the language processor. The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences.1 Several variants of calculating surprisal have been developed in the literature since using different parsing strategies 1While hearing a sentence like The horse raced past the barn fell (Bever 1970), English speakers are inclined to interpreted horse as the subject of raced expecting the sentence to end at the word barn. So upon hearing the word fell they are forced to revise their analysis of the sentence thus far and </context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>Hale, John. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association. Association for Computational Linguistics, Pittsburgh, PA, volume 2, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>Cognitively plausible models of human language processing.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="41848" citStr="Keller (2010)" startWordPosition="6732" endWordPosition="6733">to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. An interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of LSA or LDA representations with an incremental parser yield a better fit of the behavioral data. In the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key</context>
</contexts>
<marker>Keller, 2010</marker>
<rawString>Keller, Frank. 2010. Cognitively plausible models of human language processing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Kennedy</author>
<author>Joel Pynte</author>
</authors>
<title>Parafovealon-foveal effects in normal reading.</title>
<date>2005</date>
<journal>Vision Research</journal>
<pages>45--153</pages>
<contexts>
<context position="15026" citStr="Kennedy and Pynte 2005" startWordPosition="2375" endWordPosition="2378">int (i.e., the word was expected) and analogously low similarity indicates low semantic constraint (i.e., the word was unexpected). They oper198 ationalize preceding contexts in two ways, either as the word immediately preceding the next word as the sentence fragment preceding it. Sentence fragments are represented as the average of the words they contain independently of their order. The model takes into account only content words, function words are of little interest here as they can be found in any context. Pynte et al. (2008) analyze reading times on the French part of the Dundee corpus (Kennedy and Pynte 2005) and find that word-level LSA similarities are predictive of first fixation and first pass durations, whereas sentence-level LSA is only predictive of first pass duration (i.e., for a measure that includes refixation). This latter finding is somewhat counterintuitive, one would expect longer contexts to have an immediate effect as they are presumably more constraining. One reason why sentence-level influences are only visible on first pass duration may be due to LSA itself, which is syntax-blind. Another reason relates to the way sentential context was modeled as vector addition (or averaging)</context>
<context position="24926" citStr="Kennedy and Pynte 2005" startWordPosition="3984" endWordPosition="3987"> improves further upon a semantic language model in terms of perplexity. We argue that the probabilities from this model give us a means to model the incrementally and predictivity of the language processor in a manner that integrates both syntactic and semantic constraints. Converting these probabilities to surprisal should result in a single measure of the processing cost associated with semantic and syntactic expectations. 4 Method Data The models discussed in the previous section were evaluated against an eye-tracking corpus. Specifically, we used the English portion of the Dundee Corpus (Kennedy and Pynte 2005) which contains 20 texts taken from The Independent newspaper. The corpus consists of 51,502 tokens and 9,776 types in total. It is annotated with the eye-movement records of 10 English native speakers, who each read the whole corpus. The eye-tracking data was preprocessed following the methodology described in Demberg and Keller (2008). From this data, we computed total reading time for each word in the corpus. Our statistical analyses were based on actual reading times, and so we only included words that were not skipped. We also excluded words for which the previous word had been skipped, a</context>
</contexts>
<marker>Kennedy, Pynte, 2005</marker>
<rawString>Kennedy, Alan and Joel Pynte. 2005. Parafovealon-foveal effects in normal reading. Vision Research 45:153–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Konieczny</author>
</authors>
<title>Locality and parsing complexity.</title>
<date>2000</date>
<journal>Journal of Psycholinguistic Research</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="1296" citStr="Konieczny 2000" startWordPosition="181" endWordPosition="182">in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. 1 Introduction Psycholinguists have long realized that language comprehension is highly incremental, with readers and listeners continuously extracting the meaning of utterances on a word-by-word basis. As soon as they encounter a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far (Marslen-Wilson 1973; Konieczny 2000; Tanenhaus et al. 1995; Sturt and Lombardo 2005). Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is</context>
</contexts>
<marker>Konieczny, 2000</marker>
<rawString>Konieczny, Lars. 2000. Locality and parsing complexity. Journal of Psycholinguistic Research 29(6):627–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3494" citStr="Landauer and Dumais 1997" startWordPosition="531" endWordPosition="535">upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures </context>
<context position="13203" citStr="Landauer and Dumais 1997" startWordPosition="2077" endWordPosition="2080"> used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be found in similar contexts. In putting this idea into practice, the meaning of a word is then represented as a vector in a high dimensional space, with the vector components relating to the strength on occurrence of that word in various types of context. Semantic similarities are then modeled in terms of geometric similarities within the space. To give a concrete example, Latent Semantic Analysis (LSA, Landauer and Dumais 1997) creates a meaning representation for words by constructing a word-document co-occurrence matrix from a large collection of documents. Each row in the matrix represents a word, each column a document, and each entry the frequency with which the word appeared within that document. Because this matrix tends to be quite large it is often transformed via a singular value decomposition (Berry et al. 1995) into three component matrices: a matrix of word vectors, a matrix of document vectors, and a diagonal matrix containing singular values. Re-multiplying these matrices together using only the initi</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="10085" citStr="Levy 2008" startWordPosition="1568" endWordPosition="1569">l context of a word can also influence reading times. Mc197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express these expec</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Levy, Roger. 2008. Expectation-based syntactic comprehension. Cognition 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William D Marslen-Wilson</author>
</authors>
<title>Linguistic structure and speech shadowing at very short latencies.</title>
<date>1973</date>
<journal>Nature</journal>
<pages>244--522</pages>
<contexts>
<context position="1280" citStr="Marslen-Wilson 1973" startWordPosition="179" endWordPosition="180">d syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. 1 Introduction Psycholinguists have long realized that language comprehension is highly incremental, with readers and listeners continuously extracting the meaning of utterances on a word-by-word basis. As soon as they encounter a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far (Marslen-Wilson 1973; Konieczny 2000; Tanenhaus et al. 1995; Sturt and Lombardo 2005). Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sent</context>
</contexts>
<marker>Marslen-Wilson, 1973</marker>
<rawString>Marslen-Wilson, William D. 1973. Linguistic structure and speech shadowing at very short latencies. Nature 244:522–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
</authors>
<title>Environmental Determinants of Lexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="16057" citStr="McDonald 2000" startWordPosition="2541" endWordPosition="2542">nly visible on first pass duration may be due to LSA itself, which is syntax-blind. Another reason relates to the way sentential context was modeled as vector addition (or averaging). The idea of averaging is not very attractive from a linguistic perspective as it blends the meanings of individual words together. Ideally, the combination of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker 1994). The only other model of semantic constraint we are aware of is Incremental Contextual Distinctiveness (ICD, McDonald 2000; McDonald and Brew 2004). ICD assumes that words carry prior semantic expectations which are updated upon seeing the next word. Context is represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. When the latter is observed, the prior expectation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this m</context>
<context position="18891" citStr="McDonald (2000)" startWordPosition="3009" endWordPosition="3011">ular semantic spaces. Composition models can be straightforwardly used as predictors of processing difficulty, again via measuring the cosine of the angle between a vector w representing the upcoming word and a vector h representing the words preceding it: w · h sim(w,h) = (5) |w||h| where h is created compositionally, via some (additive or multiplicative) function f. In this paper we evaluate additive and compositional models in their ability to capture semantic prediction. We also examine the influence of the underlying meaning representations by comparing a simple semantic space similar to McDonald (2000) against Latent Dirichlet Allocation (Blei et al. 2003; Griffiths et al. 2007). Specifically, the simpler space is based on word cooccurrence counts; it constructs the vector representing a given target word, t, by identifying all the tokens of t in a corpus and recording the counts of context words, ci (within a specific window). The context words, ci, are limited to a set of the n most 199 common content words and each vector component is given by the ratio of the probability of a ci given t to the overall probability of ci. vi = p(ci|t) (6) p(ci) Despite its simplicity, the above semantic s</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>McDonald, Scott. 2000. Environmental Determinants of Lexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
<author>Chris Brew</author>
</authors>
<title>A distributional model of semantic context effects in lexical processing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>17--24</pages>
<location>Barcelona,</location>
<contexts>
<context position="3778" citStr="McDonald and Brew 2004" startWordPosition="582" endWordPosition="585">rsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading. Ample evidence 196 Proceedings of the 48th Annual Meeting of the Association fo</context>
<context position="16082" citStr="McDonald and Brew 2004" startWordPosition="2543" endWordPosition="2546">first pass duration may be due to LSA itself, which is syntax-blind. Another reason relates to the way sentential context was modeled as vector addition (or averaging). The idea of averaging is not very attractive from a linguistic perspective as it blends the meanings of individual words together. Ideally, the combination of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker 1994). The only other model of semantic constraint we are aware of is Incremental Contextual Distinctiveness (ICD, McDonald 2000; McDonald and Brew 2004). ICD assumes that words carry prior semantic expectations which are updated upon seeing the next word. Context is represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. When the latter is observed, the prior expectation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to su</context>
</contexts>
<marker>McDonald, Brew, 2004</marker>
<rawString>McDonald, Scott and Chris Brew. 2004. A distributional model of semantic context effects in lexical processing. In Proceedings of the 42th Annual Meeting of the Association for Computational Linguistics. Barcelona, Spain, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott A McDonald</author>
<author>Richard C Shillcock</author>
</authors>
<title>Low-level predictive inference in reading: The influence of transitional probabilities on eye movements.</title>
<date>2003</date>
<journal>Vision Research</journal>
<volume>43</volume>
<pages>1751</pages>
<contexts>
<context position="16862" citStr="McDonald and Shillcock 2003" startWordPosition="2664" endWordPosition="2668">ies which reflects the likely location in semantic space of the upcoming word. When the latter is observed, the prior expectation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to successfully simulate single- and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Their aim is not so much to model processing difficulty, but to construct vector-based meaning representations that go beyond individual words. They introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v: h = f (u,v) (2) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allo</context>
</contexts>
<marker>McDonald, Shillcock, 2003</marker>
<rawString>McDonald, Scott A. and Richard C. Shillcock. 2003. Low-level predictive inference in reading: The influence of transitional probabilities on eye movements. Vision Research 43:1735– 1751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vectorbased models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT.</booktitle>
<pages>236--244</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="16984" citStr="Mitchell and Lapata (2008)" startWordPosition="2684" endWordPosition="2687">ation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to successfully simulate single- and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Their aim is not so much to model processing difficulty, but to construct vector-based meaning representations that go beyond individual words. They introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v: h = f (u,v) (2) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + v</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Mitchell, Jeff and Mirella Lapata. 2008. Vectorbased models of semantic composition. In Proceedings of ACL-08: HLT. Columbus, OH, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Singapore,</booktitle>
<pages>430--439</pages>
<contexts>
<context position="5182" citStr="Mitchell and Lapata 2009" startWordPosition="790" endWordPosition="794">lated to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore, the models of Narayanan and Jurafsky (2002) and Pad´o et al. (2009) do </context>
<context position="22128" citStr="Mitchell and Lapata (2009)" startWordPosition="3526" endWordPosition="3530">oaches, surprisal and similarity, produce mathematically different types of measures. Formally, it would be preferable to have a single approach to capturing constraint and the obvious solution is to derive some form of semantic surprisal rather than sticking with similarity. This can be achieved by turning a vector model of semantic similarity into a probabilistic language model. There are in fact a number of approaches to deriving language models from distributional models of semantics (e.g., Bellegarda 2000; Coccaro and Jurafsky 1998; Gildea and Hofmann 1999). We focus here on the model of Mitchell and Lapata (2009) which tackles the issue of the composition of semantic vectors and also integrates the output of an incremental parser. The core of their model is based on the product of a trigram model p(wn|wn−1 n−2) and a semantic component Δ(wn,h) which determines the factor by which this probability should be scaled up or down given the prior semantic context h: p(wn) = p(wn|wn−1 n−2) · Δ(wn,h) (7) The factor Δ(wn,h) is essentially based on a comparison between the vector representing the current word wn and the vector representing the prior history h. Varying the method for constructing word vectors (e.</context>
<context position="24283" citStr="Mitchell and Lapata (2009)" startWordPosition="3882" endWordPosition="3885">probability: p(w|h) = λp1(w|h) + (1− λ)p2(w|h) (9) where p1(w|h) is computed as in Equation (7) and p2(w|h) is computed by the parser. Their implementation uses Roark’s (2001) top-down incremental parser which estimates the probability of 200 the next word based upon the previous words of the sentence. These prefix probabilities are calculated from a grammar, by considering the likelihood of seeing the next word given the possible grammatical relations representing the prior context. Equation (9) essentially defines a language model which combines semantic, syntactic and n-gram structure, and Mitchell and Lapata (2009) demonstrate that it improves further upon a semantic language model in terms of perplexity. We argue that the probabilities from this model give us a means to model the incrementally and predictivity of the language processor in a manner that integrates both syntactic and semantic constraints. Converting these probabilities to surprisal should result in a single measure of the processing cost associated with semantic and syntactic expectations. 4 Method Data The models discussed in the previous section were evaluated against an eye-tracking corpus. Specifically, we used the English portion of</context>
<context position="27110" citStr="Mitchell and Lapata (2009)" startWordPosition="4341" endWordPosition="4344">ght movement of gaze had been interrupted, e.g., by blinks and regressions, which results in the final total to 53,704 data points. Model Implementation All elements of our model were trained on the BLLIP corpus, a collection of texts from the Wall Street Journal (years 1987–89). The training corpus consisted of 38,521,346 words. We used a development corpus of 50,006 words and a test corpus of similar size. All words were converted to lowercase and numbers were replaced with the symbol (num). A vocabulary of 20,000 words was chosen and the remaining tokens were replaced with (unk). Following Mitchell and Lapata (2009), we constructed a simple semantic space based on cooccurrence statistics from the BLLIP training set. We used the 2,000 most frequent word types as contexts and a symmetric five word window. Vector components were defined as in Equation (6). We also trained the LDA model on BLLIP, using the Gibb’s sampling procedure discussed in Griffiths et al. (2007). We experimented with different numbers of topics on the development set (from 10 to 1,000) and report results on the test set with 100 topics. In our experiments, the hyperparameter a was initialized to .5, and the b word probabilities were in</context>
<context position="35709" citStr="Mitchell and Lapata (2009)" startWordPosition="5743" endWordPosition="5746"> the baseline models as the dependent variable. Considering the trigram model first, we find that adding this factor to the model gives a significant improvement in fit. Also adding the semantic component (−log(Δ)) improves fit further, both for additive and multiplicative composition functions using a simple semantic space. Finally, the addition of the parser probabilities (log(λ + (1−λ) p2 p1 )) again improves model fit significantly. As far as LDA is concerned, the additive model significantly improves model fit, whereas the multiplicative one does not. These results mirror the findings of Mitchell and Lapata (2009), who report that a multiplicative composition function produced the lowest perplexity for the simple semantic space model, whereas an additive function gave the best perplexity for the LDA space. Table 3 lists the coefficients for the nested models for LDA Additive .00817*** Multiplicative .00640*** Table 4: Coefficients of LME models with integrated surprisal measure (based on SSS or LDA) as factor all four variants of our semantic constraint measure. Finally, we built a separate LME model where we added the integrated surprisal measure (see Equation (9)) to the model only containing the ran</context>
</contexts>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Mitchell, Jeff and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Singapore, pages 430–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Daniel Jurafsky</author>
</authors>
<title>A Bayesian model predicts human parse preference and reading time in sentence processing.</title>
<date>2002</date>
<booktitle>In Thomas G. Dietterich, Sue Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14.</booktitle>
<pages>59--65</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="5508" citStr="Narayanan and Jurafsky 2002" startWordPosition="835" endWordPosition="838">ing. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore, the models of Narayanan and Jurafsky (2002) and Pad´o et al. (2009) do not explicitly model prediction, but rather focus on accounting for garden path effects. The proposed model simultaneously captures semantic and syntactic effects in a single measure which we empirically show is predictive of processing difficulty as manifested in eyemovements. 2 Models of Processing Difficulty As described </context>
</contexts>
<marker>Narayanan, Jurafsky, 2002</marker>
<rawString>Narayanan, Srini and Daniel Jurafsky. 2002. A Bayesian model predicts human parse preference and reading time in sentence processing. In Thomas G. Dietterich, Sue Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems 14. MIT Press, Cambridge, MA, pages 59–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Pad´o, Sebastian and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Pad´o</author>
<author>Matthew W Crocker</author>
<author>Frank Keller</author>
</authors>
<title>A probabilistic model of semantic plausibility in sentence processing.</title>
<date>2009</date>
<journal>Cognitive Science</journal>
<volume>33</volume>
<issue>5</issue>
<marker>Pad´o, Crocker, Keller, 2009</marker>
<rawString>Pad´o, Ulrike, Matthew W. Crocker, and Frank Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science 33(5):794–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose C Pinheiro</author>
<author>Douglas M Bates</author>
</authors>
<date>2000</date>
<booktitle>Mixed-effects Models in S and S-PLUS.</booktitle>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="28364" citStr="Pinheiro and Bates 2000" startWordPosition="4543" endWordPosition="4546">ted our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke 2002) with backoff and Kneser-Ney smoothing. As our incremental parser we used Roark’s (2001) parser trained on sections 2–21 of the Penn Treebank containing 936,017 words. The parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. Statistical Analysis The statistical analyses in this paper were carried out using linear mixed effects models (LME, Pinheiro and Bates 2000). The latter can be thought of as generalization of linear regression that allows the inclusion of random factors (such as participants or items) as well as fixed factors (e.g., word frequency). In our analyses, we treat participant as a random factor, which means that our models contain an intercept term for each participant, representing the individual differences in the rates at which they read.3 We evaluated the effect of adding a factor to a model by comparing the likelihoods of the models with and without that factor. If a c2 test on the 3Other random factors that are appropriate for our</context>
</contexts>
<marker>Pinheiro, Bates, 2000</marker>
<rawString>Pinheiro, Jose C. and Douglas M. Bates. 2000. Mixed-effects Models in S and S-PLUS. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Pinker</author>
</authors>
<title>The Language Instinct: How the Mind Creates Language. HarperCollins,</title>
<date>1994</date>
<location>New York.</location>
<contexts>
<context position="15934" citStr="Pinker 1994" startWordPosition="2522" endWordPosition="2523">texts to have an immediate effect as they are presumably more constraining. One reason why sentence-level influences are only visible on first pass duration may be due to LSA itself, which is syntax-blind. Another reason relates to the way sentential context was modeled as vector addition (or averaging). The idea of averaging is not very attractive from a linguistic perspective as it blends the meanings of individual words together. Ideally, the combination of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker 1994). The only other model of semantic constraint we are aware of is Incremental Contextual Distinctiveness (ICD, McDonald 2000; McDonald and Brew 2004). ICD assumes that words carry prior semantic expectations which are updated upon seeing the next word. Context is represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. When the latter is observed, the prior expectation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ</context>
</contexts>
<marker>Pinker, 1994</marker>
<rawString>Pinker, Steven. 1994. The Language Instinct: How the Mind Creates Language. HarperCollins, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony A Plate</author>
</authors>
<title>Holographic reduced representations.</title>
<date>1995</date>
<journal>IEEE Transactions on Neural Networks</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="17960" citStr="Plate 1995" startWordPosition="2862" endWordPosition="2863">rise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui · vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the wellknown tensor products (Smolensky 1990) and circular convolution (Plate 1995). Importantly, composition models are not defined with a specific semantic space in mind, they could easily be adapted to LSA, or simple co-occurrence vectors, or more sophisticated semantic representations (e.g., Griffiths et al. 2007), although admittedly some composition functions may be better suited for particular semantic spaces. Composition models can be straightforwardly used as predictors of processing difficulty, again via measuring the cosine of the angle between a vector w representing the upcoming word and a vector h representing the words preceding it: w · h sim(w,h) = (5) |w||h|</context>
</contexts>
<marker>Plate, 1995</marker>
<rawString>Plate, Tony A. 1995. Holographic reduced representations. IEEE Transactions on Neural Networks 6(3):623–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Pynte</author>
<author>Boris New</author>
<author>Alan Kennedy</author>
</authors>
<title>On-line contextual influences during reading normal text: A multiple-regression analysis.</title>
<date>2008</date>
<journal>Vision Research</journal>
<pages>48--2172</pages>
<contexts>
<context position="3433" citStr="Pynte et al. (2008)" startWordPosition="521" endWordPosition="524">and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely l</context>
<context position="14258" citStr="Pynte et al. (2008)" startWordPosition="2251" endWordPosition="2254"> matrix of word vectors, a matrix of document vectors, and a diagonal matrix containing singular values. Re-multiplying these matrices together using only the initial portions of each (corresponding to the use of a lower dimensional spatial representation) produces a tractable approximation to the original matrix. In this framework, the similarity between two words can be easily quantified, e.g., by measuring the cosine of the angle of the vectors representing them. As LSA is one the best known semantic space models it comes as no surprise that it has been used to analyze semantic constraint. Pynte et al. (2008) measure the similarity between the next word and its preceding context under the assumption that high similarity indicates high semantic constraint (i.e., the word was expected) and analogously low similarity indicates low semantic constraint (i.e., the word was unexpected). They oper198 ationalize preceding contexts in two ways, either as the word immediately preceding the next word as the sentence fragment preceding it. Sentence fragments are represented as the average of the words they contain independently of their order. The model takes into account only content words, function words are</context>
<context position="26353" citStr="Pynte et al. (2008)" startWordPosition="4215" endWordPosition="4218">ords whose prior sentential context contained at least two further content words. The resulting data set consisted of 53,704 data points, which is about 10% of the theoretically possible total.2 2The total of all words read by all subjects is 515,020. The pre-processing recommended by Demberg and Keller’s (2008) results in a data sets containing 436,000 data points. Removing non-content words leaves 205,922 data points. It only makes sense to consider words that were actually fixated (the eye-tracking measures used are not defined on skipped words), which leaves 162,129 data points. Following Pynte et al. (2008), we require that the previous word was fixated, with 70,051 data points remaining. We exclude words on which the normal left to right movement of gaze had been interrupted, e.g., by blinks and regressions, which results in the final total to 53,704 data points. Model Implementation All elements of our model were trained on the BLLIP corpus, a collection of texts from the Wall Street Journal (years 1987–89). The training corpus consisted of 38,521,346 words. We used a development corpus of 50,006 words and a test corpus of similar size. All words were converted to lowercase and numbers were re</context>
</contexts>
<marker>Pynte, New, Kennedy, 2008</marker>
<rawString>Pynte, Joel, Boris New, and Alan Kennedy. 2008. On-line contextual influences during reading normal text: A multiple-regression analysis. Vision Research 48:2172–2183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
</authors>
<title>Eye movements in reading and information processing: 20 years of research.</title>
<date>1998</date>
<journal>Psychological Bulletin</journal>
<volume>124</volume>
<issue>3</issue>
<contexts>
<context position="4518" citStr="Rayner 1998" startWordPosition="687" endWordPosition="688">ch are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading. Ample evidence 196 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 196–206, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics (Rayner 1998) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together</context>
<context position="7136" citStr="Rayner 1998" startWordPosition="1099" endWordPosition="1100">sal can be thought of as measuring the cost of dealing with unexpected input. When a word conforms to the language processor’s expectations, surprisal is low, and the cognitive load associated with processing that input will also be low. In contrast, unexpected words will have a high surprisal and a high cognitive cost. However, high-level syntactic and semantic factors are only one source of cognitive costs. A sizable proportion of the variance in reading times is accounted for by costs associated with low-level features of the stimuli, e.g.. relating to orthography and eye-movement control (Rayner 1998). In addition, there may also be costs associated with the integration of new input into an incremental representation. Dependency Locality Theory (DLT, Gibson 2000) is essentially a distance-based measure of the amount of processing effort required when the head of a phrase is integrated with its syntactic dependents. We do not consider integration costs here (as they have not been shown to correlate reliably with reading times; see Demberg and Keller 2008 for details) and instead focus on the costs associated with semantic and syntactic constraint and low-level features, which appear to make</context>
</contexts>
<marker>Rayner, 1998</marker>
<rawString>Rayner, Keith. 1998. Eye movements in reading and information processing: 20 years of research. Psychological Bulletin 124(3):372–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="5109" citStr="Roark 2001" startWordPosition="780" endWordPosition="781">istics (Rayner 1998) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Roark, Brian. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics 27(2):249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore,</booktitle>
<pages>324--333</pages>
<contexts>
<context position="10317" citStr="Roark et al. 2009" startWordPosition="1605" endWordPosition="1608">itional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk|wk+1). 2.2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. 2008; Roark et al. 2009; Frank 2009). The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. These processing costs are assumed to arise from the change in the expectations of the language processor as new input arrives. If we express these expectations in terms of a distribution over all possible continuations of the input seen so far, then we can measure the magnitude of this change in terms of the Kullback-Leibler divergence of the old distribution to the updated distrib</context>
<context position="12182" citStr="Roark et al. 2009" startWordPosition="1912" endWordPosition="1915">mbiguous garden path sentences.1 Several variants of calculating surprisal have been developed in the literature since using different parsing strategies 1While hearing a sentence like The horse raced past the barn fell (Bever 1970), English speakers are inclined to interpreted horse as the subject of raced expecting the sentence to end at the word barn. So upon hearing the word fell they are forced to revise their analysis of the sentence thus far and adopt a reduced relative reading. (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.3 Semantic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be found in </context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Roark, Brian, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore, pages 324–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence</journal>
<pages>46--159</pages>
<contexts>
<context position="17922" citStr="Smolensky 1990" startWordPosition="2856" endWordPosition="2857">of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi = ui · vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the wellknown tensor products (Smolensky 1990) and circular convolution (Plate 1995). Importantly, composition models are not defined with a specific semantic space in mind, they could easily be adapted to LSA, or simple co-occurrence vectors, or more sophisticated semantic representations (e.g., Griffiths et al. 2007), although admittedly some composition functions may be better suited for particular semantic spaces. Composition models can be straightforwardly used as predictors of processing difficulty, again via measuring the cosine of the angle between a vector w representing the upcoming word and a vector h representing the words pre</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Smolensky, Paul. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence 46:159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kieth E Stanovich</author>
<author>Richard F West</author>
</authors>
<title>The effect of sentence context on ongoing word recognition: Tests of a two-pricess theory. Journal of Experimental Psychology: Human Perception and Performance 7:658–672.</title>
<date>1981</date>
<contexts>
<context position="1938" citStr="Stanovich and West 1981" startWordPosition="282" endWordPosition="285">1995; Sturt and Lombardo 2005). Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster (Stanovich and West 1981; van Berkum et al. 1999; Clifton et al. 2007). Another example is argument prediction: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide 1999). The second type of prediction is syntactic prediction. Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (Wright and Garrett 1984). Another instance of syntactic prediction has been reported by Sta</context>
</contexts>
<marker>Stanovich, West, 1981</marker>
<rawString>Stanovich, Kieth E. and Richard F. West. 1981. The effect of sentence context on ongoing word recognition: Tests of a two-pricess theory. Journal of Experimental Psychology: Human Perception and Performance 7:658–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Staub</author>
<author>Charles Clifton</author>
</authors>
<title>Syntactic prediction in language comprehension: Evidence from either ...or.</title>
<date>2006</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition</journal>
<pages>32--425</pages>
<contexts>
<context position="2559" citStr="Staub and Clifton (2006)" startWordPosition="383" endWordPosition="386">981; van Berkum et al. 1999; Clifton et al. 2007). Another example is argument prediction: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide 1999). The second type of prediction is syntactic prediction. Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (Wright and Garrett 1984). Another instance of syntactic prediction has been reported by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsel</context>
</contexts>
<marker>Staub, Clifton, 2006</marker>
<rawString>Staub, Adrian and Charles Clifton. 2006. Syntactic prediction in language comprehension: Evidence from either ...or. Journal of Experimental Psychology: Learning, Memory, and Cognition 32:425–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2007</date>
<booktitle>A Handbook of Latent Semantic Analysis,</booktitle>
<editor>In T. Landauer, D. McNamara, S Dennis, and W Kintsch, editors,</editor>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="20851" citStr="Steyvers and Griffiths 2007" startWordPosition="3333" endWordPosition="3336">er K topics, which are themselves characterized as distribution over words. The individual words in a document are generated by repeatedly sampling a topic according to the topic distribution and then sampling a single word from the chosen topic. Under this framework, word meaning is represented as a probability distribution over a set of latent topics, essentially a vector whose dimensions correspond to topics and values to the probability of the word given these topics. Topic models have been recently gaining ground as a more structured representation of word meaning (Griffiths et al. 2007; Steyvers and Griffiths 2007). In contrast to more standard semantic space models where word senses are conflated into a single representation, topics have an intuitive correspondence to coarse-grained sense distinctions. 3 Integrating Semantic Constraint into Surprisal The treatment of semantic and syntactic constraint in models of processing difficulty has been somewhat inconsistent. While surprisal is a theoretically well-motivated measure, formalizing the idea of linguistic processing being highly predictive in terms of probabilistic language models, the measurement of semantic constraint in terms of vector similariti</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Steyvers, Mark and Tom Griffiths. 2007. Probabilistic topic models. In T. Landauer, D. McNamara, S Dennis, and W Kintsch, editors, A Handbook of Latent Semantic Analysis, Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the Internatinal Conference on Spoken Language Processing.</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="27880" citStr="Stolcke 2002" startWordPosition="4473" endWordPosition="4474"> and a symmetric five word window. Vector components were defined as in Equation (6). We also trained the LDA model on BLLIP, using the Gibb’s sampling procedure discussed in Griffiths et al. (2007). We experimented with different numbers of topics on the development set (from 10 to 1,000) and report results on the test set with 100 topics. In our experiments, the hyperparameter a was initialized to .5, and the b word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke 2002) with backoff and Kneser-Ney smoothing. As our incremental parser we used Roark’s (2001) parser trained on sections 2–21 of the Penn Treebank containing 936,017 words. The parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. Statistical Analysis The statistical analyses in this paper were carried out using linear mixed effects models (LME, Pinheiro and Bates 2000). The latter can be thought of as generalization of linear regression that allows the inclusion of random factors (s</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of the Internatinal Conference on Spoken Language Processing. Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Sturt</author>
<author>Vincenzo Lombardo</author>
</authors>
<title>Processing coordinated structures: Incrementality and connectedness.</title>
<date>2005</date>
<journal>Cognitive Science</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="1345" citStr="Sturt and Lombardo 2005" startWordPosition="187" endWordPosition="190"> as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. 1 Introduction Psycholinguists have long realized that language comprehension is highly incremental, with readers and listeners continuously extracting the meaning of utterances on a word-by-word basis. As soon as they encounter a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far (Marslen-Wilson 1973; Konieczny 2000; Tanenhaus et al. 1995; Sturt and Lombardo 2005). Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster (Stanovich and West 1981; van B</context>
</contexts>
<marker>Sturt, Lombardo, 2005</marker>
<rawString>Sturt, Patrick and Vincenzo Lombardo. 2005. Processing coordinated structures: Incrementality and connectedness. Cognitive Science 29(2):291–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J SpiveyKnowlton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science</journal>
<pages>268--1632</pages>
<contexts>
<context position="1319" citStr="Tanenhaus et al. 1995" startWordPosition="183" endWordPosition="186">sentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. 1 Introduction Psycholinguists have long realized that language comprehension is highly incremental, with readers and listeners continuously extracting the meaning of utterances on a word-by-word basis. As soon as they encounter a word in a sentence, they integrate it as fully as possible into a representation of the sentence thus far (Marslen-Wilson 1973; Konieczny 2000; Tanenhaus et al. 1995; Sturt and Lombardo 2005). Recent research suggests that language comprehension can also be highly predictive, i.e., comprehenders are able to anticipate upcoming linguistic material. This is beneficial as it gives them more time to keep up with the input, and predictions can be used to compensate for problems with noise or ambiguity. Two types of prediction have been observed in the literature. The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster (Stan</context>
</contexts>
<marker>Tanenhaus, SpiveyKnowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Tanenhaus, Michael K., Michael J. SpiveyKnowlton, Kathleen M. Eberhard, and Julie C. Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science 268:1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos J A van Berkum</author>
<author>Colin M Brown</author>
<author>Peter Hagoort</author>
</authors>
<title>Early referential context effects in sentence processing: Evidence from eventrelated brain potentials.</title>
<date>1999</date>
<journal>Journal of Memory and Language</journal>
<pages>41--147</pages>
<marker>van Berkum, Brown, Hagoort, 1999</marker>
<rawString>van Berkum, Jos J. A., Colin M. Brown, and Peter Hagoort. 1999. Early referential context effects in sentence processing: Evidence from eventrelated brain potentials. Journal of Memory and Language 41:147–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barton Wright</author>
<author>Merrill F Garrett</author>
</authors>
<title>Lexical decision in sentences: Effects of syntactic structure.</title>
<date>1984</date>
<journal>Memory and Cognition</journal>
<pages>12--31</pages>
<contexts>
<context position="2471" citStr="Wright and Garrett 1984" startWordPosition="370" endWordPosition="373"> or a semantically congruous sentence fragment is processed faster (Stanovich and West 1981; van Berkum et al. 1999; Clifton et al. 2007). Another example is argument prediction: listeners are able to launch eye-movements to the predicted argument of a verb before having encountered it, e.g., they will fixate an edible object as soon as they hear the word eat (Altmann and Kamide 1999). The second type of prediction is syntactic prediction. Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (Wright and Garrett 1984). Another instance of syntactic prediction has been reported by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the b</context>
</contexts>
<marker>Wright, Garrett, 1984</marker>
<rawString>Wright, Barton and Merrill F. Garrett. 1984. Lexical decision in sentences: Effects of syntactic structure. Memory and Cognition 12:31–45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>