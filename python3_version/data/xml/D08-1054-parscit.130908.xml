<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.969038">
HTM: A Topic Model for Hypertexts
</title>
<author confidence="0.984746">
Congkai Sun*
</author>
<affiliation confidence="0.998953">
Department of Computer Science
Shanghai Jiaotong University
</affiliation>
<address confidence="0.664717">
Shanghai, P. R. China
</address>
<email confidence="0.9835">
martinsck@hotmail.com
</email>
<author confidence="0.994367">
Zhenfu Cao
</author>
<affiliation confidence="0.9990505">
Department of Computer Science
Shanghai Jiaotong University
</affiliation>
<address confidence="0.665015">
Shanghai, P. R. China
</address>
<email confidence="0.983785">
zfcao@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.99281" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994645961538462">
Previously topic models such as PLSI (Prob-
abilistic Latent Semantic Indexing) and LDA
(Latent Dirichlet Allocation) were developed
for modeling the contents of plain texts. Re-
cently, topic models for processing hyper-
texts such as web pages were also proposed.
The proposed hypertext models are generative
models giving rise to both words and hyper-
links. This paper points out that to better rep-
resent the contents of hypertexts it is more es-
sential to assume that the hyperlinks are fixed
and to define the topic model as that of gen-
erating words only. The paper then proposes
a new topic model for hypertext processing,
referred to as Hypertext Topic Model (HTM).
HTM defines the distribution of words in a
document (i.e., the content of the document)
as a mixture over latent topics in the document
itself and latent topics in the documents which
the document cites. The topics are further
characterized as distributions of words, as in
the conventional topic models. This paper fur-
ther proposes a method for learning the HTM
model. Experimental results show that HTM
outperforms the baselines on topic discovery
and document classification in three datasets.
</bodyText>
<sectionHeader confidence="0.998122" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995102">
Topic models are probabilistic and generative mod-
els representing contents of documents. Examples
of topic models include PLSI (Hofmann, 1999) and
LDA (Blei et al., 2003). The key idea in topic mod-
eling is to represent topics as distributions of words
</bodyText>
<note confidence="0.695027833333333">
* This work was conducted when the first author visited
Microsoft Research Asia as an intern.
Bin Gao
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
</note>
<email confidence="0.697076">
bingao@microsoft.com
</email>
<author confidence="0.73838">
Hang Li
</author>
<affiliation confidence="0.706726">
Microsoft Research Asia
</affiliation>
<address confidence="0.657064">
No.49 Zhichun Road
Beijing, P. R. China
</address>
<email confidence="0.864509">
hangli@microsoft.com
</email>
<bodyText confidence="0.9999805">
and define the distribution of words in document
(i.e., the content of document) as a mixture over hid-
den topics. Topic modeling technologies have been
applied to natural language processing, text min-
ing, and information retrieval, and their effective-
ness have been verified.
In this paper, we study the problem of topic mod-
eling for hypertexts. There is no doubt that this is
an important research issue, given the fact that more
and more documents are available as hypertexts cur-
rently (such as web pages). Traditional work mainly
focused on development of topic models for plain
texts. It is only recently several topic models for pro-
cessing hypertexts were proposed, including Link-
LDA and Link-PLSA-LDA (Cohn and Hofmann,
2001; Erosheva et al., 2004; Nallapati and Cohen,
2008).
We point out that existing models for hypertexts
may not be suitable for characterizing contents of
hypertext documents. This is because all the models
are assumed to generate both words and hyperlinks
(outlinks) of documents. The generation of the latter
type of data, however, may not be necessary for the
tasks related to contents of documents.
In this paper, we propose a new topic model for
hypertexts called HTM (Hypertext Topic Model),
within the Bayesian learning approach (it is simi-
lar to LDA in that sense). In HTM, the hyperlinks
of hypertext documents are supposed to be given.
Each document is associated with one topic distribu-
tion. The word distribution of a document is defined
as a mixture of latent topics of the document itself
and latent topics of documents which the document
cites. The topics are further defined as distributions
</bodyText>
<note confidence="0.826261333333333">
514
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 514–522,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999928888888889">
of words. That means the content (topic distribu-
tions for words) of a hypertext document is not only
determined by the topics of itself but also the top-
ics of documents it cites. It is easy to see that HTM
contains LDA as a special case. Although the idea of
HTM is simple and straightforward, it appears that
this is the first work which studies the model.
We further provide methods for learning and in-
ference of HTM. Our experimental results on three
web datasets show that HTM outperforms the base-
line models of LDA, Link-LDA, and Link-PLSA-
LDA, in the tasks of topic discovery and document
classification.
The rest of the paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed HTM model and its learning and infer-
ence methods. Experimental results are presented in
Section 4. Conclusions are made in the last section.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980533333334">
There has been much work on topic modeling. Many
models have been proposed including PLSI (Hof-
mann, 1999), LDA (Blei et al., 2003), and their
extensions (Griffiths et al., 2005; Blei and Lafferty,
2006; Chemudugunta et al., 2007). Inference and
learning methods have been developed, such as vari-
ational inference (Jordan et al., 1999; Wainwright
and Jordan, 2003), expectation propagation (Minka
and Lafferty, 2002), and Gibbs sampling (Griffiths
and Steyvers, 2004). Topic models have been uti-
lized in topic discovery (Blei et al., 2003), document
retrieval (Xing Wei and Bruce Croft, 2006), docu-
ment classification (Blei et al., 2003), citation analy-
sis (Dietz et al., 2007), social network analysis (Mei
et al., 2008), and so on. Most of the existing models
are for processing plain texts. There are also models
for processing hypertexts, for example, (Cohn and
Hofmann, 2001; Nallapati and Cohen, 2008; Gru-
ber et al., 2008; Dietz et al., 2007), which are most
relevant to our work.
Cohn and Hofmann (2001) introduced a topic
model for hypertexts within the framework of PLSI.
The model, which is a combination of PLSI and
PHITS (Cohn and Chang, 2000), gives rise to both
the words and hyperlinks (outlinks) of the document
in the generative process. The model is useful when
the goal is to understand the distribution of links
as well as the distribution of words. Erosheva et
al (2004) modified the model by replacing PLSI with
LDA. We refer to the modified mode as Link-LDA
and take it as a baseline in this paper. Note that the
above two models do not directly associate the top-
ics of the citing document with the topics of the cited
documents.
Nallapati and Cohn (2008) proposed an extension
of Link-LDA called Link-PLSA-LDA, which is an-
other baseline in this paper. Assuming that the cit-
ing and cited documents share similar topics, they
explicitly model the information flow from the cit-
ing documents to the cited documents. In Link-
PLSA-LDA, the link graph is converted into a bi-
partite graph in which links are connected from cit-
ing documents to cited documents. If a document
has both inlinks and outlinks, it will be duplicated
on both sides of the bipartite graph. The generative
process for the citing documents is similar to that of
Link-LDA, while the cited documents have a differ-
ent generative process.
Dietz et al (2007) proposed a topic model for ci-
tation analysis. Their goal is to find topical influ-
ence of publications in research communities. They
convert the citation graph (created from the publica-
tions) into a bipartite graph as in Link-PLSA-LDA.
The content of a citing document is assumed to be
generated by a mixture over the topic distribution
of the citing document and the topic distributions of
the cited documents. The differences between the
topic distributions of citing and cited documents are
measured, and the cited documents which have the
strongest influence on the citing document are iden-
tified.
Note that in most existing models described above
the hyperlinks are assumed to be generated and link
prediction is an important task, while in the HTM
model in this paper, the hyperlinks are assumed to
be given in advance, and the key task is topic iden-
tification. In the existing models for hypertexts, the
content of a document (the word distribution of the
document) are not decided by the other documents.
In contrast, in HTM, the content of a document is
determined by itself as well as its cited documents.
Furthermore, HTM is a generative model which can
generate the contents of all the hypertexts in a col-
lection, given the link structure of the collection.
Therefore, if the goal is to accurately learn and pre-
</bodyText>
<page confidence="0.966741">
515
</page>
<tableCaption confidence="0.999562">
Table 1: Notations and explanations.
</tableCaption>
<table confidence="0.653106888888889">
T Number of topics
D Documents in corpus
D Number of documents
αg, αp Hyperparameters for B and ,C3
A Hyperparameter to control the weight between
the citing document and the cited documents
B Topic distributions for all documents
0 Word distribution for topic
b, c, z Hidden variables for generating word
</table>
<tableCaption confidence="0.824293444444445">
d document (index)
wd Word sequence in document d
Nd Number of words in document d
Ld Number of documents cited by document d
Id Set of cited documents for document d
ids Index of lth cited document of document d
�d Distribution on cited documents of document d
Bd Topic distribution associated with document d
bdn Decision on way of generating nth word in doc-
</tableCaption>
<bodyText confidence="0.628524166666667">
ument d
cdn Cited document that generates nth word in doc-
ument d
zdn Topic of nth word in document d
dict contents of documents, the use of HTM seems
more reasonable.
</bodyText>
<sectionHeader confidence="0.986335" genericHeader="method">
3 Hypertext Topic Model
</sectionHeader>
<subsectionHeader confidence="0.997169">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.98889792">
In topic modeling, a probability distribution of
words is employed for a given document. Specifi-
cally, the probability distribution is defined as a mix-
ture over latent topics, while each topic is future
characterized by a distribution of words (Hofmann,
1999; Blei et al., 2003). In this paper, we introduce
an extension of LDA model for hypertexts. Table 1
gives the major notations and their explanations.
The graphic representation of conventional LDA
is given in Figure 1(a). The generative process of
LDA has three steps. Specifically, in each document
a topic distribution is sampled from a prior distribu-
tion defined as Dirichlet distribution. Next, a topic is
sampled from the topic distribution of the document,
which is a multinominal distribution. Finally, a word
is sampled according to the word distribution of the
topic, which also forms a multinormal distribution.
The graphic representation of HTM is given in
Figure 1(b). The generative process of HTM is de-
scribed in Algorithm 1. First, a topic distribution
is sampled for each document according to Dirich-
let distribution. Next, for generating a word in a
document, it is decided whether to use the current
Algorithm 1 Generative Process of HTM
for each document d do
</bodyText>
<figure confidence="0.892294133333333">
Draw Bd — Dir(αB).
end for
for each word wdn do
if Ld &gt; 0 then
Draw bdn — Ber(A)
Draw cdn — Uni( d)
if bdn = 1 then
Draw zdn — Multi(Bd)
else
Draw zdn — Multi(OIdcdn )
end if
else
Draw a topic zdn — Multi(0d)
end if
Draw a word wdn — P(wdn  |zdn, Q)
</figure>
<subsectionHeader confidence="0.426381">
end for
</subsectionHeader>
<bodyText confidence="0.999866722222222">
document or documents which the document cites.
(The weight between the citing document and cited
documents is controlled by an adjustable hyper-
parameter A.) It is also determined which cited doc-
ument to use (if it is to use cited documents). Then, a
topic is sampled from the topic distribution of the se-
lected document. Finally, a word is sampled accord-
ing to the word distribution of the topic. HTM natu-
rally mimics the process of writing a hypertext docu-
ment by humans (repeating the processes of writing
native texts and anchor texts).
The formal definition of HTM is given be-
low. Hypertext document d has Nd words
wd = wd1 · · · wdNd and Ld cited documents Id =
{id1, . . . , idLd1. The topic distribution of d is Bd
and topic distributions of the cited documents are
Bi, i E Id. Given A, 0, and Q, the conditional proba-
bility distribution of wd is defined as:
</bodyText>
<equation confidence="0.9698162">
p(wd|A, 0, Q) = Nd � � p(cdn|W
H bdn p(bdn|A)
n=1 cdn
� p(zdn|Bd)bdnp(zdn|Bidcdn )1−bdnp(wdn|zdn, Q).
zdn
</equation>
<bodyText confidence="0.999926">
Here fid, bdn, cdn, and zdn are hidden vari-
ables. When generating a word wdn, bdn determines
whether it is from the citing document or the cited
documents. cdn determines which cited document it
</bodyText>
<page confidence="0.608385">
516
</page>
<bodyText confidence="0.9864686">
is when bdn = 0. In this paper, for simplicity we as-
sume that the cited documents are equally likely to
be selected, i.e., ξdi = Ld1 .
Note that θ represents the topic distributions of
all the documents. For any d, its word distribution
is affected by both θd and θi, i ∈ Id. There is a
propagation of topics from the cited documents to
the citing document through the use of θi, i ∈ Id.
For a hypertext document d that does not have
cited documents. The conditional probability dis-
tribution degenerates to LDA:
p(zdn|θd)p(wdn|zdn, β).
By taking the product of the marginal probabil-
ities of hypertext documents, we obtain the condi-
tional probability of the corpus D given the hyper-
</bodyText>
<equation confidence="0.9637265">
parameters λ, αθ, β,
p(D|λ, αθ, β) =
Z YD YNd X X
p(θd|αθ) p(bdn|λ) p(cdn|ξd)
d=1 n=1 bdn cdn
X p(zdn|θd)bdnp(zdn|θIdcdn )1−bdn
zdn
p(wdn|zdn, β)dθ. (1)
</equation>
<bodyText confidence="0.999986533333333">
Note that the probability function (1) also covers the
special cases in which documents do not have cited
documents.
In HTM, the content of a document is decided by
the topics of the document as well as the topics of
the documents which the document cites. As a result
contents of documents can be ‘propagated’ along the
hyperlinks. For example, suppose web page A cites
page B and page B cites page C, then the content of
page A is influenced by that of page B, and the con-
tent of page B is further influenced by the content
of page C. Therefore, HTM is able to more accu-
rately represent the contents of hypertexts, and thus
is more useful for text processing such as topic dis-
covery and document classification.
</bodyText>
<subsectionHeader confidence="0.992693">
3.2 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.999910857142857">
An exact inference of the posterior probability of
HTM may be intractable, we employ the mean field
variational inference method (Wainwright and Jor-
dan, 2003; Jordan et al., 1999) to conduct approxi-
mation. Let I[·] be an indicator function. We first
define the following factorized variational posterior
distribution q with respect to the corpus:
</bodyText>
<equation confidence="0.998074285714286">
D
q= Y q(θd|γd)
d=1
YNdµ
¶I[Ld&gt;0]
q(xdn|ρdn)(q(cdn|ψdn) q(zdn|φdn) ,
n=1
</equation>
<bodyText confidence="0.999565666666667">
where γ, ψ, φ, and ρ denote free variational parame-
ters. Parameter γ is the posterior Dirichlet parameter
corresponding to the representations of documents
in the topic simplex. Parameters ψ, φ, and ρ cor-
respond to the posterior distributions of their asso-
ciated random variables. We then minimize the KL
divergence between q and the true posterior proba-
bility of the corpus by taking derivatives of the loss
function with respect to variational parameters. The
solution is listed as below.
Let βiv be p(wvdn = 1|zi = 1) for the word v. If
Ld &gt; 0, we have
</bodyText>
<equation confidence="0.868758625">
E-step:
γdi = αθi +
X Nd0 (1 − ρd0n)ψd0nlφd0ni .
n=1
φdni ∝ βiv exp ©ρdnEq [log (θdi) |γd]
+ (1 − ρdn) XLd ψdnlEq [log (θIdli) |γIdl] ª .
l=1
ρdn = 1 + 3exp © X ¡(φdniEq[log(θdi)|γd]
µ k
i=1
Ld
ψdnlφdniEq[log(θIdli)|γIdl]¢
l=1
+ log λ − log ¡1 − λ¢ ª´ 1¶ 1 .
Nd
p(Wd|θd,β) = Y X
n=1 zdn
Ld0
XNd
n=1
ρdnφdni +
X
l=1
I [id0l = d]
</equation>
<figure confidence="0.993519213114754">
XD
d0=1
517
�
aθ
(b) HTM
aβ
B
z
a
T
w
Nd
N M
Cited Documents Citing Documents
�
d’ w
z
z
d
Ld
αq
αp
C
B
D
Id
b
�
T
z
w
Nd
d
D
E
A
αθ
B
z
αβ
Q
T
w
Nd
D
(a) LDA
αβ
Q
T
z
w
Nd
αθ
B
z
d’
Ld
D
�
(c) Link-LDA (d) Link-PLSA-LDA
</figure>
<figureCaption confidence="0.999821">
Figure 1: Graphical model representations
</figureCaption>
<bodyText confidence="0.999960428571429">
In order to cope with the data sparseness problem
due to large vocabulary, we employ the same tech-
nique as that in (Blei et al., 2003). To be specific,
we treat β as a K * V random matrix, with each row
being independently drawn from a Dirichlet distri-
bution βi — Dir(αβ) . Variational inference is
modified appropriately.
</bodyText>
<sectionHeader confidence="0.997216" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999954">
We compared the performances of HTM and three
baseline models: LDA, Link-LDA, and Link-PLSA-
LDA in topic discovery and document classification.
Note that LDA does not consider the use of link in-
formation; we included it here for reference.
</bodyText>
<equation confidence="0.912184333333333">
ϕdnl oc ξdl exp{(1 — ρdn)
k
φdniEq[log(θIdli)|γIdl]1.
i=1
Otherwise,
Nd
γdi = αθi + φdni +
n=1
D Ld0 I [id0l = d] � Nd0 (1 — ρd0n)ψd0nlφd0ni .
d0=1 � n=1
l=1
φdni oc βiv exp {Eq [log (θdi) |γd] }.
</equation>
<bodyText confidence="0.8463245">
From the first two equations we can see that the
cited documents and the citing document jointly af-
fect the distribution of the words in the citing docu-
ment.
M-step:
φdniwjdn.
</bodyText>
<subsectionHeader confidence="0.965747">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999667375">
We made use of three datasets. The documents in the
datasets were processed by using the Lemur Took
kit (http://www.lemurproject.org), and the low fre-
quency words in the datasets were removed.
The first dataset WebKB (available at
http://www.cs.cmu.edu/˜webkb) contains six
subjects (categories). There are 3,921 documents
and 7,359 links. The vocabulary size is 5,019.
</bodyText>
<figure confidence="0.51642">
D
d=1
βij oc
Nd
n=1
518
</figure>
<bodyText confidence="0.9902504">
The second dataset Wikipedia (available at
http://www.mpi-inf.mpg.de/˜angelova) contains
four subjects (categories): Biology, Physics, Chem-
istry, and Mathematics. There are 2,970 documents
and 45,818 links. The vocabulary size is 3,287.
The third dataset is ODP composed of homepages
of researchers and their first level outlinked pages
(cited documents). We randomly selected five sub-
jects from the ODP archive. They are Cognitive
Science (CogSci), Theory, NeuralNetwork (NN),
Robotics, and Statistics. There are 3,679 pages and
2,872 links. The vocabulary size is 3,529.
WebKB and Wikipedia are public datasets widely
used in topic model studies. ODP was collected by
us in this work.
</bodyText>
<subsectionHeader confidence="0.995754">
4.2 Topic Discovery
</subsectionHeader>
<bodyText confidence="0.999997666666667">
We created four topic models HTM, LDA, Link-
LDA, and Link-PLSA-LDA using all the data in
each of the three datasets, and evaluated the top-
ics obtained in the models. We heuristically set the
numbers of topics as 10 for ODP, 12 for WebKB,
and 8 for Wikipedia (i.e., two times of the number
of true subjects). We found that overall HTM can
construct more understandable topics than the other
models. Figure 2 shows the topics related to the
subjects created by the four models from the ODP
dataset. HTM model can more accurately extract
the three topics: Theory, Statistic, and NN than the
other models. Both LDA and Link-LDA had mixed
topics, labeled as ‘Mixed’ in Figure 2. Link-PLSA-
LDA missed the topic of Statistics. Interestingly, all
the four models split Cognitive Science into two top-
ics (showed as CogSci-1 and CogSci-2), probably
because the topic itself is diverse.
</bodyText>
<subsectionHeader confidence="0.997395">
4.3 Document Classification
</subsectionHeader>
<bodyText confidence="0.999186272727273">
We applied the four models in the three datasets to
document classification. Specifically, we used the
word distributions of documents created by the mod-
els as feature vectors of the documents and used the
subjects in the datasets as categories. We further
randomly divided each dataset into three parts (train-
ing, validation, and test) and conducted 3-fold cross-
validation experiments. In each trial, we trained an
SVM classifier with the training data, chose param-
eters with the validation data, and conducted evalu-
ation on classification with the test data. For HTM,
</bodyText>
<tableCaption confidence="0.9805935">
Table 2: Classification accuracies in 3-fold cross-
validation.
</tableCaption>
<table confidence="0.99988125">
LDA HTM Link-LDA Link-PLSA-LDA
ODP 0.640 0.698 0.535 0.581
WebKB 0.786 0.795 0.775 0.774
Wikipedia 0.845 0.866 0.853 0.855
</table>
<tableCaption confidence="0.9479175">
Table 3: Sign-test results between HTM and the three
baseline models.
</tableCaption>
<table confidence="0.9966155">
LDA Link-LDA Link-PLSA-LDA
ODP 0.0237 2.15e-05 0.000287
WebKB 0.0235 0.0114 0.00903
Wikipedia 1.79e-05 0.00341 0.00424
</table>
<bodyText confidence="0.999904777777778">
we chose the best A value with the validation set in
each trial. Table 2 shows the classification accura-
cies. We can see that HTM performs better than the
other models in all three datasets.
We conducted sign-tests on all the results of the
datasets. In most cases HTM performs statistically
significantly better than LDA, Link-LDA, and Link-
PLSA-LDA (p-value &lt; 0.05). The test results are
shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.986142">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999698461538461">
We conducted analysis on the results to see why
HTM can work better. Figure 3 shows an example
homepage from the ODP dataset, where superscripts
denote the indexes of outlinked pages. The home-
page contains several topics, including Theory, Neu-
ral network, Statistics, and others, while the cited
pages contain detailed information about the topics.
Table 4 shows the topics identified by the four mod-
els for the homepage. We can see that HTM can
really more accurately identify topics than the other
models.
The major reason for the better performance by
HTM seems to be that it can fully leverage the infor-
</bodyText>
<tableCaption confidence="0.922825666666667">
Table 4: Comparison of topics identified by the four mod-
els for the example homepage. Only topics with proba-
bilities &gt; 0.1 and related to the subjects are shown.
</tableCaption>
<table confidence="0.879007281553398">
Model Topics Probabilities
LDA Mixed 0.537
HTM Theory 0.229
NN 0.278
Statistics 0.241
Link-LDA Statistics 0.281
Link-PLSA-LDA Theory 0.527
CogSci-2 0.175
519
(a) LDA (b) HTM
Mixed NN Robot CogSci-1 CogSci-2
statistic learn robot visual conscious
compute conference project model psychology
algorithm system file experiment language
theory neural software change cognitive
complex network code function experience
mathematics model program response brain
model international data process theory
science compute motor data philosophy
computation ieee read move science
problem proceedings start observe online
random process build perception mind
analysis computation comment effect concept
paper machine post figure physical
method science line temporal problem
journal artificial include sensory content
Theory Statistics NN Robot CogSci-1 CogSci-2
compute model learn robot conscious memory
science statistic system project visual psychology
algorithm data network software experience language
theory experiment neural motor change science
complex sample conference sensor perception cognitive
computation process model code move brain
mathematics method compute program theory human
paper analysis ieee build online neuroscience
problem response international line physical journal
lecture figure proceedings board concept society
random result machine read problem trauma
journal temporal process power philosophy press
bound probable computation type object learn
graph observe artificial comment content abuse
proceedings test intelligence post view associate
(c) Link-LDA
Statistics Mixed Robot CogSci-1 CogSci-2
statistic compute robot visual conscious
model conference project model psychology
data system software experiment cognitive
analysis learn file change language
method network motor function brain
learn computation robotics vision science
sample proceedings informatik process memory
algorithm neural program perception theory
process ieee build move philosophy
bayesian algorithm board response press
application international sensor temporal online
random science power object neuroscience
distribution complex code observe journal
simulate theory format sensory human
mathematics journal control figure mind
(d) Link-PLSA-LDA
Theory NN Robot CogSci-1 CogSci-2
compute conference robot conscious model
algorithm learn code experience process
computation science project language visual
theory international typeof book data
complex system motor change experiment
science compute control make function
mathematics network system problem learn
network artificial serve brain neural
paper ieee power world system
journal intelligence program read perception
proceedings robot software case represent
random technology file than vision
system proceedings build mind response
problem machine pagetracker theory object
lecture neural robotics content abstract
Figure 2: Topics identified by four models
Radford M.Neal
Professor, Dept. of Statistics and Dept. of Computer Science, University of Toronto
Ism currently highlighting the following:
* A new R function for performing univariate slice sampling.1
* A workshop paper on Computing Likelihood Functions for High-Energy Physics
Experiments when Distributions are Defined by Simulators with Nuisance Parameters.2
* Slides from a talk at the Third Workshop on Monte Carlo Methods on
“Short-Cut MCMC: An Alternative to Adaptation”, May 2007: Postscript, PDF.
Courses Ism teaching in Fall 2008 :
* STA 437: Methods for Multivariate Data3
* STA 3000: Advanced Theory of Statistics4
You can also find information on courses I’ve taught in the past.5
You can also get to information on :
* Research interests6 (with pointers to publications)
* Current and former graduate students7
* Current and former postdocs8
* Curriculum Vitae: PostScript, or PDF.
* Full publications list9
* How to contact me10
* Links to various places11
If you know what you want already, you may wish to go directly to :
* Software available on-line12
* Papers available on-line13
* Slides from talks14
* Miscellaneous other stuff15
Information in this hierarchy was last updated 2008-06-20.
</table>
<figureCaption confidence="0.724965">
Figure 3: An example homepage: http://www.cs.utoronto.ca/˜ radford/
</figureCaption>
<page confidence="0.522794">
520
</page>
<tableCaption confidence="0.993767">
Table 5: Word assignment in the example homepage.
</tableCaption>
<table confidence="0.999896038461538">
Word bdn cdn Topic Probability
mcmc 0.544 2 Stat 0.949
experiment 0.546 2 Stat 0.956
neal 0.547 8 NN 0.985
likelihood 0.550 2 Stat 0.905
sample 0.557 2 Stat 0.946
statistic 0.559 2 Stat 0.888
parameter 0.563 2 Stat 0.917
perform 0.565 2 Stat 0.908
carlo 0.568 2 Stat 0.813
monte 0.570 2 Stat 0.802
toronto 0.572 8 NN 0.969
distribution 0.578 2 Stat 0.888
slice 0.581 2 Stat 0.957
energy 0.581 13 NN 0.866
adaptation 0.591 7 Stat 0.541
teach 0.999 11 Other 0.612
current 0.999 11 Other 0.646
curriculum 0.999 11 Other 0.698
want 0.999 11 Other 0.706
highlight 0.999 10 Other 0.786
professor 0.999 11 Other 0.764
academic 0.999 11 Other 0.810
student 0.999 11 Other 0.817
contact 0.999 11 Other 0.887
graduate 0.999 11 Other 0.901
</table>
<tableCaption confidence="0.971379">
Table 6: Most salient topics in cited pages.
</tableCaption>
<figure confidence="0.8818178">
URL Topic Probability
2 Stat 0.690
7 Stat 0.467
8 NN 0.786
13 NN 0.776
</figure>
<figureCaption confidence="0.90568225">
Figure 4: Classification accuracies on three datasets with
different A values. The cross marks on the curves cor-
respond to the average values of A in the 3-fold cross-
validation experiments.
</figureCaption>
<bodyText confidence="0.999981513513513">
mation from the cited documents. We can see that
the content of the example homepage is diverse and
not very rich. It might be hard for the other base-
line models to identify topics accurately. In con-
trast, HTM can accurately learn topics by the help
of the cited documents. Specifically, if the content of
a document is diverse, then words in the document
are likely to be assigned into wrong topics by the
existing approaches. In contrast, in HTM with prop-
agation of topic distributions from cited documents,
the words of a document can be more accurately as-
signed into topics. Table 5 shows the first 15 words
and the last 10 words for the homepage given by
HTM, in ascending order of bdn, which measures
the degree of influence from the cited documents on
the words (the smaller the stronger). The table also
gives the values of cdn, indicating which cited docu-
ments have the strongest influence. Furthermore, the
topics having the largest posterior probabilities for
the words are also shown. We can see that the words
’experiment’, ’sample’, ’parameter’, ’perform’, and
’energy’ are accurately classified. Table 6 gives the
most salient topics of cited documents. It also shows
the probabilities of the topics given by HTM. We can
see that there is a large agreement between the most
salient topics in the cited documents and the topics
which are affected the most in the citing document.
Parameter A is the only parameter in HTM which
needs to be tuned. We found that the performance of
HTM is not very sensitive to the values of A, which
reflects the degree of influence from the cited doc-
uments to the citing document. HTM can perform
well with different A values. Figure 4 shows the clas-
sification accuracies of HTM with respect to differ-
ent A values for the three datasets. We can see that
HTM works better than the other models in most of
the cases (cf., Table 2).
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999945">
In this paper, we have proposed a novel topic
model for hypertexts called HTM. Existing models
for processing hypertexts were developed based on
the assumption that both words and hyperlinks are
stochastically generated by the model. The gener-
ation of latter type of data is actually unnecessary
for representing contents of hypertexts. In the HTM
model, it is assumed that the hyperlinks of hyper-
</bodyText>
<page confidence="0.642797">
521
</page>
<bodyText confidence="0.9999884">
texts are given and only the words of the hypertexts
are stochastically generated. Furthermore, the word
distribution of a document is determined not only
by the topics of the document in question but also
from the topics of the documents which the doc-
ument cites. It can be regarded as ‘propagation’
of topics reversely along hyperlinks in hypertexts,
which can lead to more accurate representations than
the existing models. HTM can naturally mimic hu-
man’s process of creating a document (i.e., by con-
sidering using the topics of the document and at the
same time the topics of the documents it cites). We
also developed methods for learning and inferring
an HTM model within the same framework as LDA
(Latent Dirichlet Allocation). Experimental results
show that the proposed HTM model outperforms
the existing models of LDA, Link-LDA, and Link-
PLSA-LDA on three datasets for topic discovery and
document classification.
As future work, we plan to compare the HTM
model with other existing models, to develop learn-
ing and inference methods for handling extremely
large-scale data sets, and to combine the current
method with a keyphrase extraction method for ex-
tracting keyphrases from web pages.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9985425">
We thank Eric Xing for his valuable comments on
this work.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789563380282">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press /Addison-
Wesley.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet allocation. Journal of machine Learning
Research, 3:993–1022.
David Blei and John Lafferty. 2005. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 12.
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd international con-
ference on Machine learning.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
David Cohn and Huan Chang. 2000. Learning to Proba-
bilistically Identify Authoritative Documents. In Pro-
ceedings of the 17rd international conference on Ma-
chine learning.
David Cohn and Thomas Hofmann. 2001. The missing
link - a probabilistic model of document content and
hypertext connectivity. In Neural Information Pro-
cessing Systems 13.
Laura Dietz, Steffen Bickel and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of the 24th international conference on Ma-
chine learning.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. In Proceedings of the National Academy of
Sciences, 101:5220–5227.
Thomas Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. In Proceedings of the National
Academy of Sciences, 101 (suppl. 1) .
Thomas Griffiths, Mark Steyvers, David Blei, and Joshua
Tenenbaum. 2005. Integrating Topics and Syntax. In
Advances in Neural Information Processing Systems,
17.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008.
Latent Topic Models for Hypertext. In Proceedings of
the 24th Conference on Uncertainty in Artificial Intel-
ligence.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Analysis. In Proceedings of the 15th Conference on
Uncertainty in Artificial Intelligence.
Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola,
and Lawrence Saul. 1999. An Introduction to Varia-
tional Methods for Graphical Models. Machine Learn-
ing, 37(2):183–233.
QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008. Topic Modeling with Network Regular-
ization. In Proceeding of the 17th international con-
ference on World Wide Web.
Thomas Minka and John Lafferty. 2002. Expectation-
Propagation for the Generative Aspect Model. In Pro-
ceedings of the 18th Conference in Uncertainty in Ar-
tificial Intelligence.
Ramesh Nallapati and William Cohen. 2008. Link-
PLSA-LDA: A new unsupervised model for topics and
influence of blogs. In International Conference for
Webblogs and Social Media.
Martin Wainwright, and Michael Jordan. 2003. Graph-
ical models, exponential families, and variational in-
ference. In UC Berkeley, Dept. of Statistics, Technical
Report, 2003.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
</reference>
<page confidence="0.871462">
522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131680">
<title confidence="0.6727805">HTM: A Topic Model for Hypertexts Department of Computer</title>
<author confidence="0.561763">Shanghai Jiaotong Shanghai</author>
<author confidence="0.561763">P R</author>
<email confidence="0.998726">martinsck@hotmail.com</email>
<author confidence="0.680668">Zhenfu</author>
<affiliation confidence="0.868416">Department of Computer Shanghai Jiaotong</affiliation>
<address confidence="0.76387">Shanghai, P. R.</address>
<email confidence="0.942716">zfcao@cs.sjtu.edu.cn</email>
<abstract confidence="0.999657962962963">Previously topic models such as PLSI (Probabilistic Latent Semantic Indexing) and LDA (Latent Dirichlet Allocation) were developed modeling the contents of Retopic models for processing hyperas web pages were also proposed. The proposed hypertext models are generative giving rise to both hyper- This paper points out that to better represent the contents of hypertexts it is more essential to assume that the hyperlinks are fixed and to define the topic model as that of generating words only. The paper then proposes a new topic model for hypertext processing, referred to as Hypertext Topic Model (HTM). HTM defines the distribution of words in a document (i.e., the content of the document) as a mixture over latent topics in the document latent topics in the documents which document The topics are further characterized as distributions of words, as in the conventional topic models. This paper further proposes a method for learning the HTM model. Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press /AddisonWesley.</publisher>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press /AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1621" citStr="Blei et al., 2003" startWordPosition="249" endWordPosition="252">ontent of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites. The topics are further characterized as distributions of words, as in the conventional topic models. This paper further proposes a method for learning the HTM model. Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets. 1 Introduction Topic models are probabilistic and generative models representing contents of documents. Examples of topic models include PLSI (Hofmann, 1999) and LDA (Blei et al., 2003). The key idea in topic modeling is to represent topics as distributions of words * This work was conducted when the first author visited Microsoft Research Asia as an intern. Bin Gao Microsoft Research Asia No.49 Zhichun Road Beijing, P. R. China bingao@microsoft.com Hang Li Microsoft Research Asia No.49 Zhichun Road Beijing, P. R. China hangli@microsoft.com and define the distribution of words in document (i.e., the content of document) as a mixture over hidden topics. Topic modeling technologies have been applied to natural language processing, text mining, and information retrieval, and th</context>
<context position="4809" citStr="Blei et al., 2003" startWordPosition="773" endWordPosition="776">ning and inference of HTM. Our experimental results on three web datasets show that HTM outperforms the baseline models of LDA, Link-LDA, and Link-PLSALDA, in the tasks of topic discovery and document classification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so o</context>
<context position="9528" citStr="Blei et al., 2003" startWordPosition="1574" endWordPosition="1577">ibution on cited documents of document d Bd Topic distribution associated with document d bdn Decision on way of generating nth word in document d cdn Cited document that generates nth word in document d zdn Topic of nth word in document d dict contents of documents, the use of HTM seems more reasonable. 3 Hypertext Topic Model 3.1 Model In topic modeling, a probability distribution of words is employed for a given document. Specifically, the probability distribution is defined as a mixture over latent topics, while each topic is future characterized by a distribution of words (Hofmann, 1999; Blei et al., 2003). In this paper, we introduce an extension of LDA model for hypertexts. Table 1 gives the major notations and their explanations. The graphic representation of conventional LDA is given in Figure 1(a). The generative process of LDA has three steps. Specifically, in each document a topic distribution is sampled from a prior distribution defined as Dirichlet distribution. Next, a topic is sampled from the topic distribution of the document, which is a multinominal distribution. Finally, a word is sampled according to the word distribution of the topic, which also forms a multinormal distribution</context>
<context position="15192" citStr="Blei et al., 2003" startWordPosition="2631" endWordPosition="2634">ψdnlEq [log (θIdli) |γIdl] ª . l=1 ρdn = 1 + 3exp © X ¡(φdniEq[log(θdi)|γd] µ k i=1 Ld ψdnlφdniEq[log(θIdli)|γIdl]¢ l=1 + log λ − log ¡1 − λ¢ ª´ 1¶ 1 . Nd p(Wd|θd,β) = Y X n=1 zdn Ld0 XNd n=1 ρdnφdni + X l=1 I [id0l = d] XD d0=1 517 � aθ (b) HTM aβ B z a T w Nd N M Cited Documents Citing Documents � d’ w z z d Ld αq αp C B D Id b � T z w Nd d D E A αθ B z αβ Q T w Nd D (a) LDA αβ Q T z w Nd αθ B z d’ Ld D � (c) Link-LDA (d) Link-PLSA-LDA Figure 1: Graphical model representations In order to cope with the data sparseness problem due to large vocabulary, we employ the same technique as that in (Blei et al., 2003). To be specific, we treat β as a K * V random matrix, with each row being independently drawn from a Dirichlet distribution βi — Dir(αβ) . Variational inference is modified appropriately. 4 Experimental Results We compared the performances of HTM and three baseline models: LDA, Link-LDA, and Link-PLSALDA in topic discovery and document classification. Note that LDA does not consider the use of link information; we included it here for reference. ϕdnl oc ξdl exp{(1 — ρdn) k φdniEq[log(θIdli)|γIdl]1. i=1 Otherwise, Nd γdi = αθi + φdni + n=1 D Ld0 I [id0l = d] � Nd0 (1 — ρd0n)ψd0nlφd0ni . d0=1 �</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated Topic Models.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems 12.</booktitle>
<marker>Blei, Lafferty, 2005</marker>
<rawString>David Blei and John Lafferty. 2005. Correlated Topic Models. In Advances in Neural Information Processing Systems 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning.</booktitle>
<contexts>
<context position="4880" citStr="Blei and Lafferty, 2006" startWordPosition="784" endWordPosition="787">atasets show that HTM outperforms the baseline models of LDA, Link-LDA, and Link-PLSALDA, in the tasks of topic discovery and document classification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There ar</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David Blei and John Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaitanya Chemudugunta</author>
<author>Padhraic Smyth</author>
<author>Mark Steyvers</author>
</authors>
<title>Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems 19.</booktitle>
<contexts>
<context position="4908" citStr="Chemudugunta et al., 2007" startWordPosition="788" endWordPosition="791">performs the baseline models of LDA, Link-LDA, and Link-PLSALDA, in the tasks of topic discovery and document classification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2007</marker>
<rawString>Chaitanya Chemudugunta, Padhraic Smyth, and Mark Steyvers. 2007. Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model. In Advances in Neural Information Processing Systems 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Huan Chang</author>
</authors>
<title>Learning to Probabilistically Identify Authoritative Documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17rd international conference on Machine learning.</booktitle>
<contexts>
<context position="5833" citStr="Cohn and Chang, 2000" startWordPosition="938" endWordPosition="941">2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distribution of links as well as the distribution of words. Erosheva et al (2004) modified the model by replacing PLSI with LDA. We refer to the modified mode as Link-LDA and take it as a baseline in this paper. Note that the above two models do not directly associate the topics of the citing document with the topics of the cited documents. Nallapati and Cohn (2008) proposed an extension of Link-LDA called Link-PLSA-LDA, which is another bas</context>
</contexts>
<marker>Cohn, Chang, 2000</marker>
<rawString>David Cohn and Huan Chang. 2000. Learning to Probabilistically Identify Authoritative Documents. In Proceedings of the 17rd international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Thomas Hofmann</author>
</authors>
<title>The missing link - a probabilistic model of document content and hypertext connectivity.</title>
<date>2001</date>
<booktitle>In Neural Information Processing Systems 13.</booktitle>
<contexts>
<context position="2713" citStr="Cohn and Hofmann, 2001" startWordPosition="425" endWordPosition="428">pics. Topic modeling technologies have been applied to natural language processing, text mining, and information retrieval, and their effectiveness have been verified. In this paper, we study the problem of topic modeling for hypertexts. There is no doubt that this is an important research issue, given the fact that more and more documents are available as hypertexts currently (such as web pages). Traditional work mainly focused on development of topic models for plain texts. It is only recently several topic models for processing hypertexts were proposed, including LinkLDA and Link-PLSA-LDA (Cohn and Hofmann, 2001; Erosheva et al., 2004; Nallapati and Cohen, 2008). We point out that existing models for hypertexts may not be suitable for characterizing contents of hypertext documents. This is because all the models are assumed to generate both words and hyperlinks (outlinks) of documents. The generation of the latter type of data, however, may not be necessary for the tasks related to contents of documents. In this paper, we propose a new topic model for hypertexts called HTM (Hypertext Topic Model), within the Bayesian learning approach (it is similar to LDA in that sense). In HTM, the hyperlinks of hy</context>
<context position="5557" citStr="Cohn and Hofmann, 2001" startWordPosition="889" endWordPosition="892">thods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distribution of links as well as the distribution of words. Erosheva et al (2004) modified the model by replacing PLSI with LDA. We refer to the modified mode as Link-LD</context>
</contexts>
<marker>Cohn, Hofmann, 2001</marker>
<rawString>David Cohn and Thomas Hofmann. 2001. The missing link - a probabilistic model of document content and hypertext connectivity. In Neural Information Processing Systems 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Dietz</author>
<author>Steffen Bickel</author>
<author>Tobias Scheffer</author>
</authors>
<title>Unsupervised prediction of citation influences.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning.</booktitle>
<contexts>
<context position="5355" citStr="Dietz et al., 2007" startWordPosition="856" endWordPosition="859">ave been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful wh</context>
<context position="7019" citStr="Dietz et al (2007)" startWordPosition="1144" endWordPosition="1147">k-PLSA-LDA, which is another baseline in this paper. Assuming that the citing and cited documents share similar topics, they explicitly model the information flow from the citing documents to the cited documents. In LinkPLSA-LDA, the link graph is converted into a bipartite graph in which links are connected from citing documents to cited documents. If a document has both inlinks and outlinks, it will be duplicated on both sides of the bipartite graph. The generative process for the citing documents is similar to that of Link-LDA, while the cited documents have a different generative process. Dietz et al (2007) proposed a topic model for citation analysis. Their goal is to find topical influence of publications in research communities. They convert the citation graph (created from the publications) into a bipartite graph as in Link-PLSA-LDA. The content of a citing document is assumed to be generated by a mixture over the topic distribution of the citing document and the topic distributions of the cited documents. The differences between the topic distributions of citing and cited documents are measured, and the cited documents which have the strongest influence on the citing document are identified</context>
</contexts>
<marker>Dietz, Bickel, Scheffer, 2007</marker>
<rawString>Laura Dietz, Steffen Bickel and Tobias Scheffer. 2007. Unsupervised prediction of citation influences. In Proceedings of the 24th international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Erosheva</author>
<author>Stephen Fienberg</author>
<author>John Lafferty</author>
</authors>
<title>Mixed-membership models of scientific publications.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5220</pages>
<contexts>
<context position="2736" citStr="Erosheva et al., 2004" startWordPosition="429" endWordPosition="432">hnologies have been applied to natural language processing, text mining, and information retrieval, and their effectiveness have been verified. In this paper, we study the problem of topic modeling for hypertexts. There is no doubt that this is an important research issue, given the fact that more and more documents are available as hypertexts currently (such as web pages). Traditional work mainly focused on development of topic models for plain texts. It is only recently several topic models for processing hypertexts were proposed, including LinkLDA and Link-PLSA-LDA (Cohn and Hofmann, 2001; Erosheva et al., 2004; Nallapati and Cohen, 2008). We point out that existing models for hypertexts may not be suitable for characterizing contents of hypertext documents. This is because all the models are assumed to generate both words and hyperlinks (outlinks) of documents. The generation of the latter type of data, however, may not be necessary for the tasks related to contents of documents. In this paper, we propose a new topic model for hypertexts called HTM (Hypertext Topic Model), within the Bayesian learning approach (it is similar to LDA in that sense). In HTM, the hyperlinks of hypertext documents are s</context>
<context position="6069" citStr="Erosheva et al (2004)" startWordPosition="979" endWordPosition="982">processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distribution of links as well as the distribution of words. Erosheva et al (2004) modified the model by replacing PLSI with LDA. We refer to the modified mode as Link-LDA and take it as a baseline in this paper. Note that the above two models do not directly associate the topics of the citing document with the topics of the cited documents. Nallapati and Cohn (2008) proposed an extension of Link-LDA called Link-PLSA-LDA, which is another baseline in this paper. Assuming that the citing and cited documents share similar topics, they explicitly model the information flow from the citing documents to the cited documents. In LinkPLSA-LDA, the link graph is converted into a bip</context>
</contexts>
<marker>Erosheva, Fienberg, Lafferty, 2004</marker>
<rawString>Elena Erosheva, Stephen Fienberg, and John Lafferty. 2004. Mixed-membership models of scientific publications. In Proceedings of the National Academy of Sciences, 101:5220–5227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding Scientific Topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>.</pages>
<contexts>
<context position="5145" citStr="Griffiths and Steyvers, 2004" startWordPosition="821" endWordPosition="824">oposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the frame</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas Griffiths and Mark Steyvers. 2004. Finding Scientific Topics. In Proceedings of the National Academy of Sciences, 101 (suppl. 1) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
<author>David Blei</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Integrating Topics and Syntax.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>17</pages>
<contexts>
<context position="4855" citStr="Griffiths et al., 2005" startWordPosition="780" endWordPosition="783">l results on three web datasets show that HTM outperforms the baseline models of LDA, Link-LDA, and Link-PLSALDA, in the tasks of topic discovery and document classification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for process</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Thomas Griffiths, Mark Steyvers, David Blei, and Joshua Tenenbaum. 2005. Integrating Topics and Syntax. In Advances in Neural Information Processing Systems, 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Latent Topic Models for Hypertext.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5605" citStr="Gruber et al., 2008" startWordPosition="897" endWordPosition="901">rence (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distribution of links as well as the distribution of words. Erosheva et al (2004) modified the model by replacing PLSI with LDA. We refer to the modified mode as Link-LDA and take it as a baseline in this paper. Note </context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2008</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008. Latent Topic Models for Hypertext. In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1593" citStr="Hofmann, 1999" startWordPosition="245" endWordPosition="246"> a document (i.e., the content of the document) as a mixture over latent topics in the document itself and latent topics in the documents which the document cites. The topics are further characterized as distributions of words, as in the conventional topic models. This paper further proposes a method for learning the HTM model. Experimental results show that HTM outperforms the baselines on topic discovery and document classification in three datasets. 1 Introduction Topic models are probabilistic and generative models representing contents of documents. Examples of topic models include PLSI (Hofmann, 1999) and LDA (Blei et al., 2003). The key idea in topic modeling is to represent topics as distributions of words * This work was conducted when the first author visited Microsoft Research Asia as an intern. Bin Gao Microsoft Research Asia No.49 Zhichun Road Beijing, P. R. China bingao@microsoft.com Hang Li Microsoft Research Asia No.49 Zhichun Road Beijing, P. R. China hangli@microsoft.com and define the distribution of words in document (i.e., the content of document) as a mixture over hidden topics. Topic modeling technologies have been applied to natural language processing, text mining, and i</context>
<context position="4784" citStr="Hofmann, 1999" startWordPosition="769" endWordPosition="771">vide methods for learning and inference of HTM. Our experimental results on three web datasets show that HTM outperforms the baseline models of LDA, Link-LDA, and Link-PLSALDA, in the tasks of topic discovery and document classification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Me</context>
<context position="9508" citStr="Hofmann, 1999" startWordPosition="1572" endWordPosition="1573">ment d �d Distribution on cited documents of document d Bd Topic distribution associated with document d bdn Decision on way of generating nth word in document d cdn Cited document that generates nth word in document d zdn Topic of nth word in document d dict contents of documents, the use of HTM seems more reasonable. 3 Hypertext Topic Model 3.1 Model In topic modeling, a probability distribution of words is employed for a given document. Specifically, the probability distribution is defined as a mixture over latent topics, while each topic is future characterized by a distribution of words (Hofmann, 1999; Blei et al., 2003). In this paper, we introduce an extension of LDA model for hypertexts. Table 1 gives the major notations and their explanations. The graphic representation of conventional LDA is given in Figure 1(a). The generative process of LDA has three steps. Specifically, in each document a topic distribution is sampled from a prior distribution defined as Dirichlet distribution. Next, a topic is sampled from the topic distribution of the document, which is a multinominal distribution. Finally, a word is sampled according to the word distribution of the topic, which also forms a mult</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Analysis. In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jordan</author>
<author>Zoubin Ghahramani</author>
<author>Tommy Jaakkola</author>
<author>Lawrence Saul</author>
</authors>
<title>An Introduction to Variational Methods for Graphical Models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="5012" citStr="Jordan et al., 1999" startWordPosition="804" endWordPosition="807">lassification. The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz</context>
<context position="13670" citStr="Jordan et al., 1999" startWordPosition="2319" endWordPosition="2322">ropagated’ along the hyperlinks. For example, suppose web page A cites page B and page B cites page C, then the content of page A is influenced by that of page B, and the content of page B is further influenced by the content of page C. Therefore, HTM is able to more accurately represent the contents of hypertexts, and thus is more useful for text processing such as topic discovery and document classification. 3.2 Inference and Learning An exact inference of the posterior probability of HTM may be intractable, we employ the mean field variational inference method (Wainwright and Jordan, 2003; Jordan et al., 1999) to conduct approximation. Let I[·] be an indicator function. We first define the following factorized variational posterior distribution q with respect to the corpus: D q= Y q(θd|γd) d=1 YNdµ ¶I[Ld&gt;0] q(xdn|ρdn)(q(cdn|ψdn) q(zdn|φdn) , n=1 where γ, ψ, φ, and ρ denote free variational parameters. Parameter γ is the posterior Dirichlet parameter corresponding to the representations of documents in the topic simplex. Parameters ψ, φ, and ρ correspond to the posterior distributions of their associated random variables. We then minimize the KL divergence between q and the true posterior probabilit</context>
</contexts>
<marker>Jordan, Ghahramani, Jaakkola, Saul, 1999</marker>
<rawString>Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola, and Lawrence Saul. 1999. An Introduction to Variational Methods for Graphical Models. Machine Learning, 37(2):183–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>QiaoZhu Mei</author>
<author>Deng Cai</author>
<author>Duo Zhang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic Modeling with Network Regularization.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th international conference on World Wide Web.</booktitle>
<contexts>
<context position="5399" citStr="Mei et al., 2008" startWordPosition="863" endWordPosition="866">9), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distributio</context>
</contexts>
<marker>Mei, Cai, Zhang, Zhai, 2008</marker>
<rawString>QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiang Zhai. 2008. Topic Modeling with Network Regularization. In Proceeding of the 17th international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
<author>John Lafferty</author>
</authors>
<title>ExpectationPropagation for the Generative Aspect Model.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5094" citStr="Minka and Lafferty, 2002" startWordPosition="814" endWordPosition="817">oduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) intro</context>
</contexts>
<marker>Minka, Lafferty, 2002</marker>
<rawString>Thomas Minka and John Lafferty. 2002. ExpectationPropagation for the Generative Aspect Model. In Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
<author>William Cohen</author>
</authors>
<title>LinkPLSA-LDA: A new unsupervised model for topics and influence of blogs.</title>
<date>2008</date>
<booktitle>In International Conference for Webblogs and Social Media.</booktitle>
<contexts>
<context position="2764" citStr="Nallapati and Cohen, 2008" startWordPosition="433" endWordPosition="436">lied to natural language processing, text mining, and information retrieval, and their effectiveness have been verified. In this paper, we study the problem of topic modeling for hypertexts. There is no doubt that this is an important research issue, given the fact that more and more documents are available as hypertexts currently (such as web pages). Traditional work mainly focused on development of topic models for plain texts. It is only recently several topic models for processing hypertexts were proposed, including LinkLDA and Link-PLSA-LDA (Cohn and Hofmann, 2001; Erosheva et al., 2004; Nallapati and Cohen, 2008). We point out that existing models for hypertexts may not be suitable for characterizing contents of hypertext documents. This is because all the models are assumed to generate both words and hyperlinks (outlinks) of documents. The generation of the latter type of data, however, may not be necessary for the tasks related to contents of documents. In this paper, we propose a new topic model for hypertexts called HTM (Hypertext Topic Model), within the Bayesian learning approach (it is similar to LDA in that sense). In HTM, the hyperlinks of hypertext documents are supposed to be given. Each do</context>
<context position="5584" citStr="Nallapati and Cohen, 2008" startWordPosition="893" endWordPosition="896">d, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most relevant to our work. Cohn and Hofmann (2001) introduced a topic model for hypertexts within the framework of PLSI. The model, which is a combination of PLSI and PHITS (Cohn and Chang, 2000), gives rise to both the words and hyperlinks (outlinks) of the document in the generative process. The model is useful when the goal is to understand the distribution of links as well as the distribution of words. Erosheva et al (2004) modified the model by replacing PLSI with LDA. We refer to the modified mode as Link-LDA and take it as a baseline</context>
</contexts>
<marker>Nallapati, Cohen, 2008</marker>
<rawString>Ramesh Nallapati and William Cohen. 2008. LinkPLSA-LDA: A new unsupervised model for topics and influence of blogs. In International Conference for Webblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Wainwright</author>
<author>Michael Jordan</author>
</authors>
<title>Graphical models, exponential families, and variational inference.</title>
<date>2003</date>
<booktitle>In UC Berkeley, Dept. of Statistics, Technical Report,</booktitle>
<contexts>
<context position="5042" citStr="Wainwright and Jordan, 2003" startWordPosition="808" endWordPosition="811">st of the paper is organized as follows. Section 2 introduces related work. Section 3 describes the proposed HTM model and its learning and inference methods. Experimental results are presented in Section 4. Conclusions are made in the last section. 2 Related Work There has been much work on topic modeling. Many models have been proposed including PLSI (Hofmann, 1999), LDA (Blei et al., 2003), and their extensions (Griffiths et al., 2005; Blei and Lafferty, 2006; Chemudugunta et al., 2007). Inference and learning methods have been developed, such as variational inference (Jordan et al., 1999; Wainwright and Jordan, 2003), expectation propagation (Minka and Lafferty, 2002), and Gibbs sampling (Griffiths and Steyvers, 2004). Topic models have been utilized in topic discovery (Blei et al., 2003), document retrieval (Xing Wei and Bruce Croft, 2006), document classification (Blei et al., 2003), citation analysis (Dietz et al., 2007), social network analysis (Mei et al., 2008), and so on. Most of the existing models are for processing plain texts. There are also models for processing hypertexts, for example, (Cohn and Hofmann, 2001; Nallapati and Cohen, 2008; Gruber et al., 2008; Dietz et al., 2007), which are most</context>
<context position="13648" citStr="Wainwright and Jordan, 2003" startWordPosition="2314" endWordPosition="2318">ntents of documents can be ‘propagated’ along the hyperlinks. For example, suppose web page A cites page B and page B cites page C, then the content of page A is influenced by that of page B, and the content of page B is further influenced by the content of page C. Therefore, HTM is able to more accurately represent the contents of hypertexts, and thus is more useful for text processing such as topic discovery and document classification. 3.2 Inference and Learning An exact inference of the posterior probability of HTM may be intractable, we employ the mean field variational inference method (Wainwright and Jordan, 2003; Jordan et al., 1999) to conduct approximation. Let I[·] be an indicator function. We first define the following factorized variational posterior distribution q with respect to the corpus: D q= Y q(θd|γd) d=1 YNdµ ¶I[Ld&gt;0] q(xdn|ρdn)(q(cdn|ψdn) q(zdn|φdn) , n=1 where γ, ψ, φ, and ρ denote free variational parameters. Parameter γ is the posterior Dirichlet parameter corresponding to the representations of documents in the topic simplex. Parameters ψ, φ, and ρ correspond to the posterior distributions of their associated random variables. We then minimize the KL divergence between q and the tru</context>
</contexts>
<marker>Wainwright, Jordan, 2003</marker>
<rawString>Martin Wainwright, and Michael Jordan. 2003. Graphical models, exponential families, and variational inference. In UC Berkeley, Dept. of Statistics, Technical Report, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>Bruce Croft</author>
</authors>
<title>LDA-based document models for ad-hoc retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<marker>Wei, Croft, 2006</marker>
<rawString>Xing Wei and Bruce Croft. 2006. LDA-based document models for ad-hoc retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>