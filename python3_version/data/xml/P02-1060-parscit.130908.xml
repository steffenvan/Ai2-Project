<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.951848">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.
</note>
<title confidence="0.994697">
Named Entity Recognition using an HMM-based Chunk Tagger
</title>
<author confidence="0.990901">
GuoDong Zhou Jian Su
</author>
<affiliation confidence="0.973773">
Laboratories for Information Technology
</affiliation>
<address confidence="0.9824875">
21 Heng Mui Keng Terrace
Singapore 119613
</address>
<email confidence="0.992635">
zhougd@lit.org.sg sujian@lit.org.sg
</email>
<sectionHeader confidence="0.995515" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882869565217">
This paper proposes a Hidden Markov
Model (HMM) and an HMM-based chunk
tagger, from which a named entity (NE)
recognition (NER) system is built to
recognize and classify names, times and
numerical quantities. Through the HMM,
our system is able to apply and integrate
four types of internal and external
evidences: 1) simple deterministic internal
feature of the words, such as capitalization
and digitalization; 2) internal semantic
feature of important triggers; 3) internal
gazetteer feature; 4) external macro context
feature. In this way, the NER problem can
be resolved effectively. Evaluation of our
system on MUC-6 and MUC-7 English NE
tasks achieves F-measures of 96.6% and
94.1% respectively. It shows that the
performance is significantly better than
reported by any other machine-learning
system. Moreover, the performance is even
consistently better than those based on
handcrafted rules.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917769911504">
Named Entity (NE) Recognition (NER) is to
classify every word in a document into some
predefined categories and &amp;quot;none-of-the-above&amp;quot;. In
the taxonomy of computational linguistics tasks, it
falls under the domain of &amp;quot;information extraction&amp;quot;,
which extracts specific kinds of information from
documents as opposed to the more general task of
&amp;quot;document management&amp;quot; which seeks to extract all
of the information found in a document.
Since entity names form the main content of a
document, NER is a very important step toward
more intelligent information extraction and
management. The atomic elements of information
extraction -- indeed, of language as a whole -- could
be considered as the &amp;quot;who&amp;quot;, &amp;quot;where&amp;quot; and &amp;quot;how
much&amp;quot; in a sentence. NER performs what is known
as surface parsing, delimiting sequences of tokens
that answer these important questions. NER can
also be used as the first step in a chain of processors:
a next level of processing could relate two or more
NEs, or perhaps even give semantics to that
relationship using a verb. In this way, further
processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of
a sentence or body of text.
While NER is relatively simple and it is fairly
easy to build a system with reasonable performance,
there are still a large number of ambiguous cases
that make it difficult to attain human performance.
There has been a considerable amount of work on
NER problem, which aims to address many of these
ambiguity, robustness and portability issues. During
last decade, NER has drawn more and more
attention from the NE tasks [Chinchor95a]
[Chinchor98a] in MUCs [MUC6] [MUC7], where
person names, location names, organization names,
dates, times, percentages and money amounts are to
be delimited in text using SGML mark-ups.
Previous approaches have typically used
manually constructed finite state patterns, which
attempt to match against a sequence of words in
much the same way as a general regular expression
matcher. Typical systems are Univ. of Sheffield&apos;s
LaSIE-II [Humphreys+98], ISOQuest&apos;s NetOwl
[Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s
LTG [Mikheev+98] [Mikheev+99] for English
NER. These systems are mainly rule-based.
However, rule-based approaches lack the ability of
coping with the problems of robustness and
portability. Each new source of text requires
significant tweaking of rules to maintain optimal
performance and the maintenance costs could be
quite steep.
The current trend in NER is to use the
machine-learning approach, which is more
attractive in that it is trainable and adaptable and the
maintenance of a machine-learning system is much
cheaper than that of a rule-based one. The
representative machine-learning approaches used in
NER are HMM (BBN&apos;s IdentiFinder in [Miller+98]
[Bikel+99] and KRDL&apos;s system [Yu+98] for
Chinese NER.), Maximum Entropy (New York
Univ.&apos;s MEME in [Borthwick+98] [Borthwich99])
and Decision Tree (New York Univ.&apos;s system in
[Sekine98] and SRA&apos;s system in [Bennett+97]).
Besides, a variant of Eric Brill&apos;s
transformation-based rules [Brill95] has been
applied to the problem [Aberdeen+95]. Among
these approaches, the evaluation performance of
HMM is higher than those of others. The main
reason may be due to its better ability of capturing
the locality of phenomena, which indicates names
in text. Moreover, HMM seems more and more
used in NE recognition because of the efficiency of
the Viterbi algorithm [Viterbi67] used in decoding
the NE-class state sequence. However, the
performance of a machine-learning system is
always poorer than that of a rule-based one by about
2% [Chinchor95b] [Chinchor98b]. This may be
because current machine-learning approaches
capture important evidence behind NER problem
much less effectively than human experts who
handcraft the rules, although machine-learning
approaches always provide important statistical
information that is not available to human experts.
As defined in [McDonald96], there are two kinds
of evidences that can be used in NER to solve the
ambiguity, robustness and portability problems
described above. The first is the internal evidence
found within the word and/or word string itself
while the second is the external evidence gathered
from its context. In order to effectively apply and
integrate internal and external evidences, we
present a NER system using a HMM. The approach
behind our NER system is based on the
HMM-based chunk tagger in text chunking, which
was ranked the best individual system [Zhou+00a]
[Zhou+00b] in CoNLL&apos;2000 [Tjong+00]. Here, a
NE is regarded as a chunk, named &amp;quot;NE-Chunk&amp;quot;. To
date, our system has been successfully trained and
applied in English NER. To our knowledge, our
system outperforms any published
machine-learning systems. Moreover, our system
even outperforms any published rule-based
systems.
The layout of this paper is as follows. Section 2
gives a description of the HMM and its application
in NER: HMM-based chunk tagger. Section 3
explains the word feature used to capture both the
internal and external evidences. Section 4 describes
the back-off schemes used to tackle the sparseness
problem. Section 5 gives the experimental results of
our system. Section 6 contains our remarks and
possible extensions of the proposed work.
</bodyText>
<sectionHeader confidence="0.975516" genericHeader="method">
2 HMM-based Chunk Tagger
</sectionHeader>
<subsectionHeader confidence="0.901973">
2.1 HMM Modeling
</subsectionHeader>
<bodyText confidence="0.979831">
Given a token sequence G1n = g1g2 g , the goal
</bodyText>
<equation confidence="0.996118583333333">
n
of NER is to find a stochastic optimal tag sequence
T1n = t1t2 t that maximizes (2-1)
n
log (  |) log ( ) log
n n n
P T G = P T +
1 1 1 P T P G
( ) ( )
n n
⋅
1 1
</equation>
<bodyText confidence="0.996453">
The second item in (2-1) is the mutual
information between T1n and n
</bodyText>
<equation confidence="0.522614">
G1 . In order to
</equation>
<bodyText confidence="0.9725725">
simplify the computation of this item, we assume
mutual information independence:
</bodyText>
<equation confidence="0.973148375">
(ti
n)
1
n P
∑= log n
i 1 P t P G
( ) (
⋅
i 1
Applying it to equation (2.1), we have:
n
P
∑ log P(ti
(T1n ) −
(2-4)
1
</equation>
<bodyText confidence="0.999927375">
The basic premise of this model is to consider
the raw text, encountered when decoding, as though
it had passed through a noisy channel, where it had
been originally marked with NE tags. The job of our
generative model is to directly generate the original
NE tags from the output words of the noisy channel.
It is obvious that our generative model is reverse to
the generative model of traditional HMM1, as used
</bodyText>
<footnote confidence="0.92707025">
1 In traditional HMM to maximise log (1  |1 )
P T n G n , first we
apply Bayes&apos; rule:
and have:
</footnote>
<equation confidence="0.991829693877551">
+
∑
P(T1n  |G1n)
=
PT G
( ,
n
1
n )
1
n
MI
MI (t
,
)
r (2-2)
(T n G n
1 , 1)=∑
i
1
o
i=1
log
P(G1n)
P (Tn , Gn)
1 1
P T G
( , )
n n
1 1
=P T P G
( ) ( )
n n
⋅
1 1
) (2-3)
log P(T1n  |Gi ) = log
n
i
=
1
 |)
G n
1
log (
P t i
)
i
=
</equation>
<bodyText confidence="0.997211769230769">
in BBN&apos;s IdentiFinder, which models the original
process that generates the NE-class annotated
words from the original NE tags. Another
difference is that our model assumes mutual
information independence (2-2) while traditional
HMM assumes conditional probability
independence (I-1). Assumption (2-2) is much
looser than assumption (I-1) because assumption
(I-1) has the same effect with the sum of
assumptions (2-2) and (I-3)2. In this way, our model
can apply more context information to determine
the tag of current token.
From equation (2-4), we can see that:
</bodyText>
<listItem confidence="0.97494">
1) The first item can be computed by applying
chain rules. In ngram modeling, each tag is
assumed to be probabilistically dependent on the
N-1 previous tags.
2) The second item is the summation of log
probabilities of all the individual tags.
3) The third item corresponds to the &amp;quot;lexical&amp;quot;
component of the tagger.
</listItem>
<bodyText confidence="0.93452">
We will not discuss both the first and second
items further in this paper. This paper will focus on
</bodyText>
<equation confidence="0.953466">
log (  |1 ) , which is the main
P ti G n
</equation>
<bodyText confidence="0.999961625">
difference between our tagger and other traditional
HMM-based taggers, as used in BBN&apos;s IdentiFinder.
Ideally, it can be estimated by using the
forward-backward algorithm [Rabiner89]
recursively for the 1st-order [Rabiner89] or 2nd
-order HMMs [Watson+92]. However, an
alternative back-off modeling approach is applied
instead in this paper (more details in section 4).
</bodyText>
<subsectionHeader confidence="0.997712">
2.2 HMM-based Chunk Tagger
</subsectionHeader>
<bodyText confidence="0.869607">
arg max log (  |)
</bodyText>
<equation confidence="0.998928666666667">
P T G
1 1
n n
T
P G T
(  |) log ( ))
n n + P T n
1 1 1
T
</equation>
<bodyText confidence="0.712123">
Then we assume conditional probability
</bodyText>
<equation confidence="0.950395052631579">
n
independence: P Gn T n
( 1  |1 ) = ∏ P(gi|ti)
and have:
P T G
(  |)
n n
1 1
(I-2)
log P(gi  |ti) + log P
(T1n))
T i
=1
2 We can obtain equation (I-2) from (2.4) by assuming
log P(ti  |Gn) =log P(gi  |ti) (I-3)
1
For NE-chunk tagging, we have
token gi =&lt; fi , wi &gt; , where W1n = w1w2 ... w is the
n
</equation>
<bodyText confidence="0.953122666666667">
word sequence and F1n = f 1 f2 ... fn is the
word-feature sequence. In the meantime, NE-chunk
tag ti is structural and consists of three parts:
</bodyText>
<listItem confidence="0.982331">
1) Boundary Category: BC = {0, 1, 2, 3}. Here 0
means that current word is a whole entity and
1/2/3 means that current word is at the
beginning/in the middle/at the end of an entity.
2) Entity Category: EC. This is used to denote the
class of the entity name.
3) Word Feature: WF. Because of the limited
number of boundary and entity categories, the
word feature is added into the structural tag to
represent more accurate models.
</listItem>
<bodyText confidence="0.890054">
Obviously, there exist some constraints between
ti −1 and ti on the boundary and entity categories, as
shown in Table 1, where &amp;quot;valid&amp;quot; / &amp;quot;invalid&amp;quot; means
the tag sequence ti−1ti is valid / invalid while &amp;quot;valid
on&amp;quot; means ti−1ti is valid with an additional
condition ECi −1 = ECi . Such constraints have been
used in Viterbi decoding algorithm to ensure valid
NE chunking.
</bodyText>
<table confidence="0.9930214">
0 1 2 3
0 Valid Valid Invalid Invalid
1 Invalid Invalid Valid on Valid on
2 Invalid Invalid Valid Valid
3 Valid Valid Invalid Invalid
</table>
<tableCaption confidence="0.915823">
Table 1: Constraints between ti −1 and ti (Column:
BCi −1 in ti −1 ; Row: BCi in ti )
</tableCaption>
<sectionHeader confidence="0.991606" genericHeader="method">
3 Determining Word Feature
</sectionHeader>
<bodyText confidence="0.999978461538461">
As stated above, token is denoted as ordered pairs of
word-feature and word itself: gi =&lt; fi , wi &gt; .
Here, the word-feature is a simple deterministic
computation performed on the word and/or word
string with appropriate consideration of context as
looked up in the lexicon or added to the context.
In our model, each word-feature consists of
several sub-features, which can be classified into
internal sub-features and external sub-features. The
internal sub-features are found within the word
and/or word string itself to capture internal
evidence while external sub-features are derived
within the context to capture external evidence.
</bodyText>
<equation confidence="0.781688307692308">
n
the third item∑=
i 1
=
max(log
arg
1
i=
arg
max log
T
n
= arg max (∑
</equation>
<subsectionHeader confidence="0.890838">
3.1 Internal Sub-Features
</subsectionHeader>
<bodyText confidence="0.974576">
Our model captures three types of internal
sub-features: 1) 1
f : simple deterministic internal
feature of the words, such as capitalization and
digitalization; 2) f 2: internal semantic feature of
important triggers; 3) f 3: internal gazetteer feature.
</bodyText>
<equation confidence="0.628496">
1) 1
</equation>
<bodyText confidence="0.985436027027027">
f is the basic sub-feature exploited in this
model, as shown in Table 2 with the descending
order of priority. For example, in the case of
non-disjoint feature classes such as
ContainsDigitAndAlpha and
ContainsDigitAndDash, the former will take
precedence. The first eleven features arise from
the need to distinguish and annotate monetary
amounts, percentages, times and dates. The rest
of the features distinguish types of capitalization
and all other words such as punctuation marks.
In particular, the FirstWord feature arises from
the fact that if a word is capitalized and is the
first word of the sentence, we have no good
information as to why it is capitalized (but note
that AllCaps and CapPeriod are computed before
FirstWord, and take precedence.) This
sub-feature is language dependent. Fortunately,
the feature computation is an extremely small
part of the implementation. This kind of internal
sub-feature has been widely used in
machine-learning systems, such as BBN&apos;s
IdendiFinder and New York Univ.&apos;s MENE. The
rationale behind this sub-feature is clear: a)
capitalization gives good evidence of NEs in
Roman languages; b) Numeric symbols can
automatically be grouped into categories.
2) 2
f is the semantic classification of important
triggers, as seen in Table 3, and is unique to our
system. It is based on the intuitions that
important triggers are useful for NER and can be
classified according to their semantics. This
sub-feature applies to both single word and
multiple words. This set of triggers is collected
semi-automatically from the NEs and their local
context of the training data.
</bodyText>
<listItem confidence="0.7333546">
3) Sub-feature f3 , as shown in Table 4, is the
internal gazetteer feature, gathered from the
look-up gazetteers: lists of names of persons,
organizations, locations and other kinds of
named entities. This sub-feature can be
</listItem>
<bodyText confidence="0.999868538461539">
determined by finding a match in the
gazetteer of the corresponding NE type
where n (in Table 4) represents the word
number in the matched word string. In stead
of collecting gazetteer lists from training
data, we collect a list of 20 public holidays in
several countries, a list of 5,000 locations
from websites such as GeoHive3, a list of
10,000 organization names from websites
such as Yahoo4 and a list of 10,000 famous
people from websites such as Scope
Systems5. Gazetters have been widely used
in NER systems to improve performance.
</bodyText>
<subsectionHeader confidence="0.980203">
3.2 External Sub-Features
</subsectionHeader>
<bodyText confidence="0.999891310344827">
For external evidence, only one external macro
context feature 4
f , as shown in Table 5, is captured
in our model. 4
f is about whether and how the
encountered NE candidate is occurred in the list of
NEs already recognized from the document, as
shown in Table 5 (n is the word number in the
matched NE from the recognized NE list and m is
the matched word number between the word string
and the matched NE with the corresponding NE
type.). This sub-feature is unique to our system. The
intuition behind this is the phenomena of name
alias.
During decoding, the NEs already recognized
from the document are stored in a list. When the
system encounters a NE candidate, a name alias
algorithm is invoked to dynamically determine its
relationship with the NEs in the recognized list.
Initially, we also consider part-of-speech (POS)
sub-feature. However, the experimental result is
disappointing that incorporation of POS even
decreases the performance by 2%. This may be
because capitalization information of a word is
submerged in the muddy of several POS tags and
the performance of POS tagging is not satisfactory,
especially for unknown capitalized words (since
many of NEs include unknown capitalized words.).
Therefore, POS is discarded.
</bodyText>
<footnote confidence="0.998922">
3 http://www.geohive.com/
4 http://www.yahoo.com/
5 http://www.scopesys.com/
</footnote>
<table confidence="0.999765333333333">
Sub-Feature 1 Example Explanation/Intuition
f
OneDigitNum 9 Digital Number
TwoDigitNum 90 Two-Digit year
FourDigitNum 1990 Four-Digit year
YearDecade 1990s Year Decade
ContainsDigitAndAlpha A8956-67 Product Code
ContainsDigitAndDash 09-99 Date
ContainsDigitAndOneSlash 3/4 Fraction or Date
ContainsDigitAndTwoSlashs 19/9/1999 DATE
ContainsDigitAndComma 19,000 Money
ContainsDigitAndPeriod 1.00 Money, Percentage
OtherContainsDigit 123124 Other Number
AllCaps IBM Organization
CapPeriod M. Person Name Initial
CapOtherPeriod St. Abbreviation
CapPeriods N.Y. Abbreviation
FirstWord First word of sentence No useful capitalization information
InitialCap Microsoft Capitalized Word
LowerCase Will Un-capitalized Word
Other $ All other words
</table>
<tableCaption confidence="0.9254265">
Table 2: Sub-Feature 1
f : the Simple Deterministic Internal Feature of the Words
</tableCaption>
<table confidence="0.999905333333333">
NE Type (No of Triggers) Sub-Feature 2 Example Explanation/Intuition
f
PERCENT (5) SuffixPERCENT % Percentage Suffix
MONEY (298) PrefixMONEY $ Money Prefix
SuffixMONEY Dollars Money Suffix
DATE (52) SuffixDATE Day Date Suffix
WeekDATE Monday Week Date
MonthDATE July Month Date
SeasonDATE Summer Season Date
PeriodDATE1 Month Period Date
PeriodDATE2 Quarter Quarter/Half of Year
EndDATE Weekend Date End
ModifierDATE Fiscal Modifier of Date
TIME (15) SuffixTIME a.m. Time Suffix
PeriodTime Morning Time Period
PERSON (179) PrefixPERSON1 Mr. Person Title
PrefixPERSON2 President Person Designation
FirstNamePERSON Micheal Person First Name
LOC (36) SuffixLOC River Location Suffix
ORG (177) SuffixORG Ltd Organization Suffix
Others (148) Cardinal, Ordinal, etc. Six,, Sixth Cardinal and Ordinal Numbers
</table>
<tableCaption confidence="0.997009">
Table 3: Sub-Feature f2 : the Semantic Classification of Important Triggers
</tableCaption>
<table confidence="0.9990555">
NE Type (Size of Gazetteer) Sub-Feature 3 Example
f
DATE (20) DATEnGn Christmas Day: DATE2G2
PERSON (10,000) PERSONnGn Bill Gates: PERSON2G2
LOC (5,000) LOCnGn Beijing: LOC1G1
ORG (10,000) ORGnGn United Nation: ORG2G2
</table>
<tableCaption confidence="0.99622">
Table 4: Sub-Feature f 3: the Internal Gazetteer Feature (G means Global gazetteer)
</tableCaption>
<table confidence="0.95098075">
NE Type Sub-Feature Example
PERSON PERSONnLm Gates: PERSON2L1 (&amp;quot;Bill Gates&amp;quot; already recognized as a person name)
LOC LOCnLm N.J.: LOC2L2 (&amp;quot;New Jersey&amp;quot; already recognized as a location name)
ORG ORGnLm UN: ORG2L2 (&amp;quot;United Nation&amp;quot; already recognized as a org name)
</table>
<tableCaption confidence="0.998991">
Table 5: Sub-feature f 4: the External Macro Context Feature (L means Local document)
</tableCaption>
<sectionHeader confidence="0.984936" genericHeader="method">
4 Back-off Modeling
</sectionHeader>
<bodyText confidence="0.996129">
Given the model in section 2 and word feature in
section 3, the main problem is how to
</bodyText>
<equation confidence="0.921703">
P ti G
( / 1 ) . Ideally, we would have
n
</equation>
<bodyText confidence="0.9999615">
sufficient training data for every event whose
conditional probability we wish to calculate.
Unfortunately, there is rarely enough training data
to compute accurate probabilities when decoding on
new data, especially considering the complex word
feature described above. In order to resolve the
sparseness problem, two levels of back-off
modeling are applied to approximate ( / 1 )
</bodyText>
<equation confidence="0.9743305">
P ti G :
n
</equation>
<bodyText confidence="0.957184666666667">
1) First level back-off scheme is based on different
contexts of word features and words themselves,
and n
</bodyText>
<equation confidence="0.873752666666667">
G1 in ( / 1 )
P ti G is approximated in the
n
</equation>
<bodyText confidence="0.699164428571429">
descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 ,
fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1
,
fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1
and fi .
2) The second level back-off scheme is based on
different combinations of the four sub-features
described in section 3, and fk is approximated
in the descending order of 12 3 4
f k f k f k fk , 1 3
fk fk ,
fk fk , 1 2
1 4fkfk and 1
fk .
</bodyText>
<sectionHeader confidence="0.991462" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.995737652173913">
In this section, we will report the experimental
results of our system for English NER on MUC-6
and MUC-7 NE shared tasks, as shown in Table 6,
and then for the impact of training data size on
performance using MUC-7 training data. For each
experiment, we have the MUC dry-run data as the
held-out development data and the MUC formal test
data as the held-out test data.
For both MUC-6 and MUC-7 NE tasks, Table 7
shows the performance of our system using MUC
evaluation while Figure 1 gives the comparisons of
our system with others. Here, the precision (P)
measures the number of correct NEs in the answer
file over the total number of NEs in the answer file
and the recall (R) measures the number of correct
NEs in the answer file over the total number of NEs
in the key file while F-measure is the weighted
harmonic mean of precision and recall:
with β2 =1. It shows that the
performance is significantly better than reported by
any other machine-learning system. Moreover, the
performance is consistently better than those based
on handcrafted rules.
</bodyText>
<table confidence="0.99401925">
Statistics Training Dry Run Formal Test
(KB) Data Data Data
MUC-6 1330 121 124
MUC-7 708 156 561
</table>
<tableCaption confidence="0.637105">
Table 6: Statistics of Data from MUC-6
and MUC-7 NE Tasks
</tableCaption>
<table confidence="0.999927">
F P R
MUC-6 96.6 96.3 96.9
MUC-7 94.1 93.7 94.5
</table>
<tableCaption confidence="0.994441">
Table 7: Performance of our System on MUC-6
</tableCaption>
<table confidence="0.606636428571429">
and MUC-7 NE Tasks
Composition F P R
f = f 1 77.6 81.0 74.1
f = f1 f2 87.4 88.6 86.1
f = f1 f2 f3 89.3 90.5 88.2
f = f1 f2 f4 92.9 92.6 93.1
f = f1 f2 f3 f4 94.1 93.7 94.5
</table>
<tableCaption confidence="0.953265">
Table 8: Impact of Different Sub-Features
</tableCaption>
<bodyText confidence="0.999796909090909">
With any learning technique, one important
question is how much training data is required to
achieve acceptable performance. More generally
how does the performance vary as the training data
size changes? The result is shown in Figure 2 for
MUC-7 NE task. It shows that 200KB of training
data would have given the performance of 90%
while reducing to 100KB would have had a
significant decrease in the performance. It also
shows that our system still has some room for
performance improvement. This may be because of
</bodyText>
<equation confidence="0.802748555555556">
n
compute ∑
i=1
( 2 1)
β + RP
β 2 R P
+
=
F
</equation>
<bodyText confidence="0.421578">
the complex word feature and the corresponding sparseness problem existing in our system.
</bodyText>
<table confidence="0.773728714285714">
Precision 100 Our MUC-6 System
95 Our MUC-7 System
90 Other MUC-6 Systems
85 Other MUC-7 Syetems
80
80 85 90 95 100
Recall
</table>
<figureCaption confidence="0.993148">
Figure 1: Comparison of our system with others
on MUC-6 and MUC-7 NE tasks
</figureCaption>
<figure confidence="0.9408925">
100 200 300 400 500 600 700 800
Training Data Size(KB)
</figure>
<figureCaption confidence="0.989369">
Figure 2: Impact of Various Training Data on Performance
</figureCaption>
<figure confidence="0.994518">
F-measure
100
95
90
85
80
MUC-7
</figure>
<bodyText confidence="0.89479224137931">
Another important question is about the effect of
different sub-features. Table 8 answers the question
on MUC-7 NE task:
1) Applying only 1
f gives our system the
performance of 77.6%.
2) 2
f is very useful for NER and increases the
performance further by 10% to 87.4%.
3) 4
f is impressive too with another 5.5%
performance improvement.
4) However, 3
f contributes only further 1.2% to
the performance. This may be because
information included in 3
f has already been
captured by 2
f and f4 . Actually, the
experiments show that the contribution of 3
f
comes from where there is no explicit indicator
information in/around the NE and there is no
reference to other NEs in the macro context of
the document. The NEs contributed by 3
f are
always well-known ones, e.g. Microsoft, IBM
and Bach (a composer), which are introduced in
texts without much helpful context.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982428571429">
This paper proposes a HMM in that a new
generative model, based on the mutual information
independence assumption (2-3) instead of the
conditional probability independence assumption
(I-1) after Bayes&apos; rule, is applied. Moreover, it
shows that the HMM-based chunk tagger can
effectively apply and integrate four different kinds
of sub-features, ranging from internal word
information to semantic information to NE
gazetteers to macro context of the document, to
capture internal and external evidences for NER
problem. It also shows that our NER system can
reach &amp;quot;near human performance&amp;quot;. To our
knowledge, our NER system outperforms any
published machine-learning system and any
published rule-based system.
While the experimental results have been
impressive, there is still much that can be done
potentially to improve the performance. In the near
feature, we would like to incorporate the following
into our system:
</bodyText>
<listItem confidence="0.998338">
• List of domain and application dependent person,
organization and location names.
• More effective name alias algorithm.
• More effective strategy to the back-off modeling
and smoothing.
</listItem>
<sectionHeader confidence="0.997534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805669811321">
[Aberdeen+95] J. Aberdeen, D. Day, L.
Hirschman, P. Robinson and M. Vilain. MITRE:
Description of the Alembic System Used for
MUC-6. MUC-6. Pages141-155. Columbia,
Maryland. 1995.
[Aone+98] C. Aone, L. Halverson, T. Hampton,
M. Ramos-Santacruz. SRA: Description of the IE2
System Used for MUC-7. MUC-7. Fairfax, Virginia.
1998.
[Bennett+96] S.W. Bennett, C. Aone and C.
Lovell. Learning to Tag Multilingual Texts
Through Observation. EMNLP&apos;1996. Pages109-116.
Providence, Rhode Island. 1996.
[Bikel+99] Daniel M. Bikel, Richard Schwartz
and Ralph M. Weischedel. An Algorithm that
Learns What&apos;s in a Name. Machine Learning
(Special Issue on NLP). 1999.
[Borthwick+98] A. Borthwick, J. Sterling, E.
Agichtein, R. Grishman. NYU: Description of the
MENE Named Entity System as Used in MUC-7.
MUC-7. Fairfax, Virginia. 1998.
[Borthwick99] Andrew Borthwick. A Maximum
Entropy Approach to Named Entity Recognition.
Ph.D. Thesis. New York University. September,
1999.
[Brill95] Eric Brill. Transform-based
Error-Driven Learning and Natural Language
Processing: A Case Study in Part-of-speech
Tagging. Computational Linguistics 21(4).
Pages543-565. 1995.
[Chinchor95a] Nancy Chinchor. MUC-6 Named
Entity Task Definition (Version 2.1). MUC-6.
Columbia, Maryland. 1995.
[Chinchor95b] Nancy Chinchor. Statistical
Significance of MUC-6 Results. MUC-6. Columbia,
Maryland. 1995.
[Chinchor98a] Nancy Chinchor. MUC-7 Named
Entity Task Definition (Version 3.5). MUC-7.
Fairfax, Virginia. 1998.
[Chinchor98b] Nancy Chinchor. Statistical
Significance of MUC-7 Results. MUC-7. Fairfax,
Virginia. 1998.
[Humphreys+98] K. Humphreys, R. Gaizauskas,
S. Azzam, C. Huyck, B. Mitchell, H. Cunningham,
Y. Wilks. Univ. of Sheffield: Description of the
LaSIE-II System as Used for MUC-7. MUC-7.
Fairfax, Virginia. 1998.
[Krupka+98] G. R. Krupka, K. Hausman.
IsoQuest Inc.: Description of the NetOwlTM
Extractor System as Used for MUC-7. MUC-7.
Fairfax, Virginia. 1998.
[McDonald96] D. McDonald. Internal and
External Evidence in the Identification and
Semantic Categorization of Proper Names. In B.
Boguraev and J. Pustejovsky editors: Corpus
Processing for Lexical Acquisition. Pages21-39.
MIT Press. Cambridge, MA. 1996.
[Miller+98] S. Miller, M. Crystal, H. Fox, L.
Ramshaw, R. Schwartz, R. Stone, R. Weischedel,
and the Annotation Group. BBN: Description of the
SIFT System as Used for MUC-7. MUC-7. Fairfax,
Virginia. 1998.
[Mikheev+98] A. Mikheev, C. Grover, M.
Moens. Description of the LTG System Used for
MUC-7. MUC-7. Fairfax, Virginia. 1998.
[Mikheev+99] A. Mikheev, M. Moens, and C.
Grover. Named entity recognition without gazeteers.
EACL&apos;1999. Pages1-8. Bergen, Norway. 1999.
[MUC6] Morgan Kaufmann Publishers, Inc.
Proceedings of the Sixth Message Understanding
Conference (MUC-6). Columbia, Maryland. 1995.
[MUC7] Morgan Kaufmann Publishers, Inc.
Proceedings of the Seventh Message Understanding
Conference (MUC-7). Fairfax, Virginia. 1998.
[Rabiner89] L. Rabiner. A Tutorial on Hidden
Markov Models and Selected Applications in
Speech Recognition”. IEEE 77(2). Pages257-285.
1989.
[Sekine98] Satoshi Sekine. Description of the
Japanese NE System Used for MET-2. MUC-7.
Fairfax, Virginia. 1998.
[Tjong+00] Erik F. Tjong Kim Sang and Sabine
Buchholz. Introduction to the CoNLL-2000 Shared
Task: Chunking. CoNLL&apos;2000. Pages127-132.
Lisbon, Portugal. 11-14 Sept 2000.
[Viterbi67] A. J. Viterbi. Error Bounds for
Convolutional Codes and an Asymptotically
Optimum Decoding Algorithm. IEEE Transactions
on Information Theory. IT(13). Pages260-269,
April 1967.
[Watson+92] B. Watson and Tsoi A Chunk.
Second Order Hidden Markov Models for Speech
Recognition”. Proceeding of 4th Australian
International Conference on Speech Science and
Technology. Pages146-151. 1992.
[Yu+98] Yu Shihong, Bai Shuanhu and Wu
Paul. Description of the Kent Ridge Digital Labs
System Used for MUC-7. MUC-7. Fairfax, Virginia.
1998.
[Zhou+00] Zhou GuoDong, Su Jian and Tey
TongGuan. Hybrid Text Chunking. CoNLL&apos;2000.
Pages163-166. Lisbon, Portugal, 11-14 Sept 2000.
[Zhou+00b] Zhou GuoDong and Su Jian,
Error-driven HMM-based Chunk Tagger with
Context-dependent Lexicon. EMNLP/ VLC&apos;2000.
Hong Kong, 7-8 Oct 2000.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.653624">
<note confidence="0.997654">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.</note>
<title confidence="0.971658">Named Entity Recognition using an HMM-based Chunk Tagger</title>
<author confidence="0.99888">GuoDong Zhou Jian Su</author>
<affiliation confidence="0.999899">Laboratories for Information Technology</affiliation>
<address confidence="0.93902">21 Heng Mui Keng Terrace Singapore 119613</address>
<email confidence="0.7467">zhougd@lit.org.sgsujian@lit.org.sg</email>
<abstract confidence="0.999180833333333">This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<date>1995</date>
<booktitle>MITRE: Description of the Alembic System Used for MUC-6. MUC-6. Pages141-155.</booktitle>
<location>Columbia, Maryland.</location>
<marker>[Aberdeen+95]</marker>
<rawString>J. Aberdeen, D. Day, L. Hirschman, P. Robinson and M. Vilain. MITRE: Description of the Alembic System Used for MUC-6. MUC-6. Pages141-155. Columbia, Maryland. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>L Halverson</author>
<author>T Hampton</author>
<author>M Ramos-Santacruz</author>
</authors>
<date>1998</date>
<booktitle>SRA: Description of the IE2 System Used for MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Aone+98]</marker>
<rawString>C. Aone, L. Halverson, T. Hampton, M. Ramos-Santacruz. SRA: Description of the IE2 System Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S W Bennett</author>
<author>C Aone</author>
<author>C Lovell</author>
</authors>
<title>Learning to Tag Multilingual Texts Through Observation.</title>
<date>1996</date>
<booktitle>EMNLP&apos;1996. Pages109-116.</booktitle>
<location>Providence, Rhode Island.</location>
<marker>[Bennett+96]</marker>
<rawString>S.W. Bennett, C. Aone and C. Lovell. Learning to Tag Multilingual Texts Through Observation. EMNLP&apos;1996. Pages109-116. Providence, Rhode Island. 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An Algorithm that Learns What&apos;s in a Name. Machine Learning (Special Issue on NLP).</title>
<date>1999</date>
<marker>[Bikel+99]</marker>
<rawString>Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel. An Algorithm that Learns What&apos;s in a Name. Machine Learning (Special Issue on NLP). 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<date>1998</date>
<booktitle>NYU: Description of the MENE Named Entity System as Used in MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Borthwick+98]</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, R. Grishman. NYU: Description of the MENE Named Entity System as Used in MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<location>Ph.D. Thesis. New York University.</location>
<marker>[Borthwick99]</marker>
<rawString>Andrew Borthwick. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. Thesis. New York University. September, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transform-based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<issue>4</issue>
<pages>543--565</pages>
<contexts>
<context position="4291" citStr="[Brill95]" startWordPosition="650" endWordPosition="650">nt trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are HMM (BBN&apos;s IdentiFinder in [Miller+98] [Bikel+99] and KRDL&apos;s system [Yu+98] for Chinese NER.), Maximum Entropy (New York Univ.&apos;s MEME in [Borthwick+98] [Borthwich99]) and Decision Tree (New York Univ.&apos;s system in [Sekine98] and SRA&apos;s system in [Bennett+97]). Besides, a variant of Eric Brill&apos;s transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95]. Among these approaches, the evaluation performance of HMM is higher than those of others. The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text. Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b]. This may be because current machin</context>
</contexts>
<marker>[Brill95]</marker>
<rawString>Eric Brill. Transform-based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-speech Tagging. Computational Linguistics 21(4). Pages543-565. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>1995</date>
<booktitle>MUC-6 Named Entity Task Definition (Version 2.1). MUC-6.</booktitle>
<location>Columbia, Maryland.</location>
<contexts>
<context position="2824" citStr="[Chinchor95a]" startWordPosition="438" endWordPosition="438"> or more NEs, or perhaps even give semantics to that relationship using a verb. In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text. While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance. There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues. During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups. Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher. Typical systems are Univ. of Sheffield&apos;s LaSIE-II [Humphreys+98], ISOQuest&apos;s NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s LTG [Mikheev+98] [Mikheev+99] for English NER. These systems are mainly rule-based. H</context>
</contexts>
<marker>[Chinchor95a]</marker>
<rawString>Nancy Chinchor. MUC-6 Named Entity Task Definition (Version 2.1). MUC-6. Columbia, Maryland. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>1995</date>
<booktitle>Statistical Significance of MUC-6 Results. MUC-6.</booktitle>
<location>Columbia, Maryland.</location>
<contexts>
<context position="4841" citStr="[Chinchor95b]" startWordPosition="738" endWordPosition="738"> a variant of Eric Brill&apos;s transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95]. Among these approaches, the evaluation performance of HMM is higher than those of others. The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text. Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b]. This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts. As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above. The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gat</context>
</contexts>
<marker>[Chinchor95b]</marker>
<rawString>Nancy Chinchor. Statistical Significance of MUC-6 Results. MUC-6. Columbia, Maryland. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>1998</date>
<booktitle>MUC-7 Named Entity Task Definition (Version 3.5). MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="2838" citStr="[Chinchor98a]" startWordPosition="439" endWordPosition="439">or perhaps even give semantics to that relationship using a verb. In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text. While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance. There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues. During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups. Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher. Typical systems are Univ. of Sheffield&apos;s LaSIE-II [Humphreys+98], ISOQuest&apos;s NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s LTG [Mikheev+98] [Mikheev+99] for English NER. These systems are mainly rule-based. However, rule-b</context>
</contexts>
<marker>[Chinchor98a]</marker>
<rawString>Nancy Chinchor. MUC-7 Named Entity Task Definition (Version 3.5). MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<date>1998</date>
<booktitle>Statistical Significance of MUC-7 Results. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="4855" citStr="[Chinchor98b]" startWordPosition="739" endWordPosition="739">Eric Brill&apos;s transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95]. Among these approaches, the evaluation performance of HMM is higher than those of others. The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text. Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b]. This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts. As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above. The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its</context>
</contexts>
<marker>[Chinchor98b]</marker>
<rawString>Nancy Chinchor. Statistical Significance of MUC-7 Results. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Humphreys</author>
<author>R Gaizauskas</author>
<author>S Azzam</author>
<author>C Huyck</author>
<author>B Mitchell</author>
<author>H Cunningham</author>
<author>Y Wilks</author>
</authors>
<date>1998</date>
<booktitle>Univ. of Sheffield: Description of the LaSIE-II System as Used for MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Humphreys+98]</marker>
<rawString>K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. Cunningham, Y. Wilks. Univ. of Sheffield: Description of the LaSIE-II System as Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Krupka</author>
<author>K Hausman</author>
</authors>
<title>IsoQuest Inc.: Description of the NetOwlTM Extractor System as Used for MUC-7. MUC-7.</title>
<date>1998</date>
<location>Fairfax, Virginia.</location>
<marker>[Krupka+98]</marker>
<rawString>G. R. Krupka, K. Hausman. IsoQuest Inc.: Description of the NetOwlTM Extractor System as Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Internal and External Evidence in the Identification and Semantic Categorization of Proper Names.</title>
<date>1996</date>
<booktitle>Corpus Processing for Lexical Acquisition. Pages21-39.</booktitle>
<editor>In B. Boguraev and J. Pustejovsky editors:</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5177" citStr="[McDonald96]" startWordPosition="781" endWordPosition="781">ems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b]. This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts. As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above. The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context. In order to effectively apply and integrate internal and external evidences, we present a NER system using a HMM. The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL&apos;2000 [Tjong+00]. Here, a NE is </context>
</contexts>
<marker>[McDonald96]</marker>
<rawString>D. McDonald. Internal and External Evidence in the Identification and Semantic Categorization of Proper Names. In B. Boguraev and J. Pustejovsky editors: Corpus Processing for Lexical Acquisition. Pages21-39. MIT Press. Cambridge, MA. 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>M Crystal</author>
<author>H Fox</author>
<author>L Ramshaw</author>
<author>R Schwartz</author>
<author>R Stone</author>
<author>R Weischedel</author>
</authors>
<date>1998</date>
<booktitle>and the Annotation Group. BBN: Description of the SIFT System as Used for MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Miller+98]</marker>
<rawString>S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwartz, R. Stone, R. Weischedel, and the Annotation Group. BBN: Description of the SIFT System as Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>C Grover</author>
<author>M Moens</author>
</authors>
<date>1998</date>
<booktitle>Description of the LTG System Used for MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Mikheev+98]</marker>
<rawString>A. Mikheev, C. Grover, M. Moens. Description of the LTG System Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
<author>M Moens</author>
<author>C Grover</author>
</authors>
<title>Named entity recognition without gazeteers.</title>
<date>1999</date>
<booktitle>EACL&apos;1999. Pages1-8.</booktitle>
<location>Bergen,</location>
<marker>[Mikheev+99]</marker>
<rawString>A. Mikheev, M. Moens, and C. Grover. Named entity recognition without gazeteers. EACL&apos;1999. Pages1-8. Bergen, Norway. 1999.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Inc. Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Columbia, Maryland.</location>
<contexts>
<context position="2853" citStr="[MUC6]" startWordPosition="442" endWordPosition="442">emantics to that relationship using a verb. In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text. While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance. There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues. During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups. Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher. Typical systems are Univ. of Sheffield&apos;s LaSIE-II [Humphreys+98], ISOQuest&apos;s NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s LTG [Mikheev+98] [Mikheev+99] for English NER. These systems are mainly rule-based. However, rule-based approaches</context>
</contexts>
<marker>[MUC6]</marker>
<rawString>Morgan Kaufmann Publishers, Inc. Proceedings of the Sixth Message Understanding Conference (MUC-6). Columbia, Maryland. 1995.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Inc. Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="2860" citStr="[MUC7]" startWordPosition="443" endWordPosition="443">s to that relationship using a verb. In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text. While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance. There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues. During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups. Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher. Typical systems are Univ. of Sheffield&apos;s LaSIE-II [Humphreys+98], ISOQuest&apos;s NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh&apos;s LTG [Mikheev+98] [Mikheev+99] for English NER. These systems are mainly rule-based. However, rule-based approaches lack t</context>
</contexts>
<marker>[MUC7]</marker>
<rawString>Morgan Kaufmann Publishers, Inc. Proceedings of the Seventh Message Understanding Conference (MUC-7). Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”.</title>
<date>1989</date>
<journal>IEEE</journal>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="8961" citStr="[Rabiner89]" startWordPosition="1505" endWordPosition="1505">omputed by applying chain rules. In ngram modeling, each tag is assumed to be probabilistically dependent on the N-1 previous tags. 2) The second item is the summation of log probabilities of all the individual tags. 3) The third item corresponds to the &amp;quot;lexical&amp;quot; component of the tagger. We will not discuss both the first and second items further in this paper. This paper will focus on log ( |1 ) , which is the main P ti G n difference between our tagger and other traditional HMM-based taggers, as used in BBN&apos;s IdentiFinder. Ideally, it can be estimated by using the forward-backward algorithm [Rabiner89] recursively for the 1st-order [Rabiner89] or 2nd -order HMMs [Watson+92]. However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). 2.2 HMM-based Chunk Tagger arg max log ( |) P T G 1 1 n n T P G T ( |) log ( )) n n + P T n 1 1 1 T Then we assume conditional probability n independence: P Gn T n ( 1 |1 ) = ∏ P(gi|ti) and have: P T G ( |) n n 1 1 (I-2) log P(gi |ti) + log P (T1n)) T i =1 2 We can obtain equation (I-2) from (2.4) by assuming log P(ti |Gn) =log P(gi |ti) (I-3) 1 For NE-chunk tagging, we have token gi =&lt; fi , wi &gt; , where W1n </context>
</contexts>
<marker>[Rabiner89]</marker>
<rawString>L. Rabiner. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition”. IEEE 77(2). Pages257-285. 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<date>1998</date>
<booktitle>Description of the Japanese NE System Used for MET-2. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<contexts>
<context position="4184" citStr="[Sekine98]" startWordPosition="636" endWordPosition="636"> tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep. The current trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are HMM (BBN&apos;s IdentiFinder in [Miller+98] [Bikel+99] and KRDL&apos;s system [Yu+98] for Chinese NER.), Maximum Entropy (New York Univ.&apos;s MEME in [Borthwick+98] [Borthwich99]) and Decision Tree (New York Univ.&apos;s system in [Sekine98] and SRA&apos;s system in [Bennett+97]). Besides, a variant of Eric Brill&apos;s transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95]. Among these approaches, the evaluation performance of HMM is higher than those of others. The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text. Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poore</context>
</contexts>
<marker>[Sekine98]</marker>
<rawString>Satoshi Sekine. Description of the Japanese NE System Used for MET-2. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Sabine Buchholz. Introduction to the</title>
<date></date>
<booktitle>CoNLL-2000 Shared Task: Chunking. CoNLL&apos;2000. Pages127-132.</booktitle>
<pages>11--14</pages>
<location>Lisbon,</location>
<marker>[Tjong+00]</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the CoNLL-2000 Shared Task: Chunking. CoNLL&apos;2000. Pages127-132. Lisbon, Portugal. 11-14 Sept 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory.</journal>
<volume>13</volume>
<pages>260--269</pages>
<contexts>
<context position="4668" citStr="[Viterbi67]" startWordPosition="711" endWordPosition="711">aximum Entropy (New York Univ.&apos;s MEME in [Borthwick+98] [Borthwich99]) and Decision Tree (New York Univ.&apos;s system in [Sekine98] and SRA&apos;s system in [Bennett+97]). Besides, a variant of Eric Brill&apos;s transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95]. Among these approaches, the evaluation performance of HMM is higher than those of others. The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text. Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence. However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b]. This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts. As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustne</context>
</contexts>
<marker>[Viterbi67]</marker>
<rawString>A. J. Viterbi. Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. IEEE Transactions on Information Theory. IT(13). Pages260-269, April 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Watson</author>
</authors>
<title>and Tsoi A Chunk. Second Order Hidden Markov Models for Speech Recognition”.</title>
<date>1992</date>
<booktitle>Proceeding of 4th Australian International Conference on Speech Science and Technology.</booktitle>
<pages>146--151</pages>
<marker>[Watson+92]</marker>
<rawString>B. Watson and Tsoi A Chunk. Second Order Hidden Markov Models for Speech Recognition”. Proceeding of 4th Australian International Conference on Speech Science and Technology. Pages146-151. 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Shihong</author>
<author>Bai Shuanhu</author>
<author>Wu Paul</author>
</authors>
<date>1998</date>
<booktitle>Description of the Kent Ridge Digital Labs System Used for MUC-7. MUC-7.</booktitle>
<location>Fairfax, Virginia.</location>
<marker>[Yu+98]</marker>
<rawString>Yu Shihong, Bai Shuanhu and Wu Paul. Description of the Kent Ridge Digital Labs System Used for MUC-7. MUC-7. Fairfax, Virginia. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
</authors>
<title>Su Jian and Tey TongGuan. Hybrid Text Chunking.</title>
<date>2000</date>
<booktitle>CoNLL&apos;2000. Pages163-166.</booktitle>
<location>Lisbon,</location>
<marker>[Zhou+00]</marker>
<rawString>Zhou GuoDong, Su Jian and Tey TongGuan. Hybrid Text Chunking. CoNLL&apos;2000. Pages163-166. Lisbon, Portugal, 11-14 Sept 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
</authors>
<title>Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon.</title>
<date>2000</date>
<booktitle>EMNLP/ VLC&apos;2000. Hong Kong,</booktitle>
<pages>7--8</pages>
<marker>[Zhou+00b]</marker>
<rawString>Zhou GuoDong and Su Jian, Error-driven HMM-based Chunk Tagger with Context-dependent Lexicon. EMNLP/ VLC&apos;2000. Hong Kong, 7-8 Oct 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>