<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.999777">
A Generalized Vector Space Model for Text Retrieval
Based on Semantic Relatedness
</title>
<author confidence="0.989498">
George Tsatsaronis and Vicky Panagiotopoulou
</author>
<affiliation confidence="0.9987995">
Department of Informatics
Athens University of Economics and Business,
</affiliation>
<address confidence="0.591137">
76, Patision Str., Athens, Greece
</address>
<email confidence="0.998451">
gbt@aueb.gr, vpanagiotopoulou@gmail.com
</email>
<sectionHeader confidence="0.993301" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996869217391304">
Generalized Vector Space Models
(GVSM) extend the standard Vector
Space Model (VSM) by embedding addi-
tional types of information, besides terms,
in the representation of documents. An
interesting type of information that can
be used in such models is semantic infor-
mation from word thesauri like WordNet.
Previous attempts to construct GVSM
reported contradicting results. The most
challenging problem is to incorporate the
semantic information in a theoretically
sound and rigorous manner and to modify
the standard interpretation of the VSM.
In this paper we present a new GVSM
model that exploits WordNet’s semantic
information. The model is based on a new
measure of semantic relatedness between
terms. Experimental study conducted
in three TREC collections reveals that
semantic information can boost text
retrieval performance with the use of the
proposed GVSM.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915304347826">
The use of semantic information into text retrieval
or text classification has been controversial. For
example in Mavroeidis et al. (2005) it was shown
that a GVSM using WordNet (Fellbaum, 1998)
senses and their hypernyms, improves text clas-
sification performance, especially for small train-
ing sets. In contrast, Sanderson (1994) reported
that even 90% accurate WSD cannot guarantee
retrieval improvement, though their experimental
methodology was based only on randomly gen-
erated pseudowords of varying sizes. Similarly,
Voorhees (1993) reported a drop in retrieval per-
formance when the retrieval model was based on
WSD information. On the contrary, the construc-
tion of a sense-based retrieval model by Stokoe
et al. (2003) improved performance, while sev-
eral years before, Krovetz and Croft (1992) had
already pointed out that resolving word senses can
improve searches requiring high levels of recall.
In this work, we argue that the incorporation
of semantic information into a GVSM retrieval
model can improve performance by considering
the semantic relatedness between the query and
document terms. The proposed model extends
the traditional VSM with term to term relatedness
measured with the use of WordNet. The success of
the method lies in three important factors, which
also constitute the points of our contribution: 1) a
new measure for computing semantic relatedness
between terms which takes into account relation
weights, and senses’ depth; 2) a new GVSM re-
trieval model, which incorporates the aforemen-
tioned semantic relatedness measure; 3) exploita-
tion of all the semantic information a thesaurus
can offer, including semantic relations crossing
parts of speech (POS). Experimental evaluation
in three TREC collections shows that the pro-
posed model can improve in certain cases the
performance of the standard TF-IDF VSM. The
rest of the paper is organized as follows: Section
2 presents preliminary concepts, regarding VSM
and GVSM. Section 3 presents the term seman-
tic relatedness measure and the proposed GVSM.
Section 4 analyzes the experimental results, and
Section 5 concludes and gives pointers to future
work.
</bodyText>
<sectionHeader confidence="0.996033" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.925013">
2.1 Vector Space Model
</subsectionHeader>
<bodyText confidence="0.9998725">
The VSM has been a standard model of represent-
ing documents in information retrieval for almost
three decades (Salton and McGill, 1983; Baeza-
Yates and Ribeiro-Neto, 1999). Let D be a docu-
ment collection and Q the set of queries represent-
ing users’ information needs. Let also tz symbol-
</bodyText>
<note confidence="0.9598005">
Proceedings of the EACL 2009 Student Research Workshop, pages 70–78,
Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997525">
70
</page>
<bodyText confidence="0.988932565217391">
ize term i used to index the documents in the col-
lection, with i = 1,.., n. The VSM assumes that
for each term ti there exists a vector ~ti in the vector
space that represents it. It then considers the set of
all term vectors {~ti} to be the generating set of the
vector space, thus the space basis. If each dk,(for
k = 1, .., p) denotes a document of the collection,
then there exists a linear combination of the term
vectors {~ti} which represents each dk in the vector
space. Similarly, any query q can be modelled as
a vector q~ that is a linear combination of the term
vectors.
In the standard VSM, the term vectors are con-
sidered pairwise orthogonal, meaning that they are
linearly independent. But this assumption is un-
realistic, since it enforces lack of relatedness be-
tween any pair of terms, whereas the terms in a
language often relate to each other. Provided that
the orthogonality assumption holds, the similarity
~
between a document vector dk and a query vec-
tor q~ in the VSM can be expressed by the cosine
measure given in equation 1.
</bodyText>
<equation confidence="0.9879215">
cos(
dk, ~q) = 2 n2qPn
Pn j=1 akjqj (
i=1 aki �j=1 qj
</equation>
<bodyText confidence="0.9999796">
where akj, qj are real numbers standing for the
weights of term j in the document dk and the
query q respectively. A standard baseline retrieval
strategy is to rank the documents according to their
cosine similarity to the query.
</bodyText>
<subsectionHeader confidence="0.98707">
2.2 Generalized Vector Space Model
</subsectionHeader>
<bodyText confidence="0.9527554375">
Wong et al. (1987) presented an analysis of the
problems that the pairwise orthogonality assump-
tion of the VSM creates. They were the first to
address these problems by expanding the VSM.
They introduced term to term correlations, which
deprecated the pairwise orthogonality assumption,
but they kept the assumption that the term vectors
are linearly independent1, creating the first GVSM
model. More specifically, they considered a new
space, where each term vector ~ti was expressed as
a linear combination of 2n vectors ~mr, r = 1..2n.
The similarity measure between a document and a
query then became as shown in equation 2, where
~ti and ~tj are now term vectors in a 2n dimensional
~
vector space, dk, q~ are the document and the query
</bodyText>
<footnote confidence="0.996839333333333">
1It is known from Linear Algebra that if every pair of vec-
tors in a set of vectors is orthogonal, then this set of vectors
is linearly independent, but not the inverse.
</footnote>
<bodyText confidence="0.955689">
vectors, respectively, as before, ´aki, ´qj are the new
weights, and n´ the new space dimensions.
</bodyText>
<equation confidence="0.979386">
P´n P´n i=1 ´aki ´qj~ti~tj
j=1
cos(~dk, ~q) = (2)
qP´ni=1 aki2 P´nj =1 ´qj2
</equation>
<bodyText confidence="0.99946">
From equation 2 it follows that the term vectors
~ti and ~tj need not be known, as long as the cor-
relations between terms ti and tj are known. If
one assumes pairwise orthogonality, the similarity
measure is reduced to that of equation 1.
</bodyText>
<subsectionHeader confidence="0.999667">
2.3 Semantic Information and GVSM
</subsectionHeader>
<bodyText confidence="0.999723205128205">
Since the introduction of the first GVSM model,
there are at least two basic directions for em-
bedding term to term relatedness, other than ex-
act keyword matching, into a retrieval model:
(a) compute semantic correlations between terms,
or (b) compute frequency co-occurrence statistics
from large corpora. In this paper we focus on the
first direction. In the past, the effect of WSD infor-
mation in text retrieval was studied (Krovetz and
Croft, 1992; Sanderson, 1994), with the results re-
vealing that under circumstances, senses informa-
tion may improve IR. More specifically, Krovetz
and Croft (1992) performed a series of three exper-
iments in two document collections, CACM and
TIMES. The results of their experiments showed
that word senses provide a clear distinction be-
tween relevant and nonrelevant documents, reject-
ing the null hypothesis that the meaning of a word
is not related to judgments of relevance. Also, they
reached the conclusion that words being worth
of disambiguation are either the words with uni-
form distribution of senses, or the words that in
the query have a different sense from the most
popular one. Sanderson (1994) studied the in-
fluence of disambiguation in IR with the use of
pseudowords and he concluded that sense ambi-
guity is problematic for IR only in the cases of
retrieving from short queries. Furthermore, his
findings regarding the WSD used were that such
a WSD system would help IR if it could perform
with very high accuracy, although his experiments
were conducted in the Reuters collection, where
standard queries with corresponding relevant doc-
uments (qrels) are not provided.
Since then, several recent approaches have
incorporated semantic information in VSM.
Mavroeidis et al. (2005) created a GVSM ker-
nel based on the use of noun senses, and their
hypernyms from WordNet. They experimentally
</bodyText>
<page confidence="0.997627">
71
</page>
<bodyText confidence="0.9999808">
showed that this can improve text categorization.
Stokoe et al. (Stokoe et al., 2003) reported an im-
provement in retrieval performance using a fully
sense-based system. Our approach differs from
the aforementioned ones in that it expands the
VSM model using the semantic information of a
word thesaurus to interpret the orthogonality of
terms and to measure semantic relatedness, in-
stead of directly replacing terms with senses, or
adding senses to the model.
</bodyText>
<sectionHeader confidence="0.992737" genericHeader="method">
3 A GVSM Model based on Semantic
</sectionHeader>
<subsectionHeader confidence="0.805112">
Relatedness of Terms
</subsectionHeader>
<bodyText confidence="0.999990545454545">
Synonymy (many words per sense) and polysemy
(many senses per word) are two fundamental prob-
lems in text retrieval. Synonymy is related with
recall, while polysemy with precision. One stan-
dard method to tackle synonymy is the expansion
of the query terms with their synonyms. This in-
creases recall, but it can reduce precision dramat-
ically. Both polysemy and synonymy can be cap-
tured on the GVSM model in the computation of
the inner product between ~ti and ~tj in equation 2,
as will be explained below.
</bodyText>
<subsectionHeader confidence="0.99909">
3.1 Semantic Relatedness
</subsectionHeader>
<bodyText confidence="0.9936373">
In our model, we measure semantic relatedness us-
ing WordNet. It considers the path length, cap-
tured by compactness (SCM), and the path depth,
captured by semantic path elaboration (SPE),
which are defined in the following. The two mea-
sures are combined to for semantic relatedness
(SR) beetween two terms. SR, presented in defini-
tion 3, is the basic module of the proposed GVSM
model. The adopted method of building seman-
tic networks and measuring semantic relatedness
from a word thesaurus is explained in the next sub-
section.
Definition 1 Given a word thesaurus O, a weight-
ing scheme for the edges that assigns a weight e ∈
(0, 1) for each edge, a pair ofsenses S = (s1, s2),
and a path of length l connecting the two senses,
the semantic compactness of S (SCM(S, O)) is
defined as Hli=1 ei, where e1, e2,..., el are the
path’s edges. Ifs1 = s2 SCM(S, O) = 1. Ifthere
is no path between s1 and s2 SCM(S, O) = 0.
Note that compactness considers the path length
and has values in the set [0, 1]. Higher com-
pactness between senses declares higher seman-
tic relatedness and larger weight are assigned to
stronger edge types. The intuition behind the as-
sumption of edges’ weighting is the fact that some
edges provide stronger semantic connections than
others. In the next subsection we propose a can-
didate method of computing weights. The com-
pactness of two senses s1 and s2, can take differ-
ent values for all the different paths that connect
the two senses. All these paths are examined, as
explained later, and the path with the maximum
weight is eventually selected (definition 3). An-
other parameter that affects term relatedness is the
depth of the sense nodes comprising the path. A
standard means of measuring depth in a word the-
saurus is the hypernym/hyponym hierarchical re-
lation for the noun and adjective POS and hyper-
nym/troponym for the verb POS. A path with shal-
low sense nodes is more general compared to a
path with deep nodes. This parameter of seman-
tic relatedness between terms is captured by the
measure of semantic path elaboration introduced
in the following definition.
Definition 2 Given a word thesaurus O and a
pair of senses S = (s1, s2), where s1,s2 ∈ O
and s1 =6 s2, and a path between the two senses
of length l, the semantic path elaboration of the
path (SPE (S, O is defined as7�7l 2didi+1 1
</bodyText>
<equation confidence="0.786315">
p ((� )) ne HI
di+di+1 dmax,
</equation>
<bodyText confidence="0.98673772">
where di is the depth of sense si according to O,
and dmax the maximum depth of O. If s1 = s2,
and d = d1 = d2, SPE(S, O) = d d x. If there is
ma
no path from s1 to s2, SPE(S, O) = 0.
Essentially, SPE is the harmonic mean of the
two depths normalized to the maximum thesaurus
depth. The harmonic mean offers a lower upper
bound than the average of depths and we think
is a more realistic estimation of the path’s depth.
SCM and SPE capture the two most important
parameters of measuring semantic relatedness be-
tween terms (Budanitsky and Hirst, 2006), namely
path length and senses depth in the used thesaurus.
We combine these two measures naturally towards
defining the Semantic Relatedness between two
terms.
Definition 3 Given a word thesaurus O, a pair of
terms T = (t1, t2), and all pairs of senses S =
(s1i, s2j), where s1i, s2j senses of t1,t2 respec-
tively. The semantic relatedness of T (SR(T,S,O))
is defined as max{SCM(S, O)·SPE(S, O)}. SR
between two terms ti, tj where ti ≡ tj ≡ t and
t ∈/ O is defined as 1. If ti ∈ O but tj ∈/ O, or
ti ∈/ O but tj ∈ O, SR is defined as 0.
</bodyText>
<page confidence="0.99609">
72
</page>
<figureCaption confidence="0.99985">
Figure 1: Computation of semantic relatedness.
</figureCaption>
<subsectionHeader confidence="0.999337">
3.2 Semantic Networks from Word Thesauri
</subsectionHeader>
<bodyText confidence="0.993415634920635">
In order to construct a semantic network for a pair
of terms t1 and t2 and a combination of their re-
spective senses, i.e., s1 and s2, we adopted the
network construction method that we introduced
in (Tsatsaronis et al., 2007). This method was pre-
ferred against other related methods, like the one
introduced in (Mihalcea et al., 2004), since it em-
beds all the available semantic information exist-
ing in WordNet, even edges that cross POS, thus
offering a richer semantic representation. Accord-
ing to the adopted semantic network construction
model, each semantic edge type is given a different
weight. The intuition behind edge types’ weight-
ing is that certain types provide stronger semantic
connections than others. The frequency of occur-
rence of the different edge types in Wordnet 2.0, is
used to define the edge types’ weights (e.g. 0.57
for hypernym/hyponym edges, 0.14 for nominal-
ization edges etc.).
Figure 1 shows the construction of a semantic
network for two terms ti and tj. Let the high-
lighted senses S.i.2 and S.j.1 be a pair of senses
of ti and tj respectively. All the semantic links
of the highlighted senses, as found in WordNet,
are added as shown in example 1 of figure 1. The
process is repeated recursively until at least one
path between S.i.2 and S.j.1 is found. It might be
the case that there is no path from S.i.2 to S.j.1.
In that case SR((ti, tj), (S.i.2, S.j.1), O) = 0.
Suppose that a path is that of example 2, where
e1, e2, e3 are the respective edge weights, d1 is the
depth of S.i.2, d2 the depth of S.i.2.1, d3 the depth
of S.i.2.2 and d4 the depth of S.j.1, and dmax the
maximum thesaurus depth. For reasons of sim-
plicity, let e1 = e2 = e3 = 0.5, and d1 = 3.
Naturally, d2 = 4, and let d3 = d4 = d2 = 4. Fi-
nally, let dmax = 14, which is the case for Word-
Net 2.0. Then, SR((ti, tj), (S.i.2, S.j.1), O) =
0.53 · 0.4615 · 0.52 = 0.01442. Example 3 of
figure 2 illustrates another possibility where S.i.7
and S.j.5 is another examined pair of senses for ti
and tj respectively. In this case, the two senses co-
incide, and SR((ti, tj), (S.i.7, S.j.5), O) = 1· d14,
where d the depth of the sense. When two senses
coincide, SCM = 1, as mentioned in definition 1,
a secondary criterion must be levied to distinguish
the relatedness of senses that match. This crite-
rion in SR is SPE, which assumes that a sense
is more specific as we traverse WordNet graph
downwards. In the specified example, SCM = 1,
but SPE = d14. This will give a final value to SR
that will be less than 1. This constitutes an intrin-
sic property of SR, which is expressed by SPE.
The rationale behind the computation of SPE
stems from the fact that word senses in WordNet
are organized into synonym sets, named synsets.
Moreover, synsets belong to hierarchies (i.e., noun
hierarchies developed by the hypernym/hyponym
relations). Thus, in case two words map into the
same synset (i.e., their senses belong to the same
synset), the computation of their semantic related-
ness must additionally take into account the depth
of that synset in WordNet.
</bodyText>
<subsectionHeader confidence="0.995304">
3.3 Computing Maximum Semantic
Relatedness
</subsectionHeader>
<bodyText confidence="0.997913692307692">
In the expansion of the VSM model we need to
weigh the inner product between any two term
vectors with their semantic relatedness. It is obvi-
ous that given a word thesaurus, there can be more
than one semantic paths that link two senses. In
these cases, we decide to use the path that max-
imizes the semantic relatedness (the product of
SCM and SPE). This computation can be done
according to the following algorithm, which is a
modification of Dijkstra’s algorithm for finding
the shortest path between two nodes in a weighted
directed graph. The proof of the algorithm’s cor-
rectness follows with theorem 1.
</bodyText>
<construct confidence="0.9508256">
Theorem 1 Given a word thesaurus O, a weight-
ingfunction w : E → (0, 1), where a higher value
declares a stronger edge, and a pair of senses
S(ss, sf) declaring source (ss) and destination
(sf) vertices, then the SCM(S, O) · SPE(S, O)
</construct>
<footnote confidence="0.725841">
is maximized for the path returned by Algorithm
1, by using the weighting scheme eij = wij ·
2·di·dj where eij the new weight of the edge
dmax · (di+dj ),
connecting senses si and sj, and wij the initial
</footnote>
<figure confidence="0.989894604651163">
Initial Phaseo
Network Expansion Example 1o
Holonymo
Meronymo
S.i.2o Hypernymo S.j.1o
...�
Antonymo
Hyponymo
S.i.1o
S.j.1o
Synonymo
...G
S.i.2o S.j.2o
toio tojo
...� ...�
S.i.7o
S.j.5o
...❑
Network Expansion Example 3o
Index:o = Word Nodeo = Sense Nodeo = Semantic Linko
..❑ Hypernymo
Synonymo
Meronymo
es,
Hyponymo
eft
Hyponymo
S.i.1o
S.j.1o
...❑
ej
Domaino
S.i.2.1o
S.i.2o S.j.2o
toio
...❑
S.i.7o
...1-1
tojo
S.j.5o
S.i.2o S.j.1o
S.i.2.2o
Network Expansion Example 2o
</figure>
<page confidence="0.647132">
73
</page>
<bodyText confidence="0.602004142857143">
Algorithm 1 MaxSR(G,u,v,w)
Require: A directed weighted graph G, two
nodes u, v and a weighting scheme w : E →
(0..1).
Ensure: The path from u to v with the maximum
product of the edges weights.
Initialize-Single-Source(G,u)
</bodyText>
<listItem confidence="0.971745304347826">
1: for all vertices v ∈ V [G] do
2: d[v] = −∞
3: 7r[v] = NULL
4: end for
5: d[u] = 1
Relax(u, v, w)
6: if d[v] &lt; d[u] · w(u, v) then
7: d[v] = d[u] · w(u, v)
8: 7r[v] = u
9: end if
Maximum-Relatedness(G,u,v,w)
10: Initialize-Single-Source(G,u)
11: S = ∅
12: Q = V [G]
13: while v ∈ Q do
14: s = Extract from Q the vertex with max d
15: S = S ∪ s
16: for all vertices k ∈ Adjacency List of s do
17: Relax(s,k,w)
18: end for
19: end while
20: return the path following all the ancestors 7r of
v back to u
</listItem>
<bodyText confidence="0.995835555555556">
weight assigned by weighting function w.
Proof 1 For the proof of this theorem we follow
the course of thinking of the proof of theorem
25.10 in (Cormen et al., 1990). We shall show
that for each vertex sf ∈ V, d[sf] is the max-
imum product of edges’ weight through the se-
lected path, starting from ss, at the time when
sf is inserted into S. From now on, the nota-
tion S(ss, sf) will represent this product. Path
p connects a vertex in S, namely ss, to a ver-
tex in V − S, namely sf. Consider the first ver-
tex sy along p such that sy ∈ V − S and let sx
be y’s predecessor. Now, path p can be decom-
posed as ss → sx → sy → sf. We claim that
d[sy] = S(ss, sy) when sf is inserted into S. Ob-
serve that sx ∈ S. Then, because sf is chosen as
the first vertex for which d[sf] =6 S(ss, sf) when it
is inserted into S, we had d[sx] = S(ss, sx) when
sx was inserted into S.
We can now obtain a contradiction to the
above to prove the theorem. Because sy oc-
curs before sf on the path from ss to sf and all
edge weights are nonnegative2 and in (0, 1) we
have S(ss, sy) ≥ S(ss, sf), and thus d[sy] =
S(ss, sy) ≥ S(ss, sf) ≥ d[sf]. But both sy
and sf were in V − S when sf was chosen,
so we have d[sf] ≥ d[sy]. Thus, d[sy] =
S(ss, sy) = S(ss, sf) = d[sf]. Consequently,
d[sf] = S(ss, sf) which contradicts our choice of
sf. We conclude that at the time each vertex sf is
inserted into S, d[sf] = S(ss, sf).
Next, to prove that the returned maximum
product is the SCM(S, O) · SPE(S, O), let
the path between ss and sf with the maximum
edge weight product have k edges. Then, Al-
gorithm 1 returns the maximum rlki=1 ei(i+1) =
</bodyText>
<equation confidence="0.995057333333333">
2·ds·d22·d2·d3
ws2 · dmax·(ds+d2) w23 · dmax·(d2+d3) · ...· wkf
2·dk·df7�7 k k 2didi+1
dmax·(dk+df) = 11i=1wi(i+1) �i=1 di+di+1
dmax = SCM(S, O) · SPE(S, O).
1
</equation>
<subsectionHeader confidence="0.965178">
3.4 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.9999816">
The reader will have noticed that our model com-
putes the SR between two terms ti,tj, based on the
pair of senses si,sj of the two terms respectively,
which maximizes the product SCM · SPE. Al-
ternatively, a WSD algorithm could have disam-
biguated the two terms, given the text fragments
where the two terms occurred. Though interesting,
this prospect is neither addressed, nor examined in
this work. Still, it is in our next plans and part of
our future work to embed in our model some of
the interesting WSD approaches, like knowledge-
based (Sinha and Mihalcea, 2007; Brody et al.,
2006), corpus-based (Mihalcea and Csomai, 2005;
McCarthy et al., 2004), or combinations with very
high accuracy (Montoyo et al., 2005).
</bodyText>
<subsectionHeader confidence="0.716083">
3.5 The GVSM Model
</subsectionHeader>
<bodyText confidence="0.9989698">
In equation 2, which captures the document-query
similarity in the GVSM model, the orthogonality
between terms ti and tj is expressed by the inner
product of the respective term vectors tits. Recall
that
</bodyText>
<equation confidence="0.87661825">
t� are in reality unknown. We estimate
ti an
d
maximizing SCM
SPE.
tit; =
O) (3)
in
</equation>
<bodyText confidence="0.69429">
model we assume that
</bodyText>
<subsectionHeader confidence="0.427235">
term can
</subsectionHeader>
<bodyText confidence="0.847638333333333">
be semantically related with any other term, an
·
SR((ti,tj),(si,sj),
Since
our
each
d
their inner product by equation 3, where si and
sj are the senses of terms ti and tj respectively,
</bodyText>
<footnote confidence="0.9316355">
2The sign of the algorithm isnot considere
d at this step.
</footnote>
<page confidence="0.984387">
74
</page>
<equation confidence="0.8262305">
5R((ti, tj), O) = 5R((tj, ti), O), the new space
is of n·(n−1)
</equation>
<bodyText confidence="0.972769888888889">
2 dimensions. In this space, each di-
mension stands for a distinct pair of terms. Given
�
a document vector dk in the VSM TF-IDF space,
we define the value in the (i, j) dimension of
the new document vector space as dk(ti, tj) =
(TF − IDF(ti, dk) + TF − IDF(tj, dk)) · �ti�tj.
We add the TF-IDF values because any product-
based value results to zero, unless both terms are
present in the document. The dimensions q(ti, tj)
of the query, are computed similarly. A GVSM
model aims at being able to retrieve documents
that not necessarily contain exact matches of the
query terms, and this is its great advantage. This
new space leads to a new GVSM model, which is
a natural extension of the standard VSM. The co-
sine similarity between a document dk and a query
q now becomes:
</bodyText>
<equation confidence="0.998164">
cos(�dk, �q) = Pi= 1Pnj=idk(ti,tj)&apos;q(ti,tj)
qPi= dk (ti, tj )2 &apos;qPi= q(ti, tj )2
(4)
</equation>
<bodyText confidence="0.9992535">
where n is the dimension of the VSM TF-IDF
space.
</bodyText>
<sectionHeader confidence="0.998314" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999693090909091">
The experimental evaluation in this work is two-
fold. First, we test the performance of the seman-
tic relatedness measure (SR) for a pair of words
in three benchmark data sets, namely the Ruben-
stein and Goodenough 65 word pairs (Ruben-
stein and Goodenough, 1965)(R&amp;G), the Miller
and Charles 30 word pairs (Miller and Charles,
1991)(M&amp;C), and the 353 similarity data set
(Finkelstein et al., 2002). Second, we evaluate
the performance of the proposed GVSM in three
TREC collections (TREC 1, 4 and 6).
</bodyText>
<subsectionHeader confidence="0.9608295">
4.1 Evaluation of the Semantic Relatedness
Measure
</subsectionHeader>
<bodyText confidence="0.9994260625">
For the evaluation of the proposed semantic re-
latedness measure between two terms we experi-
mented in three widely used data sets in which hu-
man subjects have provided scores of relatedness
for each pair. A kind of ”gold standard” ranking
of related word pairs (i.e., from the most related
words to the most irrelevant) has thus been cre-
ated, against which computer programs can test
their ability on measuring semantic relatedness be-
tween words. We compared our measure against
ten known measures of semantic relatedness: (HS)
Hirst and St-Onge (1998), (JC) Jiang and Conrath
(1997), (LC) Leacock et al. (1998), (L) Lin (1998),
(R) Resnik (1995), (JS) Jarmasz and Szpakowicz
(2003), (GM) Gabrilovich and Markovitch (2007),
(F) Finkelstein et al. (2002), (HR) ) and (SP)
Strube and Ponzetto (2006). In Table 1 the results
of SR and the ten compared measures are shown.
The reported numbers are the Spearman correla-
tion of the measures’ rankings with the gold stan-
dard (human judgements).
The correlations for the three data sets show that
SR performs better than any other measure of se-
mantic relatedness, besides the case of (HR) in the
M&amp;C data set. It surpasses HR though in the R&amp;G
and the 353-C data set. The latter contains the
word pairs of the M&amp;C data set. To visualize the
performance of our measure in a more comprehen-
sible manner, Figure 2 presents for all pairs in the
R&amp;G data set, and with increasing order of relat-
edness values based on human judgements, the re-
spective values of these pairs that SR produces. A
closer look on Figure 2 reveals that the values pro-
duced by SR (right figure) follow a pattern similar
to that of the human ratings (left figure). Note that
the x-axis in both charts begins from the least re-
lated pair of terms, according to humans, and goes
up to the most related pair of terms. The y-axis
in the left chart is the respective humans’ rating
for each pair of terms. The right figure shows SR
for each pair. The reader can consult Budanitsky
and Hirst (2006) to confirm that all the other mea-
sures of semantic relatedness we compare to, do
not follow the same pattern as the human ratings,
as closely as our measure of relatedness does (low
y values for small x values and high y values for
high x). The same pattern applies in the M&amp;C and
353-C data sets.
</bodyText>
<subsectionHeader confidence="0.999195">
4.2 Evaluation of the GVSM
</subsectionHeader>
<bodyText confidence="0.982639714285714">
For the evaluation of the proposed GVSM model,
we have experimented with three TREC collec-
tions 3, namely TREC 1 (TIPSTER disks 1 and
2), TREC 4 (TIPSTER disks 2 and 3) and TREC
6 (TIPSTER disks 4 and 5). We selected those
TREC collections in order to cover as many dif-
ferent thematic subjects as possible. For example,
TREC 1 contains documents from the Wall Street
Journal, Associated Press, Federal Register, and
abstracts of U.S. department of energy. TREC 6
differs from TREC 1, since it has documents from
Financial Times, Los Angeles Times and the For-
eign Broadcast Information Service.
For each TREC, we executed the standard base-
</bodyText>
<footnote confidence="0.96755">
3http://trec.nist.gov/
</footnote>
<page confidence="0.995274">
75
</page>
<table confidence="0.99813975">
HS JC LC L R JS GM F HR SP SR
R&amp;G 0.745 0.709 0.785 0.77 0.748 0.842 0.816 N/A 0.817 0.56 0.861
M&amp;C 0.653 0.805 0.748 0.767 0.737 0.832 0.723 N/A 0.904 0.49 0.855
353-C N/A N/A 0.34 N/A 0.35 0.55 0.75 0.56 0.552 0.48 0.61
</table>
<tableCaption confidence="0.999873">
Table 1: Correlations of semantic relatedness measures with human judgements.
</tableCaption>
<figure confidence="0.987937296296297">
HUMAN RATINGS AGAINST HUMAN RANKINGS SEMANTIC RELATEDNESS AGAINST HUMAN RANKINGS
Human Rating
3.5
2.5
0.5
1.5
4
3
2
0
1
correlation of human pairs ranking and human ratings
Semantic Relatedness
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
correlation of human pairs ranking and semantic relatedness
10 20 30 40 50 60 65
Pair Number
10 20 30 40 50 60 65
Pair Number
</figure>
<figureCaption confidence="0.999292">
Figure 2: Correlation between human ratings and SR in the R&amp;G data set.
</figureCaption>
<bodyText confidence="0.999954304347826">
line TF-IDF VSM model for the first 20 topics
of each collection. Limited resources prohibited
us from executing experiments in the top 1000
documents. To minimize the execution time, we
have indexed all the pairwise semantic related-
ness values according to the SR measure, in a
database, whose size reached 300GB. Thus, the
execution of the SR itself is really fast, as all pair-
wise SR values between WordNet synsets are in-
dexed. For TREC 1, we used topics 51 − 70, for
TREC 4 topics 201 − 220 and for TREC 6 topics
301 − 320. From the results of the VSM model,
we kept the top-50 retrieved documents. In order
to evaluate whether the proposed GVSM can aid
the VSM performance, we executed the GVSM
in the same retrieved documents. The interpo-
lated precision-recall values in the 11-standard re-
call points for these executions are shown in fig-
ure 3 (left graphs), for both VSM and GVSM. In
the right graphs of figure 3, the differences in in-
terpolated precision for the same recall levels are
depicted. For reasons of simplicity, we have ex-
cluded the recall values in the right graphs, above
which, both systems had zero precision. Thus, for
TREC 1 in the y-axis we have depicted the differ-
ence in the interpolated precision values (%) of the
GVSM from the VSM, for the first 4 recall points.
For TRECs 4 and 6 we have done the same for the
first 9 and 8 recall points respectively.
As shown in figure 3, the proposed GVSM may
improve the performance of the TFIDF VSM up to
1.93% in TREC 4, 0.99% in TREC 6 and 0.42%
in TREC 1. This small boost in performance
proves that the proposed GVSM model is promis-
ing. There are many aspects though in the GVSM
that we think require further investigation, like for
example the fact that we have not conducted WSD
so as to map each document and query term oc-
currence into its correct sense, or the fact that the
weighting scheme of the edges used in SR gen-
erates from the distribution of each edge type in
WordNet, while there might be other more sophis-
ticated ways to compute edge weights. We believe
that if these, but also more aspects discussed in
the next section, are tackled, the proposed GVSM
may improve more the retrieval performance.
</bodyText>
<sectionHeader confidence="0.99851" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.9999661875">
From the experimental evaluation we infer that
SR performs very well, and in fact better than all
the tested related measures. With regards to the
GVSM model, experimental evaluation in three
TREC collections has shown that the model is
promising and may boost retrieval performance
more if several details are further investigated and
further enhancements are made. Primarily, the
computation of the maximum semantic related-
ness between two terms includes the selection of
the semantic path between two senses that maxi-
mizes SR. This can be partially unrealistic since
we are not sure whether these senses are the cor-
rect senses of the terms. To tackle this issue,
WSD techniques may be used. In addition, the
role of phrase detection is yet to be explored and
</bodyText>
<page confidence="0.886507">
76
</page>
<figure confidence="0.999095090909091">
90
80
Precision Values (%)
70
60
50
40
30
20
10
0
1.0
Precision Difference (%)
0.7
0.3
0.0
-0.3
-0.7
-1
2.0
1.5
1
0.5
0
-1
Precision Difference (%)
-1.5
-2
100
90
80
70
Precision Values (%)
60
50
40
30
20
10
0
Precision-Recall Curves TREC 1
0 10 20 30 40
Recall Values (%)
Precision-Recall Curves TREC 4
0 10 20 30 40 50 60 70 80
Recall Values (%)
Differences from Interpolated Precision in TREC 1
0 10 20 30
Recall Values (%)
0 10 20 30 40 50 60 70 80
Recall Values (%)
VSM
GVSM
VSM
GVSM
GVSM
TFIDF VSM
Differences from Interpolated Precision in TREC 4
GVSM
TFIDF VSM
Precision-Recall Curves TREC 6
VSM
GVSM
0 10 20 30 40 50 60 70 80
Recall Values (%)
Differences from Interpolated Precision in TREC 6
GVSM
TFIDF VSM
0 10 20 30 40 50 60 70
Recall Values (%)
70
0
50
40
30
20
10
Precision Values (%)
0
2.0
1.5
1
0.5
0
-1
Precision Difference (%)
-1.5
-2
</figure>
<figureCaption confidence="0.999959">
Figure 3: Differences (%) from the baseline in interpolated precision.
</figureCaption>
<bodyText confidence="0.999917416666667">
added into the model. Since we are using a large
knowledge-base (WordNet), we can add a simple
method to look-up term occurrences in a specified
window and check whether they form a phrase.
This would also decrease the ambiguity of the re-
spective text fragment, since in WordNet a phrase
is usually monosemous.
Moreover, there are additional aspects that de-
serve further research. In previously proposed
GVSM, like the one proposed by Voorhees (1993),
or by Mavroeidis et al. (2005), it is suggested
that semantic information can create an individual
space, leading to a dual representation of each doc-
ument, namely, a vector with document’s terms
and another with semantic information. Ratio-
nally, the proposed GVSM could act complemen-
tary to the standard VSM representation. Thus, the
similarity between a query and a document may be
computed by weighting the similarity in the terms
space and the senses’ space. Finally, we should
also examine the perspective of applying the pro-
posed measure of semantic relatedness in a query
expansion technique, similarly to the work of Fang
(2008).
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999696857142857">
In this paper we presented a new measure of
semantic relatedness and expanded the standard
VSM to embed the semantic relatedness between
pairs of terms into a new GVSM model. The
semantic relatedness measure takes into account
all of the semantic links offered by WordNet. It
considers WordNet as a graph, weighs edges de-
pending on their type and depth and computes
the maximum relatedness between any two nodes,
connected via one or more paths. The com-
parison to well known measures gives promis-
ing results. The application of our measure in
the suggested GVSM demonstrates slightly im-
proved performance in information retrieval tasks.
It is on our next plans to study the influence of
WSD performance on the proposed model. Fur-
thermore, a comparative analysis between the pro-
posed GVSM and other semantic network based
models will also shed light towards the condi-
tions, under which, embedding semantic informa-
tion improves text retrieval.
</bodyText>
<page confidence="0.994009">
77
</page>
<bodyText confidence="0.97494175">
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proc, of the 42nd ACL, pages 280–287.
Spain.
</bodyText>
<sectionHeader confidence="0.933417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999837905263159">
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
S. Brody, R. Navigli, and M. Lapata. 2006. Ensemble
methods for unsupervised wsd. In Proc. of COL-
ING/ACL 2006, pages 97–104.
A. Budanitsky and G. Hirst. 2006. Evaluating
wordnet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1):13–47.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990.
Introduction to Algorithms. The MIT Press.
H. Fang. 2008. A re-examination of query expansion
using lexical resources. In Proc. ofACL 2008, pages
139–147.
C. Fellbaum. 1998. WordNet – an electronic lexical
database. MIT Press.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
TOIS, 20(1):116–131.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proc. of the 20th IJCAI, pages
1606–1611. Hyderabad, India.
G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms. In WordNet: An Electronic
Lexical Database, chapter 13, pages 305–332, Cam-
bridge. The MIT Press.
M. Jarmasz and S. Szpakowicz. 2003. Roget’s the-
saurus and semantic similarity. In Proc. of Confer-
ence on Recent Advances in Natural Language Pro-
cessing, pages 212–219.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of ROCLING X, pages 19–33.
R. Krovetz and W.B. Croft. 1992. Lexical ambigu-
ity and information retrieval. ACM Transactions on
Information Systems, 10(2):115–141.
C. Leacock, G. Miller, and M. Chodorow. 1998.
Using corpus statistics and wordnet relations for
sense identification. Computational Linguistics,
24(1):147–165, March.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proc. of the 15th International Con-
ference on Machine Learning, pages 296–304.
D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,
M. Theobald, and G. Weikum. 2005. Word sense
disambiguation for exploiting hierarchical thesauri
in text classification. In Proc. of the 9th PKDD,
pages 181–192.
R. Mihalcea and A. Csomai. 2005. Senselearner:
Word sense disambiguation for all words in unre-
stricted text. In Proc. of the 43rd ACL, pages 53–56.
R. Mihalcea, P. Tarau, and E. Figa. 2004. Pagerank on
semantic networks with application to word sense
disambiguation. In Proc. of the 20th COLING.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1–28.
A. Montoyo, A. Suarez, G. Rigau, and M. Palomar.
2005. Combining knowledge- and corpus-based
word-sense-disambiguation methods. Journal ofAr-
tificial Intelligence Research, 23:299–330, March.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proc. of the 14th IJCAI,
pages 448–453, Canada.
H. Rubenstein and J.B. Goodenough. 1965. Contex-
tual correlates of synonymy. Communications of the
ACM, 8(10):627–633.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
M. Sanderson. 1994. Word sense disambiguation and
information retrieval. In Proc. of the 17th SIGIR,
pages 142–151, Ireland. ACM.
R. Sinha and R. Mihalcea. 2007. Unsupervised graph-
based word sense disambiguation using measures of
word semantic similarity. In Proc. of the IEEE In-
ternational Conference on Semantic Computing.
C. Stokoe, M.P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proc. of the 26th SIGIR, pages 159–166.
M. Strube and S.P. Ponzetto. 2006. Wikirelate! com-
puting semantic relatedness using wikipedia. In
Proc. of the 21st AAAI.
G. Tsatsaronis, M. Vazirgiannis, and I. Androutsopou-
los. 2007. Word sense disambiguation with spread-
ing activation networks generated from thesauri. In
Proc. of the 20th IJCAI, pages 1725–1730.
E. Voorhees. 1993. Using wordnet to disambiguate
word sense for text retrieval. In Proc. of the 16th
SIGIR, pages 171–180. ACM.
S.K.M. Wong, W. Ziarko, V.V. Raghavan, and P.C.N.
Wong. 1987. On modeling of information retrieval
concepts in vector spaces. ACM Transactions on
Database Systems, 12(2):299–321.
</reference>
<page confidence="0.998827">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892628">
<title confidence="0.9991165">A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness</title>
<author confidence="0.988594">Tsatsaronis Panagiotopoulou</author>
<affiliation confidence="0.999711">Department of Informatics Athens University of Economics and Business,</affiliation>
<address confidence="0.999394">76, Patision Str., Athens, Greece</address>
<email confidence="0.999507">gbt@aueb.gr,vpanagiotopoulou@gmail.com</email>
<abstract confidence="0.995933541666667">Generalized Vector Space Models (GVSM) extend the standard Vector Space Model (VSM) by embedding additional types of information, besides terms, in the representation of documents. An interesting type of information that can be used in such models is semantic information from word thesauri like WordNet. Previous attempts to construct GVSM reported contradicting results. The most challenging problem is to incorporate the semantic information in a theoretically sound and rigorous manner and to modify the standard interpretation of the VSM. In this paper we present a new GVSM model that exploits WordNet’s semantic information. The model is based on a new measure of semantic relatedness between terms. Experimental study conducted in three TREC collections reveals that semantic information can boost text retrieval performance with the use of the proposed GVSM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison Wesley.</publisher>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>R Navigli</author>
<author>M Lapata</author>
</authors>
<title>Ensemble methods for unsupervised wsd.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL</booktitle>
<pages>97--104</pages>
<contexts>
<context position="20636" citStr="Brody et al., 2006" startWordPosition="3633" endWordPosition="3636">. 1 3.4 Word Sense Disambiguation The reader will have noticed that our model computes the SR between two terms ti,tj, based on the pair of senses si,sj of the two terms respectively, which maximizes the product SCM · SPE. Alternatively, a WSD algorithm could have disambiguated the two terms, given the text fragments where the two terms occurred. Though interesting, this prospect is neither addressed, nor examined in this work. Still, it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches, like knowledgebased (Sinha and Mihalcea, 2007; Brody et al., 2006), corpus-based (Mihalcea and Csomai, 2005; McCarthy et al., 2004), or combinations with very high accuracy (Montoyo et al., 2005). 3.5 The GVSM Model In equation 2, which captures the document-query similarity in the GVSM model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors tits. Recall that t� are in reality unknown. We estimate ti an d maximizing SCM SPE. tit; = O) (3) in model we assume that term can be semantically related with any other term, an · SR((ti,tj),(si,sj), Since our each d their inner product by equation 3, where si a</context>
</contexts>
<marker>Brody, Navigli, Lapata, 2006</marker>
<rawString>S. Brody, R. Navigli, and M. Lapata. 2006. Ensemble methods for unsupervised wsd. In Proc. of COLING/ACL 2006, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="12289" citStr="Budanitsky and Hirst, 2006" startWordPosition="2061" endWordPosition="2064">the path (SPE (S, O is defined as7�7l 2didi+1 1 p ((� )) ne HI di+di+1 dmax, where di is the depth of sense si according to O, and dmax the maximum depth of O. If s1 = s2, and d = d1 = d2, SPE(S, O) = d d x. If there is ma no path from s1 to s2, SPE(S, O) = 0. Essentially, SPE is the harmonic mean of the two depths normalized to the maximum thesaurus depth. The harmonic mean offers a lower upper bound than the average of depths and we think is a more realistic estimation of the path’s depth. SCM and SPE capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky and Hirst, 2006), namely path length and senses depth in the used thesaurus. We combine these two measures naturally towards defining the Semantic Relatedness between two terms. Definition 3 Given a word thesaurus O, a pair of terms T = (t1, t2), and all pairs of senses S = (s1i, s2j), where s1i, s2j senses of t1,t2 respectively. The semantic relatedness of T (SR(T,S,O)) is defined as max{SCM(S, O)·SPE(S, O)}. SR between two terms ti, tj where ti ≡ tj ≡ t and t ∈/ O is defined as 1. If ti ∈ O but tj ∈/ O, or ti ∈/ O but tj ∈ O, SR is defined as 0. 72 Figure 1: Computation of semantic relatedness. 3.2 Semantic</context>
<context position="24897" citStr="Budanitsky and Hirst (2006)" startWordPosition="4386" endWordPosition="4389"> in the R&amp;G data set, and with increasing order of relatedness values based on human judgements, the respective values of these pairs that SR produces. A closer look on Figure 2 reveals that the values produced by SR (right figure) follow a pattern similar to that of the human ratings (left figure). Note that the x-axis in both charts begins from the least related pair of terms, according to humans, and goes up to the most related pair of terms. The y-axis in the left chart is the respective humans’ rating for each pair of terms. The right figure shows SR for each pair. The reader can consult Budanitsky and Hirst (2006) to confirm that all the other measures of semantic relatedness we compare to, do not follow the same pattern as the human ratings, as closely as our measure of relatedness does (low y values for small x values and high y values for high x). The same pattern applies in the M&amp;C and 353-C data sets. 4.2 Evaluation of the GVSM For the evaluation of the proposed GVSM model, we have experimented with three TREC collections 3, namely TREC 1 (TIPSTER disks 1 and 2), TREC 4 (TIPSTER disks 2 and 3) and TREC 6 (TIPSTER disks 4 and 5). We selected those TREC collections in order to cover as many differen</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="18420" citStr="Cormen et al., 1990" startWordPosition="3191" endWordPosition="3194">7r[v] = NULL 4: end for 5: d[u] = 1 Relax(u, v, w) 6: if d[v] &lt; d[u] · w(u, v) then 7: d[v] = d[u] · w(u, v) 8: 7r[v] = u 9: end if Maximum-Relatedness(G,u,v,w) 10: Initialize-Single-Source(G,u) 11: S = ∅ 12: Q = V [G] 13: while v ∈ Q do 14: s = Extract from Q the vertex with max d 15: S = S ∪ s 16: for all vertices k ∈ Adjacency List of s do 17: Relax(s,k,w) 18: end for 19: end while 20: return the path following all the ancestors 7r of v back to u weight assigned by weighting function w. Proof 1 For the proof of this theorem we follow the course of thinking of the proof of theorem 25.10 in (Cormen et al., 1990). We shall show that for each vertex sf ∈ V, d[sf] is the maximum product of edges’ weight through the selected path, starting from ss, at the time when sf is inserted into S. From now on, the notation S(ss, sf) will represent this product. Path p connects a vertex in S, namely ss, to a vertex in V − S, namely sf. Consider the first vertex sy along p such that sy ∈ V − S and let sx be y’s predecessor. Now, path p can be decomposed as ss → sx → sy → sf. We claim that d[sy] = S(ss, sy) when sf is inserted into S. Observe that sx ∈ S. Then, because sf is chosen as the first vertex for which d[sf]</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. Introduction to Algorithms. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fang</author>
</authors>
<title>A re-examination of query expansion using lexical resources.</title>
<date>2008</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>139--147</pages>
<contexts>
<context position="31593" citStr="Fang (2008)" startWordPosition="5593" endWordPosition="5594">2005), it is suggested that semantic information can create an individual space, leading to a dual representation of each document, namely, a vector with document’s terms and another with semantic information. Rationally, the proposed GVSM could act complementary to the standard VSM representation. Thus, the similarity between a query and a document may be computed by weighting the similarity in the terms space and the senses’ space. Finally, we should also examine the perspective of applying the proposed measure of semantic relatedness in a query expansion technique, similarly to the work of Fang (2008). 6 Conclusions In this paper we presented a new measure of semantic relatedness and expanded the standard VSM to embed the semantic relatedness between pairs of terms into a new GVSM model. The semantic relatedness measure takes into account all of the semantic links offered by WordNet. It considers WordNet as a graph, weighs edges depending on their type and depth and computes the maximum relatedness between any two nodes, connected via one or more paths. The comparison to well known measures gives promising results. The application of our measure in the suggested GVSM demonstrates slightly </context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>H. Fang. 2008. A re-examination of query expansion using lexical resources. In Proc. ofACL 2008, pages 139–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet – an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1358" citStr="Fellbaum, 1998" startWordPosition="196" endWordPosition="197">y sound and rigorous manner and to modify the standard interpretation of the VSM. In this paper we present a new GVSM model that exploits WordNet’s semantic information. The model is based on a new measure of semantic relatedness between terms. Experimental study conducted in three TREC collections reveals that semantic information can boost text retrieval performance with the use of the proposed GVSM. 1 Introduction The use of semantic information into text retrieval or text classification has been controversial. For example in Mavroeidis et al. (2005) it was shown that a GVSM using WordNet (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved performance, while several years before, Krovetz and Croft </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet – an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM TOIS,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="22742" citStr="Finkelstein et al., 2002" startWordPosition="4010" endWordPosition="4013">he cosine similarity between a document dk and a query q now becomes: cos(�dk, �q) = Pi= 1Pnj=idk(ti,tj)&apos;q(ti,tj) qPi= dk (ti, tj )2 &apos;qPi= q(ti, tj )2 (4) where n is the dimension of the VSM TF-IDF space. 4 Experimental Evaluation The experimental evaluation in this work is twofold. First, we test the performance of the semantic relatedness measure (SR) for a pair of words in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (Rubenstein and Goodenough, 1965)(R&amp;G), the Miller and Charles 30 word pairs (Miller and Charles, 1991)(M&amp;C), and the 353 similarity data set (Finkelstein et al., 2002). Second, we evaluate the performance of the proposed GVSM in three TREC collections (TREC 1, 4 and 6). 4.1 Evaluation of the Semantic Relatedness Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words.</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM TOIS, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proc. of the 20th IJCAI,</booktitle>
<pages>1606--1611</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="23618" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="4152" endWordPosition="4155">erimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. The latter contains the word pairs of the M&amp;C data set. To visualize the performance of our measure in a more co</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proc. of the 20th IJCAI, pages 1606–1611. Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>In WordNet: An Electronic Lexical Database, chapter 13,</booktitle>
<pages>305--332</pages>
<publisher>The MIT Press.</publisher>
<location>Cambridge.</location>
<contexts>
<context position="23448" citStr="Hirst and St-Onge (1998)" startWordPosition="4126" endWordPosition="4129">ns (TREC 1, 4 and 6). 4.1 Evaluation of the Semantic Relatedness Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G. Hirst and D. St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In WordNet: An Electronic Lexical Database, chapter 13, pages 305–332, Cambridge. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jarmasz</author>
<author>S Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proc. of Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>212--219</pages>
<contexts>
<context position="23578" citStr="Jarmasz and Szpakowicz (2003)" startWordPosition="4147" endWordPosition="4150">ess measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. The latter contains the word pairs of the M&amp;C data set. To visualize the</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>M. Jarmasz and S. Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Proc. of Conference on Recent Advances in Natural Language Processing, pages 212–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of ROCLING X,</booktitle>
<pages>pages</pages>
<contexts>
<context position="23479" citStr="Jiang and Conrath (1997)" startWordPosition="4131" endWordPosition="4134">ation of the Semantic Relatedness Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of ROCLING X, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Krovetz</author>
<author>W B Croft</author>
</authors>
<title>Lexical ambiguity and information retrieval.</title>
<date>1992</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="1964" citStr="Krovetz and Croft (1992)" startWordPosition="284" endWordPosition="287">t (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved performance, while several years before, Krovetz and Croft (1992) had already pointed out that resolving word senses can improve searches requiring high levels of recall. In this work, we argue that the incorporation of semantic information into a GVSM retrieval model can improve performance by considering the semantic relatedness between the query and document terms. The proposed model extends the traditional VSM with term to term relatedness measured with the use of WordNet. The success of the method lies in three important factors, which also constitute the points of our contribution: 1) a new measure for computing semantic relatedness between terms whic</context>
<context position="6952" citStr="Krovetz and Croft, 1992" startWordPosition="1131" endWordPosition="1134">lations between terms ti and tj are known. If one assumes pairwise orthogonality, the similarity measure is reduced to that of equation 1. 2.3 Semantic Information and GVSM Since the introduction of the first GVSM model, there are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model: (a) compute semantic correlations between terms, or (b) compute frequency co-occurrence statistics from large corpora. In this paper we focus on the first direction. In the past, the effect of WSD information in text retrieval was studied (Krovetz and Croft, 1992; Sanderson, 1994), with the results revealing that under circumstances, senses information may improve IR. More specifically, Krovetz and Croft (1992) performed a series of three experiments in two document collections, CACM and TIMES. The results of their experiments showed that word senses provide a clear distinction between relevant and nonrelevant documents, rejecting the null hypothesis that the meaning of a word is not related to judgments of relevance. Also, they reached the conclusion that words being worth of disambiguation are either the words with uniform distribution of senses, or</context>
</contexts>
<marker>Krovetz, Croft, 1992</marker>
<rawString>R. Krovetz and W.B. Croft. 1992. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems, 10(2):115–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Miller</author>
<author>M Chodorow</author>
</authors>
<title>Using corpus statistics and wordnet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="23507" citStr="Leacock et al. (1998)" startWordPosition="4136" endWordPosition="4139">ss Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. T</context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>C. Leacock, G. Miller, and M. Chodorow. 1998. Using corpus statistics and wordnet relations for sense identification. Computational Linguistics, 24(1):147–165, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="23523" citStr="Lin (1998)" startWordPosition="4141" endWordPosition="4142">on of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. The latter contai</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. An information-theoretic definition of similarity. In Proc. of the 15th International Conference on Machine Learning, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mavroeidis</author>
<author>G Tsatsaronis</author>
<author>M Vazirgiannis</author>
<author>M Theobald</author>
<author>G Weikum</author>
</authors>
<title>Word sense disambiguation for exploiting hierarchical thesauri in text classification.</title>
<date>2005</date>
<booktitle>In Proc. of the 9th PKDD,</booktitle>
<pages>181--192</pages>
<contexts>
<context position="1302" citStr="Mavroeidis et al. (2005)" startWordPosition="184" endWordPosition="187">blem is to incorporate the semantic information in a theoretically sound and rigorous manner and to modify the standard interpretation of the VSM. In this paper we present a new GVSM model that exploits WordNet’s semantic information. The model is based on a new measure of semantic relatedness between terms. Experimental study conducted in three TREC collections reveals that semantic information can boost text retrieval performance with the use of the proposed GVSM. 1 Introduction The use of semantic information into text retrieval or text classification has been controversial. For example in Mavroeidis et al. (2005) it was shown that a GVSM using WordNet (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved per</context>
<context position="8233" citStr="Mavroeidis et al. (2005)" startWordPosition="1336" endWordPosition="1339">the most popular one. Sanderson (1994) studied the influence of disambiguation in IR with the use of pseudowords and he concluded that sense ambiguity is problematic for IR only in the cases of retrieving from short queries. Furthermore, his findings regarding the WSD used were that such a WSD system would help IR if it could perform with very high accuracy, although his experiments were conducted in the Reuters collection, where standard queries with corresponding relevant documents (qrels) are not provided. Since then, several recent approaches have incorporated semantic information in VSM. Mavroeidis et al. (2005) created a GVSM kernel based on the use of noun senses, and their hypernyms from WordNet. They experimentally 71 showed that this can improve text categorization. Stokoe et al. (Stokoe et al., 2003) reported an improvement in retrieval performance using a fully sense-based system. Our approach differs from the aforementioned ones in that it expands the VSM model using the semantic information of a word thesaurus to interpret the orthogonality of terms and to measure semantic relatedness, instead of directly replacing terms with senses, or adding senses to the model. 3 A GVSM Model based on Sem</context>
<context position="30987" citStr="Mavroeidis et al. (2005)" startWordPosition="5494" endWordPosition="5497">ision Values (%) 0 2.0 1.5 1 0.5 0 -1 Precision Difference (%) -1.5 -2 Figure 3: Differences (%) from the baseline in interpolated precision. added into the model. Since we are using a large knowledge-base (WordNet), we can add a simple method to look-up term occurrences in a specified window and check whether they form a phrase. This would also decrease the ambiguity of the respective text fragment, since in WordNet a phrase is usually monosemous. Moreover, there are additional aspects that deserve further research. In previously proposed GVSM, like the one proposed by Voorhees (1993), or by Mavroeidis et al. (2005), it is suggested that semantic information can create an individual space, leading to a dual representation of each document, namely, a vector with document’s terms and another with semantic information. Rationally, the proposed GVSM could act complementary to the standard VSM representation. Thus, the similarity between a query and a document may be computed by weighting the similarity in the terms space and the senses’ space. Finally, we should also examine the perspective of applying the proposed measure of semantic relatedness in a query expansion technique, similarly to the work of Fang </context>
</contexts>
<marker>Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald, Weikum, 2005</marker>
<rawString>D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis, M. Theobald, and G. Weikum. 2005. Word sense disambiguation for exploiting hierarchical thesauri in text classification. In Proc. of the 9th PKDD, pages 181–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>A Csomai</author>
</authors>
<title>Senselearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd ACL,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="20677" citStr="Mihalcea and Csomai, 2005" startWordPosition="3638" endWordPosition="3641">he reader will have noticed that our model computes the SR between two terms ti,tj, based on the pair of senses si,sj of the two terms respectively, which maximizes the product SCM · SPE. Alternatively, a WSD algorithm could have disambiguated the two terms, given the text fragments where the two terms occurred. Though interesting, this prospect is neither addressed, nor examined in this work. Still, it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches, like knowledgebased (Sinha and Mihalcea, 2007; Brody et al., 2006), corpus-based (Mihalcea and Csomai, 2005; McCarthy et al., 2004), or combinations with very high accuracy (Montoyo et al., 2005). 3.5 The GVSM Model In equation 2, which captures the document-query similarity in the GVSM model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors tits. Recall that t� are in reality unknown. We estimate ti an d maximizing SCM SPE. tit; = O) (3) in model we assume that term can be semantically related with any other term, an · SR((ti,tj),(si,sj), Since our each d their inner product by equation 3, where si and sj are the senses of terms ti and tj r</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>R. Mihalcea and A. Csomai. 2005. Senselearner: Word sense disambiguation for all words in unrestricted text. In Proc. of the 43rd ACL, pages 53–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
<author>E Figa</author>
</authors>
<title>Pagerank on semantic networks with application to word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th COLING.</booktitle>
<contexts>
<context position="13252" citStr="Mihalcea et al., 2004" startWordPosition="2242" endWordPosition="2245">of T (SR(T,S,O)) is defined as max{SCM(S, O)·SPE(S, O)}. SR between two terms ti, tj where ti ≡ tj ≡ t and t ∈/ O is defined as 1. If ti ∈ O but tj ∈/ O, or ti ∈/ O but tj ∈ O, SR is defined as 0. 72 Figure 1: Computation of semantic relatedness. 3.2 Semantic Networks from Word Thesauri In order to construct a semantic network for a pair of terms t1 and t2 and a combination of their respective senses, i.e., s1 and s2, we adopted the network construction method that we introduced in (Tsatsaronis et al., 2007). This method was preferred against other related methods, like the one introduced in (Mihalcea et al., 2004), since it embeds all the available semantic information existing in WordNet, even edges that cross POS, thus offering a richer semantic representation. According to the adopted semantic network construction model, each semantic edge type is given a different weight. The intuition behind edge types’ weighting is that certain types provide stronger semantic connections than others. The frequency of occurrence of the different edge types in Wordnet 2.0, is used to define the edge types’ weights (e.g. 0.57 for hypernym/hyponym edges, 0.14 for nominalization edges etc.). Figure 1 shows the constru</context>
</contexts>
<marker>Mihalcea, Tarau, Figa, 2004</marker>
<rawString>R. Mihalcea, P. Tarau, and E. Figa. 2004. Pagerank on semantic networks with application to word sense disambiguation. In Proc. of the 20th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="22677" citStr="Miller and Charles, 1991" startWordPosition="4000" endWordPosition="4003">w GVSM model, which is a natural extension of the standard VSM. The cosine similarity between a document dk and a query q now becomes: cos(�dk, �q) = Pi= 1Pnj=idk(ti,tj)&apos;q(ti,tj) qPi= dk (ti, tj )2 &apos;qPi= q(ti, tj )2 (4) where n is the dimension of the VSM TF-IDF space. 4 Experimental Evaluation The experimental evaluation in this work is twofold. First, we test the performance of the semantic relatedness measure (SR) for a pair of words in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (Rubenstein and Goodenough, 1965)(R&amp;G), the Miller and Charles 30 word pairs (Miller and Charles, 1991)(M&amp;C), and the 353 similarity data set (Finkelstein et al., 2002). Second, we evaluate the performance of the proposed GVSM in three TREC collections (TREC 1, 4 and 6). 4.1 Evaluation of the Semantic Relatedness Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can te</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G.A. Miller and W.G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Montoyo</author>
<author>A Suarez</author>
<author>G Rigau</author>
<author>M Palomar</author>
</authors>
<title>Combining knowledge- and corpus-based word-sense-disambiguation methods.</title>
<date>2005</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>23</volume>
<contexts>
<context position="20765" citStr="Montoyo et al., 2005" startWordPosition="3652" endWordPosition="3655"> the pair of senses si,sj of the two terms respectively, which maximizes the product SCM · SPE. Alternatively, a WSD algorithm could have disambiguated the two terms, given the text fragments where the two terms occurred. Though interesting, this prospect is neither addressed, nor examined in this work. Still, it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches, like knowledgebased (Sinha and Mihalcea, 2007; Brody et al., 2006), corpus-based (Mihalcea and Csomai, 2005; McCarthy et al., 2004), or combinations with very high accuracy (Montoyo et al., 2005). 3.5 The GVSM Model In equation 2, which captures the document-query similarity in the GVSM model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors tits. Recall that t� are in reality unknown. We estimate ti an d maximizing SCM SPE. tit; = O) (3) in model we assume that term can be semantically related with any other term, an · SR((ti,tj),(si,sj), Since our each d their inner product by equation 3, where si and sj are the senses of terms ti and tj respectively, 2The sign of the algorithm isnot considere d at this step. 74 5R((ti, tj), </context>
</contexts>
<marker>Montoyo, Suarez, Rigau, Palomar, 2005</marker>
<rawString>A. Montoyo, A. Suarez, G. Rigau, and M. Palomar. 2005. Combining knowledge- and corpus-based word-sense-disambiguation methods. Journal ofArtificial Intelligence Research, 23:299–330, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proc. of the 14th IJCAI,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="23542" citStr="Resnik (1995)" startWordPosition="4144" endWordPosition="4145">ed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. The latter contains the word pairs o</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity. In Proc. of the 14th IJCAI, pages 448–453, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="22607" citStr="Rubenstein and Goodenough, 1965" startWordPosition="3988" endWordPosition="3992">he query terms, and this is its great advantage. This new space leads to a new GVSM model, which is a natural extension of the standard VSM. The cosine similarity between a document dk and a query q now becomes: cos(�dk, �q) = Pi= 1Pnj=idk(ti,tj)&apos;q(ti,tj) qPi= dk (ti, tj )2 &apos;qPi= q(ti, tj )2 (4) where n is the dimension of the VSM TF-IDF space. 4 Experimental Evaluation The experimental evaluation in this work is twofold. First, we test the performance of the semantic relatedness measure (SR) for a pair of words in three benchmark data sets, namely the Rubenstein and Goodenough 65 word pairs (Rubenstein and Goodenough, 1965)(R&amp;G), the Miller and Charles 30 word pairs (Miller and Charles, 1991)(M&amp;C), and the 353 similarity data set (Finkelstein et al., 2002). Second, we evaluate the performance of the proposed GVSM in three TREC collections (TREC 1, 4 and 6). 4.1 Evaluation of the Semantic Relatedness Measure For the evaluation of the proposed semantic relatedness measure between two terms we experimented in three widely used data sets in which human subjects have provided scores of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irr</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H. Rubenstein and J.B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="3472" citStr="Salton and McGill, 1983" startWordPosition="519" endWordPosition="522">S). Experimental evaluation in three TREC collections shows that the proposed model can improve in certain cases the performance of the standard TF-IDF VSM. The rest of the paper is organized as follows: Section 2 presents preliminary concepts, regarding VSM and GVSM. Section 3 presents the term semantic relatedness measure and the proposed GVSM. Section 4 analyzes the experimental results, and Section 5 concludes and gives pointers to future work. 2 Background 2.1 Vector Space Model The VSM has been a standard model of representing documents in information retrieval for almost three decades (Salton and McGill, 1983; BaezaYates and Ribeiro-Neto, 1999). Let D be a document collection and Q the set of queries representing users’ information needs. Let also tz symbolProceedings of the EACL 2009 Student Research Workshop, pages 70–78, Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics 70 ize term i used to index the documents in the collection, with i = 1,.., n. The VSM assumes that for each term ti there exists a vector ~ti in the vector space that represents it. It then considers the set of all term vectors {~ti} to be the generating set of the vector space, thus the space basis</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
</authors>
<title>Word sense disambiguation and information retrieval.</title>
<date>1994</date>
<booktitle>In Proc. of the 17th SIGIR,</booktitle>
<pages>142--151</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1494" citStr="Sanderson (1994)" startWordPosition="215" endWordPosition="216">ts WordNet’s semantic information. The model is based on a new measure of semantic relatedness between terms. Experimental study conducted in three TREC collections reveals that semantic information can boost text retrieval performance with the use of the proposed GVSM. 1 Introduction The use of semantic information into text retrieval or text classification has been controversial. For example in Mavroeidis et al. (2005) it was shown that a GVSM using WordNet (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved performance, while several years before, Krovetz and Croft (1992) had already pointed out that resolving word senses can improve searches requiring high levels of recall. In this work, we argue t</context>
<context position="6970" citStr="Sanderson, 1994" startWordPosition="1135" endWordPosition="1136">and tj are known. If one assumes pairwise orthogonality, the similarity measure is reduced to that of equation 1. 2.3 Semantic Information and GVSM Since the introduction of the first GVSM model, there are at least two basic directions for embedding term to term relatedness, other than exact keyword matching, into a retrieval model: (a) compute semantic correlations between terms, or (b) compute frequency co-occurrence statistics from large corpora. In this paper we focus on the first direction. In the past, the effect of WSD information in text retrieval was studied (Krovetz and Croft, 1992; Sanderson, 1994), with the results revealing that under circumstances, senses information may improve IR. More specifically, Krovetz and Croft (1992) performed a series of three experiments in two document collections, CACM and TIMES. The results of their experiments showed that word senses provide a clear distinction between relevant and nonrelevant documents, rejecting the null hypothesis that the meaning of a word is not related to judgments of relevance. Also, they reached the conclusion that words being worth of disambiguation are either the words with uniform distribution of senses, or the words that in</context>
</contexts>
<marker>Sanderson, 1994</marker>
<rawString>M. Sanderson. 1994. Word sense disambiguation and information retrieval. In Proc. of the 17th SIGIR, pages 142–151, Ireland. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised graphbased word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<booktitle>In Proc. of the IEEE International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="20615" citStr="Sinha and Mihalcea, 2007" startWordPosition="3629" endWordPosition="3632">ax = SCM(S, O) · SPE(S, O). 1 3.4 Word Sense Disambiguation The reader will have noticed that our model computes the SR between two terms ti,tj, based on the pair of senses si,sj of the two terms respectively, which maximizes the product SCM · SPE. Alternatively, a WSD algorithm could have disambiguated the two terms, given the text fragments where the two terms occurred. Though interesting, this prospect is neither addressed, nor examined in this work. Still, it is in our next plans and part of our future work to embed in our model some of the interesting WSD approaches, like knowledgebased (Sinha and Mihalcea, 2007; Brody et al., 2006), corpus-based (Mihalcea and Csomai, 2005; McCarthy et al., 2004), or combinations with very high accuracy (Montoyo et al., 2005). 3.5 The GVSM Model In equation 2, which captures the document-query similarity in the GVSM model, the orthogonality between terms ti and tj is expressed by the inner product of the respective term vectors tits. Recall that t� are in reality unknown. We estimate ti an d maximizing SCM SPE. tit; = O) (3) in model we assume that term can be semantically related with any other term, an · SR((ti,tj),(si,sj), Since our each d their inner product by e</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>R. Sinha and R. Mihalcea. 2007. Unsupervised graphbased word sense disambiguation using measures of word semantic similarity. In Proc. of the IEEE International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stokoe</author>
<author>M P Oakes</author>
<author>J Tait</author>
</authors>
<title>Word sense disambiguation in information retrieval revisited.</title>
<date>2003</date>
<booktitle>In Proc. of the 26th SIGIR,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="1889" citStr="Stokoe et al. (2003)" startWordPosition="273" endWordPosition="276">ample in Mavroeidis et al. (2005) it was shown that a GVSM using WordNet (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved performance, while several years before, Krovetz and Croft (1992) had already pointed out that resolving word senses can improve searches requiring high levels of recall. In this work, we argue that the incorporation of semantic information into a GVSM retrieval model can improve performance by considering the semantic relatedness between the query and document terms. The proposed model extends the traditional VSM with term to term relatedness measured with the use of WordNet. The success of the method lies in three important factors, which also constitute the points of our contribut</context>
<context position="8431" citStr="Stokoe et al., 2003" startWordPosition="1370" endWordPosition="1373">ng from short queries. Furthermore, his findings regarding the WSD used were that such a WSD system would help IR if it could perform with very high accuracy, although his experiments were conducted in the Reuters collection, where standard queries with corresponding relevant documents (qrels) are not provided. Since then, several recent approaches have incorporated semantic information in VSM. Mavroeidis et al. (2005) created a GVSM kernel based on the use of noun senses, and their hypernyms from WordNet. They experimentally 71 showed that this can improve text categorization. Stokoe et al. (Stokoe et al., 2003) reported an improvement in retrieval performance using a fully sense-based system. Our approach differs from the aforementioned ones in that it expands the VSM model using the semantic information of a word thesaurus to interpret the orthogonality of terms and to measure semantic relatedness, instead of directly replacing terms with senses, or adding senses to the model. 3 A GVSM Model based on Semantic Relatedness of Terms Synonymy (many words per sense) and polysemy (many senses per word) are two fundamental problems in text retrieval. Synonymy is related with recall, while polysemy with pr</context>
</contexts>
<marker>Stokoe, Oakes, Tait, 2003</marker>
<rawString>C. Stokoe, M.P. Oakes, and J. Tait. 2003. Word sense disambiguation in information retrieval revisited. In Proc. of the 26th SIGIR, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>S P Ponzetto</author>
</authors>
<title>Wikirelate! computing semantic relatedness using wikipedia.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st AAAI.</booktitle>
<contexts>
<context position="23693" citStr="Strube and Ponzetto (2006)" startWordPosition="4165" endWordPosition="4168">res of relatedness for each pair. A kind of ”gold standard” ranking of related word pairs (i.e., from the most related words to the most irrelevant) has thus been created, against which computer programs can test their ability on measuring semantic relatedness between words. We compared our measure against ten known measures of semantic relatedness: (HS) Hirst and St-Onge (1998), (JC) Jiang and Conrath (1997), (LC) Leacock et al. (1998), (L) Lin (1998), (R) Resnik (1995), (JS) Jarmasz and Szpakowicz (2003), (GM) Gabrilovich and Markovitch (2007), (F) Finkelstein et al. (2002), (HR) ) and (SP) Strube and Ponzetto (2006). In Table 1 the results of SR and the ten compared measures are shown. The reported numbers are the Spearman correlation of the measures’ rankings with the gold standard (human judgements). The correlations for the three data sets show that SR performs better than any other measure of semantic relatedness, besides the case of (HR) in the M&amp;C data set. It surpasses HR though in the R&amp;G and the 353-C data set. The latter contains the word pairs of the M&amp;C data set. To visualize the performance of our measure in a more comprehensible manner, Figure 2 presents for all pairs in the R&amp;G data set, a</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>M. Strube and S.P. Ponzetto. 2006. Wikirelate! computing semantic relatedness using wikipedia. In Proc. of the 21st AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tsatsaronis</author>
<author>M Vazirgiannis</author>
<author>I Androutsopoulos</author>
</authors>
<title>Word sense disambiguation with spreading activation networks generated from thesauri.</title>
<date>2007</date>
<booktitle>In Proc. of the 20th IJCAI,</booktitle>
<pages>1725--1730</pages>
<contexts>
<context position="13143" citStr="Tsatsaronis et al., 2007" startWordPosition="2224" endWordPosition="2227">, and all pairs of senses S = (s1i, s2j), where s1i, s2j senses of t1,t2 respectively. The semantic relatedness of T (SR(T,S,O)) is defined as max{SCM(S, O)·SPE(S, O)}. SR between two terms ti, tj where ti ≡ tj ≡ t and t ∈/ O is defined as 1. If ti ∈ O but tj ∈/ O, or ti ∈/ O but tj ∈ O, SR is defined as 0. 72 Figure 1: Computation of semantic relatedness. 3.2 Semantic Networks from Word Thesauri In order to construct a semantic network for a pair of terms t1 and t2 and a combination of their respective senses, i.e., s1 and s2, we adopted the network construction method that we introduced in (Tsatsaronis et al., 2007). This method was preferred against other related methods, like the one introduced in (Mihalcea et al., 2004), since it embeds all the available semantic information existing in WordNet, even edges that cross POS, thus offering a richer semantic representation. According to the adopted semantic network construction model, each semantic edge type is given a different weight. The intuition behind edge types’ weighting is that certain types provide stronger semantic connections than others. The frequency of occurrence of the different edge types in Wordnet 2.0, is used to define the edge types’ w</context>
</contexts>
<marker>Tsatsaronis, Vazirgiannis, Androutsopoulos, 2007</marker>
<rawString>G. Tsatsaronis, M. Vazirgiannis, and I. Androutsopoulos. 2007. Word sense disambiguation with spreading activation networks generated from thesauri. In Proc. of the 20th IJCAI, pages 1725–1730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Using wordnet to disambiguate word sense for text retrieval.</title>
<date>1993</date>
<booktitle>In Proc. of the 16th SIGIR,</booktitle>
<pages>171--180</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1702" citStr="Voorhees (1993)" startWordPosition="243" endWordPosition="244">text retrieval performance with the use of the proposed GVSM. 1 Introduction The use of semantic information into text retrieval or text classification has been controversial. For example in Mavroeidis et al. (2005) it was shown that a GVSM using WordNet (Fellbaum, 1998) senses and their hypernyms, improves text classification performance, especially for small training sets. In contrast, Sanderson (1994) reported that even 90% accurate WSD cannot guarantee retrieval improvement, though their experimental methodology was based only on randomly generated pseudowords of varying sizes. Similarly, Voorhees (1993) reported a drop in retrieval performance when the retrieval model was based on WSD information. On the contrary, the construction of a sense-based retrieval model by Stokoe et al. (2003) improved performance, while several years before, Krovetz and Croft (1992) had already pointed out that resolving word senses can improve searches requiring high levels of recall. In this work, we argue that the incorporation of semantic information into a GVSM retrieval model can improve performance by considering the semantic relatedness between the query and document terms. The proposed model extends the t</context>
<context position="30955" citStr="Voorhees (1993)" startWordPosition="5490" endWordPosition="5491">0 0 50 40 30 20 10 Precision Values (%) 0 2.0 1.5 1 0.5 0 -1 Precision Difference (%) -1.5 -2 Figure 3: Differences (%) from the baseline in interpolated precision. added into the model. Since we are using a large knowledge-base (WordNet), we can add a simple method to look-up term occurrences in a specified window and check whether they form a phrase. This would also decrease the ambiguity of the respective text fragment, since in WordNet a phrase is usually monosemous. Moreover, there are additional aspects that deserve further research. In previously proposed GVSM, like the one proposed by Voorhees (1993), or by Mavroeidis et al. (2005), it is suggested that semantic information can create an individual space, leading to a dual representation of each document, namely, a vector with document’s terms and another with semantic information. Rationally, the proposed GVSM could act complementary to the standard VSM representation. Thus, the similarity between a query and a document may be computed by weighting the similarity in the terms space and the senses’ space. Finally, we should also examine the perspective of applying the proposed measure of semantic relatedness in a query expansion technique</context>
</contexts>
<marker>Voorhees, 1993</marker>
<rawString>E. Voorhees. 1993. Using wordnet to disambiguate word sense for text retrieval. In Proc. of the 16th SIGIR, pages 171–180. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>V V Raghavan</author>
<author>P C N Wong</author>
</authors>
<title>On modeling of information retrieval concepts in vector spaces.</title>
<date>1987</date>
<journal>ACM Transactions on Database Systems,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="5162" citStr="Wong et al. (1987)" startWordPosition="824" endWordPosition="827">ness between any pair of terms, whereas the terms in a language often relate to each other. Provided that the orthogonality assumption holds, the similarity ~ between a document vector dk and a query vector q~ in the VSM can be expressed by the cosine measure given in equation 1. cos( dk, ~q) = 2 n2qPn Pn j=1 akjqj ( i=1 aki �j=1 qj where akj, qj are real numbers standing for the weights of term j in the document dk and the query q respectively. A standard baseline retrieval strategy is to rank the documents according to their cosine similarity to the query. 2.2 Generalized Vector Space Model Wong et al. (1987) presented an analysis of the problems that the pairwise orthogonality assumption of the VSM creates. They were the first to address these problems by expanding the VSM. They introduced term to term correlations, which deprecated the pairwise orthogonality assumption, but they kept the assumption that the term vectors are linearly independent1, creating the first GVSM model. More specifically, they considered a new space, where each term vector ~ti was expressed as a linear combination of 2n vectors ~mr, r = 1..2n. The similarity measure between a document and a query then became as shown in e</context>
</contexts>
<marker>Wong, Ziarko, Raghavan, Wong, 1987</marker>
<rawString>S.K.M. Wong, W. Ziarko, V.V. Raghavan, and P.C.N. Wong. 1987. On modeling of information retrieval concepts in vector spaces. ACM Transactions on Database Systems, 12(2):299–321.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>