<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99896">
Grammar Approximation by Representative Sublanguage:
A New Model for Language Learning
</title>
<author confidence="0.996047">
Smaranda Muresan
</author>
<affiliation confidence="0.9987525">
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<address confidence="0.809957">
College Park, MD 20742, USA
</address>
<email confidence="0.997188">
smara@umiacs.umd.edu
</email>
<author confidence="0.995315">
Owen Rambow
</author>
<affiliation confidence="0.997747">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.979316">
New York, NY 10027, USA
</address>
<email confidence="0.998985">
rambow@cs.columbia.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964444444444">
We propose a new language learning model
that learns a syntactic-semantic grammar
from a small number of natural language
strings annotated with their semantics, along
with basic assumptions about natural lan-
guage syntax. We show that the search space
for grammar induction is a complete gram-
mar lattice, which guarantees the uniqueness
of the learned grammar.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999702">
There is considerable interest in learning computa-
tional grammars.&apos; While much attention has focused
on learning syntactic grammars either in a super-
vised or unsupervised manner, recently there is a
growing interest toward learning grammars/parsers
that capture semantics as well (Bos et al., 2004;
Zettlemoyer and Collins, 2005; Ge and Mooney,
2005).
Learning both syntax and semantics is arguably
more difficult than learning syntax alone. In for-
mal grammar learning theory it has been shown that
learning from “good examples,” or representative
examples, is more powerful than learning from all
the examples (Freivalds et al., 1993). Haghighi and
Klein (2006) show that using a handful of “proto-
</bodyText>
<footnote confidence="0.540836">
&apos;This research was supported by the National Science Foun-
dation under Digital Library Initiative Phase II Grant Number
IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs).
We would like to thank Judith Klavans for her contributions
over the course of this research, Kathy McKeown for her in-
put, and several anonymous reviewers for very useful feedback
on earlier drafts of this paper.
</footnote>
<page confidence="0.971406">
832
</page>
<bodyText confidence="0.999931294117647">
types” significantly improves over a fully unsuper-
vised PCFG induction model (their prototypes were
formed by sequences of POS tags; for example, pro-
totypical NPs were DT NN, JJ NN).
In this paper, we present a new grammar formal-
ism and a new learning method which together ad-
dress the problem of learning a syntactic-semantic
grammar in the presence of a representative sample
of strings annotated with their semantics, along with
minimal assumptions about syntax (such as syntac-
tic categories). The semantic representation is an
ontology-based semantic representation. The anno-
tation of the representative examples does not in-
clude the entire derivation, unlike most of the ex-
isting syntactic treebanks. The aim of the paper is to
present the formal aspects of our grammar induction
model.
In Section 2, we present a new grammar formal-
ism, called Lexicalized Well-Founded Grammars,
a type of constraint-based grammars that combine
syntax and semantics. We then turn to the two main
results of this paper. In Section 3 we show that
our grammars can always be learned from a set of
positive representative examples (with no negative
examples), and the search space for grammar in-
duction is a complete grammar lattice, which guar-
antees the uniqueness of the learned grammar. In
Section 4, we propose a new computationally effi-
cient model for grammar induction from pairs of ut-
terances and their semantic representations, called
Grammar Approximation by Representative Sublan-
guage (GARS). Section 5 discusses the practical use
of our model and Section 6 states our conclusions
and future work.
</bodyText>
<note confidence="0.915894">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832–839,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.869959" genericHeader="introduction">
2 Lexicalized Well-Founded Grammars
</sectionHeader>
<bodyText confidence="0.999271956521739">
Lexicalized Well-Founded Grammars (LWFGs) are
a type of Definite Clause Grammars (Pereira and
Warren, 1980) where: (1) the Context-Free Gram-
mar backbone is extended by introducing a par-
tial ordering relation among nonterminals (well-
founded) 2) each string is associated with a
syntactic-semantic representation called semantic
molecule; 3) grammar rules have two types of con-
straints: one for semantic composition and one for
ontology-based semantic interpretation.
The partial ordering among nonterminals allows
the ordering of the grammar rules, and thus facili-
tates the bottom-up induction of these grammars.
The semantic molecule is a syntactic-semantic
representation of natural language strings
where (head) encodes the information required
for semantic composition, and (body) is the ac-
tual semantic representation of the string. Figure 1
shows examples of semantic molecules for an ad-
jective, a noun and a noun phrase. The represen-
tations associated with the lexical items are called
elementary semantic molecules (I), while the rep-
resentations built by the combination of others are
called derived semantic molecules (II). The head
of the semantic molecule is a flat feature structure,
having at least two attributes encoding the syntac-
tic category of the associated string, cat, and the
head of the string, head. The set of attributes is
finite and known a priori for each syntactic cate-
gory. The body of the semantic molecule is a flat,
ontology-based semantic representation. It is a log-
ical form, built as a conjunction of atomic predi-
cates , where vari-
ables are either concept or slot identifiers in an on-
tology. For example, the adjective major is repre-
sented as , which
says that the meaning of an adjective is a concept
( ), which is a value of a property
of another concept ( ) in the ontology.
The grammar nonterminals are augmented with
pairs of strings and their semantic molecules. These
pairs are called syntagmas, and are denoted by
. There are two types of con-
straints at the grammar rule level — one for semantic
composition (defines how the meaning of a natural
language expression is composed from the meaning
</bodyText>
<figureCaption confidence="0.998002666666667">
Figure 1: Examples of two elementary semantic
molecules (I), a derived semantic molecule (II) ob-
tained by combining them, and a constraint grammar
</figureCaption>
<bodyText confidence="0.996266875">
rule together with the constraints ,
.
of its parts) and one for ontology-based semantic in-
terpretation. An example of a LWFG rule is given
in Figure 1(III). The composition constraints
applied to the heads of the semantic molecules, form
a system of equations that is a simplified version of
“path equations” (Shieber et al., 1983), because the
heads are flat feature structures. These constraints
are learned together with the grammar rules. The
ontology-based constraints represent the validation
on the ontology, and are applied to the body of the
semantic molecule associated with the left-hand side
nonterminal. They are not learned. Currently,
is a predicate which can succeed or fail. When it
succeeds, it instantiates the variables of the semantic
representation with concepts/slots in the ontology.
For example, given the phrase major damage,
succeeds and returns ( =MAJOR, =DAMAGE,
=DEGREE), while given the phrase major birth it
fails. We leave the discussion of the ontology con-
straints for a future paper, since it is not needed for
the main result of this paper.
We give below the formal definition of Lexical-
</bodyText>
<figure confidence="0.925810333333333">
I. Elementary Semantic Molecules
(major/adj)= cat adj
head
mod
II. Derived Semantic Molecule
(major damage)= cat n
III. Constraint Grammar Rule
(damage/noun)= cat noun
nr sg
head
returns =MAJOR, =DAMAGE, =DEGREE from ontology
nr sg
</figure>
<equation confidence="0.9068976">
head X
.isa = major, .Y=
.isa = major, X.Y= , X.isa=damage
.isa = damage
(III)
</equation>
<page confidence="0.986846">
833
</page>
<bodyText confidence="0.955005666666667">
is a finite set of elementary semantic
molecules corresponding to the set of terminal
symbols.
</bodyText>
<listItem confidence="0.9970302">
3. is a finite set of nonterminal symbols.
4. is a partial ordering relation among the non-
terminals.
5. is a set of constraint rules. A
constraint rule is written
</listItem>
<equation confidence="0.8210265">
, i.e., is a preterminal), and
.
</equation>
<bodyText confidence="0.963537636363636">
In LWFGs all syntagmas , derived
from a nonterminal have the same category of
their semantic molecules .3
The language of a grammar is the set of all
syntagmas generated from the start symbol , i.e.,
.
The set of all syntagmas generated by a grammar
is
. Given a LWFG we call a set
a sublanguage of . Extending the notation,
given a LWFG , the set of syntagmas generated by
</bodyText>
<figure confidence="0.559843333333333">
a rule is
ized Well-Founded Grammars, except that we do not
define formally the constraints due to lack of space
(see (Muresan, 2006) for details).
Definition 1. A Lexicalized Well-Founded Gram-
mar (LWFG) is a 6-tuple,
, where:
1. is a finite set of terminal symbols.
, where
</figure>
<bodyText confidence="0.95444664">
such that
.
For the rules whose left-hand side are
preterminals, , we use the notation
. There are three types of rules:
ordered non-recursive, ordered recursive,
and non-ordered rules. A grammar rule
, is an
ordered rule, if for all , we have .
In LWFGs, each nonterminal symbol is a left-
hand side in at least one ordered non-recursive
rule and the empty string cannot be derived
from any nonterminal symbol.
6. is the start nonterminal symbol, and
(we use the same notation
for the reflexive, transitive closure of ).
The relation is a partial ordering only among
nonterminals, and it should not be confused with
information ordering derived from the flat feature
structures. This relation makes the set of nontermi-
nals well-founded, which allows the ordering of the
grammar rules, as well as the ordering of the syntag-
mas generated by LWFGs.
Definition 2. Given a LWFG, ,the ground
syntagma derivation relation,
</bodyText>
<footnote confidence="0.7155425">
2The ground derivation (“reduction” in (Wintner, 1999)) can
be viewed as the bottom-up counterpart of the usual derivation.
</footnote>
<bodyText confidence="0.9842899">
,
where denotes the ground deriva-
tion obtained using the rule in
the last derivation step (we have bottom-up deriva-
tion). We will use the short notation , where
is a grammar rule.
Given a LWFG and a sublanguage (not nec-
essarily of ) we denote by ,
the set of syntagmas generated by reduced to the
sublanguage . Given a grammar rule ,
we call the set of syntagmas
generated by reduced to the sublanguage .
As we have previously mentioned, the partial or-
dering among grammar nonterminals allows the or-
dering of the syntagmas generated by the grammar,
which allows us to define the representative exam-
ples of a LWFG.
Representative Examples. Informally, the repre-
sentative examples of a LWFG, , are the sim-
plest syntagmas ground-derived by the grammar ,
i.e., for each grammar rule there exist a syntagma
which is ground-derived from it in the minimum
number of steps. Thus, the size of the representa-
tive example set is equal with the size of the set of
grammar rules, .
This set of representative examples is used by
the grammar learning model to generate the candi-
date hypotheses. For generalization, a larger sublan-
guage is used, which we call representa-
tive sublanguage.
</bodyText>
<footnote confidence="0.8426455">
3This property is used for determining the lhs nonterminal
of the learned rule.
</footnote>
<bodyText confidence="0.8634124">
fined as:
(if
, and is the semantic compo-
sition operator. For brevity, we denote a rule
by , where
</bodyText>
<footnote confidence="0.580865">
,2 is de-
</footnote>
<page confidence="0.9915">
834
</page>
<bodyText confidence="0.933053714285714">
= the, noise, loud, clear
= noise, loud noise, the noise
= clear loud noise, the loud noise
=
=clear loud noise
= the loud noise
=
</bodyText>
<figure confidence="0.957216">
Rule specialization steps
Rule generalization steps
</figure>
<figureCaption confidence="0.999165">
Figure 2: Example of a simple grammar lattice. All grammars generate , and only generates ( is
</figureCaption>
<bodyText confidence="0.682496">
a common lexicon for all the grammars)
</bodyText>
<sectionHeader confidence="0.981916" genericHeader="method">
3 A Grammar Lattice as a Search Space
for Grammar Induction
</sectionHeader>
<bodyText confidence="0.977151385964912">
In this section we present a class of Lexicalized
Well-Founded Grammars that form a complete lat-
tice. This grammar lattice is the search space for
our grammar induction model, which we present in
Section 4. An example of a grammar lattice is given
in Figure 2, where for simplicity, we only show the
context-free backbone of the grammar rules, and
only strings, not syntagmas. Intuitively, the gram-
mars found lower in the lattice are more specialized
than the ones higher in the lattice. For learning,
is used to generate the most specific hypotheses
(grammar rules), and thus all the grammars should
be able to generate those examples. The sublan-
guage is used during generalization, thus only
the most general grammar, , is able to generate
the entire sublanguage. In other words, the gener-
alization process is bounded by , that is why our
model is called Grammar Approximation by Repre-
sentative Sublanguage.
There are two properties that LWFGs should have
in order to form a complete lattice: 1) they should be
unambiguous, and 2) they should preserve the pars-
ing of the representative example set, . We define
these two properties in turn.
Definition 3. A LWFG, , is unambiguous w.r.t. a
sublanguage if there is one
and only one rule that derives .
Since the unambiguity is relative to a set of
syntagmas (pairs of strings and their semantic
molecules) and not to a set of natural language
strings, the requirement is compatible with model-
ing natural language. For example, an ambiguous
string such as John saw the man with the telescope
corresponds to two unambiguous syntagmas.
In order to define the second property, we need
to define the rule specialization step and the rule
generalization step of unambiguous LWFGs, such
that they are -parsing-preserving and are the in-
verse of each other. The property of -parsing-
preserving means that both the initial and the spe-
cialized/generalized rules ground-derive the same
syntagma, .
Definition 4. The rule specialization step:
The rule generalization step :
is -parsing-preserving, if there exists
and and . We write
.
Since is a representative example, it is derived
in the minimum number of derivation steps, and thus
the rule is always an ordered, non-recursive rule.
is -parsing-preserving, if there exists
and and , where =
, and
. We write .
=
,
=
</bodyText>
<page confidence="0.956966">
835
</page>
<bodyText confidence="0.976411951219512">
The goal of the rule specialization step is to ob-
tain a new target grammar from by modify-
ing a rule of . Similarly, the goal of the rule gen-
eralization step is to obtain a new target grammar
from by modifying a rule of . They are
not to be taken as the derivation/reduction concepts
in parsing. The specialization/generalization steps
are the inverse of each other. From both the spe-
cialization and the generalization step we have that:
.
In Figure 2, the specialization step is
-parsing-preserving, because the rule ground-
derives the syntagma loud noise. If instead we
(
), it would not be -parsing-
preserving since the syntagma loud noise could no
longer be ground-derived from the rule (which
requires two adjectives).
Definition 5. A grammar is one-step special-
ized from a grammar , , if
and , s.t. , and
iff . A grammar is specialized from
a grammar , , if it is obtained from in
-specialization steps: , where is fi-
nite. We extend the notation so that we have
Similarly, we define the concept of a grammar
generalized from a grammar ,using the
rule generalization step.
In Figure 2, the grammar is one-step special-
ized from the grammar , i.e., , since
preserve the parsing of the representative exam-
ples . A grammar which contains the rule
instead of is not specialized
from the grammar since it does not preserve the
parsing of the representative example set, . Such
grammars will not be in the lattice.
In order to define the grammar lattice we need to
introduce one more concept: a normalized grammar
w.r.t. a sublanguage.
Definition 6. A LWFG is called normalized w.r.t.
a sublanguage (not necessarily of G), if none of
the grammar rules of can be further gener-
alized to a rule by the rule generalization step
such that .
In Figure 2, grammar is normalized w.r.t. ,
while , and are not.
We now define a grammar lattice which will be
the search space for our grammar learning model.
We first define the set of lattice elements .
Let be a LWFG, normalized and unambiguous
w.r.t. a sublanguage which includes
the representative example set of the grammar
( ). Let be the set of
grammars specialized from . We call the top
element of , and the bottom element of , if
. The bottom element,
, is the grammar specialized from , such that the
right-hand side of all grammar rules contains only
preterminals. We have and
.
The system is a complete gram-
mar lattice (see (Muresan, 2006) for the full formal
proof). In Figure 2 the grammars ,, , pre-
serve the parsing of the representative examples .
We have that , ,,
and . Due to space limitation we do not define
here the least upper bound ( ), and the greatest
lower bound ( ), operators, but in this example
=
,
In oder to give a learnability theorem we need to
show that and elements of the lattice can be
built. First, an assumption in our learning model is
that the rules corresponding to the grammar preter-
minals are given. Thus, for a given set of representa-
tive examples, , we can build the grammar us-
ing a bottom-up robust parser, which returns partial
analyses (chunks) if it cannot return a full parse. In
order to soundly build the element of the grammar
lattice from the grammar through generalization,
we must give the definition of a grammar confor-
mal w.r.t. .
</bodyText>
<figure confidence="0.931710133333333">
Note that we have that for
, if
would have a specialization step
.
.
The grammars in have the following two prop-
erties (Muresan, 2006):
For two grammars and , we have that
is specialized from if and only if is gener-
alized from , with .
All grammars in preserve the parsing of the
representative example set .
then
=
.
</figure>
<page confidence="0.941054">
836
</page>
<figureCaption confidence="0.472446">
Definition 7. A LWFG is conformal w.r.t. a sub-
</figureCaption>
<bodyText confidence="0.960125366666667">
language iff is normalized and un-
ambiguous w.r.t. and the rule specialization step
guarantees that for all grammars
specialized from .
The only rule generalization steps allowed in the
grammar induction process are those which guaran-
tee the same relation , which en-
sures that all the generalized grammars belong to the
grammar lattice.
In Figure 2, is conformal to the given sub-
language . If the sublanguage were
clear loud noise then would not be con-
formal to since and thus
the specialization step would not satisfy the relation
. Dur-
ing learning, the generalization step cannot general-
ize from grammar to .
Theorem 1 (Learnability Theorem). If is the
set of representative examples associated with a
LWFG conformal w.r.t. a sublanguage ,
then can always be learned from and as
the grammar lattice top element (
The proof is given in (Muresan, 2006).
If the hypothesis of Theorem 1 holds, then any
grammar induction algorithm that uses the complete
lattice search space can converge to the lattice top el-
ement, using different search strategies. In the next
section we present our new model of grammar learn-
ing which relies on the property of the search space
as grammar lattice.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="method">
4 Grammar Induction Model
</sectionHeader>
<bodyText confidence="0.997970875">
Based on the theoretical foundation of the hypoth-
esis search space for LWFG learning given in the
previous section, we define our grammar induction
model. First, we present the LWFG induction as an
Inductive Logic Programming problem. Second, we
present our new relational learning model for LWFG
induction, called Grammar Approximation by Rep-
resentative Sublanguage (GARS).
</bodyText>
<subsectionHeader confidence="0.764371">
4.1 Grammar Induction Problem in
ILP-setting
</subsectionHeader>
<bodyText confidence="0.998962261904762">
Inductive Logic Programming (ILP) is a class of re-
lational learning methods concerned with inducing
first-order Horn clauses from examples and back-
ground knowledge. Kietz and Dˇzeroski (1994) have
formally defined the ILP-learning problem as the tu-
ple , where is the provability re-
lation (also called the generalization model), is
the language of the background knowledge, is
the language of the (positive and negative) exam-
ples, and is the hypothesis language. The gen-
eral ILP-learning problem is undecidable. Possible
choices to restrict the ILP-problem are: the provabil-
ity relation, , the background knowledge and the
hypothesis language. Research in ILP has presented
positive results only for very limited subclasses of
first-order logic (Kietz and Dˇzeroski, 1994; Cohen,
1995), which are not appropriate to model natural
language grammars.
Our grammar induction problem can be formu-
lated as an ILP-learning problem
as follows:
The provability relation, , is given by robust
parsing, and we denote it by . We use the
“parsing as deduction” technique (Shieber et
al., 1995). For all syntagmas we can say in
polynomial time whether they belong or not to
the grammar language. Thus, using the as
generalization model, our grammar induction
problem is decidable.
The language of background knowledge, ,
is the set of LWFG rules that are already
learned together with elementary syntagmas
(i.e., corresponding to the lexicon), which are
ground atoms (the variables are made con-
stants).
The language of examples, are syntagmas
of the representative sublanguage, which are
ground atoms. We only have positive examples.
The hypothesis language, , is a LWFG lat-
tice whose top element is a conformal gram-
mar, and which preserve the parsing of repre-
sentative examples.
</bodyText>
<sectionHeader confidence="0.6097125" genericHeader="method">
4.2 Grammar Approximation by
Representative Sublanguage Model
</sectionHeader>
<bodyText confidence="0.921790333333333">
We have formulated the grammar induction problem
in the ILP-setting. The theoretical learning model,
).
</bodyText>
<page confidence="0.9902">
837
</page>
<bodyText confidence="0.986458442622951">
called Grammar Approximation by Representative
Sublanguage (GARS), can be formulated as follows:
Given:
a representative example set , lexically con-
sistent (i.e., it allows the construction of the
grammar lattice element)
a finite sublanguage , conformal and thus
unambiguous, which includes the representa-
tive example set, . We called this
sublanguage, the representative sublanguage
Learn a grammar , using the above ILP-learning
setting, such that is unique and .
The hypothesis space is a complete grammar lat-
tice, and thus the uniqueness property of the learned
grammar is guaranteed by the learnability theorem
(i.e., the learned grammar is the lattice top ele-
ment). This learnability result extends significantly
the class of problems learnable by ILP methods.
The GARS model uses two polynomial algo-
rithms for LWFG learning. In the first algorithm,
the learner is presented with an ordered set of rep-
resentative examples (syntagmas), i.e., the examples
are ordered from the simplest to the most complex.
The reader should remember that for a LWFG ,
there exists a partial ordering among the grammar
nonterminals, which allows a total ordering of the
representative examples of the grammar . Thus, in
this algorithm, the learner has access to the ordered
representative syntagmas when learning the gram-
mar. However, in practice it might be difficult to
provide the learner with the “true” order of exam-
ples, especially when modeling complex language
phenomena. The second algorithm is an iterative al-
gorithm that learns starting from a random order of
the representative example set. Due to the property
of the search space, both algorithms converge to the
same target grammar.
Using ILP and theory revision terminology
(Greiner, 1999), we can establish the following anal-
ogy: syntagmas (examples) are “labeled queries”,
the LWFG lattice is the “space of theories”, and a
LWFG in the lattice is “a theory.” The first algorithm
learns from an “empty theory”, while the second al-
gorithm is an instance of “theory revision”, since the
grammar (“theory”) learned during the first iteration,
is then revised, by deleting and adding rules.
Both of these algorithms are cover set algorithms.
In the first step the most specific grammar rule
is generated from the current representative exam-
ple. The category name annotated in the represen-
tative example gives the name of the lhs nontermi-
nal (predicate invention in ILP terminology), while
the robust parser returns the minimum number of
chunks that cover the representative example. In the
second step this most specific rule is generalized us-
ing as performance criterion the number of the ex-
amples in that can be parsed using the candidate
grammar rule (hypothesis) together with the previ-
ous learned rules. For the full details for these two
algorithms, and the proof of their polynomial effi-
ciency, we refer the reader to (Muresan, 2006).
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999609375">
A practical advantage of our GARS model is that
instead of writing syntactic-semantic grammars by
hand (both rules and constraints), we construct just
a small annotated treebank - utterances and their se-
mantic molecules. If the grammar needs to be re-
fined, or enhanced, we only refine, or enhance the
representative examples/sublanguage, and not the
grammar rules and constraints, which would be a
more difficult task.
We have built a framework to test whether our
GARS model can learn diverse and complex lin-
guistic phenomena. We have primarily analyzed a
set of definitional-type sentences in the medical do-
main. The phenomena covered by our learned gram-
mar includes complex noun phrases (including noun
compounds, nominalization), prepositional phrases,
relative clauses and reduced relative clauses, finite
and non-finite verbal constructions (including, tense,
aspect, negation, and subject-verb agreement), cop-
ula to be, and raising and control constructions. We
also learned rules for wh-questions (including long-
distance dependencies). In Figure 3 we show the
ontology-level representation of a definition-type
sentence obtained using our learned grammar. It
includes the treatment of reduced relative clauses,
raising construction (tends to persist, where virus
is not the argument of tends but the argument of
persist), and noun compounds. The learned gram-
mar together with a semantic interpreter targeted
to terminological knowledge has been used in an
acquisition-query experiment, where the answers
are at the concept level (the querying is a graph
</bodyText>
<page confidence="0.993201">
838
</page>
<bodyText confidence="0.6934335">
Hepatitis B is an acute viral hepatitis caused by a virus that
tends to persist in the blood serum.
</bodyText>
<figure confidence="0.775687">
of
#blood
</figure>
<figureCaption confidence="0.978936333333333">
Figure 3: A definition-type sentence and its
ontology-based representation obtained using our
learned LWFG
</figureCaption>
<bodyText confidence="0.999933894736842">
matching problem where the “wh-word” matches
the answer concept). A detailed discussion of the
linguistic phenomena covered by our learned gram-
mar using the GARS model, as well as the use of this
grammar for terminological knowledge acquisition,
is given in (Muresan, 2006).
To learn the grammar used in these experiments
we annotated 151 representative examples and 448
examples used as a representative sublanguage for
generalization. Annotating these examples requires
knowledge about categories and their attributes. We
used 31 categories (nonterminals) and 37 attributes
(e.g., category, head, number, person). In this
experiment, we chose the representative examples
guided by the type of phenomena we wanted to mod-
eled and which occurred in our corpus. We also
used 13 lexical categories (i.e., parts of speech). The
learned grammar contains 151 rules and 151 con-
straints.
</bodyText>
<sectionHeader confidence="0.999657" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998895">
We have presented Lexicalized Well-Founded
Grammars, a type of constraint-based grammars
for natural language specifically designed to en-
able learning from representative examples anno-
tated with semantics. We have presented a new
grammar learning model and showed that the search
space is a complete grammar lattice that guarantees
the uniqueness of the learned grammar. Starting
from these fundamental theoretical results, there are
several directions into which to take this research.
A first obvious extension is to have probabilistic-
LWFGs. For example, the ontology constraints
might not be “hard” constraints, but “soft” ones (be-
cause language expressions are more or less likely to
be used in a certain context). Investigating where to
add probabilities (ontology, grammar rules, or both)
is part of our planned future work. Another future
extension of this work is to investigate how to auto-
matically select the representative examples from an
existing treebank.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999423090909091">
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of COLING-04.
William Cohen. 1995. Pac-learning recursive logic pro-
grams: Negative results. Journal of Artificial Intelli-
gence Research, 2:541–573.
Rusins Freivalds, Efim B. Kinber, and Rolf Wieha-
gen. 1993. On the power of inductive inference
from good examples. Theoretical Computer Science,
110(1):131–144.
R. Ge and R.J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Pro-
ceedings of CoNLL-2005.
Russell Greiner. 1999. The complexity of theory revi-
sion. Artificial Intelligence Journal, 107(2):175–217.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings ofACL’06.
J¨org-Uwe Kietz and Saˇso Dˇzeroski. 1994. Inductive
logic programming and learnability. ACM SIGART
Bulletin., 5(1):22–32.
Smaranda Muresan. 2006. Learning Constraint-based
Grammars from Representative Examples: Theory
and Applications. Ph.D. thesis, Columbia University.
http://www1.cs.columbia.edu/ smara/muresan thesis.pdf.
Fernando C. Pereira and David H.D Warren. 1980. Defi-
nite Clause Grammars for language analysis. Artificial
Intelligence, 13:231–278.
Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane
Robinson, and Mabry Tyson. 1983. The formalism
and implementation of PATR-II. In Barbara J. Grosz
and Mark Stickel, editors, Research on Interactive Ac-
quisition and Use ofKnowledge, pages 39–79. SRI In-
ternational, Menlo Park, CA, November.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1-2):3–
36.
Shuly Wintner. 1999. Compositional semantics for lin-
guistic formalisms. In Proceedings of the ACL’99.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI-05.
</reference>
<figure confidence="0.998780647058824">
prop
#cause
#tend
#hepatitis
th
sub
#’HepatitisB’
duration
#acute
Iind_of
#viral
ag
#virus
#persist
location
#serum
th
</figure>
<page confidence="0.971262">
839
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977048">
<title confidence="0.999887">Grammar Approximation by Representative Sublanguage: A New Model for Language Learning</title>
<author confidence="0.997237">Smaranda Muresan</author>
<affiliation confidence="0.9999045">Institute for Advanced Computer Studies University of Maryland</affiliation>
<address confidence="0.99992">College Park, MD 20742, USA</address>
<email confidence="0.999813">smara@umiacs.umd.edu</email>
<author confidence="0.998892">Owen Rambow</author>
<affiliation confidence="0.9996535">Center for Computational Learning Systems Columbia University</affiliation>
<address confidence="0.999868">New York, NY 10027, USA</address>
<email confidence="0.999863">rambow@cs.columbia.edu</email>
<abstract confidence="0.9977984">We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Wide-coverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="1019" citStr="Bos et al., 2004" startWordPosition="141" endWordPosition="144">ntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.&apos; While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto&apos;This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank </context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Wide-coverage semantic representations from a CCG parser. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Pac-learning recursive logic programs: Negative results.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--541</pages>
<contexts>
<context position="19387" citStr="Cohen, 1995" startWordPosition="3247" endWordPosition="3248">nd Dˇzeroski (1994) have formally defined the ILP-learning problem as the tuple , where is the provability relation (also called the generalization model), is the language of the background knowledge, is the language of the (positive and negative) examples, and is the hypothesis language. The general ILP-learning problem is undecidable. Possible choices to restrict the ILP-problem are: the provability relation, , the background knowledge and the hypothesis language. Research in ILP has presented positive results only for very limited subclasses of first-order logic (Kietz and Dˇzeroski, 1994; Cohen, 1995), which are not appropriate to model natural language grammars. Our grammar induction problem can be formulated as an ILP-learning problem as follows: The provability relation, , is given by robust parsing, and we denote it by . We use the “parsing as deduction” technique (Shieber et al., 1995). For all syntagmas we can say in polynomial time whether they belong or not to the grammar language. Thus, using the as generalization model, our grammar induction problem is decidable. The language of background knowledge, , is the set of LWFG rules that are already learned together with elementary syn</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William Cohen. 1995. Pac-learning recursive logic programs: Negative results. Journal of Artificial Intelligence Research, 2:541–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rusins Freivalds</author>
<author>Efim B Kinber</author>
<author>Rolf Wiehagen</author>
</authors>
<title>On the power of inductive inference from good examples.</title>
<date>1993</date>
<journal>Theoretical Computer Science,</journal>
<volume>110</volume>
<issue>1</issue>
<contexts>
<context position="1357" citStr="Freivalds et al., 1993" startWordPosition="193" endWordPosition="196">siderable interest in learning computational grammars.&apos; While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto&apos;This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her input, and several anonymous reviewers for very useful feedback on earlier drafts of this paper. 832 types” significantly improves over a fully unsupervised PCFG induction model (their prototypes were formed by sequences of POS tags; for example</context>
</contexts>
<marker>Freivalds, Kinber, Wiehagen, 1993</marker>
<rawString>Rusins Freivalds, Efim B. Kinber, and Rolf Wiehagen. 1993. On the power of inductive inference from good examples. Theoretical Computer Science, 110(1):131–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005.</booktitle>
<contexts>
<context position="1072" citStr="Ge and Mooney, 2005" startWordPosition="149" endWordPosition="152">atural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.&apos; While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto&apos;This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course </context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R.J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of CoNLL-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell Greiner</author>
</authors>
<title>The complexity of theory revision.</title>
<date>1999</date>
<journal>Artificial Intelligence Journal,</journal>
<volume>107</volume>
<issue>2</issue>
<contexts>
<context position="22280" citStr="Greiner, 1999" startWordPosition="3702" endWordPosition="3703">llows a total ordering of the representative examples of the grammar . Thus, in this algorithm, the learner has access to the ordered representative syntagmas when learning the grammar. However, in practice it might be difficult to provide the learner with the “true” order of examples, especially when modeling complex language phenomena. The second algorithm is an iterative algorithm that learns starting from a random order of the representative example set. Due to the property of the search space, both algorithms converge to the same target grammar. Using ILP and theory revision terminology (Greiner, 1999), we can establish the following analogy: syntagmas (examples) are “labeled queries”, the LWFG lattice is the “space of theories”, and a LWFG in the lattice is “a theory.” The first algorithm learns from an “empty theory”, while the second algorithm is an instance of “theory revision”, since the grammar (“theory”) learned during the first iteration, is then revised, by deleting and adding rules. Both of these algorithms are cover set algorithms. In the first step the most specific grammar rule is generated from the current representative example. The category name annotated in the representati</context>
</contexts>
<marker>Greiner, 1999</marker>
<rawString>Russell Greiner. 1999. The complexity of theory revision. Artificial Intelligence Journal, 107(2):175–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL’06.</booktitle>
<contexts>
<context position="1384" citStr="Haghighi and Klein (2006)" startWordPosition="197" endWordPosition="200">rning computational grammars.&apos; While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto&apos;This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her input, and several anonymous reviewers for very useful feedback on earlier drafts of this paper. 832 types” significantly improves over a fully unsupervised PCFG induction model (their prototypes were formed by sequences of POS tags; for example, prototypical NPs were DT </context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven grammar induction. In Proceedings ofACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org-Uwe Kietz</author>
<author>Saˇso Dˇzeroski</author>
</authors>
<title>Inductive logic programming and learnability.</title>
<date>1994</date>
<journal>ACM SIGART Bulletin.,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Kietz, Dˇzeroski, 1994</marker>
<rawString>J¨org-Uwe Kietz and Saˇso Dˇzeroski. 1994. Inductive logic programming and learnability. ACM SIGART Bulletin., 5(1):22–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smaranda Muresan</author>
</authors>
<title>Learning Constraint-based Grammars from Representative Examples: Theory and Applications.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<note>http://www1.cs.columbia.edu/ smara/muresan thesis.pdf.</note>
<contexts>
<context position="8150" citStr="Muresan, 2006" startWordPosition="1300" endWordPosition="1301">nterminals. 5. is a set of constraint rules. A constraint rule is written , i.e., is a preterminal), and . In LWFGs all syntagmas , derived from a nonterminal have the same category of their semantic molecules .3 The language of a grammar is the set of all syntagmas generated from the start symbol , i.e., . The set of all syntagmas generated by a grammar is . Given a LWFG we call a set a sublanguage of . Extending the notation, given a LWFG , the set of syntagmas generated by a rule is ized Well-Founded Grammars, except that we do not define formally the constraints due to lack of space (see (Muresan, 2006) for details). Definition 1. A Lexicalized Well-Founded Grammar (LWFG) is a 6-tuple, , where: 1. is a finite set of terminal symbols. , where such that . For the rules whose left-hand side are preterminals, , we use the notation . There are three types of rules: ordered non-recursive, ordered recursive, and non-ordered rules. A grammar rule , is an ordered rule, if for all , we have . In LWFGs, each nonterminal symbol is a lefthand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol. 6. is the start nonterminal symbol, and (we use </context>
<context position="15766" citStr="Muresan, 2006" startWordPosition="2631" endWordPosition="2632">t. , while , and are not. We now define a grammar lattice which will be the search space for our grammar learning model. We first define the set of lattice elements . Let be a LWFG, normalized and unambiguous w.r.t. a sublanguage which includes the representative example set of the grammar ( ). Let be the set of grammars specialized from . We call the top element of , and the bottom element of , if . The bottom element, , is the grammar specialized from , such that the right-hand side of all grammar rules contains only preterminals. We have and . The system is a complete grammar lattice (see (Muresan, 2006) for the full formal proof). In Figure 2 the grammars ,, , preserve the parsing of the representative examples . We have that , ,, and . Due to space limitation we do not define here the least upper bound ( ), and the greatest lower bound ( ), operators, but in this example = , In oder to give a learnability theorem we need to show that and elements of the lattice can be built. First, an assumption in our learning model is that the rules corresponding to the grammar preterminals are given. Thus, for a given set of representative examples, , we can build the grammar using a bottom-up robust par</context>
<context position="17826" citStr="Muresan, 2006" startWordPosition="3006" endWordPosition="3007">the same relation , which ensures that all the generalized grammars belong to the grammar lattice. In Figure 2, is conformal to the given sublanguage . If the sublanguage were clear loud noise then would not be conformal to since and thus the specialization step would not satisfy the relation . During learning, the generalization step cannot generalize from grammar to . Theorem 1 (Learnability Theorem). If is the set of representative examples associated with a LWFG conformal w.r.t. a sublanguage , then can always be learned from and as the grammar lattice top element ( The proof is given in (Muresan, 2006). If the hypothesis of Theorem 1 holds, then any grammar induction algorithm that uses the complete lattice search space can converge to the lattice top element, using different search strategies. In the next section we present our new model of grammar learning which relies on the property of the search space as grammar lattice. 4 Grammar Induction Model Based on the theoretical foundation of the hypothesis search space for LWFG learning given in the previous section, we define our grammar induction model. First, we present the LWFG induction as an Inductive Logic Programming problem. Second, </context>
<context position="23428" citStr="Muresan, 2006" startWordPosition="3891" endWordPosition="3892">sentative example. The category name annotated in the representative example gives the name of the lhs nonterminal (predicate invention in ILP terminology), while the robust parser returns the minimum number of chunks that cover the representative example. In the second step this most specific rule is generalized using as performance criterion the number of the examples in that can be parsed using the candidate grammar rule (hypothesis) together with the previous learned rules. For the full details for these two algorithms, and the proof of their polynomial efficiency, we refer the reader to (Muresan, 2006). 5 Discussion A practical advantage of our GARS model is that instead of writing syntactic-semantic grammars by hand (both rules and constraints), we construct just a small annotated treebank - utterances and their semantic molecules. If the grammar needs to be refined, or enhanced, we only refine, or enhance the representative examples/sublanguage, and not the grammar rules and constraints, which would be a more difficult task. We have built a framework to test whether our GARS model can learn diverse and complex linguistic phenomena. We have primarily analyzed a set of definitional-type sen</context>
<context position="25499" citStr="Muresan, 2006" startWordPosition="4205" endWordPosition="4206">e has been used in an acquisition-query experiment, where the answers are at the concept level (the querying is a graph 838 Hepatitis B is an acute viral hepatitis caused by a virus that tends to persist in the blood serum. of #blood Figure 3: A definition-type sentence and its ontology-based representation obtained using our learned LWFG matching problem where the “wh-word” matches the answer concept). A detailed discussion of the linguistic phenomena covered by our learned grammar using the GARS model, as well as the use of this grammar for terminological knowledge acquisition, is given in (Muresan, 2006). To learn the grammar used in these experiments we annotated 151 representative examples and 448 examples used as a representative sublanguage for generalization. Annotating these examples requires knowledge about categories and their attributes. We used 31 categories (nonterminals) and 37 attributes (e.g., category, head, number, person). In this experiment, we chose the representative examples guided by the type of phenomena we wanted to modeled and which occurred in our corpus. We also used 13 lexical categories (i.e., parts of speech). The learned grammar contains 151 rules and 151 constr</context>
</contexts>
<marker>Muresan, 2006</marker>
<rawString>Smaranda Muresan. 2006. Learning Constraint-based Grammars from Representative Examples: Theory and Applications. Ph.D. thesis, Columbia University. http://www1.cs.columbia.edu/ smara/muresan thesis.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite Clause Grammars for language analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="3735" citStr="Pereira and Warren, 1980" startWordPosition="564" endWordPosition="567">e a new computationally efficient model for grammar induction from pairs of utterances and their semantic representations, called Grammar Approximation by Representative Sublanguage (GARS). Section 5 discusses the practical use of our model and Section 6 states our conclusions and future work. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832–839, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Lexicalized Well-Founded Grammars Lexicalized Well-Founded Grammars (LWFGs) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the Context-Free Grammar backbone is extended by introducing a partial ordering relation among nonterminals (wellfounded) 2) each string is associated with a syntactic-semantic representation called semantic molecule; 3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation. The partial ordering among nonterminals allows the ordering of the grammar rules, and thus facilitates the bottom-up induction of these grammars. The semantic molecule is a syntactic-semantic representation of natural language strings where </context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando C. Pereira and David H.D Warren. 1980. Definite Clause Grammars for language analysis. Artificial Intelligence, 13:231–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Hans Uszkoreit</author>
<author>Fernando Pereira</author>
<author>Jane Robinson</author>
<author>Mabry Tyson</author>
</authors>
<title>The formalism and implementation of PATR-II.</title>
<date>1983</date>
<booktitle>Research on Interactive Acquisition and Use ofKnowledge,</booktitle>
<pages>39--79</pages>
<editor>In Barbara J. Grosz and Mark Stickel, editors,</editor>
<publisher>SRI International,</publisher>
<location>Menlo Park, CA,</location>
<contexts>
<context position="6245" citStr="Shieber et al., 1983" startWordPosition="974" endWordPosition="977"> the grammar rule level — one for semantic composition (defines how the meaning of a natural language expression is composed from the meaning Figure 1: Examples of two elementary semantic molecules (I), a derived semantic molecule (II) obtained by combining them, and a constraint grammar rule together with the constraints , . of its parts) and one for ontology-based semantic interpretation. An example of a LWFG rule is given in Figure 1(III). The composition constraints applied to the heads of the semantic molecules, form a system of equations that is a simplified version of “path equations” (Shieber et al., 1983), because the heads are flat feature structures. These constraints are learned together with the grammar rules. The ontology-based constraints represent the validation on the ontology, and are applied to the body of the semantic molecule associated with the left-hand side nonterminal. They are not learned. Currently, is a predicate which can succeed or fail. When it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology. For example, given the phrase major damage, succeeds and returns ( =MAJOR, =DAMAGE, =DEGREE), while given the phrase major </context>
</contexts>
<marker>Shieber, Uszkoreit, Pereira, Robinson, Tyson, 1983</marker>
<rawString>Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson. 1983. The formalism and implementation of PATR-II. In Barbara J. Grosz and Mark Stickel, editors, Research on Interactive Acquisition and Use ofKnowledge, pages 39–79. SRI International, Menlo Park, CA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="19682" citStr="Shieber et al., 1995" startWordPosition="3294" endWordPosition="3297">ge. The general ILP-learning problem is undecidable. Possible choices to restrict the ILP-problem are: the provability relation, , the background knowledge and the hypothesis language. Research in ILP has presented positive results only for very limited subclasses of first-order logic (Kietz and Dˇzeroski, 1994; Cohen, 1995), which are not appropriate to model natural language grammars. Our grammar induction problem can be formulated as an ILP-learning problem as follows: The provability relation, , is given by robust parsing, and we denote it by . We use the “parsing as deduction” technique (Shieber et al., 1995). For all syntagmas we can say in polynomial time whether they belong or not to the grammar language. Thus, using the as generalization model, our grammar induction problem is decidable. The language of background knowledge, , is the set of LWFG rules that are already learned together with elementary syntagmas (i.e., corresponding to the lexicon), which are ground atoms (the variables are made constants). The language of examples, are syntagmas of the representative sublanguage, which are ground atoms. We only have positive examples. The hypothesis language, , is a LWFG lattice whose top eleme</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
</authors>
<title>Compositional semantics for linguistic formalisms.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL’99.</booktitle>
<contexts>
<context position="9261" citStr="Wintner, 1999" startWordPosition="1487" endWordPosition="1488"> string cannot be derived from any nonterminal symbol. 6. is the start nonterminal symbol, and (we use the same notation for the reflexive, transitive closure of ). The relation is a partial ordering only among nonterminals, and it should not be confused with information ordering derived from the flat feature structures. This relation makes the set of nonterminals well-founded, which allows the ordering of the grammar rules, as well as the ordering of the syntagmas generated by LWFGs. Definition 2. Given a LWFG, ,the ground syntagma derivation relation, 2The ground derivation (“reduction” in (Wintner, 1999)) can be viewed as the bottom-up counterpart of the usual derivation. , where denotes the ground derivation obtained using the rule in the last derivation step (we have bottom-up derivation). We will use the short notation , where is a grammar rule. Given a LWFG and a sublanguage (not necessarily of ) we denote by , the set of syntagmas generated by reduced to the sublanguage . Given a grammar rule , we call the set of syntagmas generated by reduced to the sublanguage . As we have previously mentioned, the partial ordering among grammar nonterminals allows the ordering of the syntagmas generat</context>
</contexts>
<marker>Wintner, 1999</marker>
<rawString>Shuly Wintner. 1999. Compositional semantics for linguistic formalisms. In Proceedings of the ACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI-05.</booktitle>
<contexts>
<context position="1050" citStr="Zettlemoyer and Collins, 2005" startWordPosition="145" endWordPosition="148">rammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.&apos; While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto&apos;This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contribu</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI-05.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>