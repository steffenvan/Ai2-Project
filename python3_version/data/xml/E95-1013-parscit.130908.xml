<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.924272">
Literal Movement Grammars
</title>
<author confidence="0.375136">
Annius V. Groenink*
</author>
<affiliation confidence="0.214026">
CWI
</affiliation>
<address confidence="0.620137333333333">
Kruislaan 413
1098 SJ Amsterdam
The Netherlands
</address>
<email confidence="0.990947">
avg@cwi.n1
</email>
<sectionHeader confidence="0.995479" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998959666666667">
Literal movement grammars (LMGs) pro-
vide a general account of extraposition phe-
nomena through an attribute mechanism al-
lowing top-down displacement of syntacti-
cal information. LMGs provide a simple
and efficient treatment of complex linguistic
phenomena such as cross-Serial dependen-
cies in German and Dutch—separating the
treatment of natural language into a parsing
phase closely resembling traditional context-
free treatment, and a disambiguation phase
which can be carried out using matching, as
opposed to full unification employed in most
current grammar formalisms of linguistical
relevance.
</bodyText>
<sectionHeader confidence="0.995555" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994876404761905">
The motivation for the introduction of the literal move-
ment grammars presented in this paper is twofold. The
first motivation is to examine whether, and in which
ways, the use of unification is essential to automated
treatment of natural language. Unification is an ex-
pensive operation, and pinpointing its precise role in
NLP may give access to more efficient treatment of
language than in most (Prolog-based) scientific appli-
cations known today. The second motivation is the
desire to apply popular computer-science paradigms,
such as the theory of attribute grammars and modu-
lar equational specification, to problems in linguistics.
These formal specification techniques, far exceeding
the popular Prolog in declarativity, may give new in-
sight into the formal properties of natural language,
and facilitate prototyping for large language applica-
tions in the same way as they are currently being used to
facilitate prototyping of programming language tools.
For an extensive illustration of how formal specifi-
cation techniques can be made useful in the treatment
of natural language, see (Newton, 1993) which de-
scribes the abstract specification of several accounts of
phrase structure, features, movement, modularity and
This work is supported by SION grant 612-317-420
of the Netherlands Organization for Scientific Research
(NWO).
parametrization so as to abstract away from the exact
language being modelled. The specification language
(ASL) used by Newton is a very powerful formalism.
The class of specification formalisms we have in mind
includes less complex, equational techniques such as
ASF+SDF (Bergstra et al., 1989) (van Deursen, 1992)
which can be applied in practice by very efficient exe-
cution as a term rewriting system.
Literal movement grammars are a straightforward
extension of context-free grammars. The derivation
trees of an LMG analysis can be easily transformed
into trees belonging to a context-free backbone which
gives way to treatment by formal specification systems.
In order to obtain an efficient implementation, some
restrictions on the general form of the formalism are
necessary.
</bodyText>
<subsectionHeader confidence="0.9941495">
1.1 Structural Context Sensitivity in Natural
Language
</subsectionHeader>
<bodyText confidence="0.999608555555556">
Equational specification systems such as the
ASF+SDF system operate through sets of equations
over signatures that correspond to arbitrary forms
of context-free grammar. An attempt at an equa-
tional specification of a grammar based on context-
free phrase structure rules augmented with feature con-
straints may be to use the context-free backbone as a
signature, and then implement further analysis through
equations over this signature. This seems entirely ana-
loguous to the static semantics of a programming lan-
guage: the language itself is context-free, and the static
semantics are defined in terms of functions over the
constructs of the language.
In computer-science applications it is irrelevant
whether the evaluation of these functions is carried
out during the parsing phase (1-pass treatment), or
afterwards (2-pass treatment). This is not a trivial
property of computer languages: a computer language
with static semantics restrictions is a context-sensitive
sublanguage of a context-free language that is either
unambiguous or has the finite ambiguity property: for
any input sentence, there is only a finite number of
possible context-free analyses.
In section 1.3 we will show that due to phenom-
ena of extraposition or discontinuous constituency ex-
hibited by natural languages, a context-free backbone
for a sufficiently rich fragment of natural language no
</bodyText>
<page confidence="0.995473">
90
</page>
<bodyText confidence="0.999811153846154">
longer has the property of finite ambiguity. Hence an
initial stage of sentence processing cannot be based on
a purely context-free analysis.
The LMG formalism presented in this paper at-
tempts to eliminate infinite ambiguity by providing
an elementary, but adequate treatment of movement.
Experience in practice suggests that after relocating
displaced constituents, a further analysis based on fea-
ture unification no longer exploits unbounded struc-
tural embedding. Therefore it seems that after LMG-
analysis, there is no need for unification, and further
analysis can be carried out through functional match-
ing techniques.
</bodyText>
<subsectionHeader confidence="0.975325">
1.2 Aims
</subsectionHeader>
<bodyText confidence="0.997053181818182">
We aim to present a grammar formalism that
r&gt; is sufficiently powerful to model relevant frag-
ments of natural language, at least large enough
for simple applications such as an interface to a
database system over a limited domain.
t&gt; is sufficiently elementary to act as a front-end to
computer-scientific tools that operate on context-
free languages.
t&gt; has a (sufficiently large) subclass that allows ef-
ficient implementation through standard (Earley-
based) left-to-right parsing techniques.
</bodyText>
<subsectionHeader confidence="0.964391">
1.3 Requirements
</subsectionHeader>
<bodyText confidence="0.999071461538461">
Three forms of movement in Dutch will be a leading
thread throughout this paper. We will measure the
adequacy of a grammar formalism in terms of its ability
to give a unified account of these three phenomena.
Topicalization The (leftward) movement of the ob-
jects of the verb phrase, as in
[Which book]i did John forget to
return ei to the library?
Dutch sentence structure The surface order of
sentences in Dutch takes three different forms: the
finite verb appears inside the verb phrase in relative
clauses; before the verb phrase in declarative clauses,
and before the subject in questions:
</bodyText>
<listItem confidence="0.997492666666667">
(2) . . . dat Jan [vp Marie kuste ]
(3) Jan kustei [vp Marie ei]
(4) kustei Jan [vp Marie ei j?
</listItem>
<bodyText confidence="0.999159142857143">
We think of these three (surface) forms as merely being
different representations of the same (deep) structure,
and will take this deep structure to be the form (2) that
does not show movement.
Cross-serial dependencies In Dutch and German,
it is possible to construct sentences containing arbitrary
numbers of crossed dependencies, such as in
</bodyText>
<listItem confidence="0.8421395">
. dat Marie Jan i Fredi Annek
(5) that hoordei helpenj overtuigenk
</listItem>
<bodyText confidence="0.9586585">
heard help convince
(that Mary heard John help Fred convince Anne). Here
the i, j, k denote which noun is the first object of which
verb. The analysis we have in mind for this is
dat Marie Jani Fredi Annek
[vp hoorde ei helpen ei overtuigen ek.]
Note that this analysis (after relocation of the extra-
posed objects) is structurally equal to the correspond-
ing English VP. The accounts of Dutch in this paper
will consistently assign &amp;quot;deep structures&amp;quot; to sentences
of Dutch which correspond to the underlying structure
as it appears in English. Similar accounts can be given
for other languages—so as to get a uniform treatment
of a group of similar (European) languages such as
German, French and Italian.
If we combine the above three analyses, the final anal-
ysis of (3) will become
Jan kustei Mariej [vp ei e j]
Although this may look like an overcomplication, this
abundant use of movement is essential in any uniform
treatment of Dutch verb constructions. Hence it turns
out to occur in practice that a verb phrase has no lexical
expansion at all, when a sentence shows both object
and verb extraposition. Therefore, as conjectured in
the introduction, a 2-pass treatment of natural language
based on a context-free backbone will in general fail—
as there are infinitely many ways of building an empty
verb phrase from a number of empty constituents.
</bodyText>
<sectionHeader confidence="0.791327" genericHeader="introduction">
2 Definition and Examples
</sectionHeader>
<bodyText confidence="0.999964068965517">
There is evidence that suggests that the typical human
processing of movement is to first locate displaced in-
formation (the filler), and then find the logical location
(the trace), to substitute that information. It also seems
that by and large, displaced information appears earlier
than (or left of) its logical position, as in all examples
given in the previous section. The typical unification-
based approach to such movement is to structurally
analyse the displaced constituent, and use this anal-
ysed information in the treatment of the rest of the
sentence. This method is called gap-threading; see
(Alshawi, 1992).
If we bear in mind that a filler is usually found to
the left of the corresponding trace, it is worth taking
into consideration to develop a way of deferring treat-
ment of syntactical data. E.g. for example sentence 1
this means that upon finding the displaced constituent
which book, we will not evaluate that constituent, but
rather remember during the treatment of the remaining
part of the sentence, that this data is still to be fitted
into a logical place.
This is not a new idea. A number of non-
concatenative grammar formalisms has been put for-
ward, such as head-wrapping grammars (HG) (Pol-
lard, 1984), extraposition grammars (XG) (Pereira,
1981). and tree adjoining grammars (TAG) (Kroch
and Joshi, 1986). A discussion of these formalisms
as alternatives to the LMG formalism is given in sec-
tion 4.
</bodyText>
<page confidence="0.99375">
91
</page>
<bodyText confidence="0.999976791666667">
Lessons in parsing by hand in high school (e.g. in
English or Latin classes) informally illustrate the pur-
pose of literal movement grammars: as opposed to the
traditional linguistic point of view that there is only
one head which dominates a phrase, constituents of a
sentence have several key components. A verb phrase
for example not only has its finite verb, but also one or
more objects. It is precisely these key components that
can be subject to movement. Now when such a key
component is found outside the consitituent it belongs
to, the LMG formalism implements a simple mecha-
nism to pass the component down the derivation tree,
where it is picked up by the constituent that contains
its trace.
It is best to think of LMGs versus context-free
grammars as a predicate version of the (propositional)
paradigm of context-free grammars, in that nonter-
minals can have arguments. If we call the general
class of such grammars predicate grammars, the dis-
tinguishing feature of LMG with respect to other pred-
icate grammar formalisms such as indexed grammars&apos;
(Weir, 1988) (Aho, 1968) is the ability of binding or
quantification in the right hand side of a phrase struc-
ture rule.
</bodyText>
<listItem confidence="0.519208666666667">
1 2.1 Definition We fix disjoint sets N, T, V of non-
terminal symbols, terminal symbols and variables. We
will write A, B,C . . . to denote nonterminal symbols,
</listItem>
<bodyText confidence="0.920418166666667">
a,b,c . to denote terminal symbols, and x, y, z for
variables. A sequence a a2 • • an or a E T* is called
a (terminal) word or string. We will use the symbols
a, b, c for terminal words. (Note the use of bold face
for sequences.)
12.2 Definition (term) A sequence t it2 • • • tn or
t E (V U T)* is called a term. If a term consists of
variables only, we call it a vector and usually write x.
1 2.3 Definition (similarity type) A (partial) func-
tion i mapping N to the natural numbers is called a
similarity type.
12.4 Definition (predicate) Let A be a similarity
type, A E N and n A(A), and for 1 &lt; j &lt;
let ti be a term. Then a predicate cp of type A is
a terminal a (a terminal predicate) or a syntactical
unit of the form A(t 1, t2,. , tn), called a nonterminal
predicate. If all ti = xi are vectors, we say that
cp = A(xl, x2, ..., xn) is a pattern.
Informally, we think of the arguments of a nonterminal
as terminal words. A predicate A(x) then stands for
a constituent A where certain information with termi-
nal yield x has been extraposed (i.e. found outside
the constituent), and must hence be left out of the A
constituent itself.
</bodyText>
<listItem confidence="0.940465285714286">
12.5 Definition (item) Let A be a similarity type,
(i) a predicate of type A, and t a term. Then an item
of type A is a syntactical unit of one of the following
forms:
1. co (a nonterminal or terminal predicate)
2. x:co (a quantifier item)
3. co/t (a slash item)
</listItem>
<bodyText confidence="0.806487727272727">
We will use cl), IV to denote items, and a, /3,1, to denote
sequences of items.
12.6 Definition Let 1.1 be a similarity type. A
rewrite rule R of type i is a syntactical unit
01 4132 • • On where co is a pattern of type A, and for
1 &lt; i &lt; n, 0, is an item of type A.
A literal movement grammar is a triple (A, S, P)
where p. is a similarity type, S E N, R(S) = 0 and P
is a set of rewrite rules of type A.
Items on the right hand side of a rule can either refer
to variables, as in the following rule:
</bodyText>
<equation confidence="0.902097">
A(x,yz) —4 BOIx ely C(z)
or bind new variables, as the first two items in
A() —÷ x:B0 y:C(x) D(y).
</equation>
<bodyText confidence="0.999902375">
A slash item such as B()/x means that x should be
used instead of the actual &amp;quot;input&amp;quot; to recognize the non-
terminal predicate BO. I.e. the terminal word x should
be recognized as B(), and the item BOIx itself will
recognize the empty string. A quantifier item x:B0
means that a constituent BO is recognized from the
input, and the variable x, when used elsewhere in the
rule, will stand for the part of the input recognized.
</bodyText>
<subsubsectionHeader confidence="0.315628">
12.7 Definition (rewrite semantics) Let R =
</subsubsectionHeader>
<bodyText confidence="0.9872835">
A(x 1, , xn) 4) 1 (D2 • • • Om be a rewrite rule, then
an instantiation of R is the syntactical entity obtained
by substituting for each i and for each variable x E xi
a terminal word ax
A grammar derives the string a if SO =4 a where
is a relation between predicates and sequences of
items defined inductively by the following axioms and
inference rules:2
</bodyText>
<figure confidence="0.968588166666667">
a • a
co • a when a is an instantiation
of a rule in G
co .• A(t 1, • . • , in) &apos;Y A(t 1, • • • tn)
G
p a 7
• 0 11)1 a 7 bz4a
/E
7
G
cc) p x:11) 7 11) a
: E
(0 a -y)[a x]
a
MP
I Indexed grammars are a weak form of monadic predicate 2Note that [a/x] in the :E rule is not an item, but stands
grammar, as a nonterminal can have at most one argument. for the substitution of a for x.
92
/E
B(a) a/a b B(E) c
a a
G
BE) E
MP
B (a) b B(e) c
B(aa) a/a b B(a) c a==a
E
B(aa) = b B(a) c
MP
B (a) bc
B(aa) = bbcc
SO = x : AO 11(x)
AO = aa :E
SO aa B(aa)
B(aa) bbcc MP
SO aabbcc
</figure>
<figureCaption confidence="0.999598">
Figure 1. Derivation of aabbcc.
</figureCaption>
<figure confidence="0.795338222222222">
12.8 Example (am bn en) The following, very
mentary LMG recognizes the trans-context free
guage ab c:
cm:
SO x:A() B(x)
AO - a A()
A() —4 E
B(sy) - a/x b B(y) c
B(E)
</figure>
<figureCaption confidence="0.996151">
Figure 1 shows how aabbcc is derived according to
the grammar. The informal tree analysis in figure 2
</figureCaption>
<figure confidence="0.9973834">
SO
AO B(y) = B(aa)
a AO a/a b B(a) c
A 6
a AO aia b B(e)
</figure>
<figureCaption confidence="0.999957">
Figure 2. Informal tree analysis.
</figureCaption>
<bodyText confidence="0.958298117647059">
illustrates more intuitively how displaced information
(the two a symbols in this case) is &apos;moved back down&apos;
into the tree, until it gets &apos;consumed&apos; by a slash item.
It also shows how we can extract a context-free &apos;deep
structure&apos; for further analysis by, for example, formal
specification tools: if we transform the tree, as shown
in figure 3, by removing quantified (extraposed) data,
and abstracting away from the parameters, we see that
the grammar, in a sense, works by transforming the lan-
guage anbnCn to the context-free language (ab)c&apos;.
Figure 4 shows how we can derive a context free &apos;back-
bone grammar&apos; from the original grammar.
12.9 Example (cross-serial dependencies in
Dutch) The following LMG captures precisely the
three basic types of extraposition defined in section
1.3: the three Dutch verb orders, topicalization and
cross-serial verb-object dependencies.
</bodyText>
<equation confidence="0.9995734375">
S —,
Si(E) —&gt; dat NP VP(e , e)
,S1 (e) —, n:NP S&apos; (n)
S&apos; (n) —4 v:V NP VP(v , n)
S&apos; (e) —4 NP v:V VP(v , e)
VI&apos;(v , n) --4, m:NP VP(v , nm)
VP(v , n) —+ V (v , n)
V (e , e) VI
V (v , 6) —4 VI I v
- VT NPIn
V (v , n) —p VTIv NPIn
V (e,nm) —&gt; VR NPIn V (e,m)
V (v , nm) —&gt; VR I v NP I n V (e , m)
✓ —4 VI
✓ —4 VT
✓ —* VR
</equation>
<bodyText confidence="0.9999005">
A sentence S&apos; has one argument which is used, if
nonempty, to fill a noun phrase trace. A VP has two
</bodyText>
<figure confidence="0.940574333333333">
XP B
a(\&gt;c
abBC
</figure>
<figureCaption confidence="0.996053">
Figure 3. Context free backbone.
</figureCaption>
<figure confidence="0.873306142857143">
ele-
lan-
93
S --- XP B
B —■ abBc
B --- E
XP --- e
</figure>
<figureCaption confidence="0.999843">
Figure 4. Backbone grammar.
</figureCaption>
<bodyText confidence="0.8682767">
arguments: the first is used to fill verb traces, the sec-
ond is treated as a list of noun phrases to which more
noun phrases can be appended. A 1, is similar to a VP
except that it uses the list of noun phrases in its second
argument to fill noun phrase traces rather than adding
to it.
Figure 5 shows how this grammar accepts the sen-
tence
Marie zag Fred Anne kussen.
We see that it is analyzed as
</bodyText>
<subsubsectionHeader confidence="0.666942">
Marie zagi Fredi Annek
</subsubsectionHeader>
<bodyText confidence="0.964897166666667">
[IP ei ei [v, kussen ek 11
which as anticipated in section 1.3 has precisely the
basic, context-free underlying structure of the corre-
sponding English sentence Mary saw Fred kiss Anne
indicated in figure 5 by terminal words in bold face.
Note that arbitrary verbs are recognized by a quanti-
</bodyText>
<equation confidence="0.735385">
SO
(e)
V:
NP V
VP(v = zag, e)
Marie zag
VP( zag, en = Fred)
Fred VP(zag, Fred n = Fred Anne)
</equation>
<figureCaption confidence="0.998412">
Figure 5. Derivation of a Dutch sentence
</figureCaption>
<bodyText confidence="0.99918375">
fier item v:V, and only when, further down the tree, a
trace is filled with such a verb in items such as VR/v,
its subcategorization types VI, VT and VR start playing
a role.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="method">
3 Formal Properties
</sectionHeader>
<bodyText confidence="0.885470230769231">
The LMG formalism in its unrestricted form is shown
to be Turing complete in (Groenink, 1995a). But the
grammars presented in this paper satisfy a number of
vital properties that allow for efficient parsing tech-
niques.
Before building up material for a complexity result,
notice the following proposition, which shows, using
only part of the strength of the formalism, that the
literal movement grammars are closed under intersec-
tion.
13.1 Proposition (intersection) Given two lit-
eral movement grammars Gi = (111, Si, Pi) and
G2 = (112, S2, P2) such that dom(Ai ) fl dom(A2) -= 0,
</bodyText>
<equation confidence="0.99550325">
we can construct the grammar G1 = U ji U
{(S,0)}, S, P1 U P2 U {R}) where we add the rule
R:
SO x:Si() S201 x
</equation>
<bodyText confidence="0.993721666666667">
Clearly, G1 recognizes precisely those sentences which
are recognized by both G1 and G2.
We can use this knowledge in example 2.9 to restrict
movement of verbs to verbs of finite morphology, by
adding a nonterminal VFIN, replacing the quantifier
items v :V that locate verb fillers with v:VFIN, where
VFIN generates all finite verbs. Any extraposed verb
will then be required to be in the intersection of VFIN
and one of the verb types VI, VT or VR, reducing
possible ambiguity and improving the efficiency of
left-to-right recognition.
The following properties allow us to define restrictions
of the LMG formalism whose recognition problem has
a polynomial time complexity.
13.2 Definition (non-combinatorial) An LMG is
non-combinatorial if every argument of a nonterminal
on the RHS of a rule is a single variable (i.e. we do
not allow composite terms within predicates). If G
is a non-combinatorial LMG, then any terminal string
occurring (either as a sequence of items or inside a
predicate) in a full G-derivation is a substring of the
derived string. The grammar of example 2.8 is non-
combinatorial; the grammar of example 2.9 is not (the
offending rule is the first VP production).
</bodyText>
<listItem confidence="0.917712333333333">
1 3.3 Definition (left-binding) An LMG G is left-
binding when
1. W.r.t. argument positions, an item in the RHS of
a rule only depends on variables bound in items
to its left.
2. For any vector x • • • x, of n&gt; 1 variables on the
</listItem>
<bodyText confidence="0.9413774">
LHS, each of x1 upto 5._1 occurs in exactly one
item, which is of the form co/xi. Furthermore,
for each 1 &lt; 1 &lt; k &lt; n the item referring to xi
appears left of any item referring to xk
For example, the following rule is left binding:
</bodyText>
<figure confidence="0.9588923">
A(xyz,v) u:B(v) C(v)/x DQ /y E(u, z)
but these ones are not:
(a) A(y) C(x) x:D(y)
(b) A(xy) A(x) B(y)
(c) A(xyz) --+ A(z) BOIx COI y
Anne zag, Fred Anne)
VR/zag NP/Fred V&apos;(E, Anne)
•
kussen
NP/Anne
</figure>
<page confidence="0.997355">
94
</page>
<bodyText confidence="0.999526444444445">
because in (a), x is bound right of its use; in (b),
the item A(x) is not of the form cp /x and in (c), the
variables in the vector xyz occur in the wrong order
(zxy).
If a grammar satisfies condition 1, then for any deriv-
able string, there is a derivation such that the modus
ponens and elimination rules are always applied to the
leftmost item that is not a terminal. Furthermore, the
:E rule can be simplified to
</bodyText>
<subsectionHeader confidence="0.562523">
:E
</subsectionHeader>
<bodyText confidence="0.951739538461538">
co= ,3 a (y[alx])
The proof tree in example 2.8 (figure 1) is an example
of such a derivation.
Condition 2 eliminates the nondeterminism in find-
ing the right instantiation for rules with multiple vari-
able patterns in their LHS.
Both grammars from section 2 are left-binding.
13.4 Definition (left-recursive) An LMG G is
left-recursive if there exists an instantiated nonterminal
predicate cp such that there is a derivation of co cpa
for any sequence of items a.
The following two rules show that left-recursion in
LMG is not always immediately apparent:
</bodyText>
<figure confidence="0.8982746">
A(y) BOly A(c)
B() e
for we have
A(E) BO/e A(e) B() E
A(c) A(e)
</figure>
<bodyText confidence="0.988374583333333">
We now show that the recognition problem for an arbi-
trary left-binding, non-combinatorial LMG has a poly-
nomial worst-case time complexity.
13.5 Theorem (polynomial complexity) Let
G be a LMG of similarity type it that is non-
combinatorial, left binding and not left-recursive. Let
m be the maximum number of items on the right hand
side of rules in G, and let p be the greatest arity of
predicates occurring in G. Then the worst case time
complexity of the recognition problem for G does not
exceed 0(IGim(1 + p)nl+m+2P), where n is the size
of the input string al a2 • • • an.
Proof (sketch) We adopt the memoizing recursive de-
scent algorithm presented in (Leermakers, 1993). As
G is not left-binding, the terminal words associated
with variables occurring in the grammar rules can be
fully determined while proceeding through sentence
and rules from left to right. Because the grammar is
non-combinatorial, the terminal words substituted in
the argument positions of a nonterminal are always
substrings of the input sentence, and can hence be rep-
resented as a pair of integers.
The recursive descent algorithm recursively com-
putes set-valued recognition functions of the form:
</bodyText>
<equation confidence="0.86125">
[&apos;](O = I Sa == ai+1 • • 65}
</equation>
<bodyText confidence="0.9999558">
where instead of a nonterminal as in the context-
free case, so is any instantiated nonterminal predicate
A(bi , , bn). As b1, ,bn are continuous sub-
strings of the input sentence a1a2. • • an, we can re-
formulate this as
</bodyText>
<equation confidence="0.998702">
(li,r1),• • • ,(1A,rA))
A(ciii+i • • • an, • • • , a1,+1 • • • ar,
ai+i • • • ai }
</equation>
<bodyText confidence="0.98910075">
Where II =-- kt(A) &lt; p. The arguments i, /1, , l.
and r1,... ,r are integer numbers ranging from 0 to
n — 1 and 1 to n respectively. Once a result of such
a recognition function has been computed, it is stored
in a place where it can be retrieved in one atomic
operation. The number of such results to be stored is
0(n) for each possible nonterminal and each possible
combination of, at most 1 + 2p, arguments; so the total
space complexity is 0(j G1n2+2P).
Much of the extra complication w.r.t. the context-
free case is coped with at compile time; for example,
if there is one rule for nonterminal A:
</bodyText>
<equation confidence="0.862461272727273">
A(Xl, x2) X3.B1(X1) B20 B3(x3)/x2
then the code for [A](i, (11, r1), (12, r2)) will be
result empty
for k1 E [Bi](i, (11,r1))
do /3 i
r3 := k
for k2 E [B2](k1)
for k3 E [B3](12, (13, r3))
if (k3 == 7.2)
add k2 to result
return result
</equation>
<bodyText confidence="0.993835307692308">
The extra effort remaining at parse time is in copy-
ing arguments and an occasional extra comparison
(the if statement in the example), taking m(1 +
steps everytime the innermost for statement is reached,
and the fact that not 0(n), but 0(n1+2P) argument-
value pairs need to be memoized. Merging the re-
sults in a RHS sequence of m items can be done in
0(m(1 + p)nm -1) time. The result is a set of 0(n)
size. As there are at most 0(IG1n1+2P) results to be
computed, the overall time complexity of the algorithm
is 0(1G1m(1 + p)nl+m+2P). 0
13.6 Remark If all nonterminals in the grammar are
nullary (p = 0), then the complexity result coincides
with the values found for the context-free recursive
descent algorithm (Leermakers, 1993). Nullary LMG
includes the context-free case, but still allows move-
ment local to a rule; the closure result 3.1 still holds for
this class of grammars. As all we can do with binding
and slashing local to a rule is intersection, the nullary
LMGs must be precisely the closure of the context-free
grammars under finite intersection.
These results can be extended to more efficient al-
gorithms which can cope with left-recursive gram-
mars such as memoizing recursive ascent (Leermak-
ers, 1993). A very simple improvement is obtained
by bilinearizing the grammar (which is possible if it
</bodyText>
<equation confidence="0.527922">
G , G
p x:0 1, w a
/E
</equation>
<page confidence="0.920741">
95
</page>
<bodyText confidence="0.7370955">
is left binding), giving a worst case complexity of
0(IG1(1 +p)n32).
</bodyText>
<sectionHeader confidence="0.613252" genericHeader="method">
4 Other Approaches to Separation of
Movement
</sectionHeader>
<bodyText confidence="0.99997815">
A natural question to ask is whether the LMG for-
malism (for the purpose of embedding in equational
specification systems, or eliminating unification as a
stage of sentence processing) really has an advantage
over existing mildly context-sensitive approaches to
movement. Other non-concatenative formalisms are
head-wrapping grammars (HG) (Pollard, 1984), extra-
position grammars (XG) (Pereira, 1981) and various
exotic forms of tree adjoining grammar (Kroch and
Joshi, 1986). For overviews see (Weir, 1988), (Vijay-
Shanker etal., 1986) and (van Noord, 1993). The most
applicable of these formalisms for our purposes seem
to be HG and XG, as both of these show good re-
sults in modeling movement phenomena, and both are
similar in appearance to context-free grammars; as in
LMG, a context-free grammar has literally the same
representation when expressed in HG or XG. Hence it
is to be expected that incorporating these approaches
into a system based on a context-.free front-end will not
require a radical change of perspective.
</bodyText>
<subsectionHeader confidence="0.997155">
4.1 Head Grammars
</subsectionHeader>
<bodyText confidence="0.999994210526316">
A notion that plays an important role in various forms
of Linguistic theory is that of a head. Although there
is a great variation in the form and function of heads
in different theories, in general we might say that the
head of a constituent is the key component of that con-
stituent. The head grammar formalism, introduced by
Pollard in (Pollard, 1984) divides a constituent into
three components: a left context, a terminal head and
a right context. In a HG rewrite rule these parts of a
constituent can be addressed separately when building
a constituent from a number of subconstituents.
An accurate and elegant account of Dutch cross-
serial dependencies using HG is sketched in (Pollard,
1984). However, we have not been able to construct
head grammars that are able to model verb move-
ment, cross-serial dependencies and topicalization at
the same time. For every type of constituent, there
is only one head, and hence only one element of the
constituent that can be the subject to movement.3
</bodyText>
<subsectionHeader confidence="0.995598">
4.2 Extraposition Grammars
</subsectionHeader>
<bodyText confidence="0.978822833333333">
Whereas head grammars provide for an account of
verb fronting and cross-serial dependencies, Pereira,
3However, a straightforward extension of head grammars
defined in (Groen ink, 1995a) which makes use of arbitrary tu-
ples, rather than dividing constituents into three components,
is (1) capable of representing the three target phenomena of
Dutch all at once and (2) weakly equivalent to a (strongly
limiting) restriction of literal movement grammars. Head
grammars and their generalizations, being linear context-
free rewriting systems (Weir, 1988), have been shown to
have polynomial complexity.
introducing extraposition grammars in (Pereira, 1981),
is focused on displacement of noun phrases in English.
Extraposition grammars are in appearance very similar
to context-free grammars, but allow for larger patterns
on the left hand side of PS rules. This makes it possible
to allow a topical ized NP only if somewhere to its right
there is an unfilled trace:
</bodyText>
<figure confidence="0.4458795">
S Topic S
Topic. . .XP NP
</figure>
<bodyText confidence="0.998783384615385">
While XG allows for elegant accounts of cross-serial
dependencies and topicalization, it seems again hard
to simultaneously account for verb and noun move-
ment, especially if the bracketing constraint introduced
in (Pereira, 1981), which requires that XG derivation
graphs have a planar representation, is not relaxed.4
Furthermore, the practical application of XG seems
to be a problem. First, it is not obvious how we should
interpret XG derivation graphs for further analysis.
Second, as Pereira points out, it is nontrivial to make
the connection between the XG formalism and stan-
dard (e.g. Earley-based) parsing strategies so as to
obtain truly efficient implementations.
</bodyText>
<sectionHeader confidence="0.999368" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999995529411765">
We have presented the LMG formalism, examples
of its application, and a complexity result for a con-
strained subclass of the formalism. Example 2.9 shows
that an LMG can give an elegant account of movement
phenomena. The complexity result 3.5 is primarily in-
tended to give an indication of how the recognition
problem for LMG relates to that for arbitrary context
free grammars. It should be noted that the result in
this paper only applies to non-combinatorial LMGs,
excluding for instance the grammar of example 2.9 as
presented here.
There are other formalisms (HG and XG) which
provide sensible accounts of the three movement phe-
nomena sketched in section 1.3, but altogether do not
seem to be able to model all phenomena at once. In
(Groenink, 1995b) we give a more detailed analysis of
what is and is not possible in these formalisms.
</bodyText>
<subsectionHeader confidence="0.888573">
Future Work
</subsectionHeader>
<bodyText confidence="0.9987813">
1. The present proof of polynomial complexity does
not cover a very large class of literal movement gram-
mars. It is to be expected that larger, Turing complete,
classes will be formally intractable but behave reason-
ably in practice. It is worthwile to look at possible prac-
tical implementations for larger classes of LMGs, and
investigate the (theoretical and practical) performance
of these systems on various representative grammars.
2. Efficient treatment of LMG strongly depends
on the left-binding property of the grammars, which
</bodyText>
<footnote confidence="0.998397">
4Theoretically simultaneous treatment of the three move-
ment phenomena is not impossible in XG (a technique similar
to pit-stopping in GB allows one to wrap extrapositions over
natural bracketing islands), but grammars and derivations
become very hard to understand.
</footnote>
<page confidence="0.996432">
96
</page>
<bodyText confidence="0.998077333333333">
seems to restrict grammars to treatment of leftward
extraposition. In reality, a smaller class of rightward
movement phenomena will also need to be treated. It
is shown in (Groenink, 1995b) that these can easily
be circumvented in left-binding LMG, by introducing
artificial, &amp;quot;parasitic&amp;quot; extraposition.
</bodyText>
<sectionHeader confidence="0.982989" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999837">
I would like to thank Jasper Kamperman, René Leer-
makers, Jan van Eijck and Eelco Visser for their en-
thousiasm, for carefully reading this paper, and for
many general and technical comments that have con-
tributed a great deal to its consistency and readability.
</bodyText>
<sectionHeader confidence="0.997947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931418604652">
A.V. Aho. 1968. Indexed Grammars – an Extension
to Context-free grammars. JACM, 15:647-671.
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press.
J.A. Bergstra, J. Heering, and P. Klint, editors. 1989.
Algebraic Specification. ACM Press Frontier Se-
ries. The ACM Press in co-operation with Addison-
Wes I ey.
Annius V. Groenink. 1995a. Accounts of
Movement—a Formal Comparison. Unpublished
manuscript.
Annius V. Groenink. 1995b. Mechanisms for Move-
ment. Paper presented at the 5th CLIN (Compu-
tational Linguistics In the Netherlands) meeting,
November 1994.
A.S. Kroch and A.K. Joshi. 1986. Analyzing Extra-
position in a TAG. In Ojeda Huck, editor, Syntax
and Semantics: Discontinuous Constituents. Acad.
Press, New York.
René Leermakers. 1993. The Functional Treatment of
Parsing. Kluwer, The Netherlands.
Michael Newton. 1993. Formal Specification of
Grammar. Ph.D. thesis, University of Edinburgh.
Fernando Pereira. 1981. Extraposition Grammars.
Computational Linguistics, 7(4):243-256.
Carl J. Pollard. 1984. Generalized Phrase Struc-
ture Grammars, Head Grammars, and Natural Lan-
guage. Ph.D. thesis, Standford University.
Arie van Deursen. 1992. Specification and Genera-
tion of a .\-calculus environment. Technical report,
CWI, Amsterdam. Published in revised form in Van
Deursen, Executable Language Definitions—Case
Studies and Origin Tracking, PhD Thesis, Univer-
sity of Amsterdam, 1994.
Gertjan van Noord. 1993. Reversibility in Natural
Language. Ph.D. thesis, Rijksuniversiteit Gronin-
gen.
K. Vijay-Shanker, David J. Weir, and A.K. Joshi. 1986.
Tree Adjoining and Head Wrapping. In 1 1 th int.
conference on Computational Linguistics.
David J. Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Uni-
versity of Pennsylvania.
</reference>
<page confidence="0.999694">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420929">
<title confidence="0.999826">Literal Movement Grammars</title>
<author confidence="0.999469">Annius V Groenink</author>
<affiliation confidence="0.890304">CWI</affiliation>
<address confidence="0.9821">Kruislaan 413 1098 SJ Amsterdam The Netherlands</address>
<email confidence="0.50718">avg@cwi.n1</email>
<abstract confidence="0.9973818125">movement grammars provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information. LMGs provide a simple and efficient treatment of complex linguistic phenomena such as cross-Serial dependencies in German and Dutch—separating the treatment of natural language into a parsing phase closely resembling traditional contextfree treatment, and a disambiguation phase which can be carried out using matching, as opposed to full unification employed in most current grammar formalisms of linguistical relevance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
</authors>
<title>Indexed Grammars – an Extension to Context-free grammars.</title>
<date>1968</date>
<journal>JACM,</journal>
<pages>15--647</pages>
<contexts>
<context position="10412" citStr="Aho, 1968" startWordPosition="1650" endWordPosition="1651">ch a key component is found outside the consitituent it belongs to, the LMG formalism implements a simple mechanism to pass the component down the derivation tree, where it is picked up by the constituent that contains its trace. It is best to think of LMGs versus context-free grammars as a predicate version of the (propositional) paradigm of context-free grammars, in that nonterminals can have arguments. If we call the general class of such grammars predicate grammars, the distinguishing feature of LMG with respect to other predicate grammar formalisms such as indexed grammars&apos; (Weir, 1988) (Aho, 1968) is the ability of binding or quantification in the right hand side of a phrase structure rule. 1 2.1 Definition We fix disjoint sets N, T, V of nonterminal symbols, terminal symbols and variables. We will write A, B,C . . . to denote nonterminal symbols, a,b,c . to denote terminal symbols, and x, y, z for variables. A sequence a a2 • • an or a E T* is called a (terminal) word or string. We will use the symbols a, b, c for terminal words. (Note the use of bold face for sequences.) 12.2 Definition (term) A sequence t it2 • • • tn or t E (V U T)* is called a term. If a term consists of variables</context>
</contexts>
<marker>Aho, 1968</marker>
<rawString>A.V. Aho. 1968. Indexed Grammars – an Extension to Context-free grammars. JACM, 15:647-671.</rawString>
</citation>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1992</marker>
<rawString>Hiyan Alshawi, editor. 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bergstra</author>
<author>J Heering</author>
<author>P Klint</author>
<author>editors</author>
</authors>
<title>Algebraic Specification.</title>
<date>1989</date>
<booktitle>in co-operation with AddisonWes I ey.</booktitle>
<publisher>ACM Press Frontier Series. The ACM Press</publisher>
<contexts>
<context position="2362" citStr="Bergstra et al., 1989" startWordPosition="345" endWordPosition="348">fication techniques can be made useful in the treatment of natural language, see (Newton, 1993) which describes the abstract specification of several accounts of phrase structure, features, movement, modularity and This work is supported by SION grant 612-317-420 of the Netherlands Organization for Scientific Research (NWO). parametrization so as to abstract away from the exact language being modelled. The specification language (ASL) used by Newton is a very powerful formalism. The class of specification formalisms we have in mind includes less complex, equational techniques such as ASF+SDF (Bergstra et al., 1989) (van Deursen, 1992) which can be applied in practice by very efficient execution as a term rewriting system. Literal movement grammars are a straightforward extension of context-free grammars. The derivation trees of an LMG analysis can be easily transformed into trees belonging to a context-free backbone which gives way to treatment by formal specification systems. In order to obtain an efficient implementation, some restrictions on the general form of the formalism are necessary. 1.1 Structural Context Sensitivity in Natural Language Equational specification systems such as the ASF+SDF syst</context>
</contexts>
<marker>Bergstra, Heering, Klint, editors, 1989</marker>
<rawString>J.A. Bergstra, J. Heering, and P. Klint, editors. 1989. Algebraic Specification. ACM Press Frontier Series. The ACM Press in co-operation with AddisonWes I ey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annius V Groenink</author>
</authors>
<title>Accounts of Movement—a Formal Comparison.</title>
<date>1995</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="17004" citStr="Groenink, 1995" startWordPosition="3008" endWordPosition="3009">ree underlying structure of the corresponding English sentence Mary saw Fred kiss Anne indicated in figure 5 by terminal words in bold face. Note that arbitrary verbs are recognized by a quantiSO (e) V: NP V VP(v = zag, e) Marie zag VP( zag, en = Fred) Fred VP(zag, Fred n = Fred Anne) Figure 5. Derivation of a Dutch sentence fier item v:V, and only when, further down the tree, a trace is filled with such a verb in items such as VR/v, its subcategorization types VI, VT and VR start playing a role. 3 Formal Properties The LMG formalism in its unrestricted form is shown to be Turing complete in (Groenink, 1995a). But the grammars presented in this paper satisfy a number of vital properties that allow for efficient parsing techniques. Before building up material for a complexity result, notice the following proposition, which shows, using only part of the strength of the formalism, that the literal movement grammars are closed under intersection. 13.1 Proposition (intersection) Given two literal movement grammars Gi = (111, Si, Pi) and G2 = (112, S2, P2) such that dom(Ai ) fl dom(A2) -= 0, we can construct the grammar G1 = U ji U {(S,0)}, S, P1 U P2 U {R}) where we add the rule R: SO x:Si() S201 x C</context>
<context position="28783" citStr="Groenink, 1995" startWordPosition="5043" endWordPosition="5044">mple 2.9 shows that an LMG can give an elegant account of movement phenomena. The complexity result 3.5 is primarily intended to give an indication of how the recognition problem for LMG relates to that for arbitrary context free grammars. It should be noted that the result in this paper only applies to non-combinatorial LMGs, excluding for instance the grammar of example 2.9 as presented here. There are other formalisms (HG and XG) which provide sensible accounts of the three movement phenomena sketched in section 1.3, but altogether do not seem to be able to model all phenomena at once. In (Groenink, 1995b) we give a more detailed analysis of what is and is not possible in these formalisms. Future Work 1. The present proof of polynomial complexity does not cover a very large class of literal movement grammars. It is to be expected that larger, Turing complete, classes will be formally intractable but behave reasonably in practice. It is worthwile to look at possible practical implementations for larger classes of LMGs, and investigate the (theoretical and practical) performance of these systems on various representative grammars. 2. Efficient treatment of LMG strongly depends on the left-bindi</context>
</contexts>
<marker>Groenink, 1995</marker>
<rawString>Annius V. Groenink. 1995a. Accounts of Movement—a Formal Comparison. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annius V Groenink</author>
</authors>
<title>Mechanisms for Movement.</title>
<date>1995</date>
<booktitle>Paper presented at the 5th CLIN (Computational Linguistics In the Netherlands) meeting,</booktitle>
<contexts>
<context position="17004" citStr="Groenink, 1995" startWordPosition="3008" endWordPosition="3009">ree underlying structure of the corresponding English sentence Mary saw Fred kiss Anne indicated in figure 5 by terminal words in bold face. Note that arbitrary verbs are recognized by a quantiSO (e) V: NP V VP(v = zag, e) Marie zag VP( zag, en = Fred) Fred VP(zag, Fred n = Fred Anne) Figure 5. Derivation of a Dutch sentence fier item v:V, and only when, further down the tree, a trace is filled with such a verb in items such as VR/v, its subcategorization types VI, VT and VR start playing a role. 3 Formal Properties The LMG formalism in its unrestricted form is shown to be Turing complete in (Groenink, 1995a). But the grammars presented in this paper satisfy a number of vital properties that allow for efficient parsing techniques. Before building up material for a complexity result, notice the following proposition, which shows, using only part of the strength of the formalism, that the literal movement grammars are closed under intersection. 13.1 Proposition (intersection) Given two literal movement grammars Gi = (111, Si, Pi) and G2 = (112, S2, P2) such that dom(Ai ) fl dom(A2) -= 0, we can construct the grammar G1 = U ji U {(S,0)}, S, P1 U P2 U {R}) where we add the rule R: SO x:Si() S201 x C</context>
<context position="28783" citStr="Groenink, 1995" startWordPosition="5043" endWordPosition="5044">mple 2.9 shows that an LMG can give an elegant account of movement phenomena. The complexity result 3.5 is primarily intended to give an indication of how the recognition problem for LMG relates to that for arbitrary context free grammars. It should be noted that the result in this paper only applies to non-combinatorial LMGs, excluding for instance the grammar of example 2.9 as presented here. There are other formalisms (HG and XG) which provide sensible accounts of the three movement phenomena sketched in section 1.3, but altogether do not seem to be able to model all phenomena at once. In (Groenink, 1995b) we give a more detailed analysis of what is and is not possible in these formalisms. Future Work 1. The present proof of polynomial complexity does not cover a very large class of literal movement grammars. It is to be expected that larger, Turing complete, classes will be formally intractable but behave reasonably in practice. It is worthwile to look at possible practical implementations for larger classes of LMGs, and investigate the (theoretical and practical) performance of these systems on various representative grammars. 2. Efficient treatment of LMG strongly depends on the left-bindi</context>
</contexts>
<marker>Groenink, 1995</marker>
<rawString>Annius V. Groenink. 1995b. Mechanisms for Movement. Paper presented at the 5th CLIN (Computational Linguistics In the Netherlands) meeting, November 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Kroch</author>
<author>A K Joshi</author>
</authors>
<title>Analyzing Extraposition</title>
<date>1986</date>
<booktitle>Syntax and Semantics: Discontinuous Constituents.</booktitle>
<editor>in a TAG. In Ojeda Huck, editor,</editor>
<publisher>Acad. Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9231" citStr="Kroch and Joshi, 1986" startWordPosition="1448" endWordPosition="1451"> it is worth taking into consideration to develop a way of deferring treatment of syntactical data. E.g. for example sentence 1 this means that upon finding the displaced constituent which book, we will not evaluate that constituent, but rather remember during the treatment of the remaining part of the sentence, that this data is still to be fitted into a logical place. This is not a new idea. A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981). and tree adjoining grammars (TAG) (Kroch and Joshi, 1986). A discussion of these formalisms as alternatives to the LMG formalism is given in section 4. 91 Lessons in parsing by hand in high school (e.g. in English or Latin classes) informally illustrate the purpose of literal movement grammars: as opposed to the traditional linguistic point of view that there is only one head which dominates a phrase, constituents of a sentence have several key components. A verb phrase for example not only has its finite verb, but also one or more objects. It is precisely these key components that can be subject to movement. Now when such a key component is found o</context>
<context position="24758" citStr="Kroch and Joshi, 1986" startWordPosition="4389" endWordPosition="4392">ble if it G , G p x:0 1, w a /E 95 is left binding), giving a worst case complexity of 0(IG1(1 +p)n32). 4 Other Approaches to Separation of Movement A natural question to ask is whether the LMG formalism (for the purpose of embedding in equational specification systems, or eliminating unification as a stage of sentence processing) really has an advantage over existing mildly context-sensitive approaches to movement. Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). For overviews see (Weir, 1988), (VijayShanker etal., 1986) and (van Noord, 1993). The most applicable of these formalisms for our purposes seem to be HG and XG, as both of these show good results in modeling movement phenomena, and both are similar in appearance to context-free grammars; as in LMG, a context-free grammar has literally the same representation when expressed in HG or XG. Hence it is to be expected that incorporating these approaches into a system based on a context-.free front-end will not require a radical change of perspective. 4.1 Head Grammars A notion that plays an import</context>
</contexts>
<marker>Kroch, Joshi, 1986</marker>
<rawString>A.S. Kroch and A.K. Joshi. 1986. Analyzing Extraposition in a TAG. In Ojeda Huck, editor, Syntax and Semantics: Discontinuous Constituents. Acad. Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>René Leermakers</author>
</authors>
<title>The Functional Treatment of Parsing.</title>
<date>1993</date>
<publisher>Kluwer, The</publisher>
<contexts>
<context position="21165" citStr="Leermakers, 1993" startWordPosition="3755" endWordPosition="3756"> left-binding, non-combinatorial LMG has a polynomial worst-case time complexity. 13.5 Theorem (polynomial complexity) Let G be a LMG of similarity type it that is noncombinatorial, left binding and not left-recursive. Let m be the maximum number of items on the right hand side of rules in G, and let p be the greatest arity of predicates occurring in G. Then the worst case time complexity of the recognition problem for G does not exceed 0(IGim(1 + p)nl+m+2P), where n is the size of the input string al a2 • • • an. Proof (sketch) We adopt the memoizing recursive descent algorithm presented in (Leermakers, 1993). As G is not left-binding, the terminal words associated with variables occurring in the grammar rules can be fully determined while proceeding through sentence and rules from left to right. Because the grammar is non-combinatorial, the terminal words substituted in the argument positions of a nonterminal are always substrings of the input sentence, and can hence be represented as a pair of integers. The recursive descent algorithm recursively computes set-valued recognition functions of the form: [&apos;](O = I Sa == ai+1 • • 65} where instead of a nonterminal as in the contextfree case, so is an</context>
<context position="23565" citStr="Leermakers, 1993" startWordPosition="4200" endWordPosition="4201">ent in the example), taking m(1 + steps everytime the innermost for statement is reached, and the fact that not 0(n), but 0(n1+2P) argumentvalue pairs need to be memoized. Merging the results in a RHS sequence of m items can be done in 0(m(1 + p)nm -1) time. The result is a set of 0(n) size. As there are at most 0(IG1n1+2P) results to be computed, the overall time complexity of the algorithm is 0(1G1m(1 + p)nl+m+2P). 0 13.6 Remark If all nonterminals in the grammar are nullary (p = 0), then the complexity result coincides with the values found for the context-free recursive descent algorithm (Leermakers, 1993). Nullary LMG includes the context-free case, but still allows movement local to a rule; the closure result 3.1 still holds for this class of grammars. As all we can do with binding and slashing local to a rule is intersection, the nullary LMGs must be precisely the closure of the context-free grammars under finite intersection. These results can be extended to more efficient algorithms which can cope with left-recursive grammars such as memoizing recursive ascent (Leermakers, 1993). A very simple improvement is obtained by bilinearizing the grammar (which is possible if it G , G p x:0 1, w a </context>
</contexts>
<marker>Leermakers, 1993</marker>
<rawString>René Leermakers. 1993. The Functional Treatment of Parsing. Kluwer, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Newton</author>
</authors>
<title>Formal Specification of Grammar.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1835" citStr="Newton, 1993" startWordPosition="270" endWordPosition="271">y popular computer-science paradigms, such as the theory of attribute grammars and modular equational specification, to problems in linguistics. These formal specification techniques, far exceeding the popular Prolog in declarativity, may give new insight into the formal properties of natural language, and facilitate prototyping for large language applications in the same way as they are currently being used to facilitate prototyping of programming language tools. For an extensive illustration of how formal specification techniques can be made useful in the treatment of natural language, see (Newton, 1993) which describes the abstract specification of several accounts of phrase structure, features, movement, modularity and This work is supported by SION grant 612-317-420 of the Netherlands Organization for Scientific Research (NWO). parametrization so as to abstract away from the exact language being modelled. The specification language (ASL) used by Newton is a very powerful formalism. The class of specification formalisms we have in mind includes less complex, equational techniques such as ASF+SDF (Bergstra et al., 1989) (van Deursen, 1992) which can be applied in practice by very efficient e</context>
</contexts>
<marker>Newton, 1993</marker>
<rawString>Michael Newton. 1993. Formal Specification of Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>Extraposition Grammars.</title>
<date>1981</date>
<journal>Computational Linguistics,</journal>
<pages>7--4</pages>
<contexts>
<context position="9172" citStr="Pereira, 1981" startWordPosition="1441" endWordPosition="1442">ually found to the left of the corresponding trace, it is worth taking into consideration to develop a way of deferring treatment of syntactical data. E.g. for example sentence 1 this means that upon finding the displaced constituent which book, we will not evaluate that constituent, but rather remember during the treatment of the remaining part of the sentence, that this data is still to be fitted into a logical place. This is not a new idea. A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981). and tree adjoining grammars (TAG) (Kroch and Joshi, 1986). A discussion of these formalisms as alternatives to the LMG formalism is given in section 4. 91 Lessons in parsing by hand in high school (e.g. in English or Latin classes) informally illustrate the purpose of literal movement grammars: as opposed to the traditional linguistic point of view that there is only one head which dominates a phrase, constituents of a sentence have several key components. A verb phrase for example not only has its finite verb, but also one or more objects. It is precisely these key components that can be su</context>
<context position="24683" citStr="Pereira, 1981" startWordPosition="4379" endWordPosition="4380">mprovement is obtained by bilinearizing the grammar (which is possible if it G , G p x:0 1, w a /E 95 is left binding), giving a worst case complexity of 0(IG1(1 +p)n32). 4 Other Approaches to Separation of Movement A natural question to ask is whether the LMG formalism (for the purpose of embedding in equational specification systems, or eliminating unification as a stage of sentence processing) really has an advantage over existing mildly context-sensitive approaches to movement. Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). For overviews see (Weir, 1988), (VijayShanker etal., 1986) and (van Noord, 1993). The most applicable of these formalisms for our purposes seem to be HG and XG, as both of these show good results in modeling movement phenomena, and both are similar in appearance to context-free grammars; as in LMG, a context-free grammar has literally the same representation when expressed in HG or XG. Hence it is to be expected that incorporating these approaches into a system based on a context-.free front-end will not require a rad</context>
<context position="27004" citStr="Pereira, 1981" startWordPosition="4752" endWordPosition="4753">account of verb fronting and cross-serial dependencies, Pereira, 3However, a straightforward extension of head grammars defined in (Groen ink, 1995a) which makes use of arbitrary tuples, rather than dividing constituents into three components, is (1) capable of representing the three target phenomena of Dutch all at once and (2) weakly equivalent to a (strongly limiting) restriction of literal movement grammars. Head grammars and their generalizations, being linear contextfree rewriting systems (Weir, 1988), have been shown to have polynomial complexity. introducing extraposition grammars in (Pereira, 1981), is focused on displacement of noun phrases in English. Extraposition grammars are in appearance very similar to context-free grammars, but allow for larger patterns on the left hand side of PS rules. This makes it possible to allow a topical ized NP only if somewhere to its right there is an unfilled trace: S Topic S Topic. . .XP NP While XG allows for elegant accounts of cross-serial dependencies and topicalization, it seems again hard to simultaneously account for verb and noun movement, especially if the bracketing constraint introduced in (Pereira, 1981), which requires that XG derivatio</context>
</contexts>
<marker>Pereira, 1981</marker>
<rawString>Fernando Pereira. 1981. Extraposition Grammars. Computational Linguistics, 7(4):243-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl J Pollard</author>
</authors>
<title>Generalized Phrase Structure Grammars, Head Grammars, and Natural Language.</title>
<date>1984</date>
<tech>Ph.D. thesis,</tech>
<institution>Standford University.</institution>
<contexts>
<context position="9127" citStr="Pollard, 1984" startWordPosition="1435" endWordPosition="1437">1992). If we bear in mind that a filler is usually found to the left of the corresponding trace, it is worth taking into consideration to develop a way of deferring treatment of syntactical data. E.g. for example sentence 1 this means that upon finding the displaced constituent which book, we will not evaluate that constituent, but rather remember during the treatment of the remaining part of the sentence, that this data is still to be fitted into a logical place. This is not a new idea. A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981). and tree adjoining grammars (TAG) (Kroch and Joshi, 1986). A discussion of these formalisms as alternatives to the LMG formalism is given in section 4. 91 Lessons in parsing by hand in high school (e.g. in English or Latin classes) informally illustrate the purpose of literal movement grammars: as opposed to the traditional linguistic point of view that there is only one head which dominates a phrase, constituents of a sentence have several key components. A verb phrase for example not only has its finite verb, but also one or more objects. It is </context>
<context position="24638" citStr="Pollard, 1984" startWordPosition="4373" endWordPosition="4374">ve ascent (Leermakers, 1993). A very simple improvement is obtained by bilinearizing the grammar (which is possible if it G , G p x:0 1, w a /E 95 is left binding), giving a worst case complexity of 0(IG1(1 +p)n32). 4 Other Approaches to Separation of Movement A natural question to ask is whether the LMG formalism (for the purpose of embedding in equational specification systems, or eliminating unification as a stage of sentence processing) really has an advantage over existing mildly context-sensitive approaches to movement. Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). For overviews see (Weir, 1988), (VijayShanker etal., 1986) and (van Noord, 1993). The most applicable of these formalisms for our purposes seem to be HG and XG, as both of these show good results in modeling movement phenomena, and both are similar in appearance to context-free grammars; as in LMG, a context-free grammar has literally the same representation when expressed in HG or XG. Hence it is to be expected that incorporating these approaches into a system based on a c</context>
<context position="26026" citStr="Pollard, 1984" startWordPosition="4605" endWordPosition="4606"> of a head. Although there is a great variation in the form and function of heads in different theories, in general we might say that the head of a constituent is the key component of that constituent. The head grammar formalism, introduced by Pollard in (Pollard, 1984) divides a constituent into three components: a left context, a terminal head and a right context. In a HG rewrite rule these parts of a constituent can be addressed separately when building a constituent from a number of subconstituents. An accurate and elegant account of Dutch crossserial dependencies using HG is sketched in (Pollard, 1984). However, we have not been able to construct head grammars that are able to model verb movement, cross-serial dependencies and topicalization at the same time. For every type of constituent, there is only one head, and hence only one element of the constituent that can be the subject to movement.3 4.2 Extraposition Grammars Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, Pereira, 3However, a straightforward extension of head grammars defined in (Groen ink, 1995a) which makes use of arbitrary tuples, rather than dividing constituents into three comp</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Carl J. Pollard. 1984. Generalized Phrase Structure Grammars, Head Grammars, and Natural Language. Ph.D. thesis, Standford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arie van Deursen</author>
</authors>
<title>Specification and Generation of a .\-calculus environment.</title>
<date>1992</date>
<tech>Technical report, CWI,</tech>
<institution>University of Amsterdam,</institution>
<location>Amsterdam.</location>
<note>Published in revised form in</note>
<marker>van Deursen, 1992</marker>
<rawString>Arie van Deursen. 1992. Specification and Generation of a .\-calculus environment. Technical report, CWI, Amsterdam. Published in revised form in Van Deursen, Executable Language Definitions—Case Studies and Origin Tracking, PhD Thesis, University of Amsterdam, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversibility in Natural Language.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Rijksuniversiteit Groningen.</institution>
<marker>van Noord, 1993</marker>
<rawString>Gertjan van Noord. 1993. Reversibility in Natural Language. Ph.D. thesis, Rijksuniversiteit Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Tree Adjoining and Head Wrapping.</title>
<date>1986</date>
<booktitle>In 1 1 th int. conference on Computational Linguistics.</booktitle>
<marker>Vijay-Shanker, Weir, Joshi, 1986</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and A.K. Joshi. 1986. Tree Adjoining and Head Wrapping. In 1 1 th int. conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly ContextSensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10400" citStr="Weir, 1988" startWordPosition="1648" endWordPosition="1649">. Now when such a key component is found outside the consitituent it belongs to, the LMG formalism implements a simple mechanism to pass the component down the derivation tree, where it is picked up by the constituent that contains its trace. It is best to think of LMGs versus context-free grammars as a predicate version of the (propositional) paradigm of context-free grammars, in that nonterminals can have arguments. If we call the general class of such grammars predicate grammars, the distinguishing feature of LMG with respect to other predicate grammar formalisms such as indexed grammars&apos; (Weir, 1988) (Aho, 1968) is the ability of binding or quantification in the right hand side of a phrase structure rule. 1 2.1 Definition We fix disjoint sets N, T, V of nonterminal symbols, terminal symbols and variables. We will write A, B,C . . . to denote nonterminal symbols, a,b,c . to denote terminal symbols, and x, y, z for variables. A sequence a a2 • • an or a E T* is called a (terminal) word or string. We will use the symbols a, b, c for terminal words. (Note the use of bold face for sequences.) 12.2 Definition (term) A sequence t it2 • • • tn or t E (V U T)* is called a term. If a term consists </context>
<context position="24790" citStr="Weir, 1988" startWordPosition="4396" endWordPosition="4397">binding), giving a worst case complexity of 0(IG1(1 +p)n32). 4 Other Approaches to Separation of Movement A natural question to ask is whether the LMG formalism (for the purpose of embedding in equational specification systems, or eliminating unification as a stage of sentence processing) really has an advantage over existing mildly context-sensitive approaches to movement. Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extraposition grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). For overviews see (Weir, 1988), (VijayShanker etal., 1986) and (van Noord, 1993). The most applicable of these formalisms for our purposes seem to be HG and XG, as both of these show good results in modeling movement phenomena, and both are similar in appearance to context-free grammars; as in LMG, a context-free grammar has literally the same representation when expressed in HG or XG. Hence it is to be expected that incorporating these approaches into a system based on a context-.free front-end will not require a radical change of perspective. 4.1 Head Grammars A notion that plays an important role in various forms of Lin</context>
<context position="26902" citStr="Weir, 1988" startWordPosition="4739" endWordPosition="4740">t can be the subject to movement.3 4.2 Extraposition Grammars Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, Pereira, 3However, a straightforward extension of head grammars defined in (Groen ink, 1995a) which makes use of arbitrary tuples, rather than dividing constituents into three components, is (1) capable of representing the three target phenomena of Dutch all at once and (2) weakly equivalent to a (strongly limiting) restriction of literal movement grammars. Head grammars and their generalizations, being linear contextfree rewriting systems (Weir, 1988), have been shown to have polynomial complexity. introducing extraposition grammars in (Pereira, 1981), is focused on displacement of noun phrases in English. Extraposition grammars are in appearance very similar to context-free grammars, but allow for larger patterns on the left hand side of PS rules. This makes it possible to allow a topical ized NP only if somewhere to its right there is an unfilled trace: S Topic S Topic. . .XP NP While XG allows for elegant accounts of cross-serial dependencies and topicalization, it seems again hard to simultaneously account for verb and noun movement, e</context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>David J. Weir. 1988. Characterizing Mildly ContextSensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>