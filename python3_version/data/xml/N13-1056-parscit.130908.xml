<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001114">
<title confidence="0.988497">
Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals
</title>
<author confidence="0.943411">
Ann Irvine Chris Callison-Burch⇤
</author>
<affiliation confidence="0.950226">
Center for Language and Speech Processing Computer and Information Science Dept.
Johns Hopkins University University of Pennsylvania
</affiliation>
<sectionHeader confidence="0.987637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999656272727273">
Prior research into learning translations from
source and target language monolingual texts
has treated the task as an unsupervised learn-
ing problem. Although many techniques take
advantage of a seed bilingual lexicon, this
work is the first to use that data for super-
vised learning to combine a diverse set of sig-
nals derived from a pair of monolingual cor-
pora into a single discriminative model. Even
in a low resource machine translation setting,
where induced translations have the potential
to improve performance substantially, it is rea-
sonable to assume access to some amount of
data to perform this kind of optimization. Our
work shows that only a few hundred transla-
tion pairs are needed to achieve strong per-
formance on the bilingual lexicon induction
task, and our approach yields an average rel-
ative gain in accuracy of nearly 50% over an
unsupervised baseline. Large gains in accu-
racy hold for all 22 languages (low and high
resource) that we investigate.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998557159090909">
Bilingual lexicon induction is the task of identifying
word translation pairs using source and target mono-
lingual corpora, which are often comparable. Most
approaches to the task are based on the idea that
words that are translations of one another have sim-
ilar distributional properties across languages. Prior
research has shown that contextual similarity (Rapp,
1995), temporal similarity (Schafer and Yarowsky,
2002), and topical information (Mimno et al., 2009)
⇤Performed while faculty at Johns Hopkins University
are all good signals for learning translations from
monolingual texts.
Most prior work either makes use of only one or
two monolingual signals or uses unsupervised meth-
ods (like rank combination) to aggregate orthogonal
signals (Schafer and Yarowsky, 2002; Klementiev
and Roth, 2006). Surprisingly, no past research has
employed supervised approaches to combine diverse
monolingually-derived signals for bilingual lexicon
induction. The field of machine learning has shown
decisively that supervised models dramatically out-
perform unsupervised models, including for closely
related problems like statistical machine translation
(Och and Ney, 2002).
For the bilingual lexicon induction task, a super-
vised approach is natural, particularly because com-
puting contextual similarity typically requires a seed
bilingual dictionary (Rapp, 1995), and that same
dictionary may be used for estimating the param-
eters of a model to combine monolingual signals.
Alternatively, in a low resource machine transla-
tion (MT) setting, it is reasonable to assume a small
amount of parallel data from which a bilingual dic-
tionary can be extracted for supervision. In this set-
ting, bilingual lexicon induction is critical for trans-
lating source words which do not appear in the par-
allel data or dictionary.
We frame bilingual lexicon induction as a binary
classification problem; for a pair of source and tar-
get language words, we predict whether the two are
translations of one another or not. For a given source
language word, we score all target language can-
didates separately and then rerank them. We use
a variety of signals derived from source and target
</bodyText>
<page confidence="0.939764">
518
</page>
<note confidence="0.4746555">
Proceedings of NAACL-HLT 2013, pages 518–523,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.994087">
monolingual corpora as features and use supervision
to estimate the strength of each. In this work we:
</bodyText>
<listItem confidence="0.9376101">
• Use the following similarity metrics derived
from monolingual corpora to score word pairs:
contextual, temporal, topical, orthographic, and
frequency.
• For the first time, explore using supervision to
combine monolingual signals and learn a dis-
criminative model for predicting translations.
• Present results for 22 low and high resource
languages paired with English and show large
accuracy gains over an unsupervised baseline.
</listItem>
<sectionHeader confidence="0.986214" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9998728125">
Prior work suggests that a wide variety of mono-
lingual signals, including distributional, temporal,
topic, and string similarity, may inform bilingual
lexicon induction (Rapp, 1995; Fung and Yee, 1998;
Rapp, 1999; Schafer and Yarowsky, 2002; Schafer,
2006; Klementiev and Roth, 2006; Koehn and
Knight, 2002; Haghighi et al., 2008; Mimno et
al., 2009; Mausam et al., 2010). Klementiev et al.
(2012) use many of those signals to score an exist-
ing phrase table for end-to-end MT but do not learn
any new translations. Schafer and Yarowsky (2002)
use an unsupervised rank-combination method for
combining orthographic, contextual, temporal, and
frequency similarities into a single ranking.
Recently, Ravi and Knight (2011), Dou and
Knight (2012), and Nuhn et al. (2012) have worked
toward learning a phrase-based translation model
from monolingual corpora, relying on decipherment
techniques. In contrast to that work, we use a
seed bilingual lexicon for supervision and multiple
monolingual signals proposed in prior work.
Haghighi et al. (2008) and Daum´e and Jagarla-
mudi (2011) use some supervision to learn how to
project contextual and orthographic features into a
low-dimensional space, with the goal of represent-
ing words which are translations of one another
as vectors which are close together in that space.
However, both of those approaches focus on only
two signals, high resource languages, and frequent
words (frequent nouns, in the case of Haghighi et
al. (2008)). In our classification framework, we can
incorporate any number of monolingual signals, in-
</bodyText>
<table confidence="0.9998165">
Language #Words Language #Words
Nepali 0.4 Somali 0.5
Uzbek 1.4 Azeri 2.6
Tamil 3.7 Albanian 6.5
Bengali 6.6 Welsh 7.5
Bosnian 12.9 Latvian 40.2
Indonesian 21.8 Romanian 24.1
Serbian 25.8 Turkish 31.2
Ukrainian 37.6 Hindi 47.4
Bulgarian 49.5 Polish 104.5
Slovak 124.3 Urdu 287.2
Farsi 710.3 Spanish 972
</table>
<tableCaption confidence="0.9641075">
Table 1: Millions of monolingual web crawl and
Wikipedia word tokens
</tableCaption>
<bodyText confidence="0.959952">
cluding contextual and string similarity, and directly
learn how to combine them.
</bodyText>
<sectionHeader confidence="0.845446" genericHeader="method">
3 Monolingual Data and Signals
</sectionHeader>
<subsectionHeader confidence="0.986153">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999992571428572">
Throughout our experiments, we seek to learn how
to translate words in a given source language into
English. Table 1 lists our languages of interest,
along with the total amount of monolingual data
that we use for each. We use web crawled time-
stamped news articles to estimate temporal sim-
ilarity, Wikipedia pages which are inter-lingually
linked to English pages to estimate topic similarity,
and both datasets to estimate frequency and contex-
tual similarity. Following Irvine et al. (2010), we
use pairs of Wikipedia page titles to train a simple
transliterator for languages written in a non-Roman
script, which allows us to compute orthographic
similarity for pairs of words in different scripts.
</bodyText>
<subsectionHeader confidence="0.998384">
3.2 Signals
</subsectionHeader>
<bodyText confidence="0.999888333333333">
Our definitions of orthographic, topic, temporal, and
contextual similarity are taken from Klementiev et
al. (2012), and the details of each may be found
there. Here, we give briefly describe them and give
our definition of a novel, frequency-based signal.
Orthographic We measure orthographic similar-
ity between a pair of words as the normalized1 edit
distance between the two words. For non-Roman
script languages, we transliterate words into the Ro-
man script before measuring orthographic similarity.
Topic We use monolingual Wikipedia pages to es-
timate topical signatures for each source and target
</bodyText>
<footnote confidence="0.958531">
1Normalized by the average of the lengths of the two words
</footnote>
<page confidence="0.996961">
519
</page>
<bodyText confidence="0.999928103448276">
language word. Signature vectors are the length of
the number of inter-lingually linked source and En-
glish Wikipedia pages and contain counts of how
many times the word appears on each page. We use
cosine similarity to compare pairs of signatures.
Temporal We use time-stamped web crawl data
to estimate temporal signatures, which, for a given
word, are the length of the number of time-stamps
(dates) and contain counts of how many times the
word appears in news articles with the given date.
We use a sliding window of three days and use co-
sine similarity to compare signatures. We expect
that source and target language words which are
translations of one another will appear with similar
frequencies over time in monolingual data.
Contextual We score monolingual contextual
similarity by first collecting context vectors for each
source and target language word. The context vector
for a given word contains counts of how many times
words appear in its context. We use bag of words
contexts in a window of size two. We gather both
source and target language contextual vectors from
our web crawl data and Wikipedia data (separately).
Frequency Words that are translations of one an-
other are likely to have similar relative frequencies
in monolingual corpora. We measure the frequency
similarity of two words as the absolute value of the
difference between the logs of their relative mono-
lingual corpus frequencies.
</bodyText>
<sectionHeader confidence="0.96336" genericHeader="method">
4 Supervised Bilingual Lexicon Induction
</sectionHeader>
<subsectionHeader confidence="0.93608">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999954833333333">
Our unsupervised baseline method is based on
ranked lists derived from each of the signals listed
above. For each source word, we generate ranked
lists of English candidates using the following six
signals: Crawls Context, Crawls Time, Wikipedia
Context, Wikipedia Topic, Edit distance, and Log
Frequency Difference. Then, for each English can-
didate we compute its mean reciprocal rank2 (MRR)
based on the six ranked lists. The baseline ranks En-
glish candidates according to the MRR scores. For
evaluation, we use the same test sets, accuracy met-
ric, and correct translations described below.
</bodyText>
<subsectionHeader confidence="0.441277">
2The MRR of the jth English word, ej, is n, PN 1 rankij
</subsectionHeader>
<bodyText confidence="0.991153">
where N is the number of signals and rankzj is ej’s rank ac-
cording to signal i.
</bodyText>
<subsectionHeader confidence="0.997773">
4.2 Supervised Approach
</subsectionHeader>
<bodyText confidence="0.999976068965517">
In addition to the monolingual resources described
in Section 3.1, we have a bilingual dictionary for
each language, which we use to project context vec-
tors and for supervision and evaluation. For each
language, we choose up to 8, 000 source language
words among those that occur in the monolingual
data at least three times and that have at least one
translation in our dictionary. We randomly divide
the source language words into three equally sized
sets for training, development, and testing. We use
the training data to train a classifier, the develop-
ment data to choose the best classification settings
and feature set, and the test set for evaluation.
For all experiments, we use a linear classifier
trained by stochastic gradient descent to minimize
squared error3 and perform 100 passes over the
training data.4 The binary classifiers predict whether
a pair of words are translations of one another or not.
The translations in our training data serve as posi-
tive supervision, and the source language words in
the training data paired with random English words5
serve as negative supervision. We used our develop-
ment data to tune the number of negative examples
to three for each positive example. At test time, af-
ter scoring all source language words in the test set
paired with all English words in our candidate set,6
we rank the English candidates by their classifica-
tion scores and evaluate accuracy in the top-k trans-
lations.
</bodyText>
<subsectionHeader confidence="0.927411">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.99855">
Our monolingual features are listed below and are
based on raw similarity scores as well as ranks:
</bodyText>
<listItem confidence="0.995772666666667">
• Crawls Context: Web crawl context similarity score
• Crawls Context RR: reciprocal rank of crawls con-
text
</listItem>
<footnote confidence="0.9483105">
3We tried using logistic rather than linear regression, but
performance differences on our development set were very
small and not statistically significant.
4We use http://hunch.net/˜vw/ version 6.1.4, and
run it with the following arguments that affect how updates are
made in learning: –exact adaptive norm –power t 0.5
5Among those that appear at least five times in our monolin-
gual data, consistent with our candidate set.
6All English words appearing at least five times in our
monolingual data. In practice, we further limit the set to those
that occur in the top-1000 ranked list according to at least one
of our signals.
</footnote>
<page confidence="0.909484">
520
</page>
<table confidence="0.802767833333333">
●
●
0000
●
Crawl Edit Crawl Wiki Wiki Is− Diff Discrim
Context Dist Time Context Topic Ident. Lg−Frq All
</table>
<figureCaption confidence="0.760632">
Figure 1: Each box-and-whisker plot summarizes per-
formance on the development set using the given fea-
ture(s) across all 22 languages. For each source word
</figureCaption>
<bodyText confidence="0.973188">
in our development sets, we rank all English target words
according to the monolingual similarity metric(s) listed.
All but the last plot show the performance of individual
features. Discrim-All uses supervised data to train classi-
fiers for each language based on all of the features.
</bodyText>
<listItem confidence="0.998554333333333">
• Crawls Time: Web crawl temporal similarity score
• Crawls Time RR: reciprocal rank of crawls time
• Edit distance: normalized (by average length of
source and target word) edit distance
• Edit distance RR: reciprocal rank of edit distance
• Wiki Context: Wikipedia context similarity score
• Wiki Context RR: recip. rank of wiki context
• Wiki Topic: Wikipedia topic similarity score
• Wiki Topic RR: recip. rank of wiki topic
• Is-Identical: source and target words are the same
• Difference in log frequencies: Difference between
the logs of the source and target word monolingual
frequencies
• Log Freqs Diff RR: reciprocal rank of difference in
log frequencies
</listItem>
<bodyText confidence="0.99974625">
We train classifiers separately for each source lan-
guage, and the learned weights vary based on, for
example, corpora size and the relatedness of the
source language and English (e.g. edit distance is
informative if there are many cognates). In order to
use the trained classifiers to make top-k translation
predictions for a given source word, we rank candi-
dates by their classification scores.
</bodyText>
<subsectionHeader confidence="0.998889">
4.4 Feature Evaluation and Selection
</subsectionHeader>
<bodyText confidence="0.995011">
After training initial classifiers, we use our develop-
ment data to choose the most informative subset of
features. Figure 1 shows the top-10 accuracy on the
development data when we use individual features
</bodyText>
<table confidence="0.843542333333333">
●
Wiki Wiki Diff Edit Edit Crawl All
Topic Context Log−Freq Dist. Dist. RR Context Features
</table>
<figureCaption confidence="0.997823">
Figure 2: Performance on the development set goes up
as features are greedily added to the feature space. Mean
performance is slightly higher using this subset of six fea-
tures (second to last bar) than using all features (last bar).
</figureCaption>
<bodyText confidence="0.998744047619048">
to predict translations. Top-10 accuracy refers to the
percent of source language words for which a correct
English translation appears in the top-10 ranked En-
glish candidates. Each box-and-whisker plot sum-
marizes performance over the 22 languages. We
don’t display reciprocal rank features, as their per-
formance is very similar to that of the correspond-
ing raw similarity score. It’s easy to see that features
based on the Wikipedia topic signal are the most in-
formative. It is also clear that training a supervised
model to combine all of the features (the last plot)
yields performance that is dramatically higher than
using any individual feature alone.
Figure 2, from left to right, shows a greedy search
for the best subset of features among those listed
above. Again, the Wikipedia topic score is the most
informative stand-alone feature, and the Wikipedia
context score is the most informative second feature.
Adding features to the model beyond the six shown
in the figure does not yield additional performance
gains over our set of languages.
</bodyText>
<subsectionHeader confidence="0.998371">
4.5 Learning Curve Analysis
</subsectionHeader>
<bodyText confidence="0.999601625">
Figure 3 shows learning curves over the number of
positive training instances. In all cases, the number
of randomly generated negative training instances
is three times the number of positive. For all lan-
guages, performance is stable after about 300 cor-
rect translations are used for training. This shows
that our supervised method for combining signals
requires only a small training dictionary.
</bodyText>
<figure confidence="0.972603588235294">
1.0
0.8
0.6
0.4
0.2
Accuracy in Top−10
0000
0.0
1.0
0.8
0.6
0.4
0.2
Accuracy in Top−10
0.0
521
Positive training data instances
</figure>
<figureCaption confidence="0.869359">
Figure 3: Learning curves over number of positive train-
ing instances, up to 1250. For some languages, 1250
positive training instances are not available. In all cases,
evaluation is on the development data and the number of
negative training instances is three times the number of
positive. For all languages, performance is fairly stable
after about 300 positive training instances.
</figureCaption>
<sectionHeader confidence="0.999773" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999974041666666">
We use a model based on the six features shown
in Figure 2 to score and rank English translation
candidates for the test set words in each language.
Table 2 gives the result for each language for the
MRR baseline and our supervised technique. Across
languages, the average top-10 accuracy using the
MRR baseline is 30.4, and the average using our
technique is 43.9, a relative improvement of about
44%. We did not attempt a comparison with more
sophisticated unsupervised rank aggregation meth-
ods. However, we believe the improvements we
observe drastically outweigh the expected perfor-
mance differences between different rank aggrega-
tion methods. Figure 4 plots the accuracies yielded
by our supervised technique versus the total amount
of monolingual data for each language. An increase
in monolingual data tends to improve accuracy. The
correlation isn’t perfect, however. For example, per-
formance on Urdu and Farsi is relatively poor, de-
spite the large amounts of monolingual data avail-
able for each. This may be due to the fact that we
have large web crawls for those languages, but their
Wikipedia datasets, which tend to provide a strong
topic signal, are relatively small.
</bodyText>
<figure confidence="0.6245655">
1e−01 1e+00 1e+01 1e+02 1e+03
Millions of Monolingual Word Tokens
</figure>
<figureCaption confidence="0.9484925">
Figure 4: Millions of monolingual word tokens vs. Lex-
icon Induction Top-10 Accuracy
</figureCaption>
<table confidence="0.999748666666667">
Lang MRR Supv. Lang MRR Supv.
Nepali 11.2 13.6 Somali 16.7 18.1
Uzbek 23.2 29.6 Azeri 16.1 29.4
Tamil 28.4 33.3 Albanian 32.0 45.3
Bengali 19.3 32.8 Welsh 36.1 56.4
Bosnian 32.6 52.8 Latvian 29.6 47.7
Indonesian 41.5 63.5 Romanian 53.3 71.6
Serbian 29.0 33.3 Turkish 31.4 52.1
Ukrainian 29.7 46.0 Hindi 18.2 34.6
Bulgarian 40.2 57.9 Polish 47.4 67.1
Slovak 34.6 53.5 Urdu 13.2 21.2
Farsi 10.5 21.1 Spanish 74.8 85.0
</table>
<tableCaption confidence="0.989118333333333">
Table 2: Top-10 Accuracy on test set. Performance
increases for all languages moving from the baseline
(MRR) to discriminative training (Supv).
</tableCaption>
<sectionHeader confidence="0.994309" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999904">
On average, we observe relative gains of more than
44% over an unsupervised rank-combination base-
line by using a seed bilingual dictionary and a di-
verse set of monolingual signals to train a supervised
classifier. Using supervision for bilingual lexicon in-
duction makes sense. In some cases a dictionary is
already assumed for computing contextual similar-
ity, and, in the remaining cases, one could be com-
piled easy, either automatically, e.g. Haghighi et al.
(2008), or through crowdsourcing, e.g. Irvine and
Klementiev (2010) and Callison-Burch and Dredze
(2010). We have shown that only a few hundred
translation pairs are needed to achieve good perfor-
mance. Our framework has the additional advantage
that any new monolingually-derived similarity met-
rics can easily be added as new features.
</bodyText>
<figure confidence="0.99962884">
1.0
0.8
Accuracy in Top−10
0.6
0.4
0.2
●●● ●●● ●
● ● ●
● ●● ● ● ●●
● ●
● ●
●
● ● ●
●
●
● ● ●
● ●
● ●
●●●●●
●
●●
●●
● ●
● ●
●
● ● ●●
●
●
● ●
●
● ●
●
● ●
● ●
●
0.0
●
●
●
●●●
●
0 200 400 600 800 1000 1200
Spanish
Romanian
Polish
Indonesian
vak
Albanian
Bulgarian
Welsh
BosnianTurkish
Slo
Ukranian
Latvian
Serbian
Hindi
Tamil
Bengali
Uzbek Azeri
Urdu
Farsi
Somali
Nepali
Spanish
Romanian
Polish
Bulgarian
Indonesian
Welsh
Slovak
Bosnian
Latvian
Albanian
Ukrainian
Turkish
Azeri
Serbian
Hindi
Bengali
Uzbek
Farsi
Somali
Tamil
Urdu
Nepali
●
●
●
●
●
●
●
●
●
1.0
0.8
Accuracy 0.6
0.4
0.2
0.0
</figure>
<page confidence="0.986668">
522
</page>
<sectionHeader confidence="0.997114" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998135">
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and by
the Johns Hopkins University Human Language
Technology Center of Excellence. The views and
conclusions contained in this publication are those
of the authors and should not be interpreted as repre-
senting official policies or endorsements of DARPA
or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.998396" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999501109756098">
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In Proceedings of the NAACL Workshop on Cre-
ating Speech and Language Data with Amazon’s Me-
chanical Turk.
Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the Conference of the As-
sociation for Computational Linguistics (ACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Ann Irvine and Alexandre Klementiev. 2010. Using me-
chanical turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk.
Ann Irvine, Chris Callison-Burch, and Alexandre Kle-
mentiev. 2010. Transliterating from all languages. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas (AMTA).
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Alex Klementiev, Ann Irvine, Chris Callison-Burch, and
David Yarowsky. 2012. Toward statistical machine
translation without parallel corpora. In Proceedings of
the Conference of the European Association for Com-
putational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In ACL
Workshop on Unsupervised Lexical Acquisition.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Kobi Reiter, Michael Skinner, Marcus Sammer,
and Jeff Bilmes. 2010. Panlingual lexical transla-
tion via probabilistic inference. Artificial Intelligence,
174:619–637, June.
David Mimno, Hanna Wallach, Jason Naradowsky, David
Smith, and Andrew McCallum. 2009. Polylingual
topic models. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining language
models and context vectors. In Proceedings of the
Conference of the Association for Computational Lin-
guistics (ACL).
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguistics
(ACL).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the Conference of the Associ-
ation for Computational Linguistics (ACL).
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the Conference of
the Association for Computational Linguistics (ACL).
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Charles Schafer. 2006. Translation Discovery Using Di-
verse Similarity Measures. Ph.D. thesis, Johns Hop-
kins University.
</reference>
<page confidence="0.998932">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.803268">
<title confidence="0.996808">Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals</title>
<author confidence="0.990261">Irvine Chris</author>
<affiliation confidence="0.9052705">Center for Language and Speech Processing Computer and Information Science Dept. Johns Hopkins University University of Pennsylvania</affiliation>
<abstract confidence="0.999712304347826">Prior research into learning translations from source and target language monolingual texts has treated the task as an unsupervised learning problem. Although many techniques take advantage of a seed bilingual lexicon, this work is the first to use that data for supervised learning to combine a diverse set of signals derived from a pair of monolingual corpora into a single discriminative model. Even in a low resource machine translation setting, where induced translations have the potential to improve performance substantially, it is reasonable to assume access to some amount of data to perform this kind of optimization. Our work shows that only a few hundred translation pairs are needed to achieve strong performance on the bilingual lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="18680" citStr="Callison-Burch and Dredze (2010)" startWordPosition="2979" endWordPosition="2982">the baseline (MRR) to discriminative training (Supv). 6 Conclusions On average, we observe relative gains of more than 44% over an unsupervised rank-combination baseline by using a seed bilingual dictionary and a diverse set of monolingual signals to train a supervised classifier. Using supervision for bilingual lexicon induction makes sense. In some cases a dictionary is already assumed for computing contextual similarity, and, in the remaining cases, one could be compiled easy, either automatically, e.g. Haghighi et al. (2008), or through crowdsourcing, e.g. Irvine and Klementiev (2010) and Callison-Burch and Dredze (2010). We have shown that only a few hundred translation pairs are needed to achieve good performance. Our framework has the additional advantage that any new monolingually-derived similarity metrics can easily be added as new features. 1.0 0.8 Accuracy in Top−10 0.6 0.4 0.2 ●●● ●●● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●●●● ● ●● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● 0.0 ● ● ● ●●● ● 0 200 400 600 800 1000 1200 Spanish Romanian Polish Indonesian vak Albanian Bulgarian Welsh BosnianTurkish Slo Ukranian Latvian Serbian Hindi Tamil Bengali Uzbek Azeri Urdu Farsi Somali Nepali S</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Dou</author>
<author>Kevin Knight</author>
</authors>
<title>Large scale decipherment for out-of-domain machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="4817" citStr="Dou and Knight (2012)" startWordPosition="730" endWordPosition="733">ilarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. However, both of those appr</context>
</contexts>
<marker>Dou, Knight, 2012</marker>
<rawString>Qing Dou and Kevin Knight. 2012. Large scale decipherment for out-of-domain machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4275" citStr="Fung and Yee, 1998" startWordPosition="646" endWordPosition="649"> metrics derived from monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phr</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4404" citStr="Haghighi et al., 2008" startWordPosition="666" endWordPosition="669">or the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed</context>
<context position="18582" citStr="Haghighi et al. (2008)" startWordPosition="2966" endWordPosition="2969">ble 2: Top-10 Accuracy on test set. Performance increases for all languages moving from the baseline (MRR) to discriminative training (Supv). 6 Conclusions On average, we observe relative gains of more than 44% over an unsupervised rank-combination baseline by using a seed bilingual dictionary and a diverse set of monolingual signals to train a supervised classifier. Using supervision for bilingual lexicon induction makes sense. In some cases a dictionary is already assumed for computing contextual similarity, and, in the remaining cases, one could be compiled easy, either automatically, e.g. Haghighi et al. (2008), or through crowdsourcing, e.g. Irvine and Klementiev (2010) and Callison-Burch and Dredze (2010). We have shown that only a few hundred translation pairs are needed to achieve good performance. Our framework has the additional advantage that any new monolingually-derived similarity metrics can easily be added as new features. 1.0 0.8 Accuracy in Top−10 0.6 0.4 0.2 ●●● ●●● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●●●● ● ●● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● 0.0 ● ● ● ●●● ● 0 200 400 600 800 1000 1200 Spanish Romanian Polish Indonesian vak Albanian Bulgarian Welsh Bosn</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using mechanical turk to annotate lexicons for less commonly used languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="18643" citStr="Irvine and Klementiev (2010)" startWordPosition="2974" endWordPosition="2977">es for all languages moving from the baseline (MRR) to discriminative training (Supv). 6 Conclusions On average, we observe relative gains of more than 44% over an unsupervised rank-combination baseline by using a seed bilingual dictionary and a diverse set of monolingual signals to train a supervised classifier. Using supervision for bilingual lexicon induction makes sense. In some cases a dictionary is already assumed for computing contextual similarity, and, in the remaining cases, one could be compiled easy, either automatically, e.g. Haghighi et al. (2008), or through crowdsourcing, e.g. Irvine and Klementiev (2010) and Callison-Burch and Dredze (2010). We have shown that only a few hundred translation pairs are needed to achieve good performance. Our framework has the additional advantage that any new monolingually-derived similarity metrics can easily be added as new features. 1.0 0.8 Accuracy in Top−10 0.6 0.4 0.2 ●●● ●●● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●●●● ● ●● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● 0.0 ● ● ● ●●● ● 0 200 400 600 800 1000 1200 Spanish Romanian Polish Indonesian vak Albanian Bulgarian Welsh BosnianTurkish Slo Ukranian Latvian Serbian Hindi Tamil Bengali U</context>
</contexts>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using mechanical turk to annotate lexicons for less commonly used languages. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Transliterating from all languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="6626" citStr="Irvine et al. (2010)" startWordPosition="1015" endWordPosition="1018">tokens cluding contextual and string similarity, and directly learn how to combine them. 3 Monolingual Data and Signals 3.1 Data Throughout our experiments, we seek to learn how to translate words in a given source language into English. Table 1 lists our languages of interest, along with the total amount of monolingual data that we use for each. We use web crawled timestamped news articles to estimate temporal similarity, Wikipedia pages which are inter-lingually linked to English pages to estimate topic similarity, and both datasets to estimate frequency and contextual similarity. Following Irvine et al. (2010), we use pairs of Wikipedia page titles to train a simple transliterator for languages written in a non-Roman script, which allows us to compute orthographic similarity for pairs of words in different scripts. 3.2 Signals Our definitions of orthographic, topic, temporal, and contextual similarity are taken from Klementiev et al. (2012), and the details of each may be found there. Here, we give briefly describe them and give our definition of a novel, frequency-based signal. Orthographic We measure orthographic similarity between a pair of words as the normalized1 edit distance between the two </context>
</contexts>
<marker>Irvine, Callison-Burch, Klementiev, 2010</marker>
<rawString>Ann Irvine, Chris Callison-Burch, and Alexandre Klementiev. 2010. Transliterating from all languages. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Weakly supervised named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2035" citStr="Klementiev and Roth, 2006" startWordPosition="308" endWordPosition="311">k are based on the idea that words that are translations of one another have similar distributional properties across languages. Prior research has shown that contextual similarity (Rapp, 1995), temporal similarity (Schafer and Yarowsky, 2002), and topical information (Mimno et al., 2009) ⇤Performed while faculty at Johns Hopkins University are all good signals for learning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney, 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp, 1995), and that same dictionary may be used for est</context>
<context position="4357" citStr="Klementiev and Roth, 2006" startWordPosition="658" endWordPosition="661">temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment tech</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Klementiev</author>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="4472" citStr="Klementiev et al. (2012)" startWordPosition="678" endWordPosition="681">al signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals </context>
<context position="6963" citStr="Klementiev et al. (2012)" startWordPosition="1066" endWordPosition="1069">e use for each. We use web crawled timestamped news articles to estimate temporal similarity, Wikipedia pages which are inter-lingually linked to English pages to estimate topic similarity, and both datasets to estimate frequency and contextual similarity. Following Irvine et al. (2010), we use pairs of Wikipedia page titles to train a simple transliterator for languages written in a non-Roman script, which allows us to compute orthographic similarity for pairs of words in different scripts. 3.2 Signals Our definitions of orthographic, topic, temporal, and contextual similarity are taken from Klementiev et al. (2012), and the details of each may be found there. Here, we give briefly describe them and give our definition of a novel, frequency-based signal. Orthographic We measure orthographic similarity between a pair of words as the normalized1 edit distance between the two words. For non-Roman script languages, we transliterate words into the Roman script before measuring orthographic similarity. Topic We use monolingual Wikipedia pages to estimate topical signatures for each source and target 1Normalized by the average of the lengths of the two words 519 language word. Signature vectors are the length o</context>
</contexts>
<marker>Klementiev, Irvine, Callison-Burch, Yarowsky, 2012</marker>
<rawString>Alex Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky. 2012. Toward statistical machine translation without parallel corpora. In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Unsupervised Lexical Acquisition.</booktitle>
<contexts>
<context position="4381" citStr="Koehn and Knight, 2002" startWordPosition="662" endWordPosition="665">phic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to t</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL Workshop on Unsupervised Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Kobi Reiter</author>
<author>Michael Skinner</author>
<author>Marcus Sammer</author>
<author>Jeff Bilmes</author>
</authors>
<title>Panlingual lexical translation via probabilistic inference.</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<volume>174</volume>
<contexts>
<context position="4446" citStr="Mausam et al., 2010" startWordPosition="674" endWordPosition="677">n to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and mul</context>
</contexts>
<marker>Mausam, Etzioni, Weld, Reiter, Skinner, Sammer, Bilmes, 2010</marker>
<rawString>Mausam, Stephen Soderland, Oren Etzioni, Daniel S. Weld, Kobi Reiter, Michael Skinner, Marcus Sammer, and Jeff Bilmes. 2010. Panlingual lexical translation via probabilistic inference. Artificial Intelligence, 174:619–637, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Jason Naradowsky</author>
<author>David Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1698" citStr="Mimno et al., 2009" startWordPosition="258" endWordPosition="261"> nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 1 Introduction Bilingual lexicon induction is the task of identifying word translation pairs using source and target monolingual corpora, which are often comparable. Most approaches to the task are based on the idea that words that are translations of one another have similar distributional properties across languages. Prior research has shown that contextual similarity (Rapp, 1995), temporal similarity (Schafer and Yarowsky, 2002), and topical information (Mimno et al., 2009) ⇤Performed while faculty at Johns Hopkins University are all good signals for learning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised mo</context>
<context position="4424" citStr="Mimno et al., 2009" startWordPosition="670" endWordPosition="673">ore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon f</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David Mimno, Hanna Wallach, Jason Naradowsky, David Smith, and Andrew McCallum. 2009. Polylingual topic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Deciphering foreign language by combining language models and context vectors.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4841" citStr="Nuhn et al. (2012)" startWordPosition="735" endWordPosition="738">al lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. However, both of those approaches focus on only two</context>
</contexts>
<marker>Nuhn, Mauser, Ney, 2012</marker>
<rawString>Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2399" citStr="Och and Ney, 2002" startWordPosition="354" endWordPosition="357">ning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised models dramatically outperform unsupervised models, including for closely related problems like statistical machine translation (Och and Ney, 2002). For the bilingual lexicon induction task, a supervised approach is natural, particularly because computing contextual similarity typically requires a seed bilingual dictionary (Rapp, 1995), and that same dictionary may be used for estimating the parameters of a model to combine monolingual signals. Alternatively, in a low resource machine translation (MT) setting, it is reasonable to assume a small amount of parallel data from which a bilingual dictionary can be extracted for supervision. In this setting, bilingual lexicon induction is critical for translating source words which do not appea</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1602" citStr="Rapp, 1995" startWordPosition="247" endWordPosition="248"> lexicon induction task, and our approach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 1 Introduction Bilingual lexicon induction is the task of identifying word translation pairs using source and target monolingual corpora, which are often comparable. Most approaches to the task are based on the idea that words that are translations of one another have similar distributional properties across languages. Prior research has shown that contextual similarity (Rapp, 1995), temporal similarity (Schafer and Yarowsky, 2002), and topical information (Mimno et al., 2009) ⇤Performed while faculty at Johns Hopkins University are all good signals for learning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of mach</context>
<context position="4255" citStr="Rapp, 1995" startWordPosition="644" endWordPosition="645">g similarity metrics derived from monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked t</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4287" citStr="Rapp, 1999" startWordPosition="650" endWordPosition="651">m monolingual corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based tr</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4794" citStr="Ravi and Knight (2011)" startWordPosition="726" endWordPosition="729">l, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, relying on decipherment techniques. In contrast to that work, we use a seed bilingual lexicon for supervision and multiple monolingual signals proposed in prior work. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) use some supervision to learn how to project contextual and orthographic features into a low-dimensional space, with the goal of representing words which are translations of one another as vectors which are close together in that space. Howe</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011. Deciphering foreign language. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="1652" citStr="Schafer and Yarowsky, 2002" startWordPosition="251" endWordPosition="254">pproach yields an average relative gain in accuracy of nearly 50% over an unsupervised baseline. Large gains in accuracy hold for all 22 languages (low and high resource) that we investigate. 1 Introduction Bilingual lexicon induction is the task of identifying word translation pairs using source and target monolingual corpora, which are often comparable. Most approaches to the task are based on the idea that words that are translations of one another have similar distributional properties across languages. Prior research has shown that contextual similarity (Rapp, 1995), temporal similarity (Schafer and Yarowsky, 2002), and topical information (Mimno et al., 2009) ⇤Performed while faculty at Johns Hopkins University are all good signals for learning translations from monolingual texts. Most prior work either makes use of only one or two monolingual signals or uses unsupervised methods (like rank combination) to aggregate orthogonal signals (Schafer and Yarowsky, 2002; Klementiev and Roth, 2006). Surprisingly, no past research has employed supervised approaches to combine diverse monolingually-derived signals for bilingual lexicon induction. The field of machine learning has shown decisively that supervised </context>
<context position="4315" citStr="Schafer and Yarowsky, 2002" startWordPosition="652" endWordPosition="655">l corpora to score word pairs: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolin</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
</authors>
<title>Translation Discovery Using Diverse Similarity Measures.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="4330" citStr="Schafer, 2006" startWordPosition="656" endWordPosition="657">s: contextual, temporal, topical, orthographic, and frequency. • For the first time, explore using supervision to combine monolingual signals and learn a discriminative model for predicting translations. • Present results for 22 low and high resource languages paired with English and show large accuracy gains over an unsupervised baseline. 2 Previous Work Prior work suggests that a wide variety of monolingual signals, including distributional, temporal, topic, and string similarity, may inform bilingual lexicon induction (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010). Klementiev et al. (2012) use many of those signals to score an existing phrase table for end-to-end MT but do not learn any new translations. Schafer and Yarowsky (2002) use an unsupervised rank-combination method for combining orthographic, contextual, temporal, and frequency similarities into a single ranking. Recently, Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012) have worked toward learning a phrase-based translation model from monolingual corpora, r</context>
</contexts>
<marker>Schafer, 2006</marker>
<rawString>Charles Schafer. 2006. Translation Discovery Using Diverse Similarity Measures. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>