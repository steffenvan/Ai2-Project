<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001904">
<title confidence="0.9971925">
Joint Identification and Segmentation of Domain-Specific Dialogue Acts for
Conversational Dialogue Systems
</title>
<author confidence="0.994382">
Fabrizio Morbini and Kenji Sagae
</author>
<affiliation confidence="0.997386">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.940526">
12015 Waterfront Drive, Playa Vista, CA 90094
</address>
<email confidence="0.999827">
{morbini,sagae}@ict.usc.edu
</email>
<sectionHeader confidence="0.997413" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998861">
Individual utterances often serve multiple
communicative purposes in dialogue. We
present a data-driven approach for identifica-
tion of multiple dialogue acts in single utter-
ances in the context of dialogue systems with
limited training data. Our approach results in
significantly increased understanding of user
intent, compared to two strong baselines.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992789392857143">
Natural language understanding (NLU) at the level
of speech acts for conversational dialogue systems
can be performed with high accuracy in limited do-
mains using data-driven techniques (Bender et al.,
2003; Sagae et al., 2009; Gandhe et al., 2008, for
example), provided that enough training material is
available. For most systems that implement novel
conversational scenarios, however, enough exam-
ples of user utterances, which can be annotated as
NLU training data, only become available once sev-
eral users have interacted with the system. This situ-
ation is typically addressed by bootstrapping from a
relatively small set of hand-authored utterances that
perform key dialogue acts in the scenario or from
utterances collected from wizard-of-oz or role-play
exercises, and having NLU accuracy increase over
time as more users interact with the system and more
utterances are annotated for NLU training.
While this can be effective in practice for ut-
terances that perform only one of several possible
system-specific dialogue acts (often several dozens),
longer utterances that include multiple dialogue acts
pose a greater challenge: the many available combi-
nations of dialogue acts per utterance result in sparse
95
coverage of the space of possibilities, unless a very
large amount of data can be collected and anno-
tated, which is often impractical. Users of the dia-
logue system, whose utterances are collected for fur-
ther NLU improvement, tend to notice that portions
of their longer utterances are ignored and that they
are better understood when they express themselves
with simpler sentences. This results in generation of
data heavily skewed towards utterances that corre-
spond to a single dialogue act, making it difficult to
collect enough examples of utterances with multiple
dialogue acts to improve NLU, which is precisely
what would be needed to make users feel more com-
fortable with using longer utterances.
We address this chicken-and-egg problem with a
data-driven NLU approach that segments and iden-
tifies multiple dialogue acts in single utterances,
even when only short (single dialogue act) utter-
ances are available for training. In contrast to previ-
ous approaches that assume the existence of enough
training data for learning to segment utterances,
e.g. (Stolcke and Shriberg, 1996), or to align spe-
cific words to parts of the formal representation,
e.g. (Bender et al., 2003), our framework requires a
relatively small dataset, which may not contain any
utterances with multiple dialogue acts. This makes it
possible to create new conversational dialogue sys-
tem scenarios that allow and encourage users to ex-
press themselves with fewer restrictions, without an
increased burden in the collection and annotation of
NLU training data.
</bodyText>
<sectionHeader confidence="0.970529" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.9333115">
Given (1) a predefined set of possible dialogue acts
for a specific dialogue system, (2) a set of utterances
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 95–100,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
each annotated with a single dialogue act label, and
(3) a classifier trained on this annotated utterance-
label set, which assigns for a given word sequence a
dialogue act label with a corresponding confidence
score, our task is to find the best sequence of dia-
logue acts that covers a given input utterance. While
short utterances are likely to be covered entirely by a
single dialogue act that spans all of its words, longer
utterances may be composed of spans that corre-
spond to different dialogue acts.
</bodyText>
<equation confidence="0.984864153846154">
bestDialogueActEndingAt(Text,pos) begin
if pos &lt; 0 then
return (pos, (null, 1));
end
S = {};
for j = 0 to pos do
(c, p) = classify(words(Text, j, pos));
S = S U {(j, (c, p))};
end
return argmax {p · p : (h, (c�, p)) =
(k,(c,p))ES
bestDialogueActEndingAt(Text, k − 1)};
end
</equation>
<bodyText confidence="0.995082933333333">
Algorithm 1: The function classify(T) calls the
single dialogue act classifier subsystem on the in-
put text T and returns the highest scoring dia-
logue act label c with its confidence score p. The
function words(T, i, j) returns the string formed
by concatenating the words in T from the ith to
the jth included. To obtain the best segmenta-
tion of a given text, one has to work its way back
from the end of the text: start by calling (k, (c, p))
= bestDialogueActEndingAt(Text, numWords),
where numWords is the number of words
in Text. If k &gt; 0 recursively call
bestDialogueActEndingAt(Text, k − 1) to obtain
the optimal dialogue act ending at k − 1.
Algorithm 1 shows our approach for using a sin-
gle dialogue act classifier to extract the sequence of
dialogue acts with the highest overall score from a
given utterance. The framework is independent of
the particular subsystem used to select the dialogue
act label for a given segment of text. The constraint
is that this subsystem should return, for a given se-
quence of words, at least one dialogue act label and
its confidence level in a normalized range that can
be used for comparisons with subsequent runs. In
the work reported in this paper, we use an existing
data-driven NLU module (Sagae et al., 2009), de-
veloped for the SASO virtual human dialogue sys-
tem (Traum et al., 2008b), but retrained using the
data described in section 3. This NLU module per-
forms maximum entropy multiclass classification,
using features derived from the words in the input
utterance, and using dialogue act labels as classes.
The basic idea is to find the best segmentation
(that is, the one with the highest score) of the portion
of the input text up to the ith word. The base case Si
would be for i = 1 and it is the result of our classi-
fier when the input is the single first word. For any
other i &gt; 1 we construct all word spans Tj,i of the
input text, containing the words from j to i, where
1 &lt; j &lt; i, then we classify each of the Tj,i and
pick the best returned class (dialogue act label) Cj,i
(and associated score, which in the case of our maxi-
mum entropy classifier is the conditional probability
Score(Cj,i) = P(Cj,i|Tj,i)). Then we assign to the
best segmentation ending at i, Si, the label Ck,i iff:
</bodyText>
<equation confidence="0.997992">
k = argmax (Score(Ch,i) · Score(Sh_1)) (1)
1&lt;h&lt;i
</equation>
<bodyText confidence="0.999799555555556">
Algorithm 1 calls the classifier O(n2) where n
is the number of words in the input text. Note
that, as in the maximum entropy NLU of Bender et
al. (2003), this search uses the “maximum approxi-
mation,” and we do not normalize over all possible
sequences. Therefore, our scores are not true proba-
bilities, although they serve as a good approximation
in the search for the best overall segmentation.
We experimented with two other variations of
the argument of the argmax in equation 1: (1) in-
stead of considering Score(Sh_1), consider only
the last segment contained in Sh_1; and (2) instead
of using the product of the scores of all segments,
use the average score per segment: (Score(Ch,i) ·
Score(Sh_1))1/(1+N(Sh_,)) where N(Si) is the
number of segments in Si. These variants produce
similar results; the results reported in the next sec-
tion were obtained with the second variant.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.980193">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999888">
To evaluate our approach we used data collected
from users of the TACQ (Traum et al., 2008a) dia-
</bodyText>
<page confidence="0.946572">
96
</page>
<bodyText confidence="0.997917717391304">
logue system, as described by Artstein et al. (2009).
Of the utterances in that dataset, about 30% are an-
notated with multiple dialogue acts. The annotation
also contains for each dialogue act the correspond-
ing segment of the input utterance.
The dataset contains a total of 1,579 utterances.
Of these, 1,204 utterances contain only a single di-
alogue act, and 375 utterances contain multiple dia-
logue acts, according to manual dialogue act anno-
tation. Within the set of utterances that contain mul-
tiple dialogue acts, the average number of dialogue
acts per utterance is 2.3.
The dialogue act annotation scheme uses a total
of 77 distinct labels, with each label corresponding
to a domain-specific dialogue act, including some
semantic information. Each of these 77 labels is
composed at least of a core speech act type (e.g.
wh-question, offer), and possibly also attributes that
reflect semantics in the domain. For example, the
dialogue act annotation for the utterance What is
the strange man’s name? would be whq(obj:
strangeMan, attr: name), reflecting that
it is a wh-question, with a specific object and at-
tribute. In the set of utterances with only one speech
act, 70 of the possible 77 dialogue act labels are
used. In the remaining utterances (which contain
multiple speech acts per utterance), 59 unique dia-
logue act labels are used, including 7 that are not
used in utterances with only a single dialogue act
(these 7 labels are used in only 1% of those utter-
ances). A total of 18 unique labels are used only
in the set of utterances with one dialogue act (these
labels are used in 5% of those utterances). Table 1
shows the frequency information for the five most
common dialogue act labels in our dataset.
The average number of words in utterances with
only a single dialogue act is 7.5 (with a maximum
of 34, and minimum of 1), and the average length of
utterances with multiple dialogue acts is 15.7 (max-
imum of 66, minimum of 2). To give a better idea of
the dataset used here, we list below two examples of
utterances in the dataset, and their dialogue act an-
notation. We add word indices as subscripts in the
utterances for illustration purposes only, to facilitate
identification of the word spans for each dialogue
act. The annotation consists of a word interval and a
</bodyText>
<table confidence="0.997811666666667">
Single DA Utt. [%] Multiple DA Utt. [%]
Wh-questions 51 Wh-questions 31
Yes/No-questions 14 Offers to agent 24
Offers to agent 9 Yes answer 11
Yes answer 7 Yes/No-questions 8
Greeting 7 Thanks 7
</table>
<tableCaption confidence="0.996422">
Table 1: The frequency of the dialogue act classes most
</tableCaption>
<bodyText confidence="0.6366253">
used in the TACQ dataset (Artstein et al., 2009). The
left column reports the statistics for the set of utterances
annotated with a single dialogue act the right those for the
utterances annotated with multiple dialogue acts. Each
dialogue act class typically contains several more specific
dialogue acts that include domain-specific semantics (for
example, there are 29 subtypes of wh-questions that can
be performed in the domain, each with a separate domain-
specific dialogue act label).
dialogue act label1.
</bodyText>
<listItem confidence="0.9585607">
1. ( 0 his 1 name, 2 any 3 other 4 informa-
tion 5 about 6 him, 7 where 8 he 9 lives
10) is labeled with: [0 2] whq(obj:
strangeMan, attr: name), [2 7]
whq(obj: strangeMan) and [7 10]
whq(obj: strangeMan, attr:
location).
2. ( 0 I 1 can’t 2 offer 3 you 4 money 5 but 6 I 7 can
8 offer 9 you 10 protection 11) is labeled with:
[0 5] reject, [5 11] offer(safety).
</listItem>
<subsectionHeader confidence="0.995922">
3.2 Setup
</subsectionHeader>
<bodyText confidence="0.999987230769231">
In our experiments, we performed 10-fold cross-
validation using the dataset described above. For
the training folds, we use only utterances with a sin-
gle dialogue act (utterances containing multiple dia-
logue acts are split into separate utterances), and the
training procedure consists only of training a max-
imum entropy text classifier, which we use as our
single dialogue act classifier subsystem.
For each evaluation fold we run the procedure de-
scribed in Section 2, using the classifier obtained
from the corresponding training fold. The segments
present in the manual annotation are then aligned
with the segments identified by our system (the
</bodyText>
<footnote confidence="0.539879">
1Although the dialogue act labels could be thought of as
compositional, since they include separate parts, we treat them
as atomic labels.
</footnote>
<page confidence="0.999442">
97
</page>
<bodyText confidence="0.991387441860465">
alignment takes in consideration both the word span
and the dialogue act label associated to each seg-
ment). The evaluation then considers as correct only
the subset of dialogue acts identified automatically
that were successfully aligned with the same dia-
logue act label in the gold-standard annotation.
We compared the performance of our proposed
approach to two baselines; both use the same max-
imum entropy classifier used internally by our pro-
posed approach.
1. The first baseline simply uses the single dia-
logue act label chosen by the maximum entropy
classifier as the only dialogue act for each ut-
terance. In other words, this baseline corre-
sponds to the NLU developed for the SASO di-
alogue system (Traum et al., 2008b) by Sagae
et al. (2009)2. This baseline is expected to have
lower recall for those utterances that contain
multiple dialogue acts, but potentially higher
precision overall, since most utterances in the
dataset contain only one dialogue act label.
2. For the second baseline, we treat multiple dia-
logue act detection as a set of binary classifica-
tion tasks, one for each possible dialogue act la-
bel in the domain. We start from the same train-
ing data as above, and create N copies, where
N is the number of unique dialogue acts labels
in the training set. Each utterance-label pair in
the original training set is now present in all N
training sets. If in the original training set an ut-
terance was labeled with the ith dialogue act la-
bel, now it will be labeled as a positive example
in the ith training set and as a negative exam-
ple in all other training sets. Binary classifiers
for each N dialogue act labels are then trained.
During run-time, each utterance is classified by
all N models and the result is the subset of di-
alogue acts associated with the models that la-
beled the example as positive. This baseline is
excepted to be much closer in performance to
our approach, but it is incapable of determining
what words in the utterance correspond to each
dialogue act3.
</bodyText>
<footnote confidence="0.99858925">
2We do not use the incremental processing version of the
NLU described by Sagae et al., only the baseline NLU, which
consist only of a maximum entropy classifier.
3This corresponds to the transformation of a multi-label
</footnote>
<table confidence="0.9998657">
P [%] R [%] F [%]
Single this 73 77 75
2ndbl 86 71 78
1stbl 82 77 80
Multiple this 87 66 75
2ndbl 85 55 67
1stbl 91 39 55
Overall this 78 72 75
2ndbl 86 64 73
1stbl 84 61 71
</table>
<tableCaption confidence="0.477348375">
Table 2: Performance on the TACQ dataset obtained by
our proposed approach (denoted by “this”) and the two
baseline methods. Single indicates the performance when
tested only on utterances annotated with a single dialogue
act. Multiple is for utterances annotated with more than
one dialogue act, and Overall indicates the performance
over the entire set. P stands for precision, R for recall,
and F for F-score.
</tableCaption>
<subsectionHeader confidence="0.958122">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.992442583333333">
Table 2 shows the performance of our approach and
the two baselines. All measures show that the pro-
posed approach has considerably improved perfor-
mance for utterances that contain multiple dialogue
acts, with only a small increase in the number of er-
rors for the utterances containing only a single dia-
logue act. In fact, even though more than 70% of
the utterances in the dataset contain only a single di-
alogue act, our approach for segmenting and iden-
tifying multiple dialogue acts increases overall F-
score by about 4% when compared to the first base-
line and by about 2% when compared to the sec-
ond (strong) baseline, which suffers from the addi-
tional deficiency of not identifying what spans cor-
respond to what dialogue acts. The differences in
F-score over the entire dataset (shown in the Over-
all portion of Table 2) are statistically significant
(p &lt; 0.05). As a drawback of our approach, it
is on average 25 times slower than our first base-
line, which is incapable of identifying multiple di-
alogue acts in a utterance4. Our approach is still
about 15% faster than our second baseline, which
classification problem into several binary classifiers, described
as PT4 by Tsoumakas and Katakis (?).
</bodyText>
<footnote confidence="0.9367185">
4In our dataset, our method takes on average about 102ms
to process an utterance that was originally labeled with multiple
dialogue acts, and 12ms to process one annotated with a single
dialogue act.
</footnote>
<page confidence="0.994453">
98
</page>
<figure confidence="0.985915470588235">
500
400
300
200
100
Execution time [ms]
Histogram (number of utterances)
0
this
1&amp;quot;bl
2&amp;quot;dbl
histogram
0 10 20 30 40 50 60 70
Number of words in input text
Multiple speech acts: µ=1.64 a=2.22
All data: µ=1.07 a=1.69
Single speech act: µ=0.72 a=1.12
</figure>
<figureCaption confidence="0.9905915">
Figure 1: Execution time in milliseconds of the classifier
with respect to the number of words in the input text.
</figureCaption>
<bodyText confidence="0.999947166666667">
identifies multiple speech acts, but without segmen-
tation, and with lower F-score. Figure 1 shows the
execution time versus the length of the input text. It
also shows a histogram of utterance lengths in the
dataset, suggesting that our approach is suitable for
most utterances in our dataset, but may be too slow
for some of the longer utterances (with 30 words or
more).
Figure 2 shows the histogram of the average error
(absolute value of word offset) in the start and end
of the dialogue act segmentation. Each dialogue act
identified by Algorithm 1 is associated with a start-
ing and ending index that corresponds to the por-
tion of the input text that has been classified with
the given dialogue act. During the evaluation, we
find the best alignment between the manual annota-
tion and the segmentation we computed. For each
of the aligned pairs (i.e. extracted dialogue act and
dialogue act present in the annotation) we compute
the absolute error between the starting point of the
extracted dialogue act and the starting point of the
paired annotation. We do the same for the ending
point and we average the two error figures. The
result is binned to form the histogram displayed in
figure 2. The figure also shows the average error
and the standard deviation. The largest average er-
ror happens with the data annotated with multiple
dialogue acts. In that case, the extracted segments
have a starting and ending point that in average are
misplaced by about f2 words.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9994375">
We described a method to segment a given utter-
ance into non-overlapping portions, each associated
</bodyText>
<figure confidence="0.690534">
0 1 2 3 4 5 6 7 8 9 10
Average error in the starting and ending indexes of each speech act segment
</figure>
<figureCaption confidence="0.992523333333333">
Figure 2: Histogram of the average absolute error in the
two extremes (i.e. start and end) of segments correspond-
ing to the dialogue acts identified in the dataset.
</figureCaption>
<bodyText confidence="0.999985095238095">
with a dialogue act. The method addresses the prob-
lem that, in development of new scenarios for con-
versational dialogue systems, there is typically not
enough training data covering all or most configu-
rations of how multiple dialogue acts appear in sin-
gle utterances. Our approach requires only labeled
utterances (or utterance segments) corresponding to
a single dialogue act, which tends to be the easiest
type of training data to author and to collect.
We performed an evaluation using existing data
annotated with multiple dialogue acts for each utter-
ance. We showed a significant improvement in over-
all performance compared to two strong baselines.
The main drawback of the proposed approach is the
complexity of the segment optimization that requires
calling the dialogue act classifier 0(n2) times with
n representing the length of the input utterance. The
benefit, however, is that having the ability to identify
multiple dialogue acts in utterances takes us one step
closer towards giving users more freedom to express
themselves naturally with dialogue systems.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991625">
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred. We would also like to thank the anonymous
reviewers for their helpful comments.
</bodyText>
<page confidence="0.998162">
99
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811975609756">
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009. Viability of a simple dialogue
act scheme for a tactical questioning dialogue system.
In DiaHolmia 2009: Proceedings of the 13th Work-
shop on the Semantics and Pragmatics of Dialogue,
page 43–50, Stockholm, Sweden, June.
Oliver Bender, Klaus Macherey, Franz Josef Och, and
Hermann Ney. 2003. Comparison of alignment tem-
plates and maximum entropy models for natural lan-
guage understanding. In Proceedings of the tenth
conference on European chapter of the Association
for Computational Linguistics - Volume 1, EACL ’03,
pages 11–18, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Sudeep Gandhe, David DeVault, Antonio Roque, Bilyana
Martinovski, Ron Artstein, Anton Leuski, Jillian
Gerten, and David R. Traum. 2008. From domain
specification to virtual humans: An integrated ap-
proach to authoring tactical questioning characters.
In Proceedings of Interspeech, Brisbane, Australia,
September.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results in
dialogue systems. In Short Paper Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT) 2009 conference.
Andreas Stolcke and Elizabeth Shriberg. 1996. Au-
tomatic linguistic segmentation of conversational
speech. In Proc. ICSLP, pages 1005–1008.
David R. Traum, Anton Leuski, Antonio Roque, Sudeep
Gandhe, David DeVault, Jillian Gerten, Susan Robin-
son, and Bilyana Martinovski. 2008a. Natural lan-
guage dialogue architectures for tactical questioning
characters. In Army Science Conference, Florida,
12/2008.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008b. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal vir-
tual agents. In IVA, pages 117–130.
</reference>
<page confidence="0.990754">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897923">
<title confidence="0.99866">Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</title>
<author confidence="0.917182">Morbini</author>
<affiliation confidence="0.9979225">Institute for Creative University of Southern</affiliation>
<address confidence="0.999439">12015 Waterfront Drive, Playa Vista, CA</address>
<abstract confidence="0.997930777777778">Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Sudeep Gandhe</author>
<author>Michael Rushforth</author>
<author>David R Traum</author>
</authors>
<title>Viability of a simple dialogue act scheme for a tactical questioning dialogue system.</title>
<date>2009</date>
<booktitle>In DiaHolmia 2009: Proceedings of the 13th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>43--50</pages>
<location>Stockholm, Sweden,</location>
<contexts>
<context position="7871" citStr="Artstein et al. (2009)" startWordPosition="1292" endWordPosition="1295">ions of the argument of the argmax in equation 1: (1) instead of considering Score(Sh_1), consider only the last segment contained in Sh_1; and (2) instead of using the product of the scores of all segments, use the average score per segment: (Score(Ch,i) · Score(Sh_1))1/(1+N(Sh_,)) where N(Si) is the number of segments in Si. These variants produce similar results; the results reported in the next section were obtained with the second variant. 3 Evaluation 3.1 Data To evaluate our approach we used data collected from users of the TACQ (Traum et al., 2008a) dia96 logue system, as described by Artstein et al. (2009). Of the utterances in that dataset, about 30% are annotated with multiple dialogue acts. The annotation also contains for each dialogue act the corresponding segment of the input utterance. The dataset contains a total of 1,579 utterances. Of these, 1,204 utterances contain only a single dialogue act, and 375 utterances contain multiple dialogue acts, according to manual dialogue act annotation. Within the set of utterances that contain multiple dialogue acts, the average number of dialogue acts per utterance is 2.3. The dialogue act annotation scheme uses a total of 77 distinct labels, with </context>
<context position="10409" citStr="Artstein et al., 2009" startWordPosition="1729" endWordPosition="1732">a of the dataset used here, we list below two examples of utterances in the dataset, and their dialogue act annotation. We add word indices as subscripts in the utterances for illustration purposes only, to facilitate identification of the word spans for each dialogue act. The annotation consists of a word interval and a Single DA Utt. [%] Multiple DA Utt. [%] Wh-questions 51 Wh-questions 31 Yes/No-questions 14 Offers to agent 24 Offers to agent 9 Yes answer 11 Yes answer 7 Yes/No-questions 8 Greeting 7 Thanks 7 Table 1: The frequency of the dialogue act classes most used in the TACQ dataset (Artstein et al., 2009). The left column reports the statistics for the set of utterances annotated with a single dialogue act the right those for the utterances annotated with multiple dialogue acts. Each dialogue act class typically contains several more specific dialogue acts that include domain-specific semantics (for example, there are 29 subtypes of wh-questions that can be performed in the domain, each with a separate domainspecific dialogue act label). dialogue act label1. 1. ( 0 his 1 name, 2 any 3 other 4 information 5 about 6 him, 7 where 8 he 9 lives 10) is labeled with: [0 2] whq(obj: strangeMan, attr: </context>
</contexts>
<marker>Artstein, Gandhe, Rushforth, Traum, 2009</marker>
<rawString>Ron Artstein, Sudeep Gandhe, Michael Rushforth, and David R. Traum. 2009. Viability of a simple dialogue act scheme for a tactical questioning dialogue system. In DiaHolmia 2009: Proceedings of the 13th Workshop on the Semantics and Pragmatics of Dialogue, page 43–50, Stockholm, Sweden, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Klaus Macherey</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of alignment templates and maximum entropy models for natural language understanding.</title>
<date>2003</date>
<booktitle>In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03,</booktitle>
<pages>11--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="867" citStr="Bender et al., 2003" startWordPosition="113" endWordPosition="116">A 90094 {morbini,sagae}@ict.usc.edu Abstract Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines. 1 Introduction Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (Bender et al., 2003; Sagae et al., 2009; Gandhe et al., 2008, for example), provided that enough training material is available. For most systems that implement novel conversational scenarios, however, enough examples of user utterances, which can be annotated as NLU training data, only become available once several users have interacted with the system. This situation is typically addressed by bootstrapping from a relatively small set of hand-authored utterances that perform key dialogue acts in the scenario or from utterances collected from wizard-of-oz or role-play exercises, and having NLU accuracy increase </context>
<context position="3062" citStr="Bender et al., 2003" startWordPosition="457" endWordPosition="460">ultiple dialogue acts to improve NLU, which is precisely what would be needed to make users feel more comfortable with using longer utterances. We address this chicken-and-egg problem with a data-driven NLU approach that segments and identifies multiple dialogue acts in single utterances, even when only short (single dialogue act) utterances are available for training. In contrast to previous approaches that assume the existence of enough training data for learning to segment utterances, e.g. (Stolcke and Shriberg, 1996), or to align specific words to parts of the formal representation, e.g. (Bender et al., 2003), our framework requires a relatively small dataset, which may not contain any utterances with multiple dialogue acts. This makes it possible to create new conversational dialogue system scenarios that allow and encourage users to express themselves with fewer restrictions, without an increased burden in the collection and annotation of NLU training data. 2 Method Given (1) a predefined set of possible dialogue acts for a specific dialogue system, (2) a set of utterances Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 95–100, Portland,</context>
<context position="6968" citStr="Bender et al. (2003)" startWordPosition="1142" endWordPosition="1145">her i &gt; 1 we construct all word spans Tj,i of the input text, containing the words from j to i, where 1 &lt; j &lt; i, then we classify each of the Tj,i and pick the best returned class (dialogue act label) Cj,i (and associated score, which in the case of our maximum entropy classifier is the conditional probability Score(Cj,i) = P(Cj,i|Tj,i)). Then we assign to the best segmentation ending at i, Si, the label Ck,i iff: k = argmax (Score(Ch,i) · Score(Sh_1)) (1) 1&lt;h&lt;i Algorithm 1 calls the classifier O(n2) where n is the number of words in the input text. Note that, as in the maximum entropy NLU of Bender et al. (2003), this search uses the “maximum approximation,” and we do not normalize over all possible sequences. Therefore, our scores are not true probabilities, although they serve as a good approximation in the search for the best overall segmentation. We experimented with two other variations of the argument of the argmax in equation 1: (1) instead of considering Score(Sh_1), consider only the last segment contained in Sh_1; and (2) instead of using the product of the scores of all segments, use the average score per segment: (Score(Ch,i) · Score(Sh_1))1/(1+N(Sh_,)) where N(Si) is the number of segmen</context>
</contexts>
<marker>Bender, Macherey, Och, Ney, 2003</marker>
<rawString>Oliver Bender, Klaus Macherey, Franz Josef Och, and Hermann Ney. 2003. Comparison of alignment templates and maximum entropy models for natural language understanding. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics - Volume 1, EACL ’03, pages 11–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>David DeVault</author>
<author>Antonio Roque</author>
<author>Bilyana Martinovski</author>
<author>Ron Artstein</author>
<author>Anton Leuski</author>
<author>Jillian Gerten</author>
<author>David R Traum</author>
</authors>
<title>From domain specification to virtual humans: An integrated approach to authoring tactical questioning characters.</title>
<date>2008</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<location>Brisbane, Australia,</location>
<contexts>
<context position="908" citStr="Gandhe et al., 2008" startWordPosition="121" endWordPosition="124">act Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines. 1 Introduction Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (Bender et al., 2003; Sagae et al., 2009; Gandhe et al., 2008, for example), provided that enough training material is available. For most systems that implement novel conversational scenarios, however, enough examples of user utterances, which can be annotated as NLU training data, only become available once several users have interacted with the system. This situation is typically addressed by bootstrapping from a relatively small set of hand-authored utterances that perform key dialogue acts in the scenario or from utterances collected from wizard-of-oz or role-play exercises, and having NLU accuracy increase over time as more users interact with the</context>
</contexts>
<marker>Gandhe, DeVault, Roque, Martinovski, Artstein, Leuski, Gerten, Traum, 2008</marker>
<rawString>Sudeep Gandhe, David DeVault, Antonio Roque, Bilyana Martinovski, Ron Artstein, Anton Leuski, Jillian Gerten, and David R. Traum. 2008. From domain specification to virtual humans: An integrated approach to authoring tactical questioning characters. In Proceedings of Interspeech, Brisbane, Australia, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Gwen Christian</author>
<author>David DeVault</author>
<author>David R Traum</author>
</authors>
<title>Towards natural language understanding of partial speech recognition results in dialogue systems.</title>
<date>2009</date>
<booktitle>In Short Paper Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)</booktitle>
<pages>conference.</pages>
<contexts>
<context position="887" citStr="Sagae et al., 2009" startWordPosition="117" endWordPosition="120">e}@ict.usc.edu Abstract Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines. 1 Introduction Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (Bender et al., 2003; Sagae et al., 2009; Gandhe et al., 2008, for example), provided that enough training material is available. For most systems that implement novel conversational scenarios, however, enough examples of user utterances, which can be annotated as NLU training data, only become available once several users have interacted with the system. This situation is typically addressed by bootstrapping from a relatively small set of hand-authored utterances that perform key dialogue acts in the scenario or from utterances collected from wizard-of-oz or role-play exercises, and having NLU accuracy increase over time as more us</context>
<context position="5776" citStr="Sagae et al., 2009" startWordPosition="918" endWordPosition="921">t k − 1. Algorithm 1 shows our approach for using a single dialogue act classifier to extract the sequence of dialogue acts with the highest overall score from a given utterance. The framework is independent of the particular subsystem used to select the dialogue act label for a given segment of text. The constraint is that this subsystem should return, for a given sequence of words, at least one dialogue act label and its confidence level in a normalized range that can be used for comparisons with subsequent runs. In the work reported in this paper, we use an existing data-driven NLU module (Sagae et al., 2009), developed for the SASO virtual human dialogue system (Traum et al., 2008b), but retrained using the data described in section 3. This NLU module performs maximum entropy multiclass classification, using features derived from the words in the input utterance, and using dialogue act labels as classes. The basic idea is to find the best segmentation (that is, the one with the highest score) of the portion of the input text up to the ith word. The base case Si would be for i = 1 and it is the result of our classifier when the input is the single first word. For any other i &gt; 1 we construct all w</context>
<context position="12780" citStr="Sagae et al. (2009)" startWordPosition="2130" endWordPosition="2133">onsiders as correct only the subset of dialogue acts identified automatically that were successfully aligned with the same dialogue act label in the gold-standard annotation. We compared the performance of our proposed approach to two baselines; both use the same maximum entropy classifier used internally by our proposed approach. 1. The first baseline simply uses the single dialogue act label chosen by the maximum entropy classifier as the only dialogue act for each utterance. In other words, this baseline corresponds to the NLU developed for the SASO dialogue system (Traum et al., 2008b) by Sagae et al. (2009)2. This baseline is expected to have lower recall for those utterances that contain multiple dialogue acts, but potentially higher precision overall, since most utterances in the dataset contain only one dialogue act label. 2. For the second baseline, we treat multiple dialogue act detection as a set of binary classification tasks, one for each possible dialogue act label in the domain. We start from the same training data as above, and create N copies, where N is the number of unique dialogue acts labels in the training set. Each utterance-label pair in the original training set is now presen</context>
</contexts>
<marker>Sagae, Christian, DeVault, Traum, 2009</marker>
<rawString>Kenji Sagae, Gwen Christian, David DeVault, and David R. Traum. 2009. Towards natural language understanding of partial speech recognition results in dialogue systems. In Short Paper Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT) 2009 conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic linguistic segmentation of conversational speech.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>1005--1008</pages>
<contexts>
<context position="2968" citStr="Stolcke and Shriberg, 1996" startWordPosition="440" endWordPosition="443">respond to a single dialogue act, making it difficult to collect enough examples of utterances with multiple dialogue acts to improve NLU, which is precisely what would be needed to make users feel more comfortable with using longer utterances. We address this chicken-and-egg problem with a data-driven NLU approach that segments and identifies multiple dialogue acts in single utterances, even when only short (single dialogue act) utterances are available for training. In contrast to previous approaches that assume the existence of enough training data for learning to segment utterances, e.g. (Stolcke and Shriberg, 1996), or to align specific words to parts of the formal representation, e.g. (Bender et al., 2003), our framework requires a relatively small dataset, which may not contain any utterances with multiple dialogue acts. This makes it possible to create new conversational dialogue system scenarios that allow and encourage users to express themselves with fewer restrictions, without an increased burden in the collection and annotation of NLU training data. 2 Method Given (1) a predefined set of possible dialogue acts for a specific dialogue system, (2) a set of utterances Proceedings of the 49th Annual</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>Andreas Stolcke and Elizabeth Shriberg. 1996. Automatic linguistic segmentation of conversational speech. In Proc. ICSLP, pages 1005–1008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Anton Leuski</author>
<author>Antonio Roque</author>
<author>Sudeep Gandhe</author>
<author>David DeVault</author>
<author>Jillian Gerten</author>
<author>Susan Robinson</author>
<author>Bilyana Martinovski</author>
</authors>
<title>Natural language dialogue architectures for tactical questioning characters.</title>
<date>2008</date>
<booktitle>In Army Science Conference,</booktitle>
<pages>12--2008</pages>
<location>Florida,</location>
<contexts>
<context position="5850" citStr="Traum et al., 2008" startWordPosition="932" endWordPosition="935">ssifier to extract the sequence of dialogue acts with the highest overall score from a given utterance. The framework is independent of the particular subsystem used to select the dialogue act label for a given segment of text. The constraint is that this subsystem should return, for a given sequence of words, at least one dialogue act label and its confidence level in a normalized range that can be used for comparisons with subsequent runs. In the work reported in this paper, we use an existing data-driven NLU module (Sagae et al., 2009), developed for the SASO virtual human dialogue system (Traum et al., 2008b), but retrained using the data described in section 3. This NLU module performs maximum entropy multiclass classification, using features derived from the words in the input utterance, and using dialogue act labels as classes. The basic idea is to find the best segmentation (that is, the one with the highest score) of the portion of the input text up to the ith word. The base case Si would be for i = 1 and it is the result of our classifier when the input is the single first word. For any other i &gt; 1 we construct all word spans Tj,i of the input text, containing the words from j to i, where </context>
<context position="7810" citStr="Traum et al., 2008" startWordPosition="1281" endWordPosition="1284">erall segmentation. We experimented with two other variations of the argument of the argmax in equation 1: (1) instead of considering Score(Sh_1), consider only the last segment contained in Sh_1; and (2) instead of using the product of the scores of all segments, use the average score per segment: (Score(Ch,i) · Score(Sh_1))1/(1+N(Sh_,)) where N(Si) is the number of segments in Si. These variants produce similar results; the results reported in the next section were obtained with the second variant. 3 Evaluation 3.1 Data To evaluate our approach we used data collected from users of the TACQ (Traum et al., 2008a) dia96 logue system, as described by Artstein et al. (2009). Of the utterances in that dataset, about 30% are annotated with multiple dialogue acts. The annotation also contains for each dialogue act the corresponding segment of the input utterance. The dataset contains a total of 1,579 utterances. Of these, 1,204 utterances contain only a single dialogue act, and 375 utterances contain multiple dialogue acts, according to manual dialogue act annotation. Within the set of utterances that contain multiple dialogue acts, the average number of dialogue acts per utterance is 2.3. The dialogue ac</context>
<context position="12755" citStr="Traum et al., 2008" startWordPosition="2125" endWordPosition="2128">). The evaluation then considers as correct only the subset of dialogue acts identified automatically that were successfully aligned with the same dialogue act label in the gold-standard annotation. We compared the performance of our proposed approach to two baselines; both use the same maximum entropy classifier used internally by our proposed approach. 1. The first baseline simply uses the single dialogue act label chosen by the maximum entropy classifier as the only dialogue act for each utterance. In other words, this baseline corresponds to the NLU developed for the SASO dialogue system (Traum et al., 2008b) by Sagae et al. (2009)2. This baseline is expected to have lower recall for those utterances that contain multiple dialogue acts, but potentially higher precision overall, since most utterances in the dataset contain only one dialogue act label. 2. For the second baseline, we treat multiple dialogue act detection as a set of binary classification tasks, one for each possible dialogue act label in the domain. We start from the same training data as above, and create N copies, where N is the number of unique dialogue acts labels in the training set. Each utterance-label pair in the original t</context>
</contexts>
<marker>Traum, Leuski, Roque, Gandhe, DeVault, Gerten, Robinson, Martinovski, 2008</marker>
<rawString>David R. Traum, Anton Leuski, Antonio Roque, Sudeep Gandhe, David DeVault, Jillian Gerten, Susan Robinson, and Bilyana Martinovski. 2008a. Natural language dialogue architectures for tactical questioning characters. In Army Science Conference, Florida, 12/2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Stacy Marsella</author>
<author>Jonathan Gratch</author>
<author>Jina Lee</author>
<author>Arno Hartholt</author>
</authors>
<title>Multi-party, multiissue, multi-strategy negotiation for multi-modal virtual agents.</title>
<date>2008</date>
<booktitle>In IVA,</booktitle>
<pages>117--130</pages>
<contexts>
<context position="5850" citStr="Traum et al., 2008" startWordPosition="932" endWordPosition="935">ssifier to extract the sequence of dialogue acts with the highest overall score from a given utterance. The framework is independent of the particular subsystem used to select the dialogue act label for a given segment of text. The constraint is that this subsystem should return, for a given sequence of words, at least one dialogue act label and its confidence level in a normalized range that can be used for comparisons with subsequent runs. In the work reported in this paper, we use an existing data-driven NLU module (Sagae et al., 2009), developed for the SASO virtual human dialogue system (Traum et al., 2008b), but retrained using the data described in section 3. This NLU module performs maximum entropy multiclass classification, using features derived from the words in the input utterance, and using dialogue act labels as classes. The basic idea is to find the best segmentation (that is, the one with the highest score) of the portion of the input text up to the ith word. The base case Si would be for i = 1 and it is the result of our classifier when the input is the single first word. For any other i &gt; 1 we construct all word spans Tj,i of the input text, containing the words from j to i, where </context>
<context position="7810" citStr="Traum et al., 2008" startWordPosition="1281" endWordPosition="1284">erall segmentation. We experimented with two other variations of the argument of the argmax in equation 1: (1) instead of considering Score(Sh_1), consider only the last segment contained in Sh_1; and (2) instead of using the product of the scores of all segments, use the average score per segment: (Score(Ch,i) · Score(Sh_1))1/(1+N(Sh_,)) where N(Si) is the number of segments in Si. These variants produce similar results; the results reported in the next section were obtained with the second variant. 3 Evaluation 3.1 Data To evaluate our approach we used data collected from users of the TACQ (Traum et al., 2008a) dia96 logue system, as described by Artstein et al. (2009). Of the utterances in that dataset, about 30% are annotated with multiple dialogue acts. The annotation also contains for each dialogue act the corresponding segment of the input utterance. The dataset contains a total of 1,579 utterances. Of these, 1,204 utterances contain only a single dialogue act, and 375 utterances contain multiple dialogue acts, according to manual dialogue act annotation. Within the set of utterances that contain multiple dialogue acts, the average number of dialogue acts per utterance is 2.3. The dialogue ac</context>
<context position="12755" citStr="Traum et al., 2008" startWordPosition="2125" endWordPosition="2128">). The evaluation then considers as correct only the subset of dialogue acts identified automatically that were successfully aligned with the same dialogue act label in the gold-standard annotation. We compared the performance of our proposed approach to two baselines; both use the same maximum entropy classifier used internally by our proposed approach. 1. The first baseline simply uses the single dialogue act label chosen by the maximum entropy classifier as the only dialogue act for each utterance. In other words, this baseline corresponds to the NLU developed for the SASO dialogue system (Traum et al., 2008b) by Sagae et al. (2009)2. This baseline is expected to have lower recall for those utterances that contain multiple dialogue acts, but potentially higher precision overall, since most utterances in the dataset contain only one dialogue act label. 2. For the second baseline, we treat multiple dialogue act detection as a set of binary classification tasks, one for each possible dialogue act label in the domain. We start from the same training data as above, and create N copies, where N is the number of unique dialogue acts labels in the training set. Each utterance-label pair in the original t</context>
</contexts>
<marker>Traum, Marsella, Gratch, Lee, Hartholt, 2008</marker>
<rawString>David R. Traum, Stacy Marsella, Jonathan Gratch, Jina Lee, and Arno Hartholt. 2008b. Multi-party, multiissue, multi-strategy negotiation for multi-modal virtual agents. In IVA, pages 117–130.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>