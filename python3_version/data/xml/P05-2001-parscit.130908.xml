<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002747">
<title confidence="0.996096">
Hybrid Methods for POS Guessing of Chinese Unknown Words
</title>
<author confidence="0.99863">
Xiaofei Lu
</author>
<affiliation confidence="0.995767">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.595423">
Columbus, OH 43210, USA
</address>
<email confidence="0.999027">
xflu@ling.osu.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994375">
This paper describes a hybrid model that
combines a rule-based model with two
statistical models for the task of POS
guessing of Chinese unknown words. The
rule-based model is sensitive to the type,
length, and internal structure of unknown
words, and the two statistical models uti-
lize contextual information and the like-
lihood for a character to appear in a par-
ticular position of words of a particular
length and POS category. By combining
models that use different sources of infor-
mation, the hybrid model achieves a pre-
cision of 89%, a significant improvement
over the best result reported in previous
studies, which was 69%.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939866666667">
Unknown words constitute a major source of diffi-
culty for Chinese part-of-speech (POS) tagging, yet
relatively little work has been done on POS guess-
ing of Chinese unknown words. The few existing
studies all attempted to develop a unified statistical
model to compute the probability of a word hav-
ing a particular POS category for all Chinese un-
known words (Chen et al., 1997; Wu and Jiang,
2000; Goh, 2003). This approach tends to miss
one or more pieces of information contributed by
the type, length, internal structure, or context of in-
dividual unknown words, and fails to combine the
strengths of different models. The rule-based ap-
proach was rejected with the claim that rules are
bound to overgenerate (Wu and Jiang, 2000).
</bodyText>
<page confidence="0.8458">
1
</page>
<bodyText confidence="0.999979357142857">
In this paper, we present a hybrid model that com-
bines the strengths of a rule-based model with those
of two statistical models for this task. The three
models make use of different sources of information.
The rule-based model is sensitive to the type, length,
and internal structure of unknown words, with over-
generation controlled by additional constraints. The
two statistical models make use of contextual infor-
mation and the likelihood for a character to appear in
a particular position of words of a particular length
and POS category respectively. The hybrid model
achieves a precision of 89%, a significant improve-
ment over the best result reported in previous stud-
ies, which was 69%.
</bodyText>
<sectionHeader confidence="0.985044" genericHeader="method">
2 Chinese Unknown Words
</sectionHeader>
<bodyText confidence="0.999324631578948">
The definition of what constitutes a word is prob-
lematic for Chinese, as Chinese does not have word
delimiters and the boundary between compounds
and phrases or collocations is fuzzy. Consequently,
different NLP tasks adopt different segmentation
schemes (Sproat, 2002). With respect to any Chi-
nese corpus or NLP system, therefore, unknown
words can be defined as character strings that are
not in the lexicon but should be identified as seg-
mentation units based on the segmentation scheme.
Chen and Bai (1998) categorized Chinese unknown
words into the following five types: 1) acronyms,
i.e., shortened forms of long names, e.g., bˇei-d`a for
bˇeij¯ıng-d`axu´e ‘Beijing University’; 2) proper names,
including person, place, and organization names,
e.g., M´ao-Z´ed¯ong; 3) derived words, which are cre-
ated through affixation, e.g., xi`and`ai-hu`a ‘modern-
ize’; 4) compounds, which are created through com-
pounding, e.g., zhˇı-lˇaohˇu ‘paper tiger’; and 5) nu-
</bodyText>
<note confidence="0.387273">
Proceedings of the ACL Student Research Workshop, pages 1–6,
</note>
<page confidence="0.39462">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<bodyText confidence="0.999966105263158">
meric type compounds, including numbers, dates,
time, etc., e.g., liˇang-diˇan ‘two o’clock’. Other
types of unknown words exist, such as loan words
and reduplicated words. A monosyllabic or disyl-
labic Chinese word can reduplicate in various pat-
terns, e.g., zˇou-zˇou ‘take a walk’ and pi`ao-pi`ao-
li`ang-li`ang ‘very pretty’ are formed by reduplicating
zˇou ‘walk’ and pi`ao-li`ang ‘pretty’ respectively.
The identification of acronyms, proper names,
and numeric type compounds is a separate task that
has received substantial attention. Once a charac-
ter string is identified as one of these, its POS cate-
gory also becomes known. We will therefore focus
on reduplicated and derived words and compounds
only. We will consider unknown words of the cat-
egories of noun, verb, and adjective, as most un-
known words fall under these categories (Chen and
Bai, 1998). Finally, monosyllabic words will not be
considered as they are well covered by the lexicon.
</bodyText>
<sectionHeader confidence="0.984358" genericHeader="method">
3 Previous Approaches
</sectionHeader>
<bodyText confidence="0.999953565217391">
Previous studies all attempted to develop a uni-
fied statistical model for this task. Chen et al.
(1997) examined all unknown nouns1, verbs, and
adjectives and reported a 69.13% precision using
Dice metrics to measure the affix-category associa-
tion strength and an affix-dependent entropy weight-
ing scheme for determining the weightings be-
tween prefix-category and suffix-category associa-
tions. This approach is blind to the type, length, and
context of unknown words. Wu and Jiang (2000)
calculated P(Cat,Pos,Len) for each character, where
Cat is the POS of a word containing the character,
Pos is the position of the character in that word, and
Len is the length of that word. They then calcu-
lated the POS probabilities for each unknown word
as the joint probabilities of the P(Cat,Pos,Len) of
its component characters. This approach was ap-
plied to unknown nouns, verbs, and adjectives that
are two to four characters long2. They did not re-
port results on unknown word tagging, but reported
that the new word identification and tagging mecha-
nism increased parser coverage. We will show that
this approach suffers reduced recall for multisyllabic
</bodyText>
<footnote confidence="0.990361333333333">
1Including proper names and time nouns, which we ex-
cluded for the reason discussed in section 2.
2Excluding derived words and proper names.
</footnote>
<bodyText confidence="0.9866806875">
words if the training corpus is small. Goh (2003) re-
ported a precision of 59.58% on all unknown words
using Support Vector Machines.
Several reasons were suggested for rejecting the
rule-based approach. First, Chen et al. (1997)
claimed that it does not work because the syntac-
tic and semantic information for each character or
morpheme is unavailable. This claim does not fully
hold, as the POS information about the component
words or morphemes of many unknown words is
available in the training lexicon. Second, Wu and
Jiang (2000) argued that assigning POS to Chinese
unknown words on the basis of the internal struc-
ture of those words will “result in massive over-
generation” (p. 48). We will show that overgener-
ation can be controlled by additional constraints.
</bodyText>
<sectionHeader confidence="0.982804" genericHeader="method">
4 Proposed Approach
</sectionHeader>
<bodyText confidence="0.999990625">
We propose a hybrid model that combines the
strengths of different models to arrive at better re-
sults for this task. The models we will consider are
a rule-based model, the trigram model, and the sta-
tistical model developed by Wu and Jiang (2000).
Combination of the three models will be based on
the evaluation of their individual performances on
the training data.
</bodyText>
<subsectionHeader confidence="0.996418">
4.1 The Rule-Based Model
</subsectionHeader>
<bodyText confidence="0.999952789473684">
The motivations for developing a set of rules for this
task are twofold. First, the rule-based approach was
dismissed without testing in previous studies. How-
ever, hybrid models that combine rule-based and sta-
tistical models outperform purely statistical models
in many NLP tasks. Second, the rule-based model
can incorporate information about the length, type,
and internal structure of unknown words at the same
time.
Rule development involves knowledge of Chi-
nese morphology and generalizations of the train-
ing data. Disyllabic words are harder to general-
ize than longer words, probably because their mono-
syllabic component morphemes are more fluid than
the longer component morphemes of longer words.
It is interesting to see if reduction in the degree of
fluidity of its components makes a word more pre-
dictable. We therefore develop a separate set of
rules for words that are two, three, four, and five
</bodyText>
<page confidence="0.993329">
2
</page>
<table confidence="0.997775166666667">
Chars T1 T2 T3 T4 Total
2 1 2 1 2 6
3 2 6 2 5 15
4 2 2 0 8 12
5+ 0 1 0 1 2
Total 5 11 3 16 35
</table>
<tableCaption confidence="0.99952">
Table 1: Rule distribution
</tableCaption>
<bodyText confidence="0.999958">
or more characters long. The rules developed fall
into the following four types: 1) reduplication rules
(T1), which tag reduplicated unknown words based
on knowledge about the reduplication process; 2)
derivation rules (T2), which tag derived unknown
words based on knowledge about the affixation pro-
cess; 3) compounding rules (T3), which tag un-
known compounds based on the POS information
of their component words; and 4) rules based on
generalizations about the training data (T4). Rules
may come with additional constraints to avoid over-
generation. The number of rules in each set is listed
in Table 1. The complete set of rules are developed
over a period of two weeks.
As will be shown below, the order in which the
rules in each set are applied is crucial for dealing
with ambiguous cases. To illustrate how rules work,
we discuss the complete set of rules for disyllabic
words here3. These are given in Figure 1, where
A and B refer to the component morpheme of an
unknown AB. As rules for disyllabic words tend to
overgenerate and as we prefer precision over recall
for the rule-based model, most rules in this set are
accompanied with additional constraints.
In the first reduplication rule, the order of the
three cases is crucial in that if A can be both a verb
and a noun, AA is almost always a verb. The sec-
ond rule tags a disyllabic unknown word formed by
attaching the diminutive suffix er to a monosyllabic
root as a noun. This may appear a hasty general-
ization, but examination of the data shows that er
rarely attaches to monosyllabic verbs except for the
few well-known cases. In the third rule, a catego-
rizing suffix is one that attaches to other words to
form a noun that refers to a category of people or
objects, e.g., ji¯a ‘-ist’. The constraint “A is not a
verb morpheme” excludes cases where B is polyse-
mous and does not function as a categorizing suffix
</bodyText>
<footnote confidence="0.9063945">
3Multisyllabic words can have various internal structures,
e.g., a disyllabic noun can have a N-N, Adj-N, or V-N structure.
</footnote>
<construct confidence="0.923842">
if A equals B
if A is a verb morpheme, AB is a verb
else if A is a noun morpheme, AB is a noun
</construct>
<figure confidence="0.768066153846154">
else if A is an adjective morpheme, AB is a stative
adjective/adverb
else if B equals er, AB is a noun
else if B is a categorizing suffix AND A is not a verb
morpheme, AB is a noun
else if A and B are both noun morphemes but not verb
morphemes, AB is a noun
else if A occurs verb-initially only AND B is not a noun
morpheme AND B does not occur noun-finally only,
AB is a verb
else if B occurs noun-finally only AND A is not a verb
morpheme AND A does not occur verb-initially only,
AB is a noun
</figure>
<figureCaption confidence="0.999041">
Figure 1: Rules for disyllabic words
</figureCaption>
<bodyText confidence="0.9999882">
but a noun morpheme. Thus, this rule tags b`eng-y`e
‘water-pump industry’ as a noun, but not li-y`e leave-
job ‘resign’. The fourth rule tags words such as sh¯a-
xi¯ang ‘sand-box’ as nouns, but the constraints pre-
vent verbs such as s¯ong-k`ou ‘loosen-button’ from
being tagged as nouns. S¯ong can be both a noun
and a verb, but it is used as a verb in this word.
The last two rules make use of two lists of char-
acters extracted from the list of disyllabic words in
the training data, i.e., those that have only appeared
in the verb-initial and noun-final positions respec-
tively. This is done because in Chinese, disyllabic
compound verbs tend to be head-initial, whereas di-
syllabic compound nouns tend to be head-final. The
fifth rule tags words such as d¯ıng-yˇao ‘sting-bite’ as
verbs, and the additional constraints prevent nouns
such as f´u-xi`ang ‘lying-elephant’ from being tagged
as verbs. The last rule tags words such as xuˇe-
b`ei ‘snow-quilt’ as nouns, but not zh¯ai-sh¯ao pick-tip
‘pick the tips’.
One derivation rule for trisyllabic words has a spe-
cial status. Following the tagging guidelines of our
training corpus, it tags a word ABC as verb/deverbal
noun (v/vn) if C is the suffix hu`a ‘-ize’. Disambigua-
tion is left to the statistical models.
</bodyText>
<subsectionHeader confidence="0.930999">
4.2 The Trigram Model
</subsectionHeader>
<bodyText confidence="0.999769166666667">
The trigram model is used because it captures the in-
formation about the POS context of unknown words
and returns a tag for each unknown word. We as-
sume that the unknown POS depends on the previ-
ous two POS tags, and calculate the trigram proba-
bility P(t3|t1, t2), where t3 stands for the unknown
</bodyText>
<page confidence="0.993546">
3
</page>
<bodyText confidence="0.999885454545455">
POS, and t1 and t2 stand for the two previous POS
tags. The POS tags for known words are taken from
the tagged training corpus. Following Brants (2000),
we first calculate the maximum likelihood probabil-
ities P for unigrams, bigrams, and trigrams as in
(1-3). To handle the sparse-data problem, we use
the smoothing paradigm that Brants reported as de-
livering the best result for the TnT tagger, i.e., the
context-independent variant of linear interpolation
of unigrams, bigrams, and trigrams. A trigram prob-
ability is then calculated as in (4).
</bodyText>
<equation confidence="0.999991">
Pˆ (t3) = f(t3)/N (1)
Pˆ(t3|t2) = f(t2, t3)/f(t2) (2)
Pˆ(t3|t1, t2) = f(t1, t2, t3)/f(t1, t2) (3)
P(t3|t1, t2) = A1 Pˆ(t3) + A2 Pˆ(t3|t2) + A3 Pˆ(t3|t1, t2) (4)
</equation>
<bodyText confidence="0.999989">
As in Brants (2000), A1 + A2 + A3 = 1, and the
values of A1, A2, and A3 are estimated by deleted
interpolation, following Brants’ algorithm for calcu-
lating the weights for context-independent linear in-
terpolation when the n-gram frequencies are known.
</bodyText>
<subsectionHeader confidence="0.995458">
4.3 Wu and Jiang’s (2000) Statistical Model
</subsectionHeader>
<bodyText confidence="0.999831947368421">
There are several reasons for integrating another sta-
tistical model in the model. The rule-based model is
expected to yield high precision, as over-generation
is minimized, but it is bound to suffer low recall for
disyllabic words. The trigram model covers all un-
known words, but its precision needs to be boosted.
Wu and Jiang’s (2000) model provides a good com-
plement for the two, because it achieves a higher
recall than the rule-based model and a higher pre-
cision than the trigram model for disyllabic words.
As our training corpus is relatively small, this model
will suffer a low recall for longer words, but those
are handled effectively by the rule-based model. In
principle, other statistical models can also be used,
but Wu and Jiang’s model appears more appealing
because of its relative simplicity and higher or com-
parable precision. It is used to handle disyllabic and
trisyllabic unknown words only, as recall drops sig-
nificantly for longer words.
</bodyText>
<subsectionHeader confidence="0.991467">
4.4 Combining Models
</subsectionHeader>
<bodyText confidence="0.913857928571429">
To determine the best way to combine the three
models, their individual performances are evaluated
for each unknown word
if the trigram model returns one single guess, take it
else if the rule-based model returns a non-v/vn tag, take it
else if the rule-based model returns a v/vn tag
if W&amp;J’s model returns a list of guesses
eliminate non-v/vn tags on that list and return the
rest of it
else eliminate non-v/vn tags on the list returned by the
trigram model and return the rest of it
else if W&amp;J’s model returns a list of guesses, take it
else return the list of guesses returned by the trigram
model
</bodyText>
<figureCaption confidence="0.996615">
Figure 2: Algorithm for combining models
</figureCaption>
<bodyText confidence="0.999966117647059">
in the training data first to identify their strengths.
Based on that evaluation, we come up with the al-
gorithm in Figure 2. For each unknown word, if the
trigram model returns exactly one POS tag, that tag
is prioritized, because in the training data, such tags
turn out to be always correct. Otherwise, the guess
returned by the rule-based model is prioritized, fol-
lowed by Wu and Jiang’s model. If neither of them
returns a guess, the guess returned by the trigram
model is accepted. This order of priority is based on
the precision of the individual models in the train-
ing data. If the rule-based model returns the “v/vn”
guess, we first check which of the two tags ranks
higher in the list of guesses returned by Wu and
Jiang’s model. If that list is empty, we then check
which of them ranks higher in the list of guesses re-
turned by the trigram model.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.99944">
5.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999744642857143">
The different models are trained and tested on a por-
tion of the Contemporary Chinese Corpus of Peking
University (Yu et al., 2002), which is segmented and
POS tagged. This corpus uses a tagset consisting of
40 tags. We consider unknown words that are 1) two
or more characters long, 2) formed through redupli-
cation, derivation, or compounding, and 3) in one
of the eight categories listed in Table 2. The corpus
consists of all the news articles from People’s Daily
in January, 1998. It has a total of 1,121,016 tokens,
including 947,959 word tokens and 173,057 punc-
tuation marks. 90% of the data are used for train-
ing, and the other 10% are reserved for testing. We
downloaded a reference lexicon4 containing 119,791
</bodyText>
<footnote confidence="0.998514">
4From http://www.mandarintools.com/segmenter.html.
</footnote>
<page confidence="0.996683">
4
</page>
<bodyText confidence="0.999919428571429">
entries. A word is considered unknown if it is in the
wordlist extracted from the training or test data but
is not in the reference lexicon. Given this defini-
tion, we first train and evaluate the individual mod-
els on the training data and then evaluate the final
combined model on the test data. The distribution
of unknown words is summarized in Table 3.
</bodyText>
<tableCaption confidence="0.994972">
Table 2: Categories of considered unknown words
</tableCaption>
<table confidence="0.999266428571429">
Chars Training Data Test Data
Types Tokens Types Tokens
2 2611 4789 387 464
3 3818 7378 520 764
4 490 1229 74 125
5+ 188 698 20 56
Total 7107 14094 1001 1509
</table>
<tableCaption confidence="0.999849">
Table 3: Unknown word distribution in the data
</tableCaption>
<subsectionHeader confidence="0.566408">
5.2 Results for the Individual Models
</subsectionHeader>
<bodyText confidence="0.990686333333333">
The results for the rule-based model are listed in Ta-
ble 4. Recall (R) is defined as the number of cor-
rectly tagged unknown words divided by the total
number of unknown words. Precision (P) is defined
as the number of correctly tagged unknown words
divided by the number of tagged unknown words.
The small number of words tagged “v/vn” are ex-
cluded in the count of tagged unknown words for
calculating precision, as this tag is not a final guess
but is returned to reduce the search space for the
statistical models. F-measure (F) is computed as
2 * RP/(R + P). The rule-based model achieves
very high precision, but recall for disyllabic words
is low.
The results for the trigram model are listed in Ta-
ble 5. Candidates are restricted to the eight POS cat-
egories listed in Table 2 for this model. Precision for
the best guess in both datasets is about 62%.
The results for Wu and Jiang’s model are listed in
Table 6. Recall for disyllabic words is much higher
than that of the rule-based model. Precision for di-
syllabic words reaches mid 70%, higher than that of
the trigram model. Precision for trisyllabic words is
very high, but recall is low.
</bodyText>
<table confidence="0.999637727272727">
Chars Data R P F
2 Training 24.05 96.94 38.54
Test 27.66 96.89 43.03
3 Training 93.50 99.83 96.56
Test 93.72 99.86 96.69
4 Training 98.70 99.02 98.86
Test 99.20 99.20 99.20
5+ Training 99.86 100 99.93
Test 100 100 100
Total Training 70.60 99.40 82.56
Test 69.72 99.34 81.94
</table>
<tableCaption confidence="0.938798">
Table 4: Results for the rule-based model
</tableCaption>
<table confidence="0.999551666666667">
Guesses 1-Best 2-Best 3-Best
Training 62.01 93.63 96.21
Test 62.96 92.64 94.30
</table>
<tableCaption confidence="0.863513">
Table 5: Results for the trigram model
</tableCaption>
<table confidence="0.9994958">
Chars Data R P F
2 Training 65.19 75.57 67.00
Test 63.82 77.92 70.17
3 Training 59.50 98.41 74.16
Test 55.63 99.07 71.25
</table>
<tableCaption confidence="0.983987">
Table 6: Results for Wu and Jiang’s (2000) model
</tableCaption>
<sectionHeader confidence="0.728695" genericHeader="evaluation">
5.3 Results for the Combined Model
</sectionHeader>
<bodyText confidence="0.9999919">
To evaluate the combined model, we first define the
upper bound of the precision for the model as the
number of unknown words tagged correctly by at
least one of the three models divided by the total
number of unknown words. The upper bound is
91.10% for the training data and 91.39% for the test
data. Table 7 reports the results for the combined
model. The overall precision of the model reaches
89.32% in the training data and 89.00% in the test
data, close to the upper bounds.
</bodyText>
<sectionHeader confidence="0.998656" genericHeader="conclusions">
6 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.99932375">
The results indicate that the three models have dif-
ferent strengths and weaknesses. Using rules that do
not overgenerate and that are sensitive to the type,
length, and internal structure of unknown words,
</bodyText>
<figure confidence="0.953597647058824">
Tag
Description
a Adjective
ad Deadjectval adverb
an Deadjectival noun
n Noun
v Verb
vn Deverbal noun
vd Deverbal adjective
z Stative adjective and adverb
5
Chars Training Test
2 73.27 74.47
3 97.15 97.25
4 98.78 99.20
5+ 100 100
Total 89.32 89.00
</figure>
<tableCaption confidence="0.956801">
Table 7: Results for the combined model
</tableCaption>
<bodyText confidence="0.999976298245614">
the rule-based model achieves high precision for all
words and high recall for longer words, but recall for
disyllabic words is low. The trigram model makes
use of the contextual information of unknown words
and solves the recall problem, but its precision is rel-
atively low. Wu and Jiang’s (2000) model comple-
ments the other two, as it achieves a higher recall
than the rule-based model and a higher precision
than the trigram model for disyllabic words. The
combined model outperforms each individual model
by effectively combining their strengths.
The results challenge the reasons given in previ-
ous studies for rejecting the rule-based model. Over-
generation is a problem only if one attempts to write
rules to cover the complete set of unknown words. It
can be controlled if one prefers precision over recall.
To this end, the internal structure of the unknown
words provides very useful information. Results
for the rule-based model also suggest that as un-
known words become longer and the fluidity of their
component words/morphemes reduces, they become
more predictable and generalizable by rules.
The results achieved in this study prove a signif-
icant improvement over those reported in previous
studies. To our knowledge, the best result on this
task was reported by Chen et al. (1997), which was
69.13%. However, they considered fourteen POS
categories, whereas we examined only eight. This
difference is brought about by the different tagsets
used in the different corpora and the decision to in-
clude or exclude proper names and numeric type
compounds. To make the results more compara-
ble, we replicated their model, and the results we
found were consistent with what they reported, i.e.,
69.12% for our training data and 68.79% for our test
data, as opposed to our 89.32% and 89% respec-
tively.
Several avenues can be taken for future research.
First, it will be useful to identify a statistical model
that achieves higher precision for disyllabic words,
as this seems to be the bottleneck. It will also be rel-
evant to apply advanced statistical models that can
incorporate various useful information to this task,
e.g., the maximum entropy model (Ratnaparkhi,
1996). Second, for better evaluation, it would be
helpful to use a larger corpus and evaluate the in-
dividual models on a held-out dataset, to compare
our model with other models on more compara-
ble datasets, and to test the model on other logo-
graphic languages. Third, some grammatical con-
straints may be used for the detection and correction
of tagging errors in a post-processing step. Finally,
as part of a bigger project on Chinese unknown word
resolution, we would like to see how well the general
methodology used and the specifics acquired in this
task can benefit the identification and sense-tagging
of unknown words.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999787142857143">
Thorsten Brants. 2000. TnT – a statistical part-of-speech
tagger. In Proceedings of the 6th Conference on Ap-
plied Natural Language Processing, pages 224–231.
Keh-Jiann Chen and Ming-Hong Bai. 1998. Unknown
word detection for Chinese by a corpus-based learning
method. International Journal of Computational Lin-
guistics and Chinese Language Processing, 3(1):27–
44.
Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen.
1997. Category guessing for Chinese unknown words.
In Proceedings ofNLPRS, pages 35–40.
Chooi-Ling Goh. 2003. Chinese unknown word identifi-
cation by combining statistical models. Master’s the-
sis, Nara Institute of Science and Technology, Japan.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In Proceedings of EMNLP, pages
133–142.
Richard Sproat. 2002. Corpus-based methods in Chinese
morphology. Tutorial at the 19th COLING.
Andy Wu and Zixin Jiang. 2000. Statistically-enhanced
new word identification in a rule-based Chinese sys-
tem. In Proceedings of the 2nd Chinese Language
Processing Workshop, pages 46–51.
Shiwen Yu, Huiming Duan, Xuefeng Zhu, and Bing Sun.
2002. The basic processing of Contemporary Chinese
Corpus at Peking University. Technical report, Insti-
tute of Computational Linguistics, Peking University,
Beijing, China.
</reference>
<page confidence="0.998785">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933968">
<title confidence="0.999798">Hybrid Methods for POS Guessing of Chinese Unknown Words</title>
<author confidence="0.999039">Xiaofei Lu</author>
<affiliation confidence="0.999785">Department of Linguistics The Ohio State University</affiliation>
<address confidence="0.999991">Columbus, OH 43210, USA</address>
<email confidence="0.999771">xflu@ling.osu.edu</email>
<abstract confidence="0.996170411764706">This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words. The rule-based model is sensitive to the type, length, and internal structure of unknown words, and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Conference on Applied Natural Language Processing,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="12196" citStr="Brants (2000)" startWordPosition="2062" endWordPosition="2063">ning corpus, it tags a word ABC as verb/deverbal noun (v/vn) if C is the suffix hu`a ‘-ize’. Disambiguation is left to the statistical models. 4.2 The Trigram Model The trigram model is used because it captures the information about the POS context of unknown words and returns a tag for each unknown word. We assume that the unknown POS depends on the previous two POS tags, and calculate the trigram probability P(t3|t1, t2), where t3 stands for the unknown 3 POS, and t1 and t2 stand for the two previous POS tags. The POS tags for known words are taken from the tagged training corpus. Following Brants (2000), we first calculate the maximum likelihood probabilities P for unigrams, bigrams, and trigrams as in (1-3). To handle the sparse-data problem, we use the smoothing paradigm that Brants reported as delivering the best result for the TnT tagger, i.e., the context-independent variant of linear interpolation of unigrams, bigrams, and trigrams. A trigram probability is then calculated as in (4). Pˆ (t3) = f(t3)/N (1) Pˆ(t3|t2) = f(t2, t3)/f(t2) (2) Pˆ(t3|t1, t2) = f(t1, t2, t3)/f(t1, t2) (3) P(t3|t1, t2) = A1 Pˆ(t3) + A2 Pˆ(t3|t2) + A3 Pˆ(t3|t1, t2) (4) As in Brants (2000), A1 + A2 + A3 = 1, and t</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the 6th Conference on Applied Natural Language Processing, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Ming-Hong Bai</author>
</authors>
<title>Unknown word detection for Chinese by a corpus-based learning method.</title>
<date>1998</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>44</pages>
<contexts>
<context position="2777" citStr="Chen and Bai (1998)" startWordPosition="451" endWordPosition="454">ficant improvement over the best result reported in previous studies, which was 69%. 2 Chinese Unknown Words The definition of what constitutes a word is problematic for Chinese, as Chinese does not have word delimiters and the boundary between compounds and phrases or collocations is fuzzy. Consequently, different NLP tasks adopt different segmentation schemes (Sproat, 2002). With respect to any Chinese corpus or NLP system, therefore, unknown words can be defined as character strings that are not in the lexicon but should be identified as segmentation units based on the segmentation scheme. Chen and Bai (1998) categorized Chinese unknown words into the following five types: 1) acronyms, i.e., shortened forms of long names, e.g., bˇei-d`a for bˇeij¯ıng-d`axu´e ‘Beijing University’; 2) proper names, including person, place, and organization names, e.g., M´ao-Z´ed¯ong; 3) derived words, which are created through affixation, e.g., xi`and`ai-hu`a ‘modernize’; 4) compounds, which are created through compounding, e.g., zhˇı-lˇaohˇu ‘paper tiger’; and 5) nuProceedings of the ACL Student Research Workshop, pages 1–6, Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics meric type</context>
<context position="4224" citStr="Chen and Bai, 1998" startWordPosition="666" endWordPosition="669">ous patterns, e.g., zˇou-zˇou ‘take a walk’ and pi`ao-pi`aoli`ang-li`ang ‘very pretty’ are formed by reduplicating zˇou ‘walk’ and pi`ao-li`ang ‘pretty’ respectively. The identification of acronyms, proper names, and numeric type compounds is a separate task that has received substantial attention. Once a character string is identified as one of these, its POS category also becomes known. We will therefore focus on reduplicated and derived words and compounds only. We will consider unknown words of the categories of noun, verb, and adjective, as most unknown words fall under these categories (Chen and Bai, 1998). Finally, monosyllabic words will not be considered as they are well covered by the lexicon. 3 Previous Approaches Previous studies all attempted to develop a unified statistical model for this task. Chen et al. (1997) examined all unknown nouns1, verbs, and adjectives and reported a 69.13% precision using Dice metrics to measure the affix-category association strength and an affix-dependent entropy weighting scheme for determining the weightings between prefix-category and suffix-category associations. This approach is blind to the type, length, and context of unknown words. Wu and Jiang (20</context>
</contexts>
<marker>Chen, Bai, 1998</marker>
<rawString>Keh-Jiann Chen and Ming-Hong Bai. 1998. Unknown word detection for Chinese by a corpus-based learning method. International Journal of Computational Linguistics and Chinese Language Processing, 3(1):27– 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Jan Chen</author>
<author>Ming-Hong Bai</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Category guessing for Chinese unknown words.</title>
<date>1997</date>
<booktitle>In Proceedings ofNLPRS,</booktitle>
<pages>35--40</pages>
<contexts>
<context position="1192" citStr="Chen et al., 1997" startWordPosition="191" endWordPosition="194"> length and POS category. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. 1 Introduction Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; Wu and Jiang, 2000; Goh, 2003). This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate (Wu and Jiang, 2000). 1 In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information. The rule-based model is sensitive to</context>
<context position="4443" citStr="Chen et al. (1997)" startWordPosition="702" endWordPosition="705">eric type compounds is a separate task that has received substantial attention. Once a character string is identified as one of these, its POS category also becomes known. We will therefore focus on reduplicated and derived words and compounds only. We will consider unknown words of the categories of noun, verb, and adjective, as most unknown words fall under these categories (Chen and Bai, 1998). Finally, monosyllabic words will not be considered as they are well covered by the lexicon. 3 Previous Approaches Previous studies all attempted to develop a unified statistical model for this task. Chen et al. (1997) examined all unknown nouns1, verbs, and adjectives and reported a 69.13% precision using Dice metrics to measure the affix-category association strength and an affix-dependent entropy weighting scheme for determining the weightings between prefix-category and suffix-category associations. This approach is blind to the type, length, and context of unknown words. Wu and Jiang (2000) calculated P(Cat,Pos,Len) for each character, where Cat is the POS of a word containing the character, Pos is the position of the character in that word, and Len is the length of that word. They then calculated the </context>
<context position="5855" citStr="Chen et al. (1997)" startWordPosition="928" endWordPosition="931">wo to four characters long2. They did not report results on unknown word tagging, but reported that the new word identification and tagging mechanism increased parser coverage. We will show that this approach suffers reduced recall for multisyllabic 1Including proper names and time nouns, which we excluded for the reason discussed in section 2. 2Excluding derived words and proper names. words if the training corpus is small. Goh (2003) reported a precision of 59.58% on all unknown words using Support Vector Machines. Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second, Wu and Jiang (2000) argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will “result in massive overgeneration” (p. 48). We will show that overgeneration can be controlled by additional constraints. 4 Proposed Approach We propose a hybrid model that combines t</context>
<context position="21067" citStr="Chen et al. (1997)" startWordPosition="3597" endWordPosition="3600">nly if one attempts to write rules to cover the complete set of unknown words. It can be controlled if one prefers precision over recall. To this end, the internal structure of the unknown words provides very useful information. Results for the rule-based model also suggest that as unknown words become longer and the fluidity of their component words/morphemes reduces, they become more predictable and generalizable by rules. The results achieved in this study prove a significant improvement over those reported in previous studies. To our knowledge, the best result on this task was reported by Chen et al. (1997), which was 69.13%. However, they considered fourteen POS categories, whereas we examined only eight. This difference is brought about by the different tagsets used in the different corpora and the decision to include or exclude proper names and numeric type compounds. To make the results more comparable, we replicated their model, and the results we found were consistent with what they reported, i.e., 69.12% for our training data and 68.79% for our test data, as opposed to our 89.32% and 89% respectively. Several avenues can be taken for future research. First, it will be useful to identify a</context>
</contexts>
<marker>Chen, Bai, Chen, 1997</marker>
<rawString>Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen. 1997. Category guessing for Chinese unknown words. In Proceedings ofNLPRS, pages 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chooi-Ling Goh</author>
</authors>
<title>Chinese unknown word identification by combining statistical models.</title>
<date>2003</date>
<tech>Master’s thesis,</tech>
<institution>Nara Institute of Science and Technology,</institution>
<contexts>
<context position="1224" citStr="Goh, 2003" startWordPosition="199" endWordPosition="200">models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. 1 Introduction Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; Wu and Jiang, 2000; Goh, 2003). This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate (Wu and Jiang, 2000). 1 In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information. The rule-based model is sensitive to the type, length, and internal </context>
<context position="5676" citStr="Goh (2003)" startWordPosition="902" endWordPosition="903">ach unknown word as the joint probabilities of the P(Cat,Pos,Len) of its component characters. This approach was applied to unknown nouns, verbs, and adjectives that are two to four characters long2. They did not report results on unknown word tagging, but reported that the new word identification and tagging mechanism increased parser coverage. We will show that this approach suffers reduced recall for multisyllabic 1Including proper names and time nouns, which we excluded for the reason discussed in section 2. 2Excluding derived words and proper names. words if the training corpus is small. Goh (2003) reported a precision of 59.58% on all unknown words using Support Vector Machines. Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second, Wu and Jiang (2000) argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will “res</context>
</contexts>
<marker>Goh, 2003</marker>
<rawString>Chooi-Ling Goh. 2003. Chinese unknown word identification by combining statistical models. Master’s thesis, Nara Institute of Science and Technology, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="21950" citStr="Ratnaparkhi, 1996" startWordPosition="3743" endWordPosition="3744">nds. To make the results more comparable, we replicated their model, and the results we found were consistent with what they reported, i.e., 69.12% for our training data and 68.79% for our test data, as opposed to our 89.32% and 89% respectively. Several avenues can be taken for future research. First, it will be useful to identify a statistical model that achieves higher precision for disyllabic words, as this seems to be the bottleneck. It will also be relevant to apply advanced statistical models that can incorporate various useful information to this task, e.g., the maximum entropy model (Ratnaparkhi, 1996). Second, for better evaluation, it would be helpful to use a larger corpus and evaluate the individual models on a held-out dataset, to compare our model with other models on more comparable datasets, and to test the model on other logographic languages. Third, some grammatical constraints may be used for the detection and correction of tagging errors in a post-processing step. Finally, as part of a bigger project on Chinese unknown word resolution, we would like to see how well the general methodology used and the specifics acquired in this task can benefit the identification and sense-taggi</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In Proceedings of EMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<date>2002</date>
<booktitle>Corpus-based methods in Chinese morphology. Tutorial at the 19th COLING.</booktitle>
<contexts>
<context position="2536" citStr="Sproat, 2002" startWordPosition="411" endWordPosition="412">stical models make use of contextual information and the likelihood for a character to appear in a particular position of words of a particular length and POS category respectively. The hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. 2 Chinese Unknown Words The definition of what constitutes a word is problematic for Chinese, as Chinese does not have word delimiters and the boundary between compounds and phrases or collocations is fuzzy. Consequently, different NLP tasks adopt different segmentation schemes (Sproat, 2002). With respect to any Chinese corpus or NLP system, therefore, unknown words can be defined as character strings that are not in the lexicon but should be identified as segmentation units based on the segmentation scheme. Chen and Bai (1998) categorized Chinese unknown words into the following five types: 1) acronyms, i.e., shortened forms of long names, e.g., bˇei-d`a for bˇeij¯ıng-d`axu´e ‘Beijing University’; 2) proper names, including person, place, and organization names, e.g., M´ao-Z´ed¯ong; 3) derived words, which are created through affixation, e.g., xi`and`ai-hu`a ‘modernize’; 4) comp</context>
</contexts>
<marker>Sproat, 2002</marker>
<rawString>Richard Sproat. 2002. Corpus-based methods in Chinese morphology. Tutorial at the 19th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andy Wu</author>
<author>Zixin Jiang</author>
</authors>
<title>Statistically-enhanced new word identification in a rule-based Chinese system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Chinese Language Processing Workshop,</booktitle>
<pages>46--51</pages>
<contexts>
<context position="1212" citStr="Wu and Jiang, 2000" startWordPosition="195" endWordPosition="198">egory. By combining models that use different sources of information, the hybrid model achieves a precision of 89%, a significant improvement over the best result reported in previous studies, which was 69%. 1 Introduction Unknown words constitute a major source of difficulty for Chinese part-of-speech (POS) tagging, yet relatively little work has been done on POS guessing of Chinese unknown words. The few existing studies all attempted to develop a unified statistical model to compute the probability of a word having a particular POS category for all Chinese unknown words (Chen et al., 1997; Wu and Jiang, 2000; Goh, 2003). This approach tends to miss one or more pieces of information contributed by the type, length, internal structure, or context of individual unknown words, and fails to combine the strengths of different models. The rule-based approach was rejected with the claim that rules are bound to overgenerate (Wu and Jiang, 2000). 1 In this paper, we present a hybrid model that combines the strengths of a rule-based model with those of two statistical models for this task. The three models make use of different sources of information. The rule-based model is sensitive to the type, length, a</context>
<context position="4827" citStr="Wu and Jiang (2000)" startWordPosition="759" endWordPosition="762">n and Bai, 1998). Finally, monosyllabic words will not be considered as they are well covered by the lexicon. 3 Previous Approaches Previous studies all attempted to develop a unified statistical model for this task. Chen et al. (1997) examined all unknown nouns1, verbs, and adjectives and reported a 69.13% precision using Dice metrics to measure the affix-category association strength and an affix-dependent entropy weighting scheme for determining the weightings between prefix-category and suffix-category associations. This approach is blind to the type, length, and context of unknown words. Wu and Jiang (2000) calculated P(Cat,Pos,Len) for each character, where Cat is the POS of a word containing the character, Pos is the position of the character in that word, and Len is the length of that word. They then calculated the POS probabilities for each unknown word as the joint probabilities of the P(Cat,Pos,Len) of its component characters. This approach was applied to unknown nouns, verbs, and adjectives that are two to four characters long2. They did not report results on unknown word tagging, but reported that the new word identification and tagging mechanism increased parser coverage. We will show </context>
<context position="6161" citStr="Wu and Jiang (2000)" startWordPosition="979" endWordPosition="982">ed for the reason discussed in section 2. 2Excluding derived words and proper names. words if the training corpus is small. Goh (2003) reported a precision of 59.58% on all unknown words using Support Vector Machines. Several reasons were suggested for rejecting the rule-based approach. First, Chen et al. (1997) claimed that it does not work because the syntactic and semantic information for each character or morpheme is unavailable. This claim does not fully hold, as the POS information about the component words or morphemes of many unknown words is available in the training lexicon. Second, Wu and Jiang (2000) argued that assigning POS to Chinese unknown words on the basis of the internal structure of those words will “result in massive overgeneration” (p. 48). We will show that overgeneration can be controlled by additional constraints. 4 Proposed Approach We propose a hybrid model that combines the strengths of different models to arrive at better results for this task. The models we will consider are a rule-based model, the trigram model, and the statistical model developed by Wu and Jiang (2000). Combination of the three models will be based on the evaluation of their individual performances on</context>
</contexts>
<marker>Wu, Jiang, 2000</marker>
<rawString>Andy Wu and Zixin Jiang. 2000. Statistically-enhanced new word identification in a rule-based Chinese system. In Proceedings of the 2nd Chinese Language Processing Workshop, pages 46–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwen Yu</author>
<author>Huiming Duan</author>
<author>Xuefeng Zhu</author>
<author>Bing Sun</author>
</authors>
<title>The basic processing of Contemporary Chinese Corpus at Peking University.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Institute of Computational Linguistics, Peking University,</institution>
<location>Beijing, China.</location>
<contexts>
<context position="15694" citStr="Yu et al., 2002" startWordPosition="2666" endWordPosition="2669">ither of them returns a guess, the guess returned by the trigram model is accepted. This order of priority is based on the precision of the individual models in the training data. If the rule-based model returns the “v/vn” guess, we first check which of the two tags ranks higher in the list of guesses returned by Wu and Jiang’s model. If that list is empty, we then check which of them ranks higher in the list of guesses returned by the trigram model. 5 Results 5.1 Experiment Setup The different models are trained and tested on a portion of the Contemporary Chinese Corpus of Peking University (Yu et al., 2002), which is segmented and POS tagged. This corpus uses a tagset consisting of 40 tags. We consider unknown words that are 1) two or more characters long, 2) formed through reduplication, derivation, or compounding, and 3) in one of the eight categories listed in Table 2. The corpus consists of all the news articles from People’s Daily in January, 1998. It has a total of 1,121,016 tokens, including 947,959 word tokens and 173,057 punctuation marks. 90% of the data are used for training, and the other 10% are reserved for testing. We downloaded a reference lexicon4 containing 119,791 4From http:/</context>
</contexts>
<marker>Yu, Duan, Zhu, Sun, 2002</marker>
<rawString>Shiwen Yu, Huiming Duan, Xuefeng Zhu, and Bing Sun. 2002. The basic processing of Contemporary Chinese Corpus at Peking University. Technical report, Institute of Computational Linguistics, Peking University, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>