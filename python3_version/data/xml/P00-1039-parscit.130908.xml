<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000038">
<title confidence="0.9994485">
An Algorithm for One-page Summarization of a Long Text
Based on Thematic Hierarchy Detection
</title>
<author confidence="0.987197">
Yoshio Nakao
</author>
<affiliation confidence="0.934191">
Fujitsu Laboratories Ltd.
</affiliation>
<address confidence="0.616726">
Kamikodanaka 4-1-1, Nakahara-ku, Kawasaki, Japan, 211-8588
</address>
<email confidence="0.996365">
nakao@flab.fujitsu.co.jp
</email>
<sectionHeader confidence="0.979943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935136363637">
This paper presents an algorithm for
text summarization using the the-
matic hierarchy of a text. The algo-
rithm is intended to generate a one-
page summary for the user, thereby
enabling the user to skim large vol-
umes of an electronic book on a
computer display. The algorithm
first detects the thematic hierarchy
of a source text with lexical cohe-
sion measured by term repetitions.
Then, it identifies boundary sen-
tences at which a topic of appropri-
ate grading probably starts. Finally,
it generates a structured summary
indicating the outline of the the-
matic hierarchy. This paper mainly
describes and evaluates the part for
boundary sentence identification in
the algorithm, and then briefly dis-
cusses the readability of one-page
summaries.
</bodyText>
<sectionHeader confidence="0.997336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998446920000001">
This paper presents an algorithm for text
summarization using the thematic hierarchy
of a long text, especially for use by readers
who want to skim an electronic book of sev-
eral dozens of pages on a computer display.
For those who want an outline to quickly
understand important parts of a long text,
a one-page summary is more useful than a
quarter-size summary, such as that gener-
ated by a typical automatic text summa-
rizer. Moreover, a one-page summary helps
users reading a long text online because the
whole summary can appear at one time on
the screen of a computer display.
To make such a highly compressed sum-
mary, topics of appropriate grading must be
extracted according to the size of the sum-
mary to be output, and selected topics must
be condensed as much as possible. The pro-
posed algorithm decomposes a text into an
appropriate number of textual units by their
subtopics, and then generates short extracts
for each unit. For example, if a thirty-
sentence summary is required to contain as
many topics as possible, the proposed algo-
rithm decomposes a source text into approxi-
mately ten textual units, and then generates a
summary composed of two- or three-sentence
extracts of these units.
The proposed algorithm consists of three
stages. In the first stage, it detects the the-
matic hierarchy of a source text to decom-
pose a source text into an appropriate num-
ber of textual units of approximately the same
size. In the second stage, it adjusts each
boundary between these textual units to iden-
tify a boundary sentence, indicating where a
topic corresponding to a textual unit proba-
bly starts. It then selects a lead sentence that
probably indicates the contents of subsequent
parts in the same textual unit. In the last
stage, it generates a structured summary of
these sentences, thereby providing an outline
of the thematic hierarchy of the source text.
The remainder of this paper includes the
following: an explanation of problems in one-
page summarization that the proposed algo-
rithm is intended to solve; brief explanations
of a previously published algorithm for the-
matic hierarchy detection (Nakao, 1999) and
a problem that must be solved to successfully
realize one-page summarization; a description
and evaluation of the algorithm for boundary
sentence identification; a brief explanation of
an algorithm for structured summary con-
struction; and some points of discussion on
one-page summarization for further research.
2 Problems in one-page
summarization of a long text
This section examines problems in one-page
summarization. The proposed algorithm is
intended to solve three such problems.
The first problem is related to text decom-
position. Newspaper editorials or technical
papers can be decomposed based on their
rhetorical structures. However, a long ag-
gregated text, such as a long technical sur-
vey report, cannot be decomposed in the
same way, because large textual units, such
as those longer than one section, are usually
constructed with only weak and vague rela-
tionships. Likewise, their arrangement may
seem almost at random if analyzed accord-
ing to their logical or rhetorical relationships.
Thus, a method for detecting such large tex-
tual units is required.
Since a large textual unit often corresponds
to a logical document element, such as a part
or section, rendering features of logical ele-
ments can have an important role in detecting
such a unit. For example, a section header
is distinguishable because it often consists
of a decimal number followed by capitalized
words. However, a method for detecting a
large textual unit by rendering features is not
expected to have wide range of applicability.
In other words, since the process for render-
ing features of logical elements varies accord-
ing to document type, heuristic rules for de-
tection must be prepared for every document
type. That is a problem. Moreover, the log-
ical structure of a text does not always cor-
respond to its thematic hierarchy, especially
if a section consists of an overview clause fol-
lowed by other clauses that can be divided
into several groups by their subtopics.
Since then, based on Hearst&apos;s work (1994),
an algorithm for detecting the thematic hi-
erarchy of a text using only lexical cohesion
(Haliday and Hasan, 1976) measured by term
repetitions was developed (Nakao, 1999). In
comparison with some alternatives (Salton et
al., 1996; Yaari, 1998), one of the features
of the algorithm is that it can decompose a
text into thematic textual units of approxi-
mately the same size, ranging from units just
smaller than the entire text to units of about
one paragraph. In this paper, a summariza-
tion algorithm based on this feature is pro-
posed.
The second problem is related to the tex-
tual coherence of a one-page summary itself.
A three-sentence extract of a large text, which
the proposed algorithm is designed to gener-
ate for an appropriate grading topic, tend to
form a collection of unrelated sentences if it is
generated by simple extraction of important
sentences. Furthermore, the summary should
provides new information to a reader, so an
introduction is necessary to help a reader un-
derstand it. Figure 4 shows a summary exam-
ple of a technical survey report consisting of
one hundred thousand characters. It was gen-
erated by extracting sentences with multiple
significant terms as determined by the like-
lihood ratio test of goodness-of-fit for term
frequency distribution. It seems to have sen-
tences with some important concepts (key-
words), but they do not relate much to one
another. Moreover, inferring the contexts in
which they appear is difficult.
To prevent this problem, the proposed al-
gorithm is designed to extract sentences from
only the lead part of every topic.
The third problem is related to the read-
ability of a summary. A one-page summary
is much shorter than a very long text, such
as a one-hundred-page book, but is too long
to read easily without some breaks indicating
segues of topics. Even for an entire exposi-
tory text, for which a method for displaying
the thematic hierarchy with generated head-
ers was proposed to assist a reader to explore
the content (Yaari, 1998), a good summary is
required to help a user understand quickly.
To improve readability, the proposed algo-
rithm divides every one-page summary into
several parts, each of which consists of a
heading-like sentence followed by some para-
graphs.
</bodyText>
<sectionHeader confidence="0.952888" genericHeader="introduction">
3 Text Summarization Algorithm
</sectionHeader>
<subsectionHeader confidence="0.996281">
3.1 Thematic Hierarchy Detection
</subsectionHeader>
<bodyText confidence="0.999947545454545">
In the first stage, the proposed algorithm uses
the previously published algorithm (Nakao,
1999) to detect the thematic hierarchy of a
text based on lexical cohesion measured by
term repetitions. The output of this stage is
a set of lists consisting of thematic boundary
candidate sections (TBCS). The lists corre-
spond individually to every layer of the hier-
archy and are composed of TBCSs that sep-
arate the source text into thematic textual
units of approximately the same size.
</bodyText>
<sectionHeader confidence="0.546109" genericHeader="method">
3.1.1 Thematic Hierarchy Detection
Algorithm
</sectionHeader>
<bodyText confidence="0.999243125">
First, the algorithm calculates a cohesion
score at fixed-width intervals in a source text.
According to Hearst&apos;s work (1994), a cohesion
score is calculated based on the lexical sim-
ilarity of two adjacent fixed-width windows
(which are eight times larger than the interval
width) set at a specific point by the following
formula:
</bodyText>
<equation confidence="0.896510333333333">
Etwt bl wt br
c(bl, br) _
�Etwt;bl Etw�t;br
</equation>
<bodyText confidence="0.998383444444445">
where bl and br are the textual block in the
left and right windows, respectively, and wt;bl
is the frequency of term&apos; t for bl, and wt;br
is the frequency t for br. Hereafter, the point
between the left and right windows is referred
to as the reference point of a cohesion score.
The algorithm then detects thematic
boundaries according to the minimal points of
four-item moving average (arithmetic mean of
four consecutive scores) of the cohesion score
series. After that, it selects the textual area
contributing the most to every minimal value
and identifies it as a TBCS.
Figure 1 shows the results of a TBCS de-
tection example, where EC is, Forward Co-
hesion, a series of average values plotted at
&apos;All content words (i.e., verbs, nouns, and adjec-
tives) extracted by a tokenizer for Japanese sentences.
</bodyText>
<figureCaption confidence="0.999586">
Figure 1: Example of TBCS Detection
</figureCaption>
<bodyText confidence="0.999981666666667">
the reference point of the first averaged score,
and BC is, Backward Cohesion, a series of
averaged values plotted at the reference point
of the last averaged score. Since the textual
area just before the point at which EC plotted
is always in the left window when one of the
averaged cohesion scores is calculated, EC in-
dicates the strength of forward (left-to-right)
cohesion at a point. Conversely, BC indicates
the strength of backward cohesion at a point.
In the figure, EP is, Equilibrium Point, the
point at which EC and BC have an identi-
cal value. The algorithm checks for EC and
BC starting from the beginning till the end
of the source text; and it records a TBCS, as
depicted by the rectangle, whenever an equi-
librium point is detected (see (Nakao, 1999)
for more information).
</bodyText>
<figure confidence="0.953373">
0 2000 4000 6000 8000 10000 12000 14000 16000 18000
Location in Text [words]
</figure>
<figureCaption confidence="0.99992">
Figure 2: Example of Thematic Hierarchy
</figureCaption>
<bodyText confidence="0.8996845">
For a sample text, Figure 2 shows the re-
sulting thematic hierarchy that was detected
</bodyText>
<figure confidence="0.999589277777777">
10900 11000 11100 11200 11300 11400 11500 11600 11700
Location in Text [words]
Cohesion Score
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
minimal
FC
(4.4)
moving average range
EP
FC &lt; BC FC &gt; BC
(4.4.1)
minimal
BC
C
FC
BC
TBCS
Section Boundary
Window Width [words]
entire
5120
2560
1280
B(0)
640
[0]
[0] [1] [2]
[0] [1] [2] [3] [4]
[0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]
(4.2)
(4.2.1)
(4.2.2)
(ref)
(4.3)
[6]
(4.3.1) (4.3.2)
(ref)
(4.3.3)
(4
4) TBCS
Section Boundary
(4.4.1)
(4.4.2)
(4.4.3)
(4.4.4)
(ref)
</figure>
<tableCaption confidence="0.947416">
Table 1: Accuracy of Thematic Hierarchy Detection
</tableCaption>
<figure confidence="0.968539770833333">
Window Boundary # Original TBCS Unified TBCS
100 (22)
100 (22)
100 (27)
90 (23)
67 (22)
70 (22)
57 (25)
52 (25)
50 (11)
50 (11)
30 (8.1)
64 (16)
47 (16)
35 (11)
23 (10)
13 (6.2)
100 (0.3)
50 (0.5)
67 (1.4)
57 (2.3)
46 (4.5)
51 (9.1)
57 (21)
71 (42)
50 (0.1)
25 (0.3)
20 (0.4)
40 (1.7)
33 (3.2)
25 (4.6)
23 (8.2)
17 (10)
5120 1 2
2560 2 4
1280 3 10
640 30 42
320 114 163
160 184 365
80 322 813
40 403 1681
width
cor.
res.
Recall
Precision
Recall
Precision
</figure>
<bodyText confidence="0.972817307692308">
The figures in parentheses are the baseline rates.
by the aforementioned procedure using vary-
ing window widths (the ordinates). Each hor-
izontal sequence of rectangles depicts a list
of TBCSs detected using a specific window
width.
To narrow the width of candidate sections,
the algorithm then unifies a TBCS with an-
other TBCS in the layer immediate below. It
continued the process until TBCSs in all lay-
ers, from the top to the bottom, are unified.
After that, it outputs the thematic hierarchy
as a set of lists of TBCS data:
i: layer index of the thematic
hierarchy
B(i)[j]: TBCS data containing the
following data members:
ep: equilibrium point
range: thematic boundary
candidate section.
In Figure 2, for example, B(1)[1] is unified
with B(2)[1],B(3)[4], B(4)[6],..., and the val-
ues of its data members (ep and range) are
replaced by those of the unified TBCS in the
bottom layer, which has been detected using
the minimum window width (40 words).
</bodyText>
<sectionHeader confidence="0.8960795" genericHeader="method">
3.1.2 Results of Thematic Hierarchy
Detection
</sectionHeader>
<bodyText confidence="0.986135666666667">
Table 1 summarizes the accuracy of the-
matic hierarchy detection in an experiment
using the following three kinds of Japanese
text as test data: a technical survey report2
that consists of three main sections and con-
tains 17,816 content words; eight series of
</bodyText>
<footnote confidence="0.8733095">
2&amp;quot;Progress Report of Technical Committee on Net-
work Access&amp;quot; in Survey on Natural Language Process-
ing Systems by Japan Electronic Industry Develop-
ment Association, chapter 4, pp. 117{197, Mar. 1997.
</footnote>
<bodyText confidence="0.999733655172414">
newspaper columns3, each of which consists of
4 to 24 articles containing about 400 words;
and twelve economic research reports4, each
of which consists of about ten articles con-
taining 33 to 2,375 words.
In the table, cor. denotes the number of the
correct data values composed of the starting
points of sections that contain the same num-
ber of words or more than the window width
listed in the same row5. In addition, res. de-
notes the number of TBCSs. The original
TBCS columns list the recall and precision
rates of detected TBCSs before TBCS unifica-
tion, and the unified TBCS columns list those
rates after TBCS unification. On each layer,
the width of candidate sections for original
TBCS is about half of the window width; and
that of unified TBCS is 25 words (about half
of the minimum window width). The figures
shown in parentheses are the baseline rates
corresponding to random selection. That is,
parts are randomly selected from the source
text whose total size is equal to the total area
size of TBCSs.
As the boundary figures indicate, the pro-
posed algorithm decomposes a text into tex-
tual units of about equivalent window widths.
In addition, the rates of detected TBCSs are
clearly larger than their baselines. Further-
</bodyText>
<footnote confidence="0.973901">
3Obtained from the Daily Yomiuri On-line
(http://www.yomiuri.co.jp/).
4Monthly reports written for a Japanese company
by a Japanese professor living in the U.S.A.
5Only headings and intentional breaks, such as
symbol lines inserted to separate a prologue or epi-
logue from a main body, are used as correct bound-
aries. As a result, the precision rates of using smaller
window widths tend to degrade because of insufficient
amounts of correct data.
</footnote>
<bodyText confidence="0.999991884615385">
more, for two relatively large series of news-
paper columns, the major boundaries were
detected properly. That is, using larger win-
dow widths, those boundaries were selectively
detected that separate groups of columns by
their subtopics. For example, the starting
point of a set of three consecutive columns
identically entitled &amp;quot;The Great Cultural Rev-
olution&amp;quot; in the &amp;quot;Chinese Revolution&amp;quot; series
was detected using 1,280 word width window,
as well as those of other three sets of consec-
utive columns entitled identically. Thus, the
proposed algorithm is expected to be effec-
tive for arbitrarily selecting the size of tex-
tual units corresponding to different grading
topics.
However, there are problems about how to
determine a boundary point in the range de-
fined by a TBCS. Although the previously
published algorithm (Nakao, 1999) deter-
mines a boundary point with minimal points
of cohesion scores for the smallest window
width, the accuracy degrades substantially
(see Table 3). The boundary sentence identi-
fication algorithm given below is a solution to
this problem.
</bodyText>
<subsectionHeader confidence="0.999155">
3.2 Boundary Sentence Identification
</subsectionHeader>
<bodyText confidence="0.999981625">
In the second stage, from sentences in a
TBCS, the algorithm identifies a boundary
sentence, indicating where a topic corre-
sponding to a textual unit probably starts,
and selects a lead sentence that probably in-
dicates the contents of subsequent parts in the
same textual unit. Figure 3 shows the algo-
rithm in detail.
</bodyText>
<sectionHeader confidence="0.8517105" genericHeader="method">
3.2.1 Forward/Backward Relevance
Calculation
</sectionHeader>
<bodyText confidence="0.9996967">
In steps 2 and 3, boundaries are identified
and lead sentences are selected based on two
kinds of relevance scores for a sentence: for-
ward relevance indicating the sentence rele-
vance to the textual unit immediately after
the sentence, and backward relevance indicat-
ing the sentence relevance to the textual unit
immediately before the sentence. The differ-
ence between the forward and the backward
relevance is referred to as relative forward rel-
</bodyText>
<listItem confidence="0.996172904761905">
1. Assign the target layer as the bottom layer of
the thematic hierarchy: i �— i—,,,.
2. For each TBCS in the target layer, B(i)[j], do
he following:
(a) If i �— i—,,,, then select and identify
all sentences in B(i)[j].range as Bound-
ary Sentence Candicates (B.S.C.); oth-
erwise, select and identify the sentences
in B(i)[j].range located before or identi-
cal to the boundary sentence of B(i + 1)
as B.S.C.
(b) From the B.S.C., identify a sentence as
a Boundary Sentence (B.S.), whose rel-
ative forward relevance is greater than 0
and has the most increment from that of
the previous sentence.
(c) Among the sentences in the B.S.C. lo-
cated after or identical to the B.S., select
the sentence that has the greatest for-
ward relevance as a Lead Sentence (L.S.).
3. If i &gt; 1, then i �— i-1, and repeat from step 2.
</listItem>
<figureCaption confidence="0.954619">
Figure 3: Boundary Sentence Identification
Algorithm
</figureCaption>
<bodyText confidence="0.982671545454545">
evance.
Forward or backward relevance is calcu-
lated using the formula below, where every
textual unit is partitioned at the equilibrium
points of two adjacent TBCSs in the target
layer, the equilibrium point of each TBCS is
initially set by the thematic hierarchy detec-
tion algorithm, and the point is replaced by
the location of the boundary sentence after
the boundary sentence is identified (i.e., step
2b is completed).
</bodyText>
<equation confidence="0.985125666666667">
1 tft;u jDj
Sj E uj x log( dft ))
teS
</equation>
<bodyText confidence="0.9945331">
total number of terms in sentence S
juj total number of terms in textual unit u
tft;u frequency of term t in textual unit u
jDj total number of fixed-width (80 words)
blocks in the source text
dft total number of fixed-width blocks
where term t appears
The use of this formula was proposed as
an effective and simple measure for term im-
portance estimation (Nakao, 1998)6. It is a
</bodyText>
<subsectionHeader confidence="0.290095">
6An experiment reported in (Nakao, 1998) indi-
</subsectionHeader>
<bodyText confidence="0.475689">
rS;u �
jSj
</bodyText>
<tableCaption confidence="0.989334">
Table 2: Example of Boundary Sentence Identification
</tableCaption>
<table confidence="0.999359363636363">
Location Relevance Sentence [partially presented]
(translation)
Backward Forward Relative
O.R.11122 0 0.017 0.017 [吉村他, 86] ([Yoshimura et. al])
11124 0.021 0.004 -0.017 吉村賢治...: &amp;quot;...の自動抽出システム &amp;quot;, ..., pp.33-40, 1986
(Yoshimura, Kenji ... : Automatic Extraction System of ...)
B.S. 11146 0 0.016 0.016 4.4. 検索エンジン (Search Engine)
L.S. 11148 0.005 0.022 0.017 ここでは...知的情報アクセスにおける...ついて報告する。
(This section reports on ... of intelligent information access.)
11170 0.010 0.016 0.006 以下の各節の報告に共通するテーマは、...である。
(The key issue of the reports in the following clauses is ... )
</table>
<bodyText confidence="0.943191">
modified version of entropy, where informa-
tion bit (log part of the formula) is calcu-
lated by reducing the effect of term repeti-
tions in a short period. The modification was
done to increase the scores for an important
term higher, based on the reported observa-
tion that content bearing words tend to occur
in clumps (Bookstein et al., 1998).
3.2.2 Example of Boundary Sentence
Identification
Table 2 summarizes an example of bound-
ary sentence identification of a TBCS located
just before the 12,000th word in Figure 2. Ev-
ery row in the table except the first row, which
is marked with O.R., shows a candidate sen-
tence. The row marked B.S. shows a bound-
ary sentence, which has positive relative for-
ward relevance (0.016 in the fourth column of
the row) and the greatest increment from the
previous value (-0.017). The row marked L.S.
shows a lead sentence, which has the great-
est forward relevance (0.022 in the third col-
umn of the row) among all sentences after the
boundary sentence.
</bodyText>
<subsectionHeader confidence="0.4690785">
3.2.3 Evaluation of Boundary
Identification
</subsectionHeader>
<bodyText confidence="0.9997829">
Table 3 shows recall and precision rates of
the boundary identification algorithm in the
same format as Table 1. Compared with the
results obtained using the previous version of
the algorithm (Nakao, 1999), as shown in the
minimal cohesion columns, the proposed al-
gorithm identifies more accurate boundaries
cates that heading terms (i.e., terms appeared in head-
ings) are effectively detected by scoring terms with the
part of the formula in the summation operator.
(the boundary sentence columns). In ad-
dition, boundary sentence identification was
successful for 75% of the correct TBCSs, that
is, TBCSs including correct boundaries� (see
unified TBCS in Table 1). Thus, the proposed
boundary sentence identification algorithm is
judged to be effective.
Table 3 also summarizes a feature of the
proposed algorithm that it tends to detect
and identify headings as boundary sentences
(the heading rate columns). For the part cor-
responding to larger textual units, which the
proposed algorithm mainly used, the figures
in the overall columns indicate that half of
boundary sentences or more are identical to
headings in the original text; and the figures
in the identification columns indicate that
the proposed algorithm identifies headings as
boundary sentences for more than 80% of the
case where TBCSs including headings.
</bodyText>
<subsectionHeader confidence="0.99731">
3.3 Summary Construction
</subsectionHeader>
<bodyText confidence="0.95511795">
In the third and last stage, the algorithm
outputs the boundary and lead sentences of
TBCSs on a layer that probably corresponds
to topics of appropriate grading. Based on the
ratio of source text size to a given summary
size, the algorithm chooses a layer that con-
tains an appropriate number of TBCSs, and
generates a summary with some breaks to in-
dicate thematic changes.
For example, to generate a 1,000-character
summary consisting of several parts of ap-
proximately 200 characters for each topic, a
text decomposition consisting of five textual
7For the correct TBCSs, the average number of
boundary sentence candidates is 4.4.
units is appropriate for summarization. Since
the sample text used here was decomposed
into five textual units on the B(2) layer (see
Figure 2), it outputs the boundary sentences
and lead sentences of all TBCSs in B(2).
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999988326923077">
Figure 5 shows a one-page summary of a tech-
nical survey report, where (a) is a part of
the summary automatically generated, and
(b) is its translation. It corresponds to the
part of the source text between B(1)[1] and
B(1)[2] (in Figure 2). It is composed of three
parts corresponding to B(2)[1], B(2)[2], and
B(3)[6]. Each part consists of a boundary sen-
tence, presented as a heading, followed by a
lead sentence.
In comparison with the keyword-based
summary shown in Figure 4, generated in the
process described in Section 2, the one-page
summary gives a good impression as being
easy to understand. In fact, when we in-
formally asked more than five colleagues to
state their impression of these summaries,
they agreed with this point. As described
in Section 2, one of the reasons for the good
impression should be the difference in coher-
ence. The relationship among sentences in
the keyword-based summary is not clear; con-
versely, the second sentence of the one-page
summary introduces the outline of the clause,
and it is closely related to the sentences that
follow it. The fact that the one-page sum-
mary provides at least two sentences, includ-
ing a heading, for each topic is also considered
to make coherence strong.
As shown in Table 3, the proposed algo-
rithm is expected to extract headings effec-
tively. However, there is a problem that de-
tected headings do not always correspond to
topics of appropriate grading. For example,
the second boundary sentence in the exam-
ple is not appropriate because it is a heading
of a subclause much smaller than the window
width corresponding to B(2)[2], and its pre-
vious sentence &amp;quot;4.3.2 Technical Trend of IR
Techniques&amp;quot; is more appropriate one.
This example is also related to another lim-
itation of the proposed algorithm. Since there
is no outline description in the subsequent
part of the heading of clause 4.3.2, the pro-
posed algorithm could not generate a coher-
ent extract if it had identified the heading as
a boundary sentence.
It is a future issue to develop more elab-
orated algorithm for summarizing detected
topics especially for the user who wants richer
information than that can be provided in a
extract consisting of two or three sentences.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984222222222">
This paper has proposed an algorithm for one-
page summarization to help a user skim a
long text. It has mainly described and re-
ported the effectiveness of the boundary sen-
tence identification part of the algorithm. It
has also discussed the readability of one-page
summaries. The effectiveness of structured
summaries using the thematic hierarchy is an
issue for future evaluation.
</bodyText>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997538538461538">
A. Bookstein, S. T. Klein, and T. Raita. 1998.
Clumping properties of content-bearing words.
Journal of the American Society for Informa-
tion Science, 49(2):102-114.
Michael A.K. Haliday and Ruqaiya Hasan. 1976.
Cohesion in English. Longman, London.
Marti A. Hearst. 1994. Multi-paragraph segmen-
tation of expository text. In Proc. of the 32nd
Annual Meeting of Association for Computa-
tional Linguistics, pages 9-16.
Yoshio Nakao. 1998. Automatic keyword extrac-
tion based on the topic structure of a text. IPSJ
SIG Notes FI-50-1. (in Japanese).
Yoshio Nakao. 1999. Thematic hierarchy detec-
tion of a text using lexical cohesion. Journal of
the Association for Natural Language Process-
ing, 6(6):83-112. (in Japanese).
Gerard Salton, Amit Singhal, Chris Buckley, and
Mandar Mitra. 1996. Automatic text decom-
position using text segments and text themes.
In Proc. of Hypertext &apos;96, pages 53{65. the As-
sociation for Computing Machinery.
Yaakov Yaari. 1998. Texplore — exploring expos-
itory texts via hierarchical representation. In
Proc. of CVIF &apos;98, pages 25{31. Association for
Computational Linguistics.
</reference>
<tableCaption confidence="0.994638">
Table 3: Evaluation of Boundary Sentence Identification
</tableCaption>
<table confidence="0.7894295">
Window Boundary # Minimal cohesion Boundary sentence Heading rate
width
cor. res. Recall Precision Recall Precision Overall Identification
5120 1 2 0 (0.1) 0 (.05) 100 (0.1) 50 (.05) 100 (6.6) 100 (29)
2560 2 4 0 (0.2) 0 (0.1) 100 (0.2) 50 (.05) 100 (6.6) 100 (29)
1280 3 10 33 (0.5) 10 (0.2) 67 (0.5) 20 (0.2) 80 (6.6) 80 (30)
640 30 42 27 (1.0) 19 (0.7) 47 (1.0) 33 (0.7) 67 (6.3) 88 (34)
320 114 163 26 (1.8) 18 (1.3) 40 (1.8) 28 (1.3) 54 (5.0) 82 (31)
160 184 365 28 (3.5) 14 (1.8) 43 (3.5) 22 (1.8) 37 (4.8) 77 (28)
80 322 813 29 (7.8) 12 (3.1) 45 (7.8) 18 (3.1) 23 (4.8) 70 (26)
40 403 1681 37 (17) 9 (3.9) 46 (16) 11 (3.9) 12 (4.8) 58 (26)
The figures in parentheses are the baseline rates.
</table>
<figure confidence="0.983407648648649">
4.3 ネットワーク上の検索サービス
...また検索精度を高めるために、高頻度語は検索の対象
としない、タイトルや見出しに含まれる語に重みをつけ
る、などの工夫がなされている。
...また、検索サービスが収集したページ数が膨大になる
につれて、ヒット数も膨大になってきたため、すばやく
必要な情報を探すために、よりわかりやすい自動抄録作
成技術が必要となる。...
... tf・ idf 方式とは、単語に分割された文章の各単語の
重要度を、その単語が文書中に出現する頻度tfと、その
単語を含む文書が文書集合中に出現する頻度の逆数 idf の
積によってその単語の重要さを数値化する手法である。
...[河合, 92] の研究 キーワードのカイ二乗値から各キー
ワードの分類に対する得点を計算する場合に、シソーラ
ス辞書から得られる抽象的な意味を得点に加える手法で
ある。...
ta part of a summary condensed to 1.3% of the
source text
(a) Original
ネットワーク上の検索サービス [4.3 参照]
本節では、 WWW 上の検索サービスと電子出版及び
電子図書館について、現在行われている各サービスの
特徴、技術的なポイント、問題点等を調査すると同時
に、関連する研究分野も調査し、将来どのようなサー
ビスが望まれるか、また、そこに必要となる技術は何
であるか、についてまとめる。...
キーワード抽出 [(1) 参照]
ネットワーク上の文書をアクセスする方法の 1 つとし
てキーワード検索がある。...
分散検索 [(4) 参照]
情報を一ヶ所に集中登録するタイプの検索サービス
では、今後ますます肥大化・多様化していく WWW
には対応しきれなくなることが予想される。...
ta part of a summary condensed to 1% of the
source text
(a) Original
4.3 Internet Services Internet Services [see 4.3]
</figure>
<bodyText confidence="0.991120171428572">
This clause surveys internet services, electronic
publishing, and digital libraries, reports on their
features, technical points, and problems observed
in their typical cases, and suggests the desired ser-
vices in the future and the required technology for
their realization based on the investigation of re-
lated research areas. ...
Keyword Extraction [see (1)]
Keyword-based IR is a popular access method for
retrieving document on the networks. ...
Distributed IR Systems [see (4)]
In near future, it will be impossible for a single
IR system storing all resources in a single
database to handle the increasing number of
large WWW text collections. ...
(b) Translation
... They are also enhanced with some techniques,
such as eliminating high frequency words, weighing
a term in document titles and headings, etc., to
achieve high precision. ...
... In addition, since the greatly increasing amount of
pages provided by an Internet service causes a great
increase of average hit number for a query, more
effective automatic text summarization technique
is required for helping a user to find out required
information quickly. ...
... Tf•idf method weighs a term in a document with
a product of the term frequency (tf) in a document
and inverse document frequency (idf), i.e., inverse
of the number of document that the term appears. ...
... [Kawai, 92] A document classification method cal-
culates a score based on x2 values of not only keyword
frequencies but also semantic frequencies correspond-
ing to occurrences of abstracted semantic category in
target divisions. ...
</bodyText>
<figure confidence="0.863157">
(b) Translation
</figure>
<figureCaption confidence="0.99655325">
Figure 4: Example of Keyword-based Sum-
mary (partially presented)
Figure 5: Example of One-page Summary
(partially presented)
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862607">
<title confidence="0.9997565">An Algorithm for One-page Summarization of a Long Text Based on Thematic Hierarchy Detection</title>
<author confidence="0.993976">Yoshio Nakao</author>
<affiliation confidence="0.976138">Fujitsu Laboratories Ltd.</affiliation>
<address confidence="0.933674">Kamikodanaka 4-1-1, Nakahara-ku, Kawasaki, Japan, 211-8588</address>
<email confidence="0.9697">nakao@flab.fujitsu.co.jp</email>
<abstract confidence="0.998163260869565">This paper presents an algorithm for text summarization using the thematic hierarchy of a text. The algorithm is intended to generate a onepage summary for the user, thereby enabling the user to skim large volumes of an electronic book on a computer display. The algorithm first detects the thematic hierarchy of a source text with lexical cohesion measured by term repetitions. Then, it identifies boundary sentences at which a topic of appropriate grading probably starts. Finally, it generates a structured summary indicating the outline of the thematic hierarchy. This paper mainly describes and evaluates the part for boundary sentence identification in the algorithm, and then briefly discusses the readability of one-page summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bookstein</author>
<author>S T Klein</author>
<author>T Raita</author>
</authors>
<title>Clumping properties of content-bearing words.</title>
<date>1998</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>49--2</pages>
<contexts>
<context position="18917" citStr="Bookstein et al., 1998" startWordPosition="3158" endWordPosition="3161"> 0 0.016 0.016 4.4. 検索エンジン (Search Engine) L.S. 11148 0.005 0.022 0.017 ここでは...知的情報アクセスにおける...ついて報告する。 (This section reports on ... of intelligent information access.) 11170 0.010 0.016 0.006 以下の各節の報告に共通するテーマは、...である。 (The key issue of the reports in the following clauses is ... ) modified version of entropy, where information bit (log part of the formula) is calculated by reducing the effect of term repetitions in a short period. The modification was done to increase the scores for an important term higher, based on the reported observation that content bearing words tend to occur in clumps (Bookstein et al., 1998). 3.2.2 Example of Boundary Sentence Identification Table 2 summarizes an example of boundary sentence identification of a TBCS located just before the 12,000th word in Figure 2. Every row in the table except the first row, which is marked with O.R., shows a candidate sentence. The row marked B.S. shows a boundary sentence, which has positive relative forward relevance (0.016 in the fourth column of the row) and the greatest increment from the previous value (-0.017). The row marked L.S. shows a lead sentence, which has the greatest forward relevance (0.022 in the third column of the row) amon</context>
</contexts>
<marker>Bookstein, Klein, Raita, 1998</marker>
<rawString>A. Bookstein, S. T. Klein, and T. Raita. 1998. Clumping properties of content-bearing words. Journal of the American Society for Information Science, 49(2):102-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Haliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="5223" citStr="Haliday and Hasan, 1976" startWordPosition="846" endWordPosition="849">ide range of applicability. In other words, since the process for rendering features of logical elements varies according to document type, heuristic rules for detection must be prepared for every document type. That is a problem. Moreover, the logical structure of a text does not always correspond to its thematic hierarchy, especially if a section consists of an overview clause followed by other clauses that can be divided into several groups by their subtopics. Since then, based on Hearst&apos;s work (1994), an algorithm for detecting the thematic hierarchy of a text using only lexical cohesion (Haliday and Hasan, 1976) measured by term repetitions was developed (Nakao, 1999). In comparison with some alternatives (Salton et al., 1996; Yaari, 1998), one of the features of the algorithm is that it can decompose a text into thematic textual units of approximately the same size, ranging from units just smaller than the entire text to units of about one paragraph. In this paper, a summarization algorithm based on this feature is proposed. The second problem is related to the textual coherence of a one-page summary itself. A three-sentence extract of a large text, which the proposed algorithm is designed to genera</context>
</contexts>
<marker>Haliday, Hasan, 1976</marker>
<rawString>Michael A.K. Haliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proc. of the 32nd Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proc. of the 32nd Annual Meeting of Association for Computational Linguistics, pages 9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshio Nakao</author>
</authors>
<title>Automatic keyword extraction based on the topic structure of a text.</title>
<date>1998</date>
<journal>IPSJ SIG Notes</journal>
<pages>50--1</pages>
<note>(in Japanese).</note>
<contexts>
<context position="17888" citStr="Nakao, 1998" startWordPosition="3001" endWordPosition="3002">S is initially set by the thematic hierarchy detection algorithm, and the point is replaced by the location of the boundary sentence after the boundary sentence is identified (i.e., step 2b is completed). 1 tft;u jDj Sj E uj x log( dft )) teS total number of terms in sentence S juj total number of terms in textual unit u tft;u frequency of term t in textual unit u jDj total number of fixed-width (80 words) blocks in the source text dft total number of fixed-width blocks where term t appears The use of this formula was proposed as an effective and simple measure for term importance estimation (Nakao, 1998)6. It is a 6An experiment reported in (Nakao, 1998) indirS;u � jSj Table 2: Example of Boundary Sentence Identification Location Relevance Sentence [partially presented] (translation) Backward Forward Relative O.R.11122 0 0.017 0.017 [吉村他, 86] ([Yoshimura et. al]) 11124 0.021 0.004 -0.017 吉村賢治...: &amp;quot;...の自動抽出システム &amp;quot;, ..., pp.33-40, 1986 (Yoshimura, Kenji ... : Automatic Extraction System of ...) B.S. 11146 0 0.016 0.016 4.4. 検索エンジン (Search Engine) L.S. 11148 0.005 0.022 0.017 ここでは...知的情報アクセスにおける...ついて報告する。 (This section reports on ... of intelligent information access.) 11170 0.010 0.016 0.006 以下</context>
</contexts>
<marker>Nakao, 1998</marker>
<rawString>Yoshio Nakao. 1998. Automatic keyword extraction based on the topic structure of a text. IPSJ SIG Notes FI-50-1. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshio Nakao</author>
</authors>
<title>Thematic hierarchy detection of a text using lexical cohesion.</title>
<date>1999</date>
<journal>Journal of the Association for Natural Language Processing,</journal>
<pages>6--6</pages>
<note>(in Japanese).</note>
<contexts>
<context position="3104" citStr="Nakao, 1999" startWordPosition="507" endWordPosition="508"> boundary sentence, indicating where a topic corresponding to a textual unit probably starts. It then selects a lead sentence that probably indicates the contents of subsequent parts in the same textual unit. In the last stage, it generates a structured summary of these sentences, thereby providing an outline of the thematic hierarchy of the source text. The remainder of this paper includes the following: an explanation of problems in onepage summarization that the proposed algorithm is intended to solve; brief explanations of a previously published algorithm for thematic hierarchy detection (Nakao, 1999) and a problem that must be solved to successfully realize one-page summarization; a description and evaluation of the algorithm for boundary sentence identification; a brief explanation of an algorithm for structured summary construction; and some points of discussion on one-page summarization for further research. 2 Problems in one-page summarization of a long text This section examines problems in one-page summarization. The proposed algorithm is intended to solve three such problems. The first problem is related to text decomposition. Newspaper editorials or technical papers can be decompo</context>
<context position="5280" citStr="Nakao, 1999" startWordPosition="856" endWordPosition="857">dering features of logical elements varies according to document type, heuristic rules for detection must be prepared for every document type. That is a problem. Moreover, the logical structure of a text does not always correspond to its thematic hierarchy, especially if a section consists of an overview clause followed by other clauses that can be divided into several groups by their subtopics. Since then, based on Hearst&apos;s work (1994), an algorithm for detecting the thematic hierarchy of a text using only lexical cohesion (Haliday and Hasan, 1976) measured by term repetitions was developed (Nakao, 1999). In comparison with some alternatives (Salton et al., 1996; Yaari, 1998), one of the features of the algorithm is that it can decompose a text into thematic textual units of approximately the same size, ranging from units just smaller than the entire text to units of about one paragraph. In this paper, a summarization algorithm based on this feature is proposed. The second problem is related to the textual coherence of a one-page summary itself. A three-sentence extract of a large text, which the proposed algorithm is designed to generate for an appropriate grading topic, tend to form a colle</context>
<context position="7503" citStr="Nakao, 1999" startWordPosition="1225" endWordPosition="1226">dicating segues of topics. Even for an entire expository text, for which a method for displaying the thematic hierarchy with generated headers was proposed to assist a reader to explore the content (Yaari, 1998), a good summary is required to help a user understand quickly. To improve readability, the proposed algorithm divides every one-page summary into several parts, each of which consists of a heading-like sentence followed by some paragraphs. 3 Text Summarization Algorithm 3.1 Thematic Hierarchy Detection In the first stage, the proposed algorithm uses the previously published algorithm (Nakao, 1999) to detect the thematic hierarchy of a text based on lexical cohesion measured by term repetitions. The output of this stage is a set of lists consisting of thematic boundary candidate sections (TBCS). The lists correspond individually to every layer of the hierarchy and are composed of TBCSs that separate the source text into thematic textual units of approximately the same size. 3.1.1 Thematic Hierarchy Detection Algorithm First, the algorithm calculates a cohesion score at fixed-width intervals in a source text. According to Hearst&apos;s work (1994), a cohesion score is calculated based on the </context>
<context position="9915" citStr="Nakao, 1999" startWordPosition="1637" endWordPosition="1638">raged score. Since the textual area just before the point at which EC plotted is always in the left window when one of the averaged cohesion scores is calculated, EC indicates the strength of forward (left-to-right) cohesion at a point. Conversely, BC indicates the strength of backward cohesion at a point. In the figure, EP is, Equilibrium Point, the point at which EC and BC have an identical value. The algorithm checks for EC and BC starting from the beginning till the end of the source text; and it records a TBCS, as depicted by the rectangle, whenever an equilibrium point is detected (see (Nakao, 1999) for more information). 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 Location in Text [words] Figure 2: Example of Thematic Hierarchy For a sample text, Figure 2 shows the resulting thematic hierarchy that was detected 10900 11000 11100 11200 11300 11400 11500 11600 11700 Location in Text [words] Cohesion Score 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 minimal FC (4.4) moving average range EP FC &lt; BC FC &gt; BC (4.4.1) minimal BC C FC BC TBCS Section Boundary Window Width [words] entire 5120 2560 1280 B(0) 640 [0] [0] [1] [2] [0] [1] [2] [3] [4] [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] (4.2) (</context>
<context position="15112" citStr="Nakao, 1999" startWordPosition="2530" endWordPosition="2531">y their subtopics. For example, the starting point of a set of three consecutive columns identically entitled &amp;quot;The Great Cultural Revolution&amp;quot; in the &amp;quot;Chinese Revolution&amp;quot; series was detected using 1,280 word width window, as well as those of other three sets of consecutive columns entitled identically. Thus, the proposed algorithm is expected to be effective for arbitrarily selecting the size of textual units corresponding to different grading topics. However, there are problems about how to determine a boundary point in the range defined by a TBCS. Although the previously published algorithm (Nakao, 1999) determines a boundary point with minimal points of cohesion scores for the smallest window width, the accuracy degrades substantially (see Table 3). The boundary sentence identification algorithm given below is a solution to this problem. 3.2 Boundary Sentence Identification In the second stage, from sentences in a TBCS, the algorithm identifies a boundary sentence, indicating where a topic corresponding to a textual unit probably starts, and selects a lead sentence that probably indicates the contents of subsequent parts in the same textual unit. Figure 3 shows the algorithm in detail. 3.2.1</context>
<context position="19811" citStr="Nakao, 1999" startWordPosition="3310" endWordPosition="3311"> row marked B.S. shows a boundary sentence, which has positive relative forward relevance (0.016 in the fourth column of the row) and the greatest increment from the previous value (-0.017). The row marked L.S. shows a lead sentence, which has the greatest forward relevance (0.022 in the third column of the row) among all sentences after the boundary sentence. 3.2.3 Evaluation of Boundary Identification Table 3 shows recall and precision rates of the boundary identification algorithm in the same format as Table 1. Compared with the results obtained using the previous version of the algorithm (Nakao, 1999), as shown in the minimal cohesion columns, the proposed algorithm identifies more accurate boundaries cates that heading terms (i.e., terms appeared in headings) are effectively detected by scoring terms with the part of the formula in the summation operator. (the boundary sentence columns). In addition, boundary sentence identification was successful for 75% of the correct TBCSs, that is, TBCSs including correct boundaries� (see unified TBCS in Table 1). Thus, the proposed boundary sentence identification algorithm is judged to be effective. Table 3 also summarizes a feature of the proposed </context>
</contexts>
<marker>Nakao, 1999</marker>
<rawString>Yoshio Nakao. 1999. Thematic hierarchy detection of a text using lexical cohesion. Journal of the Association for Natural Language Processing, 6(6):83-112. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
<author>Mandar Mitra</author>
</authors>
<title>Automatic text decomposition using text segments and text themes.</title>
<date>1996</date>
<booktitle>In Proc. of Hypertext &apos;96,</booktitle>
<pages>53--65</pages>
<contexts>
<context position="5339" citStr="Salton et al., 1996" startWordPosition="863" endWordPosition="866"> to document type, heuristic rules for detection must be prepared for every document type. That is a problem. Moreover, the logical structure of a text does not always correspond to its thematic hierarchy, especially if a section consists of an overview clause followed by other clauses that can be divided into several groups by their subtopics. Since then, based on Hearst&apos;s work (1994), an algorithm for detecting the thematic hierarchy of a text using only lexical cohesion (Haliday and Hasan, 1976) measured by term repetitions was developed (Nakao, 1999). In comparison with some alternatives (Salton et al., 1996; Yaari, 1998), one of the features of the algorithm is that it can decompose a text into thematic textual units of approximately the same size, ranging from units just smaller than the entire text to units of about one paragraph. In this paper, a summarization algorithm based on this feature is proposed. The second problem is related to the textual coherence of a one-page summary itself. A three-sentence extract of a large text, which the proposed algorithm is designed to generate for an appropriate grading topic, tend to form a collection of unrelated sentences if it is generated by simple e</context>
</contexts>
<marker>Salton, Singhal, Buckley, Mitra, 1996</marker>
<rawString>Gerard Salton, Amit Singhal, Chris Buckley, and Mandar Mitra. 1996. Automatic text decomposition using text segments and text themes. In Proc. of Hypertext &apos;96, pages 53{65. the Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Yaari</author>
</authors>
<title>Texplore — exploring expository texts via hierarchical representation.</title>
<date>1998</date>
<booktitle>In Proc. of CVIF &apos;98,</booktitle>
<pages>25--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5353" citStr="Yaari, 1998" startWordPosition="867" endWordPosition="868">uristic rules for detection must be prepared for every document type. That is a problem. Moreover, the logical structure of a text does not always correspond to its thematic hierarchy, especially if a section consists of an overview clause followed by other clauses that can be divided into several groups by their subtopics. Since then, based on Hearst&apos;s work (1994), an algorithm for detecting the thematic hierarchy of a text using only lexical cohesion (Haliday and Hasan, 1976) measured by term repetitions was developed (Nakao, 1999). In comparison with some alternatives (Salton et al., 1996; Yaari, 1998), one of the features of the algorithm is that it can decompose a text into thematic textual units of approximately the same size, ranging from units just smaller than the entire text to units of about one paragraph. In this paper, a summarization algorithm based on this feature is proposed. The second problem is related to the textual coherence of a one-page summary itself. A three-sentence extract of a large text, which the proposed algorithm is designed to generate for an appropriate grading topic, tend to form a collection of unrelated sentences if it is generated by simple extraction of i</context>
<context position="7102" citStr="Yaari, 1998" startWordPosition="1165" endWordPosition="1166">ther. Moreover, inferring the contexts in which they appear is difficult. To prevent this problem, the proposed algorithm is designed to extract sentences from only the lead part of every topic. The third problem is related to the readability of a summary. A one-page summary is much shorter than a very long text, such as a one-hundred-page book, but is too long to read easily without some breaks indicating segues of topics. Even for an entire expository text, for which a method for displaying the thematic hierarchy with generated headers was proposed to assist a reader to explore the content (Yaari, 1998), a good summary is required to help a user understand quickly. To improve readability, the proposed algorithm divides every one-page summary into several parts, each of which consists of a heading-like sentence followed by some paragraphs. 3 Text Summarization Algorithm 3.1 Thematic Hierarchy Detection In the first stage, the proposed algorithm uses the previously published algorithm (Nakao, 1999) to detect the thematic hierarchy of a text based on lexical cohesion measured by term repetitions. The output of this stage is a set of lists consisting of thematic boundary candidate sections (TBCS</context>
</contexts>
<marker>Yaari, 1998</marker>
<rawString>Yaakov Yaari. 1998. Texplore — exploring expository texts via hierarchical representation. In Proc. of CVIF &apos;98, pages 25{31. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>