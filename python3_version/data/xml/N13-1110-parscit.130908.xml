<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996849">
Same Referent, Different Words:
Unsupervised Mining of Opaque Coreferent Mentions
</title>
<author confidence="0.999446">
Marta Recasens*, Matthew Can†, and Dan Jurafsky*
</author>
<affiliation confidence="0.9932705">
*Linguistics Department, Stanford University, Stanford, CA 94305
†Computer Science Department, Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.998776">
recasens@google.com, {mattcan,jurafsky}@stanford.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840526315789">
Coreference resolution systems rely heav-
ily on string overlap (e.g., Google Inc. and
Google), performing badly on mentions with
very different words (opaque mentions) like
Google and the search giant. Yet prior at-
tempts to resolve opaque pairs using ontolo-
gies or distributional semantics hurt precision
more than improved recall. We present a new
unsupervised method for mining opaque pairs.
Our intuition is to restrict distributional se-
mantics to articles about the same event, thus
promoting referential match. Using an En-
glish comparable corpus of tech news, we built
a dictionary of opaque coreferent mentions
(only 3% are in WordNet). Our dictionary can
be integrated into any coreference system (it
increases the performance of a state-of-the-art
system by 1% F1 on all measures) and is eas-
ily extendable by using news aggregators.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999318">
Repetition is one of the most common coreferential
devices in written text, making string-match features
important to all coreference resolution systems. In
fact, the scores achieved by just head match and a
rudimentary form of pronominal resolution1 are not
far from that of state-of-the-art systems (Recasens
and Hovy, 2010). This suggests that opaque men-
tions (i.e., lexically different) such as iPad and the
Cupertino slate are a serious problem for modern
systems: they comprise 65% of the non-pronominal
</bodyText>
<footnote confidence="0.855623">
1Closest NP with the same gender and number.
</footnote>
<bodyText confidence="0.999867533333333">
errors made by the Stanford system on the CoNLL-
2011 data. Solving this problem is critical for over-
coming the recall gap of state-of-the-art systems
(Haghighi and Klein, 2010; Stoyanov et al., 2009).
Previous systems have turned either to ontologies
(Ponzetto and Strube, 2006; Uryupina et al., 2011;
Rahman and Ng, 2011) or distributional semantics
(Yang and Su, 2007; Kobdani et al., 2011; Bansal
and Klein, 2012) to help solve these errors. But nei-
ther semantic similarity nor hypernymy are the same
as coreference: Microsoft and Google are distribu-
tionally similar but not coreferent; people is a hy-
pernym of both voters and scientists, but the peo-
ple can corefer with the voters, but is less likely
to corefer with the scientists. Thus ontologies lead
to precision problems, and to recall problems like
missing NE descriptions (e.g., Apple and the iPhone
maker) and metonymies (e.g., agreement and word-
ing), while distributional systems lead to precision
problems like coreferring Microsoft and the Moun-
tain View giant because of their similar vector rep-
resentation (release, software, update).
We increase precision by drawing on the intuition
that referents that are both similar and participate in
the same event are likely to corefer. We restrict dis-
tributional similarity to collections of articles that
discuss the same event. In the following two doc-
uments on the Nexus One from different sources,
we take the subjects of the identical verb release—
Google and the Mountain View giant—as coreferent.
</bodyText>
<figureCaption confidence="0.573388">
Document 1: Google has released a software update.
Document 2: The Mountain View giant released an update.
</figureCaption>
<bodyText confidence="0.4860245">
Based on this idea, we introduce a new unsuper-
vised method that uses verbs in comparable corpora
</bodyText>
<page confidence="0.949378">
897
</page>
<note confidence="0.474279">
Proceedings of NAACL-HLT 2013, pages 897–906,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999898285714286">
as pivots for extracting the hard cases of corefer-
ence resolution, and build a dictionary of opaque
coreferent mentions (i.e., the dictionary entries are
pairs of mentions). This dictionary is then inte-
grated into the Stanford coreference system (Lee et
al., 2011), resulting in an average 1% improvement
in the F1 score of all the evaluation measures.
Our work points out the importance of context to
decide whether a specific mention pair is coreferent.
On the one hand, we need to know what semantic
relations are potentially coreferent (e.g., content and
video). On the other, we need to distinguish contexts
that are compatible for coreference—(1) and (2-a)—
from those that are not—(1) and (2-b).
</bodyText>
<listItem confidence="0.9195292">
(1) Elemental helps those big media entities process
content across a full slate of mobile devices.
(2) a. Elemental provides the picks and shovels to
make video work across multiple devices.
b. Elemental is powering the video for HBO Go.
</listItem>
<bodyText confidence="0.9769555">
Our dictionary of opaque coreferent pairs is our so-
lution to the first problem, and we report on some
preliminary work on context compatibility to ad-
dress the second problem.
2 Building a Dictionary for Coreference
To build a dictionary of semantic relations that are
appropriate for coreference we will use a cluster of
documents about the same news event, which we
call a story. Consider as an example the story Sprint
blocks out vacation days for employees. We deter-
mine using tf-idf the representative verbs for this
story, the main actions and events of the story (e.g.,
block out). Since these verbs are representative of
the story, different instances across documents in the
cluster are likely to refer to the same events (Sprint
blocks out... and the carrier blocks out... ). By the
same logic, the subjects and objects of the verbs are
also likely to be coreferent (Sprint and the carrier).
</bodyText>
<subsectionHeader confidence="0.993455">
2.1 Comparable corpus
</subsectionHeader>
<bodyText confidence="0.999987625">
To build our dictionary, we require a monolingual
comparable corpus, containing clusters of docu-
ments from different sources that discuss the same
story. To ensure likely coreference, the story must
be the very same; documents that are merely clus-
tered by (general) topic do not suffice. The corpus
does not need to be parallel in the sense that docu-
ments in the same cluster do not need to be sentence
aligned.
We used Techmeme,2 a news aggregator for tech-
nology news, to construct a comparable corpus. Its
website lists the major tech stories, each with links
to several articles from different sources. We used
the Readability API3 to download and extract the ar-
ticle text for each document. We scraped two years
worth of data from Techmeme and only took stories
containing at least 5 documents. Our corpus con-
tains approximately 160 million words, 25k stories,
and 375k documents. Using a corpus from Tech-
meme means that our current coreference dictionary
is focused on the technological domain. Our method
can be easily extended to other domains, however,
since getting comparable corpora is relatively sim-
ple from the many similar news aggregator sites.
</bodyText>
<subsectionHeader confidence="0.995299">
2.2 Extraction
</subsectionHeader>
<bodyText confidence="0.99992125">
After building our corpus, we used Stanford’s
CoreNLP tools4 to tokenize the text and annotate it
with POS tags and named entity types. We parsed
the text using the MaltParser 1.7, a linear time de-
pendency parser (Nivre et al., 2004).5
We then extracted the representative verbs of each
story by ranking the verbs in each story according
to their tf-idf scores. We took the top ten to be the
representative set. For each of these verbs, we clus-
tered together its subjects and objects (separately)
across instances of the verb in the document clus-
ter, excluding pronouns and NPs headed by the same
noun. For example, suppose that crawl is a represen-
tative verb and that in one document we have Google
crawls web pages and The search giant crawls sites
in another document. We will create the clusters
{Google, the search giant} and {web pages, sites}.
When detecting representative verbs, we kept
phrasal verbs as a unit (e.g., give up) and excluded
auxiliary and copular verbs,6 light verbs,7 and report
</bodyText>
<footnote confidence="0.996464375">
2http://www.techmeme.com
3http://www.readability.com/developers/api
4http://nlp.stanford.edu/software/corenlp.shtml
5http://www.maltparser.org
6Auxiliary and copular verbs include appear, be, become,
do, have, seem.
7Light verbs include do, get, give, go, have, keep, make, put,
set, take.
</footnote>
<page confidence="0.99703">
898
</page>
<bodyText confidence="0.9999024375">
verbs,8 as they are rarely representative of a story
and tend to add noise to our dictionary. To increase
recall, we also considered the synonyms from Word-
Net and nominalizations from NomBank of the rep-
resentative verbs, thus clustering together the sub-
jects and objects of any synonym as well as the ar-
guments of nominalizations.9 We used syntactic re-
lations instead of semantic roles because the Malt-
Parser is faster than any SRL system, but we checked
for frequent syntactic structures in which the agent
and patient are inverted, such as passive and ergative
constructions.10
From each cluster of subject or object mentions,
we generated all pairs of mentions. This forms the
initial version of our dictionary. The next sections
describe how we filter and generalize these pairs.
</bodyText>
<subsectionHeader confidence="0.997453">
2.3 Filtering
</subsectionHeader>
<bodyText confidence="0.999871722222222">
We manually analyzed 200 random pairs and clas-
sified them into coreference and spurious relations.
The spurious relations were caused by errors due to
the parser, the text extraction, and violations of our
algorithm assumption (i.e., the representative verb
does not refer to a unique event). We employed a fil-
tering strategy to improve the precision of the dictio-
nary. We used a total of thirteen simple rules, which
are shown in Table 1. For instance, we sometimes
get the same verb with non-coreferent arguments,
especially in tech news that compare companies or
products. In these cases, NEs are often used, and so
we can get rid of a large number of errors by auto-
matically removing pairs in which both mentions are
NEs (e.g., Google and Samsung).
Before filtering, 53% of all relations were good
coreference relations versus 47% spurious ones. Of
the relations that remained after filtering, 74% were
</bodyText>
<footnote confidence="0.831529857142857">
8Report verbs include argue, claim, say, suggest, tell, etc.
9As a general rule, we extract possessive phrases as subjects
(e.g. Samsung’s plan) and of-phrases as objects (e.g. develop-
ment of the new logo).
10We can easily detect passive subjects (i-b) as they have their
own dependency label, and ergative subjects (ii-b) using a list
of ergative verbs extracted from Levin (1993).
</footnote>
<page confidence="0.332632">
(i) a. Developers hacked the device.
</page>
<figure confidence="0.8763239375">
b. The device was hacked.
(ii) a. Police scattered the crowds.
b. The crowds scattered.
Both mentions are NEs
Both mentions appear in the same document
Object of a negated verb
Enumeration or list environment
Sentence is ill-formed
Number NE
Temporal NE
Quantifying noun
Coordinated
Verb is preceded by a determiner or an adjective
Head is not nominal
Sentence length &gt; 100
Mention length &gt; 70% of sentence length
</figure>
<tableCaption confidence="0.683528">
Table 1: Filters to improve the dictionary precision. Un-
less otherwise noted, the filter was applied if either men-
tion in the relation satisfied the condition.
</tableCaption>
<bodyText confidence="0.9995355">
coreferent and only 26% were spurious. In total,
about half of the dictionary relations were removed
in the filtering process, resulting in a total of 128,492
coreferent pairs.
</bodyText>
<subsectionHeader confidence="0.985106">
2.4 Generalization
</subsectionHeader>
<bodyText confidence="0.9999776">
The final step of generating our dictionary is to pro-
cess the opaque mention pairs so that they gener-
alize better. We strip mentions of any determiners,
relative clauses, and -ing and -ed clauses. However,
we retain adjectives and prepositional modifiers be-
cause they are sometimes necessary for corefer-
ence to hold (e.g., online piracy and distribution
of pirated material). We also generalize NEs to
their types so that our dictionary entries can func-
tion as templates (e.g., Cook’s departure becomes
&lt;person&gt;’s departure), but we keep NE tokens that
are in the head position as these are pairs containing
world knowledge (e.g., iPad and slate). Finally, we
replace all tokens with their lemmas. Table 2 shows
a snapshot of the dictionary.
</bodyText>
<subsectionHeader confidence="0.999942">
2.5 Semantics of coreference
</subsectionHeader>
<bodyText confidence="0.999946888888889">
From manually classifying a sample of 200 dictio-
nary pairs (e.g., Table 2), we find that our dictio-
nary includes many synonymy (e.g., IPO and offer-
ing) and hypernymy relations (e.g., phone and de-
vice), which are the relations that are typically ex-
tracted from ontologies for coreference resolution.
However, not all synonyms and hypernyms are valid
for coreference (recall the voters-people vs. scien-
tists-people example in the introduction), so our dic-
</bodyText>
<page confidence="0.990234">
899
</page>
<figure confidence="0.994997260869565">
Mention 1 Mention 2
offering IPO
user consumer
phone device
Apple company
hardware key digital lock
iPad slate
content photo
bug issue
password login information
Google search giant
site company
filing complaint
company government
TouchPad tablet
medical record medical file
version handset
information credit card
government chairman
app software
Android platform
the leadership change &lt;person&gt;’s departure
change update
</figure>
<tableCaption confidence="0.983026">
Table 2: Coreference relations in our dictionary.
</tableCaption>
<bodyText confidence="0.9999752">
tionary only includes the ones that are relevant for
coreference (e.g., update and change). Furthermore,
only 3% of our 128,492 opaque pairs are related in
WordNet, confirming that our method is introducing
a large number of new semantic relations.
We also discover other semantic relations that are
relevant for coreference, such as various metonymy
relations like mentioning the part for the whole.
Again though, we can use some part-whole rela-
tions coreferentially (e.g., car and engine) but not
others (e.g., car and window). Our dictionary in-
cludes part-whole relations that have been observed
as coreferent at least once (e.g., company and site).
We also extract world-knowledge descriptions for
NEs (e.g., Google and the Internet giant).
</bodyText>
<sectionHeader confidence="0.958785" genericHeader="general terms">
3 Integration into a Coreference System
</sectionHeader>
<bodyText confidence="0.999857">
We next integrated our dictionary into an existing
coreference resolution system to see if it improves
resolution.
</bodyText>
<subsectionHeader confidence="0.999722">
3.1 Stanford coreference resolution system
</subsectionHeader>
<bodyText confidence="0.998840333333333">
Our baseline is the Stanford coreference resolution
system (Lee et al., 2011) which was the highest-
scoring system in the CoNLL-2011 Shared Task,
</bodyText>
<figure confidence="0.906142222222222">
Sieve number Sieve name
1 Discourse processing
2 Exact string match
3 Relaxed string match
4 Precise constructs
5–7 Strict head match
8 Proper head noun match
9 Relaxed head match
10 Pronoun match
</figure>
<tableCaption confidence="0.989768">
Table 3: Rules of the baseline system.
</tableCaption>
<bodyText confidence="0.999908333333333">
and was also part of the highest-scoring system in
the CoNLL-2012 Shared Task (Fernandes et al.,
2012). It is a rule-based system that includes a to-
tal of ten rules (or “sieves”) for entity coreference,
shown in Table 3. The sieves are applied from high-
est to lowest precision, each rule extending entities
(i.e., mention clusters) built by the previous tiers, but
never modifying links previously made. The major-
ity of the sieves rely on string overlap.11
The highly modular architecture made it easy for
us to integrate additional sieves using our dictionary
to increase recall.
</bodyText>
<subsectionHeader confidence="0.997744">
3.2 Dictionary sieves
</subsectionHeader>
<bodyText confidence="0.999883578947368">
We propose four new sieves, each one using a differ-
ent granularity level from our dictionary, with each
consecutive sieve using higher precision relations
than the previous one. The Dict 1 sieve uses only
the heads of mentions in each relation (e.g., devices).
Dict 2 uses the heads and one premodifier, if it ex-
ists (e.g., iOS devices). Dict 3 uses the heads and up
to two premodifiers (e.g., new iOS devices). Dict 4
uses the full mentions, including any postmodifiers
(e.g., new iOS devices for businesses).
We take advantage of frequency counts to get rid
of low-precision coreference pairs and only keep
(i) pairs that have been seen more than 75 times
(Dict 1) or 15 times (Dict 2, Dict 3, Dict 4);
and (ii) pairs with a frequency count larger than 8
(Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normal-
ized PMI score larger than 0.18. We use the nor-
malized PMI score (Bouma, 2009) as a measure of
association between the mentions mi and mj of a
</bodyText>
<footnote confidence="0.82740025">
11Exceptions: sieve 1 links first-person pronouns inside a
quotation with the speaker; sieve 4 links mention pairs that ap-
pear in an appositive, copular, acronym, etc., construction; sieve
10 implements generic pronominal coreference resolution.
</footnote>
<page confidence="0.979174">
900
</page>
<bodyText confidence="0.97166355">
dictionary pair, computed as
`ln p(( )p(m3)) / − ln as
mj )
These thresholds were set on the development set.
Since the different coreference rules in the Stan-
ford system are arranged in decreasing order of pre-
cision, we start by applying the sieve that uses the
highest-precision relations in the dictionary (Dict 4),
followed by Dict 3, Dict 2, and Dict 1. We add
these new sieves right before the last sieve, as the
pronominal sieve can perform better if opaque men-
tions have been successfully linked. The current
sieves only use the dictionary for linking singular
mentions, as the experiments on the dev showed that
plural mentions brought too much noise.
For any mention pair under analysis, each sieve
checks whether it is supported by the dictionary as
well as whether basic constraints are satisfied, such
as number, animacy and NE-type agreement, and
NE–common noun order (not the opposite).
</bodyText>
<sectionHeader confidence="0.999598" genericHeader="keywords">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.941166">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999964636363637">
Although our dictionary creation technology can ap-
ply across domains, our current coreference dictio-
nary is focused on the technical domain, so we cre-
ated a coreference labeled corpus in this domain for
evaluation. We extracted new data from Techmeme
(different from that used to extract the dictionary) to
create a development and a test set. It is important
to note that we do not need comparable data at this
stage. A massive comparable corpus is only needed
for mining the coreference dictionary (Section 2);
once it is built, it can be used for solving corefer-
ence within and across documents.
The annotation was performed by two experts, us-
ing the Callisto annotation tool. The development
and test sets were annotated with coreference rela-
tions following the OntoNotes guidelines (Pradhan
et al., 2007). We annotated full NPs (with all mod-
ifiers), excluding appositive phrases and predicate
nominals. Only premodifiers that were proper nouns
or possessive phrases were annotated. We extended
the OntoNotes guidelines by also annotating single-
tons. Table 4 shows the dataset statistics.
</bodyText>
<table confidence="0.999631333333333">
Dataset Stories Docs Tokens Entities Mentions
Dev 4 27 7837 1360 2279
Test 24 24 8547 1341 2452
</table>
<tableCaption confidence="0.999475">
Table 4: Dataset statistics: development (dev) and test.
</tableCaption>
<subsectionHeader confidence="0.972371">
4.2 Evaluation measures
</subsectionHeader>
<bodyText confidence="0.970432">
We evaluated using six coreference measures, as
they sometimes provide different results and there is
no agreement on a standard. We used the scorer of
the CoNLL-2011 Shared Task (Pradhan et al., 2011).
</bodyText>
<listItem confidence="0.995252176470588">
• MUC (Vilain et al., 1995). Link-based metric
that measures how many links the true and sys-
tem partitions have in common.
• B3 (Bagga and Baldwin, 1998). Mention-based
metric that measures the proportion of mention
overlap between gold and predicted entities.
• CEAF-03 (Luo, 2005). Mention-based metric
that, unlike B3, enforces a one-to-one align-
ment between gold and predicted entities.
• CEAF-04 (Luo, 2005). The entity-based ver-
sion of the above metric.
• BLANC (Recasens and Hovy, 2011). Link-
based metric that considers both coreference
and non-coreference links.
• CoNLL (Denis and Baldridge, 2009). Average
of MUC, B3 and CEAF-04. It was the official
metric of the CoNLL-2011 Shared Task.
</listItem>
<subsectionHeader confidence="0.811258">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9997944375">
We always start from the baseline, which corre-
sponds to the Stanford system with the sieves listed
in Table 3. This is the set of sieves that won the
CoNLL-2011 Shared Task (Pradhan et al., 2011),
and they exclude WordNet.
Table 5 shows the incremental scores, on the de-
velopment set, for the four sieves that use the dictio-
nary, corresponding to the different granularity lev-
els, from the highest precision one (Dict 4) to the
lowest one (Dict 1). The largest improvement is
achieved by Dict 4 and Dict 3, as they improve re-
call (R) without hurting precision (P). R is equiva-
lent to P for CEAF-04, and vice versa. The other
two sieves increase R further, especially Dict 1,
but also decrease P, although the trade-off for the
F-score (F1) is still positive. It is the best score, with
</bodyText>
<page confidence="0.854877">
3.
</page>
<bodyText confidence="0.731762">
the exception of B
</bodyText>
<page confidence="0.961644">
901
</page>
<figure confidence="0.970311538461539">
MUC
B3
CEAF-03
CEAF-04
BLANC
75.3
74.6
75.5
75.4
75.5
System
R P F1
R / P / F1
R P F1
Baseline
+Dict 4
+Dict 3
+Dict 2
+Dict 1
74.5
75.0
75.2
75.1
75.2
55.9 72.8 63.3
57.0 72.8 63.9
</figure>
<table confidence="0.882928772727273">
57.6 72.8 64.3
57.6 72.5 64.2
58.4 71.9 64.5
74.1 89.8 81.2
75.1 89.4 81.6
75.4 89.3 81.7
75.4 89.1 81.7
75.7 88.5 81.6
R P F1
85.2 73.6 79.0
85.2 74.3 79.4
85.1 74.6 79.5
85.0 74.6 79.5
84.6 75.1 79.6
R P B
66.6 87.1 72.6
68.2 87.3 74.2
68.4 87.2 74.4
68.4 87.0 74.3
68.6 86.6 74.4
CoNLL
F1
</table>
<tableCaption confidence="0.9934535">
Table 5: Incremental results for the four sieves using our dictionary on the development set. Baseline is the Stanford
system without the WordNet sieves. Scores are on gold mentions.
</tableCaption>
<table confidence="0.999493285714286">
System R MUC F1 R B3 F1 CEAF-03 CEAF-04 F1 BLANC B CoNLL
P P R / P / F1 R P R P F1
Baseline 62.4 78.2 69.4 73.7 89.5 80.8 75.1 86.2 73.8 79.5 71.4 88.6 77.3 76.6
w/ WN 63.5 75.3 68.9 74.2 87.5 80.3 74.1 83.7 74.1 78.6 71.8 87.3 77.3 75.9
w/ Dict 64.7* 77.6* 70.6* 75.7* 88.5* 81.6* 76.5* 85.3* 75.0* 79.9* 74.6* 88.6 79.9* 77.3*
w/ Dict + 64.8* 77.8* 70.7* 75.7* 88.6* 81.7* 76.5* 85.5* 75.1* 80.0* 74.6* 88.7 79.9* 77.5*
Context
</table>
<tableCaption confidence="0.9648135">
Table 6: Performance on the test set. Scores are on gold mentions. Stars indicate a statistically significant difference
with respect to the baseline.
</tableCaption>
<bodyText confidence="0.992429714285714">
Table 6 reports the scores on the test set and com-
pares the scores obtained by adding the WordNet
sieves to the baseline (w/ WN) with those obtained
by adding the dictionary sieves (w/ Dict). Whereas
adding WordNet only brings a small improvement
in R that is much lower than the loss in P, the dic-
tionary sieves succeed in increasing R by a larger
amount and at a smaller cost to P, resulting in a sig-
nificant improvement in F1: 1.2 points according to
MUC, 0.8 points according to B3, 1.4 points accord-
ing to CEAF-03, 0.4 points according to CEAF-04,
2.6 points according to BLANC, and 0.7 points ac-
cording to CoNLL. Section 5.2 presents the last line
(w/ Dict + Context).
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="introduction">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.971909">
5.1 Error analysis
</subsectionHeader>
<bodyText confidence="0.999640333333333">
Thanks to the dictionary, the coreference system im-
proves the baseline by establishing coreference links
between the bolded mentions in (3) and (4).
</bodyText>
<listItem confidence="0.854733">
(3) With Groupon Inc.’s stock down by half from its IPO
</listItem>
<bodyText confidence="0.88327475">
price and the company heading into its first earnings
report since an accounting blowup [...] outlining op-
portunity ahead and the promise of new products for
the daily-deals company.
</bodyText>
<listItem confidence="0.603375666666667">
(4) Thompson revealed the diagnosis as evidence arose
that seemed to contradict his story about why he was
not responsible for a degree listed on his resume that
he does not have, the newspaper reports, citing anony-
mous sources familiar with the situation [...] a Yahoo
board committee appointed to investigate the matter.
</listItem>
<bodyText confidence="0.999454636363636">
The first case requires world knowledge and the sec-
ond case, semantic knowledge.
We manually analyzed 40 false positive errors
caused by the dictionary sieves. Only a small num-
ber of them were due to noise in the dictionary. The
majority of errors were due to the discourse context:
the two mentions could be coreferent, but not in the
given context. For example, Apple and company are
potentially coreferent—which is successfully cap-
tured by our dictionary—and while they are coref-
erent in (5), they are not in (6).12
</bodyText>
<listItem confidence="0.6499215">
(5) It will only get better as Apple will be updating it
with iOS6, an operating system that the company will
likely be showing off this summer.
(6) Since Apple reinvented the segment, Microsoft is the
latest entrant into the tablet market, banking on its
Windows 8 products to bridge the gap between PCs
and tablets. [...] The company showed off Windows 8
last September.
</listItem>
<footnote confidence="0.8166705">
12Examples in this section show gold coreference relations in
bold and incorrectly predicted coreferent mentions in italics.
</footnote>
<page confidence="0.993169">
902
</page>
<bodyText confidence="0.9999876">
In these cases it does not suffice to check whether
the opaque mention pair is included in the corefer-
ence dictionary, but we need a method for taking the
surrounding context into account. In the next section
we present our preliminary work in this direction.
</bodyText>
<subsectionHeader confidence="0.999348">
5.2 Context fit
</subsectionHeader>
<bodyText confidence="0.997266047619047">
To help the coreference system choose the right an-
tecedent in examples like (6), we exploit the fact
that the company is closely followed by Windows 8,
which is a clue for selecting Microsoft instead of Ap-
ple as the antecedent. We devise a contextual con-
straint that rules out a mention pair if the contexts are
incompatible. To check for context compatibility,
we borrow the idea of topic signatures from Lin and
Hovy (2000) and that Agirre et al. (2001) used for
Word Sense Disambiguation. Instead of identifying
the keywords of a topic, we find the NEs that tend
to co-occur with another NE. For example, the sig-
nature for Apple should include terms like iPhone,
MacBook, iOS, Steve Jobs, etc. This is what we call
the NE signature for Apple.
To construct NE signatures, we first compute the
log-likelihood ratio (LLR) statistic between NEs in
our corpus (the same one used to build the dictio-
nary). Then, the signature for a NE, w, is the list of
k other NEs that have the highest LLR with w. The
LLR between two NEs, w1 and w2, is −2ln L(H1),
</bodyText>
<equation confidence="0.7383004">
L(H2)
where H1 is the hypothesis that
P(w1 E sentjw2 E sent) = P(w1 E sentjw2 E� sent),
H2 is the hypothesis that
P(w1 E sentjw2 E sent) =� P(w1 E sentjw2 E� sent),
</equation>
<bodyText confidence="0.989670212121212">
and L(·) is the likelihood. We assume a binomial
distribution for the likelihood.
Once we have NE signatures, we determine the
context fit as follows. When the system compares a
NE antecedent with a (non-NE) anaphor, we check
whether any NEs in the anaphor’s sentence are in
the antecedent’s signature. We also check whether
the antecedent is in the signature list of any NE’s in
the anaphor’s sentence. If neither of these is true,
we do not allow the system to link the antecedent
and the anaphor. In (6), Apple is not linked with the
company because it is not in Windows’ signature,
and Windows is not in Apple’s signature either (but
Microsoft is in Windows’ signature).
The last two lines in Table 6 compare the scores
without using this contextual feature (w/ Dict) with
those using context (w/ Dict + Context). Our feature
for context compatibility leads to a small but posi-
tive improvement, taking the final improvement of
the dictionary sieves to be about 1 percentage point
above the baseline according to all six evaluation
measures. We leave as future work to test this idea
on a larger test set and refine it further so as to ad-
dress more challenging cases where comparing NEs
is not enough, like in (7).
(7) Snapchat will notify users [...] The program is avail-
able for free in Apple’s App Store [...] While the com-
pany “attempts to delete image data as soon as possi-
ble after the message is transmitted,” it cannot guaran-
tee messages will always be deleted.
To resolve (7), it would be helpful to know that
Snapchat is a picture messaging platform, as the
context mentions image data and messages.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999931785714286">
Existing ontologies are not optimal for solving
opaque coreferent mentions because of both a preci-
sion and a recall problem (Lee et al., 2011; Uryupina
et al., 2011). On the other hand, using data-driven
methods such as distributional semantics for coref-
erence resolution suffers especially from a precision
problem (Ng, 2007). Our work combines ideas from
distributional semantics and paraphrase acquisition
methods in order to efficiently use contextual infor-
mation to extract coreference relations.
The main idea that we borrow from paraphrase
acquisition is the use of monolingual (non-parallel)
comparable corpora, which have been exploited
to extract both sentence-level (Barzilay and McK-
eown, 2001) and sub-sentential-level paraphrases
(Shinyama and Sekine, 2003; Wang and Callison-
Burch, 2011). To ensure that the NPs are coreferent,
we limit the meaning of comparable corpora to col-
lections of documents that report on the very same
story, as opposed to collections of documents that
are about the same (general) topic. However, the
distinguishing factor is that while most paraphrasing
studies, including Lin and Pantel (2001), use NEs—
or nouns in general—as pivots to learn paraphrases
of their surrounding context, we use verbs as pivots
to learn coreference relations at the NP level.
There are many similarities between paraphrase
and coreference, and our work is most similar to
</bodyText>
<page confidence="0.996109">
903
</page>
<bodyText confidence="0.999832764705882">
that by Wang and Callison-Burch (2011). However,
some paraphrases that might not be considered to
be valid (e.g., under $200 and around $200) can
be acceptable coreference relations. Unlike Wang
and Callison-Burch (2011), we do not work on doc-
ument pairs but on sets of at least five (comparable)
documents, and we do not require sentence align-
ment, but just verb alignment.
Another source of inspiration is the work by Bean
and Riloff (2004). They use contextual roles (i.e.,
the role that an NP plays in an event) for extract-
ing patterns that can be used in coreference reso-
lution, showing the relevance of verbs in deciding
on coreference between their arguments. However,
they use a very small corpus (two domains) and do
not aim to build a dictionary. The idea of creating
a repository of extracted concept-instance relations
appears in Fleischman et al. (2003), but restricted
to person-role pairs, e.g. Yasser Arafat and leader.
Although it was originally designed for answering
who-is questions, Daum´e III and Marcu (2005) suc-
cessfully used it for coreference resolution.
The coreference relations that we extract might
overlap but go beyond those detected by Bansal and
Klein (2012)’s Web-based features. First, they focus
on NP headwords, while we extract full NPs, includ-
ing multi-word mentions. Second, the fact that they
use the Google n-gram corpus means that the two
headwords must appear at most four words apart,
thus ruling out coreferent mentions that can only ap-
pear far from each other. Finally, while their extrac-
tion patterns focus on synonymy and hypernymy re-
lations, we discover other types of semantic relations
that are relevant for coreference (Section 2.5).
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988511111111">
We have pointed out an important problem with cur-
rent coreference resolution systems: their heavy re-
liance on string overlap. Pronouns aside, opaque
mentions account for 65% of the errors made by
state-of-the-art systems. To improve coreference
scores beyond 60-70%, we therefore need to make
better use of semantic and world knowledge to deal
with non-identical-string coreference. But, as we
have also shown, coreference is not the same as se-
mantic similarity or hypernymy. Only certain se-
mantic relations in certain contexts are good cues for
coreference. We therefore need semantic resources
specifically targeted at coreference.
We proposed a new solution for detecting opaque
mention pairs: restricting distributional similarity to
a comparable corpus of articles about the very same
story, thus ensuring that similar mentions will also
likely be coreferent. We used this corpus to build a
dictionary focused on coreference, and successfully
extracted the specific semantic and world knowledge
relevant for coreference. The resulting dictionary
can be added on top of any coreference system to
increase recall at a minimum cost to precision. Inte-
grated into the Stanford coreference resolution sys-
tem, which won the CoNLL-2011 shared task, the
F-score increases about 1 percentage point accord-
ing to all of the six evaluation measures. The dictio-
nary and NE signatures are available on the Web.13
We showed that apart from the need for extracting
coreference-specific semantic and world knowledge,
we need to take into account the context surrounding
the mentions. The results from our preliminary work
for identifying incompatible contexts is promising.
Our unsupervised method for extracting opaque
coreference relations can be easily extended to other
domains by using online news aggregators, and
trained on more data to build a more comprehensive
dictionary that can increase recall even further. We
integrated the dictionary into a rule-based corefer-
ence system, but it remains for future work to in-
tegrate it into a learning-based architecture, where
the system can combine the dictionary features with
other features. This can also make it easier to in-
clude contextual features that take into account how
well a dictionary pair fits in a specific context.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999351">
We would like to thank the members of the Stanford
NLP Group, Valentin Spitkovsky, and Ed Hovy for
valuable comments at various stages of the project.
The first author was supported by a Beatriu de
Pin´os postdoctoral scholarship (2010 BP-A 00149)
from Generalitat de Catalunya. We also gratefully
acknowledge the support of Defense Advanced Re-
search Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181.
</bodyText>
<footnote confidence="0.829599">
13http://nlp.stanford.edu/pubs/coref-dictionary.zip
</footnote>
<page confidence="0.994693">
904
</page>
<sectionHeader confidence="0.989381" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855075471698">
Eneko Agirre, Olatz Ansa, David Martinez, and Eduard
Hovy. 2001. Enriching wordnet concepts with topic
signatures. In Proceedings of the NAACL Workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, pages 23–28.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC 1998 Workshop on Linguistic Coreference,
pages 563–566.
Mohit Bansal and Dan Klein. 2012. Coreference seman-
tics from web features. In Proceedings of ACL, pages
389–398.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50–57.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of NAACL-HTL.
Geolof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceedings
of the Biennial GSCL Conference, pages 31–40.
Hal Daum´e III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
HLT-EMNLP, pages 97–104.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87–96.
Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Pro-
ceedings of CoNLL - Shared Task, pages 41–48.
Michael Fleischman, Eduard Hovy, and Abdessamad
Echihabi. 2003. Offline strategies for online question
answering: answering questions before they are asked.
In Proceedings of ACL, pages 1–7.
Aria Haghighi and Dan Klein. 2010. Coreference resolu-
tion in a modular, entity-centered model. In Proceed-
ings of HLT-NAACL, pages 385–393.
Hamidreza Kobdani, Hinrich Sch¨utze, Michael
Schiehlen, and Hans Kamp. 2011. Bootstrap-
ping coreference resolution using word associations.
In Proceedings of ACL, pages 783–792.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford’s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 Shared Task. In Proceedings
of CoNLL - Shared Task, pages 28–34.
Beth Levin. 1993. English Verb Class and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings of COLING, pages 495–501.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of inference rules from text. In Proceedings of the
ACM SIGKDD, pages 323–328.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of HLT-EMNLP, pages
25–32.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689–
1694.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceedings
of CoNLL, pages 49–56.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of HLT-NAACL, pages 192–199.
Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted coreference: Identifying entities and events in
OntoNotes. In Proceedings of ICSC, pages 446–453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL - Shared Task, pages 1–27.
Altaf Rahman and Vincent Ng. 2011. Coreference reso-
lution with world knowledge. In Proceedings of ACL,
pages 814–824.
Marta Recasens and Eduard Hovy. 2010. Corefer-
ence resolution across corpora: Languages, coding
schemes, and preprocessing information. In Proceed-
ings of ACL, pages 1423–1432.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(4):485–510.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Proceedings
of ACL, pages 65–71.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-art.
In Proceedings of ACL-IJCNLP, pages 656–664.
Olga Uryupina, Massimo Poesio, Claudio Giuliano, and
Kateryna Tymoshenko. 2011. Disambiguation and
filtering methods in using web knowledge for coref-
erence resolution. In Proceedings of FLAIRS, pages
317–322.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC-6, pages 45–52.
</reference>
<page confidence="0.986499">
905
</page>
<reference confidence="0.997607777777778">
Rui Wang and Chris Callison-Burch. 2011. Para-
phrase fragment extraction from monolingual compa-
rable corpora. In Proceedings of the 4th ACL Work-
shop on Building and Using Comparable Corpora,
pages 52–60.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from auto-
matically discovered patterns. In Proceedings of ACL,
pages 528–535.
</reference>
<page confidence="0.998601">
906
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321601">
<title confidence="0.994652">Same Referent, Different Unsupervised Mining of Opaque Coreferent Mentions</title>
<author confidence="0.950149">Matthew</author>
<author confidence="0.950149">Dan</author>
<affiliation confidence="0.43482">Department, Stanford University, Stanford, CA Science Department, Stanford University, Stanford, CA</affiliation>
<abstract confidence="0.99869875">Coreference resolution systems rely heavon string overlap (e.g., Inc. performing badly on mentions with different words like search Yet prior attempts to resolve opaque pairs using ontologies or distributional semantics hurt precision more than improved recall. We present a new unsupervised method for mining opaque pairs. intuition is to semantics to articles about the same event, thus promoting referential match. Using an English comparable corpus of tech news, we built a dictionary of opaque coreferent mentions (only 3% are in WordNet). Our dictionary can be integrated into any coreference system (it increases the performance of a state-of-the-art system by 1% F1 on all measures) and is easily extendable by using news aggregators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
<author>Eduard Hovy</author>
</authors>
<title>Enriching wordnet concepts with topic signatures.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<pages>23--28</pages>
<contexts>
<context position="23976" citStr="Agirre et al. (2001)" startWordPosition="3946" endWordPosition="3949">ionary, but we need a method for taking the surrounding context into account. In the next section we present our preliminary work in this direction. 5.2 Context fit To help the coreference system choose the right antecedent in examples like (6), we exploit the fact that the company is closely followed by Windows 8, which is a clue for selecting Microsoft instead of Apple as the antecedent. We devise a contextual constraint that rules out a mention pair if the contexts are incompatible. To check for context compatibility, we borrow the idea of topic signatures from Lin and Hovy (2000) and that Agirre et al. (2001) used for Word Sense Disambiguation. Instead of identifying the keywords of a topic, we find the NEs that tend to co-occur with another NE. For example, the signature for Apple should include terms like iPhone, MacBook, iOS, Steve Jobs, etc. This is what we call the NE signature for Apple. To construct NE signatures, we first compute the log-likelihood ratio (LLR) statistic between NEs in our corpus (the same one used to build the dictionary). Then, the signature for a NE, w, is the list of k other NEs that have the highest LLR with w. The LLR between two NEs, w1 and w2, is −2ln L(H1), L(H2) w</context>
</contexts>
<marker>Agirre, Ansa, Martinez, Hovy, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, David Martinez, and Eduard Hovy. 2001. Enriching wordnet concepts with topic signatures. In Proceedings of the NAACL Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 23–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the LREC 1998 Workshop on Linguistic Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="18199" citStr="Bagga and Baldwin, 1998" startWordPosition="2922" endWordPosition="2925"> OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. Dataset Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 and CEAF-04. It was the official metric of the CoNLL-2011 Shared Task. 4.3 Results We always start from the baseline, which co</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the LREC 1998 Workshop on Linguistic Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Coreference semantics from web features.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>389--398</pages>
<contexts>
<context position="2153" citStr="Bansal and Klein, 2012" startWordPosition="318" endWordPosition="321">opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement and wording), while distributional systems lead to precision problems like coreferring Microsoft and the Mountain Vi</context>
<context position="28952" citStr="Bansal and Klein (2012)" startWordPosition="4782" endWordPosition="4785">ce resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successfully used it for coreference resolution. The coreference relations that we extract might overlap but go beyond those detected by Bansal and Klein (2012)’s Web-based features. First, they focus on NP headwords, while we extract full NPs, including multi-word mentions. Second, the fact that they use the Google n-gram corpus means that the two headwords must appear at most four words apart, thus ruling out coreferent mentions that can only appear far from each other. Finally, while their extraction patterns focus on synonymy and hypernymy relations, we discover other types of semantic relations that are relevant for coreference (Section 2.5). 7 Conclusions We have pointed out an important problem with current coreference resolution systems: thei</context>
</contexts>
<marker>Bansal, Klein, 2012</marker>
<rawString>Mohit Bansal and Dan Klein. 2012. Coreference semantics from web features. In Proceedings of ACL, pages 389–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="27067" citStr="Barzilay and McKeown, 2001" startWordPosition="4478" endWordPosition="4482">ause of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There a</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of ACL, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HTL.</booktitle>
<contexts>
<context position="28202" citStr="Bean and Riloff (2004)" startWordPosition="4662" endWordPosition="4665">context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our work is most similar to 903 that by Wang and Callison-Burch (2011). However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations. Unlike Wang and Callison-Burch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successful</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>David Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proceedings of NAACL-HTL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geolof Bouma</author>
</authors>
<title>Normalized (pointwise) mutual information in collocation extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Biennial GSCL Conference,</booktitle>
<pages>31--40</pages>
<contexts>
<context position="15333" citStr="Bouma, 2009" startWordPosition="2456" endWordPosition="2457">he heads and one premodifier, if it exists (e.g., iOS devices). Dict 3 uses the heads and up to two premodifiers (e.g., new iOS devices). Dict 4 uses the full mentions, including any postmodifiers (e.g., new iOS devices for businesses). We take advantage of frequency counts to get rid of low-precision coreference pairs and only keep (i) pairs that have been seen more than 75 times (Dict 1) or 15 times (Dict 2, Dict 3, Dict 4); and (ii) pairs with a frequency count larger than 8 (Dict 1) or 2 (Dict 2, Dict 3, Dict 4) and a normalized PMI score larger than 0.18. We use the normalized PMI score (Bouma, 2009) as a measure of association between the mentions mi and mj of a 11Exceptions: sieve 1 links first-person pronouns inside a quotation with the speaker; sieve 4 links mention pairs that appear in an appositive, copular, acronym, etc., construction; sieve 10 implements generic pronominal coreference resolution. 900 dictionary pair, computed as `ln p(( )p(m3)) / − ln as mj ) These thresholds were set on the development set. Since the different coreference rules in the Stanford system are arranged in decreasing order of precision, we start by applying the sieve that uses the highest-precision rela</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Geolof Bouma. 2009. Normalized (pointwise) mutual information in collocation extraction. In Proceedings of the Biennial GSCL Conference, pages 31–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A large-scale exploration of effective global features for a joint entity detection and tracking model.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>97--104</pages>
<marker>Daum´e, Marcu, 2005</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2005. A large-scale exploration of effective global features for a joint entity detection and tracking model. In Proceedings of HLT-EMNLP, pages 97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Global joint models for coreference resolution and named entity classification.</title>
<date>2009</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>42--87</pages>
<contexts>
<context position="18652" citStr="Denis and Baldridge, 2009" startWordPosition="2989" endWordPosition="2992">dhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 and CEAF-04. It was the official metric of the CoNLL-2011 Shared Task. 4.3 Results We always start from the baseline, which corresponds to the Stanford system with the sieves listed in Table 3. This is the set of sieves that won the CoNLL-2011 Shared Task (Pradhan et al., 2011), and they exclude WordNet. Table 5 shows the incremental scores, on the development set, for the four sieves that use the dictionary, corresponding to the different granularity levels, from the highest precision one (Dict 4) to the lowest one (Dict 1). The largest improvement is achieved by Dict 4 a</context>
</contexts>
<marker>Denis, Baldridge, 2009</marker>
<rawString>Pascal Denis and Jason Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42:87–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eraldo Fernandes</author>
<author>C´ıcero dos Santos</author>
<author>Ruy Milidi´u</author>
</authors>
<title>Latent structure perceptron with feature induction for unrestricted coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of CoNLL - Shared Task,</booktitle>
<pages>41--48</pages>
<marker>Fernandes, Santos, Milidi´u, 2012</marker>
<rawString>Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. In Proceedings of CoNLL - Shared Task, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Offline strategies for online question answering: answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="28626" citStr="Fleischman et al. (2003)" startWordPosition="4733" endWordPosition="4736">cument pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments. However, they use a very small corpus (two domains) and do not aim to build a dictionary. The idea of creating a repository of extracted concept-instance relations appears in Fleischman et al. (2003), but restricted to person-role pairs, e.g. Yasser Arafat and leader. Although it was originally designed for answering who-is questions, Daum´e III and Marcu (2005) successfully used it for coreference resolution. The coreference relations that we extract might overlap but go beyond those detected by Bansal and Klein (2012)’s Web-based features. First, they focus on NP headwords, while we extract full NPs, including multi-word mentions. Second, the fact that they use the Google n-gram corpus means that the two headwords must appear at most four words apart, thus ruling out coreferent mentions</context>
</contexts>
<marker>Fleischman, Hovy, Echihabi, 2003</marker>
<rawString>Michael Fleischman, Eduard Hovy, and Abdessamad Echihabi. 2003. Offline strategies for online question answering: answering questions before they are asked. In Proceedings of ACL, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="1912" citStr="Haghighi and Klein, 2010" startWordPosition="280" endWordPosition="283">important to all coreference resolution systems. In fact, the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision prob</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Proceedings of HLT-NAACL, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamidreza Kobdani</author>
<author>Hinrich Sch¨utze</author>
<author>Michael Schiehlen</author>
<author>Hans Kamp</author>
</authors>
<title>Bootstrapping coreference resolution using word associations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>783--792</pages>
<marker>Kobdani, Sch¨utze, Schiehlen, Kamp, 2011</marker>
<rawString>Hamidreza Kobdani, Hinrich Sch¨utze, Michael Schiehlen, and Hans Kamp. 2011. Bootstrapping coreference resolution using word associations. In Proceedings of ACL, pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL - Shared Task,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="3851" citStr="Lee et al., 2011" startWordPosition="586" endWordPosition="589">—as coreferent. Document 1: Google has released a software update. Document 2: The Mountain View giant released an update. Based on this idea, we introduce a new unsupervised method that uses verbs in comparable corpora 897 Proceedings of NAACL-HLT 2013, pages 897–906, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics as pivots for extracting the hard cases of coreference resolution, and build a dictionary of opaque coreferent mentions (i.e., the dictionary entries are pairs of mentions). This dictionary is then integrated into the Stanford coreference system (Lee et al., 2011), resulting in an average 1% improvement in the F1 score of all the evaluation measures. Our work points out the importance of context to decide whether a specific mention pair is coreferent. On the one hand, we need to know what semantic relations are potentially coreferent (e.g., content and video). On the other, we need to distinguish contexts that are compatible for coreference—(1) and (2-a)— from those that are not—(1) and (2-b). (1) Elemental helps those big media entities process content across a full slate of mobile devices. (2) a. Elemental provides the picks and shovels to make video</context>
<context position="13540" citStr="Lee et al., 2011" startWordPosition="2141" endWordPosition="2144">ain though, we can use some part-whole relations coreferentially (e.g., car and engine) but not others (e.g., car and window). Our dictionary includes part-whole relations that have been observed as coreferent at least once (e.g., company and site). We also extract world-knowledge descriptions for NEs (e.g., Google and the Internet giant). 3 Integration into a Coreference System We next integrated our dictionary into an existing coreference resolution system to see if it improves resolution. 3.1 Stanford coreference resolution system Our baseline is the Stanford coreference resolution system (Lee et al., 2011) which was the highestscoring system in the CoNLL-2011 Shared Task, Sieve number Sieve name 1 Discourse processing 2 Exact string match 3 Relaxed string match 4 Precise constructs 5–7 Strict head match 8 Proper head noun match 9 Relaxed head match 10 Pronoun match Table 3: Rules of the baseline system. and was also part of the highest-scoring system in the CoNLL-2012 Shared Task (Fernandes et al., 2012). It is a rule-based system that includes a total of ten rules (or “sieves”) for entity coreference, shown in Table 3. The sieves are applied from highest to lowest precision, each rule extendin</context>
<context position="26503" citStr="Lee et al., 2011" startWordPosition="4399" endWordPosition="4402">hallenging cases where comparing NEs is not enough, like in (7). (7) Snapchat will notify users [...] The program is available for free in Apple’s App Store [...] While the company “attempts to delete image data as soon as possible after the message is transmitted,” it cannot guarantee messages will always be deleted. To resolve (7), it would be helpful to know that Snapchat is a picture messaging platform, as the context mentions image data and messages. 6 Related Work Existing ontologies are not optimal for solving opaque coreferent mentions because of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrase</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 Shared Task. In Proceedings of CoNLL - Shared Task, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Class and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="10011" citStr="Levin (1993)" startWordPosition="1594" endWordPosition="1595">ly removing pairs in which both mentions are NEs (e.g., Google and Samsung). Before filtering, 53% of all relations were good coreference relations versus 47% spurious ones. Of the relations that remained after filtering, 74% were 8Report verbs include argue, claim, say, suggest, tell, etc. 9As a general rule, we extract possessive phrases as subjects (e.g. Samsung’s plan) and of-phrases as objects (e.g. development of the new logo). 10We can easily detect passive subjects (i-b) as they have their own dependency label, and ergative subjects (ii-b) using a list of ergative verbs extracted from Levin (1993). (i) a. Developers hacked the device. b. The device was hacked. (ii) a. Police scattered the crowds. b. The crowds scattered. Both mentions are NEs Both mentions appear in the same document Object of a negated verb Enumeration or list environment Sentence is ill-formed Number NE Temporal NE Quantifying noun Coordinated Verb is preceded by a determiner or an adjective Head is not nominal Sentence length &gt; 100 Mention length &gt; 70% of sentence length Table 1: Filters to improve the dictionary precision. Unless otherwise noted, the filter was applied if either mention in the relation satisfied th</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Class and Alternations: A Preliminary Investigation. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="23946" citStr="Lin and Hovy (2000)" startWordPosition="3940" endWordPosition="3943">luded in the coreference dictionary, but we need a method for taking the surrounding context into account. In the next section we present our preliminary work in this direction. 5.2 Context fit To help the coreference system choose the right antecedent in examples like (6), we exploit the fact that the company is closely followed by Windows 8, which is a clue for selecting Microsoft instead of Apple as the antecedent. We devise a contextual constraint that rules out a mention pair if the contexts are incompatible. To check for context compatibility, we borrow the idea of topic signatures from Lin and Hovy (2000) and that Agirre et al. (2001) used for Word Sense Disambiguation. Instead of identifying the keywords of a topic, we find the NEs that tend to co-occur with another NE. For example, the signature for Apple should include terms like iPhone, MacBook, iOS, Steve Jobs, etc. This is what we call the NE signature for Apple. To construct NE signatures, we first compute the log-likelihood ratio (LLR) statistic between NEs in our corpus (the same one used to build the dictionary). Then, the signature for a NE, w, is the list of k other NEs that have the highest LLR with w. The LLR between two NEs, w1 </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of COLING, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - Discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACM SIGKDD,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="27497" citStr="Lin and Pantel (2001)" startWordPosition="4545" endWordPosition="4548">a that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our work is most similar to 903 that by Wang and Callison-Burch (2011). However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations. Unlike Wang and Callison-Burch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentenc</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery of inference rules from text. In Proceedings of the ACM SIGKDD, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="18328" citStr="Luo, 2005" startWordPosition="2942" endWordPosition="2943">27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 and CEAF-04. It was the official metric of the CoNLL-2011 Shared Task. 4.3 Results We always start from the baseline, which corresponds to the Stanford system with the sieves listed in Table 3. This is the set of sieves that won the CoNLL-2011 Shared Task</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of HLT-EMNLP, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Shallow semantics for coreference resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>1689--1694</pages>
<contexts>
<context position="26688" citStr="Ng, 2007" startWordPosition="4428" endWordPosition="4429">o delete image data as soon as possible after the message is transmitted,” it cannot guarantee messages will always be deleted. To resolve (7), it would be helpful to know that Snapchat is a picture messaging platform, as the context mentions image data and messages. 6 Related Work Existing ontologies are not optimal for solving opaque coreferent mentions because of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>Vincent Ng. 2007. Shallow semantics for coreference resolution. In Proceedings of IJCAI, pages 1689– 1694.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="6860" citStr="Nivre et al., 2004" startWordPosition="1089" endWordPosition="1092">ocuments. Our corpus contains approximately 160 million words, 25k stories, and 375k documents. Using a corpus from Techmeme means that our current coreference dictionary is focused on the technological domain. Our method can be easily extended to other domains, however, since getting comparable corpora is relatively simple from the many similar news aggregator sites. 2.2 Extraction After building our corpus, we used Stanford’s CoreNLP tools4 to tokenize the text and annotate it with POS tags and named entity types. We parsed the text using the MaltParser 1.7, a linear time dependency parser (Nivre et al., 2004).5 We then extracted the representative verbs of each story by ranking the verbs in each story according to their tf-idf scores. We took the top ten to be the representative set. For each of these verbs, we clustered together its subjects and objects (separately) across instances of the verb in the document cluster, excluding pronouns and NPs headed by the same noun. For example, suppose that crawl is a representative verb and that in one document we have Google crawls web pages and The search giant crawls sites in another document. We will create the clusters {Google, the search giant} and {w</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="2014" citStr="Ponzetto and Strube, 2006" startWordPosition="295" endWordPosition="298">a rudimentary form of pronominal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and meton</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proceedings of HLT-NAACL, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
<author>Jessica MacBride</author>
<author>Linnea Micciulla</author>
</authors>
<title>Unrestricted coreference: Identifying entities and events in OntoNotes.</title>
<date>2007</date>
<booktitle>In Proceedings of ICSC,</booktitle>
<pages>446--453</pages>
<contexts>
<context position="17382" citStr="Pradhan et al., 2007" startWordPosition="2791" endWordPosition="2794">his domain for evaluation. We extracted new data from Techmeme (different from that used to extract the dictionary) to create a development and a test set. It is important to note that we do not need comparable data at this stage. A massive comparable corpus is only needed for mining the coreference dictionary (Section 2); once it is built, it can be used for solving coreference within and across documents. The annotation was performed by two experts, using the Callisto annotation tool. The development and test sets were annotated with coreference relations following the OntoNotes guidelines (Pradhan et al., 2007). We annotated full NPs (with all modifiers), excluding appositive phrases and predicate nominals. Only premodifiers that were proper nouns or possessive phrases were annotated. We extended the OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. Dataset Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used t</context>
</contexts>
<marker>Pradhan, Ramshaw, Weischedel, MacBride, Micciulla, 2007</marker>
<rawString>Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel, Jessica MacBride, and Linnea Micciulla. 2007. Unrestricted coreference: Identifying entities and events in OntoNotes. In Proceedings of ICSC, pages 446–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Ralph Weischedel</author>
<author>Nianwen Xue</author>
</authors>
<title>CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes.</title>
<date>2011</date>
<booktitle>In Proceedings of CoNLL - Shared Task,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="18044" citStr="Pradhan et al., 2011" startWordPosition="2894" endWordPosition="2897">rs), excluding appositive phrases and predicate nominals. Only premodifiers that were proper nouns or possessive phrases were annotated. We extended the OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. Dataset Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridg</context>
</contexts>
<marker>Pradhan, Ramshaw, Marcus, Palmer, Weischedel, Xue, 2011</marker>
<rawString>Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, Martha Palmer, Ralph Weischedel, and Nianwen Xue. 2011. CoNLL-2011 Shared Task: Modeling unrestricted coreference in OntoNotes. In Proceedings of CoNLL - Shared Task, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>814--824</pages>
<contexts>
<context position="2059" citStr="Rahman and Ng, 2011" startWordPosition="303" endWordPosition="306">ot far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement and wording), while di</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of ACL, pages 814–824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>Coreference resolution across corpora: Languages, coding schemes, and preprocessing information.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1423--1432</pages>
<contexts>
<context position="1509" citStr="Recasens and Hovy, 2010" startWordPosition="214" endWordPosition="217"> news, we built a dictionary of opaque coreferent mentions (only 3% are in WordNet). Our dictionary can be integrated into any coreference system (it increases the performance of a state-of-the-art system by 1% F1 on all measures) and is easily extendable by using news aggregators. 1 Introduction Repetition is one of the most common coreferential devices in written text, making string-match features important to all coreference resolution systems. In fact, the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; K</context>
</contexts>
<marker>Recasens, Hovy, 2010</marker>
<rawString>Marta Recasens and Eduard Hovy. 2010. Coreference resolution across corpora: Languages, coding schemes, and preprocessing information. In Proceedings of ACL, pages 1423–1432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>BLANC: Implementing the Rand index for coreference evaluation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="18539" citStr="Recasens and Hovy, 2011" startWordPosition="2973" endWordPosition="2976">ifferent results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 and CEAF-04. It was the official metric of the CoNLL-2011 Shared Task. 4.3 Results We always start from the baseline, which corresponds to the Stanford system with the sieves listed in Table 3. This is the set of sieves that won the CoNLL-2011 Shared Task (Pradhan et al., 2011), and they exclude WordNet. Table 5 shows the incremental scores, on the development set, for the four sieves that use the dictionary, corresponding to the different granularity levels, fr</context>
</contexts>
<marker>Recasens, Hovy, 2011</marker>
<rawString>Marta Recasens and Eduard Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(4):485–510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Paraphrase acquisition for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>65--71</pages>
<contexts>
<context position="27131" citStr="Shinyama and Sekine, 2003" startWordPosition="4486" endWordPosition="4489">ryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, 2003; Wang and CallisonBurch, 2011). To ensure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our</context>
</contexts>
<marker>Shinyama, Sekine, 2003</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase acquisition for information extraction. In Proceedings of ACL, pages 65–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>656--664</pages>
<contexts>
<context position="1936" citStr="Stoyanov et al., 2009" startWordPosition="284" endWordPosition="287">ce resolution systems. In fact, the scores achieved by just head match and a rudimentary form of pronominal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall prob</context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. In Proceedings of ACL-IJCNLP, pages 656–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
<author>Massimo Poesio</author>
<author>Claudio Giuliano</author>
<author>Kateryna Tymoshenko</author>
</authors>
<title>Disambiguation and filtering methods in using web knowledge for coreference resolution.</title>
<date>2011</date>
<booktitle>In Proceedings of FLAIRS,</booktitle>
<pages>317--322</pages>
<contexts>
<context position="2037" citStr="Uryupina et al., 2011" startWordPosition="299" endWordPosition="302">minal resolution1 are not far from that of state-of-the-art systems (Recasens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement </context>
<context position="26527" citStr="Uryupina et al., 2011" startWordPosition="4403" endWordPosition="4406">here comparing NEs is not enough, like in (7). (7) Snapchat will notify users [...] The program is available for free in Apple’s App Store [...] While the company “attempts to delete image data as soon as possible after the message is transmitted,” it cannot guarantee messages will always be deleted. To resolve (7), it would be helpful to know that Snapchat is a picture messaging platform, as the context mentions image data and messages. 6 Related Work Existing ontologies are not optimal for solving opaque coreferent mentions because of both a precision and a recall problem (Lee et al., 2011; Uryupina et al., 2011). On the other hand, using data-driven methods such as distributional semantics for coreference resolution suffers especially from a precision problem (Ng, 2007). Our work combines ideas from distributional semantics and paraphrase acquisition methods in order to efficiently use contextual information to extract coreference relations. The main idea that we borrow from paraphrase acquisition is the use of monolingual (non-parallel) comparable corpora, which have been exploited to extract both sentence-level (Barzilay and McKeown, 2001) and sub-sentential-level paraphrases (Shinyama and Sekine, </context>
</contexts>
<marker>Uryupina, Poesio, Giuliano, Tymoshenko, 2011</marker>
<rawString>Olga Uryupina, Massimo Poesio, Claudio Giuliano, and Kateryna Tymoshenko. 2011. Disambiguation and filtering methods in using web knowledge for coreference resolution. In Proceedings of FLAIRS, pages 317–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="18073" citStr="Vilain et al., 1995" startWordPosition="2900" endWordPosition="2903">ses and predicate nominals. Only premodifiers that were proper nouns or possessive phrases were annotated. We extended the OntoNotes guidelines by also annotating singletons. Table 4 shows the dataset statistics. Dataset Stories Docs Tokens Entities Mentions Dev 4 27 7837 1360 2279 Test 24 24 8547 1341 2452 Table 4: Dataset statistics: development (dev) and test. 4.2 Evaluation measures We evaluated using six coreference measures, as they sometimes provide different results and there is no agreement on a standard. We used the scorer of the CoNLL-2011 Shared Task (Pradhan et al., 2011). • MUC (Vilain et al., 1995). Link-based metric that measures how many links the true and system partitions have in common. • B3 (Bagga and Baldwin, 1998). Mention-based metric that measures the proportion of mention overlap between gold and predicted entities. • CEAF-03 (Luo, 2005). Mention-based metric that, unlike B3, enforces a one-to-one alignment between gold and predicted entities. • CEAF-04 (Luo, 2005). The entity-based version of the above metric. • BLANC (Recasens and Hovy, 2011). Linkbased metric that considers both coreference and non-coreference links. • CoNLL (Denis and Baldridge, 2009). Average of MUC, B3 </context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrase fragment extraction from monolingual comparable corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th ACL Workshop on Building and Using Comparable Corpora,</booktitle>
<pages>52--60</pages>
<contexts>
<context position="27798" citStr="Wang and Callison-Burch (2011)" startWordPosition="4594" endWordPosition="4597">nsure that the NPs are coreferent, we limit the meaning of comparable corpora to collections of documents that report on the very same story, as opposed to collections of documents that are about the same (general) topic. However, the distinguishing factor is that while most paraphrasing studies, including Lin and Pantel (2001), use NEs— or nouns in general—as pivots to learn paraphrases of their surrounding context, we use verbs as pivots to learn coreference relations at the NP level. There are many similarities between paraphrase and coreference, and our work is most similar to 903 that by Wang and Callison-Burch (2011). However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations. Unlike Wang and Callison-Burch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment. Another source of inspiration is the work by Bean and Riloff (2004). They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on corefere</context>
</contexts>
<marker>Wang, Callison-Burch, 2011</marker>
<rawString>Rui Wang and Chris Callison-Burch. 2011. Paraphrase fragment extraction from monolingual comparable corpora. In Proceedings of the 4th ACL Workshop on Building and Using Comparable Corpora, pages 52–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
</authors>
<title>Coreference resolution using semantic relatedness information from automatically discovered patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>528--535</pages>
<contexts>
<context position="2106" citStr="Yang and Su, 2007" startWordPosition="310" endWordPosition="313">sens and Hovy, 2010). This suggests that opaque mentions (i.e., lexically different) such as iPad and the Cupertino slate are a serious problem for modern systems: they comprise 65% of the non-pronominal 1Closest NP with the same gender and number. errors made by the Stanford system on the CoNLL2011 data. Solving this problem is critical for overcoming the recall gap of state-of-the-art systems (Haghighi and Klein, 2010; Stoyanov et al., 2009). Previous systems have turned either to ontologies (Ponzetto and Strube, 2006; Uryupina et al., 2011; Rahman and Ng, 2011) or distributional semantics (Yang and Su, 2007; Kobdani et al., 2011; Bansal and Klein, 2012) to help solve these errors. But neither semantic similarity nor hypernymy are the same as coreference: Microsoft and Google are distributionally similar but not coreferent; people is a hypernym of both voters and scientists, but the people can corefer with the voters, but is less likely to corefer with the scientists. Thus ontologies lead to precision problems, and to recall problems like missing NE descriptions (e.g., Apple and the iPhone maker) and metonymies (e.g., agreement and wording), while distributional systems lead to precision problems</context>
</contexts>
<marker>Yang, Su, 2007</marker>
<rawString>Xiaofeng Yang and Jian Su. 2007. Coreference resolution using semantic relatedness information from automatically discovered patterns. In Proceedings of ACL, pages 528–535.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>