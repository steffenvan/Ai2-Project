<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.078907">
<title confidence="0.924941">
Reducing SMT Rule Table with Monolingual Key Phrase
</title>
<author confidence="0.939571">
Zhongjun He† Yao Meng† Yajuan Lii $ Hao Yu† Qun Liu$
</author>
<affiliation confidence="0.65217">
† Fujitsu R&amp;D Center CO., LTD, Beijing, China
</affiliation>
<email confidence="0.785291">
{hezhongjun, mengyao, yu}@cn.fujitsu.com
</email>
<affiliation confidence="0.9323295">
$ Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.99755">
{lvyajuan, liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947363636363">
This paper presents an effective approach
to discard most entries of the rule table for
statistical machine translation. The rule ta-
ble is filtered by monolingual key phrases,
which are extracted from source text us-
ing a technique based on term extraction.
Experiments show that 78% of the rule ta-
ble is reduced without worsening trans-
lation performance. In most cases, our
approach results in measurable improve-
ments in BLEU score.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967490566038">
In statistical machine translation (SMT) commu-
nity, the state-of-the-art method is to use rules that
contain hierarchical structures to model transla-
tion, such as the hierarchical phrase-based model
(Chiang, 2005). Rules are more powerful than
conventional phrase pairs because they contain
structural information for capturing long distance
reorderings. However, hierarchical translation
systems often suffer from a large rule table (the
collection of rules), which makes decoding slow
and memory-consuming.
In the training procedure of SMT systems, nu-
merous rules are extracted from the bilingual cor-
pus. During decoding, however, many of them are
rarely used. One of the reasons is that these rules
have low quality. The rule quality are usually eval-
uated by the conditional translation probabilities,
which focus on the correspondence between the
source and target phrases, while ignore the quality
of phrases in a monolingual corpus.
In this paper, we address the problem of reduc-
ing the rule table with the information of mono-
lingual corpus. We use C-value, a measurement
of automatic term recognition, to score source
phrases. A source phrase is regarded as a key
phrase if its score greater than a threshold. Note
that a source phrase is either a flat phrase consists
of words, or a hierarchical phrase consists of both
words and variables. For rule table reduction, the
rule whose source-side is not key phrase is dis-
carded.
Our approach is different from the previous re-
search. Johnson et al. (2007) reduced the phrase
table based on the significance testing of phrase
pair co-occurrence in bilingual corpus. The ba-
sic difference is that they used statistical infor-
mation of bilingual corpus while we use that of
monolingual corpus. Shen et al. (2008) pro-
posed a string-to-dependency model, which re-
stricted the target-side of a rule by dependency
structures. Their approach greatly reduced the rule
table, however, caused a slight decrease of trans-
lation quality. They obtained improvements by
incorporating an additional dependency language
model. Different from their research, we restrict
rules on the source-side. Furthermore, the system
complexity is not increased because no additional
model is introduced.
The hierarchical phrase-based model (Chiang,
2005) is used to build a translation system. Exper-
iments show that our approach discards 78% of the
rule table without worsening the translation qual-
ity.
</bodyText>
<sectionHeader confidence="0.941689" genericHeader="method">
2 Monolingual Phrase Scoring
</sectionHeader>
<subsectionHeader confidence="0.924651">
2.1 Frequency
</subsectionHeader>
<bodyText confidence="0.9992312">
The basic metrics for phrase scoring is the fre-
quency that a phrase appears in a monolingual cor-
pus. The more frequent a source phrase appears in
a corpus, the greater possibility the rule that con-
tains the source phrase may be used.
However, one limitation of this metrics is that if
we filter the rule table by the source phrase with
lower frequency, most long phrase pairs will be
discarded. Because the longer the phrase is, the
less possibility it appears. However, long phrases
</bodyText>
<page confidence="0.973099">
121
</page>
<note confidence="0.9259005">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121–124,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.997278222222222">
are very helpful for reducing ambiguity since they
contains more information than short phrases.
Another limitation is that the frequency metrics
focuses on a phrase appearing by itself while ig-
nores it appears as a substring of longer phrases.
It is therefore inadequate for hierarchical phrases.
We use an example for illustration. Considering
the following three rules (the subscripts indicate
word alignments):
</bodyText>
<equation confidence="0.999608666666667">
R1 :
�É1 &apos;f�Ÿ2 �A3 ��4 MAA5
accept1 President3 Bush2 ’s4 invitation5
R2 :
�É1 &apos;f�Ÿ2 X3 ��4 MAA5
accept1 X3 Bush2 ’s4 invitation5
R3 :
li�É1 X2 �` 3 MAA4
accept1 X2 ’s3 invitation4
</equation>
<bodyText confidence="0.999942636363636">
We use f1, f2 and f3 to represent their source-
sides, respectively. The hierarchical phrases f2
and f3 are sub-strings of f1. However, R3 is sug-
gested to be more useful than R2. The reason is
that f3 may appears in various phrases, such as
“I�É %�Q �� MAA, accept France ’s invitation”.
While f2 almost always appears in f1, indicating
that the variable X may not be replaced with other
words expect “President”. It indicates that R2 is
not likely to be useful, although f2 may appears
frequently in a corpus.
</bodyText>
<subsectionHeader confidence="0.998944">
2.2 C-value
</subsectionHeader>
<bodyText confidence="0.999297769230769">
C-value, a measurement of automatic term recog-
nition, is proposed by Frantzi and Ananiadou
(1996) to extract nested collocations, collocations
that substrings of other longer ones.
We use C-value for two reasons: on one hand,
it uses rich factors besides phrase frequency, e.g.
the phrase length, the frequency that a sub-phrase
appears in longer phrases. Thus it is appropriate
for extracting hierarchical phrases. On the other
hand, the computation of C-value is efficient.
Analogous to (Frantzi and Ananiadou, 1996),
we use 4 factors (L, F, S, N) to determine if a
phrase p is a key phrase:
</bodyText>
<listItem confidence="0.915154185185185">
1. L(p), the length of p;
2. F(p), the frequency that p appears in a cor-
pus;
Algorithm 1 Key Phrase Extraction
Input: Monolingual Text
Output: Key Phrase Table KP
1: Extract candidate phrases
2: for all phrases p in length descending order
do
3: if N(p) = 0 then
4: C-value = (L(p) − 1) x F(p)
5: else
C-value = (L(p) − 1) x (F(p) − S(p)
6: N(p))
7: end if
8: if C-value &gt; ε then
9: add p to KP
10: end if
11: for all sub-strings q of p do
12: S(q) = S(q) + F(p) − S(p)
13: N(q) = N(q) + 1
14: end for
15: end for
3. S(p), the frequency that p appears as a sub-
string in other longer phrases;
4. N(p), the number of phrases that contain p as
a substring.
</listItem>
<bodyText confidence="0.999902">
Given a monolingual corpus, key phrases can be
extracted efficiently according to Algorithm 1.
Firstly (line 1), all possible phrases are ex-
tracted as candidates of key phrases. This step
is analogous to the rule extraction as described in
(Chiang, 2005). The basic difference is that there
are no word alignment constraints for monolingual
phrase extraction, which therefore results in a sub-
stantial number of candidate phrases. We use the
following restrictions to limit the phrase number:
</bodyText>
<listItem confidence="0.990362444444445">
1. The length of a candidate phrase is limited to
pl;
2. The length of the initial phrase used to create
hierarchical phrases is limited to ipl;
3. The number of variables in hierarchical
phrases is limited to nv, and there should be
at least 1 word between variables;
4. The frequency of a candidate phrase appears
in a corpus should be greater than freq.
</listItem>
<bodyText confidence="0.997984666666667">
In our experiments, we set pl = 5, ipl = 10, nv =
2, freq = 3. Note that the first 3 settings are used
in (Chiang, 2005) for rule extraction.
</bodyText>
<page confidence="0.995487">
122
</page>
<bodyText confidence="0.998156821428571">
Secondly (line 3 to 7), for each candidate
phrase, C-value is computed according to the
phrase appears by itself (line 4) or as a substring
of other long phrases (line 6). The C-value is in
direct proportion to the phrase length (L) and oc-
currences (F, 5), while in inverse proportion to the
number of phrases that contain the phrase as a sub-
string (N). This overcomes the limitations of fre-
quency measurement. A phrase is regarded as a
key phrase if its C-value is greater than a thresh-
old ε.
Finally (line 11 to 14), 5(q) and N(q) are up-
dated for each substring q.
We use the example in Section 2.1 for illustra-
tion. The quadruple for f1 is (5, 2, 0, 0), indicating
that the phrase length is 5 and appears 2 times by
itself in the corpus. Therefore C-value(f1) = 8.
The quadruple for f2 is (4, 2, 2, 1), indicating that
the phrase length is 4 and appears 2 times in the
corpus. However, the occurrences are as a sub-
string of the phrase f1. Therefore,C-value(f2) =
0. While the quadruple for f3 is (3,11,11, 9),
which indicates that the phrase length is 3 and ap-
pears 11 times as a substring in 9 phrases, thus
C-value(f3) = 19.6. Given the threshold ε = 5,
f1 and f3 are viewed as key phrases. Thus R2 will
be discarded because its source-side is not a key
phrase.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99608">
Our experiments were carried out on two language
pairs:
</bodyText>
<listItem confidence="0.983436533333334">
• Chinese-English: For this task, the corpora
are from the NIST evaluation. The parallel
corpus 1 consists of 1M sentence pairs . We
trained two trigram language models: one on
the Xinhua portion of the Gigaword corpus,
and the other on the target-side of the paral-
lel corpus. The test sets were NIST MT06
GALE set (06G) and NIST set (06N) and
NIST MT08 test set.
• German-English: For this task, the corpora
are from the WMT 2 evaluation. The paral-
lel corpus contains 1.3M sentence pairs. The
target-side was used to train a trigram lan-
guage model. The test sets were WMT06 and
WMT07.
</listItem>
<footnote confidence="0.99887275">
1LDC2002E18 (4,000 sentences), LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T10,
LDC2004T08 HK Hansards (500,000 sentences)
2http://www.statmt.org/wmt07/shared-task.html
</footnote>
<bodyText confidence="0.9995062">
For both the tasks, the word alignment were
trained by GIZA++ in two translation directions
and refined by “grow-diag-final” method (Koehn
et al., 2003). The source-side of the parallel cor-
pus is used to extract key phrases.
</bodyText>
<subsectionHeader confidence="0.925968">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.999938">
We reimplemented the state-of-the-art hierarchical
MT system, Hiero (Chiang, 2005), as the baseline
system. The results of the experiments are shown
in Table 1 and Table 2.
Table 1 shows the C-value threshold effect
on the size of the rule table, as well as the
BLEU scores. Originally, 103M and 195M rules
are respectively extracted for Chinese-English and
German-English. For both the two tasks, about
78% reduction of the rule table (for Chinese-
English ε = 200 and for German-English ε =
100) does not worsen translation performance. We
achieved improvements in BLEU on most of the
test corpora, except a slight decrease (0.06 point)
on WMT07.
We also compared the effects of frequency and
C-value metrics for the rule table reduction on
Chinese-English test sets. The rule table is re-
duced to the same size (22% of original table)
using the two metrics, separately. However, as
shown in Table 2, the frequency method decreases
the BLEU scores, while the C-value achieves im-
provements. It indicates that C-value is more ap-
propriate than frequency to evaluate the impor-
tance of phrases, because it considers more fac-
tors.
With the rule table filtered by key phrases on
the source side, the number of source phrases re-
duces. Therefore during decoding, a source sen-
tence is suggested to be decomposed into a num-
ber of “key phrases”, which are more reliable than
the discarded phrases. Thus the translation quality
does not become worse.
</bodyText>
<subsectionHeader confidence="0.999721">
3.2 Adding C-value as a Feature
</subsectionHeader>
<bodyText confidence="0.999375555555555">
Conventional phrase-based approaches performed
phrase segmentation for a source sentence with a
uniform distribution. However, they do not con-
sider the weights of source phrases. Although any
strings can be phrases, it is believed that some
strings are more likely phrases than others. We
use C-value to describe the weight of a phrase in
a monolingual corpus and add it as a feature to the
translation model:
</bodyText>
<page confidence="0.99697">
123
</page>
<table confidence="0.999283272727273">
C-value Chinese-English Germany-English
Threshold ε
Rule Table (%) 06G 06N 08 Rule Table (%) 06 07
0 100% 12.43 28.58 21.57 100% 27.30 27.95
5 61% 12.22 28.40 21.33 54% 27.39 28.05
20 44% 12.24 28.29 21.21 37% 27.47 27.94
100 28% 12.36 28.56 21.67 22% 27.54 27.89
200 22% 12.66 28.69 22.12 17% 27.26 27.80
300 20% 12.41 27.76 21.52 15% 27.41 27.69
400 18% 11.88 26.98 20.70 13% 27.36 27.76
500 16% 11.65 26.40 20.32 12% 27.25 27.76
</table>
<tableCaption confidence="0.99823">
Table 1: C-value threshold effect on the rule table size and BLEU scores.
</tableCaption>
<table confidence="0.9999078">
System Rule Table (%) 06G 06N 08
Baseline 100% 12.43 28.58 21.57
Frequency 22% 12.24 27.77 21.20
C-value 22% 12.66 28.69 22.12*
+CV-Feature 22% 12.89* 29.22*+ 22.56*+
</table>
<tableCaption confidence="0.977996">
Table 2: BLEU scores on the test sets of the Chinese-English task. * means significantly better than
baseline at p &lt; 0.01. + means significantly better than C-value at p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.9994155">
where, fk is the source-side of a rule.
The results are shown in the row +CV-Feature
in Table 2. Measurable improvements are ob-
tained on all test corpora of the Chinese-English
task by adding the C-value feature. The improve-
ments over the baseline are statistically significant
at p &lt; 0.01 by using the significant test method
described in (Koehn, 2004).
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999975416666667">
In this paper, we successfully discarded most
entries of the rule table with monolingual key
phrases. Experiments show that about 78% of the
rule table is reduced and the translation quality
does not become worse. We achieve measurable
improvements by incorporating C-value into the
translation model.
The use of key phrases is one of the simplest
method for the rule table reduction. In the future,
we will use sophisticated metrics to score phrases
and reduce the rule table size with the information
of both the source and target sides.
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999577592592593">
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263–270.
Katerina T. Frantzi and Sophia Ananiadou. 1996.
Extracting nested collocations. In COLING1996,
pages 41–46.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975, Prague, Czech Republic,
June.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
ofHLT-NAACL 2003, pages 127–133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388–395.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL-08: HLT, pages 577–585,
Columbus, Ohio, June.
</reference>
<equation confidence="0.988057333333333">
K
h(F1J) = E log(C-value( fk)) (1)
k=1
</equation>
<page confidence="0.993153">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.520747">
<title confidence="0.987308">Reducing SMT Rule Table with Monolingual Key Phrase</title>
<address confidence="0.747643">R&amp;D Center CO., LTD, Beijing, China</address>
<email confidence="0.905755">mengyao,</email>
<affiliation confidence="0.888559">Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China</affiliation>
<abstract confidence="0.993883">This paper presents an effective approach to discard most entries of the rule table for statistical machine translation. The rule tais filtered by monolingual which are extracted from source text using a technique based on term extraction. Experiments show that 78% of the rule table is reduced without worsening translation performance. In most cases, our approach results in measurable improvements in BLEU score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1025" citStr="Chiang, 2005" startWordPosition="151" endWordPosition="152">card most entries of the rule table for statistical machine translation. The rule table is filtered by monolingual key phrases, which are extracted from source text using a technique based on term extraction. Experiments show that 78% of the rule table is reduced without worsening translation performance. In most cases, our approach results in measurable improvements in BLEU score. 1 Introduction In statistical machine translation (SMT) community, the state-of-the-art method is to use rules that contain hierarchical structures to model translation, such as the hierarchical phrase-based model (Chiang, 2005). Rules are more powerful than conventional phrase pairs because they contain structural information for capturing long distance reorderings. However, hierarchical translation systems often suffer from a large rule table (the collection of rules), which makes decoding slow and memory-consuming. In the training procedure of SMT systems, numerous rules are extracted from the bilingual corpus. During decoding, however, many of them are rarely used. One of the reasons is that these rules have low quality. The rule quality are usually evaluated by the conditional translation probabilities, which fo</context>
<context position="3086" citStr="Chiang, 2005" startWordPosition="472" endWordPosition="473">tistical information of bilingual corpus while we use that of monolingual corpus. Shen et al. (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. Their approach greatly reduced the rule table, however, caused a slight decrease of translation quality. They obtained improvements by incorporating an additional dependency language model. Different from their research, we restrict rules on the source-side. Furthermore, the system complexity is not increased because no additional model is introduced. The hierarchical phrase-based model (Chiang, 2005) is used to build a translation system. Experiments show that our approach discards 78% of the rule table without worsening the translation quality. 2 Monolingual Phrase Scoring 2.1 Frequency The basic metrics for phrase scoring is the frequency that a phrase appears in a monolingual corpus. The more frequent a source phrase appears in a corpus, the greater possibility the rule that contains the source phrase may be used. However, one limitation of this metrics is that if we filter the rule table by the source phrase with lower frequency, most long phrase pairs will be discarded. Because the l</context>
<context position="6514" citStr="Chiang, 2005" startWordPosition="1080" endWordPosition="1081">: else C-value = (L(p) − 1) x (F(p) − S(p) 6: N(p)) 7: end if 8: if C-value &gt; ε then 9: add p to KP 10: end if 11: for all sub-strings q of p do 12: S(q) = S(q) + F(p) − S(p) 13: N(q) = N(q) + 1 14: end for 15: end for 3. S(p), the frequency that p appears as a substring in other longer phrases; 4. N(p), the number of phrases that contain p as a substring. Given a monolingual corpus, key phrases can be extracted efficiently according to Algorithm 1. Firstly (line 1), all possible phrases are extracted as candidates of key phrases. This step is analogous to the rule extraction as described in (Chiang, 2005). The basic difference is that there are no word alignment constraints for monolingual phrase extraction, which therefore results in a substantial number of candidate phrases. We use the following restrictions to limit the phrase number: 1. The length of a candidate phrase is limited to pl; 2. The length of the initial phrase used to create hierarchical phrases is limited to ipl; 3. The number of variables in hierarchical phrases is limited to nv, and there should be at least 1 word between variables; 4. The frequency of a candidate phrase appears in a corpus should be greater than freq. In ou</context>
<context position="9677" citStr="Chiang, 2005" startWordPosition="1633" endWordPosition="1634">M sentence pairs. The target-side was used to train a trigram language model. The test sets were WMT06 and WMT07. 1LDC2002E18 (4,000 sentences), LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T10, LDC2004T08 HK Hansards (500,000 sentences) 2http://www.statmt.org/wmt07/shared-task.html For both the tasks, the word alignment were trained by GIZA++ in two translation directions and refined by “grow-diag-final” method (Koehn et al., 2003). The source-side of the parallel corpus is used to extract key phrases. 3.1 Results We reimplemented the state-of-the-art hierarchical MT system, Hiero (Chiang, 2005), as the baseline system. The results of the experiments are shown in Table 1 and Table 2. Table 1 shows the C-value threshold effect on the size of the rule table, as well as the BLEU scores. Originally, 103M and 195M rules are respectively extracted for Chinese-English and German-English. For both the two tasks, about 78% reduction of the rule table (for ChineseEnglish ε = 200 and for German-English ε = 100) does not worsen translation performance. We achieved improvements in BLEU on most of the test corpora, except a slight decrease (0.06 point) on WMT07. We also compared the effects of fre</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Extracting nested collocations.</title>
<date>1996</date>
<booktitle>In COLING1996,</booktitle>
<pages>41--46</pages>
<contexts>
<context position="5109" citStr="Frantzi and Ananiadou (1996)" startWordPosition="812" endWordPosition="815">vitation4 We use f1, f2 and f3 to represent their sourcesides, respectively. The hierarchical phrases f2 and f3 are sub-strings of f1. However, R3 is suggested to be more useful than R2. The reason is that f3 may appears in various phrases, such as “I�É %�Q �� MAA, accept France ’s invitation”. While f2 almost always appears in f1, indicating that the variable X may not be replaced with other words expect “President”. It indicates that R2 is not likely to be useful, although f2 may appears frequently in a corpus. 2.2 C-value C-value, a measurement of automatic term recognition, is proposed by Frantzi and Ananiadou (1996) to extract nested collocations, collocations that substrings of other longer ones. We use C-value for two reasons: on one hand, it uses rich factors besides phrase frequency, e.g. the phrase length, the frequency that a sub-phrase appears in longer phrases. Thus it is appropriate for extracting hierarchical phrases. On the other hand, the computation of C-value is efficient. Analogous to (Frantzi and Ananiadou, 1996), we use 4 factors (L, F, S, N) to determine if a phrase p is a key phrase: 1. L(p), the length of p; 2. F(p), the frequency that p appears in a corpus; Algorithm 1 Key Phrase Ext</context>
</contexts>
<marker>Frantzi, Ananiadou, 1996</marker>
<rawString>Katerina T. Frantzi and Sophia Ananiadou. 1996. Extracting nested collocations. In COLING1996, pages 41–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>967--975</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2321" citStr="Johnson et al. (2007)" startWordPosition="358" endWordPosition="361">re the quality of phrases in a monolingual corpus. In this paper, we address the problem of reducing the rule table with the information of monolingual corpus. We use C-value, a measurement of automatic term recognition, to score source phrases. A source phrase is regarded as a key phrase if its score greater than a threshold. Note that a source phrase is either a flat phrase consists of words, or a hierarchical phrase consists of both words and variables. For rule table reduction, the rule whose source-side is not key phrase is discarded. Our approach is different from the previous research. Johnson et al. (2007) reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus. The basic difference is that they used statistical information of bilingual corpus while we use that of monolingual corpus. Shen et al. (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. Their approach greatly reduced the rule table, however, caused a slight decrease of translation quality. They obtained improvements by incorporating an additional dependency language model. Different from their research, we restrict rules </context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 967–975, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="9510" citStr="Koehn et al., 2003" startWordPosition="1606" endWordPosition="1609">T MT06 GALE set (06G) and NIST set (06N) and NIST MT08 test set. • German-English: For this task, the corpora are from the WMT 2 evaluation. The parallel corpus contains 1.3M sentence pairs. The target-side was used to train a trigram language model. The test sets were WMT06 and WMT07. 1LDC2002E18 (4,000 sentences), LDC2002T01, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T10, LDC2004T08 HK Hansards (500,000 sentences) 2http://www.statmt.org/wmt07/shared-task.html For both the tasks, the word alignment were trained by GIZA++ in two translation directions and refined by “grow-diag-final” method (Koehn et al., 2003). The source-side of the parallel corpus is used to extract key phrases. 3.1 Results We reimplemented the state-of-the-art hierarchical MT system, Hiero (Chiang, 2005), as the baseline system. The results of the experiments are shown in Table 1 and Table 2. Table 1 shows the C-value threshold effect on the size of the rule table, as well as the BLEU scores. Originally, 103M and 195M rules are respectively extracted for Chinese-English and German-English. For both the two tasks, about 78% reduction of the rule table (for ChineseEnglish ε = 200 and for German-English ε = 100) does not worsen tra</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>577--585</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2573" citStr="Shen et al. (2008)" startWordPosition="399" endWordPosition="402">phrase is regarded as a key phrase if its score greater than a threshold. Note that a source phrase is either a flat phrase consists of words, or a hierarchical phrase consists of both words and variables. For rule table reduction, the rule whose source-side is not key phrase is discarded. Our approach is different from the previous research. Johnson et al. (2007) reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus. The basic difference is that they used statistical information of bilingual corpus while we use that of monolingual corpus. Shen et al. (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. Their approach greatly reduced the rule table, however, caused a slight decrease of translation quality. They obtained improvements by incorporating an additional dependency language model. Different from their research, we restrict rules on the source-side. Furthermore, the system complexity is not increased because no additional model is introduced. The hierarchical phrase-based model (Chiang, 2005) is used to build a translation system. Experiments show that our approach discards 78%</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577–585, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>