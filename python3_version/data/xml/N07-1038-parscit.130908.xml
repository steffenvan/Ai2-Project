<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.991236">
Multiple Aspect Ranking using the Good Grief Algorithm
</title>
<author confidence="0.927119">
Benjamin Snyder and Regina Barzilay
</author>
<affiliation confidence="0.934127">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.998422">
{bsnyder,regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981333333333">
We address the problem of analyzing mul-
tiple related opinions in a text. For in-
stance, in a restaurant review such opin-
ions may include food, ambience and ser-
vice. We formulate this task as a multiple
aspect ranking problem, where the goal is
to produce a set of numerical scores, one
for each aspect. We present an algorithm
that jointly learns ranking models for in-
dividual aspects by modeling the depen-
dencies between assigned ranks. This al-
gorithm guides the prediction of individ-
ual rankers by analyzing meta-relations
between opinions, such as agreement and
contrast. We prove that our agreement-
based joint model is more expressive than
individual ranking models. Our empirical
results further confirm the strength of the
model: the algorithm provides significant
improvement over both individual rankers
and a state-of-the-art joint ranking model.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999262238095238">
Previous work on sentiment categorization makes an
implicit assumption that a single score can express
the polarity of an opinion text (Pang et al., 2002;
Turney, 2002; Yu and Hatzivassiloglou, 2003).
However, multiple opinions on related matters are
often intertwined throughout a text. For example,
a restaurant review may express judgment on food
quality as well as the service and ambience of the
restaurant. Rather than lumping these aspects into a
single score, we would like to capture each aspect of
the writer’s opinion separately, thereby providing a
more fine-grained view of opinions in the review.
To this end, we aim to predict a set of numeric
ranks that reflects the user’s satisfaction for each as-
pect. In the example above, we would assign a nu-
meric rank from 1-5 for each of: food quality, ser-
vice, and ambience.
A straightforward approach to this task would be
to rank&apos; the text independently for each aspect, us-
ing standard ranking techniques such as regression
or classification. However, this approach fails to ex-
ploit meaningful dependencies between users’ judg-
ments across different aspects. Knowledge of these
dependencies can be crucial in predicting accurate
ranks, as a user’s opinions on one aspect can influ-
ence his or her opinions on others.
The algorithm presented in this paper models
the dependencies between different labels via the
agreement relation. The agreement relation captures
whether the user equally likes all aspects of the item
or whether he or she expresses different degrees of
satisfaction. Since this relation can often be deter-
mined automatically for a given text (Marcu and
Echihabi, 2002), we can readily use it to improve
rank prediction.
The Good Grief model consists of a ranking
model for each aspect as well as an agreement model
which predicts whether or not all rank aspects are
&apos;In this paper, ranking refers to the task of assigning an inte-
ger from 1 to k to each instance. This task is sometimes referred
to as “ordinal regression” (Crammer and Singer, 2001) and “rat-
ing prediction” (Pang and Lee, 2005).
</bodyText>
<page confidence="0.967998">
300
</page>
<note confidence="0.799707">
Proceedings of NAACL HLT 2007, pages 300–307,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997187083333334">
equal. The Good Grief decoding algorithm pre-
dicts a set of ranks – one for each aspect – which
maximally satisfy the preferences of the individual
rankers and the agreement model. For example, if
the agreement model predicts consensus but the in-
dividual rankers select ranks (5, 5, 4), then the de-
coder decides whether to trust the the third ranker,
or alter its prediction and output (5, 5, 5) to be con-
sistent with the agreement prediction. To obtain a
model well-suited for this decoding, we also develop
a joint training method that conjoins the training of
multiple aspect models.
We demonstrate that the agreement-based joint
model is more expressive than individual ranking
models. That is, every training set that can be per-
fectly ranked by individual ranking models for each
aspect can also be perfectly ranked with our joint
model. In addition, we give a simple example of a
training set which cannot be perfectly ranked with-
out agreement-based joint inference. Our experi-
mental results further confirm the strength of the
Good Grief model. Our model significantly outper-
forms individual ranking models as well as a state-
of-the-art joint ranking model.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.994900755102041">
Sentiment Classification Traditionally, categoriza-
tion of opinion texts has been cast as a binary classi-
fication task (Pang et al., 2002; Turney, 2002; Yu and
Hatzivassiloglou, 2003; Dave et al., 2003). More
recent work (Pang and Lee, 2005; Goldberg and
Zhu, 2006) has expanded this analysis to the rank-
ing framework where the goal is to assess review
polarity on a multi-point scale. While this approach
provides a richer representation of a single opinion,
it still operates on the assumption of one opinion per
text. Our work generalizes this setting to the prob-
lem of analyzing multiple opinions – or multiple as-
pects of an opinion. Since multiple opinions in a sin-
gle text are related, it is insufficient to treat them as
separate single-aspect ranking tasks. This motivates
our exploration of a new method for joint multiple
aspect ranking.
Ranking The ranking, or ordinal regression,
problem has been extensivly studied in the Machine
Learning and Information Retrieval communities. In
this section we focus on two online ranking methods
which form the basis of our approach. The first is
a model proposed by Crammer and Singer (2001).
The task is to predict a rank y E I1, ..., k} for ev-
ery input x E R&apos;. Their model stores a weight
vector w E R&apos; and a vector of increasing bound-
aries b0 = −00 &lt; b1 &lt; ... &lt; bk−1 &lt; bk = 00
which divide the real line into k segments, one for
each possible rank. The model first scores each input
with the weight vector: score(x) = w · x. Finally,
the model locates score(x) on the real line and re-
turns the appropriate rank as indicated by the bound-
aries. Formally, the model returns the rank r such
that br−1 &lt; score(x) &lt; br. The model is trained
with the Perceptron Ranking algorithm (or “PRank
algorithm”), which reacts to incorrect predictions on
the training set by updating the weight and boundary
vectors. The PRanking model and algorithm were
tested on the EachMovie dataset with a separate
ranking model learned for each user in the database.
An extension of this model is provided by Basil-
ico and Hofmann (2004) in the context of collabora-
tive filtering. Instead of training a separate model for
each user, Basilico and Hofmann train a joint rank-
ing model which shares a set of boundaries across all
users. In addition to these shared boundaries, user-
specific weight vectors are stored. To compute the
score for input x and user i, the weight vectors for
all users are employed:
</bodyText>
<equation confidence="0.8737875">
�scorei(x) = w[i] · x + sim(i, j)(w[j] · x) (1)
9
</equation>
<bodyText confidence="0.999813272727273">
where 0 &lt; sim(i, j) &lt; 1 is the cosine similarity be-
tween users i and j, computed on the entire training
set. Once the score has been computed, the predic-
tion rule follows that of the PRanking model. The
model is trained using the PRank algorithm, with the
exception of the new definition for the scoring func-
tion.2 While this model shares information between
the different ranking problems, it fails to explicitly
model relations between the rank predictions. In
contrast, our algorithm uses an agreement model to
learn such relations and inform joint predictions.
</bodyText>
<footnote confidence="0.778194">
2In the notation of Basilico and Hofmann (2004), this def-
inition of scorei(x) corresponds to the kernel K = (Kid � +
K�� � ) � Kat � .
</footnote>
<page confidence="0.997882">
301
</page>
<sectionHeader confidence="0.996495" genericHeader="method">
3 The Algorithm
</sectionHeader>
<bodyText confidence="0.999742454545455">
The goal of our algorithm is to find a rank assign-
ment that is consistent with predictions of individ-
ual rankers and the agreement model. To this end,
we develop the Good Grief decoding procedure that
minimizes the dissatisfaction (grief) of individual
components with a joint prediction. In this section,
we formally define the grief of each component, and
a mechanism for its minimization. We then describe
our method for joint training of individual rankers
that takes into account the Good Grief decoding pro-
cedure.
</bodyText>
<subsectionHeader confidence="0.998075">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.9975904">
In an m-aspect ranking problem, we are given
a training sequence of instance-label pairs
(x1, y1), ..., (xt, yt), .... Each instance xt is a
feature vector in Rn and the label yt is a vector of
m ranks in Ym, where Y = 11, .., k} is the set of
possible ranks. The ith component of yt is the rank
for the ith aspect, and will be denoted by y[i]t. The
goal is to learn a mapping from instances to rank
sets, H : X —* Ym, which minimizes the distance
between predicted ranks and true ranks.
</bodyText>
<subsectionHeader confidence="0.990688">
3.2 The Model
</subsectionHeader>
<bodyText confidence="0.97323244">
Our m-aspect ranking model contains m+1 compo-
nents: ((w[1], b[1]), ..., (w[m], b[m]), a). The first
m components are individual ranking models, one
for each aspect, and the final component is the agree-
ment model. For each aspect i E 1...m, w[i] E Rn
is a vector of weights on the input features, and
b[i] E Rk−1 is a vector of boundaries which divide
the real line into k intervals, corresponding to the
k possible ranks. The default prediction of the as-
pect ranking model simply uses the ranking rule of
the PRank algorithm. This rule predicts the rank r
such that b[i]r−1 &lt; scorei(x) &lt; b[i]r.3 The value
scorei(x) can be defined simply as the dot product
w[i]·x, or it can take into account the weight vectors
for other aspects weighted by a measure of inter-
aspect similarity. We adopt the definition given in
equation 1, replacing the user-specific weight vec-
tors with our aspect-specific weight vectors.
3More precisely (taking into account the possibility of ties):
y[i] = min1E{1,..,k}{r : scorei(x) − b[i],. &lt; 01
The agreement model is a vector of weights a E
Rn. A value of a · x &gt; 0 predicts that the ranks of
all m aspects are equal, and a value of a · x &lt; 0
indicates disagreement. The absolute value Ja · xJ
indicates the confidence in the agreement prediction.
The goal of the decoding procedure is to predict a
joint rank for the m aspects which satisfies the in-
dividual ranking models as well as the agreement
model. For a given input x, the individual model
for aspect i predicts a default rank y[i] based on its
feature weight and boundary vectors (w[i], b[i]). In
addition, the agreement model makes a prediction
regarding rank consensus based on a · x. However,
the default aspect predictions 9[1] ... y[m] may not
accord with the agreement model. For example, if
a · x &gt; 0, but 9[i] =� y[j] for some i, j E 1...m, then
the agreement model predicts complete consensus,
whereas the individual aspect models do not.
We therefore adopt a joint prediction criterion
which simultaneously takes into account all model
components – individual aspect models as well as
the agreement model. For each possible predic-
tion r = (r[1], ..., r[m]) this criterion assesses the
level of grief associated with the ith-aspect ranking
model, gi(x, r[i]). Similarly, we compute the grief
of the agreement model with the joint prediction,
ga(x, r) (both gi and ga are defined formally below).
The decoder then predicts the m ranks which mini-
mize the overall grief:
�
</bodyText>
<equation confidence="0.9553815">
H(x) = arg rmyn ga (x, r) + � gi (x, r [i] )
i=1
</equation>
<bodyText confidence="0.998706642857143">
(2)
If the default rank predictions for the aspect models,
y� = (0[1], ..., y[m]), are in accord with the agree-
ment model (both indicating consensus or both in-
dicating contrast), then the grief of all model com-
ponents will be zero, and we simply output y. On
the other hand, if y� indicates disagreement but the
agreement model predicts consensus, then we have
the option of predicting y� and bearing the grief of
the agreement model. Alternatively, we can predict
some consensus y0 (i.e. with y0[i] = y0[j], Vi, j) and
bear the grief of the component ranking models. The
decoder H chooses the option with lowest overall
grief.4
</bodyText>
<subsectionHeader confidence="0.662821">
4This decoding criterion assumes that the griefs of the com-
</subsectionHeader>
<bodyText confidence="0.657932">
m
</bodyText>
<page confidence="0.972039">
302
</page>
<bodyText confidence="0.994913">
Now we formally define the measures of grief
used in this criterion.
Aspect Model Grief We define the grief of the ith-
aspect ranking model with respect to a rank r to be
the smallest magnitude correction term which places
the input’s score into the rth segment of the real line:
</bodyText>
<equation confidence="0.425103666666667">
gi(x, r) = min |c|
s.t.
b[i]r_1 &lt; scorei(x) + c &lt; b[i]r
</equation>
<bodyText confidence="0.788074833333333">
Agreement Model Grief Similarly, we define the
grief of the agreement model with respect to a joint
rank r = (r[1], ... , r[m]) as the smallest correction
needed to bring the agreement score into accord with
the agreement relation between the individual ranks
r[1], ... , r[m]:
</bodyText>
<figure confidence="0.9480158">
ga(x, r) = min |c|
s.t.
a x + c &gt; 0 A Vi, j E 1...m : r[i] = r[j]
V
a x + c &lt; 0 A Ii,j E 1...m : r[i] =� r[j]
</figure>
<subsectionHeader confidence="0.98341">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9901195">
Ranking models Pseudo-code for Good Grief train-
ing is shown in Figure 1. This training algorithm
is based on PRanking (Crammer and Singer, 2001),
an online perceptron algorithm. The training is per-
formed by iteratively ranking each training input x
and updating the model. If the predicted rank y� is
equal to the true rank y, the weight and boundaries
vectors remain unchanged. On the other hand, if
y� =� y, then the weights and boundaries are updated
to improve the prediction for x (step 4.c in Figure 1).
See (Crammer and Singer, 2001) for explanation
and analysis of this update rule.
Our algorithm departs from PRanking by con-
joining the updates for the m ranking models. We
achieve this by using Good Grief decoding at each
step throughout training. Our decoder H(x) (from
equation 2) uses all the aspect component models
ponent models are comparable. In practice, we take an uncali-
brated agreement model a&apos; and reweight it with a tuning param-
eter: a = αa&apos;. The value of α is estimated using the develop-
ment set. We assume that the griefs of the ranking models are
comparable since they are jointly trained.
as well as the (previously trained) agreement model
to determine the predicted rank for each aspect. In
concrete terms, for every training instance x, we pre-
dict the ranks of all aspects simultaneously (step 2 in
Figure 1). Then, for each aspect we make a separate
update based on this joint prediction (step 4 in Fig-
ure 1), instead of using the individual models’ pre-
dictions.
Agreement model The agreement model a is as-
sumed to have been previously trained on the same
training data. An instance is labeled with a positive
label if all the ranks associated with this instance are
equal. The rest of the instances are labeled as nega-
tive. This model can use any standard training algo-
rithm for binary classification such as Perceptron or
SVM optimization.
</bodyText>
<subsectionHeader confidence="0.923561">
3.4 Feature Representation
</subsectionHeader>
<bodyText confidence="0.9990691">
Ranking Models Following previous work on senti-
ment classification (Pang et al., 2002), we represent
each review as a vector of lexical features. More
specifically, we extract all unigrams and bigrams,
discarding those that appear fewer than three times.
This process yields about 30,000 features.
Agreement Model The agreement model also op-
erates over lexicalized features. The effectiveness
of these features for recognition of discourse rela-
tions has been previously shown by Marcu and Echi-
habi (2002). In addition to unigrams and bigrams,
we also introduce a feature that measures the maxi-
mum contrastive distance between pairs of words in
a review. For example, the presence of “delicious”
and “dirty” indicate high contrast, whereas the pair
“expensive” and “slow” indicate low contrast. The
contrastive distance for a pair of words is computed
by considering the difference in relative weight as-
signed to the words in individually trained PRanking
models.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999904142857143">
In this section, we prove that our model is able to
perfectly rank a strict superset of the training cor-
pora perfectly rankable by m ranking models indi-
vidually. We first show that if the independent rank-
ing models can individually rank a training set per-
fectly, then our model can do so as well. Next, we
show that our model is more expressive by providing
</bodyText>
<page confidence="0.998263">
303
</page>
<note confidence="0.586267333333333">
Input: (x1, y1), ..., (xT, yT), Agreement model a, Decoder defintion H(x) (from equation 2).
Initialize : Set w[i]1 = 0, b[i]i, ..., b[i]1�−1 = 0, b[i]1� = , i  1...m.
Loop: Fort = 1, 2, ..., T :
</note>
<listItem confidence="0.60622025">
1. Get a new instance xt  Rn.
2. Predict Yt = H(x; wt, bt, a) (Equation 2).
3. Get a new label yt.
4. For aspect i = 1, ..., m:
</listItem>
<figure confidence="0.972864">
If y[i]t = y[i]t update model (otherwise set w[i]t+1 = w[i]t, b[i]t+1
r = b[i]tr, r):
4.a For r = 1, ..., k − 1 : If y[i]t  r then y[i]t r = −1
else y[i]t r = 1.
4.b For r = 1, ..., k − 1 : If (y[i]t − r)y[i]tr  0 then τ[i]tr = y[i]tr
else τ[i]t r = 0.
4.c Update w[i]t+1  w[i]t + (Er τ[i]tr)xt.
For r = 1, ..., k − 1 update: b[i]t+1
r  b[i]tr − τ[i]tr.
Output : H(x; wT+1, bT+1, a).
</figure>
<figureCaption confidence="0.987518">
Figure 1: Good Grief Training. The algorithm is based on PRanking training algorithm. Our algorithm
differs in the joint computation of all aspect predictions Yt based on the Good Grief Criterion (step 2) and
the calculation of updates for each aspect based on the joint prediction (step 4).
</figureCaption>
<bodyText confidence="0.997685555555556">
a simple illustrative example of a training set which
can only be perfectly ranked with the inclusion of an
agreement model.
First we introduce some notation. For each train-
ing instance (xt, yt), each aspect i  1...m, and
each rank r  1...k, define an auxiliary variable
y[i]t r with y[i]t r = −1 if y[i]t  r and y[i]t r = 1
if y[i]t &gt; r. In words, y[i]t r indicates whether the
true rank y[i]t is to the right or left of a potential
rank r.
Now suppose that a training set
(x1, y1), ..., (xT , yT) is perfectly rankable for
each aspect independently. That is, for each
aspect i  1...m, there exists some ideal model
v[i]∗ = (w[i]∗, b[i]∗) such that the signed dis-
tance from the prediction to the rth boundary:
w[i]∗ · xt − b[i]∗r has the same sign as the auxil-
iary variable y[i]tr. In other words, the minimum
margin over all training instances and ranks,
γ = minr�t{(w[i]∗ · xt − b[i]∗r)y[i]tr}, is no less than
zero.
Now for the tth training instance, define an agree-
ment auxiliary variable at, where at = 1 when all
aspects agree in rank and at = −1 when at least
two aspects disagree in rank. First consider the case
where the agreement model a perfectly classifies all
training instances: (a · xt)at &gt; 0, t. It is clear
that Good Grief decoding with the ideal joint model
(w[1]∗, b[1]∗, ..., w[m]∗, b[m]∗, a) will produce
the same output as the component ranking models
run separately (since the grief will always be zero for
the default rank predictions). Now consider the case
where the training data is not linearly separable with
regard to agreement classification. Define the mar-
gin of the worst case error to be β = maxt{|(a·xt) |:
(a·xt)at &lt; 0}. If β &lt; γ, then again Good Grief de-
coding will always produce the default results (since
the grief of the agreement model will be at most β in
cases of error, whereas the grief of the ranking mod-
els for any deviation from their default predictions
will be at least γ). On the other hand, if β  γ, then
the agreement model errors could potentially disrupt
the perfect ranking. However, we need only rescale
w∗ := w∗(� � + 0 and b∗ := b∗(� + E) to ensure that
the grief of the ranking models will always exceed
the grief of the agreement model in cases where the
latter is in error. Thus whenever independent rank-
ing models can perfectly rank a training set, a joint
ranking model with Good Grief decoding can do so
as well.
Now we give a simple example of a training set
which can only be perfectly ranked with the addi-
tion of an agreement model. Consider a training set
of four instances with two rank aspects:
</bodyText>
<page confidence="0.979714">
304
</page>
<equation confidence="0.9997325">
(x1, y1) = ((1, 0, 1), (2,1))
(x2, y2) = ((1, 0, 0), (2,2))
(x3, y3) = ((0, 1, 1), (1,2))
(x4, y4) = ((0, 1, 0), (1, 1))
</equation>
<bodyText confidence="0.99994864">
We can interpret these inputs as feature vectors cor-
responding to the presence of “good”, “bad”, and
“but not” in the following four sentences:
The food was good, but not the ambience.
The food was good, and so was the ambience.
The food was bad, but not the ambience.
The food was bad, and so was the ambience.
We can further interpret the first rank aspect as the
quality of food, and the second as the quality of the
ambience, both on a scale of 1-2.
A simple ranking model which only considers the
words “good” and “bad” perfectly ranks the food as-
pect. However, it is easy to see that no single model
perfectly ranks the ambience aspect. Consider any
model (w, b = (b)). Note that w · x1 &lt; b and
w · x2 &gt; b together imply that w3 &lt; 0, whereas
w · x3 &gt; b and w · x4 &lt; b together imply that
w3 &gt; 0. Thus independent ranking models cannot
perfectly rank this corpus.
The addition of an agreement model, however,
can easily yield a perfect ranking. With a =
(0, 0, −5) (which predicts contrast with the presence
of the words “but not”) and a ranking model for the
ambience aspect such as w = (1, −1, 0), b = (0),
the Good Grief decoder will produce a perfect rank.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="method">
5 Experimental Set-Up
</sectionHeader>
<bodyText confidence="0.969773">
We evaluate our multi-aspect ranking algorithm on a
corpus of restaurant reviews available on the web-
site http://www.we8there.com. Reviews
from this website have been previously used in other
sentiment analysis tasks (Higashinaka et al., 2006).
Each review is accompanied by a set of five ranks,
each on a scale of 1-5, covering food, ambience, ser-
vice, value, and overall experience. These ranks are
provided by consumers who wrote original reviews.
Our corpus does not contain incomplete data points
since all the reviews available on this website con-
tain both a review text and the values for all the five
aspects.
Training and Testing Division Our corpus con-
</bodyText>
<footnote confidence="0.8963485">
5Data and code used in this paper are available at
http://people.csail.mit.edu/bsnyder/naacl07
</footnote>
<bodyText confidence="0.973875594594594">
tains 4,488 reviews, averaging 115 words. We ran-
domly select 3,488 reviews for training, 500 for de-
velopment and 500 for testing.
Parameter Tuning We used the development set
to determine optimal numbers of training iterations
for our model and for the baseline models. Also,
given an initial uncalibrated agreement model a&apos;, we
define our agreement model to be a = αa&apos; for an
appropriate scaling factor α. We tune the value of α
on the development set.
Corpus Statistics Our training corpus contains
528 among 55 = 3025 possible rank sets. The most
frequent rank set (5, 5, 5, 5, 5) accounts for 30.5%
of the training set. However, no other rank set com-
prises more than 5% of the data. To cover 90% of
occurrences in the training set, 227 rank sets are re-
quired. Therefore, treating a rank tuple as a single
label is not a viable option for this task. We also
find that reviews with full agreement across rank as-
pects are quite common in our corpus, accounting
for 38% of the training data. Thus an agreement-
based approach is natural and relevant.
A rank of 5 is the most common rank for all as-
pects and thus a prediction of all 5’s gives a MAJOR-
ITY baseline and a natural indication of task diffi-
culty.
Evaluation Measures We evaluate our algorithm
and the baseline using ranking loss (Crammer and
Singer, 2001; Basilico and Hofmann, 2004). Rank-
ing loss measures the average distance between
the true rank and the predicted rank. Formally,
given N test instances (x1, y1), ..., (xN, yN) of an
m-aspect ranking problem and the corresponding
predictions ˆy1, ..., ˆyN, ranking loss is defined as
Et,i |y[i]t_ˆy[i]t|. Lower values of this measure cor-
mN
respond to a better performance of the algorithm.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999043111111111">
Comparison with Baselines Table 1 shows the per-
formance of the Good Grief training algorithm GG
TRAIN+DECODE along with various baselines, in-
cluding the simple MAJORITY baseline mentioned
in section 5. The first competitive baseline, PRANK,
learns a separate ranker for each aspect using the
PRank algorithm. The second competitive baseline,
SIM, shares the weight vectors across aspects using
a similarity measure (Basilico and Hofmann, 2004).
</bodyText>
<page confidence="0.996044">
305
</page>
<table confidence="0.999536428571429">
Food Service Value Atmosphere Experience Total
MAJORITY 0.848 1.056 1.030 1.044 1.028 1.001
PRANK 0.606 0.676 0.700 0.776 0.618 0.675
SIM 0.562 0.648 0.706 0.798 0.600 0.663
GG DECODE 0.544 0.648 0.704 0.798 0.584 0.656
GG TRAIN+DECODE 0.534 0.622 0.644 0.774 0.584 0.632
GG ORACLE 0.510 0.578 0.674 0.694 0.518 0.595
</table>
<tableCaption confidence="0.999918">
Table 1: Ranking loss on the test set for variants of Good Grief and various baselines.
</tableCaption>
<figureCaption confidence="0.8711105">
Figure 2: Rank loss for our algorithm and baselines
as a function of training round.
</figureCaption>
<bodyText confidence="0.971176857142857">
Both of these methods are described in detail in Sec-
tion 2. In addition, we consider two variants of our
algorithm: GG DECODE employs the PRank train-
ing algorithm to independently train all component
ranking models and only applies Good Grief decod-
ing at test time. GG ORACLE uses Good Grief train-
ing and decoding but in both cases is given perfect
knowledge of whether or not the true ranks all agree
(instead of using the trained agreement model).
Our model achieves a rank error of 0.632, com-
pared to 0.675 for PRANK and 0.663 for SIM. Both
of these differences are statistically significant at
p &lt; 0.002 by a Fisher Sign Test. The gain in perfor-
mance is observed across all five aspects. Our model
also yields significant improvement (p &lt; 0.05) over
the decoding-only variant GG DECODE, confirm-
ing the importance of joint training. As shown in
Figure 2, our model demonstrates consistent im-
provement over the baselines across all the training
rounds.
Model Analysis We separately analyze our per-
</bodyText>
<table confidence="0.99616075">
Consensus Non-consensus
PRANK 0.414 0.864
GG TRAIN+DECODE 0.324 0.854
GG ORACLE 0.281 0.830
</table>
<tableCaption confidence="0.99832">
Table 2: Ranking loss for our model and PRANK
</tableCaption>
<bodyText confidence="0.978989833333333">
computed separately on cases of actual consensus
and actual disagreement.
formance on the 210 test instances where all the
target ranks agree and the remaining 290 instances
where there is some contrast. As Table 2 shows, we
outperform the PRANK baseline in both cases. How-
ever on the consensus instances we achieve a relative
reduction in error of 21.8% compared to only a 1.1%
reduction for the other set. In cases of consensus,
the agreement model can guide the ranking models
by reducing the decision space to five rank sets. In
cases of disagreement, however, our model does not
provide sufficient constraints as the vast majority of
ranking sets remain viable. This explains the perfor-
mance of GG ORACLE, the variant of our algorithm
with perfect knowledge of agreement/disagreement
facts. As shown in Table 1, GG ORACLE yields sub-
stantial improvement over our algorithm, but most
of this gain comes from consensus instances (see Ta-
ble 2).
We also examine the impact of the agreement
model accuracy on our algorithm. The agreement
model, when considered on its own, achieves clas-
sification accuracy of 67% on the test set, compared
to a majority baseline of 58%. However, those in-
stances with high confidence |a · x |exhibit substan-
tially higher classification accuracy. Figure 3 shows
the performance of the agreement model as a func-
tion of the confidence value. The 10% of the data
with highest confidence values can be classified by
</bodyText>
<page confidence="0.997751">
306
</page>
<figureCaption confidence="0.967963">
Figure 3: Accuracy of the agreement model on sub-
sets of test instances with highest confidence |a · x|.
</figureCaption>
<bodyText confidence="0.981036272727273">
the agreement model with 90% accuracy, and the
third of the data with highest confidence can be clas-
sified at 80% accuracy.
This property explains why the agreement model
helps in joint ranking even though its overall accu-
racy may seem low. Under the Good Grief criterion,
the agreement model’s prediction will only be en-
forced when its grief outweighs that of the ranking
models. Thus in cases where the prediction confi-
dence (|a·x|) is relatively low,6 the agreement model
will essentially be ignored.
</bodyText>
<sectionHeader confidence="0.997262" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.968223434782609">
We considered the problem of analyzing multiple re-
lated aspects of user reviews. The algorithm pre-
sented jointly learns ranking models for individual
aspects by modeling the dependencies between as-
signed ranks. The strength of our algorithm lies
in its ability to guide the prediction of individual
rankers using rhetorical relations between aspects
such as agreement and contrast. Our method yields
significant empirical improvements over individual
rankers as well as a state-of-the-art joint ranking
model.
Our current model employs a single rhetorical re-
lation – agreement vs. contrast – to model depen-
dencies between different opinions. As our analy-
6What counts as “relatively low” will depend on both the
value of the tuning parameter α and the confidence of the com-
ponent ranking models for a particular input x.
sis shows, this relation does not provide sufficient
constraints for non-consensus instances. An avenue
for future research is to consider the impact of addi-
tional rhetorical relations between aspects. We also
plan to theoretically analyze the convergence prop-
erties of this and other joint perceptron algorithms.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9200979">
The authors acknowledge the support of the National Sci-
ence Foundation (CAREER grant IIS-0448168 and grant IIS-
0415865) and the Microsoft Research Faculty Fellowship.
Thanks to Michael Collins, Pawan Deshpande, Jacob Eisen-
stein, Igor Malioutov, Luke Zettlemoyer, and the anonymous
reviewers for helpful comments and suggestions. Thanks also
to Vasumathi Raman for programming assistance. Any opin-
ions, findings, and conclusions or recommendations expressed
above are those of the authors and do not necessarily reflect the
views of the NSF.
</bodyText>
<sectionHeader confidence="0.996385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913818181818">
J. Basilico, T. Hofmann. 2004. Unifying collabora-
tive and content-based filtering. In Proceedings of the
ICML, 65–72.
K. Crammer, Y. Singer. 2001. Pranking with ranking. In
NIPS, 641–647.
K. Dave, S. Lawrence, D. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
WWW, 519–528.
A. B. Goldberg, X. Zhu. 2006. Seeing stars when there
aren’t many stars: Graph-based semi-supervised learn-
ing for sentiment categorization. In Proceedings of
HLT/NAACL workshop on TextGraphs, 45–52.
R. Higashinaka, R. Prasad, M. Walker. 2006. Learn-
ing to generate naturalistic utterances using reviews
in spoken dialogue systems. In Proceedings of COL-
ING/ACL, 265–272.
D. Marcu, A. Echihabi. 2002. An unsupervised approach
to recognizing discourse relations. In Proceedings of
ACL, 368–375.
B. Pang, L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the ACL, 115–124.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up?
sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP, 79–86.
P. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classsification of
reviews. In Proceedings of the ACL, 417–424.
H. Yu, V. Hatzivassiloglou. 2003. Towards answering
opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Pro-
ceedings of EMNLP, 129–136.
</reference>
<page confidence="0.998601">
307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938223">
<title confidence="0.999878">Multiple Aspect Ranking using the Good Grief Algorithm</title>
<author confidence="0.991932">Benjamin Snyder</author>
<author confidence="0.991932">Regina</author>
<affiliation confidence="0.9704785">Computer Science and Artificial Intelligence Massachusetts Institute of</affiliation>
<abstract confidence="0.999902772727273">We address the problem of analyzing multiple related opinions in a text. For instance, in a restaurant review such opinions may include food, ambience and service. We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect. We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Basilico</author>
<author>T Hofmann</author>
</authors>
<title>Unifying collaborative and content-based filtering.</title>
<date>2004</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="6530" citStr="Basilico and Hofmann (2004)" startWordPosition="1075" endWordPosition="1079">t with the weight vector: score(x) = w · x. Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries. Formally, the model returns the rank r such that br−1 &lt; score(x) &lt; br. The model is trained with the Perceptron Ranking algorithm (or “PRank algorithm”), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors. The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database. An extension of this model is provided by Basilico and Hofmann (2004) in the context of collaborative filtering. Instead of training a separate model for each user, Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users. In addition to these shared boundaries, userspecific weight vectors are stored. To compute the score for input x and user i, the weight vectors for all users are employed: �scorei(x) = w[i] · x + sim(i, j)(w[j] · x) (1) 9 where 0 &lt; sim(i, j) &lt; 1 is the cosine similarity between users i and j, computed on the entire training set. Once the score has been computed, the prediction rule follows that of the</context>
<context position="22893" citStr="Basilico and Hofmann, 2004" startWordPosition="4054" endWordPosition="4057"> in the training set, 227 rank sets are required. Therefore, treating a rank tuple as a single label is not a viable option for this task. We also find that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data. Thus an agreementbased approach is natural and relevant. A rank of 5 is the most common rank for all aspects and thus a prediction of all 5’s gives a MAJORITY baseline and a natural indication of task difficulty. Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004). Ranking loss measures the average distance between the true rank and the predicted rank. Formally, given N test instances (x1, y1), ..., (xN, yN) of an m-aspect ranking problem and the corresponding predictions ˆy1, ..., ˆyN, ranking loss is defined as Et,i |y[i]t_ˆy[i]t|. Lower values of this measure cormN respond to a better performance of the algorithm. 6 Results Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5. The first competitive basel</context>
</contexts>
<marker>Basilico, Hofmann, 2004</marker>
<rawString>J. Basilico, T. Hofmann. 2004. Unifying collaborative and content-based filtering. In Proceedings of the ICML, 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Pranking with ranking.</title>
<date>2001</date>
<booktitle>In NIPS,</booktitle>
<pages>641--647</pages>
<contexts>
<context position="3118" citStr="Crammer and Singer, 2001" startWordPosition="493" endWordPosition="496">ment relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are &apos;In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). 300 Proceedings of NAACL HLT 2007, pages 300–307, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics equal. The Good Grief decoding algorithm predicts a set of ranks – one for each aspect – which maximally satisfy the preferences of the individual rankers and the agreement model. For example, if the agreement model predicts consensus but the individual rankers select ranks (5, 5, 4), then the decoder decides whether to trust the the third ranker, or alter its prediction and output (5, 5, 5) to be consistent with the agreem</context>
<context position="5610" citStr="Crammer and Singer (2001)" startWordPosition="901" endWordPosition="904">xt. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a new method for joint multiple aspect ranking. Ranking The ranking, or ordinal regression, problem has been extensivly studied in the Machine Learning and Information Retrieval communities. In this section we focus on two online ranking methods which form the basis of our approach. The first is a model proposed by Crammer and Singer (2001). The task is to predict a rank y E I1, ..., k} for every input x E R&apos;. Their model stores a weight vector w E R&apos; and a vector of increasing boundaries b0 = −00 &lt; b1 &lt; ... &lt; bk−1 &lt; bk = 00 which divide the real line into k segments, one for each possible rank. The model first scores each input with the weight vector: score(x) = w · x. Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries. Formally, the model returns the rank r such that br−1 &lt; score(x) &lt; br. The model is trained with the Perceptron Ranking algorithm (or “PRank algo</context>
<context position="12801" citStr="Crammer and Singer, 2001" startWordPosition="2214" endWordPosition="2217">gment of the real line: gi(x, r) = min |c| s.t. b[i]r_1 &lt; scorei(x) + c &lt; b[i]r Agreement Model Grief Similarly, we define the grief of the agreement model with respect to a joint rank r = (r[1], ... , r[m]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r[1], ... , r[m]: ga(x, r) = min |c| s.t. a x + c &gt; 0 A Vi, j E 1...m : r[i] = r[j] V a x + c &lt; 0 A Ii,j E 1...m : r[i] =� r[j] 3.3 Training Ranking models Pseudo-code for Good Grief training is shown in Figure 1. This training algorithm is based on PRanking (Crammer and Singer, 2001), an online perceptron algorithm. The training is performed by iteratively ranking each training input x and updating the model. If the predicted rank y� is equal to the true rank y, the weight and boundaries vectors remain unchanged. On the other hand, if y� =� y, then the weights and boundaries are updated to improve the prediction for x (step 4.c in Figure 1). See (Crammer and Singer, 2001) for explanation and analysis of this update rule. Our algorithm departs from PRanking by conjoining the updates for the m ranking models. We achieve this by using Good Grief decoding at each step through</context>
<context position="22864" citStr="Crammer and Singer, 2001" startWordPosition="4050" endWordPosition="4053">o cover 90% of occurrences in the training set, 227 rank sets are required. Therefore, treating a rank tuple as a single label is not a viable option for this task. We also find that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data. Thus an agreementbased approach is natural and relevant. A rank of 5 is the most common rank for all aspects and thus a prediction of all 5’s gives a MAJORITY baseline and a natural indication of task difficulty. Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004). Ranking loss measures the average distance between the true rank and the predicted rank. Formally, given N test instances (x1, y1), ..., (xN, yN) of an m-aspect ranking problem and the corresponding predictions ˆy1, ..., ˆyN, ranking loss is defined as Et,i |y[i]t_ˆy[i]t|. Lower values of this measure cormN respond to a better performance of the algorithm. 6 Results Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>K. Crammer, Y. Singer. 2001. Pranking with ranking. In NIPS, 641–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="4670" citStr="Dave et al., 2003" startWordPosition="745" endWordPosition="748">anking models for each aspect can also be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a </context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, D. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of WWW, 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Goldberg</author>
<author>X Zhu</author>
</authors>
<title>Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT/NAACL workshop on TextGraphs,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="4733" citStr="Goldberg and Zhu, 2006" startWordPosition="756" endWordPosition="759"> with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a new method for joint multiple aspect ranking. Ranking The ranki</context>
</contexts>
<marker>Goldberg, Zhu, 2006</marker>
<rawString>A. B. Goldberg, X. Zhu. 2006. Seeing stars when there aren’t many stars: Graph-based semi-supervised learning for sentiment categorization. In Proceedings of HLT/NAACL workshop on TextGraphs, 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Higashinaka</author>
<author>R Prasad</author>
<author>M Walker</author>
</authors>
<title>Learning to generate naturalistic utterances using reviews in spoken dialogue systems.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="21038" citStr="Higashinaka et al., 2006" startWordPosition="3730" endWordPosition="3733"> independent ranking models cannot perfectly rank this corpus. The addition of an agreement model, however, can easily yield a perfect ranking. With a = (0, 0, −5) (which predicts contrast with the presence of the words “but not”) and a ranking model for the ambience aspect such as w = (1, −1, 0), b = (0), the Good Grief decoder will produce a perfect rank. 5 Experimental Set-Up We evaluate our multi-aspect ranking algorithm on a corpus of restaurant reviews available on the website http://www.we8there.com. Reviews from this website have been previously used in other sentiment analysis tasks (Higashinaka et al., 2006). Each review is accompanied by a set of five ranks, each on a scale of 1-5, covering food, ambience, service, value, and overall experience. These ranks are provided by consumers who wrote original reviews. Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the five aspects. Training and Testing Division Our corpus con5Data and code used in this paper are available at http://people.csail.mit.edu/bsnyder/naacl07 tains 4,488 reviews, averaging 115 words. We randomly select 3,488 reviews for trainin</context>
</contexts>
<marker>Higashinaka, Prasad, Walker, 2006</marker>
<rawString>R. Higashinaka, R. Prasad, M. Walker. 2006. Learning to generate naturalistic utterances using reviews in spoken dialogue systems. In Proceedings of COLING/ACL, 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="2738" citStr="Marcu and Echihabi, 2002" startWordPosition="424" endWordPosition="427">proach fails to exploit meaningful dependencies between users’ judgments across different aspects. Knowledge of these dependencies can be crucial in predicting accurate ranks, as a user’s opinions on one aspect can influence his or her opinions on others. The algorithm presented in this paper models the dependencies between different labels via the agreement relation. The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are &apos;In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). 300 Proceedings of NAACL HLT 2007, pages 300–307, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics equal. The Good Grief decoding algorithm predic</context>
<context position="15061" citStr="Marcu and Echihabi (2002)" startWordPosition="2594" endWordPosition="2598">standard training algorithm for binary classification such as Perceptron or SVM optimization. 3.4 Feature Representation Ranking Models Following previous work on sentiment classification (Pang et al., 2002), we represent each review as a vector of lexical features. More specifically, we extract all unigrams and bigrams, discarding those that appear fewer than three times. This process yields about 30,000 features. Agreement Model The agreement model also operates over lexicalized features. The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002). In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review. For example, the presence of “delicious” and “dirty” indicate high contrast, whereas the pair “expensive” and “slow” indicate low contrast. The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models. 4 Analysis In this section, we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>D. Marcu, A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of ACL, 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="3163" citStr="Pang and Lee, 2005" startWordPosition="501" endWordPosition="504">es all aspects of the item or whether he or she expresses different degrees of satisfaction. Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction. The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are &apos;In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance. This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). 300 Proceedings of NAACL HLT 2007, pages 300–307, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics equal. The Good Grief decoding algorithm predicts a set of ranks – one for each aspect – which maximally satisfy the preferences of the individual rankers and the agreement model. For example, if the agreement model predicts consensus but the individual rankers select ranks (5, 5, 4), then the decoder decides whether to trust the the third ranker, or alter its prediction and output (5, 5, 5) to be consistent with the agreement prediction. To obtain a model well-suited</context>
<context position="4708" citStr="Pang and Lee, 2005" startWordPosition="752" endWordPosition="755"> be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates our exploration of a new method for joint multiple aspect r</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>B. Pang, L. Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1248" citStr="Pang et al., 2002" startWordPosition="184" endWordPosition="187">encies between assigned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would as</context>
<context position="4605" citStr="Pang et al., 2002" startWordPosition="735" endWordPosition="738"> every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate </context>
<context position="14643" citStr="Pang et al., 2002" startWordPosition="2531" endWordPosition="2534">e a separate update based on this joint prediction (step 4 in Figure 1), instead of using the individual models’ predictions. Agreement model The agreement model a is assumed to have been previously trained on the same training data. An instance is labeled with a positive label if all the ranks associated with this instance are equal. The rest of the instances are labeled as negative. This model can use any standard training algorithm for binary classification such as Perceptron or SVM optimization. 3.4 Feature Representation Ranking Models Following previous work on sentiment classification (Pang et al., 2002), we represent each review as a vector of lexical features. More specifically, we extract all unigrams and bigrams, discarding those that appear fewer than three times. This process yields about 30,000 features. Agreement Model The agreement model also operates over lexicalized features. The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002). In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review. For example, the presence of “de</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP, 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classsification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1262" citStr="Turney, 2002" startWordPosition="188" endWordPosition="189">gned ranks. This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would assign a numeric</context>
<context position="4619" citStr="Turney, 2002" startWordPosition="739" endWordPosition="740"> that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classsification of reviews. In Proceedings of the ACL, 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="1294" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="190" endWordPosition="193">is algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast. We prove that our agreementbased joint model is more expressive than individual ranking models. Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model. 1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003). However, multiple opinions on related matters are often intertwined throughout a text. For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant. Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review. To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect. In the example above, we would assign a numeric rank from 1-5 for each of: food</context>
<context position="4650" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="741" endWordPosition="744">erfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model. In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference. Our experimental results further confirm the strength of the Good Grief model. Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model. 2 Related Work Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003). More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale. While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text. Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion. Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks. This motivates o</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu, V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP, 129–136.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>