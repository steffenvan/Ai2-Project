<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.99315">
Bootstrapping Coreference Resolution Using Word Associations
</title>
<author confidence="0.998838">
Hamidreza Kobdani, Hinrich Sch¨utze, Michael Schiehlen and Hans Kamp
</author>
<affiliation confidence="0.997691">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.993878">
kobdani@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999497842105263">
In this paper, we present an unsupervised
framework that bootstraps a complete corefer-
ence resolution (CoRe) system from word as-
sociations mined from a large unlabeled cor-
pus. We show that word associations are use-
ful for CoRe – e.g., the strong association be-
tween Obama and President is an indicator
of likely coreference. Association information
has so far not been used in CoRe because it is
sparse and difficult to learn from small labeled
corpora. Since unlabeled text is readily avail-
able, our unsupervised approach addresses the
sparseness problem. In a self-training frame-
work, we train a decision tree on a corpus that
is automatically labeled using word associa-
tions. We show that this unsupervised system
has better CoRe performance than other learn-
ing approaches that do not use manually la-
beled data.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998767826086956">
Coreference resolution (CoRe) is the process of find-
ing markables (noun phrases) referring to the same
real world entity or concept. Until recently, most ap-
proaches tried to solve the problem by binary classi-
fication, where the probability of a pair of markables
being coreferent is estimated from labeled data. Al-
ternatively, a model that determines whether a mark-
able is coreferent with a preceding cluster can be
used. For both pair-based and cluster-based models,
a well established feature model plays an important
role. Typical systems use a rich feature space based
on lexical, syntactic and semantic knowledge. Most
commonly used features are described by Soon et al.
(2001).
Most existing systems are supervised systems,
trained on human-labeled benchmark data sets for
English. These systems use linguistic features based
on number, gender, person etc. It is a challenge to
adapt these systems to new domains, genres and lan-
guages because a significant human labeling effort is
usually necessary to get good performance.
To address this challenge, we pursue an unsuper-
vised self-training approach. We train a classifier
on a corpus that is automatically labeled using asso-
ciation information. Self-training approaches usu-
ally include the use of some manually labeled data.
In contrast, our self-trained system is not trained on
any manually labeled data and is therefore a com-
pletely unsupervised system. Although training on
automatically labeled data can be viewed as a form
of supervision, we reserve the term supervised sys-
tem for systems that are trained on manually labeled
data.
The key novelty of our approach is that we boot-
strap a competitive CoRe system from association
information that is mined from an unlabeled cor-
pus in a completely unsupervised fashion. While
this method is shallow, it provides valuable informa-
tion for CoRe because it considers the actual iden-
tity of the words in question. Consider the pair of
markables (Obama, President). It is a likely coref-
erence pair, but this information is not accessible
to standard CoRe systems because they only use
string-based features (often called lexical features),
named entity features and semantic word class fea-
tures (e.g., from WordNet) that do not distinguish,
</bodyText>
<page confidence="0.981589">
783
</page>
<note confidence="0.979966">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 783–792,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.784606734939759">
say, Obama from Hawking. quality and domain. Not only is it able to deal
In our approach, word association information is with shallow information spaces (A-INF), but
used for clustering markables in unsupervised learn- it can also deliver competitive results for rich
ing. Association information is calculated as asso- feature spaces (SUCRE and UNSEL).
ciation scores between heads of markables as de- This paper is organized as follows. Related work
scribed below. We view association information as is discussed in Section 2. In Section 3, we present
an example of a shallow feature space which con- our system architecture. Section 4 describes the ex-
trasts with the rich feature space that is generally periments and Section 5 presents and discusses our
used in CoRe. results. The final section presents our conclusions.
Our experiments are conducted using the 2 Related Work
MCORE system (“Modular COreference REso- There are three main approaches to CoRe: super-
lution”).1 MCORE can operate in three different vised, semi-supervised (or weakly supervised) and
settings: unsupervised (subsystem A-INF), super- unsupervised. We use the term semi-supervised for
vised (subsystem SUCRE (Kobdani and Sch¨utze, approaches that use some amount of human-labeled
2010)), and self-trained (subsystem UNSEL). The coreference pairs.
unsupervised subsystem A-INF (“Association M¨uller et al. (2002) used co-training for coref-
INFormation”) uses the association scores between erence resolution, a semi-supervised method. Co-
heads as the distance measure when clustering training puts features into disjoint subsets when
markables. SUCRE (“SUpervised Coreference learning from labeled and unlabeled data and tries
REsolution”) is trained on a labeled corpus to leverage this split for better performance. Ng and
(manually or automatically labeled) similar to Cardie (2003) use self-training in a multiple-learner
standard CoRe systems. Finally, the unsupervised framework and report performance superior to co-
self-trained subsystem UNSEL (“UNsupervised training. They argue that the multiple learner ap-
SELf-trained”) uses the unsupervised subsystem proach is a better choice for CoRe than the multi-
A-INF to automatically label an unlabeled corpus ple view approach of co-training. Our self-trained
that is then used as a training set for SUCRE. model combines multiple learners (A-INF and SU-
Our main contributions in this paper are as fol- CRE) and multiple views (shallow/rich informa-
lows: tion). A key difference to the work by M¨uller et al.
1. We demonstrate that word association informa- (2002) and Ng and Cardie (2003) is that we do not
tion can be used to develop an unsupervised use any human-labeled coreference pairs.
model for shallow coreference resolution (sub- Our basic idea of self-training without human la-
system A-INF). bels is similar to (Kehler et al., 2004), but we ad-
2. We introduce an unsupervised self-trained dress the general CoRe problem, not just pronoun
method (UNSEL) that takes a two-learner two- interpretation.
feature-space approach. The two learners are Turning to unsupervised CoRe, Haghighi and
A-INF and SUCRE. The feature spaces are the Klein (2007) proposed a generative Bayesian model
shallow and rich feature spaces. with good performance. Poon and Domingos (2008)
3. We show that the performance of UNSEL is introduced an unsupervised system in the framework
better than the performance of other unsuper- of Markov logic. Ng (2008) presented a generative
vised systems when it is self-trained on the au- model that views coreference as an EM clustering
tomatically labeled corpus and uses the lever- process. We will show that our system, which is
aging effect of a rich feature space. simpler than prior work, outperforms these systems.
4. MCORE is a flexible and modular framework Haghighi and Klein (2010) present an “almost un-
that is able to learn from data with different supervised” CoRe system. In this paper, we only
compare with completely unsupervised approaches,
1MCORE can be downloaded from ifnlp.org/
˜schuetze/mcore.
784
not with approaches that make some limited use of
labeled data.
Recent work by Haghighi and Klein (2009), Klen-
ner and Ailloud (2009) and Raghunathan et al.
(2010) challenges the appropriateness of machine
learning methods for CoRe. These researchers show
that a “deterministic” system (essentially a rule-
based system) that uses a rich feature space includ-
ing lexical, syntactic and semantic features can im-
prove CoRe performance. Almost all CoRe systems,
including ours, use a limited number of rules or fil-
ters, e.g., to implement binding condition A that re-
flexives must have a close antecedent in some sense
of “close”. In our view, systems that use a few ba-
sic filters are fundamentally different from carefully
tuned systems with a large number of complex rules,
some of which use specific lexical information. A
limitation of complex rule-based systems is that they
require substantial effort to encode the large number
of deterministic constraints that guarantee good per-
formance. Moreover, these systems are not adapt-
able (since they are not machine-learned) and may
have to be rewritten for each new domain, genre
and language. Consequently, we do not compare our
performance with deterministic systems.
Ponzetto (2010) extracts metadata from
Wikipedia for supervised CoRe. Using such
additional resources in our unsupervised system
should further improve CoRe performance. Elsner
et al. (2009) present an unsupervised algorithm
for identifying clusters of entities that belong to
the same named entity (NE) class. Determining
common membership in an NE class like person is
an easier task than determining coreference of two
NEs.
</bodyText>
<sectionHeader confidence="0.966749" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.9959592">
Figure 1 illustrates the system architecture of our
unsupervised self-trained CoRe system (UNSEL).
Oval nodes are data, box nodes are processes. We
take a self-training approach to coreference resolu-
tion: We first label the corpus using the unsuper-
vised model A-INF and then train the supervised
model SUCRE on this automatically labeled train-
ing corpus. Even though we train on a labeled cor-
pus, the labeling of the corpus is produced in a com-
pletely automatic fashion, without recourse to hu-
</bodyText>
<figure confidence="0.754366">
Unsupervised Model (A-INF)
Supervised Model (SUCRE)
</figure>
<figureCaption confidence="0.9913305">
Figure 1: System Architecture of UNSEL (Unsupervised
Self-Trained Model).
</figureCaption>
<bodyText confidence="0.9977585">
man labeling. Thus, it is an unsupervised approach.
The MCORE architecture is very flexible; in par-
ticular, as will be explained presently, it can be eas-
ily adapted for supervised as well as unsupervised
settings.
The unsupervised and supervised models have an
identical top level architecture; we illustrate this in
Figure 2. In preprocessing, tokens (words), mark-
ables and their attributes are extracted from the input
text. The key difference between the unsupervised
and supervised approaches is in how pair estimation
is accomplished — see Sections 3.1 &amp; 3.2 for de-
tails.
The main task in chain estimation is clustering.
Figure 3 presents our clustering method, which is
used for both supervised and unsupervised CoRe.
We search for the best predicted antecedent (with
coreference probability p &gt; 0.5) from right to left
starting from the end of the document. McEnery et
al. (1997) showed that in 98.68% of cases the an-
tecedent is within a 10-sentence window; hence we
use a window of 10 sentences for search. We have
found that limiting the search to a window increases
both efficiency and effectiveness.
Filtering. We use a feature definition language
to define the templates according to which the fil-
ters and features are calculated. These templates
are hard constraints that filter out all cases that are
clearly disreferent, e.g., (he, she) or (he, they). We
use the following filters: (i) the antecedent of a re-
flexive pronoun must be in the same sentence; (ii)
the antecedent of a pronoun must occur at a distance
of at most 3 sentences; (iii) a coreferent pair of a
noun and a pronoun or of two pronouns must not
</bodyText>
<figure confidence="0.980714">
Unlabeled Data
Automatically Labeled Data
785
Input Text
Preprocessing
Markables
Pair Estimation
Markable Chains
Chain Estimation
Markable Pairs
</figure>
<figureCaption confidence="0.983668">
Figure 2: Common architecture of unsupervised (A-INF) and supervised (SUCRE) models.
</figureCaption>
<bodyText confidence="0.614133">
Chain Estimation (M1, M2,..., Mn)
</bodyText>
<listItem confidence="0.913643888888889">
1. t +— 1
2. For each markable Mi: Ci +— {Mi}
3. Proceed through the markables from the end
of the document. For each Mj, consider each
preceding Mi within 10 sentences:
If Pair Estimation(Mi,Mj)&gt;=t: Ci +— CiUCj
4. t +— t − 0.01
5. If t &gt;= 0.5: go to step 3
Pair Estimation (Mi, Mj):
</listItem>
<bodyText confidence="0.813375285714286">
If Filtering(Mi, Mj)==FALSE then return 0;
else return the probability p (or association
score N) of markable pair (Mi, Mj) being
coreferent.
Filtering (Mi, Mj):
return TRUE if all filters for (Mi, Mj) are
TRUE else FALSE
</bodyText>
<figureCaption confidence="0.995764666666667">
Figure 3: MCORE chain estimation (clustering) algo-
rithm (test). t is the clustering threshold. Ci refers to
the cluster that Mi is a member of.
</figureCaption>
<bodyText confidence="0.997878">
disagree in number; (iv) a coreferent pair of two pro-
nouns must not disagree in gender. These four filters
are used in supervised and unsupervised modes of
MCORE.
</bodyText>
<subsectionHeader confidence="0.988605">
3.1 Unsupervised Model (A-INF)
</subsectionHeader>
<bodyText confidence="0.997150458333333">
Figure 4 (top) shows how A-INF performs pair esti-
mation. First, in the pair generation step, all possible
pairs inside 10 sentences are generated. Other steps
are separately explained for train and test as follows.
Train. In addition to the filters (i)–(iv) described
above, we use the following filter: (v) If the head
of markable M2 matches the head of the preceding
markable M1, then we ignore all other pairs for M2
in the calculation of association scores.
This additional filter is necessary because an ap-
proach without some kind of string matching con-
straint yields poor results, given the importance of
string matching for CoRe. As we will show below,
even the simple filters (i)–(v) are sufficient to learn
high-quality association scores; this means that we
do not need the complex features of “determinis-
tic” systems. However, if such complex features are
available, then we can use them to improve perfor-
mance in our self-trained setting.
To learn word association information from an
unlabeled corpus (see Section 4), we compute mu-
tual information (MI) scores between heads of mark-
ables. We define MI as follows: (Cover and Thomas,
1991)
</bodyText>
<equation confidence="0.9991335">
P (i, j)
P(i, j) log2 P(i)P(j)
</equation>
<bodyText confidence="0.999001916666667">
E.g., P(a, b) is the probability of a pair whose two
elements are a and a word not equal to b.
Test. A key virtue of our approach is that in the
classification of pairs as coreferent/disreferent, the
coreference probability p estimated in supervised
learning plays exactly the same role as the associ-
ation information score N (defined below). For p, it
is important that we only consider pairs with p &gt; 0.5
as potentially coreferent (see Figure 3). To be able to
impose the same constraint on N, we normalize the
MI scores by the maximum values of the two words
and take the average:
</bodyText>
<equation confidence="0.598969">
1 MI(a, b)
</equation>
<bodyText confidence="0.99970275">
strongly two words are associated. N is normalized
to ensure 0 &lt; N &lt; 1. If a or b did not occur, then
we set N =0.
In filtering for test, we use filters (i)–(iv). We then
fetch the MI values and calculate N values. The
clustering algorithm described in Figure 3 uses these
N values in exactly the same way as p: we search for
the antecedent with the maximum association score
</bodyText>
<figure confidence="0.8934625">
�
MI(a, b) =
iE{¯a,a}
jE{b,b}
2( MI(a, b)
N(a, b) =
argmaxxMI(a, x)+ argmaxxMI(x, b))
In the above equation, the value of N indicates how
</figure>
<page confidence="0.925912">
786
</page>
<bodyText confidence="0.897659571428571">
N greater than 0.5 from right to left starting from tree3 (Quinlan, 1993) that is trained on the training
the end of the document. set to estimate the coreference probability p for a
As we will see below, using N scores acquired pair and then applied to the test set. Note that, as
from an unlabeled corpus as the only source of in- is standard in CoRe, filtering and feature calculation
formation for CoRe performs surprising well. How- are exactly the same for training and test, but that
ever, the weaknesses of this approach are (i) the fail- pair generation is different as described above.
ure to cover pairs that do not occur in the unlabeled 4 Experimental Setup
</bodyText>
<table confidence="0.530463555555556">
corpus (negatively affecting recall) and (ii) the gen- 4.1 Data Sets
eration of pairs that are not plausible candidates for For computing word association, we used a cor-
coreference (negatively affecting precision). To ad- pus of about 63,000 documents from the 2009 En-
dress these problems, we train a model on a corpus glish Wikipedia (the articles that were larger than
labeled by A-INF in a self-training approach. 200 bytes). This corpus consists of more than 33.8
3.2 Supervised Model (SUCRE) million tokens; the average document length is 500
Figure 4 (bottom) presents the architecture of pair tokens. The corpus was parsed using the Berkeley
estimation for the supervised approach (SUCRE). parser (Petrov and Klein, 2007). We ignored all sen-
In the pair generation step for train, we take each tences that had no parse output. The number of de-
coreferent markable pair (MZ, M3) without inter- tected markables (all noun phrases extracted from
vening coreferent markables and use (MZ, M3) as a parse trees) is about 9 million.
positive training instance and (MZ, Mk), i &lt; k &lt; j, We evaluate unsupervised, supervised and self-
as negative training instances. For test, we generate trained models on ACE (Phase 2) (Mitchell et al.,
all possible pairs within 10 sentences. After filter- 2003).4 This data set is one of the most widely
ing, we then calculate a feature vector for each gen- used CoRe benchmarks and was used by the sys-
erated pair that survived filters (i)–(iv). tems that are most comparable to our approach; in
Our basic features are similar to those described particular, it was used in most prior work on unsu-
by Soon et al. (2001): string-based features, dis- pervised CoRe. The corpus is composed of three
tance features, span features, part-of-speech fea- data sets from three different news sources. We give
tures, grammatical features, semantic features, and the number of test documents for each: (i) Broadcast
agreement features. These basic features are engi- News (BNEWS): 51. (ii) Newspaper (NPAPER):
neered with the goal of creating a feature set that 17. (iii) Newswire (NWIRE): 29. We report re-
will result in good performance. For this purpose sults for true markables (markables extracted from
we used the relational feature engineering frame- the answer keys) to be able to compare with other
work which has been presented in (Kobdani et al., systems that use true markables.
2010). It includes powerful and flexible methods for In addition, we use the recently published
implementing and extracting new features. It allows OntoNotes benchmark (Recasens et al., 2010).
systematic and fast search of the space of features OntoNotes is an excerpt of news from the OntoNotes
and thereby reduces the time and effort needed for Corpus Release 2.0 (Pradhan et al., 2007). The ad-
defining optimal features. We believe that the good vantage of OntoNotes is that it contains two parallel
performance of our supervised system SUCRE (ta- annotations: (i) a gold setting, gold standard manual
bles 1 and 2) is the result of our feature engineering annotations of the preprocessing information and (ii)
approach.2 an automatic setting, automatically predicted anno-
As our classification method, we use a decision tations of the preprocessing information. The au-
tomatic setting reflects the situation a CoRe system
2While this is not the focus of this paper, SUCRE has per-
formance comparable to other state-of-the-art supervised sys-
tems. E.g., B3/MUC F1 are 75.6/72.4 on ACE-2 and 69.4/70.6
on MUC-6 compared to 78.3/66.0 on ACE-2 and 70.9/68.5 on
MUC-6 for Reconcile (Stoyanov et al., 2010)
3We also tried support vector machines and maximum en-
tropy models, but they did not perform better.
4We used two variants of ACE (Phase 2): ACE-2 and
ACE2003
787
</table>
<figureCaption confidence="0.80702">
Figure 4: Pair estimation in the unsupervised model A-INF (top) and in the supervised model SUCRE (bottom).
</figureCaption>
<figure confidence="0.729574692307692">
Markable Pairs
Filtering
Association Calculation
Pair Generation
Train/Test
Filter Templates Association Information
Markable Pairs
Filtering
Feature Calculation
Feature Vectors
Pair Generation
Train/Test
Filter Templates Feature Templates
</figure>
<bodyText confidence="0.996559702702703">
faces in reality; in contrast, the gold setting should
be considered less realistic.
The issue of gold vs. automatic setting is directly
related to a second important evaluation issue: the
influence of markable detection on CoRe evaluation
measures. In a real application, we do not have ac-
cess to true markables, so an evaluation on system
markables (markables automatically detected by the
system) reflects actual expected performance better.
However, reporting only CoRe numbers (even for
system markables) is not sufficient either since ac-
curacy of markable detection is necessary to inter-
pret CoRe scores. Thus, we need (i) measures of
the quality of system markables (i.e., an evaluation
of the markable detection subtask) and CoRe per-
formance on system markables as well as (ii) a mea-
sure of CoRe performance on true markables. We
use OntoNotes in this paper to perform such a, in
our view, complete and realistic evaluation of CoRe.
The two evaluations correspond to the two evalua-
tions performed at SemEval-2010 (Recasens et al.,
2010): the automatic setting with system markables
and the gold setting with true markables. Test set
size is 85 documents.
In the experiments with A-INF we use Wikipedia
to compute association information and then evalu-
ate the model on the test sets of ACE and OntoNotes.
For the experiments with UNSEL, we use its unsu-
pervised subsystem A-INF (which uses Wikipedia
association scores) to automatically label the train-
ing sets of ACE and OntoNotes. Then for each data
set, the supervised subsystem of UNSEL (i.e., SU-
CRE) is trained on its automatically labeled training
set and then evaluated on its test set. Finally, for
the supervised experiments, we use the manually la-
beled training sets and evaluate on the corresponding
test sets.
</bodyText>
<subsectionHeader confidence="0.957413">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99909028">
We report recall, precision, and F1 for MUC (Vilain
et al., 1995), B3 (Bagga and Baldwin, 1998), and
CEAF (Luo, 2005). We selected these three met-
rics because a single metric is often misleading and
because we need to use metrics that were used in
previous unsupervised work.
It is well known that MUC by itself is insuffi-
cient because it gives misleadingly high scores to the
“single-chain” system that puts all markables into
one chain (Luo et al., 2004; Finkel and Manning,
2008). However, B3 and CEAF have a different
bias: they give high scores to the “all-singletons”
system that puts each markable in a separate chain.
On OntoNotes test, we get B3 = 83.2 and CEAF
= 71.2 for all-singletons, which incorrectly sug-
gests that performance is good; but MUC F1 is 0 in
this case, demonstrating that all-singletons performs
poorly. With the goal of performing a complete eval-
uation, one that punishes all-singletons as well as
single-chain, we use one of the following two com-
binations: (i) MUC and B3 or (ii) MUC and CEAF.
Recasens et al. (2010) showed that B3 and CEAF
are highly correlated (Pearson’s r = 0.91). There-
fore, either combination (i) or combination (ii) fairly
characterizes CoRe performance.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.784717">
Table 1 compares our unsupervised self-trained
model UNSEL and unsupervised model A-INF to
</bodyText>
<page confidence="0.985297">
788
</page>
<table confidence="0.99996425">
MUC B3 CEAF
BNEWS-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
1 P&amp;D 68.3 66.6 67.4 70.3 65.3 67.7 – – –
2 A-INF 60.8 61.4 61.1 55.5 69.0 61.5 52.6 52.0 52.3
3 UNSEL 72.5 65.6 68.9 72.5 66.4 69.3 56.7 64.8 60.5
4 SUCRE 86.6 60.3 71.0 87.6 64.6 74.4 56.1 81.6 66.5
NWIRE-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
5 P&amp;D 67.7 67.3 67.4 74.7 68.8 71.6 – – –
6 A-INF 62.4 57.4 59.8 59.2 62.4 60.7 46.8 52.5 49.5
7 UNSEL 76.2 61.5 68.1 81.5 67.6 73.9 61.5 77.1 68.4
8 SUCRE 82.5 65.7 73.1 85.4 72.3 78.3 63.5 80.6 71.0
NPAPER-ACE-2 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
9 P&amp;D 69.2 71.7 70.4 70.0 66.5 68.2 – – –
10 A-INF 60.6 56.0 58.2 52.4 60.3 56.0 38.9 44.0 41.3
11 UNSEL 78.6 65.7 71.6 74.0 68.0 70.9 57.6 73.2 64.5
12 SUCRE 82.5 67.0 73.9 80.7 69.5 74.6 58.8 77.1 66.7
BNEWS-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
13 H&amp;K 68.3 56.8 62.0 – – – 59.9 53.9 56.7
14 Ng 71.4 56.1 62.8 – – – 60.5 53.3 56.7
15 A-INF 60.9 64.9 62.8 50.9 72.5 59.8 53.8 49.4 51.5
16 UNSEL 69.5 65.0 67.1 70.2 65.9 68.0 58.5 64.2 61.2
17 SUCRE 73.9 68.5 71.1 75.4 69.6 72.4 60.1 66.6 63.2
NWIRE-ACE2003 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
18 H&amp;K 66.2 46.8 54.8 – – – 62.8 49.6 55.4
19 Ng 68.3 47.0 55.7 – – – 60.7 49.2 54.4
20 A-INF 62.7 60.5 61.6 54.8 66.1 59.9 47.7 50.2 49.0
21 UNSEL 64.8 68.6 66.6 61.5 73.6 67.0 59.8 55.1 57.3
22 SUCRE 77.6 69.3 73.2 78.8 75.2 76.9 65.1 74.4 69.5
</table>
<tableCaption confidence="0.998403">
Table 1: Scores for MCORE (A-INF, SUCRE and UNSEL) and three comparable systems on ACE-2 and ACE2003.
</tableCaption>
<bodyText confidence="0.998982461538462">
P&amp;D (Poon and Domingos, 2008) on ACE-2; and
to Ng (Ng, 2008) and H&amp;K5 (Haghighi and Klein,
2007) on ACE2003. To our knowledge, these three
papers are the best and most recent evaluation results
for unsupervised learning and they all report results
on ACE-2 and ACE-2003. Results on SUCRE will
be discussed later in this section.
A-INF scores are below some of the earlier unsu-
pervised work reported in the literature (lines 2, 6,
10) although they are close to competitive on two
of the datasets (lines 15 and 20: MUC scores are
equal or better, CEAF scores are worse). Given the
simplicity of A-INF, which uses nothing but asso-
</bodyText>
<footnote confidence="0.6754305">
5We report numbers for the better performing Pronoun-only
Salience variant of H&amp;K proposed by Ng (2008).
</footnote>
<bodyText confidence="0.997321866666667">
ciations mined from a large unannotated corpus, its
performance is surprisingly good.
Turning to UNSEL, we see that F1 is always bet-
ter for UNSEL than for A-INF, for all three mea-
sures (lines 3 vs 2, 7 vs 6, 11 vs 10, 16 vs 15, 21
vs 20). This demonstrates that the self-training step
of UNSEL is able to correct many of the errors that
A-INF commits. Both precision and recall are im-
proved with two exceptions: recall of B3 decreases
from line 2 to 3 and from 15 to 16.
When comparing the unsupervised system UN-
SEL to previous unsupervised results, we find that
UNSEL’s F1 is higher in all runs (lines 3 vs 1, 7 vs
5, 11 vs 9, 16 vs 13&amp;14, 21 vs 18&amp;19). The differ-
ences are large (up to 11%) compared to H&amp;K and
</bodyText>
<page confidence="0.99613">
789
</page>
<bodyText confidence="0.9994535">
Ng. The difference to P&amp;D is smaller, ranging from
2.7% (B3, lines 11 vs 9) to 0.7% (MUC, lines 7 vs
5). Given that MCORE is a simpler and more ef-
ficient system than this prior work on unsupervised
CoRe, these results are promising.
In contrast to F1, there is no consistent trend for
precision and recall. For example, P&amp;D is better
than UNSEL on MUC recall for BNEWS-ACE-2
(lines 1 vs 3) and H&amp;K is better than UNSEL on
CEAF precision for NWIRE-ACE2003 (lines 18 vs
21). But this higher variability for precision and re-
call is to be expected since every system trades the
two measures off differently.
These results show that the application of self-
training significantly improves performance. As dis-
cussed in Section 3.1, self-training has positive ef-
fects on both recall and precision. We now present
two simplified examples that illustrate this point.
Example for recall. Consider the markable pair
(Novoselov6,he) in the test set. Its N score is 0 be-
cause our subset of 2009 Wikipedia sentences has
no occurrence of Novoselov. However, A-INF finds
many similar pairs like (Einstein,he) and (Hawk-
ing,he), pairs that have high N scores. Suppose
we represent pairs using the following five fea-
tures: &lt;sentence distance, string match, type of
first markable, type of second markable, number
agreement&gt;. Then (Einstein,he), (Hawking,he) and
(Novoselov,he) will all be assigned the feature vector
&lt;1, No, Proper Noun, Personal Pronoun, Yes&gt;. We
can now automatically label Wikipedia using A-INF
– this will label (Einstein,he) and (Hawking,he) as
coreferent – and train SUCRE on the resulting train-
ing set. SUCRE can then resolve the coreference
(Novoselov,he) correctly. We call this the better re-
call effect.
Example for precision. Using the same repre-
sentation of pairs, suppose that for the sequence of
markables Biden, Obama, President the markable
pairs (Biden,President) and (Obama,President) are
assigned the feature vectors &lt;8, No, Proper Noun,
Proper Noun, Yes&gt; and &lt;1, No, Proper Noun,
Proper Noun, Yes&gt;, respectively. Since both pairs
have N scores &gt; 0.5, A-INF incorrectly puts the
three markables into one cluster. But as we would
expect, A-INF labels many more markable pairs
</bodyText>
<footnote confidence="0.977219">
6The 2010 physics Nobel laureate.
</footnote>
<figure confidence="0.841396">
100 20000 40000 60000
Number of input Wikipedia articles
</figure>
<figureCaption confidence="0.999544">
Figure 5: MUC learning curve for A-INF.
</figureCaption>
<bodyText confidence="0.994125882352941">
with the second feature vector (distance=1) as coref-
erent than with the first one (distance=8) in the en-
tire automatically labeled training set. If we now
train SUCRE on this training set, it can resolve such
cases in the test set correctly even though they are
so similar: (Biden,President) is classified as disref-
erent and (Obama,President) as coreferent. We call
this the better precision effect.
Recall that UNSEL has better recall and precision
than A-INF in almost all cases (discussion of Ta-
ble 1). This result shows that better precision and
better recall effects do indeed benefit UNSEL.
To summarize, the advantages of our self-training
approach are: (i) We cover cases that do not occur
in the unlabeled corpus (better recall effect); and (ii)
we use the leveraging effect of a rich feature space
including distance, person, number, gender etc. to
improve precision (better precision effect).
Learning curve. Figure 5 presents MUC scores
of A-INF as a function of the number of Wikipedia
articles used in unsupervised learning. We can see
that a small number of input articles (e.g., 100) re-
sults in low recall and high precision. When we in-
crease the number of input articles, recall rapidly in-
creases and precision rapidly decreases up to about
10,000 articles. Increase and decrease continue
more slowly after that. F1 increases throughout be-
cause lower precision is compensated by higher re-
call. This learning curve demonstrates the impor-
tance of the size of the corpus for A-INF.
Comparison of UNSEL with SUCRE
Table 2 compares our unsupervised self-trained
(UNSEL) and supervised (SUCRE) models with
the recently published SemEval-2010 OntoNotes re-
</bodyText>
<figure confidence="0.94802475">
80
70
60
50
40
30
20
10
MUC-Prec.
MUC-Rec.
MUC-F1
Prec., Rec. and F1
</figure>
<page confidence="0.950821">
790
</page>
<table confidence="0.999576916666667">
Gold setting + True markables
System MD MUC B3 CEAF
Relax 100 33.7 84.5 75.6
SUCRE2010 100 60.8 82.4 74.3
SUCRE 100 64.3 87.0 80.1
UNSEL 100 63.0 86.9 79.7
Automatic setting + System markables
System MD MUC B3 CEAF
SUCRE2010 80.7 52.5 67.1 62.7
Tanl-1 73.9 24.6 61.3 57.3
SUCRE 80.9 55.7 69.7 66.6
UNSEL 80.9 55.0 69.8 66.3
</table>
<tableCaption confidence="0.971850333333333">
Table 2: Fl scores for MCORE (SUCRE and UNSEL)
and the best comparable systems in SemEval-2010. MD:
Markable Detection Fl (Recasens et al., 2010).
</tableCaption>
<bodyText confidence="0.99742364">
sults (gold and automatic settings). We compare
with the scores of the two best systems, Relax and
SUCRE20107 (for the gold setting with true mark-
ables) and SUCRE2010 and Tanl-1 (for the automatic
setting with system markables, 89.9% markable de-
tection (MD) F1). It is apparent from this table that
our supervised and unsupervised self-trained mod-
els outperform Relax, SUCRE2010 and Tanl-1. We
should make clear that we did not use the test set for
development to ensure a fair comparison with the
participant systems at SemEval-2010.
Table 1 shows that the unsupervised self-trained
system (UNSEL) does a lot worse than the su-
pervised system (SUCRE) on ACE.8 In contrast,
UNSEL performs almost as well as SUCRE on
OntoNotes (Table 2), for both gold and automatic
settings: F1 differences range from +.1 (Auto-
matic, B3) to −1.3 (Gold, MUC). We suspect that
this is partly due to the much higher proportion
of singletons in OntoNotes than in ACE-2: 85.2%
(OntoNotes) vs. 60.2% (ACE-2). The low recall of
the automatic labeling by A-INF introduces a bias
for singletons when UNSEL is self-trained. Another
reason is that the OntoNotes training set is about
4 times larger than each of BNEWS, NWIRE and
</bodyText>
<footnote confidence="0.902712833333333">
7It is the first version of our supervised system that took part
in SemEval-2010. We call it SUCRE2010.
8A reviewer observes that SUCRE’s performance is better
than the supervised system of Ng (2008). This may indicate
that part of our improved unsupervised performance in Table 1
is due to better feature engineering implemented in SUCRE.
</footnote>
<bodyText confidence="0.9986788">
NPAPER training sets. With more training data,
UNSEL can correct more of its precision and re-
call errors. For an unsupervised approach, which
only needs unlabeled data, there is little cost to cre-
ating large training sets. Thus, this comparison of
ACE-2/Ontonotes results is evidence that in a realis-
tic scenario using association information in an un-
supervised self-trained system is almost as good as
a system trained on manually labeled data.
It is important to note that the comparison of
SUCRE to UNSEL is the most direct comparison
of supervised and unsupervised CoRe learning we
are aware of. The two systems are identical with the
single exception that they are trained on manual vs.
automatic coreference labels.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999749333333333">
In this paper, we have demonstrated the utility of
association information for coreference resolution.
We first developed a simple unsupervised model for
shallow CoRe that only uses association information
for finding coreference chains. We then introduced
an unsupervised self-trained approach where a su-
pervised model is trained on a corpus that was auto-
matically labeled by the unsupervised model based
on the association information. The results of the ex-
periments indicate that the performance of the unsu-
pervised self-trained approach is better than the per-
formance of other unsupervised learning systems. In
addition, we showed that our system is a flexible and
modular framework that is able to learn from data
with different quality (perfect vs noisy markable de-
tection) and domain; and is able to deliver good re-
sults for shallow information spaces and competitive
results for rich feature spaces. Finally, our frame-
work is the first CoRe system that is designed to sup-
port three major modes of machine learning equally
well: supervised, self-trained and unsupervised.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999872">
This research was funded by DFG (grant SCHU
2246/4).
We thank Aoife Cahill, Alexander Fraser, Thomas
M¨uller and the anonymous reviewers for their help-
ful comments.
</bodyText>
<page confidence="0.995452">
791
</page>
<reference confidence="0.927262056603774">
References Christoph M¨uller, Stefan Rapp, and Michael Strube.
Amit Bagga and Breck Baldwin. 1998. Algorithms for 2002. Applying co-training to reference resolution. In
scoring coreference chains. In LREC Workshop on ACL ’02, pages 352–359.
Linguistics Coreference ’98, pages 563–566. Vincent Ng and Claire Cardie. 2003. Bootstrapping
Thomas M. Cover and Joy A. Thomas. 1991. Elements coreference classifiers with multiple machine learning
of Information Theory. Wiley. algorithms. In EMNLP ’03, pages 113–120.
Micha Elsner, Eugene Charniak, and Mark Johnson. Vincent Ng. 2008. Unsupervised models for coreference
2009. Structured generative models for unsupervised resolution. In EMNLP ’08, pages 640–649.
named-entity clustering. In HLT-NAACL ’09, pages Slav Petrov and Dan Klein. 2007. Improved inference
164–172. for unlexicalized parsing. In HLT-NAACL ’07, pages
Jenny Rose Finkel and Christopher D. Manning. 2008. 404–411.
Enforcing transitivity in coreference resolution. In Simone Paolo Ponzetto. 2010. Knowledge Acquisi-
HLT ’08, pages 45–48. tion from a Collaboratively Generated Encyclopedia,
Aria Haghighi and Dan Klein. 2007. Unsupervised volume 327 of Dissertations in Arti�cial Intelligence.
coreference resolution in a nonparametric bayesian Amsterdam, The Netherlands: IOS Press &amp; Heidel-
model. In ACL ’07, pages 848–855. berg, Germany: AKA Verlag.
Aria Haghighi and Dan Klein. 2009. Simple coreference Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
resolution with rich syntactic and semantic features. In pervised coreference resolution with markov logic. In
EMNLP ’09, pages 1152–1161. EMNLP ’08, pages 650–659.
Aria Haghighi and Dan Klein. 2010. Coreference resolu- Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha
tion in a modular, entity-centered model. In NAACL- Palmer, Lance Ramshaw, and Ralph Weischedel.
HLT ’10, pages 385–393. 2007. Ontonotes: A unified relational semantic rep-
Andrew Kehler, Douglas E. Appelt, Lara Taylor, and resentation. In ICSC ’07, pages 517–526.
Aleksandr Simma. 2004. Competitive Self-Trained J. Ross Quinlan. 1993. C4.5: Programs for machine
Pronoun Interpretation. In HLT-NAACL ’04, pages learning. Morgan Kaufmann Publishers Inc., San
33–36. Francisco, CA, USA.
Manfred Klenner and ´Etienne Ailloud. 2009. Opti- Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
mization in coreference resolution is not needed: A garajan, Nathanael Chambers, Mihai Surdeanu, Dan
nearly-optimal algorithm with intensional constraints. Jurafsky, and Christopher Manning. 2010. A multi-
In EACL, pages 442–450. pass sieve for coreference resolution. In EMNLP ’10,
Hamidreza Kobdani and Hinrich Sch¨utze. 2010. Sucre: pages 492–501.
A modular system for coreference resolution. In Se- Marta Recasens, Llu´ıs M`arquez, Emili Sapena,
mEval ’10, pages 92–95. M.Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste,
Hamidreza Kobdani, Hinrich Sch¨utze, Andre Burkovski, Massimo Poesio, and Yannick Versley. 2010.
Wiltrud Kessler, and Gunther Heidemann. 2010. Re- SemEval-2010 Task 1: Coreference resolution in
lational feature engineering of natural language pro- multiple languages. In SemEval ’10, pages 70–75.
cessing. In CIKM ’10. ACM. Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Lim. 2001. A machine learning approach to coref-
Kambhatla, and Salim Roukos. 2004. A erence resolution of noun phrases. In CL ’01, pages
Mention-Synchronous Coreference Resolution Algo- 521–544.
rithm Based on the Bell Tree. In ACL ’04, pages 135– Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
142. Riloff, David Buttler, and David Hysom. 2010. Coref-
Xiaoqiang Luo. 2005. On coreference resolution perfor- erence resolution with reconcile. In ACL ’10, pages
mance metrics. In HLT ’05, pages 25–32. 156–161.
A. McEnery, I. Tanaka, and S. Botley. 1997. Corpus Marc Vilain, John Burger, John Aberdeen, Dennis Con-
annotation and reference resolution. In ANARESOLU- nolly, and Lynette Hirschman. 1995. A model-
TION ’97, pages 67–74. theoretic coreference scoring scheme. In MUC6 ’95,
Alexis Mitchell, Stephanie Strassel, Mark Przybocki, pages 45–52.
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth
Sundheim. 2003. ACE-2 version 1.0. Linguistic Data
Consortium, Philadelphia.
792
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920802">
<title confidence="0.999586">Bootstrapping Coreference Resolution Using Word Associations</title>
<author confidence="0.99791">Hinrich Sch¨utze Kobdani</author>
<author confidence="0.99791">Michael Schiehlen</author>
<affiliation confidence="0.999019">Institute for Natural Language University of</affiliation>
<email confidence="0.968661">kobdani@ims.uni-stuttgart.de</email>
<abstract confidence="0.9977133">In this paper, we present an unsupervised framework that bootstraps a complete coreferresolution (CoRe) system from asfrom a large unlabeled corpus. We show that word associations are useful for CoRe – e.g., the strong association bean indicator of likely coreference. Association information has so far not been used in CoRe because it is sparse and difficult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system has better CoRe performance than other learning approaches that do not use manually labeled data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>References Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<journal>Aria Haghighi</journal>
<booktitle>In LREC Workshop on Linguistics Coreference ’98,</booktitle>
<volume>792</volume>
<pages>563--566</pages>
<publisher>Wiley. Micha</publisher>
<institution>USA. Karthik Raghunathan, Heeyoung Lee, Sudarshan</institution>
<location>Hongyan Jing, Nanda</location>
<note>Slav Petrov</note>
<contexts>
<context position="21568" citStr="Bagga and Baldwin, 1998" startWordPosition="3464" endWordPosition="3467"> on the test sets of ACE and OntoNotes. For the experiments with UNSEL, we use its unsupervised subsystem A-INF (which uses Wikipedia association scores) to automatically label the training sets of ACE and OntoNotes. Then for each data set, the supervised subsystem of UNSEL (i.e., SUCRE) is trained on its automatically labeled training set and then evaluated on its test set. Finally, for the supervised experiments, we use the manually labeled training sets and evaluate on the corresponding test sets. 4.2 Evaluation Metrics We report recall, precision, and F1 for MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005). We selected these three metrics because a single metric is often misleading and because we need to use metrics that were used in previous unsupervised work. It is well known that MUC by itself is insufficient because it gives misleadingly high scores to the “single-chain” system that puts all markables into one chain (Luo et al., 2004; Finkel and Manning, 2008). However, B3 and CEAF have a different bias: they give high scores to the “all-singletons” system that puts each markable in a separate chain. On OntoNotes test, we get B3 = 83.2 and CEAF = 71.2 for all-singleton</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>References Amit Bagga and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In LREC Workshop on Linguistics Coreference ’98, pages 563–566. Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley. Micha Elsner, Eugene Charniak, and Mark Johnson. 2009. Structured generative models for unsupervised named-entity clustering. In HLT-NAACL ’09, pages 164–172. Jenny Rose Finkel and Christopher D. Manning. 2008. Enforcing transitivity in coreference resolution. In HLT ’08, pages 45–48. Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric bayesian model. In ACL ’07, pages 848–855. Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In EMNLP ’09, pages 1152–1161. Aria Haghighi and Dan Klein. 2010. Coreference resolu- tion in a modular, entity-centered model. In NAACL- HLT ’10, pages 385–393. Andrew Kehler, Douglas E. Appelt, Lara Taylor, and Aleksandr Simma. 2004. Competitive Self-Trained Pronoun Interpretation. In HLT-NAACL ’04, pages 33–36. Manfred Klenner and ´Etienne Ailloud. 2009. Opti- mization in coreference resolution is not needed: A nearly-optimal algorithm with intensional constraints. In EACL, pages 442–450. Hamidreza Kobdani and Hinrich Sch¨utze. 2010. Sucre: A modular system for coreference resolution. In Se- mEval ’10, pages 92–95. Hamidreza Kobdani, Hinrich Sch¨utze, Andre Burkovski, Wiltrud Kessler, and Gunther Heidemann. 2010. Re- lational feature engineering of natural language pro- cessing. In CIKM ’10. ACM. Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A Mention-Synchronous Coreference Resolution Algo- rithm Based on the Bell Tree. In ACL ’04, pages 135– 142. Xiaoqiang Luo. 2005. On coreference resolution perfor- mance metrics. In HLT ’05, pages 25–32. A. McEnery, I. Tanaka, and S. Botley. 1997. Corpus annotation and reference resolution. In ANARESOLU- TION ’97, pages 67–74. Alexis Mitchell, Stephanie Strassel, Mark Przybocki, JK Davis, George Doddington, Ralph Grishman, Adam Meyers, Ada Brunstein, Lisa Ferro, and Beth Sundheim. 2003. ACE-2 version 1.0. Linguistic Data Consortium, Philadelphia. 792 Christoph M¨uller, Stefan Rapp, and Michael Strube. 2002. Applying co-training to reference resolution. In ACL ’02, pages 352–359. Vincent Ng and Claire Cardie. 2003. Bootstrapping coreference classifiers with multiple machine learning algorithms. In EMNLP ’03, pages 113–120. Vincent Ng. 2008. Unsupervised models for coreference resolution. In EMNLP ’08, pages 640–649. Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL ’07, pages 404–411. Simone Paolo Ponzetto. 2010. Knowledge Acquisi- tion from a Collaboratively Generated Encyclopedia, volume 327 of Dissertations in Arti�cial Intelligence. Amsterdam, The Netherlands: IOS Press &amp; Heidel- berg, Germany: AKA Verlag. Hoifung Poon and Pedro Domingos. 2008. Joint unsu- pervised coreference resolution with markov logic. In EMNLP ’08, pages 650–659. Sameer S. Pradhan, Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2007. Ontonotes: A unified relational semantic rep- resentation. In ICSC ’07, pages 517–526. J. Ross Quinlan. 1993. C4.5: Programs for machine learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran- garajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multi- pass sieve for coreference resolution. In EMNLP ’10, pages 492–501. Marta Recasens, Llu´ıs M`arquez, Emili Sapena, M.Ant`onia Mart´ı, Mariona Taul´e, V´eronique Hoste, Massimo Poesio, and Yannick Versley. 2010. SemEval-2010 Task 1: Coreference resolution in multiple languages. In SemEval ’10, pages 70–75. Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coref- erence resolution of noun phrases. In CL ’01, pages 521–544. Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2010. Coref- erence resolution with reconcile. In ACL ’10, pages 156–161. Marc Vilain, John Burger, John Aberdeen, Dennis Con- nolly, and Lynette Hirschman. 1995. A model- theoretic coreference scoring scheme. In MUC6 ’95, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>