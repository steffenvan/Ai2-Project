<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.935066">
Maximum Entropy Based Restoration of Arabic Diacritics
</title>
<author confidence="0.652585">
Imed Zitouni, Jeffrey S. Sorensen, Ruhi Sarikaya
</author>
<affiliation confidence="0.5512">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.718266">
1101 Kitchawan Rd, Yorktown Heights, NY 10598
</address>
<email confidence="0.926557">
{izitouni, sorenj, sarikaya}@us.ibm.com
</email>
<sectionHeader confidence="0.993645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908666666667">
Short vowels and other diacritics are not
part of written Arabic scripts. Exceptions
are made for important political and reli-
gious texts and in scripts for beginning stu-
dents of Arabic. Script without diacritics
have considerable ambiguity because many
words with different diacritic patterns ap-
pear identical in a diacritic-less setting. We
propose in this paper a maximum entropy
approach for restoring diacritics in a doc-
ument. The approach can easily integrate
and make effective use of diverse types of
information; the model we propose inte-
grates a wide array of lexical, segment-
based and part-of-speech tag features. The
combination of these feature types leads
to a state-of-the-art diacritization model.
Using a publicly available corpus (LDC&apos;s
Arabic Treebank Part 3), we achieve a di-
acritic error rate of 5.1%, a segment error
rate 8.5%, and a word error rate of 17.3%.
In case-ending-less setting, we obtain a di-
acritic error rate of 2.2%, a segment error
rate 4.0%, and a word error rate of 7.2%.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999757568965518">
Modern Arabic written texts are composed of
scripts without short vowels and other diacritic
marks. This often leads to considerable ambigu-
ity since several words that have different diacritic
patterns may appear identical in a diacritic-less
setting. Educated modern Arabic speakers are able
to accurately restore diacritics in a document. This
is based on the context and their knowledge of the
grammar and the lexicon of Arabic. However, a
text without diacritics becomes a source of confu-
sion for beginning readers and people with learning
disabilities. A text without diacritics is also prob-
lematic for applications such as text-to-speech or
speech-to-text, where the lack of diacritics adds
another layer of ambiguity when processing the
data. As an example, full vocalization of text is
required for text-to-speech applications, where the
mapping from graphemes to phonemes is simple
compared to languages such as English and French;
where there is, in most cases, one-to-one relation-
ship. Also, using data with diacritics shows an
improvement in the accuracy of speech-recognition
applications (Afify et al., 2004). Currently, text-to-
speech, speech-to-text, and other applications use
data where diacritics are placed manually, which
is a tedious and time consuming excercise. A di-
acritization system that restores the diacritics of
scripts, i.e. supply the full diacritical markings,
would be of interest to these applications. It also
would greatly benefit nonnative speakers, sufferers
of dyslexia and could assist in restoring diacritics
of children’s and poetry books, a task that is cur-
rently done manually.
We propose in this paper a statistical approach
that restores diacritics in a text document. The
proposed approach is based on the maximum en-
tropy framework where several diverse sources of
information are employed. The model implicitly
learns the correlation between these types of infor-
mation and the output diacritics.
In the next section, we present the set of diacrit-
ics to be restored and the ambiguity we face when
processing a non-diacritized text. Section 3 gives
a brief summary of previous related works. Sec-
tion 4 presents our diacritization model; we ex-
plain the training and decoding process as well as
the different feature categories employed to restore
the diacritics. Section 5 describes a clearly defined
and replicable split of the LDC&apos;s Arabic Treebank
Part 3 corpus, used to built and evaluate the sys-
tem, so that the reproduction of the results and
future comparison can accurately be established.
Section 6 presents the experimental results. Sec-
tion 7 reports a comparison of our approach to
the finite state machine modeling technique that
showed promissing results in (Nelken and Shieber,
2005). Finally, section 8 concludes the paper and
discusses future directions.
</bodyText>
<sectionHeader confidence="0.995523" genericHeader="introduction">
2 Arabic Diacritics
</sectionHeader>
<bodyText confidence="0.99650625">
The Arabic alphabet consists of 28 letters that can
be extended to a set of 90 by additional shapes,
marks, and vowels (Tayli and Al-Salamah, 1990).
The 28 letters represent the consonants and long
</bodyText>
<page confidence="0.93787">
577
</page>
<note confidence="0.815718181818182">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 577–584,
Sydney, July 2006. c�2006 Association for Computational Linguistics
vowels such as ,  (both pronounced as /a:/),
 (pronounced as /i:/), and  (pronounced as
/u:/). Long vowels are constructed by combin-
ing , , , and  with the short vowels. The
short vowels and certain other phonetic informa-
tion such as consonant doubling (shadda) are not
represented by letters, but by diacritics. A dia-
critic is a short stroke placed above or below the
consonant. Table 1 shows the complete set of Ara-
</note>
<table confidence="0.997669941176471">
Diacritic Name Meaning
 Pronunciation
on 
Short vowels
 fatha /a/
 damma /u/
 kasra /i/
Doubled case ending (“tanween&amp;quot;)
  tanween /an/
 al-fatha /un/
 tanween al-damma /in/
tanween al-kasra
Syllabification marks
 shadda consonant
  sukuun doubling
vowel
absence
</table>
<tableCaption confidence="0.7691925">
Table 1: Arabic diacritics on the letter – consonant

</tableCaption>
<bodyText confidence="0.903297714285714">
–  (pronounced as /t/).
bic diacritics. We split the Arabic diacritics into
three sets: short vowels, doubled case endings, and
syllabification marks. Short vowels are written as
symbols either above or below the letter in text
with diacritics, and dropped all together in text
without diacritics. We find three short vowels:
</bodyText>
<listItem confidence="0.931502111111111">
• fatha: it represents the /a/ sound and is an
oblique dash over a consonant as in  (c.f.
fourth row of Table 1).
• damma: it represents the /u/ sound and is
a loop over a consonant that resembles the
shape of a comma (c.f. fifth row of Table 1).
• kasra: it represents the /i/ sound and is an
oblique dash under a consonant (c.f. sixth row
of Table 1).
</listItem>
<bodyText confidence="0.996941090909091">
The doubled case ending diacritics are vowels used
at the end of the words to mark case distinction,
which can be considered as a double short vowels;
the term “tanween&amp;quot; is used to express this phe-
nomenon. Similar to short vowels, there are three
different diacritics for tanween: tanween al-fatha,
tanween al-damma, and tanween al-kasra. They
are placed on the last letter of the word and have
the phonetic effect of placing an “N&amp;quot; at the end
of the word. Text with diacritics contains also two
syllabification marks:
</bodyText>
<listItem confidence="0.995295666666667">
• shadda: it is a gemination mark placed above

the Arabic letters as in . It denotes the dou-
</listItem>
<subsectionHeader confidence="0.276071">
bling of the consonant. The shadda is usually
</subsectionHeader>
<bodyText confidence="0.580066">

</bodyText>
<subsectionHeader confidence="0.398654">
combined with a short vowel such as in .
</subsectionHeader>
<bodyText confidence="0.583015">

</bodyText>
<listItem confidence="0.994012">
• sukuun: written as a small circle as in . It is
</listItem>
<bodyText confidence="0.9813145">
used to indicate that the letter doesn’t contain
vowels.
Figure 1 shows an Arabic sentence transcribed with
and without diacritics. In modern Arabic, writing
scripts without diacritics is the most natural way.
Because many words with different vowel patterns
may appear identical in a diacritic-less setting,
considerable ambiguity exists at the word level.
The word  , for example, has 21 possible forms
that have valid interpretations when adding dia-
critics (Kirchhoff and Vergyri, 2005). It may have 
the interpretation of the verb “to write” in  
(pronounced /kataba/). Also, it can be interpreted
as “books” in the noun form  
tubun/). A study made by (Debili et al., 2002)
shows that there is an average of 11.6 possible di-
acritizations for every non-diacritized word when
analyzing a text of 23,000 script forms.
</bodyText>
<equation confidence="0.9683066">
    
  
    


</equation>
<figureCaption confidence="0.952374">
Figure 1: The same Arabic sentence without (up-
</figureCaption>
<bodyText confidence="0.956671846153846">
per row) and with (lower row) diacritics. The En-
glish translation is “the president wrote the docu-
ment.&amp;quot;
Arabic diacritic restoration is a non-trivial task as
expressed in (El-Imam, 2003). Native speakers of
Arabic are able, in most cases, to accurately vo-
calize words in text based on their context, the
speaker’s knowledge of the grammar, and the lex-
icon of Arabic. Our goal is to convert knowledge
used by native speakers into features and incor-
porate them into a maximum entropy model. We
assume that the input text does not contain any
diacritics.
</bodyText>
<sectionHeader confidence="0.99874" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.98474475">
Diacritic restoration has been receiving increas-
ing attention and has been the focus of several
studies. In (El-Sadany and Hashish, 1988), a rule
based method that uses morphological analyzer for
</bodyText>
<equation confidence="0.871283">
 
(pronounced /ku-
</equation>
<page confidence="0.989004">
578
</page>
<bodyText confidence="0.999863965909091">
vowelization was proposed. Another, rule-based
grapheme to sound conversion approach was ap-
peared in 2003 by Y. El-Imam (El-Imam, 2003).
The main drawbacks of these rule based methods is
that it is difficult to maintain the rules up-to-date
and extend them to other Arabic dialects. Also,
new rules are required due to the changing nature
of any “living” language.
More recently, there have been several new stud-
ies that use alternative approaches for the diacriti-
zation problem. In (Emam and Fisher, 2004) an
example based hierarchical top-down approach is
proposed. First, the training data is searched hi-
erarchically for a matching sentence. If there is
a matching sentence, the whole utterance is used.
Otherwise they search for matching phrases, then
words to restore diacritics. If there is no match at
all, character n-gram models are used to diacritize
each word in the utterance.
In (Vergyri and Kirchhoff, 2004), diacritics in
conversational Arabic are restored by combining
morphological and contextual information with an
acoustic signal. Diacritization is treated as an un-
supervised tagging problem where each word is
tagged as one of the many possible forms provided
by the Buckwalter&apos;s morphological analyzer (Buck-
walter, 2002). The Expectation Maximization
(EM) algorithm is used to learn the tag sequences.
Y. Gal in (Gal, 2002) used a HMM-based diacriti-
zation approach. This method is a white-space
delimited word based approach that restores only
vowels (a subset of all diacritics).
Most recently, a weighted finite state machine
based algorithm is proposed (Nelken and Shieber,
2005). This method employs characters and larger
morphological units in addition to words. Among
all the previous studies this one is more sophisti-
cated in terms of integrating multiple information
sources and formulating the problem as a search
task within a unified framework. This approach
also shows competitive results in terms of accuracy
when compared to previous studies. In their algo-
rithm, a character based generative diacritization
scheme is enabled only for words that do not occur
in the training data. It is not clearly stated in the
paper whether their method predict the diacritics
shedda and sukuun.
Even though the methods proposed for diacritic
restoration have been maturing and improving over
time, they are still limited in terms of coverage and
accuracy. In the approach we present in this paper,
we propose to restore the most comprehensive list
of the diacritics that are used in any Arabic text.
Our method differs from the previous approaches
in the way the diacritization problem is formulated
and because multiple information sources are inte-
grated. We view the diacritic restoration problem
as sequence classification, where given a sequence
of characters our goal is to assign diacritics to each
character. Our appoach is based on Maximum
Entropy (MaxEnt henceforth) technique (Berger
et al., 1996). MaxEnt can be used for sequence
classification, by converting the activation scores
into probabilities (through the soft-max function,
for instance) and using the standard dynamic pro-
gramming search algorithm (also known as Viterbi
search). We find in the literature several other
approaches of sequence classification such as (Mc-
Callum et al., 2000) and (Lafferty et al., 2001).
The conditional random fields method presented
in (Lafferty et al., 2001) is essentially a MaxEnt
model over the entire sequence: it differs from the
Maxent in that it models the sequence informa-
tion, whereas the Maxent makes a decision for each
state independently of the other states. The ap-
proach presented in (McCallum et al., 2000) com-
bines Maxent with Hidden Markov models to allow
observations to be presented as arbitrary overlap-
ping features, and define the probability of state
sequences given observation sequences.
We report in section 7 a comparative study be-
tween our approach and the most competitive dia-
critic restoration method that uses finite state ma-
chine algorithm (Nelken and Shieber, 2005). The
MaxEnt framework was successfully used to com-
bine a diverse collection of information sources and
yielded a highly competitive model that achieves a
5.1% DER.
</bodyText>
<sectionHeader confidence="0.992532" genericHeader="method">
4 Automatic Diacritization
</sectionHeader>
<bodyText confidence="0.999806">
The performance of many natural language pro-
cessing tasks, such as shallow parsing (Zhang et
al., 2002) and named entity recognition (Florian
et al., 2004), has been shown to depend on inte-
grating many sources of information. Given the
stated focus of integrating many feature types, we
selected the MaxEnt classifier. MaxEnt has the
ability to integrate arbitrary types of information
and make a classification decision by aggregating
all information available for a given classification.
</bodyText>
<subsectionHeader confidence="0.999623">
4.1 Maximum Entropy Classifiers
</subsectionHeader>
<bodyText confidence="0.913247866666666">
We formulate the task of restoring diacritics as
a classification problem, where we assign to each
character in the text a label (i.e., diacritic). Be-
fore formally describing the method&apos;, we introduce
some notations: let Y = {yl, ... , yn} be the set of
diacritics to predict or restore, X be the example
space and J7 = {0, 1} &apos; be a feature space. Each ex-
ample x E X has associated a vector of binary fea-
tures f (x) = (fl (x) , ... , f,,,, (x)). In a supervised
framework, like the one we are considering here, we
have access to a set of training examples together
with their classifications: {(xi, yi) , . . . , (xk, yk)}.
&apos;This is not meant to be an in-depth introduction
to the method, but a brief overview to familiarize the
reader with them.
</bodyText>
<page confidence="0.991737">
579
</page>
<subsectionHeader confidence="0.852107">
The MaxEnt algorithm associates a set of weights
</subsectionHeader>
<bodyText confidence="0.999368571428571">
(αij)i=1...n
j=1...m with the features, which are estimated
during the training phase to maximize the likeli-
hood of the data (Berger et al., 1996). Given these
weights, the model computes the probability dis-
tribution over labels for a particular example x as
follows:
</bodyText>
<equation confidence="0.88869925">
1
P (y |x) = 11 f4 (x) Z(x)
11
Z(x)j=1 aZj ,=i j
</equation>
<bodyText confidence="0.99946225">
where Z(X) is a normalization factor. To esti-
mate the optimal αj values, we train our Max-
Ent model using the sequential conditional gener-
alized iterative scaling (SCGIS) technique (Good-
man, 2002). While the MaxEnt method can nicely
integrate multiple feature types seamlessly, in cer-
tain cases it is known to overestimate its confidence
in especially low-frequency features. To overcome
this problem, we use the regularization method
based on adding Gaussian priors as described in
(Chen and Rosenfeld, 2000). After computing the
class probability distribution, the chosen diacritic
is the one with the most aposteriori probability.
The decoding algorithm, described in section 4.2,
performs sequence classification, through dynamic
programming.
</bodyText>
<subsectionHeader confidence="0.998615">
4.2 Search to Restore Diacritics
</subsectionHeader>
<bodyText confidence="0.976312578947369">
We are interested in finding the diacritics of all
characters in a script or a sentence. These dia-
critics have strong interdependencies which can-
not be properly modeled if the classification is per-
formed independently for each character. We view
this problem as sequence classification, as con-
trasted with an example-based classification prob-
lem: given a sequence of characters in a sentence
x1x2 ... xL, our goal is to assign diacritics (labels)
to each character, resulting in a sequence of diacrit-
ics y1y2 ... yL. We make an assumption that dia-
critics can be modeled as a limited order Markov
sequence: the diacritic associated with the char-
acter i depends only on the diacritics associated
with the k previous diacritics, where k is usually
equal to 3. Given this assumption, and the nota-
tion xL1 = x1 ... xL, the conditional probability of
assigning the diacritic sequence yL1 to the character
sequence xL1 becomes
</bodyText>
<equation confidence="0.998197666666667">
p (yL1 |x1) =
p (y|x1p|x1y1) �x1yL+1)1)(y2,p (YL -k
(1)
</equation>
<bodyText confidence="0.9877445">
and our goal is to find the sequence that maximizes
this conditional probability
</bodyText>
<equation confidence="0.941092">
p (yL�
1 |xL (2)
1
</equation>
<bodyText confidence="0.9975405">
While we restricted the conditioning on the classi-
fication tag sequence to the previous k diacritics,
we do not impose any restrictions on the condition-
ing on the characters - the probability is computed
using the entire character sequence xL1 .
To obtain the sequence in Equation (2), we create
a classification tag lattice (also called trellis), as
follows:
</bodyText>
<listItem confidence="0.978348157894737">
• Let xL1 be the input sequence of character and
S = {s1, s2, ... , sm} be an enumeration of Yk
(m = |Y|k) - we will call an element sj a state.
Every such state corresponds to the labeling
of k successive characters. We find it useful
to think of an element si as a vector with k
elements. We use the notations si U] for jth
element of such a vector (the label associated
with the token xi−k+j+1) and si U1 ... j2] for
the sequence of elements between indices j1
and j2.
• We conceptually associate every character
xi, i = 1, ... , L with a copy of S, Si =
{si1, ... , si}; this set represents all the possi-
ble labelings of characters xii−k+1 at the stage
where xi is examined.
• We then create links from the set Si to the
Si+1, for all i = 1... L − 1, with the property
that
</listItem>
<equation confidence="0.906562333333333">
p (si+1
j1 [k] |xL1 , si+1
j2 [1..k − 1])
if sij1 [2..k] = si+1
j2 [1..k − 1]
0 otherwise
</equation>
<bodyText confidence="0.759867333333333">
These weights correspond to probability of a
transition from the state sij1 to the state si+1
j2 .
</bodyText>
<listItem confidence="0.7146755">
• For every character xi, we compute recur-
sively2
</listItem>
<equation confidence="0.998241285714286">
β0 (sj) = 0, j = 1,..., k
βi−1 (sj1) + log w (si−1 )
βi (sj) = max j1 , si j
j1=1,...,M
γi (sj) =
i−1 i
β i−1 (sj1) + log w (sj1 , sj)
</equation>
<bodyText confidence="0.9716568">
Intuitively, βi (sj) represents the log-
probability of the most probable path through
the lattice that ends in state sj after i steps,
and γi (sj) represents the state just before sj
on that particular path.
</bodyText>
<listItem confidence="0.9984534">
• Having computed the (βi)i values, the algo-
rithm for finding the best path, which corre-
sponds to the solution of Equation (2) is
1. Identify sLL = arg maxj=1...L βL (sj)
2. For i = L − 1...1, compute
</listItem>
<equation confidence="0.586725">
Sii = γi+1 N+1)
</equation>
<footnote confidence="0.983857">
2For convenience, the index i associated with state
).si j is moved to β; the function βi (sj) is in fact β (si j
</footnote>
<equation confidence="0.963917909090909">
αf4(x)
ij
yL1 = arg max
y�
1
( si
i+1) =
w s71 , sj2
I
arg max
j1=1,...,M
</equation>
<page confidence="0.948719">
580
</page>
<bodyText confidence="0.400202">
3. The solution for Equation (2) is given by
</bodyText>
<equation confidence="0.7373635">
y = {s11[k], s22[k], ... , s�� [k]}
( )
</equation>
<bodyText confidence="0.9959585">
The runtime of the algorithm is � |Y|k · L , linear
in the size of the sentence L but exponential in the
size of the Markov dependency, k. To reduce the
search space, we use beam-search.
</bodyText>
<subsectionHeader confidence="0.997401">
4.3 Features Employed
</subsectionHeader>
<bodyText confidence="0.998666625">
Within the MaxEnt framework, any type of fea-
tures can be used, enabling the system designer to
experiment with interesting feature types, rather
than worry about specific feature interactions. In
contrast, with a rule based system, the system de-
signer would have to consider how, for instance,
lexical derived information for a particular exam-
ple interacts with character context information.
That is not to say, ultimately, that rule-based sys-
tems are in some way inferior to statistical mod-
els – they are built using valuable insight which
is hard to obtain from a statistical-model-only ap-
proach. Instead, we are merely suggesting that the
output of such a rule-based system can be easily
integrated into the MaxEnt framework as one of
the input features, most likely leading to improved
performance.
Features employed in our system can be divided
into three different categories: lexical, segment-
based, and part-of-speech tag (POS) features. We
also use the previously assigned two diacritics as
additional features.
In the following, we briefly describe the different
categories of features:
</bodyText>
<listItem confidence="0.805613272727273">
• Lexical Features: we include the charac-
ter n-gram spanning the curent character xi,
both preceding and following it in a win-
dow of 7: {xi−3, . . . , xi+3}. We use the cur-
rent word wi and its word context in a win-
dow of 5 (forward and backward trigram):
{wi−2, . . . , wi+2}. We specify if the character
of analysis is at the beginning or at the end
of a word. We also add joint features between
the above source of information.
• Segment-Based Features : Arabic blank-
</listItem>
<bodyText confidence="0.997912">
delimited words are composed of zero or more
prefixes, followed by a stem and zero or more
suffixes. Each prefix, stem or suffix will be
called a segment in this paper. Segments are
often the subject of analysis when processing
Arabic (Zitouni et al., 2005). Syntactic in-
formation such as POS or parse information
is usually computed on segments rather than
words. As an example, the Arabic white-space
delimited word ��&amp;quot;����l&amp;quot;�
</bodyText>
<subsectionHeader confidence="0.417104">
s contains a verb���ls, a
</subsectionHeader>
<bodyText confidence="0.965982">
&amp;quot;
third-person feminine singular subject-marker
u (she), and a pronoun suffix �&amp;A (them); it
is also a complete sentence meaning “she met
them.” To separate the Arabic white-space
delimited words into segments, we use a seg-
mentation model similar to the one presented
by (Lee et al., 2003). The model obtains an
accuracy of about 98%. In order to simulate
real applications, we only use segments gener-
ated by the model rather than true segments.
In the diacritization system, we include the
current segment ai and its word segment con-
text in a window of 5 (forward and backward
trigram): {ai−2, . . . , ai+2}. We specify if the
character of analysis is at the beginning or at
the end of a segment. We also add joint infor-
mation with lexical features.
</bodyText>
<listItem confidence="0.7324035">
• POS Features : we attach to the segment
ai of the current character, its POS: POS(ai).
</listItem>
<bodyText confidence="0.994956">
This is combined with joint features that in-
clude the lexical and segment-based informa-
tion. We use a statistical POS tagging system
built on Arabic Treebank data with MaxEnt
framework (Ratnaparkhi, 1996). The model
has an accuracy of about 96%. We did not
want to use the true POS tags because we
would not have access to such information in
real applications.
</bodyText>
<sectionHeader confidence="0.998082" genericHeader="method">
5 Data
</sectionHeader>
<bodyText confidence="0.999976285714286">
The diacritization system we present here is
trained and evaluated on the LDC&apos;s Arabic Tree-
bank of diacritized news stories – Part 3 v1.0: cata-
log number LDC2004T11 and ISBN 1-58563-298-8.
The corpus includes complete vocalization (includ-
ing case-endings). We introduce here a clearly de-
fined and replicable split of the corpus, so that the
reproduction of the results or future investigations
can accurately and correctly be established. This
corpus includes 600 documents from the An Nahar
News Text. There are a total of 340,281 words. We
split the corpus into two sets: training data and de-
velopment test (devtest) data. The training data
contains 288,000 words approximately, whereas the
devtest contains close to 52,000 words. The 90
documents of the devtest data are created by tak-
ing the last (in chronological order) 15% of docu-
ments dating from “20021015 0101” (i.e., October
15, 2002) to “20021215 0045” (i.e., December 15,
2002). The time span of the devtest is intention-
ally non-overlapping with that of the training set,
as this models how the system will perform in the
real world.
Previously published papers use proprietary cor-
pus or lack clear description of the training/devtest
data split, which make the comparison to other
techniques difficult. By clearly reporting the split
of the publicly available LDC&apos;s Arabic Treebank
</bodyText>
<page confidence="0.995882">
581
</page>
<bodyText confidence="0.7817385">
corpus in this section, we want future comparisons
to be correctly established.
</bodyText>
<sectionHeader confidence="0.996952" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999876628571428">
Experiments are reported in terms of word error
rate (WER), segment error rate (SER), and di-
acritization error rate (DER). The DER is the
proportion of incorrectly restored diacritics. The
WER is the percentage of incorrectly diacritized
white-space delimited words: in order to be
counted as incorrect, at least one character in the
word must have a diacritization error. The SER
is similar to WER but indicates the proportion of
incorrectly diacritized segments. A segment can
be a prefix, a stem, or a suffix. Segments are often
the subject of analysis when processing Arabic (Zi-
touni et al., 2005). Syntactic information such as
POS or parse information is based on segments
rather than words. Consequently, it is important
to know the SER in cases where the diacritization
system may be used to help disambiguate syntactic
information.
Several modern Arabic scripts contains the con-
sonant doubling “shadda&amp;quot;; it is common for na-
tive speakers to write without diacritics except the
shadda. In this case the role of the diacritization
system will be to restore the short vowels, doubled
case ending, and the vowel absence “sukuun&amp;quot;. We
run two batches of experiments: a first experiment
where documents contain the original shadda and
a second one where documents don’t contain any
diacritics including the shadda. The diacritization
system proceeds in two steps when it has to pre-
dict the shadda: a first step where only shadda is
restored and a second step where other diacritics
(excluding shadda) are predicted.
To assess the performance of the system under dif-
ferent conditions, we consider three cases based on
the kind of features employed:
</bodyText>
<listItem confidence="0.916522">
1. system that has access to lexical features only;
2. system that has access to lexical and segment-
based features;
3. system that has access to lexical, segment-
based and POS features.
</listItem>
<bodyText confidence="0.987974727272727">
The different system types described above use the
two previously assigned diacritics as additional fea-
ture. The DER of the shadda restoration step is
equal to 5% when we use lexical features only, 0.4%
when we add segment-based information, and 0.3%
when we employ lexical, POS, and segment-based
features.
Table 2 reports experimental results of the diacriti-
zation system with different feature sets. Using
only lexical features, we observe a DER of 8.2%
and a WER of 25.1% which is competitive to a
</bodyText>
<table confidence="0.99917025">
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
24.8 12.6 7.9 25.1 13.0 8.2
Lexical + segment-based features
18.2 9.0 5.5 18.8 9.4 5.8
Lexical + segment-based + POS features
17.3 8.5 5.1 18.0 8.9 5.5
</table>
<tableCaption confidence="0.997807">
Table 2: The impact of features on the diacriti-
</tableCaption>
<bodyText confidence="0.994539021276596">
zation system performance. The columns marked
with “True shadda&amp;quot; represent results on docu-
ments containing the original consonant doubling
“shadda&amp;quot; while columns marked with “Predicted
shadda&amp;quot; represent results where the system re-
stored all diacritics including shadda.
state-of-the-art system evaluated on Arabic Tree-
bank Part 2: in (Nelken and Shieber, 2005) a DER
of 12.79% and a WER of 23.61% are reported.
The system they described in (Nelken and Shieber,
2005) uses lexical, segment-based, and morpholog-
ical information. Table 2 also shows that, when
segment-based information is added to our sys-
tem, a significant improvement is achieved: 25%
for WER (18.8 vs. 25.1), 38% for SER (9.4 vs.
13.0), and 41% for DER (5.8 vs. 8.2). Similar be-
havior is observed when the documents contain the
original shadda. POS features are also helpful in
improving the performance of the system. They
improved the WER by 4% (18.0 vs. 18.8), SER by
5% (8.9 vs. 9.4), and DER by 5% (5.5 vs. 5.8).
Case-ending in Arabic documents consists of the
diacritic attributed to the last character in a white-
space delimited word. Restoring them is the most
difficult part in the diacritization of a document.
Case endings are only present in formal or highly
literary scripts. Only educated speakers of mod-
ern standard Arabic master their use. Technically,
every noun has such an ending, although at the
end of a sentence no inflection is pronounced, even
in formal speech, because of the rules of ‘pause’.
For this reason, we conduct another experiment in
which case-endings were stripped throughout the
training and testing data without the attempt to
restore them.
We present in Table 3 the performance of the di-
acritization system on documents without case-
endings. Results clearly show that when case-
endings are omitted, the WER declines by 58%
(7.2% vs. 17.3%), SER is decreased by 52% (4.0%
vs. 8.5%), and DER is reduced by 56% (2.2% vs.
5.1%). Also, Table 3 shows again that a richer
set of features results in a better performance;
compared to a system using lexical features only,
adding POS and segment-based features improved
the WER by 38% (7.2% vs. 11.8%), the SER by
39% (4.0% vs. 6.6%), and DER by 38% (2.2% vs.
</bodyText>
<page confidence="0.992789">
582
</page>
<table confidence="0.999255625">
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
11.8 6.6 3.6 12.4 7.0 3.9
Lexical + segment-based features
7.8 4.4 2.4 8.6 4.8 2.7
Lexical + segment-based + POS features
7.2 4.0 2.2 7.9 4.4 2.5
</table>
<tableCaption confidence="0.99912">
Table 3: Performance of the diacritization system
</tableCaption>
<bodyText confidence="0.9770866">
based on employed features. System is trained
and evaluated on documents without case-ending.
Columns marked with “True shadda&amp;quot; represent re-
sults on documents containing the original con-
sonant doubling “shadda&amp;quot; while columns marked
with “Predicted shadda&amp;quot; represent results where
the system restored all diacritics including shadda.
3.6%). Similar to the results reported in Table 2,
we show that the performance of the system are
similar whether the document contains the origi-
nal shadda or not. A system like this trained on
non case-ending documents can be of interest to
applications such as speech recognition, where the
last state of a word HMM model can be defined to
absorb all possible vowels (Afify et al., 2004).
</bodyText>
<sectionHeader confidence="0.953465" genericHeader="method">
7 Comparison to other approaches
</sectionHeader>
<bodyText confidence="0.998244833333333">
As stated in section 3, the most recent and ad-
vanced approach to diacritic restoration is the one
presented in (Nelken and Shieber, 2005): they
showed a DER of 12.79% and a WER of 23.61% on
Arabic Treebank corpus using finite state transduc-
ers (FST) with a Katz language modeling (LM) as
described in (Chen and Goodman, 1999). Because
they didn’t describe how they split their corpus
into training/test sets, we were not able to use the
same data for comparison purpose.
In this section, we want essentially to duplicate
the aforementioned FST result for comparison us-
ing the identical training and testing set we use for
our experiments. We also propose some new vari-
ations on the finite state machine modeling tech-
nique which improve performance considerably.
The algorithm for FST based vowel restoration
could not be simpler: between every pair of char-
acters we insert diacritics if doing so improves
the likelihood of the sequence as scored by a sta-
tistical n-gram model trained upon the training
corpus. Thus, in between every pair of charac-
ters we propose and score all possible diacritical
insertions. Results reported in Table 4 indicate
the error rates of diacritic restoration (including
shadda). We show performance using both Kneser-
Ney and Katz LMs (Chen and Goodman, 1999)
with increasingly large n-grams. It is our opinion
that large n-grams effectively duplicate the use of
a lexicon. It is unfortunate but true that, even for
a rich resource like the Arabic Treebank, the choice
of modeling heuristic and the effects of small sam-
ple size are considerable. Using the finite state ma-
chine modeling technique, we obtain similar results
to those reported in (Nelken and Shieber, 2005): a
WER of 23% and a DER of 15%. Better perfor-
mance is reached with the use of Kneser-Ney LM.
These results still under-perform those obtained
by MaxEnt approach presented in Table 2. When
all sources of information are included, the Max-
Ent technique outperforms the FST model by 21%
(22% vs. 18%) in terms of WER and 39% (9% vs.
5.5%) in terms of DER.
The SER reported on Table 2 and Table 3 are based
on the Arabic segmentation system we use in the
MaxEnt approach. Since, the FST model doesn’t
use such a system, we found inappropriate to re-
port SER in this section.
</bodyText>
<table confidence="0.94605775">
n-gram size Katz LM Kneser -Ney LM
WER DER WER DER
3 63 31 55 28
4 54 25 38 19
5 51 21 28 13
6 44 18 24 11
7 39 16 23 11
8 37 15 23 10
</table>
<tableCaption confidence="0.7726205">
Table 4: Error Rate in % for n-gram diacritic
restoration using FST.
</tableCaption>
<bodyText confidence="0.999947538461538">
We propose in the following an extension to the
aforementioned FST model, where we jointly de-
termines not only diacritics but segmentation into
affixes as described in (Lee et al., 2003). Table 5
gives the performance of the extended FST model
where Kneser-Ney LM is used, since it produces
better results. This should be a much more dif-
ficult task, as there are more than twice as many
possible insertions. However, the choice of diacrit-
ics is related to and dependent upon the choice of
segmentation. Thus, we demonstrate that a richer
internal representation produces a more powerful
model.
</bodyText>
<sectionHeader confidence="0.997661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999872928571429">
We presented in this paper a statistical model for
Arabic diacritic restoration. The approach we pro-
pose is based on the Maximum entropy framework,
which gives the system the ability to integrate dif-
ferent sources of knowledge. Our model has the ad-
vantage of successfully combining diverse sources
of information ranging from lexical, segment-based
and POS features. Both POS and segment-based
features are generated by separate statistical sys-
tems – not extracted manually – in order to sim-
ulate real world applications. The segment-based
features are extracted from a statistical morpho-
logical analysis system using WFST approach and
the POS features are generated by a parsing model
</bodyText>
<page confidence="0.995098">
583
</page>
<figure confidence="0.641433">
True Shadda Predicted Shadda
n-gram size Kneser-Ney Kneser-Ney
WER DER WER DER
3 49 23 52 27
4 34 14 35 17
5 26 11 26 12
6 23 10 23 10
7 23 9 22 10
8 23 9 22 10
</figure>
<tableCaption confidence="0.907067">
Table 5: Error Rate in % for n-gram dia-
</tableCaption>
<bodyText confidence="0.970857176470588">
critic restoration and segmentation using FST
and Kneser-Ney LM. Columns marked with “True
shadda&amp;quot; represent results on documents contain-
ing the original consonant doubling “shadda&amp;quot; while
columns marked with “Predicted shadda&amp;quot; repre-
sent results where the system restored all diacritics
including shadda.
that also uses Maximum entropy framework. Eval-
uation results show that combining these sources of
information lead to state-of-the-art performance.
As future work, we plan to incorporate Buckwalter
morphological analyzer information to extract new
features that reduce the search space. One idea will
be to reduce the search to the number of hypothe-
ses, if any, proposed by the morphological analyzer.
We also plan to investigate additional conjunction
features to improve the accuracy of the model.
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999155333333333">
Grateful thanks are extended to Radu Florian for
his constructive comments regarding the maximum
entropy classifier.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930275">
M. Afify, S. Abdou, J. Makhoul, L. Nguyen, and B. Xi-
ang. 2004. The BBN RT04 BN Arabic System. In
RT04 Workshop, Palisades NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
T. Buckwalter. 2002. Buckwalter Arabic morpholog-
ical analyzer version 1.0. Technical report, Linguis-
tic Data Consortium, LDC2002L49 and ISBN 1-58563-
257-0.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. computer speech and language. Computer
Speech and Language, 4(13):359–393.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. IEEE Trans.
on Speech and Audio Processing.
F. Debili, H. Achour, and E. Souissi. 2002. De
l&apos;etiquetage grammatical a` la voyellation automatique
de l&apos;arabe. Technical report, Correspondances de
l&apos;Institut de Recherche sur le Maghreb Contemporain
17.
Y. El-Imam. 2003. Phonetization of arabic: rules and
algorithms. Computer Speech and Language, 18:339–
373.
T. El-Sadany and M. Hashish. 1988. Semi-automatic
vowelization of Arabic verbs. In 10th NC Conference,
Jeddah, Saudi Arabia.
O. Emam and V. Fisher. 2004. A hierarchical ap-
proach for the statistical vowelization of Arabic text.
Technical report, IBM patent filed, DE9-2004-0006, US
patent application US2005/0192809 A1.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004.
A statistical model for multilingual entity detection
and tracking. In Proceedings of HLT-NAACL 2004,
pages 1–8.
Y. Gal. 2002. An HMM approach to vowel restora-
tion in Arabic and Hebrew. In ACL-02 Workshop on
Computational Approaches to Semitic Languages.
Joshua Goodman. 2002. Sequential conditional gener-
alized iterative scaling. In Proceedings of ACL&apos;02.
K. Kirchhoff and D. Vergyri. 2005. Cross-dialectal
data sharing for acoustic modeling in Arabic speech
recognition. Speech Communication, 46(1):37–51, May.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and
H. Hassan. 2003. Language model based Arabic word
segmentation. In Proceedings of the ACL&apos;03, pages
399–406.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In ICML.
Rani Nelken and Stuart M. Shieber. 2005. Arabic
diacritization using weighted finite-state transducers.
In ACL-05 Workshop on Computational Approaches to
Semitic Languages, pages 79–86, Ann Arbor, Michigan.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Conference on
Empirical Methods in Natural Language Processing.
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495–505.
D. Vergyri and K. Kirchhoff. 2004. Automatic dia-
critization of Arabic for acoustic modeling in speech
recognition. In COLING Workshop on Arabic-script
Based Languages, Geneva, Switzerland.
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of Win-
now. Journal of Machine Learning Research, 2:615–
637.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stemming
on Arabic mention detection and coreference resolu-
tion. In Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages 63–
70, Ann Arbor, June.
</reference>
<page confidence="0.998598">
584
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753228">
<title confidence="0.999475">Maximum Entropy Based Restoration of Arabic Diacritics</title>
<author confidence="0.973575">Imed Zitouni</author>
<author confidence="0.973575">Jeffrey S Sorensen</author>
<author confidence="0.973575">Ruhi Sarikaya</author>
<affiliation confidence="0.999929">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.998956">1101 Kitchawan Rd, Yorktown Heights, NY 10598</address>
<email confidence="0.977108">sorenj,</email>
<abstract confidence="0.99136152">Short vowels and other diacritics are not part of written Arabic scripts. Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic. Script without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting. We propose in this paper a maximum entropy approach for restoring diacritics in a document. The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segmentand features. The combination of these feature types leads to a state-of-the-art diacritization model. Using a publicly available corpus (LDC&apos;s Arabic Treebank Part 3), we achieve a dierror rate of a segment error and a word error rate of In case-ending-less setting, we obtain a dierror rate of a segment error and a word error rate of</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Afify</author>
<author>S Abdou</author>
<author>J Makhoul</author>
<author>L Nguyen</author>
<author>B Xiang</author>
</authors>
<date>2004</date>
<booktitle>The BBN RT04 BN Arabic System. In RT04 Workshop,</booktitle>
<location>Palisades NY.</location>
<contexts>
<context position="2379" citStr="Afify et al., 2004" startWordPosition="364" endWordPosition="367">ginning readers and people with learning disabilities. A text without diacritics is also problematic for applications such as text-to-speech or speech-to-text, where the lack of diacritics adds another layer of ambiguity when processing the data. As an example, full vocalization of text is required for text-to-speech applications, where the mapping from graphemes to phonemes is simple compared to languages such as English and French; where there is, in most cases, one-to-one relationship. Also, using data with diacritics shows an improvement in the accuracy of speech-recognition applications (Afify et al., 2004). Currently, text-tospeech, speech-to-text, and other applications use data where diacritics are placed manually, which is a tedious and time consuming excercise. A diacritization system that restores the diacritics of scripts, i.e. supply the full diacritical markings, would be of interest to these applications. It also would greatly benefit nonnative speakers, sufferers of dyslexia and could assist in restoring diacritics of children’s and poetry books, a task that is currently done manually. We propose in this paper a statistical approach that restores diacritics in a text document. The pro</context>
<context position="29074" citStr="Afify et al., 2004" startWordPosition="4867" endWordPosition="4870">arked with “True shadda&amp;quot; represent results on documents containing the original consonant doubling “shadda&amp;quot; while columns marked with “Predicted shadda&amp;quot; represent results where the system restored all diacritics including shadda. 3.6%). Similar to the results reported in Table 2, we show that the performance of the system are similar whether the document contains the original shadda or not. A system like this trained on non case-ending documents can be of interest to applications such as speech recognition, where the last state of a word HMM model can be defined to absorb all possible vowels (Afify et al., 2004). 7 Comparison to other approaches As stated in section 3, the most recent and advanced approach to diacritic restoration is the one presented in (Nelken and Shieber, 2005): they showed a DER of 12.79% and a WER of 23.61% on Arabic Treebank corpus using finite state transducers (FST) with a Katz language modeling (LM) as described in (Chen and Goodman, 1999). Because they didn’t describe how they split their corpus into training/test sets, we were not able to use the same data for comparison purpose. In this section, we want essentially to duplicate the aforementioned FST result for comparison</context>
</contexts>
<marker>Afify, Abdou, Makhoul, Nguyen, Xiang, 2004</marker>
<rawString>M. Afify, S. Abdou, J. Makhoul, L. Nguyen, and B. Xiang. 2004. The BBN RT04 BN Arabic System. In RT04 Workshop, Palisades NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11396" citStr="Berger et al., 1996" startWordPosition="1839" endWordPosition="1842">over time, they are still limited in terms of coverage and accuracy. In the approach we present in this paper, we propose to restore the most comprehensive list of the diacritics that are used in any Arabic text. Our method differs from the previous approaches in the way the diacritization problem is formulated and because multiple information sources are integrated. We view the diacritic restoration problem as sequence classification, where given a sequence of characters our goal is to assign diacritics to each character. Our appoach is based on Maximum Entropy (MaxEnt henceforth) technique (Berger et al., 1996). MaxEnt can be used for sequence classification, by converting the activation scores into probabilities (through the soft-max function, for instance) and using the standard dynamic programming search algorithm (also known as Viterbi search). We find in the literature several other approaches of sequence classification such as (McCallum et al., 2000) and (Lafferty et al., 2001). The conditional random fields method presented in (Lafferty et al., 2001) is essentially a MaxEnt model over the entire sequence: it differs from the Maxent in that it models the sequence information, whereas the Maxen</context>
<context position="14151" citStr="Berger et al., 1996" startWordPosition="2296" endWordPosition="2299">&apos; be a feature space. Each example x E X has associated a vector of binary features f (x) = (fl (x) , ... , f,,,, (x)). In a supervised framework, like the one we are considering here, we have access to a set of training examples together with their classifications: {(xi, yi) , . . . , (xk, yk)}. &apos;This is not meant to be an in-depth introduction to the method, but a brief overview to familiarize the reader with them. 579 The MaxEnt algorithm associates a set of weights (αij)i=1...n j=1...m with the features, which are estimated during the training phase to maximize the likelihood of the data (Berger et al., 1996). Given these weights, the model computes the probability distribution over labels for a particular example x as follows: 1 P (y |x) = 11 f4 (x) Z(x) 11 Z(x)j=1 aZj ,=i j where Z(X) is a normalization factor. To estimate the optimal αj values, we train our MaxEnt model using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002). While the MaxEnt method can nicely integrate multiple feature types seamlessly, in certain cases it is known to overestimate its confidence in especially low-frequency features. To overcome this problem, we use the regularization me</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Buckwalter Arabic morphological analyzer version 1.0.</title>
<date>2002</date>
<booktitle>Linguistic Data Consortium, LDC2002L49 and ISBN</booktitle>
<tech>Technical report,</tech>
<pages>1--58563</pages>
<contexts>
<context position="9709" citStr="Buckwalter, 2002" startWordPosition="1575" endWordPosition="1577"> matching sentence. If there is a matching sentence, the whole utterance is used. Otherwise they search for matching phrases, then words to restore diacritics. If there is no match at all, character n-gram models are used to diacritize each word in the utterance. In (Vergyri and Kirchhoff, 2004), diacritics in conversational Arabic are restored by combining morphological and contextual information with an acoustic signal. Diacritization is treated as an unsupervised tagging problem where each word is tagged as one of the many possible forms provided by the Buckwalter&apos;s morphological analyzer (Buckwalter, 2002). The Expectation Maximization (EM) algorithm is used to learn the tag sequences. Y. Gal in (Gal, 2002) used a HMM-based diacritization approach. This method is a white-space delimited word based approach that restores only vowels (a subset of all diacritics). Most recently, a weighted finite state machine based algorithm is proposed (Nelken and Shieber, 2005). This method employs characters and larger morphological units in addition to words. Among all the previous studies this one is more sophisticated in terms of integrating multiple information sources and formulating the problem as a sear</context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>T. Buckwalter. 2002. Buckwalter Arabic morphological analyzer version 1.0. Technical report, Linguistic Data Consortium, LDC2002L49 and ISBN 1-58563-257-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. computer speech and language.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<issue>13</issue>
<contexts>
<context position="29434" citStr="Chen and Goodman, 1999" startWordPosition="4930" endWordPosition="4933"> contains the original shadda or not. A system like this trained on non case-ending documents can be of interest to applications such as speech recognition, where the last state of a word HMM model can be defined to absorb all possible vowels (Afify et al., 2004). 7 Comparison to other approaches As stated in section 3, the most recent and advanced approach to diacritic restoration is the one presented in (Nelken and Shieber, 2005): they showed a DER of 12.79% and a WER of 23.61% on Arabic Treebank corpus using finite state transducers (FST) with a Katz language modeling (LM) as described in (Chen and Goodman, 1999). Because they didn’t describe how they split their corpus into training/test sets, we were not able to use the same data for comparison purpose. In this section, we want essentially to duplicate the aforementioned FST result for comparison using the identical training and testing set we use for our experiments. We also propose some new variations on the finite state machine modeling technique which improve performance considerably. The algorithm for FST based vowel restoration could not be simpler: between every pair of characters we insert diacritics if doing so improves the likelihood of th</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. computer speech and language. Computer Speech and Language, 4(13):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for me models.</title>
<date>2000</date>
<journal>IEEE Trans. on Speech and Audio Processing.</journal>
<contexts>
<context position="14830" citStr="Chen and Rosenfeld, 2000" startWordPosition="2408" endWordPosition="2411">ity distribution over labels for a particular example x as follows: 1 P (y |x) = 11 f4 (x) Z(x) 11 Z(x)j=1 aZj ,=i j where Z(X) is a normalization factor. To estimate the optimal αj values, we train our MaxEnt model using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002). While the MaxEnt method can nicely integrate multiple feature types seamlessly, in certain cases it is known to overestimate its confidence in especially low-frequency features. To overcome this problem, we use the regularization method based on adding Gaussian priors as described in (Chen and Rosenfeld, 2000). After computing the class probability distribution, the chosen diacritic is the one with the most aposteriori probability. The decoding algorithm, described in section 4.2, performs sequence classification, through dynamic programming. 4.2 Search to Restore Diacritics We are interested in finding the diacritics of all characters in a script or a sentence. These diacritics have strong interdependencies which cannot be properly modeled if the classification is performed independently for each character. We view this problem as sequence classification, as contrasted with an example-based classi</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for me models. IEEE Trans. on Speech and Audio Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debili</author>
<author>H Achour</author>
<author>E Souissi</author>
</authors>
<title>De l&apos;etiquetage grammatical a` la voyellation automatique de l&apos;arabe.</title>
<date>2002</date>
<booktitle>Correspondances de l&apos;Institut de Recherche sur le Maghreb Contemporain 17.</booktitle>
<tech>Technical report,</tech>
<contexts>
<context position="7414" citStr="Debili et al., 2002" startWordPosition="1202" endWordPosition="1205">Arabic sentence transcribed with and without diacritics. In modern Arabic, writing scripts without diacritics is the most natural way. Because many words with different vowel patterns may appear identical in a diacritic-less setting, considerable ambiguity exists at the word level. The word  , for example, has 21 possible forms that have valid interpretations when adding diacritics (Kirchhoff and Vergyri, 2005). It may have  the interpretation of the verb “to write” in   (pronounced /kataba/). Also, it can be interpreted as “books” in the noun form   tubun/). A study made by (Debili et al., 2002) shows that there is an average of 11.6 possible diacritizations for every non-diacritized word when analyzing a text of 23,000 script forms.                Figure 1: The same Arabic sentence without (upper row) and with (lower row) diacritics. The English translation is “the president wrote the document.&amp;quot; Arabic diacritic restoration is a non-trivial task as expressed in (El-Imam, 2003). Native speakers of Arabic are able, in most cases, to accurately vocalize words in text based on their context, the speaker’s knowledge of the gramm</context>
</contexts>
<marker>Debili, Achour, Souissi, 2002</marker>
<rawString>F. Debili, H. Achour, and E. Souissi. 2002. De l&apos;etiquetage grammatical a` la voyellation automatique de l&apos;arabe. Technical report, Correspondances de l&apos;Institut de Recherche sur le Maghreb Contemporain 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y El-Imam</author>
</authors>
<title>Phonetization of arabic: rules and algorithms.</title>
<date>2003</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<pages>373</pages>
<contexts>
<context position="7864" citStr="El-Imam, 2003" startWordPosition="1280" endWordPosition="1281"> the verb “to write” in   (pronounced /kataba/). Also, it can be interpreted as “books” in the noun form   tubun/). A study made by (Debili et al., 2002) shows that there is an average of 11.6 possible diacritizations for every non-diacritized word when analyzing a text of 23,000 script forms.                Figure 1: The same Arabic sentence without (upper row) and with (lower row) diacritics. The English translation is “the president wrote the document.&amp;quot; Arabic diacritic restoration is a non-trivial task as expressed in (El-Imam, 2003). Native speakers of Arabic are able, in most cases, to accurately vocalize words in text based on their context, the speaker’s knowledge of the grammar, and the lexicon of Arabic. Our goal is to convert knowledge used by native speakers into features and incorporate them into a maximum entropy model. We assume that the input text does not contain any diacritics. 3 Previous Work Diacritic restoration has been receiving increasing attention and has been the focus of several studies. In (El-Sadany and Hashish, 1988), a rule based method that uses morphological analyzer for   (pronounced /ku5</context>
</contexts>
<marker>El-Imam, 2003</marker>
<rawString>Y. El-Imam. 2003. Phonetization of arabic: rules and algorithms. Computer Speech and Language, 18:339– 373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T El-Sadany</author>
<author>M Hashish</author>
</authors>
<title>Semi-automatic vowelization of Arabic verbs.</title>
<date>1988</date>
<booktitle>In 10th NC Conference,</booktitle>
<location>Jeddah, Saudi Arabia.</location>
<contexts>
<context position="8383" citStr="El-Sadany and Hashish, 1988" startWordPosition="1366" endWordPosition="1369">ident wrote the document.&amp;quot; Arabic diacritic restoration is a non-trivial task as expressed in (El-Imam, 2003). Native speakers of Arabic are able, in most cases, to accurately vocalize words in text based on their context, the speaker’s knowledge of the grammar, and the lexicon of Arabic. Our goal is to convert knowledge used by native speakers into features and incorporate them into a maximum entropy model. We assume that the input text does not contain any diacritics. 3 Previous Work Diacritic restoration has been receiving increasing attention and has been the focus of several studies. In (El-Sadany and Hashish, 1988), a rule based method that uses morphological analyzer for   (pronounced /ku578 vowelization was proposed. Another, rule-based grapheme to sound conversion approach was appeared in 2003 by Y. El-Imam (El-Imam, 2003). The main drawbacks of these rule based methods is that it is difficult to maintain the rules up-to-date and extend them to other Arabic dialects. Also, new rules are required due to the changing nature of any “living” language. More recently, there have been several new studies that use alternative approaches for the diacritization problem. In (Emam and Fisher, 2004) an exampl</context>
</contexts>
<marker>El-Sadany, Hashish, 1988</marker>
<rawString>T. El-Sadany and M. Hashish. 1988. Semi-automatic vowelization of Arabic verbs. In 10th NC Conference, Jeddah, Saudi Arabia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Emam</author>
<author>V Fisher</author>
</authors>
<title>A hierarchical approach for the statistical vowelization of Arabic text.</title>
<date>2004</date>
<tech>Technical report, IBM patent</tech>
<note>filed, DE9-2004-0006, US patent application US2005/0192809 A1.</note>
<contexts>
<context position="8973" citStr="Emam and Fisher, 2004" startWordPosition="1462" endWordPosition="1465">In (El-Sadany and Hashish, 1988), a rule based method that uses morphological analyzer for   (pronounced /ku578 vowelization was proposed. Another, rule-based grapheme to sound conversion approach was appeared in 2003 by Y. El-Imam (El-Imam, 2003). The main drawbacks of these rule based methods is that it is difficult to maintain the rules up-to-date and extend them to other Arabic dialects. Also, new rules are required due to the changing nature of any “living” language. More recently, there have been several new studies that use alternative approaches for the diacritization problem. In (Emam and Fisher, 2004) an example based hierarchical top-down approach is proposed. First, the training data is searched hierarchically for a matching sentence. If there is a matching sentence, the whole utterance is used. Otherwise they search for matching phrases, then words to restore diacritics. If there is no match at all, character n-gram models are used to diacritize each word in the utterance. In (Vergyri and Kirchhoff, 2004), diacritics in conversational Arabic are restored by combining morphological and contextual information with an acoustic signal. Diacritization is treated as an unsupervised tagging pr</context>
</contexts>
<marker>Emam, Fisher, 2004</marker>
<rawString>O. Emam and V. Fisher. 2004. A hierarchical approach for the statistical vowelization of Arabic text. Technical report, IBM patent filed, DE9-2004-0006, US patent application US2005/0192809 A1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12837" citStr="Florian et al., 2004" startWordPosition="2065" endWordPosition="2068">ing features, and define the probability of state sequences given observation sequences. We report in section 7 a comparative study between our approach and the most competitive diacritic restoration method that uses finite state machine algorithm (Nelken and Shieber, 2005). The MaxEnt framework was successfully used to combine a diverse collection of information sources and yielded a highly competitive model that achieves a 5.1% DER. 4 Automatic Diacritization The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florian et al., 2004), has been shown to depend on integrating many sources of information. Given the stated focus of integrating many feature types, we selected the MaxEnt classifier. MaxEnt has the ability to integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. 4.1 Maximum Entropy Classifiers We formulate the task of restoring diacritics as a classification problem, where we assign to each character in the text a label (i.e., diacritic). Before formally describing the method&apos;, we introduce some notations: let Y = {yl, ...</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of HLT-NAACL 2004, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gal</author>
</authors>
<title>An HMM approach to vowel restoration in Arabic and Hebrew.</title>
<date>2002</date>
<booktitle>In ACL-02 Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="9812" citStr="Gal, 2002" startWordPosition="1593" endWordPosition="1594">tching phrases, then words to restore diacritics. If there is no match at all, character n-gram models are used to diacritize each word in the utterance. In (Vergyri and Kirchhoff, 2004), diacritics in conversational Arabic are restored by combining morphological and contextual information with an acoustic signal. Diacritization is treated as an unsupervised tagging problem where each word is tagged as one of the many possible forms provided by the Buckwalter&apos;s morphological analyzer (Buckwalter, 2002). The Expectation Maximization (EM) algorithm is used to learn the tag sequences. Y. Gal in (Gal, 2002) used a HMM-based diacritization approach. This method is a white-space delimited word based approach that restores only vowels (a subset of all diacritics). Most recently, a weighted finite state machine based algorithm is proposed (Nelken and Shieber, 2005). This method employs characters and larger morphological units in addition to words. Among all the previous studies this one is more sophisticated in terms of integrating multiple information sources and formulating the problem as a search task within a unified framework. This approach also shows competitive results in terms of accuracy w</context>
</contexts>
<marker>Gal, 2002</marker>
<rawString>Y. Gal. 2002. An HMM approach to vowel restoration in Arabic and Hebrew. In ACL-02 Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Sequential conditional generalized iterative scaling.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL&apos;02.</booktitle>
<contexts>
<context position="14517" citStr="Goodman, 2002" startWordPosition="2362" endWordPosition="2364">ef overview to familiarize the reader with them. 579 The MaxEnt algorithm associates a set of weights (αij)i=1...n j=1...m with the features, which are estimated during the training phase to maximize the likelihood of the data (Berger et al., 1996). Given these weights, the model computes the probability distribution over labels for a particular example x as follows: 1 P (y |x) = 11 f4 (x) Z(x) 11 Z(x)j=1 aZj ,=i j where Z(X) is a normalization factor. To estimate the optimal αj values, we train our MaxEnt model using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002). While the MaxEnt method can nicely integrate multiple feature types seamlessly, in certain cases it is known to overestimate its confidence in especially low-frequency features. To overcome this problem, we use the regularization method based on adding Gaussian priors as described in (Chen and Rosenfeld, 2000). After computing the class probability distribution, the chosen diacritic is the one with the most aposteriori probability. The decoding algorithm, described in section 4.2, performs sequence classification, through dynamic programming. 4.2 Search to Restore Diacritics We are intereste</context>
</contexts>
<marker>Goodman, 2002</marker>
<rawString>Joshua Goodman. 2002. Sequential conditional generalized iterative scaling. In Proceedings of ACL&apos;02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>D Vergyri</author>
</authors>
<title>Cross-dialectal data sharing for acoustic modeling in Arabic speech recognition.</title>
<date>2005</date>
<journal>Speech Communication,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="7213" citStr="Kirchhoff and Vergyri, 2005" startWordPosition="1164" endWordPosition="1167"> consonant. The shadda is usually  combined with a short vowel such as in .  • sukuun: written as a small circle as in . It is used to indicate that the letter doesn’t contain vowels. Figure 1 shows an Arabic sentence transcribed with and without diacritics. In modern Arabic, writing scripts without diacritics is the most natural way. Because many words with different vowel patterns may appear identical in a diacritic-less setting, considerable ambiguity exists at the word level. The word  , for example, has 21 possible forms that have valid interpretations when adding diacritics (Kirchhoff and Vergyri, 2005). It may have  the interpretation of the verb “to write” in   (pronounced /kataba/). Also, it can be interpreted as “books” in the noun form   tubun/). A study made by (Debili et al., 2002) shows that there is an average of 11.6 possible diacritizations for every non-diacritized word when analyzing a text of 23,000 script forms.                Figure 1: The same Arabic sentence without (upper row) and with (lower row) diacritics. The English translation is “the president wrote the document.&amp;quot; Arabic diacritic restoration is</context>
</contexts>
<marker>Kirchhoff, Vergyri, 2005</marker>
<rawString>K. Kirchhoff and D. Vergyri. 2005. Cross-dialectal data sharing for acoustic modeling in Arabic speech recognition. Speech Communication, 46(1):37–51, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11776" citStr="Lafferty et al., 2001" startWordPosition="1896" endWordPosition="1899">the diacritic restoration problem as sequence classification, where given a sequence of characters our goal is to assign diacritics to each character. Our appoach is based on Maximum Entropy (MaxEnt henceforth) technique (Berger et al., 1996). MaxEnt can be used for sequence classification, by converting the activation scores into probabilities (through the soft-max function, for instance) and using the standard dynamic programming search algorithm (also known as Viterbi search). We find in the literature several other approaches of sequence classification such as (McCallum et al., 2000) and (Lafferty et al., 2001). The conditional random fields method presented in (Lafferty et al., 2001) is essentially a MaxEnt model over the entire sequence: it differs from the Maxent in that it models the sequence information, whereas the Maxent makes a decision for each state independently of the other states. The approach presented in (McCallum et al., 2000) combines Maxent with Hidden Markov models to allow observations to be presented as arbitrary overlapping features, and define the probability of state sequences given observation sequences. We report in section 7 a comparative study between our approach and the</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-S Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language model based Arabic word segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL&apos;03,</booktitle>
<pages>399--406</pages>
<contexts>
<context position="20909" citStr="Lee et al., 2003" startWordPosition="3506" endWordPosition="3509">x will be called a segment in this paper. Segments are often the subject of analysis when processing Arabic (Zitouni et al., 2005). Syntactic information such as POS or parse information is usually computed on segments rather than words. As an example, the Arabic white-space delimited word ��&amp;quot;����l&amp;quot;� s contains a verb���ls, a &amp;quot; third-person feminine singular subject-marker u (she), and a pronoun suffix �&amp;A (them); it is also a complete sentence meaning “she met them.” To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al., 2003). The model obtains an accuracy of about 98%. In order to simulate real applications, we only use segments generated by the model rather than true segments. In the diacritization system, we include the current segment ai and its word segment context in a window of 5 (forward and backward trigram): {ai−2, . . . , ai+2}. We specify if the character of analysis is at the beginning or at the end of a segment. We also add joint information with lexical features. • POS Features : we attach to the segment ai of the current character, its POS: POS(ai). This is combined with joint features that include</context>
<context position="31766" citStr="Lee et al., 2003" startWordPosition="5345" endWordPosition="5348">s of DER. The SER reported on Table 2 and Table 3 are based on the Arabic segmentation system we use in the MaxEnt approach. Since, the FST model doesn’t use such a system, we found inappropriate to report SER in this section. n-gram size Katz LM Kneser -Ney LM WER DER WER DER 3 63 31 55 28 4 54 25 38 19 5 51 21 28 13 6 44 18 24 11 7 39 16 23 11 8 37 15 23 10 Table 4: Error Rate in % for n-gram diacritic restoration using FST. We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al., 2003). Table 5 gives the performance of the extended FST model where Kneser-Ney LM is used, since it produces better results. This should be a much more difficult task, as there are more than twice as many possible insertions. However, the choice of diacritics is related to and dependent upon the choice of segmentation. Thus, we demonstrate that a richer internal representation produces a more powerful model. 8 Conclusion We presented in this paper a statistical model for Arabic diacritic restoration. The approach we propose is based on the Maximum entropy framework, which gives the system the abil</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan. 2003. Language model based Arabic word segmentation. In Proceedings of the ACL&apos;03, pages 399–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="11748" citStr="McCallum et al., 2000" startWordPosition="1890" endWordPosition="1894">ces are integrated. We view the diacritic restoration problem as sequence classification, where given a sequence of characters our goal is to assign diacritics to each character. Our appoach is based on Maximum Entropy (MaxEnt henceforth) technique (Berger et al., 1996). MaxEnt can be used for sequence classification, by converting the activation scores into probabilities (through the soft-max function, for instance) and using the standard dynamic programming search algorithm (also known as Viterbi search). We find in the literature several other approaches of sequence classification such as (McCallum et al., 2000) and (Lafferty et al., 2001). The conditional random fields method presented in (Lafferty et al., 2001) is essentially a MaxEnt model over the entire sequence: it differs from the Maxent in that it models the sequence information, whereas the Maxent makes a decision for each state independently of the other states. The approach presented in (McCallum et al., 2000) combines Maxent with Hidden Markov models to allow observations to be presented as arbitrary overlapping features, and define the probability of state sequences given observation sequences. We report in section 7 a comparative study </context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Arabic diacritization using weighted finite-state transducers.</title>
<date>2005</date>
<booktitle>In ACL-05 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>79--86</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="3999" citStr="Nelken and Shieber, 2005" startWordPosition="619" endWordPosition="622">vious related works. Section 4 presents our diacritization model; we explain the training and decoding process as well as the different feature categories employed to restore the diacritics. Section 5 describes a clearly defined and replicable split of the LDC&apos;s Arabic Treebank Part 3 corpus, used to built and evaluate the system, so that the reproduction of the results and future comparison can accurately be established. Section 6 presents the experimental results. Section 7 reports a comparison of our approach to the finite state machine modeling technique that showed promissing results in (Nelken and Shieber, 2005). Finally, section 8 concludes the paper and discusses future directions. 2 Arabic Diacritics The Arabic alphabet consists of 28 letters that can be extended to a set of 90 by additional shapes, marks, and vowels (Tayli and Al-Salamah, 1990). The 28 letters represent the consonants and long 577 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 577–584, Sydney, July 2006. c�2006 Association for Computational Linguistics vowels such as ,  (both pronounced as /a:/),  (pronounced as /i:/), and  (pronounced as /u:/). L</context>
<context position="10071" citStr="Nelken and Shieber, 2005" startWordPosition="1630" endWordPosition="1633">ining morphological and contextual information with an acoustic signal. Diacritization is treated as an unsupervised tagging problem where each word is tagged as one of the many possible forms provided by the Buckwalter&apos;s morphological analyzer (Buckwalter, 2002). The Expectation Maximization (EM) algorithm is used to learn the tag sequences. Y. Gal in (Gal, 2002) used a HMM-based diacritization approach. This method is a white-space delimited word based approach that restores only vowels (a subset of all diacritics). Most recently, a weighted finite state machine based algorithm is proposed (Nelken and Shieber, 2005). This method employs characters and larger morphological units in addition to words. Among all the previous studies this one is more sophisticated in terms of integrating multiple information sources and formulating the problem as a search task within a unified framework. This approach also shows competitive results in terms of accuracy when compared to previous studies. In their algorithm, a character based generative diacritization scheme is enabled only for words that do not occur in the training data. It is not clearly stated in the paper whether their method predict the diacritics shedda</context>
<context position="12490" citStr="Nelken and Shieber, 2005" startWordPosition="2011" endWordPosition="2014">y a MaxEnt model over the entire sequence: it differs from the Maxent in that it models the sequence information, whereas the Maxent makes a decision for each state independently of the other states. The approach presented in (McCallum et al., 2000) combines Maxent with Hidden Markov models to allow observations to be presented as arbitrary overlapping features, and define the probability of state sequences given observation sequences. We report in section 7 a comparative study between our approach and the most competitive diacritic restoration method that uses finite state machine algorithm (Nelken and Shieber, 2005). The MaxEnt framework was successfully used to combine a diverse collection of information sources and yielded a highly competitive model that achieves a 5.1% DER. 4 Automatic Diacritization The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florian et al., 2004), has been shown to depend on integrating many sources of information. Given the stated focus of integrating many feature types, we selected the MaxEnt classifier. MaxEnt has the ability to integrate arbitrary types of information and make a classificat</context>
<context position="26238" citStr="Nelken and Shieber, 2005" startWordPosition="4386" endWordPosition="4389"> shadda Predicted shadda WER SER DER WER SER DER Lexical features 24.8 12.6 7.9 25.1 13.0 8.2 Lexical + segment-based features 18.2 9.0 5.5 18.8 9.4 5.8 Lexical + segment-based + POS features 17.3 8.5 5.1 18.0 8.9 5.5 Table 2: The impact of features on the diacritization system performance. The columns marked with “True shadda&amp;quot; represent results on documents containing the original consonant doubling “shadda&amp;quot; while columns marked with “Predicted shadda&amp;quot; represent results where the system restored all diacritics including shadda. state-of-the-art system evaluated on Arabic Treebank Part 2: in (Nelken and Shieber, 2005) a DER of 12.79% and a WER of 23.61% are reported. The system they described in (Nelken and Shieber, 2005) uses lexical, segment-based, and morphological information. Table 2 also shows that, when segment-based information is added to our system, a significant improvement is achieved: 25% for WER (18.8 vs. 25.1), 38% for SER (9.4 vs. 13.0), and 41% for DER (5.8 vs. 8.2). Similar behavior is observed when the documents contain the original shadda. POS features are also helpful in improving the performance of the system. They improved the WER by 4% (18.0 vs. 18.8), SER by 5% (8.9 vs. 9.4), and D</context>
<context position="29246" citStr="Nelken and Shieber, 2005" startWordPosition="4896" endWordPosition="4899">ults where the system restored all diacritics including shadda. 3.6%). Similar to the results reported in Table 2, we show that the performance of the system are similar whether the document contains the original shadda or not. A system like this trained on non case-ending documents can be of interest to applications such as speech recognition, where the last state of a word HMM model can be defined to absorb all possible vowels (Afify et al., 2004). 7 Comparison to other approaches As stated in section 3, the most recent and advanced approach to diacritic restoration is the one presented in (Nelken and Shieber, 2005): they showed a DER of 12.79% and a WER of 23.61% on Arabic Treebank corpus using finite state transducers (FST) with a Katz language modeling (LM) as described in (Chen and Goodman, 1999). Because they didn’t describe how they split their corpus into training/test sets, we were not able to use the same data for comparison purpose. In this section, we want essentially to duplicate the aforementioned FST result for comparison using the identical training and testing set we use for our experiments. We also propose some new variations on the finite state machine modeling technique which improve p</context>
<context position="30806" citStr="Nelken and Shieber, 2005" startWordPosition="5154" endWordPosition="5157"> all possible diacritical insertions. Results reported in Table 4 indicate the error rates of diacritic restoration (including shadda). We show performance using both KneserNey and Katz LMs (Chen and Goodman, 1999) with increasingly large n-grams. It is our opinion that large n-grams effectively duplicate the use of a lexicon. It is unfortunate but true that, even for a rich resource like the Arabic Treebank, the choice of modeling heuristic and the effects of small sample size are considerable. Using the finite state machine modeling technique, we obtain similar results to those reported in (Nelken and Shieber, 2005): a WER of 23% and a DER of 15%. Better performance is reached with the use of Kneser-Ney LM. These results still under-perform those obtained by MaxEnt approach presented in Table 2. When all sources of information are included, the MaxEnt technique outperforms the FST model by 21% (22% vs. 18%) in terms of WER and 39% (9% vs. 5.5%) in terms of DER. The SER reported on Table 2 and Table 3 are based on the Arabic segmentation system we use in the MaxEnt approach. Since, the FST model doesn’t use such a system, we found inappropriate to report SER in this section. n-gram size Katz LM Kneser -Ne</context>
</contexts>
<marker>Nelken, Shieber, 2005</marker>
<rawString>Rani Nelken and Stuart M. Shieber. 2005. Arabic diacritization using weighted finite-state transducers. In ACL-05 Workshop on Computational Approaches to Semitic Languages, pages 79–86, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<booktitle>In Conference on Empirical Methods in Natural Language Processing. M. Tayli</booktitle>
<volume>33</volume>
<issue>5</issue>
<contexts>
<context position="21664" citStr="Ratnaparkhi, 1996" startWordPosition="3642" endWordPosition="3643">than true segments. In the diacritization system, we include the current segment ai and its word segment context in a window of 5 (forward and backward trigram): {ai−2, . . . , ai+2}. We specify if the character of analysis is at the beginning or at the end of a segment. We also add joint information with lexical features. • POS Features : we attach to the segment ai of the current character, its POS: POS(ai). This is combined with joint features that include the lexical and segment-based information. We use a statistical POS tagging system built on Arabic Treebank data with MaxEnt framework (Ratnaparkhi, 1996). The model has an accuracy of about 96%. We did not want to use the true POS tags because we would not have access to such information in real applications. 5 Data The diacritization system we present here is trained and evaluated on the LDC&apos;s Arabic Treebank of diacritized news stories – Part 3 v1.0: catalog number LDC2004T11 and ISBN 1-58563-298-8. The corpus includes complete vocalization (including case-endings). We introduce here a clearly defined and replicable split of the corpus, so that the reproduction of the results or future investigations can accurately and correctly be establish</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Processing. M. Tayli and A. Al-Salamah. 1990. Building bilingual microcomputer systems. Communications of the ACM, 33(5):495–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vergyri</author>
<author>K Kirchhoff</author>
</authors>
<title>Automatic diacritization of Arabic for acoustic modeling in speech recognition.</title>
<date>2004</date>
<booktitle>In COLING Workshop on Arabic-script Based Languages,</booktitle>
<location>Geneva,</location>
<contexts>
<context position="9388" citStr="Vergyri and Kirchhoff, 2004" startWordPosition="1528" endWordPosition="1531">rules are required due to the changing nature of any “living” language. More recently, there have been several new studies that use alternative approaches for the diacritization problem. In (Emam and Fisher, 2004) an example based hierarchical top-down approach is proposed. First, the training data is searched hierarchically for a matching sentence. If there is a matching sentence, the whole utterance is used. Otherwise they search for matching phrases, then words to restore diacritics. If there is no match at all, character n-gram models are used to diacritize each word in the utterance. In (Vergyri and Kirchhoff, 2004), diacritics in conversational Arabic are restored by combining morphological and contextual information with an acoustic signal. Diacritization is treated as an unsupervised tagging problem where each word is tagged as one of the many possible forms provided by the Buckwalter&apos;s morphological analyzer (Buckwalter, 2002). The Expectation Maximization (EM) algorithm is used to learn the tag sequences. Y. Gal in (Gal, 2002) used a HMM-based diacritization approach. This method is a white-space delimited word based approach that restores only vowels (a subset of all diacritics). Most recently, a w</context>
</contexts>
<marker>Vergyri, Kirchhoff, 2004</marker>
<rawString>D. Vergyri and K. Kirchhoff. 2004. Automatic diacritization of Arabic for acoustic modeling in speech recognition. In COLING Workshop on Arabic-script Based Languages, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Fred Damerau</author>
<author>David E Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2</volume>
<pages>637</pages>
<contexts>
<context position="12785" citStr="Zhang et al., 2002" startWordPosition="2057" endWordPosition="2060">observations to be presented as arbitrary overlapping features, and define the probability of state sequences given observation sequences. We report in section 7 a comparative study between our approach and the most competitive diacritic restoration method that uses finite state machine algorithm (Nelken and Shieber, 2005). The MaxEnt framework was successfully used to combine a diverse collection of information sources and yielded a highly competitive model that achieves a 5.1% DER. 4 Automatic Diacritization The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florian et al., 2004), has been shown to depend on integrating many sources of information. Given the stated focus of integrating many feature types, we selected the MaxEnt classifier. MaxEnt has the ability to integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. 4.1 Maximum Entropy Classifiers We formulate the task of restoring diacritics as a classification problem, where we assign to each character in the text a label (i.e., diacritic). Before formally describing the me</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>Tong Zhang, Fred Damerau, and David E. Johnson. 2002. Text chunking based on a generalization of Winnow. Journal of Machine Learning Research, 2:615– 637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Jeff Sorensen</author>
<author>Xiaoqiang Luo</author>
<author>Radu Florian</author>
</authors>
<title>The impact of morphological stemming on Arabic mention detection and coreference resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>63--70</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="20422" citStr="Zitouni et al., 2005" startWordPosition="3427" endWordPosition="3430">following it in a window of 7: {xi−3, . . . , xi+3}. We use the current word wi and its word context in a window of 5 (forward and backward trigram): {wi−2, . . . , wi+2}. We specify if the character of analysis is at the beginning or at the end of a word. We also add joint features between the above source of information. • Segment-Based Features : Arabic blankdelimited words are composed of zero or more prefixes, followed by a stem and zero or more suffixes. Each prefix, stem or suffix will be called a segment in this paper. Segments are often the subject of analysis when processing Arabic (Zitouni et al., 2005). Syntactic information such as POS or parse information is usually computed on segments rather than words. As an example, the Arabic white-space delimited word ��&amp;quot;����l&amp;quot;� s contains a verb���ls, a &amp;quot; third-person feminine singular subject-marker u (she), and a pronoun suffix �&amp;A (them); it is also a complete sentence meaning “she met them.” To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al., 2003). The model obtains an accuracy of about 98%. In order to simulate real applications, we only use segments genera</context>
<context position="23874" citStr="Zitouni et al., 2005" startWordPosition="4001" endWordPosition="4005">shed. 6 Experiments Experiments are reported in terms of word error rate (WER), segment error rate (SER), and diacritization error rate (DER). The DER is the proportion of incorrectly restored diacritics. The WER is the percentage of incorrectly diacritized white-space delimited words: in order to be counted as incorrect, at least one character in the word must have a diacritization error. The SER is similar to WER but indicates the proportion of incorrectly diacritized segments. A segment can be a prefix, a stem, or a suffix. Segments are often the subject of analysis when processing Arabic (Zitouni et al., 2005). Syntactic information such as POS or parse information is based on segments rather than words. Consequently, it is important to know the SER in cases where the diacritization system may be used to help disambiguate syntactic information. Several modern Arabic scripts contains the consonant doubling “shadda&amp;quot;; it is common for native speakers to write without diacritics except the shadda. In this case the role of the diacritization system will be to restore the short vowels, doubled case ending, and the vowel absence “sukuun&amp;quot;. We run two batches of experiments: a first experiment where documen</context>
</contexts>
<marker>Zitouni, Sorensen, Luo, Florian, 2005</marker>
<rawString>Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu Florian. 2005. The impact of morphological stemming on Arabic mention detection and coreference resolution. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63– 70, Ann Arbor, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>