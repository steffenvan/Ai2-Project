<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.5014484">
Text Summarization Model
based on Maximum Coverage Problem and its Variant
Hiroya Takamura and Manabu Okumura
Precision and Intelligence Laboratory, Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503
</note>
<email confidence="0.996613">
takamura@pi.titech.ac.jp oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.994746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999402666666667">
We discuss text summarization in terms of
maximum coverage problem and its vari-
ant. We explore some decoding algorithms
including the ones never used in this sum-
marization formulation, such as a greedy
algorithm with performance guarantee, a
randomized algorithm, and a branch-and-
bound method. On the basis of the results
of comparative experiments, we also aug-
ment the summarization model so that it
takes into account the relevance to the doc-
ument cluster. Through experiments, we
showed that the augmented model is su-
perior to the best-performing method of
DUC’04 on ROUGE-1 without stopwords.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978563636364">
Automatic text summarization is one of the tasks
that have long been studied in natural language
processing. This task is to create a summary, or
a short and concise document that describes the
content of a given set of documents (Mani, 2001).
One well-known approach to text summariza-
tion is the extractive method, which selects some
linguistic units (e.g., sentences) from given doc-
uments in order to generate a summary. The ex-
tractive method has an advantage that the gram-
maticality is guaranteed at least at the level of the
linguistic units. Since the actual generation of
linguistic expressions has not achieved the level
of the practical use, we focus on the extractive
method in this paper, especially the method based
on the sentence extraction. Most of the extractive
summarization methods rely on sequentially solv-
ing binary classification problems of determining
whether each sentence should be selected or not.
In such sequential methods, however, the view-
point regarding whether the summary is good as
a whole, is not taken into consideration, although
a summary conveys information as a whole.
We represent text summarization as an opti-
mization problem and attempt to globally solve
the problem. In particular, we represent text sum-
marization as a maximum coverage problem with
knapsack constraint (MCKP). One of the advan-
tages of this representation is that MCKP can di-
rectly model whether each concept in the given
documents is covered by the summary or not,
and can dispense with rather counter-intuitive ap-
proaches such as giving penalty to each pair of two
similar sentences. By formally apprehending the
target problem, we can use a lot of knowledge and
techniques developed in the combinatorial mathe-
matics, and also analyse results more precisely. In
fact, on the basis of the results of the experiments,
we augmented the summarization model.
The contributions of this paper are as follows.
We are not the first to represent text summarization
as MCKP. However, no researchers have exploited
the decoding algorithms for solving MCKP in
the summarization task. We conduct compre-
hensive comparative experiments of those algo-
rithms. Specifically, we test the greedy algorithm,
the greedy algorithm with performance guarantee,
the stack decoding, the linear relaxation problem
with randomized decoding, and the branch-and-
bound method. On the basis of the experimental
results, we then propose an augmented model that
takes into account the relevance to the document
cluster. We empirically show that the augmented
model is superior to the best-performing method
of DUC’04 on ROUGE-1 without stopwords.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999428125">
Carbonell and Goldstein (2000) used sequential
sentence selection in combination with maximal
marginal relevance (MMR), which gives penalty
to sentences that are similar to the already se-
lected sentences. Schiffman et al.’s method (2002)
is also based on sequential sentence selection.
Radev et al. (2004), in their method MEAD, used
a clustering technique to find the centroid, that
</bodyText>
<note confidence="0.9229965">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 781–789,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.99657">
781
</page>
<bodyText confidence="0.999946206896552">
is, the words with high relevance to the topic
of the document cluster. They used the centroid
to rank sentences, together with the MMR-like
redundancy score. Both relevance and redun-
dancy are taken into consideration, but no global
viewpoint is given. In CLASSY, which is the
best-performing method in DUC’04, Conroy et
al. (2004) scored sentences with the sum of tf-idf
scores of words. They also incorporated sentence
compression based on syntactic or heuristic rules.
McDonald (2007) formulated text summariza-
tion as a knapsack problem and obtained the
global solution and its approximate solutions. Its
relation to our method will be discussed in Sec-
tion 6.1. Filatova and Hatzivassiloglou (2004) first
formulated text summarization as MCKP. Their
decoding method is a greedy one and will be em-
pirically compared with other decoding methods
in this paper. Yih et al. (2007) used a slightly-
modified stack decoding. The optimization prob-
lem they solved was the MCKP with the last sen-
tence truncation. Their stack decoding is one of
the decoding methods discussed in this paper. Ye
et al. (2007) is another example of coverage-based
methods. Shen et al. (2007) regarded summariza-
tion as a sequential labelling task and solved it
with Conditional Random Fields. Although the
model is globally optimized in terms of likelihood,
the coverage of concepts is not taken into account.
</bodyText>
<sectionHeader confidence="0.916176" genericHeader="method">
3 Modeling text summarization
</sectionHeader>
<bodyText confidence="0.999971483333334">
In this paper, we focus on the extractive summa-
rization, which generates a summary by select-
ing linguistic units (e.g., sentences) in given doc-
uments. There are two types of summarization
tasks: single-document summarization and multi-
document summarization. While single-document
summarization is to generate a summary from a
single document, multi-document summarization
is to generate a summary from multiple documents
regarding one topic. Such a set of multiple docu-
ments is called a document cluster. The method
proposed in this paper is applicable to both tasks.
In both tasks, documents are split into several lin-
guistic units D = {si, · · · , s|D|} in preprocess-
ing. We will select some linguistic units from D to
generate a summary. Among other linguistic units
that can be used in the method, we use sentences
so that the grammaticality at the sentence level is
going to be guaranteed.
We introduce conceptual units (Filatova and
Hatzivassiloglou, 2004), which compose the
meaning of a sentence. Sentence si is represented
by a set of conceptual units {ei,, · · · , ei|si|}. For
example, the sentence “The man bought a book
and read it” could be regarded as consisting of two
conceptual units “the man bought a book” and “the
man read the book”. It is not easy, however, to
determine the appropriate granularity of concep-
tual units. A simple way would be to regard the
above sentence as consisting of four conceptual
units “man”, “book”, “buy”, and “read”. There
is some work on the definition of conceptual units.
Hovy et al. (2006) proposed to use basic elements,
which are dependency subtrees obtained by trim-
ming dependency trees. Although basic elements
were proposed for evaluation of summaries, they
can probably be used also for summary genera-
tion. However, such novel units have not proved
to be useful for summary generation. Since we fo-
cus more on algorithms and models in this paper,
we simply use words as conceptual units.
The goal of text summarization is to cover as
many conceptual units as possible using only a
small number of sentences. In other words, the
goal is to find a subset S(C D) that covers as
many conceptual units as possible. In the follow-
ing, we introduce models for that purpose. We
think of the situation that the summary length must
be at most K (cardinality constraint) and the sum-
mary length is measured by the number of words
or bytes in the summary.
Let xi denote a variable which is 1 if sentence
si is selected, otherwise 0, aij denote a constant
which is 1 if sentence si contains word ej, oth-
erwise 0. We regard word ej as covered when at
least one sentence containing ej is selected as part
of the summary. That is, word ej is covered if and
only if Ei aijxi &gt; 1. Now our objective is to find
the binary assignment on xi with the best coverage
such that the summary length is at most K:
</bodyText>
<equation confidence="0.985454">
|{j |E
max. i aijxi &gt; 11|
s.t. Ei cixi G K; bi, xi E {0, 11,
</equation>
<bodyText confidence="0.988440666666667">
where ci is the cost of selecting si, i.e., the number
of words or bytes in si.
For convenience, we rewrite the problem above:
</bodyText>
<equation confidence="0.57269075">
E
max. j zj
s.t. Ei cixi G K; bj, Ei aijxi &gt; zj;
bi, xi E {0,1}; bj, zj E {0, 11,
</equation>
<page confidence="0.98574">
782
</page>
<bodyText confidence="0.999910351351351">
where zj is 1 when ej is covered, 0 otherwise. No-
tice that this new problem is equivalent to the pre-
vious one.
Since not all the words are equally important,
we introduce weights wj on words ej. Then the
objective is restated as maximizing the weighted
sum ∑j wjzj such that the summary length is at
most K. This problem is called maximum cov-
erage problem with knapsack constraint (MCKP),
which is an NP-hard problem (Khuller et al.,
1999). We should note that MCKP is different
from a knapsack problem. MCKP merely has a
constraint of knapsack form. Filatova and Hatzi-
vassiloglou (2004) pointed out that text summa-
rization can be formalized by MCKP.
The performance of the method depends on how
to represent words and which words to use. We
represent words with their stems. We use only
the words that are content words (nouns, verbs,
or adjectives) and not in the stopword list used in
ROUGE (Lin, 2004).
The weights wj of words are also an impor-
tant factor of good performance. We tested two
weighting schemes proposed by Yih et al. (2007).
The first one is interpolated weights, which are in-
terpolated values of the generative word probabil-
ity in the entire document and that in the beginning
part of the document (namely, the first 100 words).
Each probability is estimated with the maximum
likelihood principle. The second one is trained
weights. These values are estimated by the logis-
tic regression trained on data instances, which are
labeled 1 if the word appears in a summary in the
training dataset, 0 otherwise. The feature set for
the logistic regression includes the frequency of
the word in the document cluster and the position
of the word instance and others.
</bodyText>
<sectionHeader confidence="0.998749" genericHeader="method">
4 Algorithms for solving MCKP
</sectionHeader>
<bodyText confidence="0.998593620689655">
We explain how to solve MCKP. We first explain
the greedy algorithm applied to text summariza-
tion by Filatova and Hatzivassiloglou (2004). We
then introduce a greedy algorithm with perfor-
mance guarantee. This algorithm has never been
applied to text summarization. We next explain the
stack decoding used by Yih et al. (2007). We then
introduce an approximate method based on linear
relaxation and a randomized algorithm, followed
by the branch-and-bound method, which provides
the exact solution.
Although the algorithms used in this paper
themselves are not novel, this work is the first
to apply the greedy algorithm with performance
guarantee, the randomized algorithm, and the
branch-and-bound to solve the MCKP and auto-
matically create a summary. In addition, we con-
duct a comparative study on summarization algo-
rithms including the above.
There are some other well-known methods for
similar problems (e.g., the method of conditional
probability (Hromkoviˇc, 2003)). A pipage ap-
proach (Ageev and Sviridenko, 2004) has been
proposed for MCKP, but we do not use this algo-
rithm, since it requires costly partial enumeration
and solutions to many linear relaxation problems.
As in the previous section, D denotes the set of
sentences {si, · · · , s|D|}, and S denotes a subset
of D and thus represents a summary.
</bodyText>
<subsectionHeader confidence="0.995146">
4.1 Greedy algorithm
</subsectionHeader>
<bodyText confidence="0.924099625">
Filatova and Hatzivassiloglou (2004) used a
greedy algorithm. In this section, Wl denotes the
sum of the weights of the words covered by sen-
tence sl. Wl denotes the sum of the weights of the
words covered by sl, but not by current summary
S. This algorithm sequentially selects sentence sl
with the largest Wl.
Greedy Algorithm
</bodyText>
<equation confidence="0.773595857142857">
U +-- D, S +-- φ
while U =� φ
si +-- argmaxslU Wl
if ci + ∑slS cl &lt; K then insert si into S
delete si in U
end while
output S.
</equation>
<bodyText confidence="0.9998158">
This algorithm has performance guarantee
when the problem has a unit cost (i.e., when each
sentence has the same length), but no performance
guarantee for the general case where costs can
have different values.
</bodyText>
<subsectionHeader confidence="0.6399475">
4.2 Greedy algorithm with performance
guarantee
</subsectionHeader>
<bodyText confidence="0.999993444444444">
We describe a greedy algorithm with performance
guarantee proposed by Khuller et al. (1999), which
proves to achieve an approximation factor of (1 −
1/e)/2 for MCKP. This algorithm sequentially se-
lects sentence sl with the largest ratio Wl /cl. Af-
ter the sequential selection, the set of the selected
sentences is compared with the single-sentence
summary that has the largest value of the objec-
tive function. The larger of the two is going to
</bodyText>
<page confidence="0.991894">
783
</page>
<bodyText confidence="0.998702333333333">
be the output of this new greedy algorithm. Here
score(S) is ∑j wjzj, the value of the objective
function for summary S.
</bodyText>
<figure confidence="0.7952795">
Greedy Algorithm with Performance Guarantee
U ← D, S ← φ
while U = φ
si ← arg maxslEU Wl� /cl
if ci + ∑slES cl ≤ K then insert si into S
delete si in U
end while
st ← arg maxsl Wl
</figure>
<construct confidence="0.338635">
if score(S) ≥ Wt, output S,
</construct>
<bodyText confidence="0.9159248">
otherwise, output {st}.
They also proposed an algorithm with a better per-
formance guarantee, which is not used in this pa-
per because it is costly due to its partial enumera-
tion.
</bodyText>
<subsectionHeader confidence="0.998493">
4.3 Stack decoding
</subsectionHeader>
<bodyText confidence="0.960044">
Stack decoding is a decoding method proposed by
Jelinek (1969). This algorithm requires K priority
queues, k-th of which is the queue for summaries
of length k. The objective function value is used
for the priority measure. A new solution (sum-
mary) is generated by adding a sentence to a cur-
rent solution in k-th queue and inserted into a suc-
ceeding queue.&apos; The “pop” operation in stack de-
coding pops the candidate summary with the least
priority in the queue. By restricting the size of
each queue to a certain constant stacksize, we can
obtain an approximate solution within a practical
computational time.
Stack Decoding
</bodyText>
<equation confidence="0.9279242">
for k = 0 to K − 1
for each S ∈ queues[k]
for each sl ∈ D
insert sl into S
insert S into queues[k + cl]
</equation>
<bodyText confidence="0.661097">
pop if queue-size exceeds the stacksize
</bodyText>
<listItem confidence="0.84410425">
end for
end for
end for
return the best solution in queues[K]
</listItem>
<subsectionHeader confidence="0.995999">
4.4 Randomized algorithm
</subsectionHeader>
<bodyText confidence="0.99915252173913">
Khuller et al. (2006) proposed a randomized al-
gorithm (Hromkoviˇc, 2003) for MCKP. In this al-
gorithm, a relaxation linear problem is generated
by replacing the integer constraints xi ∈ {0, 1}
&apos;We should be aware that stack in a strict data-structure
sense is not used in the algorithm.
and zj ∈ {0, 1} with linear constraints xi ∈ [0, 1]
and zj ∈ [0, 1]. The optimal solution xz to the re-
laxation problem is regarded as the probability of
sentence si being selected as a part of summary:
xz = P(xi = 1). The algorithm randomly se-
lects sentence si with probability xz , in order to
generate a summary. It has been proved that the
expected length of each randomly-generated sum-
mary is upper-bounded by K, and the expected
value of the objective function is at least the op-
timal value multiplied by (1−1/e) (Khuller et al.,
2006). This random generation of a summary is it-
erated many times, and the summaries that are not
longer than K are stored as candidate summaries.
Among those many candidate summaries, the one
with the highest value of the objective function is
going to be the output by this algorithm.
</bodyText>
<subsectionHeader confidence="0.833078">
4.5 Branch-and-bound method
</subsectionHeader>
<bodyText confidence="0.999929888888889">
The branch-and-bound method (Hromkoviˇc,
2003) is an efficient method for finding the exact
solutions to integer problems. Since MCKP is an
NP-hard problem, it cannot generally be solved in
polynomial time under a reasonable assumption
that NP=P. However, if the size of the problem
is limited, sometimes we can obtain the exact
solution within a practical time by means of the
branch-and-bound method.
</bodyText>
<subsectionHeader confidence="0.915711">
4.6 Weakly-constrained algorithms
</subsectionHeader>
<bodyText confidence="0.999875333333333">
In evaluation with ROUGE (Lin, 2004), sum-
maries are truncated to a target length K. Yih et
al. (2007) used a stack decoding with a slight mod-
ification, which allows the last sentence in a sum-
mary to be truncated to a target length. Let us call
this modified algorithm the weakly-constrained
stack decoding. The weakly-constrained stack de-
coding can be implemented simply by replacing
queues[k + cl] with queues[min(k + cl, K)]. We
can also think of weakly-constrained versions of
the greedy and randomized algorithms introduced
before.
In this paper, we do not adopt weakly-
constrained algorithms, because although an ad-
vantage of the extractive summarization is the
guaranteed grammaticality at the sentence level,
the summaries with a truncated sentence will relin-
quish this advantage. We mentioned the weakly-
constrained algorithms in order to explain the re-
lation between the proposed model and the model
proposed by Yih et al. (2007).
</bodyText>
<page confidence="0.996059">
784
</page>
<sectionHeader confidence="0.994022" genericHeader="method">
5 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.97346">
5.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999935344827586">
We conducted experiments on the dataset of
DUC’04 (2004) with settings of task 2, which is
a multi-document summarization task. 50 docu-
ment clusters, each of which consists of 10 doc-
uments, are given. One summary is to be gen-
erated for each cluster. Following the most rel-
evant previous method (Yih et al., 2007), we set
the target length to 100 words. DUC’03 (2003)
dataset was used as the training dataset for trained
weights. All the documents were segmented
into sentences using a script distributed by DUC.
Words are stemmed by Porter’s stemmer (Porter,
1980). ROUGE version 1.5.5 (Lin, 2004) was
used for evaluation.2 Among others, we focus
on ROUGE-1 in the discussion of the result, be-
cause ROUGE-1 has proved to have strong corre-
lation with human annotation (Lin, 2004; Lin and
Hovy, 2003). Wilcoxon signed rank test for paired
samples with significance level 0.05 was used for
the significance test of the difference in ROUGE-
1. The simplex method and the branch-and-bound
method implemented in GLPK (Makhorin, 2006)
were used to solve respectively linear and integer
programming problems.
The methods that are compared here are the
greedy algorithm (greedy), the greedy algorithm
with performance guarantee (g-greedy), the ran-
domized algorithm (rand), the stack decoding
(stack), and the branch-and-bound method (exact).
</bodyText>
<subsectionHeader confidence="0.769949">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9999150625">
The experimental results are shown in Tables 1
and 2. The columns 1, 2, and SU4 in the ta-
bles respectively refer to ROUGE-1, ROUGE-2,
and ROUGE-SU4. In addition, rand100k refers to
the randomized algorithm with 100,000 randomly-
generated solution candidates, and stack30 refers
to stack with the stacksize being 30. The right-
most column (‘time’) shows the average computa-
tional time required for generating a summary for
a document cluster.
Both with interpolated (Table 1) and trained
weights (Table 2), g-greedy significantly outper-
formed greedy. With interpolated weights, there
was no significant difference between exact and
g-greedy, and between exact and stack30. With
trained weights, there was no significant differ-
</bodyText>
<footnote confidence="0.604891">
2With options -n 4 -m -2 4 -u -f A -p 0.5 -l 100 -t 0 -d -s.
</footnote>
<tableCaption confidence="0.9010235">
Table 1: ROUGE of MCKP with interpolated
weights. Underlined ROUGE-1 scores are signif-
icantly different from the score of exact. Compu-
tational time was measured in seconds.
Table 2: ROUGE of MCKP with trained weights.
Underlined ROUGE-1 scores are significantly dif-
ferent from the score of exact. Computational time
was measured in seconds.
</tableCaption>
<bodyText confidence="0.99929124">
ence between exact and the other algorithms ex-
cept for greedy and rand100k. The result sug-
gests that approximate fast algorithms can yield
results comparable to the exact method in terms of
ROUGE-1 score. We will later discuss the results
in terms of objective function values and search
errors in Table 4.
We should notice that stack outperformed ex-
act with interpolated weights. To examine this
counter-intuitive point, we changed the stack-
size of stack with interpolated weights (inter) and
trained weights (train) from 10 to 100 and ob-
tained Table 3. This table shows that the ROUGE-
1 value does not increase as the stacksize does;
ROUGE-1 for stack with interpolated weights
does not change much with the stacksize, and the
peak of ROUGE-1 for trained weights is at the
stacksize of 20. Since stack with a larger stack-
size selects a solution from a larger number of so-
lution candidates, this result is counter-intuitive in
the sense that non-global decoding by stack has a
favorable effect.
We also counted the number of the document
clusters for which an approximate algorithm with
interpolated weights yielded the same solution as
</bodyText>
<figure confidence="0.995575370967742">
ROUGE
time
1
2
SU4
(sec)
greedy
g-greedy
rand100k
stack30
exact
0.283
0.294
0.300
0.304
0.305
0.083
0.080
0.079
0.078
0.081
0.123
0.121
0.119
0.120
0.121
&lt;0.01
0.01
1.88
4.53
4.04
ROUGE
time
1
2
SU4
(sec)
greedy
g-greedy
rand100k
stack30
exact
0.283
0.310
0.299
0.309
0.307
0.080
0.077
0.077
0.080
0.078
0.121
0.118
0.117
0.120
0.119
&lt; 0.01
0.01
1.93
4.23
4.56
</figure>
<page confidence="0.995555">
785
</page>
<tableCaption confidence="0.999445">
Table 3: ROUGE of stack with various stacksizes
</tableCaption>
<table confidence="0.974396666666667">
size 10 20 30 50 100
inter 0.304 0.304 0.304 0.304 0.303
train 0.308 0.310 0.309 0.308 0.307
</table>
<tableCaption confidence="0.9331605">
Table 4: Search errors of MCKP with interpolated
weights
</tableCaption>
<table confidence="0.957913333333333">
solution same search error
ROUGE (_) _ ⇓ ⇑
greedy 0 1 35 14
g-greedy 0 5 26 19
rand100k 6 5 25 14
stack30 16 11 8 11
</table>
<bodyText confidence="0.999319863636363">
exact (‘same solution’ column in Table 4). If
the approximate algorithm failed to yield the ex-
act solution (‘search error’ column), we checked
whether the search error made ROUGE score
unchanged (‘_’ column), decreased (‘⇓’ col-
umn), or increased (‘⇑’ column) compared with
ROUGE score of exact. Table 4 shows that (i)
stack30 is a better optimizer than other approx-
imate algorithms, (ii) when the search error oc-
curs, stack30 increases ROUGE-1 more often than
it decreases ROUGE-1 compared with exact in
spite of stack30’s inaccurate solution, (iii) ap-
proximate algorithms sometimes achieved better
ROUGE scores. We observed similar phenomena
for trained weights, though we skip the details due
to space limitation.
These observations on stacksize and search er-
rors suggest that there exists another maximization
problem that is more suitable to summarization.
We should attempt to find the more suitable maxi-
mization problem and solve it using some existing
optimization and approximation techniques.
</bodyText>
<sectionHeader confidence="0.75434" genericHeader="method">
6 Augmentation of the model
</sectionHeader>
<bodyText confidence="0.999794545454545">
On the basis of the experimental results in the pre-
vious section, we augment our text summarization
model. We first examine the current model more
carefully. As mentioned before, we used words
as conceptual units because defining those units
is hard and still under development by many re-
searchers. Suppose here that a more suitable unit
has more detailed information, such as “A did B
to C”. Then the event “A did D to E” is a com-
pletely different unit from “A did B to C”. How-
ever, when words are used as conceptual units, the
two events have a redundant part “A”. It can hap-
pen that a document is concise as a summary, but
redundant on word level. By being to some extent
redundant on the word level, a summary can have
sentences that are more relevant to the document
cluster, as both of the sentences above are relevant
to the document cluster if the document cluster is
about “A”. A summary with high cohesion and co-
herence would have redundancy to some extent. In
this section, we will use this conjecture to augment
our model.
</bodyText>
<subsectionHeader confidence="0.996855">
6.1 Augmented summarization model
</subsectionHeader>
<bodyText confidence="0.999817625">
The objective function of MCKP consists of only
one term that corresponds to coverage. We add
another term ∑i(∑j wjaij)xi that corresponds
to relevance to the topic of the document clus-
ter. We represent the relevance of sentence si by
the sum of the weights of words in the sentence
(∑j wjaij). We take the summation of the rele-
vance values of the selected sentences:
</bodyText>
<equation confidence="0.991724">
max. (1 − λ) ∑j wjzj + λ∑i(∑j wjaij)xi
s.t. ∑i cixi ≤ K; ∀j, ∑i aijxi ≥ zj;
∀i, xi ∈ {0,1}; ∀j, zj ∈ {0, 1},
</equation>
<bodyText confidence="0.99998488">
where λ is a constant. We call this model MCKP-
Rel, because the relevance to the document cluster
is taken into account.
We discuss the relation to the model proposed
by McDonald (2007), whose objective function
consists of a relevance term and a negative re-
dundancy term. We believe that MCKP-Rel is
more intuitive and suitable for summarization, be-
cause coverage in McDonald (2007) is measured
by subtracting the redundancy represented with
the sum of similarities between two sentences,
while MCKP-Rel focuses directly on coverage.
Suppose sentence s1 contains conceptual units A
and B, s2 contains A, and s3 contains B. The
proposed coverage-based methods can capture the
fact that s1 has the same information as {s2, s3},
while similarity-based methods only learn that s1
is somewhat similar to each of s2 and s3. We
also empirically showed that our method outper-
forms McDonald (2007)’s method in experiments
on DUC’02, where our method achieved 0.354
ROUGE-1 score with interpolated weights and
0.359 with trained weights when the optimal λ is
given, while McDonald (2007)’s method yielded
at most 0.348. However, this very point can also
</bodyText>
<page confidence="0.998062">
786
</page>
<tableCaption confidence="0.620416666666667">
Table 5: ROUGE-1 of MCKP-Rel with inter-
polated weights. The values in the parentheses
are the corresponding values of A predicted using
DUC’03 as development data. Underlined are the
values that are significantly different from the cor-
responding values of MCKP.
</tableCaption>
<table confidence="0.994131714285714">
interpolated trained
0.287 (0.1) 0.288 (0.8)
0.307 (0.3) 0.320 (0.4)
0.310 (0.1) 0.316 (0.5)
0.324 (0.1) 0.327 (0.3)
0.320 (0.3) 0.329 (0.5)
0.327 (0.2) 0.329 (0.5)
</table>
<bodyText confidence="0.9983794">
be a drawback of our method, since our method
premises that a sentence is represented as a set
of conceptual units. Similarity-based methods are
free from such a premise. Taking advantages of
both models is left for future work.
The decoding algorithms introduced before are
also applicable to MCKP-Rel, because MCKP-Rel
can be reduced to MCKP by adding, for each sen-
tence si, a dummy conceptual unit which exists
only in si and has the weight ∑j wjaij.
</bodyText>
<subsectionHeader confidence="0.999789">
6.2 Experiments of the augmented model
</subsectionHeader>
<bodyText confidence="0.999962">
We ran greedy, g-greedy, rand100k, stack30
and exact to solve MCKP-Rel. We experimented
on DUC’04 with the same experimental setting as
the previous ones.
</bodyText>
<subsectionHeader confidence="0.543844">
6.2.1 Experiments with the predicted A
</subsectionHeader>
<bodyText confidence="0.999978769230769">
We determined the value of A for each method us-
ing DUC’03 as development data. Specifically, we
conducted experiments on DUC’03 with different
A (E 10.0, 0.1, · · · ,1.0}) and simply selected the
one with the highest ROUGE-1 value.
The results with these predicted A are shown
in Table 5. Only ROUGE-1 values are shown.
Method exactopt is exact with the optimal A, and
can be regarded as the upperbound of MCKP-Rel.
To evaluate the appropriateness of models without
regard to search quality, we first focused on exact
and found that MCKP-Rel outperformed MCKP
with exact. This means that MCKP-Rel model
is superior to MCKP model. Among the algo-
rithms, stack30 and exact performed well. All
methods except for greedy yielded significantly
better ROUGE values compared with the corre-
sponding results in Tables 1 and 2.
Figures 1 and 2 show ROUGE-1 for different
values of A. The leftmost part (A = 0.0) cor-
responds to MCKP. We can see from the figures,
that MCKP-Rel at the best A always outperforms
MCKP, and that MCKP-Rel tends to degrade for
very large A. This means that excessive weight on
relevance has an adversative effect on performance
and therefore the coverage is important.
</bodyText>
<figure confidence="0.9222955">
0 0.2 0.4 0.6 0.8 1
lambda
</figure>
<figureCaption confidence="0.996465">
Figure 1: MCKP-Rel with interpolated weights
</figureCaption>
<figure confidence="0.9813">
0 0.2 0.4 0.6 0.8 1
lambda
</figure>
<figureCaption confidence="0.998866">
Figure 2: MCKP-Rel with trained weights
</figureCaption>
<subsectionHeader confidence="0.587718">
6.2.2 Experiments with the optimal A
</subsectionHeader>
<bodyText confidence="0.999827">
In the experiments above, we found that A =
0.2 is the optimal value for exact with interpo-
lated weights. We suppose that this A gives the
best model, and examined search errors as we
did in Section 5.2. We obtained Table 6, which
shows that search errors in MCKP-Rel counter-
intuitively increase (1-) ROUGE-1 score less of-
ten than MCKP did in Table 4. This was the case
also for trained weights. This result suggests that
MCKP-Rel is more suitable to text summariza-
tion than MCKP is. However, exact with trained
weights at the optimal A(= 0.4) in Figure 2 was
outperformed by stack30. It suggests that there is
still room for future improvement in the model.
</bodyText>
<figure confidence="0.9993840625">
ROUGE-1
0.34
0.33
0.32
0.31
0.29
0.28
0.3
exact
stack30
rand100k
g-greedy
greedy
ROUGE-1
0.34
0.33
0.32
0.31
0.29
0.28
0.3
exact
stack30
rand100k
g-greedy
greedy
greedy
g-greedy
rand100k
stack30
exact
exactopt
</figure>
<page confidence="0.983942">
787
</page>
<tableCaption confidence="0.98583">
Table 6: Search errors of MCKP-Rel with interpo-
lated weights (A = 0.2).
</tableCaption>
<table confidence="0.998094166666667">
solution same search error
ROUGE (=) = ⇓ ⇑
greedy 0 2 42 6
g-greedy 1 0 34 15
rand100k 3 6 33 8
stack30 14 13 14 10
</table>
<subsectionHeader confidence="0.782496">
6.2.3 Comparison with DUC results
</subsectionHeader>
<bodyText confidence="0.962995606060606">
In Section 6.2.1, we empirically showed that
the augmented model MCKP-Rel is better than
MCKP, whose optimization problem is used also
in one of the state-of-the-art methods by Yih et
al. (2007). It would also be beneficial to read-
ers to directly compare our method with DUC
results. For that purpose, we conducted experi-
ments with the cardinality constraint of DUC’04,
i.e., each summary should be 665 bytes long or
shorter. Other settings remained unchanged. We
compared the MCKP-Rel with peer65 (Conroy et
al., 2004) of DUC’04, which performed best in
terms of ROUGE-1 in the competition. Tables 7
and 8 are the ROUGE-1 scores, respectively eval-
uated without and with stopwords. The latter is the
official evaluation measure of DUC’04.
Table 7: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated without stopwords. Underlined
are the values significantly different from peer65.
interpolated train
greedy 0.289 (0.1) 0.284 (0.8)
g-greedy 0.297 (0.4) 0.323 (0.3)
rand100k 0.315 (0.2) 0.308 (0.4)
stack30 0.324 (0.2) 0.323 (0.3)
exact 0.325 (0.3) 0.326 (0.5)
exact,,pt 0.325 (0.3) 0.329 (0.4)
peer65 0.309
In Table 7, MCKP-Rel with stack30 and exact
yielded significantly better ROUGE-1 scores than
peer65. Although stack30 and exact yielded
greater ROUGE-1 scores than peer65 also in Ta-
ble 8, the difference was not significant. Only
greedy was significantly worse than peer65.3 One
</bodyText>
<footnote confidence="0.919119">
3We actually succeeded in greatly improving the
ROUGE-1 value of MCKP-Rel evaluated with stopwords by
using all the words including stopwords as conceptual units.
However, we ignore those results in this paper, because it
</footnote>
<tableCaption confidence="0.921221666666667">
Table 8: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated with stopwords. Underlined are
the values significantly different from peer65.
</tableCaption>
<table confidence="0.974935125">
interpolated train
greedy 0.374 (0.1) 0.377 (0.4)
g-greedy 0.371 (0.0) 0.385 (0.2)
rand100k 0.373 (0.2) 0.366 (0.3)
stack30 0.384 (0.1) 0.386 (0.3)
exact 0.383 (0.3) 0.384 (0.4)
exact,,pt 0.385 (0.1) 0.384 (0.4)
peer65 0.382
</table>
<bodyText confidence="0.97785975">
possible explanation on the difference between Ta-
ble 7 and Table 8 is that peer65 would probably be
tuned to the evaluation with stopwords, since it is
the official setting of DUC’04.
From these results, we can conclude that the
MCKP-Rel is at least comparable to the best-
performing method, if we choose a powerful de-
coding method, such as stack and exact.
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.994196703703704">
We regarded text summarization as MCKP. We
applied some algorithms to solve the MCKP and
conducted comparative experiments. We con-
ducted comparative experiments. We also aug-
mented our model to MCKP-Rel, which takes into
consideration the relevance to the document clus-
ter and performs well.
For future work, we will try other conceptual
units such as basic elements (Hovy et al., 2006)
proposed for summary evaluation. We also plan to
include compressed sentences into the set of can-
didate sentences to be selected as done by Yih et
al. (2007). We also plan to design other decod-
ing algorithms for text summarization (e.g., pipage
approach (Ageev and Sviridenko, 2004)). As dis-
cussed in Section 6.2, integration with similarity-
based models is worth consideration. We will in-
corporate techniques for arranging sentences into
an appropriate order, while the current work con-
cerns only selection. Deshpande et al. (2007) pro-
posed a selection and ordering technique, which is
applicable only to the unit cost case such as selec-
tion and ordering of words for title generation. We
plan to refine their model so that it can be applied
to general text summarization.
just trickily uses non-content words to increase the evalua-
tion measure, disregarding the actual quality of summaries.
</bodyText>
<page confidence="0.99654">
788
</page>
<sectionHeader confidence="0.993835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885369565218">
Alexander A. Ageev and Maxim Sviridenko. 2004. Pi-
page rounding: A new method of constructing algo-
rithms with proven performance guarantee. Journal
of Combinatorial Optimization, 8(3):307–328.
John M. Conroy, Judith D. Schlesinger, John Goldstein,
and Dianne P. O’Leary. 2004. Left-brain/right-brain
multi-document summarization. In Proceedings of
the Document Understanding Conference (DUC).
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Proceedings of the Human
Language Technologies Conference and the North
American Chapter of the Association for Compu-
tational Linguistics Annual Meeting (HLT/NAACL),
pages 444–451.
DUC. 2003. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
DUC. 2004. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the
20th International Conference on Computational
Linguistics (COLING), pages 397–403.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In Proceedings of
ANLP/NAACL Workshop on Automatic Summariza-
tion, pages 40–48.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC).
Juraj Hromkoviˇc. 2003. Algorithmics for Hard Prob-
lems. Springer.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, 13:675–685.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39–45.
Samir Khuller, Louiqa Raschid, and Yao Wu. 2006.
LP randomized rounding for maximum coverage
problem and minimum set cover with threshold
problem. Technical Report CS-TR-4805, The Uni-
versity of Maryland.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology (HLT-NAACL’03), pages
71–78.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pages 74–81.
Andrew Makhorin, 2006. Reference Manual of GNU
Linear Programming Kit, version 4.9.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Publisher.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the 29th European Conference on Infor-
mation Retrieval (ECIR), pages 557–564.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
Management, 40(6):919–938.
Barry Schiffman, Ani Nenkova, and Kathleen McKe-
own. 2002. Experiments in multidocument sum-
marization. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 52–58.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence (IJCAI), pages 2862–2867.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643–1662.
Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence (IJCAI), pages
1776–1782.
</reference>
<page confidence="0.998606">
789
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802897">
<title confidence="0.996904">Text Summarization Model based on Maximum Coverage Problem and its Variant</title>
<author confidence="0.996594">Takamura Okumura</author>
<affiliation confidence="0.997849">Precision and Intelligence Laboratory, Tokyo Institute of Technology</affiliation>
<address confidence="0.999335">4259 Nagatsuta Midori-ku Yokohama, 226-8503</address>
<email confidence="0.85615">takamura@pi.titech.ac.jpoku@pi.titech.ac.jp</email>
<abstract confidence="0.9945919375">We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander A Ageev</author>
<author>Maxim Sviridenko</author>
</authors>
<title>Pipage rounding: A new method of constructing algorithms with proven performance guarantee.</title>
<date>2004</date>
<journal>Journal of Combinatorial Optimization,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="11342" citStr="Ageev and Sviridenko, 2004" startWordPosition="1872" endWordPosition="1875">n and a randomized algorithm, followed by the branch-and-bound method, which provides the exact solution. Although the algorithms used in this paper themselves are not novel, this work is the first to apply the greedy algorithm with performance guarantee, the randomized algorithm, and the branch-and-bound to solve the MCKP and automatically create a summary. In addition, we conduct a comparative study on summarization algorithms including the above. There are some other well-known methods for similar problems (e.g., the method of conditional probability (Hromkoviˇc, 2003)). A pipage approach (Ageev and Sviridenko, 2004) has been proposed for MCKP, but we do not use this algorithm, since it requires costly partial enumeration and solutions to many linear relaxation problems. As in the previous section, D denotes the set of sentences {si, · · · , s|D|}, and S denotes a subset of D and thus represents a summary. 4.1 Greedy algorithm Filatova and Hatzivassiloglou (2004) used a greedy algorithm. In this section, Wl denotes the sum of the weights of the words covered by sentence sl. Wl denotes the sum of the weights of the words covered by sl, but not by current summary S. This algorithm sequentially selects sent</context>
<context position="31370" citStr="Ageev and Sviridenko, 2004" startWordPosition="5268" endWordPosition="5271">e applied some algorithms to solve the MCKP and conducted comparative experiments. We conducted comparative experiments. We also augmented our model to MCKP-Rel, which takes into consideration the relevance to the document cluster and performs well. For future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation. We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al. (2007). We also plan to design other decoding algorithms for text summarization (e.g., pipage approach (Ageev and Sviridenko, 2004)). As discussed in Section 6.2, integration with similaritybased models is worth consideration. We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection. Deshpande et al. (2007) proposed a selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation. We plan to refine their model so that it can be applied to general text summarization. just trickily uses non-content words to increase the evaluation measure, disregarding the actual quality of s</context>
</contexts>
<marker>Ageev, Sviridenko, 2004</marker>
<rawString>Alexander A. Ageev and Maxim Sviridenko. 2004. Pipage rounding: A new method of constructing algorithms with proven performance guarantee. Journal of Combinatorial Optimization, 8(3):307–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>John Goldstein</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC).</booktitle>
<marker>Conroy, Schlesinger, Goldstein, O’Leary, 2004</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, John Goldstein, and Dianne P. O’Leary. 2004. Left-brain/right-brain multi-document summarization. In Proceedings of the Document Understanding Conference (DUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pawan Deshpande</author>
<author>Regina Barzilay</author>
<author>David Karger</author>
</authors>
<title>Randomized decoding for selection-andordering problems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Human Language Technologies Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL),</booktitle>
<pages>444--451</pages>
<marker>Deshpande, Barzilay, Karger, 2007</marker>
<rawString>Pawan Deshpande, Regina Barzilay, and David Karger. 2007. Randomized decoding for selection-andordering problems. In Proceedings of the Human Language Technologies Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL), pages 444–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>Document Understanding Conference.</title>
<date>2003</date>
<booktitle>In HLT/NAACL Workshop on Text Summarization.</booktitle>
<marker>DUC, 2003</marker>
<rawString>DUC. 2003. Document Understanding Conference. In HLT/NAACL Workshop on Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>Document Understanding Conference.</title>
<date>2004</date>
<booktitle>In HLT/NAACL Workshop on Text Summarization.</booktitle>
<marker>DUC, 2004</marker>
<rawString>DUC. 2004. Document Understanding Conference. In HLT/NAACL Workshop on Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>397--403</pages>
<contexts>
<context position="4790" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="739" endWordPosition="742">uster. They used the centroid to rank sentences, together with the MMR-like redundancy score. Both relevance and redundancy are taken into consideration, but no global viewpoint is given. In CLASSY, which is the best-performing method in DUC’04, Conroy et al. (2004) scored sentences with the sum of tf-idf scores of words. They also incorporated sentence compression based on syntactic or heuristic rules. McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) first formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodified stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimiz</context>
<context position="6461" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="1010" endWordPosition="1013">single document, multi-document summarization is to generate a summary from multiple documents regarding one topic. Such a set of multiple documents is called a document cluster. The method proposed in this paper is applicable to both tasks. In both tasks, documents are split into several linguistic units D = {si, · · · , s|D|} in preprocessing. We will select some linguistic units from D to generate a summary. Among other linguistic units that can be used in the method, we use sentences so that the grammaticality at the sentence level is going to be guaranteed. We introduce conceptual units (Filatova and Hatzivassiloglou, 2004), which compose the meaning of a sentence. Sentence si is represented by a set of conceptual units {ei,, · · · , ei|si|}. For example, the sentence “The man bought a book and read it” could be regarded as consisting of two conceptual units “the man bought a book” and “the man read the book”. It is not easy, however, to determine the appropriate granularity of conceptual units. A simple way would be to regard the above sentence as consisting of four conceptual units “man”, “book”, “buy”, and “read”. There is some work on the definition of conceptual units. Hovy et al. (2006) proposed to use bas</context>
<context position="9202" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="1521" endWordPosition="1525">aijxi &gt; zj; bi, xi E {0,1}; bj, zj E {0, 11, 782 where zj is 1 when ej is covered, 0 otherwise. Notice that this new problem is equivalent to the previous one. Since not all the words are equally important, we introduce weights wj on words ej. Then the objective is restated as maximizing the weighted sum ∑j wjzj such that the summary length is at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (2007). The first one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the </context>
<context position="10460" citStr="Filatova and Hatzivassiloglou (2004)" startWordPosition="1737" endWordPosition="1740">document (namely, the first 100 words). Each probability is estimated with the maximum likelihood principle. The second one is trained weights. These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training dataset, 0 otherwise. The feature set for the logistic regression includes the frequency of the word in the document cluster and the position of the word instance and others. 4 Algorithms for solving MCKP We explain how to solve MCKP. We first explain the greedy algorithm applied to text summarization by Filatova and Hatzivassiloglou (2004). We then introduce a greedy algorithm with performance guarantee. This algorithm has never been applied to text summarization. We next explain the stack decoding used by Yih et al. (2007). We then introduce an approximate method based on linear relaxation and a randomized algorithm, followed by the branch-and-bound method, which provides the exact solution. Although the algorithms used in this paper themselves are not novel, this work is the first to apply the greedy algorithm with performance guarantee, the randomized algorithm, and the branch-and-bound to solve the MCKP and automatically cr</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In Proceedings of the 20th International Conference on Computational Linguistics (COLING), pages 397–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP/NAACL Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of ANLP/NAACL Workshop on Automatic Summarization, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhou</author>
<author>Junichi Fukumoto</author>
</authors>
<title>Automated summarization evaluation with basic elements.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="7041" citStr="Hovy et al. (2006)" startWordPosition="1114" endWordPosition="1117"> (Filatova and Hatzivassiloglou, 2004), which compose the meaning of a sentence. Sentence si is represented by a set of conceptual units {ei,, · · · , ei|si|}. For example, the sentence “The man bought a book and read it” could be regarded as consisting of two conceptual units “the man bought a book” and “the man read the book”. It is not easy, however, to determine the appropriate granularity of conceptual units. A simple way would be to regard the above sentence as consisting of four conceptual units “man”, “book”, “buy”, and “read”. There is some work on the definition of conceptual units. Hovy et al. (2006) proposed to use basic elements, which are dependency subtrees obtained by trimming dependency trees. Although basic elements were proposed for evaluation of summaries, they can probably be used also for summary generation. However, such novel units have not proved to be useful for summary generation. Since we focus more on algorithms and models in this paper, we simply use words as conceptual units. The goal of text summarization is to cover as many conceptual units as possible using only a small number of sentences. In other words, the goal is to find a subset S(C D) that covers as many conc</context>
<context position="31087" citStr="Hovy et al., 2006" startWordPosition="5221" endWordPosition="5224">s, since it is the official setting of DUC’04. From these results, we can conclude that the MCKP-Rel is at least comparable to the bestperforming method, if we choose a powerful decoding method, such as stack and exact. 7 Conclusion We regarded text summarization as MCKP. We applied some algorithms to solve the MCKP and conducted comparative experiments. We conducted comparative experiments. We also augmented our model to MCKP-Rel, which takes into consideration the relevance to the document cluster and performs well. For future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation. We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al. (2007). We also plan to design other decoding algorithms for text summarization (e.g., pipage approach (Ageev and Sviridenko, 2004)). As discussed in Section 6.2, integration with similaritybased models is worth consideration. We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection. Deshpande et al. (2007) proposed a selection and ordering technique, which is applicable on</context>
</contexts>
<marker>Hovy, Lin, Zhou, Fukumoto, 2006</marker>
<rawString>Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi Fukumoto. 2006. Automated summarization evaluation with basic elements. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juraj Hromkoviˇc</author>
</authors>
<title>Algorithmics for Hard Problems.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<marker>Hromkoviˇc, 2003</marker>
<rawString>Juraj Hromkoviˇc. 2003. Algorithmics for Hard Problems. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Fast sequential decoding algorithm using a stack.</title>
<date>1969</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>13--675</pages>
<contexts>
<context position="13416" citStr="Jelinek (1969)" startWordPosition="2256" endWordPosition="2257"> the two is going to 783 be the output of this new greedy algorithm. Here score(S) is ∑j wjzj, the value of the objective function for summary S. Greedy Algorithm with Performance Guarantee U ← D, S ← φ while U = φ si ← arg maxslEU Wl� /cl if ci + ∑slES cl ≤ K then insert si into S delete si in U end while st ← arg maxsl Wl if score(S) ≥ Wt, output S, otherwise, output {st}. They also proposed an algorithm with a better performance guarantee, which is not used in this paper because it is costly due to its partial enumeration. 4.3 Stack decoding Stack decoding is a decoding method proposed by Jelinek (1969). This algorithm requires K priority queues, k-th of which is the queue for summaries of length k. The objective function value is used for the priority measure. A new solution (summary) is generated by adding a sentence to a current solution in k-th queue and inserted into a succeeding queue.&apos; The “pop” operation in stack decoding pops the candidate summary with the least priority in the queue. By restricting the size of each queue to a certain constant stacksize, we can obtain an approximate solution within a practical computational time. Stack Decoding for k = 0 to K − 1 for each S ∈ queues</context>
</contexts>
<marker>Jelinek, 1969</marker>
<rawString>Frederick Jelinek. 1969. Fast sequential decoding algorithm using a stack. IBM Journal of Research and Development, 13:675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samir Khuller</author>
<author>Anna Moss</author>
<author>Joseph S Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="9054" citStr="Khuller et al., 1999" startWordPosition="1498" endWordPosition="1501">ng si, i.e., the number of words or bytes in si. For convenience, we rewrite the problem above: E max. j zj s.t. Ei cixi G K; bj, Ei aijxi &gt; zj; bi, xi E {0,1}; bj, zj E {0, 11, 782 where zj is 1 when ej is covered, 0 otherwise. Notice that this new problem is equivalent to the previous one. Since not all the words are equally important, we introduce weights wj on words ej. Then the objective is restated as maximizing the weighted sum ∑j wjzj such that the summary length is at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (20</context>
<context position="12469" citStr="Khuller et al. (1999)" startWordPosition="2075" endWordPosition="2078">he words covered by sl, but not by current summary S. This algorithm sequentially selects sentence sl with the largest Wl. Greedy Algorithm U +-- D, S +-- φ while U =� φ si +-- argmaxslU Wl if ci + ∑slS cl &lt; K then insert si into S delete si in U end while output S. This algorithm has performance guarantee when the problem has a unit cost (i.e., when each sentence has the same length), but no performance guarantee for the general case where costs can have different values. 4.2 Greedy algorithm with performance guarantee We describe a greedy algorithm with performance guarantee proposed by Khuller et al. (1999), which proves to achieve an approximation factor of (1 − 1/e)/2 for MCKP. This algorithm sequentially selects sentence sl with the largest ratio Wl /cl. After the sequential selection, the set of the selected sentences is compared with the single-sentence summary that has the largest value of the objective function. The larger of the two is going to 783 be the output of this new greedy algorithm. Here score(S) is ∑j wjzj, the value of the objective function for summary S. Greedy Algorithm with Performance Guarantee U ← D, S ← φ while U = φ si ← arg maxslEU Wl� /cl if ci + ∑slES cl ≤ K then </context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>Samir Khuller, Anna Moss, and Joseph S. Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samir Khuller</author>
<author>Louiqa Raschid</author>
<author>Yao Wu</author>
</authors>
<title>LP randomized rounding for maximum coverage problem and minimum set cover with threshold problem.</title>
<date>2006</date>
<tech>Technical Report CS-TR-4805,</tech>
<institution>The University of Maryland.</institution>
<contexts>
<context position="14230" citStr="Khuller et al. (2006)" startWordPosition="2403" endWordPosition="2406">erated by adding a sentence to a current solution in k-th queue and inserted into a succeeding queue.&apos; The “pop” operation in stack decoding pops the candidate summary with the least priority in the queue. By restricting the size of each queue to a certain constant stacksize, we can obtain an approximate solution within a practical computational time. Stack Decoding for k = 0 to K − 1 for each S ∈ queues[k] for each sl ∈ D insert sl into S insert S into queues[k + cl] pop if queue-size exceeds the stacksize end for end for end for return the best solution in queues[K] 4.4 Randomized algorithm Khuller et al. (2006) proposed a randomized algorithm (Hromkoviˇc, 2003) for MCKP. In this algorithm, a relaxation linear problem is generated by replacing the integer constraints xi ∈ {0, 1} &apos;We should be aware that stack in a strict data-structure sense is not used in the algorithm. and zj ∈ {0, 1} with linear constraints xi ∈ [0, 1] and zj ∈ [0, 1]. The optimal solution xz to the relaxation problem is regarded as the probability of sentence si being selected as a part of summary: xz = P(xi = 1). The algorithm randomly selects sentence si with probability xz , in order to generate a summary. It has been proved t</context>
</contexts>
<marker>Khuller, Raschid, Wu, 2006</marker>
<rawString>Samir Khuller, Louiqa Raschid, and Yao Wu. 2006. LP randomized rounding for maximum coverage problem and minimum set cover with threshold problem. Technical Report CS-TR-4805, The University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL’03),</booktitle>
<pages>71--78</pages>
<contexts>
<context position="17577" citStr="Lin and Hovy, 2003" startWordPosition="2964" endWordPosition="2967">documents, are given. One summary is to be generated for each cluster. Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words. DUC’03 (2003) dataset was used as the training dataset for trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with significance level 0.05 was used for the significance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact). 5.2 Results The experimental results are shown in Tables 1 and 2. T</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLT-NAACL’03), pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="9520" citStr="Lin, 2004" startWordPosition="1581" endWordPosition="1582">s at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (2007). The first one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the beginning part of the document (namely, the first 100 words). Each probability is estimated with the maximum likelihood principle. The second one is trained weights. These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training da</context>
<context position="15819" citStr="Lin, 2004" startWordPosition="2676" endWordPosition="2677">ries, the one with the highest value of the objective function is going to be the output by this algorithm. 4.5 Branch-and-bound method The branch-and-bound method (Hromkoviˇc, 2003) is an efficient method for finding the exact solutions to integer problems. Since MCKP is an NP-hard problem, it cannot generally be solved in polynomial time under a reasonable assumption that NP=P. However, if the size of the problem is limited, sometimes we can obtain the exact solution within a practical time by means of the branch-and-bound method. 4.6 Weakly-constrained algorithms In evaluation with ROUGE (Lin, 2004), summaries are truncated to a target length K. Yih et al. (2007) used a stack decoding with a slight modification, which allows the last sentence in a summary to be truncated to a target length. Let us call this modified algorithm the weakly-constrained stack decoding. The weakly-constrained stack decoding can be implemented simply by replacing queues[k + cl] with queues[min(k + cl, K)]. We can also think of weakly-constrained versions of the greedy and randomized algorithms introduced before. In this paper, we do not adopt weaklyconstrained algorithms, because although an advantage of the ex</context>
<context position="17376" citStr="Lin, 2004" startWordPosition="2931" endWordPosition="2932">ntal Setting We conducted experiments on the dataset of DUC’04 (2004) with settings of task 2, which is a multi-document summarization task. 50 document clusters, each of which consists of 10 documents, are given. One summary is to be generated for each cluster. Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words. DUC’03 (2003) dataset was used as the training dataset for trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with significance level 0.05 was used for the significance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with pe</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Makhorin</author>
</authors>
<date>2006</date>
<journal>Reference Manual of GNU Linear Programming Kit, version</journal>
<volume>4</volume>
<contexts>
<context position="17804" citStr="Makhorin, 2006" startWordPosition="3001" endWordPosition="3002">r trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with significance level 0.05 was used for the significance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact). 5.2 Results The experimental results are shown in Tables 1 and 2. The columns 1, 2, and SU4 in the tables respectively refer to ROUGE-1, ROUGE-2, and ROUGE-SU4. In addition, rand100k refers to the randomized algorithm with 100,000 randomlygenerated solution candidates, and stack30 refers to st</context>
</contexts>
<marker>Makhorin, 2006</marker>
<rawString>Andrew Makhorin, 2006. Reference Manual of GNU Linear Programming Kit, version 4.9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publisher.</publisher>
<contexts>
<context position="1131" citStr="Mani, 2001" startWordPosition="169" endWordPosition="170">andomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords. 1 Introduction Automatic text summarization is one of the tasks that have long been studied in natural language processing. This task is to create a summary, or a short and concise document that describes the content of a given set of documents (Mani, 2001). One well-known approach to text summarization is the extractive method, which selects some linguistic units (e.g., sentences) from given documents in order to generate a summary. The extractive method has an advantage that the grammaticality is guaranteed at least at the level of the linguistic units. Since the actual generation of linguistic expressions has not achieved the level of the practical use, we focus on the extractive method in this paper, especially the method based on the sentence extraction. Most of the extractive summarization methods rely on sequentially solving binary classi</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Summarization. John Benjamins Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th European Conference on Information Retrieval (ECIR),</booktitle>
<pages>557--564</pages>
<contexts>
<context position="4576" citStr="McDonald (2007)" startWordPosition="708" endWordPosition="709">hapter of the ACL, pages 781–789, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 781 is, the words with high relevance to the topic of the document cluster. They used the centroid to rank sentences, together with the MMR-like redundancy score. Both relevance and redundancy are taken into consideration, but no global viewpoint is given. In CLASSY, which is the best-performing method in DUC’04, Conroy et al. (2004) scored sentences with the sum of tf-idf scores of words. They also incorporated sentence compression based on syntactic or heuristic rules. McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) first formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodified stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et </context>
<context position="23824" citStr="McDonald (2007)" startWordPosition="4023" endWordPosition="4024">y one term that corresponds to coverage. We add another term ∑i(∑j wjaij)xi that corresponds to relevance to the topic of the document cluster. We represent the relevance of sentence si by the sum of the weights of words in the sentence (∑j wjaij). We take the summation of the relevance values of the selected sentences: max. (1 − λ) ∑j wjzj + λ∑i(∑j wjaij)xi s.t. ∑i cixi ≤ K; ∀j, ∑i aijxi ≥ zj; ∀i, xi ∈ {0,1}; ∀j, zj ∈ {0, 1}, where λ is a constant. We call this model MCKPRel, because the relevance to the document cluster is taken into account. We discuss the relation to the model proposed by McDonald (2007), whose objective function consists of a relevance term and a negative redundancy term. We believe that MCKP-Rel is more intuitive and suitable for summarization, because coverage in McDonald (2007) is measured by subtracting the redundancy represented with the sum of similarities between two sentences, while MCKP-Rel focuses directly on coverage. Suppose sentence s1 contains conceptual units A and B, s2 contains A, and s3 contains B. The proposed coverage-based methods can capture the fact that s1 has the same information as {s2, s3}, while similarity-based methods only learn that s1 is somew</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European Conference on Information Retrieval (ECIR), pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="17343" citStr="Porter, 1980" startWordPosition="2926" endWordPosition="2927">eriments and Discussion 5.1 Experimental Setting We conducted experiments on the dataset of DUC’04 (2004) with settings of task 2, which is a multi-document summarization task. 50 document clusters, each of which consists of 10 documents, are given. One summary is to be generated for each cluster. Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words. DUC’03 (2003) dataset was used as the training dataset for trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with significance level 0.05 was used for the significance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (gree</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Małgorzata Sty´s</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing Management,</journal>
<volume>40</volume>
<issue>6</issue>
<marker>Radev, Jing, Sty´s, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Małgorzata Sty´s, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing Management, 40(6):919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Schiffman</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Experiments in multidocument summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>52--58</pages>
<marker>Schiffman, Nenkova, McKeown, 2002</marker>
<rawString>Barry Schiffman, Ani Nenkova, and Kathleen McKeown. 2002. Experiments in multidocument summarization. In Proceedings of the Second International Conference on Human Language Technology Research, pages 52–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>2862--2867</pages>
<contexts>
<context position="5251" citStr="Shen et al. (2007)" startWordPosition="817" endWordPosition="820">obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) first formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodified stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimized in terms of likelihood, the coverage of concepts is not taken into account. 3 Modeling text summarization In this paper, we focus on the extractive summarization, which generates a summary by selecting linguistic units (e.g., sentences) in given documents. There are two types of summarization tasks: single-document summarization and multidocument summarization. While single-document summarization is to generate a summary from a single document, multi-doc</context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional random fields. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 2862–2867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiren Ye</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
<author>Long Qiu</author>
</authors>
<title>Document concept lattice for text understanding and summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--6</pages>
<contexts>
<context position="5186" citStr="Ye et al. (2007)" startWordPosition="807" endWordPosition="810">(2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) first formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodified stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimized in terms of likelihood, the coverage of concepts is not taken into account. 3 Modeling text summarization In this paper, we focus on the extractive summarization, which generates a summary by selecting linguistic units (e.g., sentences) in given documents. There are two types of summarization tasks: single-document summarization and multidocument summarization. While single-document summari</context>
</contexts>
<marker>Ye, Chua, Kan, Qiu, 2007</marker>
<rawString>Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long Qiu. 2007. Document concept lattice for text understanding and summarization. Information Processing and Management, 43(6):1643–1662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1776--1782</pages>
<contexts>
<context position="4967" citStr="Yih et al. (2007)" startWordPosition="769" endWordPosition="772">Y, which is the best-performing method in DUC’04, Conroy et al. (2004) scored sentences with the sum of tf-idf scores of words. They also incorporated sentence compression based on syntactic or heuristic rules. McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) first formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodified stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimized in terms of likelihood, the coverage of concepts is not taken into account. 3 Modeling text summarization In this paper, we focus on the extractive summarization, which gener</context>
<context position="9657" citStr="Yih et al. (2007)" startWordPosition="1604" endWordPosition="1607"> et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (2007). The first one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the beginning part of the document (namely, the first 100 words). Each probability is estimated with the maximum likelihood principle. The second one is trained weights. These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training dataset, 0 otherwise. The feature set for the logistic regression includes the frequency of the word in the document cluster and the positi</context>
<context position="15884" citStr="Yih et al. (2007)" startWordPosition="2687" endWordPosition="2690">tion is going to be the output by this algorithm. 4.5 Branch-and-bound method The branch-and-bound method (Hromkoviˇc, 2003) is an efficient method for finding the exact solutions to integer problems. Since MCKP is an NP-hard problem, it cannot generally be solved in polynomial time under a reasonable assumption that NP=P. However, if the size of the problem is limited, sometimes we can obtain the exact solution within a practical time by means of the branch-and-bound method. 4.6 Weakly-constrained algorithms In evaluation with ROUGE (Lin, 2004), summaries are truncated to a target length K. Yih et al. (2007) used a stack decoding with a slight modification, which allows the last sentence in a summary to be truncated to a target length. Let us call this modified algorithm the weakly-constrained stack decoding. The weakly-constrained stack decoding can be implemented simply by replacing queues[k + cl] with queues[min(k + cl, K)]. We can also think of weakly-constrained versions of the greedy and randomized algorithms introduced before. In this paper, we do not adopt weaklyconstrained algorithms, because although an advantage of the extractive summarization is the guaranteed grammaticality at the se</context>
<context position="28552" citStr="Yih et al. (2007)" startWordPosition="4820" endWordPosition="4823">0.33 0.32 0.31 0.29 0.28 0.3 exact stack30 rand100k g-greedy greedy ROUGE-1 0.34 0.33 0.32 0.31 0.29 0.28 0.3 exact stack30 rand100k g-greedy greedy greedy g-greedy rand100k stack30 exact exactopt 787 Table 6: Search errors of MCKP-Rel with interpolated weights (A = 0.2). solution same search error ROUGE (=) = ⇓ ⇑ greedy 0 2 42 6 g-greedy 1 0 34 15 rand100k 3 6 33 8 stack30 14 13 14 10 6.2.3 Comparison with DUC results In Section 6.2.1, we empirically showed that the augmented model MCKP-Rel is better than MCKP, whose optimization problem is used also in one of the state-of-the-art methods by Yih et al. (2007). It would also be beneficial to readers to directly compare our method with DUC results. For that purpose, we conducted experiments with the cardinality constraint of DUC’04, i.e., each summary should be 665 bytes long or shorter. Other settings remained unchanged. We compared the MCKP-Rel with peer65 (Conroy et al., 2004) of DUC’04, which performed best in terms of ROUGE-1 in the competition. Tables 7 and 8 are the ROUGE-1 scores, respectively evaluated without and with stopwords. The latter is the official evaluation measure of DUC’04. Table 7: ROUGE-1 of MCKP-Rel with byte constraints, eva</context>
<context position="31245" citStr="Yih et al. (2007)" startWordPosition="5249" endWordPosition="5252"> choose a powerful decoding method, such as stack and exact. 7 Conclusion We regarded text summarization as MCKP. We applied some algorithms to solve the MCKP and conducted comparative experiments. We conducted comparative experiments. We also augmented our model to MCKP-Rel, which takes into consideration the relevance to the document cluster and performs well. For future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation. We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al. (2007). We also plan to design other decoding algorithms for text summarization (e.g., pipage approach (Ageev and Sviridenko, 2004)). As discussed in Section 6.2, integration with similaritybased models is worth consideration. We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection. Deshpande et al. (2007) proposed a selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation. We plan to refine their model so that it can be applied to general text</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI), pages 1776–1782.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>