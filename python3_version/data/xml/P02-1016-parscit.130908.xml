<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994376">
Active Learning for Statistical Natural Language Parsing
</title>
<author confidence="0.952944">
Min Tang
</author>
<affiliation confidence="0.839915666666667">
Spoken Language Systems Group
MIT Laboratory for Computer Science
Cambridge, Massachusetts 02139, USA
</affiliation>
<email confidence="0.996772">
mtang@sls.lcs.mit.edu
</email>
<note confidence="0.816834333333333">
Xiaoqiang Luo Salim Roukos
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
</note>
<email confidence="0.977926">
xiaoluo,roukos@us.ibm.com
</email>
<sectionHeader confidence="0.996797" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965904">
It is necessary to have a (large) annotated cor-
pus to build a statistical parser. Acquisition of
such a corpus is costly and time-consuming.
This paper presents a method to reduce this
demand using active learning, which selects
what samples to annotate, instead of annotating
blindly the whole training corpus.
Sample selection for annotation is based upon
“representativeness” and “usefulness”. A
model-based distance is proposed to measure
the difference of two sentences and their most
likely parse trees. Based on this distance, the
active learning process analyzes the sample dis-
tribution by clustering and calculates the den-
sity of each sample to quantify its representa-
tiveness. Further more, a sentence is deemed as
useful if the existing model is highly uncertain
about its parses, where uncertainty is measured
by various entropy-based scores.
Experiments are carried out in the shallow se-
mantic parser of an air travel dialog system.
Our result shows that for about the same pars-
ing accuracy, we only need to annotate a third
of the samples as compared to the usual random
selection method.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920479166667">
A prerequisite for building statistical parsers (Jelinek et
al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak,
1997) is the availability of a (large) corpus of parsed sen-
tences. Acquiring such a corpus is expensive and time-
consuming and is often the bottleneck to build a parser
for a new application or domain. The goal of this study is
to reduce the amount of annotated sentences (and hence
the development time) required for a statistical parser to
achieve a satisfactory performance using active learning.
Active learning has been studied in the context of many
natural language processing (NLP) applications such as
information extraction(Thompson et al., 1999), text clas-
sification(McCallum and Nigam, 1998) and natural lan-
guage parsing(Thompson et al., 1999; Hwa, 2000), to
name a few. The basic idea is to couple tightly knowl-
edge acquisition, e.g., annotating sentences for parsing,
with model-training, as opposed to treating them sepa-
rately. In our setup, we assume that a small amount of
annotated sentences is initially available, which is used
to build a statistical parser. We also assume that there is
a large corpus of unannotated sentences at our disposal –
this corpus is called active training set. A batch of sam-
ples&apos; is selected using algorithms developed here, and are
annotated by human beings and are then added to training
data to rebuild the model. The procedure is iterated until
the model reaches a certain accuracy level.
Our efforts are devoted to two aspects: first, we be-
lieve that the selected samples should reflect the underly-
ing distribution of the training corpus. In other words, the
selected samples need to be representative. To this end,
a model-based structural distance is defined to quantify
how “far” two sentences are apart, and with the help of
this distance, the active training set is clustered so that
we can define and compute the “density” of a sample;
second, we propose and test several entropy-based mea-
sures to quantify the uncertainty of a sample in the active
training set using an existing model, as it makes sense
to ask human beings to annotate the portion of data for
which the existing model is not doing well. Samples are
selected from the clusters based on uncertainty scores.
The rest of the paper is organized as follows. In Sec-
tion 2, a structural distance is first defined based on the se-
quential representation of a parse tree. It is then straight-
forward to employ a k-means algorithm to cluster sen-
tences in the active training set. Section 3 is devoted to
confidence measures, where three uncertainty measures
are proposed. Active learning results on the shallow se-
mantic parser of an air travel dialog system are presented
</bodyText>
<note confidence="0.7506565">
&apos;A sample means a sentence in this paper.
Proceedings of the 40th Annual Meeting of the Association for
</note>
<page confidence="0.276103">
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 120-127.
</page>
<bodyText confidence="0.854979666666667">
in Section 4. A summary of related work is given in
Section 5. The paper closes with conclusions and future
work.
</bodyText>
<sectionHeader confidence="0.830564" genericHeader="method">
2 Sentence Distance and Clustering
</sectionHeader>
<bodyText confidence="0.999864833333333">
To characterize the “representativeness” of a sentence, we
need to know how far two sentences are apart so that we
can measure roughly how many similar sentences there
are in the active training set. For our purpose, the dis-
tance ought to have the property that two sentences with
similar structures have a small distance, even if they are
lexically different. This leads us to define the distance be-
tween two sentences based on their parse trees, which are
obtained by applying an existing model to the active train-
ing set. However, computing the distance of two parse
trees requires a digression of how they are represented in
our parser.
</bodyText>
<subsectionHeader confidence="0.7626865">
2.1 Event Representation of Parse Trees
A statistical parser computes , the probability of a
</subsectionHeader>
<bodyText confidence="0.99681252631579">
parse given a sentence. Since the space of the entire
parses is too large and cannot be modeled directly, a parse
tree is decomposed as a series of individual actions
. In the parser (Jelinek et al., 1994) we
used in this study, this is accomplished through a bottom-
up-left-most (BULM) derivation. In the BULM deriva-
tion, there are three types of parse actions: tag, label and
extension. There is a corresponding vocabulary for tag
or label, and there are four extension directions: RIGHT,
LEFT, UP and UNIQUE. If a child node is the only node
under a label, the child node is said to extend UNIQUE
to its parent node; if there are multiple children under a
parent node, the left-most child is said to extend RIGHT
to the parent node, the right-most child node is said to
extend LEFT to the parent node, while all the other in-
termediate children are said to extend UP to their parent
node. The BULM derivation can be best explained by an
example in Figure 1.
fly from new york to boston
</bodyText>
<figureCaption confidence="0.9086382">
Figure 1: Serial decomposition of a parse tree
as 17 parsing actions: tags (1,3,5,7,11,13) – blue
boxes, labels (9,15,17)–green underlines, extensions
(2,4,6,8,10,12,14,16)– red parentheses. Numbers indi-
cate the order of actions.
</figureCaption>
<bodyText confidence="0.995536095238095">
The input sentence is fly from new york to
boston. Numbers on its semantic parse tree indicate
the order of parse actions while colors indicate types of
actions: tags are numbered in blue boxes, extensions in
red parentheses and labels in green underlines. For this
example, the first action is tagging the first word fly
given the sentence; the second action is extending the tag
wd RIGHT, as the tag wd is the left-most child of the con-
stituent S; and the third action is tagging the second word
from given the sentence and the two proceeding actions,
and so on and so forth.
We define an event as a parse action together with its
context. It is clear that the BULM derivation converts a
parse tree into a unique sequence of parse events, and a
valid event sequence corresponds to a unique parse tree.
Therefore a parse tree can be equivalently represented by
a sequence of events. Let be the set of tagging ac-
tions, be the labeling actions and be the ex-
tending actions of, and let be the sequence of ac-
tions ahead of the action, then can be rewritten
as:
</bodyText>
<equation confidence="0.7794">
(1)
</equation>
<bodyText confidence="0.9806169">
Note that . The three
models (1) can be trained using decision trees (Jelinek et
al., 1994; Breiman et al., 1984).
Note that raw context space is too huge to
store and manipulate efficiently. In our implementation,
contexts are internally represented as bitstrings through a
set of pre-designed questions. Answers of each question
are represented as bitstrings. To support questions like
“what is the previous word (or tag, label, extension)?”,
word, tag, label and extension vocabularies are all en-
coded as bitstrings. Words are encoded through an au-
tomatic clustering algorithm (Brown et al., 1992) while
tags, labels and extensions are normally encoded using
diagonal bits. An example can be found in (Luo et al.,
2002).
In summary, a parse tree can be represented uniquely
by a sequence of events, while each event can in turn be
represented as a bitstring. With this in mind, we are now
ready to define a structural distance for two sentences
given an existing model.
</bodyText>
<subsectionHeader confidence="0.999073">
2.2 Sentence Distance
</subsectionHeader>
<bodyText confidence="0.999238">
Recall that it is assumed that there is a statistical parser
trained with a small amount of annotated data. To
infer structures of two sentencesand, we use
</bodyText>
<equation confidence="0.8881072">
S
(17)
(16)
(12)
LOC (9) (15)
LOC
(6) (8)
wd wd city city wd city
1 3 5 7 11
(10)
(2)
(4)
(14)
13
(7)
</equation>
<bodyText confidence="0.977743842105263">
to decodeandand get their most likely parse trees
and. The distance betweenand, given ,
is defined as the distance between and ,
or:
(2)
To emphasize the dependency on , we denote the dis-
tance as . Note that we assume here that
and have similar “true” parses if they have similar
structures under the current model .
We have shown in Section 2.1 that a parse tree can
be represented by a sequence of events, each of which
can in turn be represented as bitstrings through answer-
ing questions. Let be the
sequence representation for ( ), where
, and is the context and is the
parsing action of the event of the parse tree. We
can define the distance between two sentencesas
(3)
The distance between two sequences andis com-
puted as the editing distance using dynamic program-
ming (Rabiner and Juang, 1993). We now describe the
distance between two individual events.
We take advantage of the fact that contexts can
be encoded as bitstrings, and define the distance between
two contexts as the Hamming distance between their bit-
string representations. We further define the distance be-
tween two parsing actions as follows: it is either or a
constantif two parse actions are of the same type (re-
call there are three types of parsing actions: tag, label and
extension), and infinity if different types. We chooseto
be the number of bits in to emphasize the importance
of parsing actions in distance computation. Formally, let
be the type of action, then
The distance makes it possible to characterize
how dense a sentence is. Given a set of sentences
, the density of sampleis defined as:
That is, the sample density is defined as the inverse of
its average distance to other samples. We also define the
</bodyText>
<equation confidence="0.7184745">
centroid2 of S as
argmax (8)
</equation>
<subsectionHeader confidence="0.700527">
2.3 K-Means Clustering
</subsectionHeader>
<bodyText confidence="0.9761238">
With the model-based distance measure defined above,
we can use the K-means algorithm to cluster sentences.
A sketch of the algorithm (Jelinek, 1997) is as follows.
Let be the set of sentences to be
clustered.
</bodyText>
<listItem confidence="0.909107">
1. Initialization. Partition into k ini-
tial clusters ( ). Let .
2. Find the centroidfor each collection , that is:
argmin
3. Re-partition into clusters
, where
4. Let . Repeat Step 2 and Step 3 untill the al-
</listItem>
<bodyText confidence="0.937544">
gorithm converges (e.g., relative change of the total
distortion is smaller than a threshold).
For each iteration we need to compute:
where is the Hamming distance, and
if
</bodyText>
<equation confidence="0.731188666666667">
if Y( ) = Y(
)
if Y( Y( ).
</equation>
<bodyText confidence="0.928642777777778">
Computing the editing distance (3) requires dynamic
programming and it is computationally extensive. To
speed up computation, we can choose to ignore the dif-
ference in contexts, or in other words, (4) becomes
(6)
,
the pair-wise distances within each cluster.
The basic operation here is to compute the distance be-
tween two sentences, which involves a dynamic program-
ming process and is time-consuming. The complexity of
this algorithm is, if we assume the N samples are uni-
formly distributed between the k clusters, approximately
, or when . In our experi-
ments and , we need to call the
dynamic programming routine times each itera-
tion!
2We constrain the centroid to be an element of the set as it
is not clear how to “average” sentences.
</bodyText>
<figure confidence="0.626596333333333">
(5)
(4)
the distance between samplesand cluster centers
</figure>
<bodyText confidence="0.998009857142857">
To speed up, dynamic programming is constrained so
that only the band surrounding the diagonal line (Rabiner
and Juang, 1993) is allowed, and repeated sentences are
stored as a unique copy with its count so that computation
for the same sentence pair is never repeated. The latter is
a quite effective for dialog systems as a sentence is often
seen more than once in the training corpus.
</bodyText>
<sectionHeader confidence="0.92875" genericHeader="method">
3 Uncertainty Measures
</sectionHeader>
<bodyText confidence="0.995778625">
Intuitively, we would like to select samples that the cur-
rent model is not doing well. The current model’s un-
certainty about a sentence could be because similar sen-
tences are under-represented in the (annotated) training
set, or similar sentences are intrinsically difficult. We
take advantage of the availability of parsing scores from
the existing statistical parser and propose three entropy-
based uncertainty scores.
</bodyText>
<subsectionHeader confidence="0.987048">
3.1 Change of Entropy
</subsectionHeader>
<bodyText confidence="0.993639615384615">
After decision trees are grown, we can compute the en-
tropy of each leaf nodeas:
wheresums over either tag, label or extension vocab-
ulary, and is simply , where is the
count ofin leaf node. The model entropy is the
weighted sum of :
where . Note that is the log proba-
bility of training events.
After seeing an unlabeled sentence, we can decode it
using the existing model and get its most probable parse
. The tree can then be represented by a sequence of
events, which can be “poured” down the grown trees, and
the count can be updated accordingly – denote the
updated count as . A new model entropy can be
computed based on , and the absolute difference,
after it is normalized by the number of events in, is
the change of entropy we are after:
It is worth pointing out that is a “local” quantity in
that the vast majority of is equal to , and thus
we only have to visit leaf nodes where counts change. In
other words, can be computed efficiently.
characterizes how a sentence “surprises” the ex-
isting model: if the addition of events due to changes a
lot of , and consequently, , the sentence is proba-
bly not well represented in the initial training set and
will be large. We would like to annotate these sentences.
</bodyText>
<subsectionHeader confidence="0.99491">
3.2 Sentence Entropy
</subsectionHeader>
<bodyText confidence="0.981516615384615">
Now let us consider another measurement which seeks to
address the intrinsic difficulty of a sentence. Intuitively,
we can consider a sentence more difficult if there are po-
tentially more parses. We calculate the entropy of the dis-
tribution over all candidate parses as the sentence entropy
to measure the intrinsic ambiguity.
Given a sentence, the existing model could gener-
ate the top most likely parses ,
eachhaving a probability:
whereis the possible parse andis its associated
score. Without confusion, we drop’s dependency on
and define the sentence entropy as:
where:
</bodyText>
<subsectionHeader confidence="0.966007">
3.3 Word Entropy
</subsectionHeader>
<bodyText confidence="0.997772833333333">
As we can imagine, a long sentence tends to have more
possible parsing results not because it is difficult but sim-
ply because it is long. To counter this effect, we can nor-
malize the sentence entropy by the length of sentence to
calculate per word entropy of a sentence:
where is the number of words in.
</bodyText>
<figure confidence="0.9979177">
0.14
4
0.1
0.12
3.5
0.1
0.1
0.08
0.08
2
0.06
0.06
1.5
0.04
1
0.02
0.5
0
20 40 60 80 100 120
Sentence Length
</figure>
<figureCaption confidence="0.997325666666667">
Figure 2: Histograms of 3 uncertainty scores vs. sentence
lengths
Figure 2 illustrates the distribution of the three differ-
</figureCaption>
<figure confidence="0.969673">
ent uncertainty scores versus sentence lengths. favors
Average Change of Entropy H&amp;
Average Word Entropy Hw
2
0
Average Sentence Entropy Hs
2.5
3
00 120
20 40 60 80 100 120
20 40 60 80 1
Sentence Length
Sentence Length
0.04
0.02
0
</figure>
<bodyText confidence="0.9995596">
longer sentences more. This can be explained as follows:
longer sentences tend to have more complex structures
( extension and labeling ) than shorter sentences. And
the models for these complex structures are relatively less
trained as compared with models for tagging. As a result,
longer sentences would have higher change of entropy, in
other words, larger impact on models.
As explained above, longer sentences also have larger
sentence entropy. After normalizing, this trend is re-
versed in word entropy.
</bodyText>
<sectionHeader confidence="0.972172" genericHeader="method">
4 Experimental Results and Analysis
</sectionHeader>
<bodyText confidence="0.9998881875">
All experiments are done with a shallow semantic parser
(a.k.a. classer (Davies et al, 1999)) of the natural
language understanding part in DARPA Communica-
tor (DARPA Communicator Website, 2000). We built an
initial model using 1000 sentences. We have 20951 un-
labeled sentences for the active learner to select samples.
An independent test set consists of 4254 sentences. A
fixed batch size is used through out our experi-
ments.
Exact match is used to compute the accuracy, i.e.,
the accuracy is the number of sentences whose decod-
ing trees are exactly the same as human annotation di-
vided by the number of sentences in the test set. The ef-
fectiveness of active learning is measured by comparing
learning curves (i.e., test accuracy vs. number of training
sentences ) of active learning and random selection.
</bodyText>
<subsectionHeader confidence="0.998077">
4.1 Sample Selection Schemes
</subsectionHeader>
<bodyText confidence="0.999543571428571">
We experimented two basic sample selection algorithms.
The first one is selecting samples based solely on uncer-
tainty scores, while the second one clusters sentences,
and then selects the most uncertain ones from each clus-
ter.
Uncertainty Only: at each active learning iteration,
the most uncertain sentences are selected.
The drawback of this selection method is that it risks
selecting outliers because outliers are likely to get
high uncertainty scores under the existing models.
Figure 3 shows the test accuracy of this selection
method against the number of samples selected from
the active training set.
Short sentences tends to have higher value of
while sentence-based uncertainty scores (in terms of
or ) are low. Since we use the sentences as
the basic units, it is not surprising that -based
method performs poorly while the other two perform
very well.
Most Uncertain Per Cluster: In our implemen-
tation, we cluster the active training set so that
</bodyText>
<figure confidence="0.826147">
Sample Selection By Confidence Only
Number of Sentences Selected
</figure>
<figureCaption confidence="0.9842655">
Figure 3: Learning curves using uncertainty score only:
pick samples with highest entropies
</figureCaption>
<bodyText confidence="0.993524833333333">
the number of clusters equals the batch size. This
scheme selects the sentence with the highest uncer-
tain score from each cluster.
We expect that restricting sample selection to each
cluster would fix the problem that tends to be
large for short sentences, as short sentences are
likely to be in one cluster and long sentences will get
a fair chance to be selected in other clusters. This is
verified by the learning curves in Figure 4. Indeed,
performs as well as most of the time. And all
active learning algorithms perform better than ran-
dom selection.
</bodyText>
<figure confidence="0.877554">
Accuracy of Sample Selection(No Weighting)
Number of Sentences Selected
</figure>
<figureCaption confidence="0.9789715">
Figure 4: Learning curves of selecting the most uncertain
sample from each cluster.
</figureCaption>
<subsectionHeader confidence="0.996042">
4.2 Weighting Samples
</subsectionHeader>
<bodyText confidence="0.99952175">
In the sample selection process we calculated the density
of each sample. For those samples selected, we also have
the knowledge of their correct annotations, which can
be used to evalutate the model’s performance on them.
</bodyText>
<figure confidence="0.98626744117647">
200
300 400
90
85
80
Accuracy(%)
75
70
65
Random Selection
HA: Change Entropy
H: Word Entropy
HZ Sentence Entropy
500 600
700 800 900 1000
60
100
200
300 400
90
85
80
Accuracy(%)
75
70
65
Random Selection
HA: Change Entropy
H: Word Entropy
HZ Sentence Entropy
500 600
700 800 900 1000
60
100
</figure>
<bodyText confidence="0.980053">
We exploit this knowledge and experiment two weight-
ing schemes.
</bodyText>
<subsectionHeader confidence="0.806995">
Weight by Density:
</subsectionHeader>
<bodyText confidence="0.990610875">
A sample with higher density should be assigned
greater weights because the model can benefit
more by learning from this sample as it has more
neighbors. We calculate the density of a sample
inside its cluster so we need to adjust the density by
cluster size to avoid the unwanted bias toward small
clusters. For cluster , the weight for
sampleis proportional to .
</bodyText>
<note confidence="0.471895">
Accuracy of Sample Selection(Weighted by Performance)
</note>
<figure confidence="0.979786777777778">
200
300 400
90
85
80
Accuracy(%)
75
70
65
Random Selection
HA: Change Entropy
ZH Word Entropy
Hs: Sentence Entropy
500 600
700 800 900 1000
60
100
Number of Sentences Selected
</figure>
<bodyText confidence="0.99196925">
Weight by Performance: The idea of weight by
performance is to focus the model on its weakness
when it knows about it. The model can test itself on
its training set where the truth is known and assign
greater weights to sentences it parses incorrectly.
In our experiment, weights are updated as follows:
the initial weight for a sentence is its count; and if
the human annotation of a selected sentence differs
from the current model output, its weight is multi-
plied by . We did not experiment more compli-
cated weighting scheme (like AdaBoost) since we
only want to see if weighting has any effect on ac-
tive learning result.
Figure 5 and Figure 6 are learning curves when se-
lected samples are weighted by density and performance,
which are described in Section 4.2.
</bodyText>
<figure confidence="0.535077">
Accuracy of Sample Selection(Weighted by Density)
Number of Sentences Selected
</figure>
<figureCaption confidence="0.9644755">
Figure 5: Active learning curve: selected sentences are
weighted by density
</figureCaption>
<bodyText confidence="0.999108666666667">
The effect of weighting samples is highlighted in Ta-
ble 1, where results are obtained after 1000 samples are
selected using the same uncertainty score , but with
different weighting schemes. Weighting samples by den-
sity leads to the best performance. Since weighting sam-
ples by density is a way to tweak sample distribution of
</bodyText>
<figureCaption confidence="0.962902">
Figure 6: Active learning curve: selected sentences are
weighted based on performance
</figureCaption>
<bodyText confidence="0.9979574">
training set toward the distribution of the entire sample
space, including unannotated sentences, it indicates that
it is important to ensure the distribution of training set
matches that of the sample space. Therefore, we believe
that clustering is a necessary and useful step.
</bodyText>
<tableCaption confidence="0.997784">
Table 1: Weighting effect
</tableCaption>
<subsectionHeader confidence="0.959739">
4.3 Effect of Clustering
</subsectionHeader>
<figureCaption confidence="0.825472">
Figure 7 compares the best learning curve using only un-
certainty score(i.e., sentence entropy in Figure 3) to select
samples with the best learning curve resulted from clus-
</figureCaption>
<bodyText confidence="0.9992605">
tering and the word entropy . It is clear that clustering
results in a better learning curve.
</bodyText>
<subsectionHeader confidence="0.999424">
4.4 Summary Result
</subsectionHeader>
<bodyText confidence="0.999810666666667">
Figure 8 shows the best active learning result compared
with that of random selection. The learning curve for ac-
tive learning is obtained using as uncertainty measure
and selected samples are weighted by density. Both ac-
tive learning and random selection are run 40 times, each
time selecting 100 samples. The horizontal line on the
graph is the performance if all 20K sentences are used. It
is remarkable to notice that active learning can use far less
samples ( usually less than one third ) to achieve the same
level of performance of random selection. And after only
about 2800 sentences are selected, the active learning re-
sult becomes very close to the best possible accuracy.
</bodyText>
<sectionHeader confidence="0.997944" genericHeader="method">
5 Previous Work
</sectionHeader>
<bodyText confidence="0.9960395">
While active learning has been studied extensively in the
context of machine learning (Cohn et al., 1996; Freund
</bodyText>
<figure confidence="0.984878666666667">
90
85
80
Accuracy(%)
75
70
65
Random Selection
HA: Change Entropy
Hw: Word Entropy
Hs: Sentence Entropy
60
100 200 300 400
500 600
700 800 900 1000
Weighting
none density performance
Test Accuracy(%)
79.8 84.3 80.7
Effect of Clustering
Number of Sentences Selected
</figure>
<figureCaption confidence="0.98673">
Figure 7: Effect of clustering: entropy-based learning
curve (in plus) vs. sample selection with clustering and
uncertainty score(in triangle).
</figureCaption>
<figure confidence="0.9903465">
Active Learning vs. Random Selection
Number of Sentences Selected
</figure>
<figureCaption confidence="0.857888666666667">
Figure 8: Active learner uses one-third (about 1300 sen-
tences) of training data to achieve similar performance to
random selection (about 4000 sentence).
</figureCaption>
<bodyText confidence="0.999417567567568">
et al., 1997), and has been applied to text classifica-
tion (McCallum and Nigam, 1998) and part-of-speech
tagging (Dagan and Engelson, 1995), there are only a
handful studies on natural language parsing (Thompson
et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson
et al., 1999) uses active learning to acquire a shift-reduce
parser, and the uncertainty of an unparseable sentence is
defined as the number of operators applied successfully
divided by the number of words. It is more natural to de-
fine uncertainty scores in our study because of the avail-
bility of parse scores. (Hwa, 2000; Hwa, 2001) is related
closely to our work in that both use entropy-based un-
certainty scores, but Hwa does not characterize the dis-
tribution of sample space. Knowing the distribution of
sample space is important since uncertainty measure, if
used alone for sample selection, will be likely to select
outliers. (Stolcke, 1998) used an entropy-based criterion
to reduce the size of backoff n-gram language models.
The major contribution of this paper is that a model-
based distance measure is proposed and used in active
learning. The distance measures structural difference of
two sentences relative to an existing model. Similar idea
is also exploited in (McCallum and Nigam, 1998) where
authors use the divergence between the unigram word
distributions of two documents to measure their differ-
ence. This distance enables us to cluster the active train-
ing set and a sample is then selected and weighted based
on both its uncertainty score and its density. (Sarkar,
2001) applied co-training to statistical parsing, where two
component models are trained and the most confident
parsing outputs of the existing model are incorporated
into the next training. This is a different venue for reduc-
ing annotation work in that the current model output is
directly used and no human annotation is assumed. (Luo
et al., 1999; Luo, 2000) also aimed to making use of unla-
beled data to improve statistical parsers by transforming
model parameters.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999984238095238">
We have examined three entropy-based uncertainty
scores to measure the “usefulness” of a sample to im-
proving a statistical model. We also define a distance for
sentences of natural languages. Based on this distance,
we are able to quantify concepts such as sentence density
and homogeneity of a corpus. Sentence clustering algo-
rithms are also developed with the help of these concepts.
Armed with uncertainty scores and sentence clusters, we
have developed sample selection algorithms which has
achieved significant savings in terms of labeling cost: we
have shown that we can use one-third of training data of
random selection and reach the same level of parsing ac-
curacy.
While we have shown the importance of both con-
fidence score and modeling the distribution of sample
space, it is not clear whether or not it is the best way to
combine or reconcile the two. It would be nice to have a
single number to rank candidate sentences. We also want
to test the algorithms developed here on other domains
(e.g., Wall Street Journal corpus). Improving speed of
sentence clustering is also worthwhile.
</bodyText>
<sectionHeader confidence="0.999319" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998512">
We thank Kishore Papineni and Todd Ward for many use-
ful discussions. The anonymous reviewer’s suggestions
to improve the paper is greatly appreciated. This work is
partially supported by DARPA under SPAWAR contract
number N66001-99-2-8916.
</bodyText>
<figure confidence="0.980509956521739">
90
85
80
Accuracy(%)
75
70
65
Word Entropy(Hw)
Use sentence entropy only
500 1000 1500 2000 2500
60
60
500 1000 1500 2000 2500 3000 3500 4000
90
85
80
70
65
Word Entropy(Hw), weighted by density
Random Selection
Use 20k Samples
Accuracy(q)
75
</figure>
<sectionHeader confidence="0.957202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999307789473684">
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classfication And Regres-
sion Trees. Wadsworth Inc.
P.F Brown, V.J.Della Pietra, P.V. deSouza, J.C Lai, and
R.L. Mercer. 1992. Class-based n-gram models of
natural language. Computational Linguistics, 18:467–
480.
E. Charniak. 1997. Statistical parsing with context-free
grammar and word statistics. In Proceedings of the
14th National Conference on Artificial Intelligence.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models. J.
of Artificial Intelligence Research, 4:129–145.
Michael Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. Annual Meet-
ing of the Association for Computational Linguistics,
pages 184–191.
I. Dagan and S. Engelson. 1995. Committee-based sam-
pling for training probabilistic classifiers. In ICML.
DARPA Communicator Website. 2000.
http://fofoca.mitre.org.
K. Davies et al. 1999. The IBM conversational tele-
phony system for financial applications. In Proc. of
EuroSpeech, volume I, pages 275–278.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naf-
tali Tishby. 1997. Selective sampling using query by
committee algorithm. Machine Leanring, 28:133–168.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Proc. EMNLP/VLC, pages
45–52.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Proc. Computational Natu-
ral Language Learning Workshop. Morgan Kaufmann,
San Francisco, CA.
F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Rat-
naparkhi, and S. Roukos. 1994. Decision tree parsing
using a hidden derivation model. In Proc. Human Lan-
guage Technology Workshop, pages 272–277.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. MIT Press.
X. Luo, S. Roukos, and T. Ward. 1999. Unsupervised
adaptation of statistical parsers based on Markov trans-
form. In Proc. IEEE Workshop on Automatic Speech
Recognition and Understanding.
Xiaoqiang Luo, Salim Roukos, and Min Tang. 2002. Ac-
tive learning for statistical parsing. Technical report,
IBM Research Report.
X. Luo. 2000. Parser adaptation via Householder trans-
form. In Proc. ICASSP.
Andrew McCallum and Kamal Nigam. 1998. Employ-
ing EM and pool-based active learning for text clas-
sification. In Machine Learning: Proceedings of the
Fifteenth International Conference (ICML ’98), pages
359–367.
L. R. Rabiner and B. H. Juang. 1993. Fundamentals of
Speech Recognition. Prentice-Hall, Englewood Cliffs,
NJ.
Adwait Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy Mod-
els. In Claire Cardie and Ralph Weischedel, editors,
Second Conference on Empirical Methods in Natural
Language Processing, pages 1 – 10, Providence, R.I.,
Aug. 1–2.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of the Second Meet-
ing of the North American Chapter of the Association
for Computational Linguistics.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In Broadcast News Transcription
and Understanding Workshop, Lansdowne, Virginia.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Proc.
International Conf. on Machine Learning, pages
❞
406–414. Morgan Kaufmann, San Francisco, CA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931071">
<title confidence="0.999953">Active Learning for Statistical Natural Language Parsing</title>
<author confidence="0.99958">Min Tang</author>
<affiliation confidence="0.9981395">Spoken Language Systems Group MIT Laboratory for Computer Science</affiliation>
<address confidence="0.999493">Cambridge, Massachusetts 02139, USA</address>
<email confidence="0.999732">mtang@sls.lcs.mit.edu</email>
<author confidence="0.997678">Xiaoqiang Luo Salim Roukos</author>
<affiliation confidence="0.999955">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.958643">Yorktown Heights, NY 10598</address>
<email confidence="0.999562">xiaoluo,roukos@us.ibm.com</email>
<abstract confidence="0.999175961538461">It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus. Sample selection for annotation is based upon “representativeness” and “usefulness”. A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome H Friedman</author>
<author>Richard A Olshen</author>
<author>Charles J Stone</author>
</authors>
<title>Classfication And Regression Trees.</title>
<date>1984</date>
<publisher>Wadsworth Inc.</publisher>
<contexts>
<context position="7580" citStr="Breiman et al., 1984" startWordPosition="1264" endWordPosition="1267">and so on and so forth. We define an event as a parse action together with its context. It is clear that the BULM derivation converts a parse tree into a unique sequence of parse events, and a valid event sequence corresponds to a unique parse tree. Therefore a parse tree can be equivalently represented by a sequence of events. Let be the set of tagging actions, be the labeling actions and be the extending actions of, and let be the sequence of actions ahead of the action, then can be rewritten as: (1) Note that . The three models (1) can be trained using decision trees (Jelinek et al., 1994; Breiman et al., 1984). Note that raw context space is too huge to store and manipulate efficiently. In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions. Answers of each question are represented as bitstrings. To support questions like “what is the previous word (or tag, label, extension)?”, word, tag, label and extension vocabularies are all encoded as bitstrings. Words are encoded through an automatic clustering algorithm (Brown et al., 1992) while tags, labels and extensions are normally encoded using diagonal bits. An example can be found in (Luo et a</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Classfication And Regression Trees. Wadsworth Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>P V deSouza</author>
<author>J C Lai</author>
<author>R L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>480</pages>
<contexts>
<context position="8067" citStr="Brown et al., 1992" startWordPosition="1338" endWordPosition="1341"> rewritten as: (1) Note that . The three models (1) can be trained using decision trees (Jelinek et al., 1994; Breiman et al., 1984). Note that raw context space is too huge to store and manipulate efficiently. In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions. Answers of each question are represented as bitstrings. To support questions like “what is the previous word (or tag, label, extension)?”, word, tag, label and extension vocabularies are all encoded as bitstrings. Words are encoded through an automatic clustering algorithm (Brown et al., 1992) while tags, labels and extensions are normally encoded using diagonal bits. An example can be found in (Luo et al., 2002). In summary, a parse tree can be represented uniquely by a sequence of events, while each event can in turn be represented as a bitstring. With this in mind, we are now ready to define a structural distance for two sentences given an existing model. 2.2 Sentence Distance Recall that it is assumed that there is a statistical parser trained with a small amount of annotated data. To infer structures of two sentencesand, we use S (17) (16) (12) LOC (9) (15) LOC (6) (8) wd wd c</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>P.F Brown, V.J.Della Pietra, P.V. deSouza, J.C Lai, and R.L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467– 480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1549" citStr="Charniak, 1997" startWordPosition="231" endWordPosition="232"> the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. 1 Introduction A prerequisite for building statistical parsers (Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classification(McCallum and Nigam, 1998) an</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with context-free grammar and word statistics. In Proceedings of the 14th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>J. of Artificial Intelligence Research,</journal>
<pages>4--129</pages>
<contexts>
<context position="22508" citStr="Cohn et al., 1996" startWordPosition="3815" endWordPosition="3818">s are weighted by density. Both active learning and random selection are run 40 times, each time selecting 100 samples. The horizontal line on the graph is the performance if all 20K sentences are used. It is remarkable to notice that active learning can use far less samples ( usually less than one third ) to achieve the same level of performance of random selection. And after only about 2800 sentences are selected, the active learning result becomes very close to the best possible accuracy. 5 Previous Work While active learning has been studied extensively in the context of machine learning (Cohn et al., 1996; Freund 90 85 80 Accuracy(%) 75 70 65 Random Selection HA: Change Entropy Hw: Word Entropy Hs: Sentence Entropy 60 100 200 300 400 500 600 700 800 900 1000 Weighting none density performance Test Accuracy(%) 79.8 84.3 80.7 Effect of Clustering Number of Sentences Selected Figure 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to ra</context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1996. Active learning with statistical models. J. of Artificial Intelligence Research, 4:129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="1513" citStr="Collins, 1996" startWordPosition="227" endWordPosition="228">ution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. 1 Introduction A prerequisite for building statistical parsers (Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classif</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. Annual Meeting of the Association for Computational Linguistics, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In ICML. DARPA Communicator Website.</booktitle>
<note>http://fofoca.mitre.org.</note>
<contexts>
<context position="23285" citStr="Dagan and Engelson, 1995" startWordPosition="3934" endWordPosition="3937">1000 Weighting none density performance Test Accuracy(%) 79.8 84.3 80.7 Effect of Clustering Number of Sentences Selected Figure 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to random selection (about 4000 sentence). et al., 1997), and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995), there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample s</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>I. Dagan and S. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In ICML. DARPA Communicator Website. 2000. http://fofoca.mitre.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Davies</author>
</authors>
<title>The IBM conversational telephony system for financial applications.</title>
<date>1999</date>
<booktitle>In Proc. of EuroSpeech, volume I,</booktitle>
<pages>275--278</pages>
<marker>Davies, 1999</marker>
<rawString>K. Davies et al. 1999. The IBM conversational telephony system for financial applications. In Proc. of EuroSpeech, volume I, pages 275–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>H Sebastian Seung</author>
<author>Eli Shamir</author>
<author>Naftali Tishby</author>
</authors>
<title>Selective sampling using query by committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Leanring,</booktitle>
<pages>28--133</pages>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali Tishby. 1997. Selective sampling using query by committee algorithm. Machine Leanring, 28:133–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In Proc. EMNLP/VLC,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="2209" citStr="Hwa, 2000" startWordPosition="335" endWordPosition="336"> sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classification(McCallum and Nigam, 1998) and natural language parsing(Thompson et al., 1999; Hwa, 2000), to name a few. The basic idea is to couple tightly knowledge acquisition, e.g., annotating sentences for parsing, with model-training, as opposed to treating them separately. In our setup, we assume that a small amount of annotated sentences is initially available, which is used to build a statistical parser. We also assume that there is a large corpus of unannotated sentences at our disposal – this corpus is called active training set. A batch of samples&apos; is selected using algorithms developed here, and are annotated by human beings and are then added to training data to rebuild the model. </context>
<context position="23386" citStr="Hwa, 2000" startWordPosition="3953" endWordPosition="3954">cted Figure 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to random selection (about 4000 sentence). et al., 1997), and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995), there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample space. Knowing the distribution of sample space is important since uncertainty measure, if used alone </context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In Proc. EMNLP/VLC, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>On minimizing training corpus for parser acquisition.</title>
<date>2001</date>
<booktitle>In Proc. Computational Natural Language Learning Workshop.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="23398" citStr="Hwa, 2001" startWordPosition="3955" endWordPosition="3956"> 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to random selection (about 4000 sentence). et al., 1997), and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995), there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample space. Knowing the distribution of sample space is important since uncertainty measure, if used alone for sample s</context>
</contexts>
<marker>Hwa, 2001</marker>
<rawString>Rebecca Hwa. 2001. On minimizing training corpus for parser acquisition. In Proc. Computational Natural Language Learning Workshop. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision tree parsing using a hidden derivation model.</title>
<date>1994</date>
<booktitle>In Proc. Human Language Technology Workshop,</booktitle>
<pages>272--277</pages>
<contexts>
<context position="1498" citStr="Jelinek et al., 1994" startWordPosition="223" endWordPosition="226">zes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. 1 Introduction A prerequisite for building statistical parsers (Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999</context>
<context position="5393" citStr="Jelinek et al., 1994" startWordPosition="875" endWordPosition="878">nce, even if they are lexically different. This leads us to define the distance between two sentences based on their parse trees, which are obtained by applying an existing model to the active training set. However, computing the distance of two parse trees requires a digression of how they are represented in our parser. 2.1 Event Representation of Parse Trees A statistical parser computes , the probability of a parse given a sentence. Since the space of the entire parses is too large and cannot be modeled directly, a parse tree is decomposed as a series of individual actions . In the parser (Jelinek et al., 1994) we used in this study, this is accomplished through a bottomup-left-most (BULM) derivation. In the BULM derivation, there are three types of parse actions: tag, label and extension. There is a corresponding vocabulary for tag or label, and there are four extension directions: RIGHT, LEFT, UP and UNIQUE. If a child node is the only node under a label, the child node is said to extend UNIQUE to its parent node; if there are multiple children under a parent node, the left-most child is said to extend RIGHT to the parent node, the right-most child node is said to extend LEFT to the parent node, w</context>
<context position="7557" citStr="Jelinek et al., 1994" startWordPosition="1260" endWordPosition="1263">o proceeding actions, and so on and so forth. We define an event as a parse action together with its context. It is clear that the BULM derivation converts a parse tree into a unique sequence of parse events, and a valid event sequence corresponds to a unique parse tree. Therefore a parse tree can be equivalently represented by a sequence of events. Let be the set of tagging actions, be the labeling actions and be the extending actions of, and let be the sequence of actions ahead of the action, then can be rewritten as: (1) Note that . The three models (1) can be trained using decision trees (Jelinek et al., 1994; Breiman et al., 1984). Note that raw context space is too huge to store and manipulate efficiently. In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions. Answers of each question are represented as bitstrings. To support questions like “what is the previous word (or tag, label, extension)?”, word, tag, label and extension vocabularies are all encoded as bitstrings. Words are encoded through an automatic clustering algorithm (Brown et al., 1992) while tags, labels and extensions are normally encoded using diagonal bits. An example ca</context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Ratnaparkhi, and S. Roukos. 1994. Decision tree parsing using a hidden derivation model. In Proc. Human Language Technology Workshop, pages 272–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10618" citStr="Jelinek, 1997" startWordPosition="1793" endWordPosition="1794">rent types. We chooseto be the number of bits in to emphasize the importance of parsing actions in distance computation. Formally, let be the type of action, then The distance makes it possible to characterize how dense a sentence is. Given a set of sentences , the density of sampleis defined as: That is, the sample density is defined as the inverse of its average distance to other samples. We also define the centroid2 of S as argmax (8) 2.3 K-Means Clustering With the model-based distance measure defined above, we can use the K-means algorithm to cluster sentences. A sketch of the algorithm (Jelinek, 1997) is as follows. Let be the set of sentences to be clustered. 1. Initialization. Partition into k initial clusters ( ). Let . 2. Find the centroidfor each collection , that is: argmin 3. Re-partition into clusters , where 4. Let . Repeat Step 2 and Step 3 untill the algorithm converges (e.g., relative change of the total distortion is smaller than a threshold). For each iteration we need to compute: where is the Hamming distance, and if if Y( ) = Y( ) if Y( Y( ). Computing the editing distance (3) requires dynamic programming and it is computationally extensive. To speed up computation, we can </context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>S Roukos</author>
<author>T Ward</author>
</authors>
<title>Unsupervised adaptation of statistical parsers based on Markov transform.</title>
<date>1999</date>
<booktitle>In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="25046" citStr="Luo et al., 1999" startWordPosition="4221" endWordPosition="4224">e the divergence between the unigram word distributions of two documents to measure their difference. This distance enables us to cluster the active training set and a sample is then selected and weighted based on both its uncertainty score and its density. (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. This is a different venue for reducing annotation work in that the current model output is directly used and no human annotation is assumed. (Luo et al., 1999; Luo, 2000) also aimed to making use of unlabeled data to improve statistical parsers by transforming model parameters. 6 Conclusions and Future Work We have examined three entropy-based uncertainty scores to measure the “usefulness” of a sample to improving a statistical model. We also define a distance for sentences of natural languages. Based on this distance, we are able to quantify concepts such as sentence density and homogeneity of a corpus. Sentence clustering algorithms are also developed with the help of these concepts. Armed with uncertainty scores and sentence clusters, we have de</context>
</contexts>
<marker>Luo, Roukos, Ward, 1999</marker>
<rawString>X. Luo, S. Roukos, and T. Ward. 1999. Unsupervised adaptation of statistical parsers based on Markov transform. In Proc. IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
<author>Min Tang</author>
</authors>
<title>Active learning for statistical parsing.</title>
<date>2002</date>
<tech>Technical report, IBM Research Report.</tech>
<contexts>
<context position="8189" citStr="Luo et al., 2002" startWordPosition="1359" endWordPosition="1362">., 1984). Note that raw context space is too huge to store and manipulate efficiently. In our implementation, contexts are internally represented as bitstrings through a set of pre-designed questions. Answers of each question are represented as bitstrings. To support questions like “what is the previous word (or tag, label, extension)?”, word, tag, label and extension vocabularies are all encoded as bitstrings. Words are encoded through an automatic clustering algorithm (Brown et al., 1992) while tags, labels and extensions are normally encoded using diagonal bits. An example can be found in (Luo et al., 2002). In summary, a parse tree can be represented uniquely by a sequence of events, while each event can in turn be represented as a bitstring. With this in mind, we are now ready to define a structural distance for two sentences given an existing model. 2.2 Sentence Distance Recall that it is assumed that there is a statistical parser trained with a small amount of annotated data. To infer structures of two sentencesand, we use S (17) (16) (12) LOC (9) (15) LOC (6) (8) wd wd city city wd city 1 3 5 7 11 (10) (2) (4) (14) 13 (7) to decodeandand get their most likely parse trees and. The distance b</context>
</contexts>
<marker>Luo, Roukos, Tang, 2002</marker>
<rawString>Xiaoqiang Luo, Salim Roukos, and Min Tang. 2002. Active learning for statistical parsing. Technical report, IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>Parser adaptation via Householder transform.</title>
<date>2000</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="25058" citStr="Luo, 2000" startWordPosition="4225" endWordPosition="4226">etween the unigram word distributions of two documents to measure their difference. This distance enables us to cluster the active training set and a sample is then selected and weighted based on both its uncertainty score and its density. (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. This is a different venue for reducing annotation work in that the current model output is directly used and no human annotation is assumed. (Luo et al., 1999; Luo, 2000) also aimed to making use of unlabeled data to improve statistical parsers by transforming model parameters. 6 Conclusions and Future Work We have examined three entropy-based uncertainty scores to measure the “usefulness” of a sample to improving a statistical model. We also define a distance for sentences of natural languages. Based on this distance, we are able to quantify concepts such as sentence density and homogeneity of a corpus. Sentence clustering algorithms are also developed with the help of these concepts. Armed with uncertainty scores and sentence clusters, we have developed samp</context>
</contexts>
<marker>Luo, 2000</marker>
<rawString>X. Luo. 2000. Parser adaptation via Householder transform. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
</authors>
<title>Employing EM and pool-based active learning for text classification.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proceedings of the Fifteenth International Conference (ICML ’98),</booktitle>
<pages>359--367</pages>
<contexts>
<context position="2146" citStr="McCallum and Nigam, 1998" startWordPosition="322" endWordPosition="326">arkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classification(McCallum and Nigam, 1998) and natural language parsing(Thompson et al., 1999; Hwa, 2000), to name a few. The basic idea is to couple tightly knowledge acquisition, e.g., annotating sentences for parsing, with model-training, as opposed to treating them separately. In our setup, we assume that a small amount of annotated sentences is initially available, which is used to build a statistical parser. We also assume that there is a large corpus of unannotated sentences at our disposal – this corpus is called active training set. A batch of samples&apos; is selected using algorithms developed here, and are annotated by human be</context>
<context position="23231" citStr="McCallum and Nigam, 1998" startWordPosition="3927" endWordPosition="3930">ntence Entropy 60 100 200 300 400 500 600 700 800 900 1000 Weighting none density performance Test Accuracy(%) 79.8 84.3 80.7 Effect of Clustering Number of Sentences Selected Figure 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to random selection (about 4000 sentence). et al., 1997), and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995), there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but </context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>Andrew McCallum and Kamal Nigam. 1998. Employing EM and pool-based active learning for text classification. In Machine Learning: Proceedings of the Fifteenth International Conference (ICML ’98), pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B H Juang</author>
</authors>
<title>Fundamentals of Speech Recognition.</title>
<date>1993</date>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="9521" citStr="Rabiner and Juang, 1993" startWordPosition="1606" endWordPosition="1609">te the distance as . Note that we assume here that and have similar “true” parses if they have similar structures under the current model . We have shown in Section 2.1 that a parse tree can be represented by a sequence of events, each of which can in turn be represented as bitstrings through answering questions. Let be the sequence representation for ( ), where , and is the context and is the parsing action of the event of the parse tree. We can define the distance between two sentencesas (3) The distance between two sequences andis computed as the editing distance using dynamic programming (Rabiner and Juang, 1993). We now describe the distance between two individual events. We take advantage of the fact that contexts can be encoded as bitstrings, and define the distance between two contexts as the Hamming distance between their bitstring representations. We further define the distance between two parsing actions as follows: it is either or a constantif two parse actions are of the same type (recall there are three types of parsing actions: tag, label and extension), and infinity if different types. We chooseto be the number of bits in to emphasize the importance of parsing actions in distance computati</context>
<context position="12006" citStr="Rabiner and Juang, 1993" startWordPosition="2036" endWordPosition="2039">e the distance between two sentences, which involves a dynamic programming process and is time-consuming. The complexity of this algorithm is, if we assume the N samples are uniformly distributed between the k clusters, approximately , or when . In our experiments and , we need to call the dynamic programming routine times each iteration! 2We constrain the centroid to be an element of the set as it is not clear how to “average” sentences. (5) (4) the distance between samplesand cluster centers To speed up, dynamic programming is constrained so that only the band surrounding the diagonal line (Rabiner and Juang, 1993) is allowed, and repeated sentences are stored as a unique copy with its count so that computation for the same sentence pair is never repeated. The latter is a quite effective for dialog systems as a sentence is often seen more than once in the training corpus. 3 Uncertainty Measures Intuitively, we would like to select samples that the current model is not doing well. The current model’s uncertainty about a sentence could be because similar sentences are under-represented in the (annotated) training set, or similar sentences are intrinsically difficult. We take advantage of the availability </context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>L. R. Rabiner and B. H. Juang. 1993. Fundamentals of Speech Recognition. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing, pages 1 – 10,</booktitle>
<location>Providence, R.I.,</location>
<contexts>
<context position="1532" citStr="Ratnaparkhi, 1997" startWordPosition="229" endWordPosition="230">ring and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method. 1 Introduction A prerequisite for building statistical parsers (Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classification(McCallum an</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Claire Cardie and Ralph Weischedel, editors, Second Conference on Empirical Methods in Natural Language Processing, pages 1 – 10, Providence, R.I., Aug. 1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24702" citStr="Sarkar, 2001" startWordPosition="4167" endWordPosition="4168">n to reduce the size of backoff n-gram language models. The major contribution of this paper is that a modelbased distance measure is proposed and used in active learning. The distance measures structural difference of two sentences relative to an existing model. Similar idea is also exploited in (McCallum and Nigam, 1998) where authors use the divergence between the unigram word distributions of two documents to measure their difference. This distance enables us to cluster the active training set and a sample is then selected and weighted based on both its uncertainty score and its density. (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. This is a different venue for reducing annotation work in that the current model output is directly used and no human annotation is assumed. (Luo et al., 1999; Luo, 2000) also aimed to making use of unlabeled data to improve statistical parsers by transforming model parameters. 6 Conclusions and Future Work We have examined three entropy-based uncertainty scores to measure the “usefulness” of a sample to improv</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Broadcast News Transcription and Understanding Workshop,</booktitle>
<location>Lansdowne, Virginia.</location>
<contexts>
<context position="24058" citStr="Stolcke, 1998" startWordPosition="4063" endWordPosition="4064"> to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample space. Knowing the distribution of sample space is important since uncertainty measure, if used alone for sample selection, will be likely to select outliers. (Stolcke, 1998) used an entropy-based criterion to reduce the size of backoff n-gram language models. The major contribution of this paper is that a modelbased distance measure is proposed and used in active learning. The distance measures structural difference of two sentences relative to an existing model. Similar idea is also exploited in (McCallum and Nigam, 1998) where authors use the divergence between the unigram word distributions of two documents to measure their difference. This distance enables us to cluster the active training set and a sample is then selected and weighted based on both its uncer</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In Broadcast News Transcription and Understanding Workshop, Lansdowne, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proc. International Conf. on Machine Learning,</booktitle>
<pages>406--414</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="2099" citStr="Thompson et al., 1999" startWordPosition="317" endWordPosition="320">(Jelinek et al., 1994; Collins, 1996; Ratnaparkhi, 1997; Charniak, 1997) is the availability of a (large) corpus of parsed sentences. Acquiring such a corpus is expensive and timeconsuming and is often the bottleneck to build a parser for a new application or domain. The goal of this study is to reduce the amount of annotated sentences (and hence the development time) required for a statistical parser to achieve a satisfactory performance using active learning. Active learning has been studied in the context of many natural language processing (NLP) applications such as information extraction(Thompson et al., 1999), text classification(McCallum and Nigam, 1998) and natural language parsing(Thompson et al., 1999; Hwa, 2000), to name a few. The basic idea is to couple tightly knowledge acquisition, e.g., annotating sentences for parsing, with model-training, as opposed to treating them separately. In our setup, we assume that a small amount of annotated sentences is initially available, which is used to build a statistical parser. We also assume that there is a large corpus of unannotated sentences at our disposal – this corpus is called active training set. A batch of samples&apos; is selected using algorithm</context>
<context position="23371" citStr="Thompson et al., 1999" startWordPosition="3948" endWordPosition="3951">ing Number of Sentences Selected Figure 7: Effect of clustering: entropy-based learning curve (in plus) vs. sample selection with clustering and uncertainty score(in triangle). Active Learning vs. Random Selection Number of Sentences Selected Figure 8: Active learner uses one-third (about 1300 sentences) of training data to achieve similar performance to random selection (about 4000 sentence). et al., 1997), and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995), there are only a handful studies on natural language parsing (Thompson et al., 1999) and (Hwa, 2000; Hwa, 2001). (Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words. It is more natural to define uncertainty scores in our study because of the availbility of parse scores. (Hwa, 2000; Hwa, 2001) is related closely to our work in that both use entropy-based uncertainty scores, but Hwa does not characterize the distribution of sample space. Knowing the distribution of sample space is important since uncertainty measure,</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proc. International Conf. on Machine Learning, pages ❞ 406–414. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>