<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000044">
<title confidence="0.998982">
Finding Word Substitutions Using a Distributional Similarity Baseline
and Immediate Context Overlap
</title>
<author confidence="0.995105">
Aurelie Herbelot
</author>
<affiliation confidence="0.957802">
University of Cambridge
Computer Laboratory
J.J. Thompson Avenue
</affiliation>
<address confidence="0.542808">
Cambridge
</address>
<email confidence="0.996953">
ah433@cam.ac.uk
</email>
<sectionHeader confidence="0.993838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784235294118">
This paper deals with the task of find-
ing generally applicable substitutions for a
given input term. We show that the output
of a distributional similarity system base-
line can be filtered to obtain terms that are
not simply similar but frequently substi-
tutable. Our filter relies on the fact that
when two terms are in a common entail-
ment relation, it should be possible to sub-
stitute one for the other in their most fre-
quent surface contexts. Using the Google
5-gram corpus to find such characteris-
tic contexts, we show that for the given
task, our filter improves the precision of a
distributional similarity system from 41%
to 56% on a test set comprising common
transitive verbs.
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.8479488">
This paper looks at the task of finding word substi-
tutions for simple statements in the context of KB
querying. Let us assume that we have a knowl-
edge base made of statements of the type ‘subject
– verb – object’:
</bodyText>
<listItem confidence="0.998408333333333">
1. Bank of America – acquire – Merrill Lynch
2. Lloyd’s – buy – HBOS
3. Iceland – nationalise – Kaupthing
</listItem>
<bodyText confidence="0.999636612244898">
Let us also assume a simple querying facility,
where the user can enter a word and be presented
with all statements containing that word, in a typ-
ical search engine fashion. If we want to return all
acquisition events present in the knowledge base
above (as opposed to nationalisation events), we
might search for ‘acquire’. This will return the
first statement (about the acquisition of Merrill
Lynch) but not the second statement about HBOS.
Ideally, we would like a system able to generate
words similar to our query, so that a statement
containing the verb ‘buy’ gets returned when we
search for ‘acquire’.
This problem is closely related to the clustering
of semantically similar terms, which has received
much attention in the literature. Systems that
perform such clustering usually do so under the
assumption of distributional similarity (Harris,
1954) which state that two words appearing
in similar contexts will be close in meaning.
This observation is statistically useful and has
contributed to successful systems within two
approaches: the pattern-based approach and the
feature vector approach (we describe those two
approaches in the next section). The definition
of similarity used by those systems is fairly
wide, however. Typically, a query on the verb
‘produce’ will return verbs such as ‘export’, ‘im-
port’ or ‘sell’, for instance (see DIRT demo from
http://demo.patrickpantel.com/Content/Lex
Sem/paraphrase.htm, Lin and Pantel, 2001.)
This fairly wide notion of similarity is not fully
appropriate for our word substitutions task: al-
though cats and dogs are similar types of enti-
ties, querying a knowledge base for ‘cat’ shouldn’t
return statements about dogs; statements about
Siamese, however, should be acceptable. So, fol-
lowing Dagan and Glickman (2004), we refine our
concept of similarity as that of entailment, defined
here as the relation whereby the meaning of a word
w1 is ‘included’ in the meaning of word w2 (prac-
tically speaking, we assume that the ‘meaning’ of
a word is represented by the contexts in which it
appears and require that if w1 entails w2, the con-
texts of w2 should be a subset of the contexts of
w1). Given an input term w, we therefore attempt
to extract words which either entail or are entailed
by w. (We do not extract directionality at this
stage.)
</bodyText>
<note confidence="0.959806">
Proceedings of the EACL 2009 Student Research Workshop, pages 28–36,
Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997754">
28
</page>
<bodyText confidence="0.999972704545455">
The definition of entailment usually implies that
an entailing word must be substitutable for the en-
tailed one, in some contexts at least. Here, we con-
sider word substitution queries in cases where no
additional contextual information is given, so we
cannot assume that possible, but rare, substitutions
will fit the query intended by the user (‘believe’
correctly entails ‘buy’ in some cases but we can
be reasonably sure that the query ‘buy’ is meant
in the ‘purchase’ sense.) We thus require that our
output will fit the most common contexts. For in-
stance, given the query ‘kill’, we want to return
‘murder’ but not ‘stop’. Given ‘produce’, we want
to return both ‘release’ and ‘generate’ but not ‘fab-
ricate’ or ‘hatch’.1 Taking this into account, we
generally define substitutability as the ability of a
word to replace another one in a given sentence
without changing the meaning or acceptability of
the sentence, and this in the most frequent cases.
(By acceptability, we mean whether the sentence
is likely to be uttered by a native speaker of the
language under consideration.)
In order to achieve both entailment and general
substitutability, we propose to filter the output of
a conventional distributional similarity system us-
ing a check for lexical substitutability in frequent
contexts. The idea of the filter relies on the ob-
servation that entailing words tend to share more
frequent immediate contexts than just related ones.
For instance, when looking at the top 200 most fre-
quent Google 3-gram contexts (Brants and Franz,
2006) appearing after the terms ‘kill’, ‘murder’
and ‘abduct’, we find that ‘kill’ and ‘murder’ share
54 while ‘kill’ and ‘abduct’ only share 2, giving
us the indication that as far as usage is concerned,
‘murder’ is closer to ‘kill’ than ‘abduct’. Addi-
tionally, context frequency provides a way to iden-
tify substitutability for the most common uses of
the word, as required.
In what follows, we briefly present related
work, and introduce our corpus and algorithm, in-
cluding a discussion of our ‘immediate context
overlap’ filter. We then review the results of an
experiment on the extraction of entailment pairs
</bodyText>
<footnote confidence="0.987125888888889">
1In fact, we argue that even in systems where context is
available, searching for all entailing words is not necessary an
advantage: consider the query ‘What does Dole produce?’ to
a search engine. The verb ‘fabricate’ entails ‘produce’ in the
correct sense of the word, but because of its own polysemy,
and unless an expensive layer of WSD is added to the system,
it will return sentences such as ‘Dole fabricated stories about
her opponent’, which is clearly not the information that the
user was looking for.
</footnote>
<bodyText confidence="0.816127">
for 30 input verbs.
</bodyText>
<sectionHeader confidence="0.995135" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.9986925">
2.1 Distributional Similarity
2.1.1 Principles
</subsectionHeader>
<bodyText confidence="0.9989095">
Systems using distributional similarity usually fall
under two approaches:
</bodyText>
<listItem confidence="0.939096947368421">
1. The pattern-based approach (e.g. Ravichad-
ran and Hovy, 2002). The most significant
contexts for an input seed are extracted as
features and those features used to discover
words related to the input (under the assump-
tion that words appearing in at least one sig-
nificant context are similar to the seed word).
There is also a non-distributional strand of
this approach: it uses Hearst-like patterns
(Hearst, 1992) which are supposed to indi-
cate the presence of two terms in a certain re-
lation - most often hyponymy or meronymy
(see Chklovski and Pantel, 2004).
2. The feature vector approach (e.g. Lin and
Pantel, 2001). This method fully embraces
the definition of distributional similarity by
making the assumption that two words ap-
pearing in similar sets of features must be re-
lated.
</listItem>
<subsectionHeader confidence="0.925961">
2.1.2 Limitations
</subsectionHeader>
<bodyText confidence="0.99994765">
The problems of the distributional similarity as-
sumption are well-known: the facts that ‘a bank
lends money’ and ‘Smith’s brother lent him
money’ do not imply that banks and brothers are
similar entities. This effect becomes particularly
evident in cases where antonyms are returned by
the system; in those cases, a very high distribu-
tional similarity actually corresponds to opposite
meanings. Producing an output ranked accord-
ing to distributional similarity scores (weeding out
anything under a certain threshold) is therefore
not sufficient to retain good precisions for many
tasks. Some work has thus focused on a re-ranking
strategies (see Geffet and Dagan, 2004 and Gef-
fet and Dagan, 2005, who improve the output of a
distributional similarity system for an entailment
task using a web-based feature inclusion check,
and comment that their filtering produces better
outputs than cutting off the similarity pairs with
the lowest ranking.)
</bodyText>
<page confidence="0.997224">
29
</page>
<subsectionHeader confidence="0.999295">
2.2 Extraction Systems
</subsectionHeader>
<bodyText confidence="0.999925673913043">
Prominent entailment rule acquisition systems in-
clude DIRT (Lin and Pantel, 2001), which uses
distributional similarity on a 1 GB corpus to iden-
tify semantically similar words and expressions,
and TEASE (Szpektor et al., 2004), which ex-
tracts entailment relations from the web for a given
word by computing characteristic contexts for that
word.
Recently, systems that combine both pattern-
based and feature vector approaches have also
been presented. Lin et al. (2003) and Pantel and
Ravichandran (2004) have proposed to classify the
output of systems based on feature vectors using
lexico-syntactic patterns, respectively in order to
remove antonyms from a related words list and to
name clusters of related terms.
Even more related to our work, Mirkin et al.
(2006) integrate both approaches by constructing
features for the output of both a pattern-based and
a vector-based systems, and by filtering incorrect
entries with a supervised SVM classifier. (The
pattern-based approach uses a set of manually-
constructed patterns applied to a web search.)
In the same vein, Geffet and Dagan (2005) fil-
ter the result of a pattern-based system using fea-
ture vectors. They get their features out of an 18
million word corpus augmented by a web search.
Their idea is that for any pair of potentially simi-
lar words, the features of the entailed one should
comprise all the features of the entailing one.
The main difference between our work and the
last two quoted papers is that we add a new layer
of verification: we extract pairs of verbs using au-
tomatically derived semantic patterns, perform a
first stage of filtering using the semantic signa-
tures of each word and apply a final stage of filter-
ing relying on surface substitutability, which we
name ‘immediate context overlap’ method. We
also experiment with a smaller size corpus to pro-
duce our distributional similarity baseline (a sub-
set of Wikipedia) in an attempt to show that a good
semantic parse and adequate filtering can provide
reasonable performance even on domains where
data is sparse. Our method does not need man-
ually constructed patterns or supervised classifier
training.
</bodyText>
<subsectionHeader confidence="0.988809">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999990428571429">
The evaluation of KB or ontology extraction sys-
tems is typically done by presenting human judges
with a subset of extracted data and asking them to
annotate it according to certain correctness crite-
ria. For entailment systems, the annotation usu-
ally relies on two tests: whether the meaning of
one word entails the other one in some senses of
those words, and whether the judges can come up
with contexts in which the words are directly sub-
stitutable. Szpektor et al. (2007) point out the dif-
ficulties in applying those criteria. They note the
low inter-annotator agreements obtained in previ-
ous studies and propose a new evaluation method
based on precise judgement questions applied to
a set of relevant contexts. Using their methods,
they evaluate the DIRT (Lin and Pantel, 2001) and
TEASE (Szpektor et al., 2004) algorithms and ob-
tain upper bound precisions of 44% and 38% re-
spectively on 646 entailment rules for 30 transitive
verbs. We follow here their methodology to check
the results obtained via the traditional annotation.
</bodyText>
<sectionHeader confidence="0.988103" genericHeader="method">
3 The Data
</sectionHeader>
<bodyText confidence="0.975429433333333">
The corpus used for our distributional similar-
ity baseline consists of a subset of Wikipedia to-
talling 500 MB in size, parsed first with RASP2
(Briscoe et al., 2006) and then into a Robust Min-
imal Recursion Semantics form (RMRS, Copes-
take, 2004) using a RASP-to-RMRS converter.
The RMRS representation consists of trees (or tree
fragments when a complete parse is not possible)
which comprise, for each phrase in the sentence, a
semantic head and its arguments. For instance, in
the sentence ‘Lloyd’s rescues failing bank’, three
subtrees can be extracted:
lemma:rescue arg:ARG1 var:Lloyd’s
which indicates that ‘Lloyd’s’ is subject of the
head ‘rescue’,
lemma:rescue arg:ARG2 var:bank
which indicates that ‘bank’ is object of the head
‘rescue’, and
lemma:failing arg:ARG1 var:bank
which indicates that the argument of ‘failing’ is
‘bank’.
Note that any tree can be transformed into
a feature for a particular lexical item by re-
placing the slot containing the word with a
hole: lemma:rescue arg:ARG2 var:bank be-
comes lemma:hole arg:ARG2 var:bank, a po-
tentially characteristic context for ‘rescue’.
All the experiments reported in this paper con-
cern transitive verbs. In order to speed up
processing, we reduced the RMRS corpus to a
</bodyText>
<page confidence="0.990175">
30
</page>
<bodyText confidence="0.99969275">
list of relations with a verbal head and at least
two arguments: lemma:verb-query arg:ARG1
var:subject arg:ARG2 var:object. Note that
we did not force noun phrases in the second ar-
gument of the relations and for instance, the verb
‘say’ was both considered as taking a noun or a
clause as second argument (‘to say a word’, ‘to
say that the word is...’).
</bodyText>
<sectionHeader confidence="0.975657" genericHeader="method">
4 A Baseline
</sectionHeader>
<bodyText confidence="0.99994">
We describe here our baseline, a system based on
distributional similarity.
</bodyText>
<subsectionHeader confidence="0.995615">
4.1 Step 1 - Pattern-Based Pair Extraction
</subsectionHeader>
<bodyText confidence="0.988543222222222">
The first step of our algorithm uses a pattern-based
approach to get a list of potential entailing pairs.
For each word w presented to the system, we ex-
tract all semantic patterns containing w. Those se-
mantic patterns are RMRS subtrees consisting of a
semantic head and its children (see Section 3). We
then calculate the Pointwise Mutual Information
between each pattern p and w:
where P(p) and P(w) are the probabilities of oc-
currence of the pattern and the instance respec-
tively and P(p, w) is the probability that they ap-
pear together.
PMI is known to have a bias towards less fre-
quent events. In order to counterbalance that bias,
we apply a simple logarithm function to the results
as a discount:
d = log (cwp + 1) (2)
where cwp is the cooccurrence count of an instance
and a pattern.
We multiply the original PMI value by this dis-
count to find the final PMI. We then select the n
patterns with highest PMIs and use them as rele-
vant semantic contexts to find all terms t that also
appear in those contexts. The result of this step
is a list of potential entailment relations, w − ti
... w − t,, (we do not know the direction of the
entailment).
</bodyText>
<subsectionHeader confidence="0.933833">
4.2 Step 2 - Feature vector Comparison
</subsectionHeader>
<bodyText confidence="0.957858763636364">
This step takes the output of the pattern-based ex-
traction and applies a first filter to the potential en-
tailment pairs. The filter relies on the idea that
two words that are similar will have similar fea-
ture vectors (see Geffet and Dagan, 2005). We de-
fine here the feature vector of word w as the list of
semantic features containing w, together with the
PMI of each feature in relation to w as a weight.
For each pair of words (w1, w2) we extract the
feature vectors of both w1 and w2 and calculate
their similarity using the measure of Lin (1998).
Pairs with a similarity under a certain threshold are
weeded out. (We use 0.007 in our experiments –
the value was found by comparing precisions for
various thresholds in a set of initial experiments.)
As a check of how the Lin measure performed
on our Wikipedia subset using RMRS features,
we reproduced the Miller and Charles experi-
ment (1991) which consists in asking humans to
rate the similarity of 30 noun pairs. The experi-
ment is a standard test for semantic similarity sys-
tems (see Jarmasz and Szpakowicz, 2003; Lin,
1998; Resnik, 1995 and Hirst and St Onge, 1998
amongst others). The correlations obtained by pre-
vious systems range between the high 0.6 and the
high 0.8. Those systems rely on edge counting us-
ing manually-created resources such as WordNet
and the Roget’s Thesaurus. We are not actually
aware of results obtained on totally automated sys-
tems (apart from a baseline computed by Strube
and Ponzetto, 2006, using Google hits, which re-
turn a correlation of 0.26.)
Applying our feature vector step to the Miller
and Charles pairs, we get a correlation of 0.38,
way below the edge-counting systems. It turns out,
however, that this low result is at least partially due
to data sparsity: when ignoring the pairs contain-
ing at least one word with frequency under 200
(8 of them, which means ending up with 22 pairs
left out of the initial 30), the correlation goes up
to 0.69. This is in line with the edge-counting sys-
tems and shows that our baseline system produces
a decent approximation of human performance, as
long as enough data is supplied. 2
Two issues remain, though. First, fine-grained
results cannot be obtained over a general corpus:
we note that the pairs ‘coast-forest’ and ‘coast-
hill’ get very similar scores using distributional
similarity while the latter is ranked twice as high
as the former by humans. Secondly, distribu-
2It seems then that in order to maintain precision to a
higher level on our corpus, we could simply disregard pairs
with low-frequency words. (We decided here, however, that
this would be unacceptable from the point of view of recall
and did not attempt to do so.)
</bodyText>
<equation confidence="0.998441666666667">
pmi(p, w) = log
( P (p, w) / (1)
P (p) P (w)
</equation>
<page confidence="0.996137">
31
</page>
<bodyText confidence="0.998959555555556">
tional methods promise to identify ‘semantically
similar’ words, as do the Miller and Charles ex-
periment and edge-counting systems. However,
as pointed out in the introduction, there is still
a gap between general similarity and entailment:
‘coast’ and ‘hill’ are indeed similar in some way
but never substitutable. Our baseline is therefore
constrained by a theoretical problem that further
modules must solve.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="method">
5 Immediate Context Overlap
</sectionHeader>
<bodyText confidence="0.999987421052632">
Our immediate context overlap module acts as a
filter for the system described as our baseline. The
idea is that, out of all pairs of ‘similar’ words,
we want to find those that express entailment in
at least one direction. So for instance, given the
pairs ‘kill – murder’ and ‘kill – abduct’, we would
like to keep the former and filter the latter out. We
can roughly explain why the second pair is not ac-
ceptable by saying that, although the semantics of
the two words are close (they are both about an act
of violence conducted against somebody), they are
not substitutable in a given sentence.
To satisfy substitutability, we generally specify
that if w1 entails w2, then there should be surface
contexts where w2 can replace w1, with the substi-
tution still producing an acceptable utterance (see
our definition of acceptability in the introduction).
We further suggest that if one word can substitute
the other in frequent immediate contexts, we have
the basis to believe that entailment is possible in
at least one common sense of the words – while
if substitution is impossible or rare, we can doubt
the presence of an entailment relation, at least in
common senses of the terms. This can be made
clearer with an example. We show in Table 1 some
of the most frequent trigrams to appear after the
verbs ‘to kill’, ‘to murder’ and ‘to abduct’ (those
trigrams were collected from the Google 5-gram
corpus.) It is immediately noticeable that some
contexts are not transferable from one term to the
other: phrases such as ‘to murder and forcibly
recruit someone’, or ‘to abduct cancer cells’ are
impossible – or at least unconventional. We also
show in italic some common immediate contexts
between the three words. As pointed out in the in-
troduction, when looking at the top 200 most fre-
quent contexts for each term, we find that ‘kill’
and ‘murder’ share 54 while ‘kill’ and ‘abduct’
only share 2, giving us the indication that as far as
usage is concerned, ‘murder’ is closer to ‘kill’ than
‘abduct’. Furthermore, by looking at frequency of
occurrence, we partly answer our need to find sub-
stitutions that work in very frequent sentences of
the language.
The Google 5-gram corpus gives the frequency
of each of its n-grams, allowing us to check substi-
tutability on the 5-grams with highest occurrence
counts for each potential entailment pair returned
by our baseline. For each pair (w1, w2) we select
the m most frequent contexts for both w1 and w2
and simply count the overlap between both lists. If
there is any overlap, we keep the pair; if the over-
lap is 0, we weed it out (the low threshold helps
our recall to remain acceptable). We experiment
with left and right contexts, i.e. with the query
term at the beginning and the end of the n-gram,
and with various combinations (see Section 6).
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999992481481482">
The results in this section are produced by ran-
domly selecting 30 transitive verbs out of the 500
most frequent in our Wikipedia corpus and using
our system to extract non-directional entailment
pairs for those verbs, following a similar experi-
ment by Szpektor et al. (2007). We use a list of
n = 30 features in Step 1 of the baseline. We eval-
uate the results by first annotating them according
to a broad definition of entailment: if the annota-
tor can think of any context where one word of
the pair could replace the other, preserving sur-
face form and semantics, then the two words are
in an entailment relation. (Note again that we do
not consider the directionality of entailment at this
stage.) We then re-evaluate our best score using
the Szpektor et al. method (2007), which we think
is more suited for checking true substitutability. 3
The baseline described in Section 4 produces
301 unique pairs, 124 of which we judge correct
using our broad entailment definition, yielding a
precision of 41%. The average number of rela-
tions extracted for each input term is thus 4.1.
Tables 2 and 3 show our results at the end of
the immediate context overlap step. Table 2 re-
port results using the m = 50 most frequent con-
texts for each word in the pair while Table 3 uses
an expanded list of 200 contexts. Precision is the
</bodyText>
<footnote confidence="0.991393333333333">
3Although no direct comparison with the works
of Szpektor et al. or Lin and Pantel is provided
in this paper, we are in the process of evaluating
our results against the TEASE output (available at
http://www.cs.biu.ac.il/∼szpekti/TEASE co
llection.zip) through a web-based annotation task.
</footnote>
<page confidence="0.999089">
32
</page>
<tableCaption confidence="0.882628666666667">
Table 1: Immediate Contexts for ‘kill’, ‘murder’ and ‘abduct’
kill murder abduct
two birds with babies that life her and make
cancer cells and his wife and an innocent man
a mocking bird thousands of innocent unsuspecting people and
or die for women and children suspects in foreign
or be killed her husband and a young girl
another human being in the name and forcibly recruit
thousands of people in connection with a teenage girl
</tableCaption>
<bodyText confidence="0.972407777777778">
in the name another human being and kill her
his wife and tens of thousands a child from
members of the the royal family women and children
number of correct relations amongst all those re-
turned. Recall is calculated with regard to the 124
pairs judged correct at the end of the previous step
(i.e., this is not true recall but recall relative to the
baseline results.)
We experimented with six different set-ups:
</bodyText>
<listItem confidence="0.99378975">
1- right context: the four words following the
query term are used as context
2- left context: the four words preceding the
query term are used as context
</listItem>
<tableCaption confidence="0.998835">
Table 2: Results using 50 immediate contexts
</tableCaption>
<table confidence="0.999670428571429">
Context Used Precision Recall F Returned Correct
Left 48% 63% 54% 164 78
Right 62% 26% 36% 52 32
Left and Right 53% 52% 52% 122 65
Concatenation 48% 70% 57% 181 87
Inclusion 67% 19% 30% 36 24
Right + ‘to’ 56% 85% 68% 187 105
</table>
<tableCaption confidence="0.998072">
Table 3: Results using 200 immediate contexts
</tableCaption>
<table confidence="0.761846928571429">
3- right and left contexts: the best contexts
(those with highest frequencies) are selected
out of the concatenation of both right and left
context lists
4- concatenation: the concatenation of the re-
sults obtained from 1 and 2
5- inclusion: the inclusion set of the results from
Context Used Precision Recall F Returned Correct
Left 44% 86% 58% 244 107
Right 54% 60% 57% 137 74
Left and Right 46% 85% 60% 228 105
Concatenation 44% 92% 60% 260 114
Inclusion 55% 53% 54% 121 66
Right + ‘to’ 48% 97% 64% 248 120
</table>
<bodyText confidence="0.961943153846154">
1 and 2, that is, the pairs judged correct by
both the right context and left context meth-
ods.
6- right context with ‘to’: identical to 1 but the
5-gram is required to start with ‘to’. This
ensures that only the verb form of the query
term is considered but has the disadvantage
of effectively transforming 5-grams into 4-
grams.
Our best overall results comes from using 50
immediate contexts starting with ‘to’, right con-
text only: we obtain 56% precision on a recall of
85% calculated on the results of the previous step.
</bodyText>
<subsectionHeader confidence="0.983496">
6.1 Instance-Based Evaluation
</subsectionHeader>
<bodyText confidence="0.999694166666667">
We then recalculate our best precision following
the method introduced in Szpektor et al. (2007).
This approach consists in extracting, for each po-
tential entailment relation X-verb,-Y ==&gt;. X-verb2-
Y, 15 sentences in which verbl appears and ask
annotators to provide answers to three questions:
</bodyText>
<listItem confidence="0.98744175">
1. Is the left-hand side of the relation entailed
by the sentence? If so...
2. When replacing verb, with verb2, is the sen-
tence still likely in English? If so...
</listItem>
<page confidence="0.970981">
33
</page>
<bodyText confidence="0.740008333333333">
3. Does the sentence with verba entail the sen-
tence with verb2?
We show in Table 4 some potential annotations
at various stages of the process.
For each pair, Szpektor et al. then calculate a
lower-bound precision as
</bodyText>
<equation confidence="0.9037355">
Plb = nEntailed (3)
nLeftHandEntailed
</equation>
<bodyText confidence="0.999976142857143">
where nEntailed is the number of entailed sentence
pairs (the annotator has answered ‘yes’ to the third
question) and nLeftHandEntailed is the number of
sentences where the left-hand relation is entailed
(the annotator has answered ‘yes’ to the first ques-
tion). They also calculate an upper-bound preci-
sion as
</bodyText>
<equation confidence="0.8433095">
Pub = nEntailed (4)
nAcceptable
</equation>
<bodyText confidence="0.999971225806452">
where nAcceptable is the number of acceptable
verb2 sentences (the annotator has answered ‘yes’
to the second question). A pair is deemed to con-
tain an entailment relation if the precision for that
particular pair is over 80%.
The authors comment that a large proportion of
extracted sentences lead to a ‘left-hand side not en-
tailed’ answer. In order to counteract that effect,
we only extract sentences without modals or nega-
tion from our Wikipedia corpus and consequently
only require 10 sentences per relation (only 11%
of our sentences have a ‘non-entailed’ left-hand
side relation against 43% for Szpektor et al.).
We obtain an upper bound precision of 52%,
which is slightly lower than the one initially cal-
culated using our broad definition of entailment,
showing that the more stringent evaluation is use-
ful when checking for general substitutability in
the returned pairs. When we calculate the lower
bound precision, however, we obtain a low 10%
precision due to the large number of sentences
judged as ‘unlikely English sentences’ after sub-
stitution (they amount to 33% of all examples with
a left-hand side judged ‘entailed’). This result il-
lustrates the need for a module able to check sen-
tence acceptability when applying the system to
true substitution tasks. Fortunately, as we explain
in the next section, it also takes into account re-
quirements that are only necessary for generation
tasks, and are therefore irrelevant to our querying
task.
</bodyText>
<sectionHeader confidence="0.994664" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.997933192307692">
Our main result is that the immediate context over-
lap step dramatically increases our precision (from
41% to 56%), showing that a more stringent notion
of similarity can be achieved when adequately fil-
tering the output of a distributional similarity sys-
tem. However, it also turns out that looking at
the most frequent contexts of the word to substi-
tute does not fully solve the issue of surface ac-
ceptability (leading to a high number of ‘right-
hand side not entailed’ annotations). We argue,
though, that the issue of producing an acceptable
English sentence is a generation problem separate
from the extraction task. Some systems, in fact,
are dedicated to related problems, such as identi-
fying whether the senses of two synonyms are the
same in a particular lexical context (see Dagan et
al., 2006). As far as our needs are concerned in
the task of KB querying, we only require accurate
searching capabilities as opposed to generational
capabilities: the expansion of search terms to in-
clude impossible strings is not a problem in terms
of result.
Looking at the immediate context overlaps re-
turned for each pair by the system, we find that the
overlap (the similarity) can be situated at various
linguistic layers:
</bodyText>
<listItem confidence="0.981660083333333">
• in the semantics of the verb’s object: ‘a
new album’ is something that one would fre-
quently ‘record’ or ‘release’. The phrase
boosts the similarity score between ‘record’
and ‘release’ in their music sense.
• in the clausal information of the right context:
a context starting with a clause introduced by
‘that’ is likely to be preceded by a verb ex-
pressing cognition or discourse. The tri-gram
‘that there is’ increases the similarity of pairs
such as ‘say - argue’.
• in the prepositional information of the right
</listItem>
<bodyText confidence="0.9272469">
context: ‘about’ is the preposition of choice
after cognition verbs such as ‘think’ or ‘won-
der’. The context ‘about the future’ helps the
score of the pair ‘think - speculate’ in the cog-
nitive sense (note that ‘speculate’ in a finan-
cial sense would take the preposition ‘on’.)
Some examples of overlaps are shown in Ta-
ble 5.
We also note that the system returns a fair pro-
portion of vacuous contexts such as ‘one of the’ or
</bodyText>
<page confidence="0.998544">
34
</page>
<tableCaption confidence="0.998658">
Table 4: Annotation Examples Following the Szpektor et al. Method
</tableCaption>
<table confidence="0.7420908">
Word Pair Sentence Question 1 Question 2 Question 3
acquire – buy Lloyds acquires HBOS yes yes (Lloyds buys HBOS) yes
acquire – praise Lloyds acquires HBOS yes yes (Lloyds praises HBOS) no
acquire – spend Lloyds acquires HBOS yes no (*Lloyds spends HBOS) –
acquire – buy Lloyds may acquire HBOS no – –
</table>
<tableCaption confidence="0.955856">
Table 5: Sample of Immediate Context Overlaps Table 6: Sample of Extracted Pairs
</tableCaption>
<figure confidence="0.962716444444444">
think – speculate say – claim
describe – characterise
bring – attract
make - earn
about the future
about what the
about how the
that it is
that there is
that it was
the nature of
the effects of
it as a
that they were the effect of
that they have the role of
that it has the quality of
the impact of
the dynamics of
*name - delegate
offer - provide
*perform - discharge
describe – characterise produce – release
develop – generate record – count
*release – announce
*remain – comprise
*call – form
change – alter
create – generate
*do – behave
feature – boast
*find – indicate
follow – adopt
require – demand
say – claim
*grow – contract
tell – assure
</figure>
<bodyText confidence="0.976145064516129">
‘part of the’ which contribute to the score of many
pairs. Our precision would probably benefit from
excluding such contexts.
We note that as expected, using a larger set of
contexts leads to better recall and decreased pre-
cision. The best precision is obtained by return-
ing the inclusion set of both left and right contexts
results, but at a high cost in recall. Interestingly,
we find that the right context of the verb is far
more telling than the left one (potentially, objects
are more important than subjects). This is in line
with results reported by Alfonseca and Manandhar
(2002).
Our best results yield an average of 3.4 relations
for each input term. It is in the range reported
by the authors of the TEASE system (Szpektor et
al., 2004) but well below the extrapolated figures
of over 20 relations in Szpektor et al., 2007. We
point out, however, that we only search for sin-
gle word substitutions, as opposed to single and
multi-word substitutions for Szpektor et al.. Fur-
thermore, our experiments are performed on 500
MB of text only, against 1 GB of news data for
the DIRT system and the web for the TEASE al-
gorithm. More data may help our recall, as well as
bootstrapping over our best precision system.
We show a sample of our results in Table 6. The
pairs with an asterisk were considered incorrect at
human evaluation stage.
*increase - decline
leave - abandon
</bodyText>
<sectionHeader confidence="0.996086" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.963809692307692">
We have presented here a system for the extrac-
tion of word substitutions in the context of KB
querying. We have shown that the output of a
distributional similarity baseline can be improved
by filtering it using the idea that two words in an
entailment relation are substitutable in immediate
surface contexts. We obtained a precision of 56%
(52% using our most stringent evaluation) on a test
set of 30 transitive verbs, and a yield of 3.4 rela-
tions per verb.
We also point out that relatively good precisions
can be obtained on a parsed medium-sized corpus
of 500 MB, although recall is certainly affected.
We note that our current implementation does
not always satisfy the requirement for substi-
tutability for generation tasks and point out that
the system is therefore limited to our intended use,
which involves search capabilities only.
We would like to concentrate in the future on
providing a direction for the entailment pairs ex-
tracted by the system. We also hope that recall
could possibly improve using a larger set of fea-
tures in the pattern-based step (this is suggested
also by Szpektor et al., 2004), together with ap-
think – believe
*use – abandon
</bodyText>
<page confidence="0.979007">
35
</page>
<bodyText confidence="0.76575">
propriate bootstrapping.
</bodyText>
<sectionHeader confidence="0.956766" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999917666666667">
This work was supported by the UK Engineer-
ing and Physical Sciences Research Council (EP-
SRC: EP/P502365/1). I would also like to thank
my supervisor, Dr Ann Copestake, for her support
throughout this project, as well as the anonymous
reviewers who commented on this paper.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882652173913">
Enrique Alfonseca and Suresh Manandhar. 2002. Ex-
tending a Lexical Ontology by a Combination of
Distributional Semantics Signatures. In Proceed-
ings of EKAW 2002, pp. 1–7, 2002.
Thorsten Brants and Alex Franz. 2006. Web 1T5-gram
Version 1. Linguistic Data Consortium, Philadel-
phia, 2006.
Edward Briscoe, John Carroll and Rebecca Watson.
2006. The Second Release of the RASP System. In
Proceedings of the COLING/ACL 2006 Interactive
Presentation Sessions, Sydney, Australia, 2006.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining The Web for Fine-Grained Se-
mantic Verb Relations. Proceedings of EMNLP-04,
Barcelona, Spain, 2004.
Ann Copestake. 2004. Ro-
bust Minimal Recursion Semantics.
www.cl.cam.ac.uk/∼aac10/papers/rmrs
draft.pdf.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic Textual Entailment: Generic Applied Modelling
of Language Variability. Proceedings of The PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France, 2004.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein and Carlo Strapparava. 2006. Direct
Word Sense Matching for Lexical Substitution. Pro-
ceedings of COLING-ACL 2006, 17-21 Jul 2006,
Sydney, Australia.
Maayan Geffet and Ido Dagan. 2004. Feature Vector
Quality and Distributional Similarity. Proceedings
Of the 20th International Conference on Computa-
tional Linguistics, 2004.
Maayan Geffet and Ido Dagan. 2005. The Distri-
butional Inclusion Hypothesises and Lexical Entail-
ment. In Proceedings Of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pp. 107–114, 2005.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s
Thesaurus and Semantic Similarity. In Proceedings
of International Conference RANLP–03, pp. 212–
219, 2003.
Zelig Harris. Distributional Structure. In Word, 10,
No. 2–3, pp. 146–162, 1954.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. Proceedings of
COLING-92, pp.539–545, 1992.
Graeme Hirst and David St-Onge. 1998. Lexical
Chains As Representations of Context for the Detec-
tion and Correction of Malapropisms. In ‘WordNet’,
Ed. Christiane Fellbaum, Cambridge, MA: The MIT
Press, 1998.
Dekang Lin. 2003. An Information-Theoretic Defini-
tion of Similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning, pp. 296–
304, 1998.
Dekang Lin, Shaojun Zhao, Lijuan Qin and Ming
Zhou. 2003. Identifying Synonyms among Distribu-
tionally Similar Words. In Proceedings of IJCAI-03,
Acapulco, Mexico, 2003.
Dekang Lin and Patrick Pantel. 2001. DIRT – Discov-
ery of Inference Rules from Text. In Proceedings of
ACM 2001, 2001.
George Miller and Walter Charles. 2001. Contextual
Correlates of Semantic Similarity. In Language and
Cognitive Processes, 6(1), pp. 1–28, 1991.
Shachar Mirkin, Ido Dagan and Maayan Geffet. 2004.
Integrating Pattern-Based and Distributional Simi-
larity Methods for Lexical Entailment Acquisition.
In Proceedings of COLING/ACL, Sydney, Aus-
tralia, pp.579–586, 2006.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labelling Semantic Classes. In Proceed-
ings of HLT/NAACL04, Boston, MA, pp 321328,
2004.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing Surface Text Patterns for a Question Answering
System. Proceedings of ACL, 2002.
Philip Resnik. 1995. Using Information Content to
Evaluate Semantic Similarity in a Taxonomy. In
Proceedings of IJCAI–95, 1995.
Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaven-
tura Coppola. 2004. Scaling Web-Based Acquisition
ofEntailment Relations. In Proceedings of EMNLP–
2004, pp. 41–48, 2004.
Idan Szpektor, Eyal Shnarch and Ido Dagan. 2007.
Instance-Based Evaluation of Entailment Rule Ac-
quisition. In Proceedings of ACL–07, 2007.
Michael Strube and Simone Ponzetto. 2006. WikiRe-
late! Computing Semantic Relatedness Using
Wikipedia. In Proceedings of AAAI–06, pp. 1219–
1224, 2006.
</reference>
<page confidence="0.998942">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685834">
<title confidence="0.9776375">Finding Word Substitutions Using a Distributional Similarity Baseline and Immediate Context Overlap</title>
<author confidence="0.985944">Aurelie Herbelot</author>
<affiliation confidence="0.964725666666667">University of Cambridge Computer Laboratory J.J. Thompson Avenue</affiliation>
<address confidence="0.831333">Cambridge</address>
<email confidence="0.995399">ah433@cam.ac.uk</email>
<abstract confidence="0.999521388888889">This paper deals with the task of finding generally applicable substitutions for a given input term. We show that the output of a distributional similarity system baseline can be filtered to obtain terms that are not simply similar but frequently substitutable. Our filter relies on the fact that when two terms are in a common entailment relation, it should be possible to substitute one for the other in their most frequent surface contexts. Using the Google 5-gram corpus to find such characteristic contexts, we show that for the given task, our filter improves the precision of a distributional similarity system from 41% to 56% on a test set comprising common transitive verbs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Suresh Manandhar</author>
</authors>
<title>Extending a Lexical Ontology by a Combination of Distributional Semantics Signatures.</title>
<date>2002</date>
<booktitle>In Proceedings of EKAW 2002,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="30753" citStr="Alfonseca and Manandhar (2002)" startWordPosition="5209" endWordPosition="5212">ay – claim *grow – contract tell – assure ‘part of the’ which contribute to the score of many pairs. Our precision would probably benefit from excluding such contexts. We note that as expected, using a larger set of contexts leads to better recall and decreased precision. The best precision is obtained by returning the inclusion set of both left and right contexts results, but at a high cost in recall. Interestingly, we find that the right context of the verb is far more telling than the left one (potentially, objects are more important than subjects). This is in line with results reported by Alfonseca and Manandhar (2002). Our best results yield an average of 3.4 relations for each input term. It is in the range reported by the authors of the TEASE system (Szpektor et al., 2004) but well below the extrapolated figures of over 20 relations in Szpektor et al., 2007. We point out, however, that we only search for single word substitutions, as opposed to single and multi-word substitutions for Szpektor et al.. Furthermore, our experiments are performed on 500 MB of text only, against 1 GB of news data for the DIRT system and the web for the TEASE algorithm. More data may help our recall, as well as bootstrapping o</context>
</contexts>
<marker>Alfonseca, Manandhar, 2002</marker>
<rawString>Enrique Alfonseca and Suresh Manandhar. 2002. Extending a Lexical Ontology by a Combination of Distributional Semantics Signatures. In Proceedings of EKAW 2002, pp. 1–7, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T5-gram Version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia,</location>
<contexts>
<context position="5235" citStr="Brants and Franz, 2006" startWordPosition="853" endWordPosition="856">d this in the most frequent cases. (By acceptability, we mean whether the sentence is likely to be uttered by a native speaker of the language under consideration.) In order to achieve both entailment and general substitutability, we propose to filter the output of a conventional distributional similarity system using a check for lexical substitutability in frequent contexts. The idea of the filter relies on the observation that entailing words tend to share more frequent immediate contexts than just related ones. For instance, when looking at the top 200 most frequent Google 3-gram contexts (Brants and Franz, 2006) appearing after the terms ‘kill’, ‘murder’ and ‘abduct’, we find that ‘kill’ and ‘murder’ share 54 while ‘kill’ and ‘abduct’ only share 2, giving us the indication that as far as usage is concerned, ‘murder’ is closer to ‘kill’ than ‘abduct’. Additionally, context frequency provides a way to identify substitutability for the most common uses of the word, as required. In what follows, we briefly present related work, and introduce our corpus and algorithm, including a discussion of our ‘immediate context overlap’ filter. We then review the results of an experiment on the extraction of entailme</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T5-gram Version 1. Linguistic Data Consortium, Philadelphia, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The Second Release of the RASP System.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="11654" citStr="Briscoe et al., 2006" startWordPosition="1899" endWordPosition="1902">evious studies and propose a new evaluation method based on precise judgement questions applied to a set of relevant contexts. Using their methods, they evaluate the DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004) algorithms and obtain upper bound precisions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs. We follow here their methodology to check the results obtained via the traditional annotation. 3 The Data The corpus used for our distributional similarity baseline consists of a subset of Wikipedia totalling 500 MB in size, parsed first with RASP2 (Briscoe et al., 2006) and then into a Robust Minimal Recursion Semantics form (RMRS, Copestake, 2004) using a RASP-to-RMRS converter. The RMRS representation consists of trees (or tree fragments when a complete parse is not possible) which comprise, for each phrase in the sentence, a semantic head and its arguments. For instance, in the sentence ‘Lloyd’s rescues failing bank’, three subtrees can be extracted: lemma:rescue arg:ARG1 var:Lloyd’s which indicates that ‘Lloyd’s’ is subject of the head ‘rescue’, lemma:rescue arg:ARG2 var:bank which indicates that ‘bank’ is object of the head ‘rescue’, and lemma:failing a</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Edward Briscoe, John Carroll and Rebecca Watson. 2006. The Second Release of the RASP System. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining The Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>Proceedings of EMNLP-04,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="7075" citStr="Chklovski and Pantel, 2004" startWordPosition="1156" endWordPosition="1159">sing distributional similarity usually fall under two approaches: 1. The pattern-based approach (e.g. Ravichadran and Hovy, 2002). The most significant contexts for an input seed are extracted as features and those features used to discover words related to the input (under the assumption that words appearing in at least one significant context are similar to the seed word). There is also a non-distributional strand of this approach: it uses Hearst-like patterns (Hearst, 1992) which are supposed to indicate the presence of two terms in a certain relation - most often hyponymy or meronymy (see Chklovski and Pantel, 2004). 2. The feature vector approach (e.g. Lin and Pantel, 2001). This method fully embraces the definition of distributional similarity by making the assumption that two words appearing in similar sets of features must be related. 2.1.2 Limitations The problems of the distributional similarity assumption are well-known: the facts that ‘a bank lends money’ and ‘Smith’s brother lent him money’ do not imply that banks and brothers are similar entities. This effect becomes particularly evident in cases where antonyms are returned by the system; in those cases, a very high distributional similarity ac</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining The Web for Fine-Grained Semantic Verb Relations. Proceedings of EMNLP-04, Barcelona, Spain, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Robust Minimal Recursion Semantics.</title>
<date>2004</date>
<note>www.cl.cam.ac.uk/∼aac10/papers/rmrs draft.pdf.</note>
<contexts>
<context position="11734" citStr="Copestake, 2004" startWordPosition="1914" endWordPosition="1916">ns applied to a set of relevant contexts. Using their methods, they evaluate the DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004) algorithms and obtain upper bound precisions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs. We follow here their methodology to check the results obtained via the traditional annotation. 3 The Data The corpus used for our distributional similarity baseline consists of a subset of Wikipedia totalling 500 MB in size, parsed first with RASP2 (Briscoe et al., 2006) and then into a Robust Minimal Recursion Semantics form (RMRS, Copestake, 2004) using a RASP-to-RMRS converter. The RMRS representation consists of trees (or tree fragments when a complete parse is not possible) which comprise, for each phrase in the sentence, a semantic head and its arguments. For instance, in the sentence ‘Lloyd’s rescues failing bank’, three subtrees can be extracted: lemma:rescue arg:ARG1 var:Lloyd’s which indicates that ‘Lloyd’s’ is subject of the head ‘rescue’, lemma:rescue arg:ARG2 var:bank which indicates that ‘bank’ is object of the head ‘rescue’, and lemma:failing arg:ARG1 var:bank which indicates that the argument of ‘failing’ is ‘bank’. Note </context>
</contexts>
<marker>Copestake, 2004</marker>
<rawString>Ann Copestake. 2004. Robust Minimal Recursion Semantics. www.cl.cam.ac.uk/∼aac10/papers/rmrs draft.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic Textual Entailment: Generic Applied Modelling of Language Variability.</title>
<date>2004</date>
<booktitle>Proceedings of The PASCAL Workshop on Learning Methods for Text Understanding and Mining,</booktitle>
<location>Grenoble, France,</location>
<contexts>
<context position="3014" citStr="Dagan and Glickman (2004)" startWordPosition="480" endWordPosition="483">t section). The definition of similarity used by those systems is fairly wide, however. Typically, a query on the verb ‘produce’ will return verbs such as ‘export’, ‘import’ or ‘sell’, for instance (see DIRT demo from http://demo.patrickpantel.com/Content/Lex Sem/paraphrase.htm, Lin and Pantel, 2001.) This fairly wide notion of similarity is not fully appropriate for our word substitutions task: although cats and dogs are similar types of entities, querying a knowledge base for ‘cat’ shouldn’t return statements about dogs; statements about Siamese, however, should be acceptable. So, following Dagan and Glickman (2004), we refine our concept of similarity as that of entailment, defined here as the relation whereby the meaning of a word w1 is ‘included’ in the meaning of word w2 (practically speaking, we assume that the ‘meaning’ of a word is represented by the contexts in which it appears and require that if w1 entails w2, the contexts of w2 should be a subset of the contexts of w1). Given an input term w, we therefore attempt to extract words which either entail or are entailed by w. (We do not extract directionality at this stage.) Proceedings of the EACL 2009 Student Research Workshop, pages 28–36, Athen</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic Textual Entailment: Generic Applied Modelling of Language Variability. Proceedings of The PASCAL Workshop on Learning Methods for Text Understanding and Mining, Grenoble, France, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Alfio Gliozzo, Efrat Marmorshtein and Carlo Strapparava.</title>
<date>2006</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<pages>17--21</pages>
<location>Sydney, Australia.</location>
<marker>Dagan, Glickman, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Marmorshtein and Carlo Strapparava. 2006. Direct Word Sense Matching for Lexical Substitution. Proceedings of COLING-ACL 2006, 17-21 Jul 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Feature Vector Quality and Distributional Similarity.</title>
<date>2004</date>
<booktitle>Proceedings Of the 20th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="7989" citStr="Geffet and Dagan, 2004" startWordPosition="1298" endWordPosition="1301">assumption are well-known: the facts that ‘a bank lends money’ and ‘Smith’s brother lent him money’ do not imply that banks and brothers are similar entities. This effect becomes particularly evident in cases where antonyms are returned by the system; in those cases, a very high distributional similarity actually corresponds to opposite meanings. Producing an output ranked according to distributional similarity scores (weeding out anything under a certain threshold) is therefore not sufficient to retain good precisions for many tasks. Some work has thus focused on a re-ranking strategies (see Geffet and Dagan, 2004 and Geffet and Dagan, 2005, who improve the output of a distributional similarity system for an entailment task using a web-based feature inclusion check, and comment that their filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.) 29 2.2 Extraction Systems Prominent entailment rule acquisition systems include DIRT (Lin and Pantel, 2001), which uses distributional similarity on a 1 GB corpus to identify semantically similar words and expressions, and TEASE (Szpektor et al., 2004), which extracts entailment relations from the web for a given word by </context>
</contexts>
<marker>Geffet, Dagan, 2004</marker>
<rawString>Maayan Geffet and Ido Dagan. 2004. Feature Vector Quality and Distributional Similarity. Proceedings Of the 20th International Conference on Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The Distributional Inclusion Hypothesises and Lexical Entailment.</title>
<date>2005</date>
<booktitle>In Proceedings Of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<contexts>
<context position="8016" citStr="Geffet and Dagan, 2005" startWordPosition="1303" endWordPosition="1307">the facts that ‘a bank lends money’ and ‘Smith’s brother lent him money’ do not imply that banks and brothers are similar entities. This effect becomes particularly evident in cases where antonyms are returned by the system; in those cases, a very high distributional similarity actually corresponds to opposite meanings. Producing an output ranked according to distributional similarity scores (weeding out anything under a certain threshold) is therefore not sufficient to retain good precisions for many tasks. Some work has thus focused on a re-ranking strategies (see Geffet and Dagan, 2004 and Geffet and Dagan, 2005, who improve the output of a distributional similarity system for an entailment task using a web-based feature inclusion check, and comment that their filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.) 29 2.2 Extraction Systems Prominent entailment rule acquisition systems include DIRT (Lin and Pantel, 2001), which uses distributional similarity on a 1 GB corpus to identify semantically similar words and expressions, and TEASE (Szpektor et al., 2004), which extracts entailment relations from the web for a given word by computing characteristic co</context>
<context position="9385" citStr="Geffet and Dagan (2005)" startWordPosition="1516" endWordPosition="1519">nd Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. Even more related to our work, Mirkin et al. (2006) integrate both approaches by constructing features for the output of both a pattern-based and a vector-based systems, and by filtering incorrect entries with a supervised SVM classifier. (The pattern-based approach uses a set of manuallyconstructed patterns applied to a web search.) In the same vein, Geffet and Dagan (2005) filter the result of a pattern-based system using feature vectors. They get their features out of an 18 million word corpus augmented by a web search. Their idea is that for any pair of potentially similar words, the features of the entailed one should comprise all the features of the entailing one. The main difference between our work and the last two quoted papers is that we add a new layer of verification: we extract pairs of verbs using automatically derived semantic patterns, perform a first stage of filtering using the semantic signatures of each word and apply a final stage of filterin</context>
<context position="14645" citStr="Geffet and Dagan, 2005" startWordPosition="2414" endWordPosition="2417">nal PMI value by this discount to find the final PMI. We then select the n patterns with highest PMIs and use them as relevant semantic contexts to find all terms t that also appear in those contexts. The result of this step is a list of potential entailment relations, w − ti ... w − t,, (we do not know the direction of the entailment). 4.2 Step 2 - Feature vector Comparison This step takes the output of the pattern-based extraction and applies a first filter to the potential entailment pairs. The filter relies on the idea that two words that are similar will have similar feature vectors (see Geffet and Dagan, 2005). We define here the feature vector of word w as the list of semantic features containing w, together with the PMI of each feature in relation to w as a weight. For each pair of words (w1, w2) we extract the feature vectors of both w1 and w2 and calculate their similarity using the measure of Lin (1998). Pairs with a similarity under a certain threshold are weeded out. (We use 0.007 in our experiments – the value was found by comparing precisions for various thresholds in a set of initial experiments.) As a check of how the Lin measure performed on our Wikipedia subset using RMRS features, we </context>
</contexts>
<marker>Geffet, Dagan, 2005</marker>
<rawString>Maayan Geffet and Ido Dagan. 2005. The Distributional Inclusion Hypothesises and Lexical Entailment. In Proceedings Of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 107–114, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s Thesaurus and Semantic Similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of International Conference RANLP–03,</booktitle>
<pages>212--219</pages>
<contexts>
<context position="15469" citStr="Jarmasz and Szpakowicz, 2003" startWordPosition="2562" endWordPosition="2565">extract the feature vectors of both w1 and w2 and calculate their similarity using the measure of Lin (1998). Pairs with a similarity under a certain threshold are weeded out. (We use 0.007 in our experiments – the value was found by comparing precisions for various thresholds in a set of initial experiments.) As a check of how the Lin measure performed on our Wikipedia subset using RMRS features, we reproduced the Miller and Charles experiment (1991) which consists in asking humans to rate the similarity of 30 noun pairs. The experiment is a standard test for semantic similarity systems (see Jarmasz and Szpakowicz, 2003; Lin, 1998; Resnik, 1995 and Hirst and St Onge, 1998 amongst others). The correlations obtained by previous systems range between the high 0.6 and the high 0.8. Those systems rely on edge counting using manually-created resources such as WordNet and the Roget’s Thesaurus. We are not actually aware of results obtained on totally automated systems (apart from a baseline computed by Strube and Ponzetto, 2006, using Google hits, which return a correlation of 0.26.) Applying our feature vector step to the Miller and Charles pairs, we get a correlation of 0.38, way below the edge-counting systems. </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s Thesaurus and Semantic Similarity. In Proceedings of International Conference RANLP–03, pp. 212– 219, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig Harris</author>
</authors>
<title>Distributional Structure.</title>
<date>1954</date>
<journal>In Word,</journal>
<volume>10</volume>
<pages>146--162</pages>
<contexts>
<context position="2096" citStr="Harris, 1954" startWordPosition="345" endWordPosition="346"> above (as opposed to nationalisation events), we might search for ‘acquire’. This will return the first statement (about the acquisition of Merrill Lynch) but not the second statement about HBOS. Ideally, we would like a system able to generate words similar to our query, so that a statement containing the verb ‘buy’ gets returned when we search for ‘acquire’. This problem is closely related to the clustering of semantically similar terms, which has received much attention in the literature. Systems that perform such clustering usually do so under the assumption of distributional similarity (Harris, 1954) which state that two words appearing in similar contexts will be close in meaning. This observation is statistically useful and has contributed to successful systems within two approaches: the pattern-based approach and the feature vector approach (we describe those two approaches in the next section). The definition of similarity used by those systems is fairly wide, however. Typically, a query on the verb ‘produce’ will return verbs such as ‘export’, ‘import’ or ‘sell’, for instance (see DIRT demo from http://demo.patrickpantel.com/Content/Lex Sem/paraphrase.htm, Lin and Pantel, 2001.) This</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zelig Harris. Distributional Structure. In Word, 10, No. 2–3, pp. 146–162, 1954.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>Proceedings of COLING-92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="6929" citStr="Hearst, 1992" startWordPosition="1131" endWordPosition="1132">ormation that the user was looking for. for 30 input verbs. 2 Previous Work 2.1 Distributional Similarity 2.1.1 Principles Systems using distributional similarity usually fall under two approaches: 1. The pattern-based approach (e.g. Ravichadran and Hovy, 2002). The most significant contexts for an input seed are extracted as features and those features used to discover words related to the input (under the assumption that words appearing in at least one significant context are similar to the seed word). There is also a non-distributional strand of this approach: it uses Hearst-like patterns (Hearst, 1992) which are supposed to indicate the presence of two terms in a certain relation - most often hyponymy or meronymy (see Chklovski and Pantel, 2004). 2. The feature vector approach (e.g. Lin and Pantel, 2001). This method fully embraces the definition of distributional similarity by making the assumption that two words appearing in similar sets of features must be related. 2.1.2 Limitations The problems of the distributional similarity assumption are well-known: the facts that ‘a bank lends money’ and ‘Smith’s brother lent him money’ do not imply that banks and brothers are similar entities. Thi</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. Proceedings of COLING-92, pp.539–545, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical Chains As Representations of Context for the Detection and Correction of Malapropisms.</title>
<date>1998</date>
<booktitle>In ‘WordNet’, Ed. Christiane Fellbaum,</booktitle>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA:</location>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical Chains As Representations of Context for the Detection and Correction of Malapropisms. In ‘WordNet’, Ed. Christiane Fellbaum, Cambridge, MA: The MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<marker>Lin, 2003</marker>
<rawString>Dekang Lin. 2003. An Information-Theoretic Definition of Similarity. In Proceedings of the 15th International Conference on Machine Learning, pp. 296– 304, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying Synonyms among Distributionally Similar Words.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI-03,</booktitle>
<location>Acapulco, Mexico,</location>
<contexts>
<context position="8760" citStr="Lin et al. (2003)" startWordPosition="1418" endWordPosition="1421">, and comment that their filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.) 29 2.2 Extraction Systems Prominent entailment rule acquisition systems include DIRT (Lin and Pantel, 2001), which uses distributional similarity on a 1 GB corpus to identify semantically similar words and expressions, and TEASE (Szpektor et al., 2004), which extracts entailment relations from the web for a given word by computing characteristic contexts for that word. Recently, systems that combine both patternbased and feature vector approaches have also been presented. Lin et al. (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. Even more related to our work, Mirkin et al. (2006) integrate both approaches by constructing features for the output of both a pattern-based and a vector-based systems, and by filtering incorrect entries with a supervised SVM classifier. (The pattern-based approach uses a set of manuallyconstructed patterns applied to a web search.) In the same vein</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin and Ming Zhou. 2003. Identifying Synonyms among Distributionally Similar Words. In Proceedings of IJCAI-03, Acapulco, Mexico, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM</booktitle>
<contexts>
<context position="2689" citStr="Lin and Pantel, 2001" startWordPosition="429" endWordPosition="432">l similarity (Harris, 1954) which state that two words appearing in similar contexts will be close in meaning. This observation is statistically useful and has contributed to successful systems within two approaches: the pattern-based approach and the feature vector approach (we describe those two approaches in the next section). The definition of similarity used by those systems is fairly wide, however. Typically, a query on the verb ‘produce’ will return verbs such as ‘export’, ‘import’ or ‘sell’, for instance (see DIRT demo from http://demo.patrickpantel.com/Content/Lex Sem/paraphrase.htm, Lin and Pantel, 2001.) This fairly wide notion of similarity is not fully appropriate for our word substitutions task: although cats and dogs are similar types of entities, querying a knowledge base for ‘cat’ shouldn’t return statements about dogs; statements about Siamese, however, should be acceptable. So, following Dagan and Glickman (2004), we refine our concept of similarity as that of entailment, defined here as the relation whereby the meaning of a word w1 is ‘included’ in the meaning of word w2 (practically speaking, we assume that the ‘meaning’ of a word is represented by the contexts in which it appears</context>
<context position="7135" citStr="Lin and Pantel, 2001" startWordPosition="1166" endWordPosition="1169">1. The pattern-based approach (e.g. Ravichadran and Hovy, 2002). The most significant contexts for an input seed are extracted as features and those features used to discover words related to the input (under the assumption that words appearing in at least one significant context are similar to the seed word). There is also a non-distributional strand of this approach: it uses Hearst-like patterns (Hearst, 1992) which are supposed to indicate the presence of two terms in a certain relation - most often hyponymy or meronymy (see Chklovski and Pantel, 2004). 2. The feature vector approach (e.g. Lin and Pantel, 2001). This method fully embraces the definition of distributional similarity by making the assumption that two words appearing in similar sets of features must be related. 2.1.2 Limitations The problems of the distributional similarity assumption are well-known: the facts that ‘a bank lends money’ and ‘Smith’s brother lent him money’ do not imply that banks and brothers are similar entities. This effect becomes particularly evident in cases where antonyms are returned by the system; in those cases, a very high distributional similarity actually corresponds to opposite meanings. Producing an output</context>
<context position="8373" citStr="Lin and Pantel, 2001" startWordPosition="1357" endWordPosition="1360"> to distributional similarity scores (weeding out anything under a certain threshold) is therefore not sufficient to retain good precisions for many tasks. Some work has thus focused on a re-ranking strategies (see Geffet and Dagan, 2004 and Geffet and Dagan, 2005, who improve the output of a distributional similarity system for an entailment task using a web-based feature inclusion check, and comment that their filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.) 29 2.2 Extraction Systems Prominent entailment rule acquisition systems include DIRT (Lin and Pantel, 2001), which uses distributional similarity on a 1 GB corpus to identify semantically similar words and expressions, and TEASE (Szpektor et al., 2004), which extracts entailment relations from the web for a given word by computing characteristic contexts for that word. Recently, systems that combine both patternbased and feature vector approaches have also been presented. Lin et al. (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and </context>
<context position="11226" citStr="Lin and Pantel, 2001" startWordPosition="1826" endWordPosition="1829">ccording to certain correctness criteria. For entailment systems, the annotation usually relies on two tests: whether the meaning of one word entails the other one in some senses of those words, and whether the judges can come up with contexts in which the words are directly substitutable. Szpektor et al. (2007) point out the difficulties in applying those criteria. They note the low inter-annotator agreements obtained in previous studies and propose a new evaluation method based on precise judgement questions applied to a set of relevant contexts. Using their methods, they evaluate the DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004) algorithms and obtain upper bound precisions of 44% and 38% respectively on 646 entailment rules for 30 transitive verbs. We follow here their methodology to check the results obtained via the traditional annotation. 3 The Data The corpus used for our distributional similarity baseline consists of a subset of Wikipedia totalling 500 MB in size, parsed first with RASP2 (Briscoe et al., 2006) and then into a Robust Minimal Recursion Semantics form (RMRS, Copestake, 2004) using a RASP-to-RMRS converter. The RMRS representation consists of trees (or tree fragment</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – Discovery of Inference Rules from Text. In Proceedings of ACM 2001, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Walter Charles</author>
</authors>
<title>Contextual Correlates of Semantic Similarity.</title>
<date>2001</date>
<booktitle>In Language and Cognitive Processes,</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>1--28</pages>
<marker>Miller, Charles, 2001</marker>
<rawString>George Miller and Walter Charles. 2001. Contextual Correlates of Semantic Similarity. In Language and Cognitive Processes, 6(1), pp. 1–28, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Maayan Geffet</author>
</authors>
<title>Integrating Pattern-Based and Distributional Similarity Methods for Lexical Entailment Acquisition.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<location>Sydney, Australia, pp.579–586,</location>
<marker>Mirkin, Dagan, Geffet, 2004</marker>
<rawString>Shachar Mirkin, Ido Dagan and Maayan Geffet. 2004. Integrating Pattern-Based and Distributional Similarity Methods for Lexical Entailment Acquisition. In Proceedings of COLING/ACL, Sydney, Australia, pp.579–586, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically Labelling Semantic Classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL04,</booktitle>
<pages>321328</pages>
<location>Boston, MA,</location>
<contexts>
<context position="8795" citStr="Pantel and Ravichandran (2004)" startWordPosition="1423" endWordPosition="1426">ir filtering produces better outputs than cutting off the similarity pairs with the lowest ranking.) 29 2.2 Extraction Systems Prominent entailment rule acquisition systems include DIRT (Lin and Pantel, 2001), which uses distributional similarity on a 1 GB corpus to identify semantically similar words and expressions, and TEASE (Szpektor et al., 2004), which extracts entailment relations from the web for a given word by computing characteristic contexts for that word. Recently, systems that combine both patternbased and feature vector approaches have also been presented. Lin et al. (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. Even more related to our work, Mirkin et al. (2006) integrate both approaches by constructing features for the output of both a pattern-based and a vector-based systems, and by filtering incorrect entries with a supervised SVM classifier. (The pattern-based approach uses a set of manuallyconstructed patterns applied to a web search.) In the same vein, Geffet and Dagan (2005) filter th</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labelling Semantic Classes. In Proceedings of HLT/NAACL04, Boston, MA, pp 321328, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System.</title>
<date>2002</date>
<booktitle>Proceedings of ACL,</booktitle>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. Proceedings of ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI–95,</booktitle>
<contexts>
<context position="15494" citStr="Resnik, 1995" startWordPosition="2568" endWordPosition="2569">d w2 and calculate their similarity using the measure of Lin (1998). Pairs with a similarity under a certain threshold are weeded out. (We use 0.007 in our experiments – the value was found by comparing precisions for various thresholds in a set of initial experiments.) As a check of how the Lin measure performed on our Wikipedia subset using RMRS features, we reproduced the Miller and Charles experiment (1991) which consists in asking humans to rate the similarity of 30 noun pairs. The experiment is a standard test for semantic similarity systems (see Jarmasz and Szpakowicz, 2003; Lin, 1998; Resnik, 1995 and Hirst and St Onge, 1998 amongst others). The correlations obtained by previous systems range between the high 0.6 and the high 0.8. Those systems rely on edge counting using manually-created resources such as WordNet and the Roget’s Thesaurus. We are not actually aware of results obtained on totally automated systems (apart from a baseline computed by Strube and Ponzetto, 2006, using Google hits, which return a correlation of 0.26.) Applying our feature vector step to the Miller and Charles pairs, we get a correlation of 0.38, way below the edge-counting systems. It turns out, however, th</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings of IJCAI–95, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
</authors>
<title>Hristo Tanev, Ido Dagan and Bonaventura Coppola.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP–</booktitle>
<pages>41--48</pages>
<marker>Szpektor, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan and Bonaventura Coppola. 2004. Scaling Web-Based Acquisition ofEntailment Relations. In Proceedings of EMNLP– 2004, pp. 41–48, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
</authors>
<title>Eyal Shnarch and Ido Dagan.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL–07,</booktitle>
<marker>Szpektor, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch and Ido Dagan. 2007. Instance-Based Evaluation of Entailment Rule Acquisition. In Proceedings of ACL–07, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone Ponzetto</author>
</authors>
<title>WikiRelate! Computing Semantic Relatedness Using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI–06,</booktitle>
<pages>1219--1224</pages>
<contexts>
<context position="15878" citStr="Strube and Ponzetto, 2006" startWordPosition="2630" endWordPosition="2633">ed the Miller and Charles experiment (1991) which consists in asking humans to rate the similarity of 30 noun pairs. The experiment is a standard test for semantic similarity systems (see Jarmasz and Szpakowicz, 2003; Lin, 1998; Resnik, 1995 and Hirst and St Onge, 1998 amongst others). The correlations obtained by previous systems range between the high 0.6 and the high 0.8. Those systems rely on edge counting using manually-created resources such as WordNet and the Roget’s Thesaurus. We are not actually aware of results obtained on totally automated systems (apart from a baseline computed by Strube and Ponzetto, 2006, using Google hits, which return a correlation of 0.26.) Applying our feature vector step to the Miller and Charles pairs, we get a correlation of 0.38, way below the edge-counting systems. It turns out, however, that this low result is at least partially due to data sparsity: when ignoring the pairs containing at least one word with frequency under 200 (8 of them, which means ending up with 22 pairs left out of the initial 30), the correlation goes up to 0.69. This is in line with the edge-counting systems and shows that our baseline system produces a decent approximation of human performanc</context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness Using Wikipedia. In Proceedings of AAAI–06, pp. 1219– 1224, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>