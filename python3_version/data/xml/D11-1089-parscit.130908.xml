<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.998942">
Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing:
A Study on Japanese Katakana Words
</title>
<author confidence="0.99283">
Nobuhiro Kaji Masaru Kitsuregawa
</author>
<affiliation confidence="0.9982315">
Institute of Industrial Science Institute of Industrial Science
University of Tokyo, Tokyo, Japan University of Tokyo, Tokyo, Japan
</affiliation>
<email confidence="0.996904">
kaji@tkl.iis.u-tokyo.ac.jp kitsure@tkl.iis.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999255944444444">
Word boundaries within noun compounds are
not marked by white spaces in a number of
languages, unlike in English, and it is benefi-
cial for various NLP applications to split such
noun compounds. In the case of Japanese,
noun compounds made up of katakana words
(i.e., transliterated foreign words) are par-
ticularly difficult to split, because katakana
words are highly productive and are often out-
of-vocabulary. To overcome this difficulty,
we propose using monolingual and bilingual
paraphrases of katakana noun compounds for
identifying word boundaries. Experiments
demonstrated that splitting accuracy is sub-
stantially improved by extracting such para-
phrases from unlabeled textual data, the Web
in our case, and then using that information for
constructing splitting models.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.9878095">
1.1 Japanese katakana words and noun
compound splitting
</subsectionHeader>
<bodyText confidence="0.999959777777778">
Borrowing is a major type of word formation
in Japanese, and numerous foreign words (proper
names or neologisms etc.) are continuously being
imported from other languages (Tsujimura, 2006).
Most borrowed words in modern Japanese are
transliterations1 from English and they are referred
to as katakana words because transliterated foreign
words are primarily spelled by using katakana char-
acters in the Japanese writing system.2 Compound-
</bodyText>
<footnote confidence="0.9852168">
1Some researchers use the term transcription rather than
transliteration (Breen, 2009). Our terminology is based on stud-
ies on machine transliteration (Knight and Graehl, 1998).
2The Japanese writing system has four character types: hi-
ragana, katakana, kanji, and Latin alphabet.
</footnote>
<bodyText confidence="0.999824305555556">
ing is another type of word formation that is com-
mon in Japanese (Tsujimura, 2006). In particu-
lar, noun compounds are frequently produced by
merging two or more nouns together. These two
types of word formation yield a significant amount
of katakana noun compounds, making Japanese a
highly productive language.
In Japanese as well as some European and Asian
languages (e.g., German, Dutch and Korean), con-
stituent words of compounds are not separated by
white spaces, unlike in English. In those languages,
it is beneficial for various NLP applications to split
such compounds. For example, compound splitting
enables SMT systems to translate a compound on a
word-by-word basis, even if the compound itself is
not found in the translation table (Koehn and Knight,
2003; Dyer, 2009). In the context of IR, decom-
pounding has an analogous effect to stemming, and
it significantly improves retrieval results (Braschler
and Ripplinger, 2004). In abbreviation recognition,
the definition of an abbreviation is often in the form
of a noun compound, and most abbreviation recogni-
tion algorithms assume that the definition is properly
segmented; see e.g., (Schwartz and Hearst, 2003;
Okazaki et al., 2008).
This has led NLP researchers to explore meth-
ods for splitting compounds, especially noun com-
pounds, in various languages (Koehn and Knight,
2003; Nakazawa et al., 2005; Alfonseca et al.,
2008a). While many methods have been presented,
they basically require expensive linguistic resources
to achieve high enough accuracy. For example, Al-
fonseca et al. (2008b) employed a word dictionary,
which is obviously useful for this task. Other stud-
ies have suggested using bilingual resources such as
parallel corpora (Brown, 2002; Koehn and Knight,
</bodyText>
<page confidence="0.976611">
959
</page>
<note confidence="0.959424">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959–969,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997363166666667">
2003; Nakazawa et al., 2005). The idea behind those
methods is that compounds are basically split into
constituent words when they are translated into En-
glish, where the compounded words are separated
by white spaces, and hence splitting rules can be
learned by discovering word alignments in bilingual
resources.
The largest obstacle that makes compound split-
ting difficult is the existence of out-of-vocabulary
words, which are not found in the abovemen-
tioned linguistic resources. In the Japanese case,
it is known that katakana words constitute a large
source of out-of-vocabulary words (Brill et al., 2001;
Nakazawa et al., 2005; Breen, 2009). As we have
discussed, katakana words are very productive, and
thus we can no longer expect existent linguistic re-
sources to have sufficient coverage. According to
(Breen, 2009), as many as 20% of katakana words
in news articles, which we think include less out-of-
vocabulary words than Web and other noisy textual
data, are out-of-vocabulary. Those katakana words
often form noun compounds, and pose a substantial
difficulty for Japanese text processing (Nakazawa et
al., 2005).
</bodyText>
<subsectionHeader confidence="0.976384">
1.2 Paraphrases as implicit word boundaries
</subsectionHeader>
<bodyText confidence="0.999986227272728">
To alleviate the errors caused by out-of-vocabulary
words, we explored the use of unlabeled textual
data for splitting katakana noun compounds. Since
the amount of unlabeled text available is generally
much larger than word dictionaries and other expen-
sive linguistic resources, it is crucial to establish a
methodology for taking full advantage of such eas-
ily available textual data. While several approaches
have already been proposed, their accuracies are still
unsatisfactory (section 2.1).
From a broad perspective, our approach can be
seen as using paraphrases of noun compounds. As
we will see in section 4 and 5, katakana noun com-
pounds can be paraphrased into various forms that
strongly indicate word boundaries within the origi-
nal noun compound. This paper empirically demon-
strates that splitting accuracy can be significantly
improved by extracting such paraphrases from un-
labeled text, the Web in our case, and then using that
information for constructing splitting models.
Specifically, two types of paraphrases are inves-
tigated in this paper. Section 4 explores monolin-
gual paraphrases that can be generated by inserting
certain linguistic markers between constituent words
of katakana noun compounds. Section 5, in turn,
explores bilingual paraphrases (specifically, back-
transliteration). Since katakana words are basically
transliterations from English, back-transliterating
katakana noun compounds is also useful for split-
ting. To avoid terminological confusion, mono-
lingual paraphrases are simply referred to as para-
phrases and bilingual paraphrases are referred to as
back-transliterations hereafter.
We did experiments to empirically evaluate our
method. The results demonstrated that both para-
phrase and back-transliteration substantially im-
proved the performance in terms of Fi-score, and
the best performance was achieved when they
were combined. We also confirmed that our
method outperforms the previously proposed split-
ting methods by a wide margin. All these results
strongly suggest the effectiveness of paraphrasing
and back-transliteration for identifying word bound-
aries within katakana noun compounds.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999586">
2.1 Compound splitting
</subsectionHeader>
<bodyText confidence="0.99996915">
A common approach to splitting compounds with-
out expensive linguistic resources is an unsuper-
vised method based on word or string frequen-
cies estimated from unlabeled text (Koehn and
Knight, 2003; Ando and Lee, 2003; Schiller, 2005;
Nakazawa et al., 2005; Holz and Biemann, 2008).
Amongst others, Nakazawa et al. (2005) also in-
vestigated ways of splitting katakana noun com-
pounds. Although the frequency-based method gen-
erally achieves high recall, its precision is not satis-
factory (Koehn and Knight, 2003; Nakazawa et al.,
2005). Our experiments empirically compared our
method with the frequency-based methods, and the
results demonstrate the advantage of our method.
Our approach can be seen as augmenting discrim-
inative models of compound splitting with large ex-
ternal linguistic resources, i.e., textual data on the
Web. In a similar spirit, Alfonseca et al. (2008b) pro-
posed the use of query logs for compound splitting.3
Their experimental results, however, did not clearly
</bodyText>
<footnote confidence="0.9971655">
3Although they also proposed using anchor text, this slightly
degraded the performance.
</footnote>
<page confidence="0.99704">
960
</page>
<bodyText confidence="0.9997715">
demonstrate their method’s effectiveness. Without
the query logs, the accuracy is reported to drop
only slightly from 90.55% to 90.45%. In contrast,
our experimental results showed statistically signifi-
cant improvements as a result of using additional re-
sources. Moreover, we used only textual data, which
is easily available, unlike query logs.
Holz and Biemann (2008) proposed a method
for splitting and paraphrasing German compounds.
While their work is related to ours, their algorithm
is a pipeline model and paraphrasing result is not
employed during splitting.
</bodyText>
<subsectionHeader confidence="0.999325">
2.2 Other research topics
</subsectionHeader>
<bodyText confidence="0.999855764705883">
Our study is closely related to word segmentation,
which is an important research topic in Asian lan-
guages including Japanese. Although we can use
existing word segmentation systems for splitting
katakana noun compounds, it is difficult to reach the
desired accuracy, as we will empirically demonstrate
in section 6. One reason for this is that katakana
noun compounds often include out-of-vocabulary
words, which are difficult for the existing segmen-
tation systems to deal with. See (Nakazawa et al.,
2005) for a discussion of this point. From a word
segmentation perspective, our task can be seen as
a case study focusing on a certain linguistic phe-
nomenon of particular difficulty. More importantly,
we are unaware of any attempts to use paraphrases
or transliterations for word segmentation in the same
way as we do.
Recent studies have explored using paraphrase
statistics for parsing (Nakov and Hearst, 2005a;
Nakov and Hearst, 2005b; Bansal and Klein, 2011).
Although these studies successfully demonstrated
the usefulness of paraphrases for improving parsers,
the connection between paraphrases and word seg-
mentation (or noun compound splitting) was not at
all discussed.
Our method of using back-transliterations for
splitting katakana noun compounds (section 5) is
closely related to methods for mining transliteration
from the Web text (Brill et al., 2001; Cao et al.,
2007; Oh and Isahara, 2008; Wu et al., 2009). What
most differentiates these studies from our work is
that their primary goal is to build a machine translit-
eration system or to build a bilingual dictionary it-
self; none of them explored splitting compounds.
</bodyText>
<tableCaption confidence="0.999279">
Table 1: Basic features.
</tableCaption>
<table confidence="0.7549332">
ID Feature Description
1 yz constituent word 1-gram
2 yz−1yz constituent word 2-gram
3 LEN(yz) #characters of yz (1, 2, 3, 4, or ≥5)
4 DICT(yz) true if yz is in the dictionary
</table>
<sectionHeader confidence="0.961724" genericHeader="method">
3 Supervised Approach
</sectionHeader>
<bodyText confidence="0.999913818181818">
The task we examine in this paper is splitting
a katakana noun compound x into its constituent
words, y = (y1, y2 ... y|Y|). Note that the output
can be a single word, i.e., |y |= 1. Since it is pos-
sible that the input is an out-of-vocabulary word, it
is not at all trivial to identify a single word as such.
A naive method would erroneously split an out-of-
vocabulary word into multiple constituent words.
We formalize our task as a structure prediction
problem that, given a katakana noun compound x,
predicts the most probable splitting y*.
</bodyText>
<equation confidence="0.982028">
y* = argmax w · 0(y),
YEY(X)
</equation>
<bodyText confidence="0.99987428">
where Y(x) represents the set of all splitting options
of x, 0(y) is a feature vector representation of y,
and w is a weight vector to be estimated from la-
beled data.
Table 1 summarizes our basic feature set. Fea-
tures 1 and 2 are word 1-gram and 2-gram features,
respectively. Feature 3 represents the length of the
constituent word. LEN(y) returns the number of
characters of y (1, 2, 3, 4, or ≥5). Feature 4 indi-
cates whether the constituent word is registered in
an external dictionary (see section 6.1). DICT(y) re-
turns true if the word y is in the dictionary.
In addition to those basic features, we also employ
paraphrases and back-transliterations of katakana
noun compounds as features. The features are de-
tailed in sections 4 and 5, respectively.
We can optimize the weight vector w using an ar-
bitrary training algorithm. Here we adopt the aver-
aged perceptron algorithm for the sake of time effi-
ciency (Freund and Schapire, 1999). The perceptron
offers efficient online training, and it performs com-
paratively well with batch algorithms such as SVMs.
Since we use only factored features (see table 1, sec-
tion 4 and section 5), dynamic programming can be
used to locate y*.
</bodyText>
<page confidence="0.998493">
961
</page>
<tableCaption confidence="0.9789495">
Table 2: Paraphrase rules and examples. The first column represents the type of linguistic marker to be inserted, the
second column shows the paraphrase rules, and the last column gives examples.
</tableCaption>
<table confidence="0.9944849">
Type Rule Example
Centered dot X1X2 → X1 · X2 アンチョビパスタ → アンチョビ・パスタ
(anchovy pasta) (anchovy · pasta)
Possessive marker X1X2 → X1 の X2 アンチョビパスタ → アンチョビの パスタ
(anchovy pasta) (with anchovy) (pasta)
Verbal suffix X1X2 → X1 する X2 ダウンロードファイル → ダウンロードしたファイル
X1X2 → X1 した X2 (download file) (downloaded) (file)
Adjectival suffix X1X2 → X1 な X2 サプライズギフト → サプライズな ギフト
X1X2 → X1 的 X2 (surprise gift) (surprising) (gift)
X1X2 → X1 的な X2
</table>
<sectionHeader confidence="0.994765" genericHeader="method">
4 Paraphrasing
</sectionHeader>
<bodyText confidence="0.9999502">
In this section, we argue that paraphrases of
katakana noun compounds provides useful informa-
tion on word boundaries. Consequently, we propose
using paraphrase frequencies as features for training
the discriminative model.
</bodyText>
<subsectionHeader confidence="0.999806">
4.1 Paraphrasing noun compounds
</subsectionHeader>
<bodyText confidence="0.99940975">
A katakana noun compound can be paraphrased into
various forms, some of which provide information
on the word boundaries within the original com-
pound.
</bodyText>
<equation confidence="0.582927166666667">
(1) a. アンチョビパスタ
(anchovy pasta)
b. アンチョビ・パスタ
(anchovy · pasta)
c. アンチョビの パスタ
(with anchovy) (pasta)
</equation>
<bodyText confidence="0.999965285714286">
These examples are paraphrases of each other. (1a)
is in the form of a noun compound, within which
the word boundary is ambiguous. In (1b), on the
other hand, a centered dot ・ is inserted between
the constituent words. In the Japanese writing sys-
tem, the centered dot is sometimes, but not always,
used to separate long katakana compounds for the
sake of readability. (1c) is the noun phrase gener-
ated from (1a) by inserting the possessive marker
‘の’, which can be translated as with in this context,
between the constituent words. If we observe para-
phrases of (1a) such as (1b) and (1c), we can guess
that a word boundary exists between ‘アンチョビ
(anchovy)’ and ‘パスタ (pasta)’.
</bodyText>
<subsectionHeader confidence="0.998018">
4.2 Paraphrase rules
</subsectionHeader>
<bodyText confidence="0.999891266666667">
The above discussion led us to use paraphrase
frequencies estimated from Web text for splitting
katakana noun compounds. For this purpose, we
established the seven paraphrase rules illustrated in
Table 2. The rules are in the form of X1X2 →
X1MX2, where X1 and X2 represent nouns, and
M is a certain linguistic marker (e.g., the posses-
sive marker ‘の’). The left-hand term corresponds
to a compound to be paraphrased and the right-hand
term represents its paraphrase. For instance, X1 =
‘アンチョビ (anchovy)’, X2 = ‘パスタ (pasta)’, and
M = ‘の’. The paraphrase rules we use are based on
the rules proposed by Kageura et al. (2004) for ex-
panding complex terms, primarily noun compounds,
into their variants.
</bodyText>
<subsectionHeader confidence="0.998949">
4.3 Web-based frequency as features
</subsectionHeader>
<bodyText confidence="0.9985574">
We introduce a new feature using the paraphrase
rules and Web text. As preprocessing, we use reg-
ular expressions to count the frequencies of all po-
tential paraphrases of katakana noun compounds on
the Web in advance.
</bodyText>
<equation confidence="0.99056825">
(katakana)+ ・ (katakana)+
(katakana)+ の (katakana)+
(katakana)+ する (katakana)+
. . .
</equation>
<bodyText confidence="0.999969666666667">
where (katakana) corresponds to one katakana char-
acter. Given a candidate segmentation y at test time,
we generate paraphrases of the noun compound by
setting X1 = yi−1 and X2 = yi, and applying the
paraphrase rules. We then use log(F + 1), where F
is the sum of the Web-based frequencies of the gen-
</bodyText>
<page confidence="0.986592">
962
</page>
<bodyText confidence="0.9982921">
erated paraphrases, as the feature of the boundary
between yi_1 and yi.
As the feature value, we use the logarithmic fre-
quency, rather than the raw frequency, for scaling.
Since the other features have binary value, we found,
in initial experiments, that the importance of this
feature is overemphasized if we use the raw fre-
quency. Note that we use log(F + 1) rather than
log F so as to avoid the feature value being zero
when F = 1.
</bodyText>
<sectionHeader confidence="0.956663" genericHeader="method">
5 Back-transliteration
</sectionHeader>
<bodyText confidence="0.999877636363637">
Most katakana words are transliterations from En-
glish, where words are separated by white spaces.
It is, therefore, reasonable to think that back-
transliterating katakana noun compounds into En-
glish would provide information on word bound-
aries, in a similar way to paraphrasing.
This section presents a method for extracting
back-transliterations of katakana words from mono-
lingual Web text, and establishing word alignments
between those katakana and English words (Table
3). In what follows, the pair of katakana words
and its English back-transliteration is referred to as a
transliteration pair. If the transliteration pair is an-
notated with word alignment information as in Table
3, it is referred to as a word-aligned transliteration
pair.
Using word-aligned transliteration pairs extracted
from the Web text, we derive a binary feature in-
dicating whether katakana word yi corresponds to
a single English word. Additionally, we derive an-
other feature indicating whether a katakana word 2-
gram yi_1yi corresponds to an English word 2-gram.
</bodyText>
<subsectionHeader confidence="0.989376">
5.1 Parenthetical expressions
</subsectionHeader>
<bodyText confidence="0.982665666666667">
In Japanese and other Asian languages, transliter-
ated words are sometimes followed by their English
back-transliterations inside parentheses:
</bodyText>
<equation confidence="0.674555">
(2) a. アメリカでは ジャンク
(junk)
b. トラックバック スパム
(spam)
</equation>
<bodyText confidence="0.99754">
where the underline indicates the Japanese text
that is followed by English back-transliteration.
We extract word-aligned transliteration pairs from
</bodyText>
<tableCaption confidence="0.9867675">
Table 3: Word-aligned transliteration pairs. The number
indicates the word alignment.
</tableCaption>
<figure confidence="0.222913333333333">
Japanese English
ジャンク1フード 2 junk1 food2
スパム3 spam3
</figure>
<bodyText confidence="0.993580266666667">
such parenthetical expressions by establishing the
correspondences between pre-parenthesis and in-
parenthesis words.
To accomplish this, we have to resolve three prob-
lems: (a) English words inside parenthesis do not
always provide a back-transliteration of the pre-
parenthesis text, (b) the left boundary of the pre-
parenthesis text, denoted as ‘11’ in the example, has
to be identified, and (c) pre-parenthesis text, which
is a katakana noun compound in our case, has to be
segmented into words.
Although several studies have explored mining
transliterations from such parenthetical expressions
(Cao et al., 2007; Wu et al., 2009), the last problem
has not been given much attention. In the past stud-
ies, the pre-parenthesis text is assumed to be cor-
rectly segmented by, typically, using existent word
segmentation systems. This is, however, not appro-
priate for our purpose, because pre-parenthesis text
is a katakana noun compound, which is hard for ex-
isting systems to handle, and hence the alignment
quality is inevitably affected by segmentation errors.
To handle these three problems, we use the pho-
netic properties of the transliterations. For the pur-
pose of explanation, we shall first focus on problem
(c). Since transliterated katakana words preserve the
pronunciation of the original English words to some
extent (Knight and Graehl, 1998), we can discover
the correspondences between substrings of the two
languages based on phonetic similarity:
</bodyText>
<equation confidence="0.5903785">
(3) a. [ジャン]1[ク]2[フー]3[ド ]4
b. [jun]1[k]2 [foo]3[d]4
</equation>
<bodyText confidence="0.999557333333333">
Note that these are the pre-parenthesis and in-
parenthesis text in (2a). The substrings surrounded
by square brackets with the same number corre-
spond to each other. Given such a correspondence,
we can segment the pre-parenthesis text (3a) accord-
ing to its English counterpart (3b), in which words
</bodyText>
<figure confidence="0.410401">
フード (junk food) と...
(food)
(spam) を撃退する...
</figure>
<page confidence="0.996553">
963
</page>
<tableCaption confidence="0.999139">
Table 4: Example of the substring alignment A between
</tableCaption>
<table confidence="0.979805333333333">
f =‘ジャンクフード ’ and e =‘junkfood’ (JAI = 4).
(fi, ei) log p(fi, ei)
(ジャン, jun) −10.767
(ク, k) −5.319
(フー, foo) −11.755
(ド, d) −5.178
</table>
<bodyText confidence="0.99982325">
are separated by white space. We can recognize that
the katakana string ‘ジャンク’, which is the con-
catenation of the first two substrings in (3a), forms
a single word because it corresponds to the English
word junk, and so on. Consequently, (3a) can be seg-
mented into two words, ‘ジャンク (junk)’ and ‘フー
ド (food)’. The word alignment is trivially estab-
lished.
For problems (a) and (b), we can also use the
phonetic similarity between pre-parenthesis and in-
parenthesis text. If the parenthetical expression does
not provide the transliteration, or if the left boundary
is erroneously identified, we can expect the phonetic
similarity to become small. Such situations thus can
be identified.
The remainder of this section details this ap-
proach. Section 5.2 presents a probabilistic model
for discovering substring alignment such as (3). Sec-
tion 5.3 shows how to extract word-aligned translit-
eration pairs by using the probabilistic model.
</bodyText>
<subsectionHeader confidence="0.992879">
5.2 Phonetic similarity model
</subsectionHeader>
<bodyText confidence="0.999719111111111">
To establish the substring alignment between
katakana and Latin alphabet strings, we use the
probabilistic model proposed by (Jiampojamarn et
al., 2007). Let f and e be katakana and alphabet
strings, and A be the substring alignment between
them. More precisely, A is a set of corresponding
substring pairs (fz, ez) such that f = f1f2 . . . fjAj
and e = e1e2 ... ejAj. The probability of such align-
ment is defined as
</bodyText>
<equation confidence="0.978932">
log p(f,e,A) = � logp(fz, ez).
(fi,ei)EA
</equation>
<bodyText confidence="0.999159578947369">
Since A is usually unobservable, it is treated as a
hidden variable. Table 4 illustrates an example of
the substring alignment between f =‘ジャンクフー
ド’ and e =‘junkfood’, and the likelihood of each
substring pair estimated in our experiment.
The model parameters are estimated from a set of
transliteration pairs (f, e) using the EM algorithm.
In the E-step, we estimate p(A|f, e) based on the
current parameters. In the parameter estimation, we
restrict both fz and ez to be at most three characters
long. Doing this not only makes the E-step compu-
tationally efficient but avoids over-fitting by forbid-
ding too-long substrings to be aligned. In the M-
step, the parameter is re-estimated using the result
of the E-step. We can accomplish this by using an
extension of the forward-backward algorithm. See
(Jiampojamarn et al., 2007) for details.
Given a new transliteration pair (f, e), we can de-
termine the substring alignment as
</bodyText>
<equation confidence="0.9075085">
A* = argmax log p(f, e, A).
A
</equation>
<bodyText confidence="0.999587666666667">
In finding the substring alignment, a white space on
the English side is used as a constraint, so that the
English substring ez does not span a white space.
</bodyText>
<subsectionHeader confidence="0.9114315">
5.3 Extracting word-aligned transliteration
pairs
</subsectionHeader>
<bodyText confidence="0.999950904761905">
The word-aligned transliteration pairs are extracted
using the phonetic similarity model, as follows.
First, candidate transliteration pairs (f, e) are ex-
tracted from the parenthetical expressions. This is
done by extracting English words inside parenthe-
ses and pre-parenthesis text written in katakana. En-
glish words are normalized by lower-casing capital
letters.
Second, we determine the left boundary by using
the confidence score: 1 N logp(f, e, A*), where N is
the number of English words. The term 1 N prevents
the score from being unreasonably small when there
are many words. We truncate f by removing the
leftmost characters one by one, until the confidence
score exceeds a predefined threshold θ. If f becomes
empty, the pair is regarded as a non-transliteration
and discarded.
Finally, for the remaining pairs, the Japanese side
is segmented and the word alignment is established
according to A*. This results in a list of word-
aligned transliteration pairs (Table 3).
</bodyText>
<sectionHeader confidence="0.998659" genericHeader="evaluation">
6 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999866">
We conducted experiments to investigate how the
use of the paraphrasing and the back-transliteration
</bodyText>
<page confidence="0.996341">
964
</page>
<bodyText confidence="0.9986905">
improves the performance of the discriminative
model.
</bodyText>
<subsectionHeader confidence="0.995172">
6.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999974975609756">
To train the phonetic similarity model, we used
a set of transliteration pairs extracted from the
Wikipedia.4 Since person names are almost always
transliterated when they are imported from English
into Japanese, we made use of the Wikipedia arti-
cles that belong to the Living people category. From
the titles of those articles, we automatically ex-
tracted person names written in katakana, together
with their English counterparts obtainable via the
multilingual links provided by the Wikipedia. This
yielded 17,509 transliteration pairs for training. In
performing the EM algorithm, we tried ten differ-
ent initial parameters and selected the model that
achieved the highest likelihood.
The data for training and testing the percep-
tron was built using a Japanese-English dictionary
EDICT.5 We randomly extracted 5286 entries writ-
ten in katakana from EDICT and manually anno-
tated word boundaries by establishing word corre-
spondences to their English transliterations. Since
English transliterations are already provided by
EDICT, the annotation can be trivially done by na-
tive speakers of Japanese. Using this data set, we
performed 2-fold cross-validation for testing the per-
ceptron. The number of iterations was set to 20 in all
the experiments.
To compute the dictionary-based feature DICT(y)
in our basic feature set, we used NAIST-jdic.6 It is
the largest dictionary used for Japanese word seg-
mentation, and it includes 19,885 katakana words.
As Web corpora, we used 1.7 G sentences of
blog articles. From the corpora, we extracted
14,966,205 (potential) paraphrases of katakana noun
compounds together with their frequencies. We
also extracted 151,195 word-aligned transliteration
pairs. In doing this, we ranged the threshold 0 in
{−10, −20, · · · −150} and chose the value that per-
formed the best (0 = −80).
The results were evaluated using precision, recall,
F1-score, and accuracy. Precision is the number of
correctly identified words divided by the number of
</bodyText>
<footnote confidence="0.999732333333333">
4http://ja.wikipedia.org/
5http://www.csse.monash.edu.au/˜jwb/edict doc.html
6http://sourceforge.jp/projects/naist-jdic
</footnote>
<bodyText confidence="0.999841833333333">
all identified words, recall is the number of correctly
identified words divided by the number of all ora-
cle words, the F1-score is their harmonic mean, and
accuracy is the number of correctly split katakana
noun compounds divided by the number of all the
katakana noun compounds.
</bodyText>
<subsectionHeader confidence="0.99911">
6.2 Baseline systems
</subsectionHeader>
<bodyText confidence="0.999031">
We compared our system with three frequency-
based baseline system, two supervised baselines,
and two state-of-the-art word segmentation base-
lines. The first frequency-based baseline, UNI-
GRAM, performs compound splitting based on a
word 1-gram language model (Schiller, 2005; Al-
fonseca et al., 2008b):
</bodyText>
<equation confidence="0.9992465">
�y* = argmax p(yi),
yEY(-) i
</equation>
<bodyText confidence="0.999646">
where p(yi) represents the probability of yi. The
second frequency-based baseline, GMF, outputs the
splitting option with the highest geometric mean fre-
quency of the constituent words (Koehn and Knight,
2003):
</bodyText>
<equation confidence="0.997430333333333">
y* = argmax 1/|y|
yEY(-) GMF(y) = argmax{ ri f(yi) ,
yEY(-) i
</equation>
<bodyText confidence="0.9995222">
where f(yi) represents the frequency of yi. The
third frequency-based baseline, GMF2, is a mod-
ification of GMF proposed by Nakazawa et al.
(2005). It is based on the following score instead
of GMF(y):
</bodyText>
<equation confidence="0.998983">
GMF(y) (|y |= 1)
GMF(y) (|y |? 2),
Nl +α
C
</equation>
<bodyText confidence="0.990055">
where C, N, and α are hyperparameters and l is the
average length of the constituent words. Following
(Nakazawa et al., 2005), the hyperparameters were
set as C = 2500, N = 4, and α = 0.7. We estimated
p(y) and f(y) from the Web corpora.
The first supervised baseline, AP, is the aver-
aged perceptron model trained using only the ba-
sic feature set. The second supervised baseline,
AP+GMF2 is a combination of AP and GMF2,
which performed the best amongst the frequency-
based baselines. Following (Alfonseca et al.,
</bodyText>
<equation confidence="0.94420525">
GMF2(y) =
⎧
⎨⎪
⎪⎩
</equation>
<page confidence="0.99887">
965
</page>
<tableCaption confidence="0.99997">
Table 5: Comparison with baseline systems.
</tableCaption>
<table confidence="0.999799111111111">
Type System P R Fl Acc
Frequency UNIGRAM 64.2 49.7 56.0 63.0
GMF 42.9 62.0 50.7 47.5
GMF2 67.4 76.0 71.5 72.5
Supervised AP 81.9 82.5 82.2 83.4
AP+GMF2 83.0 83.9 83.4 84.2
PROPOSED 86.4 87.4 87.1 87.6
Word seg. JUMAN 71.4 60.1 65.3 69.8
MECAB 72.4 73.7 67.8 71.6
</table>
<bodyText confidence="0.998252888888889">
2008b), GMF2 is integrated into AP as two bi-
nary features indicating whether GMF2(y) is larger
than any other candidates, and whether GMF2(y) is
larger than the non-split candidate. Although Alfon-
seca et al. (2008b) also proposed using (the log of)
the geometric mean frequency as a feature, doing so
degraded performance in our experiment.
Regarding the two state-of-the-art word segmen-
tation systems, one is JUMAN,7 a rule-based word
segmentation system (Kurohashi and Nagao, 1994),
and the other is MECAB,8 a supervised word seg-
mentation system based on CRFs (Kudo et al.,
2004). These two baselines were chosen in order to
show how well existing word segmentation systems
perform this task. Although the literature states that
it is hard for existing systems to deal with katakana
noun compounds (Nakazawa et al., 2005), no empir-
ical data on this issue has been presented until now.
</bodyText>
<subsectionHeader confidence="0.999934">
6.3 Splitting result
</subsectionHeader>
<bodyText confidence="0.9997062">
Table 5 compares the performance of our system
(PROPOSED) with the baseline systems. First of all,
we can see that PROPOSED clearly improved the per-
formance of AP, demonstrating the effectiveness of
using paraphrases and back-transliterations.
Our system also outperformed all the frequency-
based baselines (UNIGRAM, GMF, and GMF2). This
is not surprising, since the simple supervised base-
line, AP, already outperformed the unsupervised
frequency-based ones. Indeed similar experimental
results were also reported by Alfonseca (2008a). An
interesting observation here is the comparison be-
tween PROPOSED and AP+GMF2. It reveals that
our approach improved the performance of AP more
than the frequency-based method did. These results
</bodyText>
<footnote confidence="0.999577">
7http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
8http://sourceforge.net/projects/mecab
</footnote>
<bodyText confidence="0.999832607142857">
indicate that paraphrasing and back-transliteration
are more informative clues than the simple fre-
quency of constituent words. We would like to
note that the higher accuracy of PROPOSED in com-
parison with the baselines is statistically significant
(p &lt; 0.01, McNemar’s test).
The performance of the two word segmenta-
tion baselines (JUMAN and MECAB) is significantly
worse in our task than in the standard word segmen-
tation task, where nearly 99% precision and recall
are reported (Kudo et al., 2004). This demonstrates
that splitting a katakana noun compound is not at
all a trivial task to resolve, even for the state-of-the-
art word segmentation systems. On the other hand,
PROPOSED outperformed both JUMAN and MECAB
in this task, meaning that our technique can suc-
cessfully complement the weaknesses of the existing
word segmentation systems.
By analyzing the errors, we interestingly found
that some of the erroneous splitting results are still
acceptable to humans. For example, while ‘アップ
ロード (upload)’ was annotated as a single word in
the test data, our system split it into ‘アップ (up)’
and ‘ロード (load)’. Although the latter splitting
may be useful in some applications, it is judged as
wrong in our evaluation framework. This implies
the importance of evaluating the splitting results in
some extrinsic tasks. We leave it to a future work.
</bodyText>
<subsectionHeader confidence="0.997804">
6.4 Investigation on out-of-vocabulary words
</subsectionHeader>
<bodyText confidence="0.999987444444445">
In our test data, 2681 out of the 5286 katakana noun
compounds contained at least one out-of-vocabulary
word that are not registered in NAIST-jdic. Table 6
illustrates the results of the supervised systems for
those 2681 and the remaining 2605 katakana noun
compounds (referred to as w/ OOV and w/o OOV
data, respectively). While the accuracy exceeds 90%
for w/o OOV data, it is substantially degraded for w/
OOV data. This is consistent with our claim that out-
of-vocabulary words are a major source of errors in
splitting noun compounds.
The three supervised systems performed almost
equally for w/o OOV data. This is because AP triv-
ially performs very well on this subset, and it is dif-
ficult to get any further improvement. On the other
hand, we can see that there are substantial perfor-
mance gaps between the systems for w/ OOV data.
This result reflects the effect of the additional fea-
</bodyText>
<page confidence="0.997997">
966
</page>
<tableCaption confidence="0.996309">
Table 6: Splitting results of the supervised systems for w/ OOV and w/o OOV data.
</tableCaption>
<table confidence="0.8931355">
w/ OOV data w/o OOV data
System P R Fl Acc P R Fl Acc
AP 66.9 69.9 68.3 72.8 95.4 93.2 94.3 94.2
AP+GMF2 69.7 73.7 71.6 75.2 95.2 92.4 93.7 93.6
PROPOSED 76.8 79.3 78.0 80.9 95.3 94.2 94.8 94.5
tures more directly than is shown in table 5.
</table>
<subsectionHeader confidence="0.977877">
6.5 Effect of the two new features
</subsectionHeader>
<bodyText confidence="0.99990909375">
To see the effect of the new features in more detail,
we looked at the performances of our system using
different feature sets (Table 7). The first column
represents the feature set we used: BASIC, PARA,
TRANS, and ALL represent the basic features, the
paraphrase feature, the back-transliteration feature,
and all the features. The results demonstrate that
adding either of the new features improved the per-
formance, and the best result was when they were
used together. In all cases, the improvement over
BASIC was statistically significant (p &lt; 0.01, Mc-
Nemar’s test).
Next, we investigated the coverage of the features.
Our test data comprised 7709 constituent words,
4937 (64.0%) of which were covered by NAIST-
jdic. The coverage was significantly improved when
using the back-transliteration feature. We observed
that 6216 words (80.6%) are in NAIST-jdic or word-
aligned transliteration pairs extracted from the Web
text. This shows that the back-transliteration fea-
ture successfully reduced the number of out-of-
vocabulary words. On the other hand, we observed
that the paraphrase and back-transliteration features
were activated for 79.5% (1926/2423) and 15.5%
(376/2423) of the word boundaries in our test data.
Overall, we see that the coverage of these fea-
tures is reasonably good, although there is still room
for further improvement. It would be beneficial to
use larger Web corpora or more paraphrase rules,
for example, by having a system that automatically
learns rules from the corpora (Barzilay and McKe-
own, 2001; Bannard and Callison-Burch, 2005).
</bodyText>
<subsectionHeader confidence="0.992111">
6.6 Sensitivity on the threshold 0
</subsectionHeader>
<bodyText confidence="0.999279666666667">
Finally we investigated the influence of the thresh-
old 0 (Figure 1 and 2). Figure 1 illustrates the system
performance in terms of Fl-score for different values
</bodyText>
<tableCaption confidence="0.9153275">
Table 7: Effectiveness of paraphrase (PARA) and back-
transliteration feature (TRANS).
</tableCaption>
<table confidence="0.9993816">
Feature set P R Fl Acc
BASIC 81.9 82.5 82.2 83.4
BASIC+PARA 85.1 85.3 85.2 85.9
BASIC+TRANS 85.1 86.3 85.7 86.5
ALL 86.4 87.4 87.1 87.6
</table>
<bodyText confidence="0.9955987">
of 0. While the Fl-score drops when the value of 0
is too large (e.g., −20), the Fl-score is otherwise al-
most constant. This demonstrates it is generally easy
to set 0 near the optimal value. More importantly,
the Fl-score is consistently higher than BASIC irre-
spective of the value of 0. Figure 2 represents the
number of distinct word-aligned transliteration pairs
that were extracted from the Web corpora. We see
that most of the extracted transliteration pairs have
high confidence score.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986944444444">
In this paper, we explored the idea of using monolin-
gual and bilingual paraphrases for splitting katakana
noun compounds in Japanese. The experiments
demonstrated that our method significantly im-
proves the splitting accuracy by a large margin
in comparison with the previously proposed meth-
ods. This means that paraphrasing provides a sim-
ple and effective way of using unlabeled textual
data for identifying implicit word boundaries within
katakana noun compounds.
Although our investigation was restricted to
katakana noun compounds, one might expect that a
similar approach would be useful for splitting other
types of noun compounds (e.g., German noun com-
pounds), or for identifying general word boundaries,
not limited to those between nouns, in Asian lan-
guages. We think these are research directions worth
exploring in the future.
</bodyText>
<page confidence="0.969072">
967
</page>
<figure confidence="0.980444333333333">
F1-s core 88
87
86
85
84
83
82
81
Threshold
</figure>
<figureCaption confidence="0.997526333333333">
Figure 1: Influence of the threshold 0 (x-axis) on the Fl-
score (y-axis). The triangles and squares represent sys-
tems using the ALL and BASIC feature sets, respectively.
</figureCaption>
<figure confidence="0.997520833333333">
200000
150000
100000
50000
0
Threshold
</figure>
<figureCaption confidence="0.998129666666667">
Figure 2: The number of distinct word-aligned transliter-
ations pairs that were extracted from the Web corpora for
different values of 0.
</figureCaption>
<sectionHeader confidence="0.976544" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9074478">
This work was supported by the Multimedia Web
Analysis Framework towards Development of Social
Analysis Software program of the Ministry of Ed-
ucation, Culture, Sports, Science and Technology,
Japan.
</bodyText>
<sectionHeader confidence="0.995839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998825710144927">
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008a. Decompoundig query keywords from com-
pounding languages. In Proceedings of ACL, Short
Papers, pages 253–256.
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008b. German decompounding in a difficult corpus.
In Proceedings of CICLing, pages 128–139.
Rie Kubota Ando and Lillian Lee. 2003. Mostly-
unsupervised statistical segmentation of Japanese
Kanji sequences. Natural Language Engineering,
9(2):127–149.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597–604.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL, pages
693–702.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
ofACL, pages 50–57.
Martin Braschler and B¨arbel Ripplinger. 2004. How ef-
fective is stemming and decompounding for German
text retrieval? Information Retrieval, 7:291–316.
Jamese Breen. 2009. Identification of neologisms in
Japanese by corpus analysis. In Proceedings of eLexi-
cography in the 21st centry conference, pages 13–22.
Eric Brill, Gray Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-English term pairs
from search engine query logs. In Proceedings of NL-
PRS, pages 393–399.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proceedings of TMI.
Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2007. A
system to mine large-scale bilingual dictionaries from
monolingual Web pages. In Proceedings of MT Sum-
mit, pages 57–64.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL, pages 406–414.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277–296.
Florian Holz and Chris Biemann. 2008. Unsupervised
and knowledge-free learning of compound splits and
periphrases. In CICLing, pages 117–127.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignment and
hidden Markov models to letter-to-phoneme conver-
sion. In HLT-NAACL, pages 372–379.
Kyo Kageura, Fuyuki Yoshikane, and Takayuki Nozawa.
2004. Parallel bilingual paraphrase rules for noun
compounds: Concepts and rules for exploring Web
language resources. In Proceedings of Workshop on
Asian Language Resources, pages 54–61.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599–
612.
Philip Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL,
pages 187–193.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of EMNLP,
pages 230–237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer JU-
MAN. In Proceedings of the International Workshop
on Sharable Natural Language Resources, pages 22–
38.
</reference>
<figure confidence="0.9970658">
pairs
ration
e p
# o
f tra n slit
</figure>
<page confidence="0.970032">
968
</page>
<reference confidence="0.99970628125">
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of basic
Katakana lexicon from a given corpus. In Proceedings
of IJCNLP, pages 682–693.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17–24.
Preslav Nakov and Marti Hearst. 2005b. Using the Web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of HLT/EMNLP,
pages 835–342.
Jong-Hoon Oh and Hitoshi Isahara. 2008. Hypothesis
selection in machine transliteration: A Web mining
approach. In Proceedings of IJCNLP, pages 233–240.
Naoaki Okazaki, Sophia Ananiadou, and Jin’ichi Tsujii.
2008. A discriminative alignment model for abbrevi-
ation recognition. In Proceedings of COLING, pages
657–664.
Anne Schiller. 2005. German compound analysis with
wfsc. In Proceedings of Finite State Methods and Nat-
ural Language Processing, pages 239–246.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of PSB, pages 451–
462.
Natsuko Tsujimura. 2006. An Introduction to Japanese
Linguistics. Wiley-Blackwell.
Xianchao Wu, Naoaki Okazaki, and Jun’ichi Tsujii.
2009. Semi-supervised lexicon mining from paren-
thetical expressions in monolingual Web pages. In
Proceedings of NAACL, pages 424–432.
</reference>
<page confidence="0.9988">
969
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888854">
<title confidence="0.997937">Splitting Noun Compounds via Monolingual and Bilingual A Study on Japanese Katakana Words</title>
<author confidence="0.98599">Nobuhiro Kaji Masaru Kitsuregawa</author>
<affiliation confidence="0.9953975">Institute of Industrial Science Institute of Industrial Science University of Tokyo, Tokyo, Japan University of Tokyo, Tokyo,</affiliation>
<email confidence="0.917539">kaji@tkl.iis.u-tokyo.ac.jpkitsure@tkl.iis.u-tokyo.ac.jp</email>
<abstract confidence="0.999760578947368">Word boundaries within noun compounds are not marked by white spaces in a number of languages, unlike in English, and it is beneficial for various NLP applications to split such noun compounds. In the case of Japanese, noun compounds made up of katakana words (i.e., transliterated foreign words) are particularly difficult to split, because katakana words are highly productive and are often outof-vocabulary. To overcome this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Slaven Bilac</author>
<author>Stefan Pharies</author>
</authors>
<title>Decompoundig query keywords from compounding languages.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL, Short Papers,</booktitle>
<pages>253--256</pages>
<contexts>
<context position="3290" citStr="Alfonseca et al., 2008" startWordPosition="486" endWordPosition="489">3; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959–969, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics 2003; Nakazawa et al., 2005). The idea behind those</context>
<context position="8053" citStr="Alfonseca et al. (2008" startWordPosition="1197" endWordPosition="1200">, 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data on the Web. In a similar spirit, Alfonseca et al. (2008b) proposed the use of query logs for compound splitting.3 Their experimental results, however, did not clearly 3Although they also proposed using anchor text, this slightly degraded the performance. 960 demonstrate their method’s effectiveness. Without the query logs, the accuracy is reported to drop only slightly from 90.55% to 90.45%. In contrast, our experimental results showed statistically significant improvements as a result of using additional resources. Moreover, we used only textual data, which is easily available, unlike query logs. Holz and Biemann (2008) proposed a method for spli</context>
<context position="26155" citStr="Alfonseca et al., 2008" startWordPosition="4110" endWordPosition="4114">.jp/projects/naist-jdic all identified words, recall is the number of correctly identified words divided by the number of all oracle words, the F1-score is their harmonic mean, and accuracy is the number of correctly split katakana noun compounds divided by the number of all the katakana noun compounds. 6.2 Baseline systems We compared our system with three frequencybased baseline system, two supervised baselines, and two state-of-the-art word segmentation baselines. The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al., 2008b): �y* = argmax p(yi), yEY(-) i where p(yi) represents the probability of yi. The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003): y* = argmax 1/|y| yEY(-) GMF(y) = argmax{ ri f(yi) , yEY(-) i where f(yi) represents the frequency of yi. The third frequency-based baseline, GMF2, is a modification of GMF proposed by Nakazawa et al. (2005). It is based on the following score instead of GMF(y): GMF(y) (|y |= 1) GMF(y) (|y |? 2), Nl +α C where C, N, and α are hyperparameters and l is the </context>
<context position="27758" citStr="Alfonseca et al. (2008" startWordPosition="4392" endWordPosition="4396">rmed the best amongst the frequencybased baselines. Following (Alfonseca et al., GMF2(y) = ⎧ ⎨⎪ ⎪⎩ 965 Table 5: Comparison with baseline systems. Type System P R Fl Acc Frequency UNIGRAM 64.2 49.7 56.0 63.0 GMF 42.9 62.0 50.7 47.5 GMF2 67.4 76.0 71.5 72.5 Supervised AP 81.9 82.5 82.2 83.4 AP+GMF2 83.0 83.9 83.4 84.2 PROPOSED 86.4 87.4 87.1 87.6 Word seg. JUMAN 71.4 60.1 65.3 69.8 MECAB 72.4 73.7 67.8 71.6 2008b), GMF2 is integrated into AP as two binary features indicating whether GMF2(y) is larger than any other candidates, and whether GMF2(y) is larger than the non-split candidate. Although Alfonseca et al. (2008b) also proposed using (the log of) the geometric mean frequency as a feature, doing so degraded performance in our experiment. Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al., 2004). These two baselines were chosen in order to show how well existing word segmentation systems perform this task. Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et </context>
</contexts>
<marker>Alfonseca, Bilac, Pharies, 2008</marker>
<rawString>Enrique Alfonseca, Slaven Bilac, and Stefan Pharies. 2008a. Decompoundig query keywords from compounding languages. In Proceedings of ACL, Short Papers, pages 253–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Slaven Bilac</author>
<author>Stefan Pharies</author>
</authors>
<title>German decompounding in a difficult corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>128--139</pages>
<contexts>
<context position="3290" citStr="Alfonseca et al., 2008" startWordPosition="486" endWordPosition="489">3; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959–969, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics 2003; Nakazawa et al., 2005). The idea behind those</context>
<context position="8053" citStr="Alfonseca et al. (2008" startWordPosition="1197" endWordPosition="1200">, 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data on the Web. In a similar spirit, Alfonseca et al. (2008b) proposed the use of query logs for compound splitting.3 Their experimental results, however, did not clearly 3Although they also proposed using anchor text, this slightly degraded the performance. 960 demonstrate their method’s effectiveness. Without the query logs, the accuracy is reported to drop only slightly from 90.55% to 90.45%. In contrast, our experimental results showed statistically significant improvements as a result of using additional resources. Moreover, we used only textual data, which is easily available, unlike query logs. Holz and Biemann (2008) proposed a method for spli</context>
<context position="26155" citStr="Alfonseca et al., 2008" startWordPosition="4110" endWordPosition="4114">.jp/projects/naist-jdic all identified words, recall is the number of correctly identified words divided by the number of all oracle words, the F1-score is their harmonic mean, and accuracy is the number of correctly split katakana noun compounds divided by the number of all the katakana noun compounds. 6.2 Baseline systems We compared our system with three frequencybased baseline system, two supervised baselines, and two state-of-the-art word segmentation baselines. The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al., 2008b): �y* = argmax p(yi), yEY(-) i where p(yi) represents the probability of yi. The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003): y* = argmax 1/|y| yEY(-) GMF(y) = argmax{ ri f(yi) , yEY(-) i where f(yi) represents the frequency of yi. The third frequency-based baseline, GMF2, is a modification of GMF proposed by Nakazawa et al. (2005). It is based on the following score instead of GMF(y): GMF(y) (|y |= 1) GMF(y) (|y |? 2), Nl +α C where C, N, and α are hyperparameters and l is the </context>
<context position="27758" citStr="Alfonseca et al. (2008" startWordPosition="4392" endWordPosition="4396">rmed the best amongst the frequencybased baselines. Following (Alfonseca et al., GMF2(y) = ⎧ ⎨⎪ ⎪⎩ 965 Table 5: Comparison with baseline systems. Type System P R Fl Acc Frequency UNIGRAM 64.2 49.7 56.0 63.0 GMF 42.9 62.0 50.7 47.5 GMF2 67.4 76.0 71.5 72.5 Supervised AP 81.9 82.5 82.2 83.4 AP+GMF2 83.0 83.9 83.4 84.2 PROPOSED 86.4 87.4 87.1 87.6 Word seg. JUMAN 71.4 60.1 65.3 69.8 MECAB 72.4 73.7 67.8 71.6 2008b), GMF2 is integrated into AP as two binary features indicating whether GMF2(y) is larger than any other candidates, and whether GMF2(y) is larger than the non-split candidate. Although Alfonseca et al. (2008b) also proposed using (the log of) the geometric mean frequency as a feature, doing so degraded performance in our experiment. Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al., 2004). These two baselines were chosen in order to show how well existing word segmentation systems perform this task. Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et </context>
</contexts>
<marker>Alfonseca, Bilac, Pharies, 2008</marker>
<rawString>Enrique Alfonseca, Slaven Bilac, and Stefan Pharies. 2008b. German decompounding in a difficult corpus. In Proceedings of CICLing, pages 128–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Lillian Lee</author>
</authors>
<title>Mostlyunsupervised statistical segmentation of Japanese Kanji sequences.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="7398" citStr="Ando and Lee, 2003" startWordPosition="1096" endWordPosition="1099">the performance in terms of Fi-score, and the best performance was achieved when they were combined. We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. 2 Related Work 2.1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al., 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data </context>
</contexts>
<marker>Ando, Lee, 2003</marker>
<rawString>Rie Kubota Ando and Lillian Lee. 2003. Mostlyunsupervised statistical segmentation of Japanese Kanji sequences. Natural Language Engineering, 9(2):127–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="33482" citStr="Bannard and Callison-Burch, 2005" startWordPosition="5308" endWordPosition="5311">s shows that the back-transliteration feature successfully reduced the number of out-ofvocabulary words. On the other hand, we observed that the paraphrase and back-transliteration features were activated for 79.5% (1926/2423) and 15.5% (376/2423) of the word boundaries in our test data. Overall, we see that the coverage of these features is reasonably good, although there is still room for further improvement. It would be beneficial to use larger Web corpora or more paraphrase rules, for example, by having a system that automatically learns rules from the corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005). 6.6 Sensitivity on the threshold 0 Finally we investigated the influence of the threshold 0 (Figure 1 and 2). Figure 1 illustrates the system performance in terms of Fl-score for different values Table 7: Effectiveness of paraphrase (PARA) and backtransliteration feature (TRANS). Feature set P R Fl Acc BASIC 81.9 82.5 82.2 83.4 BASIC+PARA 85.1 85.3 85.2 85.9 BASIC+TRANS 85.1 86.3 85.7 86.5 ALL 86.4 87.4 87.1 87.6 of 0. While the Fl-score drops when the value of 0 is too large (e.g., −20), the Fl-score is otherwise almost constant. This demonstrates it is generally easy to set 0 near the opti</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="9815" citStr="Bansal and Klein, 2011" startWordPosition="1469" endWordPosition="1472">hat katakana noun compounds often include out-of-vocabulary words, which are difficult for the existing segmentation systems to deal with. See (Nakazawa et al., 2005) for a discussion of this point. From a word segmentation perspective, our task can be seen as a case study focusing on a certain linguistic phenomenon of particular difficulty. More importantly, we are unaware of any attempts to use paraphrases or transliterations for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL, pages 693–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="33447" citStr="Barzilay and McKeown, 2001" startWordPosition="5303" endWordPosition="5307">acted from the Web text. This shows that the back-transliteration feature successfully reduced the number of out-ofvocabulary words. On the other hand, we observed that the paraphrase and back-transliteration features were activated for 79.5% (1926/2423) and 15.5% (376/2423) of the word boundaries in our test data. Overall, we see that the coverage of these features is reasonably good, although there is still room for further improvement. It would be beneficial to use larger Web corpora or more paraphrase rules, for example, by having a system that automatically learns rules from the corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005). 6.6 Sensitivity on the threshold 0 Finally we investigated the influence of the threshold 0 (Figure 1 and 2). Figure 1 illustrates the system performance in terms of Fl-score for different values Table 7: Effectiveness of paraphrase (PARA) and backtransliteration feature (TRANS). Feature set P R Fl Acc BASIC 81.9 82.5 82.2 83.4 BASIC+PARA 85.1 85.3 85.2 85.9 BASIC+TRANS 85.1 86.3 85.7 86.5 ALL 86.4 87.4 87.1 87.6 of 0. While the Fl-score drops when the value of 0 is too large (e.g., −20), the Fl-score is otherwise almost constant. This demonstrates it is ge</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings ofACL, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
<author>B¨arbel Ripplinger</author>
</authors>
<title>How effective is stemming and decompounding for German text retrieval? Information Retrieval,</title>
<date>2004</date>
<pages>7--291</pages>
<contexts>
<context position="2837" citStr="Braschler and Ripplinger, 2004" startWordPosition="415" endWordPosition="418">uage. In Japanese as well as some European and Asian languages (e.g., German, Dutch and Korean), constituent words of compounds are not separated by white spaces, unlike in English. In those languages, it is beneficial for various NLP applications to split such compounds. For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alf</context>
</contexts>
<marker>Braschler, Ripplinger, 2004</marker>
<rawString>Martin Braschler and B¨arbel Ripplinger. 2004. How effective is stemming and decompounding for German text retrieval? Information Retrieval, 7:291–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamese Breen</author>
</authors>
<title>Identification of neologisms in Japanese by corpus analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of eLexicography in the 21st centry conference,</booktitle>
<pages>13--22</pages>
<contexts>
<context position="1706" citStr="Breen, 2009" startWordPosition="239" endWordPosition="240"> splitting models. 1 Introduction 1.1 Japanese katakana words and noun compound splitting Borrowing is a major type of word formation in Japanese, and numerous foreign words (proper names or neologisms etc.) are continuously being imported from other languages (Tsujimura, 2006). Most borrowed words in modern Japanese are transliterations1 from English and they are referred to as katakana words because transliterated foreign words are primarily spelled by using katakana characters in the Japanese writing system.2 Compound1Some researchers use the term transcription rather than transliteration (Breen, 2009). Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). 2The Japanese writing system has four character types: hiragana, katakana, kanji, and Latin alphabet. ing is another type of word formation that is common in Japanese (Tsujimura, 2006). In particular, noun compounds are frequently produced by merging two or more nouns together. These two types of word formation yield a significant amount of katakana noun compounds, making Japanese a highly productive language. In Japanese as well as some European and Asian languages (e.g., German, Dutch and Korean), con</context>
<context position="4486" citStr="Breen, 2009" startWordPosition="667" endWordPosition="668">dea behind those methods is that compounds are basically split into constituent words when they are translated into English, where the compounded words are separated by white spaces, and hence splitting rules can be learned by discovering word alignments in bilingual resources. The largest obstacle that makes compound splitting difficult is the existence of out-of-vocabulary words, which are not found in the abovementioned linguistic resources. In the Japanese case, it is known that katakana words constitute a large source of out-of-vocabulary words (Brill et al., 2001; Nakazawa et al., 2005; Breen, 2009). As we have discussed, katakana words are very productive, and thus we can no longer expect existent linguistic resources to have sufficient coverage. According to (Breen, 2009), as many as 20% of katakana words in news articles, which we think include less out-ofvocabulary words than Web and other noisy textual data, are out-of-vocabulary. Those katakana words often form noun compounds, and pose a substantial difficulty for Japanese text processing (Nakazawa et al., 2005). 1.2 Paraphrases as implicit word boundaries To alleviate the errors caused by out-of-vocabulary words, we explored the u</context>
</contexts>
<marker>Breen, 2009</marker>
<rawString>Jamese Breen. 2009. Identification of neologisms in Japanese by corpus analysis. In Proceedings of eLexicography in the 21st centry conference, pages 13–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Gray Kacmarcik</author>
<author>Chris Brockett</author>
</authors>
<title>Automatically harvesting katakana-English term pairs from search engine query logs.</title>
<date>2001</date>
<booktitle>In Proceedings of NLPRS,</booktitle>
<pages>393--399</pages>
<contexts>
<context position="4449" citStr="Brill et al., 2001" startWordPosition="659" endWordPosition="662">uistics 2003; Nakazawa et al., 2005). The idea behind those methods is that compounds are basically split into constituent words when they are translated into English, where the compounded words are separated by white spaces, and hence splitting rules can be learned by discovering word alignments in bilingual resources. The largest obstacle that makes compound splitting difficult is the existence of out-of-vocabulary words, which are not found in the abovementioned linguistic resources. In the Japanese case, it is known that katakana words constitute a large source of out-of-vocabulary words (Brill et al., 2001; Nakazawa et al., 2005; Breen, 2009). As we have discussed, katakana words are very productive, and thus we can no longer expect existent linguistic resources to have sufficient coverage. According to (Breen, 2009), as many as 20% of katakana words in news articles, which we think include less out-ofvocabulary words than Web and other noisy textual data, are out-of-vocabulary. Those katakana words often form noun compounds, and pose a substantial difficulty for Japanese text processing (Nakazawa et al., 2005). 1.2 Paraphrases as implicit word boundaries To alleviate the errors caused by out-o</context>
<context position="10217" citStr="Brill et al., 2001" startWordPosition="1526" endWordPosition="1529">ases or transliterations for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a bilingual dictionary itself; none of them explored splitting compounds. Table 1: Basic features. ID Feature Description 1 yz constituent word 1-gram 2 yz−1yz constituent word 2-gram 3 LEN(yz) #characters of yz (1, 2, 3, 4, or ≥5) 4 DICT(yz) true if yz is in the dictionary 3 Supervised Approach The task we examine in this paper is splitting a katakana noun compound x into its constituent words, y = </context>
</contexts>
<marker>Brill, Kacmarcik, Brockett, 2001</marker>
<rawString>Eric Brill, Gray Kacmarcik, and Chris Brockett. 2001. Automatically harvesting katakana-English term pairs from search engine query logs. In Proceedings of NLPRS, pages 393–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Corpus-driven splitting of compound words.</title>
<date>2002</date>
<booktitle>In Proceedings of TMI.</booktitle>
<contexts>
<context position="3620" citStr="Brown, 2002" startWordPosition="537" endWordPosition="538">ition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959–969, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics 2003; Nakazawa et al., 2005). The idea behind those methods is that compounds are basically split into constituent words when they are translated into English, where the compounded words are separated by white spaces, and hence splitting rules can be learned by discovering word alignments in bilingual resources. The largest obstacle that makes compound splitting difficult is the</context>
</contexts>
<marker>Brown, 2002</marker>
<rawString>Ralf D. Brown. 2002. Corpus-driven splitting of compound words. In Proceedings of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jianfeng Gao</author>
<author>Jian-Yun Nie</author>
</authors>
<title>A system to mine large-scale bilingual dictionaries from monolingual Web pages.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="10235" citStr="Cao et al., 2007" startWordPosition="1530" endWordPosition="1533">ions for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a bilingual dictionary itself; none of them explored splitting compounds. Table 1: Basic features. ID Feature Description 1 yz constituent word 1-gram 2 yz−1yz constituent word 2-gram 3 LEN(yz) #characters of yz (1, 2, 3, 4, or ≥5) 4 DICT(yz) true if yz is in the dictionary 3 Supervised Approach The task we examine in this paper is splitting a katakana noun compound x into its constituent words, y = (y1, y2 ... y|Y|).</context>
<context position="18266" citStr="Cao et al., 2007" startWordPosition="2861" endWordPosition="2864">spam3 such parenthetical expressions by establishing the correspondences between pre-parenthesis and inparenthesis words. To accomplish this, we have to resolve three problems: (a) English words inside parenthesis do not always provide a back-transliteration of the preparenthesis text, (b) the left boundary of the preparenthesis text, denoted as ‘11’ in the example, has to be identified, and (c) pre-parenthesis text, which is a katakana noun compound in our case, has to be segmented into words. Although several studies have explored mining transliterations from such parenthetical expressions (Cao et al., 2007; Wu et al., 2009), the last problem has not been given much attention. In the past studies, the pre-parenthesis text is assumed to be correctly segmented by, typically, using existent word segmentation systems. This is, however, not appropriate for our purpose, because pre-parenthesis text is a katakana noun compound, which is hard for existing systems to handle, and hence the alignment quality is inevitably affected by segmentation errors. To handle these three problems, we use the phonetic properties of the transliterations. For the purpose of explanation, we shall first focus on problem (c</context>
</contexts>
<marker>Cao, Gao, Nie, 2007</marker>
<rawString>Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2007. A system to mine large-scale bilingual dictionaries from monolingual Web pages. In Proceedings of MT Summit, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="2682" citStr="Dyer, 2009" startWordPosition="394" endWordPosition="395">ther. These two types of word formation yield a significant amount of katakana noun compounds, making Japanese a highly productive language. In Japanese as well as some European and Asian languages (e.g., German, Dutch and Korean), constituent words of compounds are not separated by white spaces, unlike in English. In those languages, it is beneficial for various NLP applications to split such compounds. For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et a</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of NAACL, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="12221" citStr="Freund and Schapire, 1999" startWordPosition="1882" endWordPosition="1885">the constituent word. LEN(y) returns the number of characters of y (1, 2, 3, 4, or ≥5). Feature 4 indicates whether the constituent word is registered in an external dictionary (see section 6.1). DICT(y) returns true if the word y is in the dictionary. In addition to those basic features, we also employ paraphrases and back-transliterations of katakana noun compounds as features. The features are detailed in sections 4 and 5, respectively. We can optimize the weight vector w using an arbitrary training algorithm. Here we adopt the averaged perceptron algorithm for the sake of time efficiency (Freund and Schapire, 1999). The perceptron offers efficient online training, and it performs comparatively well with batch algorithms such as SVMs. Since we use only factored features (see table 1, section 4 and section 5), dynamic programming can be used to locate y*. 961 Table 2: Paraphrase rules and examples. The first column represents the type of linguistic marker to be inserted, the second column shows the paraphrase rules, and the last column gives examples. Type Rule Example Centered dot X1X2 → X1 · X2 アンチョビパスタ → アンチョビ・パスタ (anchovy pasta) (anchovy · pasta) Possessive marker X1X2 → X1 の X2 アンチョビパスタ → アンチョビの パスタ </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Holz</author>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised and knowledge-free learning of compound splits and periphrases.</title>
<date>2008</date>
<booktitle>In CICLing,</booktitle>
<pages>117--127</pages>
<contexts>
<context position="7462" citStr="Holz and Biemann, 2008" startWordPosition="1106" endWordPosition="1109">nce was achieved when they were combined. We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. 2 Related Work 2.1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al., 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data on the Web. In a similar spirit, Alfonseca et al. (2008b) propos</context>
</contexts>
<marker>Holz, Biemann, 2008</marker>
<rawString>Florian Holz and Chris Biemann. 2008. Unsupervised and knowledge-free learning of compound splits and periphrases. In CICLing, pages 117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applying many-to-many alignment and hidden Markov models to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="20805" citStr="Jiampojamarn et al., 2007" startWordPosition="3268" endWordPosition="3271">l expression does not provide the transliteration, or if the left boundary is erroneously identified, we can expect the phonetic similarity to become small. Such situations thus can be identified. The remainder of this section details this approach. Section 5.2 presents a probabilistic model for discovering substring alignment such as (3). Section 5.3 shows how to extract word-aligned transliteration pairs by using the probabilistic model. 5.2 Phonetic similarity model To establish the substring alignment between katakana and Latin alphabet strings, we use the probabilistic model proposed by (Jiampojamarn et al., 2007). Let f and e be katakana and alphabet strings, and A be the substring alignment between them. More precisely, A is a set of corresponding substring pairs (fz, ez) such that f = f1f2 . . . fjAj and e = e1e2 ... ejAj. The probability of such alignment is defined as log p(f,e,A) = � logp(fz, ez). (fi,ei)EA Since A is usually unobservable, it is treated as a hidden variable. Table 4 illustrates an example of the substring alignment between f =‘ジャンクフー ド’ and e =‘junkfood’, and the likelihood of each substring pair estimated in our experiment. The model parameters are estimated from a set of transl</context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applying many-to-many alignment and hidden Markov models to letter-to-phoneme conversion. In HLT-NAACL, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
<author>Fuyuki Yoshikane</author>
<author>Takayuki Nozawa</author>
</authors>
<title>Parallel bilingual paraphrase rules for noun compounds: Concepts and rules for exploring Web language resources.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Asian Language Resources,</booktitle>
<pages>54--61</pages>
<contexts>
<context position="14929" citStr="Kageura et al. (2004)" startWordPosition="2338" endWordPosition="2341">he above discussion led us to use paraphrase frequencies estimated from Web text for splitting katakana noun compounds. For this purpose, we established the seven paraphrase rules illustrated in Table 2. The rules are in the form of X1X2 → X1MX2, where X1 and X2 represent nouns, and M is a certain linguistic marker (e.g., the possessive marker ‘の’). The left-hand term corresponds to a compound to be paraphrased and the right-hand term represents its paraphrase. For instance, X1 = ‘アンチョビ (anchovy)’, X2 = ‘パスタ (pasta)’, and M = ‘の’. The paraphrase rules we use are based on the rules proposed by Kageura et al. (2004) for expanding complex terms, primarily noun compounds, into their variants. 4.3 Web-based frequency as features We introduce a new feature using the paraphrase rules and Web text. As preprocessing, we use regular expressions to count the frequencies of all potential paraphrases of katakana noun compounds on the Web in advance. (katakana)+ ・ (katakana)+ (katakana)+ の (katakana)+ (katakana)+ する (katakana)+ . . . where (katakana) corresponds to one katakana character. Given a candidate segmentation y at test time, we generate paraphrases of the noun compound by setting X1 = yi−1 and X2 = yi, and</context>
</contexts>
<marker>Kageura, Yoshikane, Nozawa, 2004</marker>
<rawString>Kyo Kageura, Fuyuki Yoshikane, and Takayuki Nozawa. 2004. Parallel bilingual paraphrase rules for noun compounds: Concepts and rules for exploring Web language resources. In Proceedings of Workshop on Asian Language Resources, pages 54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<volume>24</volume>
<issue>4</issue>
<pages>612</pages>
<contexts>
<context position="1796" citStr="Knight and Graehl, 1998" startWordPosition="251" endWordPosition="254">d splitting Borrowing is a major type of word formation in Japanese, and numerous foreign words (proper names or neologisms etc.) are continuously being imported from other languages (Tsujimura, 2006). Most borrowed words in modern Japanese are transliterations1 from English and they are referred to as katakana words because transliterated foreign words are primarily spelled by using katakana characters in the Japanese writing system.2 Compound1Some researchers use the term transcription rather than transliteration (Breen, 2009). Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). 2The Japanese writing system has four character types: hiragana, katakana, kanji, and Latin alphabet. ing is another type of word formation that is common in Japanese (Tsujimura, 2006). In particular, noun compounds are frequently produced by merging two or more nouns together. These two types of word formation yield a significant amount of katakana noun compounds, making Japanese a highly productive language. In Japanese as well as some European and Asian languages (e.g., German, Dutch and Korean), constituent words of compounds are not separated by white spaces, unlike in English. In those</context>
<context position="19002" citStr="Knight and Graehl, 1998" startWordPosition="2979" endWordPosition="2982"> text is assumed to be correctly segmented by, typically, using existent word segmentation systems. This is, however, not appropriate for our purpose, because pre-parenthesis text is a katakana noun compound, which is hard for existing systems to handle, and hence the alignment quality is inevitably affected by segmentation errors. To handle these three problems, we use the phonetic properties of the transliterations. For the purpose of explanation, we shall first focus on problem (c). Since transliterated katakana words preserve the pronunciation of the original English words to some extent (Knight and Graehl, 1998), we can discover the correspondences between substrings of the two languages based on phonetic similarity: (3) a. [ジャン]1[ク]2[フー]3[ド ]4 b. [jun]1[k]2 [foo]3[d]4 Note that these are the pre-parenthesis and inparenthesis text in (2a). The substrings surrounded by square brackets with the same number correspond to each other. Given such a correspondence, we can segment the pre-parenthesis text (3a) according to its English counterpart (3b), in which words フード (junk food) と... (food) (spam) を撃退する... 963 Table 4: Example of the substring alignment A between f =‘ジャンクフード ’ and e =‘junkfood’ (JAI = 4)</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599– 612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>187--193</pages>
<contexts>
<context position="2669" citStr="Koehn and Knight, 2003" startWordPosition="390" endWordPosition="393">g two or more nouns together. These two types of word formation yield a significant amount of katakana noun compounds, making Japanese a highly productive language. In Japanese as well as some European and Asian languages (e.g., German, Dutch and Korean), constituent words of compounds are not separated by white spaces, unlike in English. In those languages, it is beneficial for various NLP applications to split such compounds. For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; A</context>
<context position="7378" citStr="Koehn and Knight, 2003" startWordPosition="1092" endWordPosition="1095"> substantially improved the performance in terms of Fi-score, and the best performance was achieved when they were combined. We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. 2 Related Work 2.1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al., 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources,</context>
<context position="26396" citStr="Koehn and Knight, 2003" startWordPosition="4148" endWordPosition="4151">pounds divided by the number of all the katakana noun compounds. 6.2 Baseline systems We compared our system with three frequencybased baseline system, two supervised baselines, and two state-of-the-art word segmentation baselines. The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al., 2008b): �y* = argmax p(yi), yEY(-) i where p(yi) represents the probability of yi. The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003): y* = argmax 1/|y| yEY(-) GMF(y) = argmax{ ri f(yi) , yEY(-) i where f(yi) represents the frequency of yi. The third frequency-based baseline, GMF2, is a modification of GMF proposed by Nakazawa et al. (2005). It is based on the following score instead of GMF(y): GMF(y) (|y |= 1) GMF(y) (|y |? 2), Nl +α C where C, N, and α are hyperparameters and l is the average length of the constituent words. Following (Nakazawa et al., 2005), the hyperparameters were set as C = 2500, N = 4, and α = 0.7. We estimated p(y) and f(y) from the Web corpora. The first supervised baseline, AP, is the averaged per</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philip Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of EACL, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="28126" citStr="Kudo et al., 2004" startWordPosition="4451" endWordPosition="4454"> 65.3 69.8 MECAB 72.4 73.7 67.8 71.6 2008b), GMF2 is integrated into AP as two binary features indicating whether GMF2(y) is larger than any other candidates, and whether GMF2(y) is larger than the non-split candidate. Although Alfonseca et al. (2008b) also proposed using (the log of) the geometric mean frequency as a feature, doing so degraded performance in our experiment. Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al., 2004). These two baselines were chosen in order to show how well existing word segmentation systems perform this task. Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et al., 2005), no empirical data on this issue has been presented until now. 6.3 Splitting result Table 5 compares the performance of our system (PROPOSED) with the baseline systems. First of all, we can see that PROPOSED clearly improved the performance of AP, demonstrating the effectiveness of using paraphrases and back-transliterations. Our system also outperformed </context>
<context position="29776" citStr="Kudo et al., 2004" startWordPosition="4693" endWordPosition="4696">method did. These results 7http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html 8http://sourceforge.net/projects/mecab indicate that paraphrasing and back-transliteration are more informative clues than the simple frequency of constituent words. We would like to note that the higher accuracy of PROPOSED in comparison with the baselines is statistically significant (p &lt; 0.01, McNemar’s test). The performance of the two word segmentation baselines (JUMAN and MECAB) is significantly worse in our task than in the standard word segmentation task, where nearly 99% precision and recall are reported (Kudo et al., 2004). This demonstrates that splitting a katakana noun compound is not at all a trivial task to resolve, even for the state-of-theart word segmentation systems. On the other hand, PROPOSED outperformed both JUMAN and MECAB in this task, meaning that our technique can successfully complement the weaknesses of the existing word segmentation systems. By analyzing the errors, we interestingly found that some of the erroneous splitting results are still acceptable to humans. For example, while ‘アップ ロード (upload)’ was annotated as a single word in the test data, our system split it into ‘アップ (up)’ and ‘ロ</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of EMNLP, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--38</pages>
<contexts>
<context position="28028" citStr="Kurohashi and Nagao, 1994" startWordPosition="4433" endWordPosition="4436"> AP 81.9 82.5 82.2 83.4 AP+GMF2 83.0 83.9 83.4 84.2 PROPOSED 86.4 87.4 87.1 87.6 Word seg. JUMAN 71.4 60.1 65.3 69.8 MECAB 72.4 73.7 67.8 71.6 2008b), GMF2 is integrated into AP as two binary features indicating whether GMF2(y) is larger than any other candidates, and whether GMF2(y) is larger than the non-split candidate. Although Alfonseca et al. (2008b) also proposed using (the log of) the geometric mean frequency as a feature, doing so degraded performance in our experiment. Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al., 2004). These two baselines were chosen in order to show how well existing word segmentation systems perform this task. Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et al., 2005), no empirical data on this issue has been presented until now. 6.3 Splitting result Table 5 compares the performance of our system (PROPOSED) with the baseline systems. First of all, we can see that PROPOSED clearly improved the performance of AP, demonstrati</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic acquisition of basic Katakana lexicon from a given corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>682--693</pages>
<contexts>
<context position="3266" citStr="Nakazawa et al., 2005" startWordPosition="482" endWordPosition="485"> (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959–969, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics 2003; Nakazawa et al., 2005</context>
<context position="4964" citStr="Nakazawa et al., 2005" startWordPosition="740" endWordPosition="743">e, it is known that katakana words constitute a large source of out-of-vocabulary words (Brill et al., 2001; Nakazawa et al., 2005; Breen, 2009). As we have discussed, katakana words are very productive, and thus we can no longer expect existent linguistic resources to have sufficient coverage. According to (Breen, 2009), as many as 20% of katakana words in news articles, which we think include less out-ofvocabulary words than Web and other noisy textual data, are out-of-vocabulary. Those katakana words often form noun compounds, and pose a substantial difficulty for Japanese text processing (Nakazawa et al., 2005). 1.2 Paraphrases as implicit word boundaries To alleviate the errors caused by out-of-vocabulary words, we explored the use of unlabeled textual data for splitting katakana noun compounds. Since the amount of unlabeled text available is generally much larger than word dictionaries and other expensive linguistic resources, it is crucial to establish a methodology for taking full advantage of such easily available textual data. While several approaches have already been proposed, their accuracies are still unsatisfactory (section 2.1). From a broad perspective, our approach can be seen as using</context>
<context position="7437" citStr="Nakazawa et al., 2005" startWordPosition="1102" endWordPosition="1105">, and the best performance was achieved when they were combined. We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. 2 Related Work 2.1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al., 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data on the Web. In a similar spirit, Alfons</context>
<context position="9358" citStr="Nakazawa et al., 2005" startWordPosition="1395" endWordPosition="1398"> algorithm is a pipeline model and paraphrasing result is not employed during splitting. 2.2 Other research topics Our study is closely related to word segmentation, which is an important research topic in Asian languages including Japanese. Although we can use existing word segmentation systems for splitting katakana noun compounds, it is difficult to reach the desired accuracy, as we will empirically demonstrate in section 6. One reason for this is that katakana noun compounds often include out-of-vocabulary words, which are difficult for the existing segmentation systems to deal with. See (Nakazawa et al., 2005) for a discussion of this point. From a word segmentation perspective, our task can be seen as a case study focusing on a certain linguistic phenomenon of particular difficulty. More importantly, we are unaware of any attempts to use paraphrases or transliterations for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and </context>
<context position="26605" citStr="Nakazawa et al. (2005)" startWordPosition="4185" endWordPosition="4188">gmentation baselines. The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al., 2008b): �y* = argmax p(yi), yEY(-) i where p(yi) represents the probability of yi. The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003): y* = argmax 1/|y| yEY(-) GMF(y) = argmax{ ri f(yi) , yEY(-) i where f(yi) represents the frequency of yi. The third frequency-based baseline, GMF2, is a modification of GMF proposed by Nakazawa et al. (2005). It is based on the following score instead of GMF(y): GMF(y) (|y |= 1) GMF(y) (|y |? 2), Nl +α C where C, N, and α are hyperparameters and l is the average length of the constituent words. Following (Nakazawa et al., 2005), the hyperparameters were set as C = 2500, N = 4, and α = 0.7. We estimated p(y) and f(y) from the Web corpora. The first supervised baseline, AP, is the averaged perceptron model trained using only the basic feature set. The second supervised baseline, AP+GMF2 is a combination of AP and GMF2, which performed the best amongst the frequencybased baselines. Following (Alfons</context>
<context position="28368" citStr="Nakazawa et al., 2005" startWordPosition="4490" endWordPosition="4493">et al. (2008b) also proposed using (the log of) the geometric mean frequency as a feature, doing so degraded performance in our experiment. Regarding the two state-of-the-art word segmentation systems, one is JUMAN,7 a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB,8 a supervised word segmentation system based on CRFs (Kudo et al., 2004). These two baselines were chosen in order to show how well existing word segmentation systems perform this task. Although the literature states that it is hard for existing systems to deal with katakana noun compounds (Nakazawa et al., 2005), no empirical data on this issue has been presented until now. 6.3 Splitting result Table 5 compares the performance of our system (PROPOSED) with the baseline systems. First of all, we can see that PROPOSED clearly improved the performance of AP, demonstrating the effectiveness of using paraphrases and back-transliterations. Our system also outperformed all the frequencybased baselines (UNIGRAM, GMF, and GMF2). This is not surprising, since the simple supervised baseline, AP, already outperformed the unsupervised frequency-based ones. Indeed similar experimental results were also reported by</context>
</contexts>
<marker>Nakazawa, Kawahara, Kurohashi, 2005</marker>
<rawString>Toshiaki Nakazawa, Daisuke Kawahara, and Sadao Kurohashi. 2005. Automatic acquisition of basic Katakana lexicon from a given corpus. In Proceedings of IJCNLP, pages 682–693.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Search engine statistics beyond the n-gram: Application to noun compound bracketing.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="9764" citStr="Nakov and Hearst, 2005" startWordPosition="1461" endWordPosition="1464">demonstrate in section 6. One reason for this is that katakana noun compounds often include out-of-vocabulary words, which are difficult for the existing segmentation systems to deal with. See (Nakazawa et al., 2005) for a discussion of this point. From a word segmentation perspective, our task can be seen as a case study focusing on a certain linguistic phenomenon of particular difficulty. More importantly, we are unaware of any attempts to use paraphrases or transliterations for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to b</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005a. Search engine statistics beyond the n-gram: Application to noun compound bracketing. In Proceedings of CoNLL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the Web as an implicit training set: Application to structural ambiguity resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>835--342</pages>
<contexts>
<context position="9764" citStr="Nakov and Hearst, 2005" startWordPosition="1461" endWordPosition="1464">demonstrate in section 6. One reason for this is that katakana noun compounds often include out-of-vocabulary words, which are difficult for the existing segmentation systems to deal with. See (Nakazawa et al., 2005) for a discussion of this point. From a word segmentation perspective, our task can be seen as a case study focusing on a certain linguistic phenomenon of particular difficulty. More importantly, we are unaware of any attempts to use paraphrases or transliterations for word segmentation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to b</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005b. Using the Web as an implicit training set: Application to structural ambiguity resolution. In Proceedings of HLT/EMNLP, pages 835–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Hypothesis selection in machine transliteration: A Web mining approach.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="10257" citStr="Oh and Isahara, 2008" startWordPosition="1534" endWordPosition="1537">entation in the same way as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a bilingual dictionary itself; none of them explored splitting compounds. Table 1: Basic features. ID Feature Description 1 yz constituent word 1-gram 2 yz−1yz constituent word 2-gram 3 LEN(yz) #characters of yz (1, 2, 3, 4, or ≥5) 4 DICT(yz) true if yz is in the dictionary 3 Supervised Approach The task we examine in this paper is splitting a katakana noun compound x into its constituent words, y = (y1, y2 ... y|Y|). Note that the output </context>
</contexts>
<marker>Oh, Isahara, 2008</marker>
<rawString>Jong-Hoon Oh and Hitoshi Isahara. 2008. Hypothesis selection in machine transliteration: A Web mining approach. In Proceedings of IJCNLP, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
<author>Jin’ichi Tsujii</author>
</authors>
<title>A discriminative alignment model for abbreviation recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>657--664</pages>
<contexts>
<context position="3097" citStr="Okazaki et al., 2008" startWordPosition="455" endWordPosition="458">s. For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Conference on Empirical Metho</context>
</contexts>
<marker>Okazaki, Ananiadou, Tsujii, 2008</marker>
<rawString>Naoaki Okazaki, Sophia Ananiadou, and Jin’ichi Tsujii. 2008. A discriminative alignment model for abbreviation recognition. In Proceedings of COLING, pages 657–664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<title>German compound analysis with wfsc.</title>
<date>2005</date>
<booktitle>In Proceedings of Finite State Methods and Natural Language Processing,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="7414" citStr="Schiller, 2005" startWordPosition="1100" endWordPosition="1101">erms of Fi-score, and the best performance was achieved when they were combined. We also confirmed that our method outperforms the previously proposed splitting methods by a wide margin. All these results strongly suggest the effectiveness of paraphrasing and back-transliteration for identifying word boundaries within katakana noun compounds. 2 Related Work 2.1 Compound splitting A common approach to splitting compounds without expensive linguistic resources is an unsupervised method based on word or string frequencies estimated from unlabeled text (Koehn and Knight, 2003; Ando and Lee, 2003; Schiller, 2005; Nakazawa et al., 2005; Holz and Biemann, 2008). Amongst others, Nakazawa et al. (2005) also investigated ways of splitting katakana noun compounds. Although the frequency-based method generally achieves high recall, its precision is not satisfactory (Koehn and Knight, 2003; Nakazawa et al., 2005). Our experiments empirically compared our method with the frequency-based methods, and the results demonstrate the advantage of our method. Our approach can be seen as augmenting discriminative models of compound splitting with large external linguistic resources, i.e., textual data on the Web. In a</context>
<context position="26131" citStr="Schiller, 2005" startWordPosition="4108" endWordPosition="4109">tp://sourceforge.jp/projects/naist-jdic all identified words, recall is the number of correctly identified words divided by the number of all oracle words, the F1-score is their harmonic mean, and accuracy is the number of correctly split katakana noun compounds divided by the number of all the katakana noun compounds. 6.2 Baseline systems We compared our system with three frequencybased baseline system, two supervised baselines, and two state-of-the-art word segmentation baselines. The first frequency-based baseline, UNIGRAM, performs compound splitting based on a word 1-gram language model (Schiller, 2005; Alfonseca et al., 2008b): �y* = argmax p(yi), yEY(-) i where p(yi) represents the probability of yi. The second frequency-based baseline, GMF, outputs the splitting option with the highest geometric mean frequency of the constituent words (Koehn and Knight, 2003): y* = argmax 1/|y| yEY(-) GMF(y) = argmax{ ri f(yi) , yEY(-) i where f(yi) represents the frequency of yi. The third frequency-based baseline, GMF2, is a modification of GMF proposed by Nakazawa et al. (2005). It is based on the following score instead of GMF(y): GMF(y) (|y |= 1) GMF(y) (|y |? 2), Nl +α C where C, N, and α are hyper</context>
</contexts>
<marker>Schiller, 2005</marker>
<rawString>Anne Schiller. 2005. German compound analysis with wfsc. In Proceedings of Finite State Methods and Natural Language Processing, pages 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Proceedings of PSB,</booktitle>
<pages>451--462</pages>
<contexts>
<context position="3074" citStr="Schwartz and Hearst, 2003" startWordPosition="451" endWordPosition="454">ions to split such compounds. For example, compound splitting enables SMT systems to translate a compound on a word-by-word basis, even if the compound itself is not found in the translation table (Koehn and Knight, 2003; Dyer, 2009). In the context of IR, decompounding has an analogous effect to stemming, and it significantly improves retrieval results (Braschler and Ripplinger, 2004). In abbreviation recognition, the definition of an abbreviation is often in the form of a noun compound, and most abbreviation recognition algorithms assume that the definition is properly segmented; see e.g., (Schwartz and Hearst, 2003; Okazaki et al., 2008). This has led NLP researchers to explore methods for splitting compounds, especially noun compounds, in various languages (Koehn and Knight, 2003; Nakazawa et al., 2005; Alfonseca et al., 2008a). While many methods have been presented, they basically require expensive linguistic resources to achieve high enough accuracy. For example, Alfonseca et al. (2008b) employed a word dictionary, which is obviously useful for this task. Other studies have suggested using bilingual resources such as parallel corpora (Brown, 2002; Koehn and Knight, 959 Proceedings of the 2011 Confer</context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In Proceedings of PSB, pages 451– 462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natsuko Tsujimura</author>
</authors>
<title>An Introduction to Japanese Linguistics.</title>
<date>2006</date>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="1372" citStr="Tsujimura, 2006" startWordPosition="192" endWordPosition="193">e this difficulty, we propose using monolingual and bilingual paraphrases of katakana noun compounds for identifying word boundaries. Experiments demonstrated that splitting accuracy is substantially improved by extracting such paraphrases from unlabeled textual data, the Web in our case, and then using that information for constructing splitting models. 1 Introduction 1.1 Japanese katakana words and noun compound splitting Borrowing is a major type of word formation in Japanese, and numerous foreign words (proper names or neologisms etc.) are continuously being imported from other languages (Tsujimura, 2006). Most borrowed words in modern Japanese are transliterations1 from English and they are referred to as katakana words because transliterated foreign words are primarily spelled by using katakana characters in the Japanese writing system.2 Compound1Some researchers use the term transcription rather than transliteration (Breen, 2009). Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). 2The Japanese writing system has four character types: hiragana, katakana, kanji, and Latin alphabet. ing is another type of word formation that is common in Japanese (Tsujim</context>
</contexts>
<marker>Tsujimura, 2006</marker>
<rawString>Natsuko Tsujimura. 2006. An Introduction to Japanese Linguistics. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Naoaki Okazaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Semi-supervised lexicon mining from parenthetical expressions in monolingual Web pages.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>424--432</pages>
<contexts>
<context position="10275" citStr="Wu et al., 2009" startWordPosition="1538" endWordPosition="1541">ay as we do. Recent studies have explored using paraphrase statistics for parsing (Nakov and Hearst, 2005a; Nakov and Hearst, 2005b; Bansal and Klein, 2011). Although these studies successfully demonstrated the usefulness of paraphrases for improving parsers, the connection between paraphrases and word segmentation (or noun compound splitting) was not at all discussed. Our method of using back-transliterations for splitting katakana noun compounds (section 5) is closely related to methods for mining transliteration from the Web text (Brill et al., 2001; Cao et al., 2007; Oh and Isahara, 2008; Wu et al., 2009). What most differentiates these studies from our work is that their primary goal is to build a machine transliteration system or to build a bilingual dictionary itself; none of them explored splitting compounds. Table 1: Basic features. ID Feature Description 1 yz constituent word 1-gram 2 yz−1yz constituent word 2-gram 3 LEN(yz) #characters of yz (1, 2, 3, 4, or ≥5) 4 DICT(yz) true if yz is in the dictionary 3 Supervised Approach The task we examine in this paper is splitting a katakana noun compound x into its constituent words, y = (y1, y2 ... y|Y|). Note that the output can be a single wo</context>
<context position="18284" citStr="Wu et al., 2009" startWordPosition="2865" endWordPosition="2868">etical expressions by establishing the correspondences between pre-parenthesis and inparenthesis words. To accomplish this, we have to resolve three problems: (a) English words inside parenthesis do not always provide a back-transliteration of the preparenthesis text, (b) the left boundary of the preparenthesis text, denoted as ‘11’ in the example, has to be identified, and (c) pre-parenthesis text, which is a katakana noun compound in our case, has to be segmented into words. Although several studies have explored mining transliterations from such parenthetical expressions (Cao et al., 2007; Wu et al., 2009), the last problem has not been given much attention. In the past studies, the pre-parenthesis text is assumed to be correctly segmented by, typically, using existent word segmentation systems. This is, however, not appropriate for our purpose, because pre-parenthesis text is a katakana noun compound, which is hard for existing systems to handle, and hence the alignment quality is inevitably affected by segmentation errors. To handle these three problems, we use the phonetic properties of the transliterations. For the purpose of explanation, we shall first focus on problem (c). Since translite</context>
</contexts>
<marker>Wu, Okazaki, Tsujii, 2009</marker>
<rawString>Xianchao Wu, Naoaki Okazaki, and Jun’ichi Tsujii. 2009. Semi-supervised lexicon mining from parenthetical expressions in monolingual Web pages. In Proceedings of NAACL, pages 424–432.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>