<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000395">
<title confidence="0.938039">
Dependency Based Chinese Sentence Realization
</title>
<author confidence="0.999324">
Wei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu1
</author>
<affiliation confidence="0.809606666666667">
1Information Retrieval Lab, Harbin Institute of Technology, Harbin, China
{whe,tliu}@ir.hit.edu.cn
2Toshiba (China) Research and Development Center, Beijing, China
</affiliation>
<email confidence="0.993186">
{wanghaifeng,guoyuqing}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.994703" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998105944444444">
This paper describes log-linear models for a
general-purpose sentence realizer based on de-
pendency structures. Unlike traditional realiz-
ers using grammar rules, our method realizes
sentences by linearizing dependency relations
directly in two steps. First, the relative order
between head and each dependent is deter-
mined by their dependency relation. Then the
best linearizations compatible with the relative
order are selected by log-linear models. The
log-linear models incorporate three types of
feature functions, including dependency rela-
tions, surface words and headwords. Our ap-
proach to sentence realization provides sim-
plicity, efficiency and competitive accuracy.
Trained on 8,975 dependency structures of a
Chinese Dependency Treebank, the realizer
achieves a BLEU score of 0.8874.
</bodyText>
<sectionHeader confidence="0.998771" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961935483872">
Sentence realization can be described as the
process of converting the semantic and syntactic
representation of a sentence or series of sen-
tences into meaningful, grammatically correct
and fluent text of a particular language.
Most previous general-purpose realization sys-
tems are developed via the application of a set of
grammar rules based on particular linguistic
theories, e.g. Lexical Functional Grammar (LFG),
Head Driven Phrase Structure Grammar (HPSG),
Combinatory Categorical Grammar (CCG), Tree
Adjoining Grammar (TAG) etc. The grammar
rules are either developed by hand, such as those
used in LinGo (Carroll et al., 1999), OpenCCG
(White, 2004) and XLE (Crouch et al., 2007), or
extracted automatically from annotated corpora,
like the HPSG (Nakanishi et al., 2005), LFG
(Cahill and van Genabith, 2006; Hogan et al.,
2007) and CCG (White et al., 2007) resources
derived from the Penn-II Treebank.
Over the last decade, there has been a lot of in-
terest in a generate-and-select paradigm for sur-
face realization. The paradigm is characterized
by a separation between realization and selection,
in which rule-based methods are used to generate
a space of possible paraphrases, and statistical
methods are used to select the most likely reali-
zation from the space. Usually, two statistical
models are used to rank the output candidates.
One is n-gram model over different units, such as
word-level bigram/trigram models (Bangalore
and Rambow, 2000; Langkilde, 2000), or fac-
tored language models integrated with syntactic
tags (White et al. 2007). The other is log-linear
model with different syntactic and semantic fea-
tures (Velldal and Oepen, 2005; Nakanishi et al.,
2005; Cahill et al., 2007).
However, little work has been done on proba-
bilistic models learning direct mapping from in-
put to surface strings, without the effort to con-
struct a grammar. Guo et al. (2008) develop a
general-purpose realizer couched in the frame-
work of Lexical Functional Grammar based on
simple n-gram models. Wan et al. (2009) present
a dependency-spanning tree algorithm for word
ordering, which first builds dependency trees to
decide linear precedence between heads and
modifiers then uses an n-gram language model to
order siblings. Compared with n-gram model,
log-linear model is more powerful in that it is
easy to integrate a variety of features, and to tune
feature weights to maximize the probability. A
few papers have presented maximum entropy
models for word or phrase ordering (Ratnaparkhi,
2000; Filippova and Strube, 2007). However,
those attempts have been limited to specialized
applications, such as air travel reservation or or-
dering constituents of a main clause in German.
This paper presents a general-purpose realizer
based on log-linear models for directly lineariz-
ing dependency relations given dependency
structures. We reduce the generation space by
</bodyText>
<page confidence="0.981342">
809
</page>
<note confidence="0.999611">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999868">
two techniques: the first is dividing the entire
dependency tree into one-depth sub-trees and
solving linearization in sub-trees; the second is
the determination of relative positions between
dependents and heads according to dependency
relations. Then the best linearization for each
sub-tree is selected by the log-linear model that
incorporates three types of feature functions, in-
cluding dependency relations, surface words and
headwords. The evaluation shows that our realiz-
er achieves competitive generation accuracy.
The paper is structured as follows. In Section
2, we describe the idea of dividing the realization
procedure for an entire dependency tree into a
series of sub-procedures for sub-trees. We de-
scribe how to determine the relative positions
between dependents and heads according to de-
pendency relations in Section 3. Section 4 gives
details of the log-linear model and the feature
functions used for sentence realization. Section 5
explains the experiments and provides the results.
</bodyText>
<sectionHeader confidence="0.9028425" genericHeader="method">
2 Sentence Realization from Dependen-
cy Structure
</sectionHeader>
<subsectionHeader confidence="0.999459">
2.1 The Dependency Input
</subsectionHeader>
<bodyText confidence="0.999978454545455">
The input to our sentence realizer is a dependen-
cy structure as represented in the HIT Chinese
Dependency Treebank (HIT-CDT)1. In our de-
pendency tree representations, dependency rela-
tions are represented as arcs pointing from a head
to a dependent. The types of dependency arcs
indicate the semantic or grammatical relation-
ships between the heads and the dependents,
which are recorded in the dependent nodes. Fig-
ure 1 gives an example of dependency tree repre-
sentation for the sentence:
</bodyText>
<equation confidence="0.999228">
(1) 这 是 武汉 航空
this is Wuhan Airline
首次 购买 波音 客机
</equation>
<bodyText confidence="0.992234">
first time buy Boeing airliner
‘This is the first time for Airline Wuhan to buy
Boeing airliners.’
In a dependency structure, dependents are un-
ordered, i.e. the string position of each node is
not recorded in the representation. Our sentence
realizer takes such an unordered dependency tree
as input, determines the linear order of the words
</bodyText>
<footnote confidence="0.963171">
1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences
and 215,334 words, which are manually annotated with
part-of-speech tags and dependency labels. (Liu et al.,
2006a)
</footnote>
<equation confidence="0.9461485">
①是(HED)
is
②这(SBV) ③购买(VOB)
this buy
④首次(ADV) ⑤客机(VOB)
first time airliner
⑦波音(ATT)
Boeing
</equation>
<figureCaption confidence="0.9848315">
Figure 1: The dependency tree for the sentence
“这是武汉航空首次购买波音客机”
</figureCaption>
<bodyText confidence="0.9999238">
as encoded in the nodes of the dependency struc-
ture and produces a grammatical sentence. As the
dependency structures input to our realizer have
been lexicalized, lexical selection is not involved
during the surface realization.
</bodyText>
<subsectionHeader confidence="0.9489045">
2.2 Divide and Conquer Strategy for Linea-
rization
</subsectionHeader>
<bodyText confidence="0.99992825">
For determining the linear order of words
represented by nodes of the given dependency
structure, in principle, the sentence realizer has
to produce all possible sequences of the nodes
from the input tree and selects the most likely
linearization among them. If the dependency tree
consists of a considerable number of nodes, this
procedure would be very time-consuming. To
reduce the number of possible realizations, our
generation algorithm adopts a divide-and-
conquer strategy, which divides the whole tree
into a set of sub-trees of depth one and recursive-
ly linearizes the sub-trees in a bottom-up fashion.
As illustrated in Figure 2, sub-trees c and d,
which are at the bottom of the tree, are linearized
first, then sub-tree b is processed, and finally
sub-tree a.
The procedure imposes a projective constraint
on the dependency structures, viz. each head
dominates a continuous substring of the sentence
realization. This assumption is feasible in the
application of the dependency-based generation,
because: (i) it has long been observed that the
dependency structures of a vast majority of sen-
tences in the languages of the world are projec-
tive (Igor, 1988) and (ii) non-projective depen-
dencies in Chinese, for the most part, are used to
account for non-local dependency phenomena.
</bodyText>
<figure confidence="0.777630333333333">
⑥航空(SBV)
airline
⑧武汉(ATT)
Wuhan
810
linearization procedure
</figure>
<figureCaption confidence="0.890104">
Figure 2: Illustration of the
</figureCaption>
<figure confidence="0.992935076923077">
sub-tree c
⑤客机(VOB)
airliner
波音 客机
⑦波音(ATT)
Boeing
sub-tree d
⑥航空(SBV)
Airline
⑧武汉(ATT)
Wuhan
武汉 航空
sub-tree a
④首次(ADV)
first time
武汉航空 首次 购买 波音客机
③购买(VOB)
buy
⑤ ⑥
sub-tree b
①是(HED)
is
②这(SBV)
this
这 是 武汉航空首次购买波音客机
③
</figure>
<table confidence="0.998780363636364">
Relation Description Postdep. Predep.
ADV adverbial 1 25977
APP appositive 807 0
ATT attribute 0 47040
CMP complement 2931 3
CNJ conjunctive 0 2124
COO coordinate 6818 0
DC dep. clause 197 0
DE DE phrase 0 10973
DEI DEI phrase 131 3
DI DI phrase 0 400
IC indep.clause 3230 0
IS indep.structure 125 794
LAD left adjunct 0 2644
MT mood-tense 3203 0
POB prep-obj 7513 0
QUN quantity 0 6092
RAD right adjunct 1332 1
SBV subject-verb 6 16016
SIM similarity 0 44
VOB verb-object 23487 21
VV verb-verb 6570 2
</table>
<tableCaption confidence="0.9842695">
Table 1: Numbers of pre/post-dependents for each
dependency relation
</tableCaption>
<bodyText confidence="0.997889571428571">
Though non-local dependencies are important for
accurate semantic analysis, they can be easily
converted to local dependencies conforming to
the projective constraint. In fact, we find that the
10, 000 manually-build dependency trees of the
HIT-CDT do not contain any non-projective de-
pendencies.
</bodyText>
<sectionHeader confidence="0.997768" genericHeader="method">
3 Relative Position Determination
</sectionHeader>
<bodyText confidence="0.999992625">
In dependency structures, the semantic or gram-
matical roles of the nodes are indicated by types
of dependency relations. For example, the VOB
dependency relation, which stands for the verb-
object structure, means that the head is a verb
and the dependent is an object of the verb; the
ATT relation, means that the dependent is an
attribute of the head. In languages with fairly
rigid word order, the relative position between
the head and dependent of a certain relation is in
a fixed order. For example in Chinese, the object
almost always occurs behind its dominating verb;
the attribute modifier always occurs in front of
its head word. Therefore, we can draw a conclu-
sion that the relative positions between head and
dependent of VOB and ATT can be determined
by the types of dependency relations.
We make a statistic on the relative positions
between head and dependent for each dependen-
cy relation type. Following (Covington, 2001),
we call a dependent that precedes its head prede-
pendent, a dependent that follows its head post-
dependent. The corpus used to gather appropriate
statistics is HIT-CDT. Table 1 gives the numbers
</bodyText>
<page confidence="0.987088">
811
</page>
<bodyText confidence="0.999831344827586">
of predependent/postdependent for each type of
dependency relations and its descriptions.
Table 1 shows that 100% dependents of ATT
relation are predependents and 23,487(99.9%)
against 21(0.1%) VOB dependents are postde-
pendents. Almost all the dependency relations
have a dominant dependent type—predependent
or postdependent. Although some dependency
relations have exceptional cases (e.g. VOB), the
number is so small that it can be ignored. The
only exception is the IS relation, which has
794(86.4%) predependents and 125(13.6%)
postdependents. The IS label is an abbreviation
for independent structure. This type of depen-
dency relation is usually used to represent inter-
jections or comments set off by brackets, which
usually has little grammatical connection with
the head. Figure 3 gives an example of indepen-
dent structure. This example is from a news re-
port, and the phrase “N*fMA” (set apart by
brackets in the original text) is a supplementary
explanation for the source of the news. The con-
nection between this phrase and the main clause
is so weak that either it precedes or follows the
head verb is acceptable in grammar. However,
this kind of news-source-explanation is customa-
ry to place at the beginning of a sentence in Chi-
nese. This can probably explain the majority of
the IS-tagged dependents are predependents.
</bodyText>
<figure confidence="0.5929665">
OO TEA(HED)
serious
ONMfMA(IS) ®_[49VN- x(SBV)
Xinhua news southern snowstorm
</figure>
<figureCaption confidence="0.999893">
Figure 3: Example of independent structure
</figureCaption>
<bodyText confidence="0.999983">
If we simply treat all the IS dependents as pre-
dependents, we can assume that every dependen-
cy relation has only one type of dependent, either
predependent or postdependent. Therefore, the
relative position between head and dependent
can be determined just by the types of dependen-
cy relations.
In the light of this assumption, all dependents
in a sub-tree can be classified into two groups—
predependents and postdependents. The prede-
pendents must precede the head, and the postde-
pendents must follow the head. This classifica-
tion not only reduces the number of possible se-
quences, but also solves the linearization of a
sub-tree if the sub-tree contains only one depen-
dent, or two dependents of different types, viz.
one predependent and one postdependent. In sub-
tree c of Figure 2, the dependency relation be-
tween the only dependent and the head is ATT,
which indicates that the dependent is a prede-
pendent. Therefore, node 7 is bound to precede
node 5, and the only linearization result is “Aa
}JL&apos;!”. In sub-tree a of the same figure, the clas-
sification for SBV is predependent, and for VOB
is postdependent, so the only linearization is
&lt;node 2, node 1, node 3&gt;.
In HIT-CDT, there are 108,086 sub-trees in
the 10,000 sentences, 65% sub-trees have only
one dependent, and 7% sub-trees have two de-
pendents of different types (one predependent
and one postdependent). This means that the
relative position classification can deterministi-
cally linearize 72% sub-trees, and only the rest
28% sub-trees with more than one predependent
or postdependent need to be further determined.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="method">
4 Log-linear Models
</sectionHeader>
<bodyText confidence="0.999809333333333">
We use log-linear models for selecting the se-
quence with the highest probability from all the
possible linearizations of a sub-tree.
</bodyText>
<subsectionHeader confidence="0.991888">
4.1 The Log-linear Model
</subsectionHeader>
<bodyText confidence="0.995452181818182">
Log-linear models employ a set of feature func-
tions to describe properties of the data, and a set
of learned weights to determine the contribution
of each feature. In this framework, we have a set
of M feature functions hm (r, t), m =1, ... , M .
For each feature function, there exists a model
parameter λm (r, t), m =1, ... , M that is fitted to
optimize the likelihood of the training data. A
conditional log-linear model for the probability
of a realization r given the dependency tree t, has
the general parametric form
</bodyText>
<equation confidence="0.996715">
1
pλ (r  |t)= exp[∑λmhm
Zλ (t)1
m
</equation>
<bodyText confidence="0.680031">
where Zλ (t) is a normalization factor defined as
</bodyText>
<equation confidence="0.999427333333333">
M
Zλ (t) = ∑ ∑
exp[ λ m hm (r&apos; , t)] (2)
r Y t
&apos; ( )
∈ m=1
</equation>
<bodyText confidence="0.9983505">
And Y(t) gives the set of all possible realizations
of the dependency tree t.
</bodyText>
<subsectionHeader confidence="0.976511">
4.2 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999953666666667">
We use three types of feature functions for cap-
turing relations among nodes on the dependency
tree. In order to better illustrate the feature func-
tions used in the log-linear model, we redraw
sub-tree b of Figure 2 in Figure 4. Here we as-
sume the linearizations of sub-tree c and d have
</bodyText>
<equation confidence="0.989066666666667">
( , )]
r t (1)
M
</equation>
<page confidence="0.977102">
812
</page>
<table confidence="0.99954925">
Feature function Examples of features
Dependency Relation “SBV ADV VOB” “ADV VOB VOB”
Word Model “武汉航空首次” “航空首次购买” “首次购买波音”“购买波音客机”
Headword Model “航空首次” “首次购买” “购买客机”
</table>
<tableCaption confidence="0.99925">
Table 2: Examples of feature functions
</tableCaption>
<bodyText confidence="0.9643275">
been finished, and the strings of linearizing re-
sults are recorded in nodes 5 and 6.
</bodyText>
<figure confidence="0.941977">
③购买(VOB)
buy
④首次(ADV) ⑤客机(VOB) ⑥航空(SBV)
first time airliner Airline
“波音客机” “武汉航空”
airliners of Boeing Airline Wuhan
</figure>
<figureCaption confidence="0.999871">
Figure 4: Sub-tree with multiple predependents
</figureCaption>
<bodyText confidence="0.999882875">
The sub-tree in Figure 4 has two predepen-
dents (SBV and ADV) and one postdependent
(VOB). As a result of this classification, the only
two possible linearizations of the sub-tree are
&lt;node 4, node 6, node 3, node 5&gt; and &lt;node 6,
node 4, node 3, node 5&gt;. Then the log-linear
model that incorporates three types of feature
functions is used to make further selection.
Dependency Relation Model: For a particular
sub-tree structure, the task of generating a string
covered by the nodes on the sub-tree is equiva-
lent to linearizing all the dependency relations in
that sub-tree. We linearize the dependency rela-
tions by computing n-gram models, similar to
traditional word-based language models, except
using the names of dependency relations instead
of words. For the two linearizations of Figure 4,
the corresponding dependency relation sequences
are “ADV SBV VOB VOB” and “SBV ADV
VOB VOB”. The dependency relation model
calculates the probability of dependency relation
n-gram P(DR) according to Eq.(3). The probabil-
ity score is integrated into the log-linear model as
a feature.
</bodyText>
<equation confidence="0.998610666666667">
P DR m = P DR DR
( 1 ) ( 1 ... ) (3)
m
m
∏ P(DRk  |DRk− n+1 )
k=1
</equation>
<bodyText confidence="0.999409727272727">
Word Model: We integrate an n-gram word
model into the log-linear model for capturing the
relation between adjacent words. For a string of
words generated from a possible sequence of
sub-tree nodes, the word models calculate word-
based n-gram probabilities of the string. For ex-
ample, in Figure 4, the strings generated by the
two possible sequences are “武汉航空 首次
买 波音客机” and “首次 武汉航空 购买 波音客
机”. The word model takes these two strings as
input, and calculates the n-gram probabilities.
Headword Model: 2 In dependency representa-
tions, heads usually play more important roles
than dependents. The headword model calculates
the n-gram probabilities of headwords, without
regard to the words occurring at dependent nodes,
in that dependent words are usually less impor-
tant than headwords. In Figure 4, the two possi-
ble sequences of headwords are “航空 首次
买 客机” and “首次 航空 购买 客机”. The
headword strings are usually more generic than
the strings including all words, and thus the
headword model is more likely to relax the data
sparseness.
Table 2 gives some examples of all the features
used in the log-linear model. The examples listed
in the table are features of the linearization
&lt;node 6, node 4, node 3, node 5&gt;, extracted from
the sub-tree in Figure 4.
In this paper, all the feature functions used in
the log-linear model are n-gram probabilities.
However, the log-linear framework has great
potential for including other types of features.
</bodyText>
<subsectionHeader confidence="0.995035">
4.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.997626">
BLEU score, a method originally proposed to
automatically evaluate machine translation quali-
ty (Papineni et al., 2002), has been widely used
as a metric to evaluate general-purpose sentence
generation (Langkilde, 2002; White et al., 2007;
Guo et al. 2008, Wan et al. 2009). The BLEU
measure computes the geometric mean of the
precision of n-grams of various lengths between
a sentence realization and a (set of) reference(s).
To estimate the parameters (A1 ,..., AM) for the
feature functions (h1,...,hM) , we use BLEU3 as
optimization objective function and adopt the
approach of minimum error rate training
</bodyText>
<footnote confidence="0.9489428">
2 Here the term “headword” is used to describe the word
that occurs at head nodes in dependency trees.
3 The BLEU scoring script is supplied by NIST Open Ma-
chine Translation Evaluation at
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
</footnote>
<page confidence="0.995681">
813
</page>
<table confidence="0.9576306">
Model BLEU ExMatch SSA
1 Random 0.1478 0.0038 0.2044
2 RPD + Random 0.5943 0.1274 0.6369
3 RPD + DR 0.7204 0.2167 0.7683
4 RPD + Bi-WM 0.8289 0.4125 0.8270
5 RPD + Tri-WM 0.8508 0.4715 0.8415
6 RPD + HW 0.7592 0.2909 0.7638
7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723
8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817
9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920
</table>
<tableCaption confidence="0.999485">
Table 3: BLEU, ExMatch and SSA scores on the test set
</tableCaption>
<bodyText confidence="0.9671105">
(MERT), which is popular in statistical machine
translation (Och, 2003).
</bodyText>
<subsectionHeader confidence="0.999497">
4.4 The Realization Algorithm
</subsectionHeader>
<bodyText confidence="0.9998434">
The realization algorithm is a recursive proce-
dure that starts from the root node of the depen-
dency tree, and traverses the tree by depth-first
search. The pseudo code of the realization algo-
rithm is shown in Figure 5.
</bodyText>
<footnote confidence="0.3947735">
1:procedure SEARCH
2:input: sub-tree T {head:H dep.:D1...Dn}
</footnote>
<listItem confidence="0.972357538461538">
3: if n = 0 then return
4: for i := 1 to n
5: SEARCH(Di)
6: Apre := {}
7: Apost := {}
8: for i := 1 to n
9: if PRE-DEP(Di) then Apre:=Apre∪{Di}
10: if POST-DEP(Di) then Apost:=Apost∪{Di}
11: for all permutations p1 of Apre
12: for all permutations p2 of Apost
13: sequence s := JOIN(p1,H,p2)
14: score r := LOG-LINEAR(s)
15: if best-score(r) then RECORD(r,s)
</listItem>
<figureCaption confidence="0.77978">
Figure 5: The algorithm for linearizations of sub-
trees
</figureCaption>
<sectionHeader confidence="0.99762" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997601">
5.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999861230769231">
Our experiments are carried out on HIT-CDT.
We randomly select 526 sentences as the test set,
and 499 sentences as the development set for
optimizing the model parameters. The rest 8,975
sentences of the HIT-CDT are used for training
of the dependency relation model. For training of
word models, we use the Xinhua News part
(6,879,644 words) of Chinese Gigaword Second
Edition (LDC2005T14), segmented by the Lan-
guage Technology Platform (LTP) 4 . And for
training the headword model, we use both the
HIT-CDT and the HIT Chinese Skeletal Depen-
dency Treebank (HIT-CSDT). HIT-CSDT is a
</bodyText>
<footnote confidence="0.847739">
4 http://ir.hit.edu.cn/demo/ltp
</footnote>
<bodyText confidence="0.985888857142857">
component of LTP and contains 49,991 sen-
tences in dependency structure representation
(without dependency relation labels).
As the input dependency representation does
not contain punctuation information, we simply
remove all punctuation marks in the test and de-
velopment sets.
</bodyText>
<subsectionHeader confidence="0.996349">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.99998925">
In addition to BLEU score, percentage of exactly
matched sentences and average NIST simple
string accuracy (SSA) are adopted as evaluation
metrics. The exact match measure is percentage
of the generated string that exactly matches the
corresponding reference sentence. The average
NIST simple string accuracy score reflects the
average number of insertion (I), deletion (D), and
substitution (S) errors between the output sen-
tence and the reference sentence. Formally, SSA
= 1 – (I + D + S) / R, where R is the number of
tokens in the reference sentence.
</bodyText>
<subsectionHeader confidence="0.997541">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999783">
All the evaluation results are shown in Table 3.
The first experiment, which is a baseline experi-
ment, ignores the tree structure and randomly
chooses position for every word. From the
second experiment, we begin to utilize the tree
structure and apply the realization algorithm de-
scribed in Section 4.4. In the second experiment,
predependents are distinguished from postdepen-
dents by the relative position determination me-
thod (RPD), then the orders inside predependents
and postdependents are chosen randomly. From
the third experiments, the log-linear models are
used for scoring the generated sequences, with
the aid of three types of feature functions as de-
scribed in Section 4.2. First, the feature functions
of trigram dependency relation model (DR), bi-
gram word model (Bi-WM), trigram word model
(Tri-WM) (with Katz backoff) and trigram
headword model (HW) are used separately in
experiments 3-6. Then we combine the feature
</bodyText>
<page confidence="0.995676">
814
</page>
<bodyText confidence="0.99828216">
functions incrementally based on the RPD and
DR model.
The relative position determination plays an
important role in the realization algorithm. We
observe that the BLEU score is boosted from
0.1478 to 0.5943 by using the RPD method. This
can be explained by the reason that the lineariza-
tions of 72% sub-trees can be definitely deter-
mined by the RPD method. All of the four fea-
ture functions we have tested achieve considera-
ble improvement in BLEU scores. The depen-
dency relation model achieves 0.7204, the bi-
gram word model 0.8289, the trigram word mod-
el 0.8508 and the headword model achieves
0.7592. While the combined models perform bet-
ter than any of their individual component mod-
els. On the foundation of relative position deter-
mination method, the combination of dependen-
cy relation and bigram word model achieves a
BLEU score of 0.8615, and the combination of
dependency relation and trigram word model
achieves a BLEU score of 0.8772. Finally the
combination of dependency relation model, tri-
gram word model and headword model achieves
the best result 0.8874.
</bodyText>
<subsectionHeader confidence="0.710941">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999972428571429">
We first inspected the errors made by the relative
position determination method. In the treebank-
tree test set, there are 7 predependents classified
as postdependents and 3 postdependents classi-
fied as predependents by error. Among the 9,384
dependents, the error rate of the relative position
determination method is very small (0.1%).
Then we make a classification on the errors in
the experiment of dependency relation model
(with relative position determination method).
Table 4 shows the distribution of the errors.
The first type of errors is caused by duplicate
dependency relations, i.e. a head with two or
more dependents that have the same dependency
relations. In this situation, only using the depen-
dency relation model cannot generate the right
linearization. However, word models, which util-
ize the word information, can make distinctions
between the dependencies. The reason for the
errors of SBV-ADV and ATT-QUN is probably
because the order of these pairs of grammar roles
</bodyText>
<table confidence="0.7090446">
Error types Proportion
1 Duplicate dependency relations 60.0%
2 SBV-ADV 20.3%
3 ATT-QUN 6.3%
4 Other 13.4%
</table>
<tableCaption confidence="0.998452">
Table 4: Error types in the RPD+DR experiment
</tableCaption>
<figure confidence="0.988430285714286">
①工作
work
②保护(ATT)
protect
“鸟/` Moll
��[� y�o„
“birds protecting”
</figure>
<figureCaption confidence="0.999917">
Figure 6: Sub-tree for “未来的鸟类保护工作”
</figureCaption>
<bodyText confidence="0.999898157894737">
is somewhat flexible. For example, the strings of
“今天(ADV)/today 我(SBV)/I” and “我(SBV)/I
今天(ADV)/today” are both very common and
acceptable in Chinese.
The word models tend to combine the nodes
that have strong correlation together. For exam-
ple in Figure 6, node 2 is more likely to precede
node 3 because the words “保护/protect” and
“未来/future” have strong correlation, but the
correct order is &lt;node 3, node 2&gt;.
Headword model only consider the words oc-
cur at head nodes, which is helpful in the situa-
tion like Figure 6. In our experiments, the head-
word model gets a relatively low performance by
itself, however, the addition of headword model
to the combination of the other two feature func-
tions improves the result from 0.8772 to 0.8874.
This indicates that the headword model is com-
plementary to the other feature functions.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999996333333333">
We have presented a general-purpose realizer
based on log-linear models, which directly maps
dependency relations into surface strings. The
linearization of a whole dependency tree is di-
vided into a series of sub-procedures on sub-trees.
The dependents in the sub-trees are classified
into two groups, predependents or postdepen-
dents, according to their dependency relations.
The evaluation shows that this relative position
determination method achieves a considerable
result. The log-linear model, which incorporates
three types of feature functions, including de-
pendency relation, surface words and headwords,
successfully captures factors in sentence realiza-
tion and demonstrates competitive performance.
</bodyText>
<sectionHeader confidence="0.997749" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9624808">
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for
Generation. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics,
pages 42-48. Saarbrücken, Germany.
</reference>
<bodyText confidence="0.7039015">
③的(SBV)
of
“未来 的”
future
</bodyText>
<page confidence="0.994964">
815
</page>
<reference confidence="0.996295726415095">
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033-
1040. Sydney, Australia.
Aoife Cahill, Martin Forst and Christian Rohrer. 2007.
Stochastic Realisation Ranking for a Free Word
Order language. In Proceedings of 11th European
Workshop on Natural Language Generation, pages
17-24. Schloss Dagstuhl, Germany.
John Carroll, Ann Copestake, Dan Flickinger, and
Victor Poznanski. 1999. An Efficient Chart Gene-
rator for (Semi-)Lexicalist Grammars. In Proceed-
ings of the 7th European Workshop on Natural
Language Generation, pages 86-95, Toulouse.
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of
the 39th Annual ACM Southeast Conference, pages
95–102.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy
King, John Maxwell, and Paula Newman. 2007.
XLE documentation. Palo Alto Research Center,
CA.
Katja Filippova and Michael Strube. 2007. Generating
Constituent Order in German Clauses. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 320-327. Pra-
gue, Czech Republic.
Yuqing Guo, Haifeng Wang and Josef van Genabith.
2008. Dependency-Based N-Gram Models for
General Purpose Sentence Realisation. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 297-304. Manchester,
UK.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill and
Josef van Genabith. 2007. Exploiting Multi-Word
Units in History-Based Probabilistic Generation. In
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and CoNLL, pages 267-276. Prague, Czech Repub-
lic.
Mel&apos;čuk Igor. 1988. Dependency syntax: Theory and
practice. In Suny Series in Linguistics. State Uni-
versity of New York Press, New York, USA.
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. In Proceedings of 1st Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 170-177. Seattle,
WA.
Irene Langkilde. 2002. An Empirical Verification of
Coverage and Correctness for a General-Purpose
Sentence Generator. In Proceedings of the Second
International Conference on Natural Language
Generation, pages 17-24. New York, USA.
Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building
a Dependency Treebank for Improving Chinese
Parser. Journal of Chinese Language and Compu-
ting, 16(4): 207-224.
Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li.
2006b. Dependency Parsing Based on Dynamic
Local Optimization. In Proceedings of CoNLL-X,
pages 211-215, New York, USA.
Hiroko Nakanishi, Yusuke Miyao and Jun’ichi Tsujii.
2005. Probabilistic Models for Disambiguation of
an HPSG-Based Chart Generator. In Proceedings
of the 9th International Workshop on Parsing
Technology, pages 93-102. Vancouver, British Co-
lumbia.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160-167, Sappo-
ro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-
318. Philadelphia, PA.
Adwait Ratnaparkhi. 2000. Trainable Methods for
Natural Language Generation. In Proceedings of
North American Chapter of the Association for
Computational Linguistics, pages 194-201. Seattle,
WA.
Erik Velldal and Stephan Oepen. 2005. Maximum
Entropy Models for Realization Ranking. In Pro-
ceedings of the 10th Machine Translation Summit,
pages 109-116. Phuket, Thailand,
Stephen Wan, Mark Dras, Robert Dale, Cécile Paris.
2009. Improving Grammaticality in Statistical Sen-
tence Generation: Introducing a Dependency Span-
ning Tree Algorithm with an Argument Satisfac-
tion Model. In Proceedings of the 12th Conference
of the European Chapter of the ACL, pages 852-
860. Athens, Greece.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proceedings of the third International Nat-
ural Language Generation Conference, pages 182-
191. Hampshire, UK.
Michael White, Rajakrishnan Rajkumar and Scott
Martin. 2007. Towards Broad Coverage Surface
Realization with CCG. In Proceedings of the Ma-
chine Translation Summit XI Workshop, pages 22-
30. Copenhagen, Danmark.
</reference>
<page confidence="0.998828">
816
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.303655">
<title confidence="0.9489455">Dependency Based Chinese Sentence Realization Haifeng Yuqing Ting</title>
<address confidence="0.644129">Retrieval Lab, Harbin Institute of Technology, Harbin, China</address>
<email confidence="0.956707">whe@ir.hit.edu.cn</email>
<email confidence="0.956707">tliu@ir.hit.edu.cn</email>
<affiliation confidence="0.570589">(China) Research and Development Center, Beijing, China</affiliation>
<email confidence="0.956611">wanghaifeng@rdc.toshiba.com.cn</email>
<email confidence="0.956611">guoyuqing@rdc.toshiba.com.cn</email>
<abstract confidence="0.997426473684211">This paper describes log-linear models for a general-purpose sentence realizer based on dependency structures. Unlike traditional realizers using grammar rules, our method realizes sentences by linearizing dependency relations directly in two steps. First, the relative order between head and each dependent is determined by their dependency relation. Then the best linearizations compatible with the relative order are selected by log-linear models. The log-linear models incorporate three types of feature functions, including dependency relations, surface words and headwords. Our approach to sentence realization provides simplicity, efficiency and competitive accuracy. Trained on 8,975 dependency structures of a Chinese Dependency Treebank, the realizer achieves a BLEU score of 0.8874.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a Probabilistic Hierarchical Model for Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>42--48</pages>
<location>Saarbrücken, Germany.</location>
<contexts>
<context position="2573" citStr="Bangalore and Rambow, 2000" startWordPosition="367" endWordPosition="370">2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree al</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a Probabilistic Hierarchical Model for Generation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 42-48. Saarbrücken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1033--1040</pages>
<location>Sydney, Australia.</location>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust PCFG-Based Generation Using Automatically Acquired LFG Approximations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1033-1040. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic Realisation Ranking for a Free Word Order language.</title>
<date>2007</date>
<booktitle>In Proceedings of 11th European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<location>Schloss Dagstuhl, Germany.</location>
<contexts>
<context position="2820" citStr="Cahill et al., 2007" startWordPosition="406" endWordPosition="409">zation and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it i</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst and Christian Rohrer. 2007. Stochastic Realisation Ranking for a Free Word Order language. In Proceedings of 11th European Workshop on Natural Language Generation, pages 17-24. Schloss Dagstuhl, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An Efficient Chart Generator for (Semi-)Lexicalist Grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation,</booktitle>
<pages>86--95</pages>
<location>Toulouse.</location>
<contexts>
<context position="1751" citStr="Carroll et al., 1999" startWordPosition="237" endWordPosition="240">lization can be described as the process of converting the semantic and syntactic representation of a sentence or series of sentences into meaningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the </context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Victor Poznanski. 1999. An Efficient Chart Generator for (Semi-)Lexicalist Grammars. In Proceedings of the 7th European Workshop on Natural Language Generation, pages 86-95, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A Fundamental Algorithm for Dependency Parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="10148" citStr="Covington, 2001" startWordPosition="1576" endWordPosition="1577">nt is an attribute of the head. In languages with fairly rigid word order, the relative position between the head and dependent of a certain relation is in a fixed order. For example in Chinese, the object almost always occurs behind its dominating verb; the attribute modifier always occurs in front of its head word. Therefore, we can draw a conclusion that the relative positions between head and dependent of VOB and ATT can be determined by the types of dependency relations. We make a statistic on the relative positions between head and dependent for each dependency relation type. Following (Covington, 2001), we call a dependent that precedes its head predependent, a dependent that follows its head postdependent. The corpus used to gather appropriate statistics is HIT-CDT. Table 1 gives the numbers 811 of predependent/postdependent for each type of dependency relations and its descriptions. Table 1 shows that 100% dependents of ATT relation are predependents and 23,487(99.9%) against 21(0.1%) VOB dependents are postdependents. Almost all the dependency relations have a dominant dependent type—predependent or postdependent. Although some dependency relations have exceptional cases (e.g. VOB), the </context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A Fundamental Algorithm for Dependency Parsing. In Proceedings of the 39th Annual ACM Southeast Conference, pages 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE documentation. Palo Alto Research</title>
<date>2007</date>
<location>Center, CA.</location>
<contexts>
<context position="1804" citStr="Crouch et al., 2007" startWordPosition="246" endWordPosition="249"> the semantic and syntactic representation of a sentence or series of sentences into meaningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two </context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2007</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King, John Maxwell, and Paula Newman. 2007. XLE documentation. Palo Alto Research Center, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Generating Constituent Order in German Clauses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3646" citStr="Filippova and Strube, 2007" startWordPosition="537" endWordPosition="540">ose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into one-depth sub-trees and solving</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Generating Constituent Order in German Clauses. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320-327. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Haifeng Wang</author>
<author>Josef van Genabith</author>
</authors>
<title>Dependency-Based N-Gram Models for General Purpose Sentence Realisation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22th International Conference on Computational Linguistics,</booktitle>
<pages>297--304</pages>
<location>Manchester, UK.</location>
<marker>Guo, Wang, van Genabith, 2008</marker>
<rawString>Yuqing Guo, Haifeng Wang and Josef van Genabith. 2008. Dependency-Based N-Gram Models for General Purpose Sentence Realisation. In Proceedings of the 22th International Conference on Computational Linguistics, pages 297-304. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deirdre Hogan</author>
<author>Conor Cafferkey</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Exploiting Multi-Word Units in History-Based Probabilistic Generation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and CoNLL,</booktitle>
<pages>267--276</pages>
<location>Prague, Czech Republic.</location>
<marker>Hogan, Cafferkey, Cahill, van Genabith, 2007</marker>
<rawString>Deirdre Hogan, Conor Cafferkey, Aoife Cahill and Josef van Genabith. 2007. Exploiting Multi-Word Units in History-Based Probabilistic Generation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and CoNLL, pages 267-276. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mel&apos;čuk Igor</author>
</authors>
<title>Dependency syntax: Theory and practice.</title>
<date>1988</date>
<booktitle>In Suny Series in Linguistics. State</booktitle>
<publisher>University of New York Press,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="7877" citStr="Igor, 1988" startWordPosition="1198" endWordPosition="1199">sively linearizes the sub-trees in a bottom-up fashion. As illustrated in Figure 2, sub-trees c and d, which are at the bottom of the tree, are linearized first, then sub-tree b is processed, and finally sub-tree a. The procedure imposes a projective constraint on the dependency structures, viz. each head dominates a continuous substring of the sentence realization. This assumption is feasible in the application of the dependency-based generation, because: (i) it has long been observed that the dependency structures of a vast majority of sentences in the languages of the world are projective (Igor, 1988) and (ii) non-projective dependencies in Chinese, for the most part, are used to account for non-local dependency phenomena. 航空(SBV) airline 武汉(ATT) Wuhan 810 linearization procedure Figure 2: Illustration of the sub-tree c 客机(VOB) airliner 波音 客机 波音(ATT) Boeing sub-tree d 航空(SBV) Airline 武汉(ATT) Wuhan 武汉 航空 sub-tree a 首次(ADV) first time 武汉航空 首次 购买 波音客机 购买(VOB) buy   sub-tree b 是(HED) is 这(SBV) this 这 是 武汉航空首次购买波音客机  Relation Description Postdep. Predep. ADV adverbial 1 25977 APP appositive 807 0 ATT attribute 0 47040 CMP complement 2931 3 CNJ conjunctive 0 2124 COO coordinate 6818</context>
</contexts>
<marker>Igor, 1988</marker>
<rawString>Mel&apos;čuk Igor. 1988. Dependency syntax: Theory and practice. In Suny Series in Linguistics. State University of New York Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-Based Statistical Sentence Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>170--177</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2591" citStr="Langkilde, 2000" startWordPosition="371" endWordPosition="372"> 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word o</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-Based Statistical Sentence Generation. In Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 170-177. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<location>New York, USA.</location>
<contexts>
<context position="17918" citStr="Langkilde, 2002" startWordPosition="2875" endWordPosition="2876">he features used in the log-linear model. The examples listed in the table are features of the linearization &lt;node 6, node 4, node 3, node 5&gt;, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (A1 ,..., AM) for the feature functions (h1,...,hM) , we use BLEU3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The BLEU scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.ncsl.ni</context>
</contexts>
<marker>Langkilde, 2002</marker>
<rawString>Irene Langkilde. 2002. An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. In Proceedings of the Second International Conference on Natural Language Generation, pages 17-24. New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jinshan Ma</author>
<author>Sheng Li</author>
</authors>
<title>Building a Dependency Treebank for Improving Chinese Parser.</title>
<date>2006</date>
<journal>Journal of Chinese Language and Computing,</journal>
<volume>16</volume>
<issue>4</issue>
<pages>207--224</pages>
<contexts>
<context position="6274" citStr="Liu et al., 2006" startWordPosition="945" endWordPosition="948">e of dependency tree representation for the sentence: (1) 这 是 武汉 航空 this is Wuhan Airline 首次 购买 波音 客机 first time buy Boeing airliner ‘This is the first time for Airline Wuhan to buy Boeing airliners.’ In a dependency structure, dependents are unordered, i.e. the string position of each node is not recorded in the representation. Our sentence realizer takes such an unordered dependency tree as input, determines the linear order of the words 1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences and 215,334 words, which are manually annotated with part-of-speech tags and dependency labels. (Liu et al., 2006a) 是(HED) is 这(SBV) 购买(VOB) this buy 首次(ADV) 客机(VOB) first time airliner 波音(ATT) Boeing Figure 1: The dependency tree for the sentence “这是武汉航空首次购买波音客机” as encoded in the nodes of the dependency structure and produces a grammatical sentence. As the dependency structures input to our realizer have been lexicalized, lexical selection is not involved during the surface realization. 2.2 Divide and Conquer Strategy for Linearization For determining the linear order of words represented by nodes of the given dependency structure, in principle, the sentence realizer has to produce all possible s</context>
</contexts>
<marker>Liu, Ma, Li, 2006</marker>
<rawString>Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building a Dependency Treebank for Improving Chinese Parser. Journal of Chinese Language and Computing, 16(4): 207-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Jinshan Ma</author>
<author>Huijia Zhu</author>
<author>Sheng Li</author>
</authors>
<title>Dependency Parsing Based on Dynamic Local Optimization. In</title>
<date>2006</date>
<booktitle>Proceedings of CoNLL-X,</booktitle>
<pages>211--215</pages>
<location>New York, USA.</location>
<contexts>
<context position="6274" citStr="Liu et al., 2006" startWordPosition="945" endWordPosition="948">e of dependency tree representation for the sentence: (1) 这 是 武汉 航空 this is Wuhan Airline 首次 购买 波音 客机 first time buy Boeing airliner ‘This is the first time for Airline Wuhan to buy Boeing airliners.’ In a dependency structure, dependents are unordered, i.e. the string position of each node is not recorded in the representation. Our sentence realizer takes such an unordered dependency tree as input, determines the linear order of the words 1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences and 215,334 words, which are manually annotated with part-of-speech tags and dependency labels. (Liu et al., 2006a) 是(HED) is 这(SBV) 购买(VOB) this buy 首次(ADV) 客机(VOB) first time airliner 波音(ATT) Boeing Figure 1: The dependency tree for the sentence “这是武汉航空首次购买波音客机” as encoded in the nodes of the dependency structure and produces a grammatical sentence. As the dependency structures input to our realizer have been lexicalized, lexical selection is not involved during the surface realization. 2.2 Divide and Conquer Strategy for Linearization For determining the linear order of words represented by nodes of the given dependency structure, in principle, the sentence realizer has to produce all possible s</context>
</contexts>
<marker>Liu, Ma, Zhu, Li, 2006</marker>
<rawString>Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li. 2006b. Dependency Parsing Based on Dynamic Local Optimization. In Proceedings of CoNLL-X, pages 211-215, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic Models for Disambiguation of an HPSG-Based Chart Generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<pages>93--102</pages>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="1895" citStr="Nakanishi et al., 2005" startWordPosition="259" endWordPosition="262">ningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over differe</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao and Jun’ichi Tsujii. 2005. Probabilistic Models for Disambiguation of an HPSG-Based Chart Generator. In Proceedings of the 9th International Workshop on Parsing Technology, pages 93-102. Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19033" citStr="Och, 2003" startWordPosition="3070" endWordPosition="3071"> scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 813 Model BLEU ExMatch SSA 1 Random 0.1478 0.0038 0.2044 2 RPD + Random 0.5943 0.1274 0.6369 3 RPD + DR 0.7204 0.2167 0.7683 4 RPD + Bi-WM 0.8289 0.4125 0.8270 5 RPD + Tri-WM 0.8508 0.4715 0.8415 6 RPD + HW 0.7592 0.2909 0.7638 7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723 8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817 9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920 Table 3: BLEU, ExMatch and SSA scores on the test set (MERT), which is popular in statistical machine translation (Och, 2003). 4.4 The Realization Algorithm The realization algorithm is a recursive procedure that starts from the root node of the dependency tree, and traverses the tree by depth-first search. The pseudo code of the realization algorithm is shown in Figure 5. 1:procedure SEARCH 2:input: sub-tree T {head:H dep.:D1...Dn} 3: if n = 0 then return 4: for i := 1 to n 5: SEARCH(Di) 6: Apre := {} 7: Apost := {} 8: for i := 1 to n 9: if PRE-DEP(Di) then Apre:=Apre∪{Di} 10: if POST-DEP(Di) then Apost:=Apost∪{Di} 11: for all permutations p1 of Apre 12: for all permutations p2 of Apost 13: sequence s := JOIN(p1,H,</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160-167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="17819" citStr="Papineni et al., 2002" startWordPosition="2859" endWordPosition="2862"> thus the headword model is more likely to relax the data sparseness. Table 2 gives some examples of all the features used in the log-linear model. The examples listed in the table are features of the linearization &lt;node 6, node 4, node 3, node 5&gt;, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (A1 ,..., AM) for the feature functions (h1,...,hM) , we use BLEU3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable Methods for Natural Language Generation.</title>
<date>2000</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>194--201</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="3617" citStr="Ratnaparkhi, 2000" startWordPosition="535" endWordPosition="536">elop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air travel reservation or ordering constituents of a main clause in German. This paper presents a general-purpose realizer based on log-linear models for directly linearizing dependency relations given dependency structures. We reduce the generation space by 809 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809–816, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP two techniques: the first is dividing the entire dependency tree into on</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable Methods for Natural Language Generation. In Proceedings of North American Chapter of the Association for Computational Linguistics, pages 194-201. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum Entropy Models for Realization Ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>109--116</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="2774" citStr="Velldal and Oepen, 2005" startWordPosition="398" endWordPosition="401">gm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, </context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum Entropy Models for Realization Ranking. In Proceedings of the 10th Machine Translation Summit, pages 109-116. Phuket, Thailand,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
</authors>
<title>Cécile Paris.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>852--860</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="3135" citStr="Wan et al. (2009)" startWordPosition="459" endWordPosition="462">level bigram/trigram models (Bangalore and Rambow, 2000; Langkilde, 2000), or factored language models integrated with syntactic tags (White et al. 2007). The other is log-linear model with different syntactic and semantic features (Velldal and Oepen, 2005; Nakanishi et al., 2005; Cahill et al., 2007). However, little work has been done on probabilistic models learning direct mapping from input to surface strings, without the effort to construct a grammar. Guo et al. (2008) develop a general-purpose realizer couched in the framework of Lexical Functional Grammar based on simple n-gram models. Wan et al. (2009) present a dependency-spanning tree algorithm for word ordering, which first builds dependency trees to decide linear precedence between heads and modifiers then uses an n-gram language model to order siblings. Compared with n-gram model, log-linear model is more powerful in that it is easy to integrate a variety of features, and to tune feature weights to maximize the probability. A few papers have presented maximum entropy models for word or phrase ordering (Ratnaparkhi, 2000; Filippova and Strube, 2007). However, those attempts have been limited to specialized applications, such as air trav</context>
<context position="17973" citStr="Wan et al. 2009" startWordPosition="2885" endWordPosition="2888"> listed in the table are features of the linearization &lt;node 6, node 4, node 3, node 5&gt;, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (A1 ,..., AM) for the feature functions (h1,...,hM) , we use BLEU3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The BLEU scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 813 Model BLEU ExMat</context>
</contexts>
<marker>Wan, Dras, Dale, 2009</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, Cécile Paris. 2009. Improving Grammaticality in Statistical Sentence Generation: Introducing a Dependency Spanning Tree Algorithm with an Argument Satisfaction Model. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 852-860. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG Chart Realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the third International Natural Language Generation Conference,</booktitle>
<pages>182--191</pages>
<publisher>Hampshire, UK.</publisher>
<contexts>
<context position="1774" citStr="White, 2004" startWordPosition="242" endWordPosition="243"> process of converting the semantic and syntactic representation of a sentence or series of sentences into meaningful, grammatically correct and fluent text of a particular language. Most previous general-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG Chart Realization. In Proceedings of the third International Natural Language Generation Conference, pages 182-191. Hampshire, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
<author>Scott Martin</author>
</authors>
<title>Towards Broad Coverage Surface Realization with CCG.</title>
<date>2007</date>
<booktitle>In Proceedings of the Machine Translation Summit XI Workshop,</booktitle>
<pages>22--30</pages>
<location>Copenhagen, Danmark.</location>
<contexts>
<context position="1981" citStr="White et al., 2007" startWordPosition="275" endWordPosition="278">eral-purpose realization systems are developed via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), Head Driven Phrase Structure Grammar (HPSG), Combinatory Categorical Grammar (CCG), Tree Adjoining Grammar (TAG) etc. The grammar rules are either developed by hand, such as those used in LinGo (Carroll et al., 1999), OpenCCG (White, 2004) and XLE (Crouch et al., 2007), or extracted automatically from annotated corpora, like the HPSG (Nakanishi et al., 2005), LFG (Cahill and van Genabith, 2006; Hogan et al., 2007) and CCG (White et al., 2007) resources derived from the Penn-II Treebank. Over the last decade, there has been a lot of interest in a generate-and-select paradigm for surface realization. The paradigm is characterized by a separation between realization and selection, in which rule-based methods are used to generate a space of possible paraphrases, and statistical methods are used to select the most likely realization from the space. Usually, two statistical models are used to rank the output candidates. One is n-gram model over different units, such as word-level bigram/trigram models (Bangalore and Rambow, 2000; Langki</context>
<context position="17938" citStr="White et al., 2007" startWordPosition="2877" endWordPosition="2880">in the log-linear model. The examples listed in the table are features of the linearization &lt;node 6, node 4, node 3, node 5&gt;, extracted from the sub-tree in Figure 4. In this paper, all the feature functions used in the log-linear model are n-gram probabilities. However, the log-linear framework has great potential for including other types of features. 4.3 Parameter Estimation BLEU score, a method originally proposed to automatically evaluate machine translation quality (Papineni et al., 2002), has been widely used as a metric to evaluate general-purpose sentence generation (Langkilde, 2002; White et al., 2007; Guo et al. 2008, Wan et al. 2009). The BLEU measure computes the geometric mean of the precision of n-grams of various lengths between a sentence realization and a (set of) reference(s). To estimate the parameters (A1 ,..., AM) for the feature functions (h1,...,hM) , we use BLEU3 as optimization objective function and adopt the approach of minimum error rate training 2 Here the term “headword” is used to describe the word that occurs at head nodes in dependency trees. 3 The BLEU scoring script is supplied by NIST Open Machine Translation Evaluation at ftp://jaguar.ncsl.nist.gov/mt/resources/</context>
</contexts>
<marker>White, Rajkumar, Martin, 2007</marker>
<rawString>Michael White, Rajakrishnan Rajkumar and Scott Martin. 2007. Towards Broad Coverage Surface Realization with CCG. In Proceedings of the Machine Translation Summit XI Workshop, pages 22-30. Copenhagen, Danmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>