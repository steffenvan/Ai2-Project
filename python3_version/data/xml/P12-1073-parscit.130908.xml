<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000225">
<title confidence="0.991185">
Multilingual Named Entity Recognition using Parallel Data and Metadata
from Wikipedia
</title>
<author confidence="0.781423">
Sungchul Kim*
</author>
<note confidence="0.6036">
POSTECH
Pohang, South Korea
</note>
<email confidence="0.862155">
subright@postech.ac.kr
</email>
<author confidence="0.927762">
Kristina Toutanova
</author>
<affiliation confidence="0.90891">
Microsoft Research
</affiliation>
<address confidence="0.948205">
Redmond, WA 98502
</address>
<email confidence="0.976473">
kristout@microsoft.com
</email>
<note confidence="0.817526333333333">
Hwanjo Yu
POSTECH
Pohang, South Korea
</note>
<email confidence="0.89758">
hwanjoyu@postech.ac.kr
</email>
<sectionHeader confidence="0.996391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.934016357142857">
In this paper we propose a method to auto-
matically label multi-lingual data with named
entity tags. We build on prior work utiliz-
ing Wikipedia metadata and show how to ef-
fectively combine the weak annotations stem-
ming from Wikipedia metadata with infor-
mation obtained through English-foreign lan-
guage parallel Wikipedia sentences. The com-
bination is achieved using a novel semi-CRF
model for foreign sentence tagging in the con-
text of a parallel English sentence. The model
outperforms both standard annotation projec-
tion methods and methods based solely on
Wikipedia metadata.
</bodyText>
<sectionHeader confidence="0.999313" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999146">
Named Entity Recognition (NER) is a frequently
needed technology in NLP applications. State-of-
the-art statistical models for NER typically require
a large amount of training data and linguistic exper-
tise to be sufficiently accurate, which makes it nearly
impossible to build high-accuracy models for a large
number of languages.
Recently, there have been two lines of work which
have offered hope for creating NER analyzers in
many languages. The first has been to devise an
algorithm to tag foreign language entities using
metadata from the semi-structured Wikipedia repos-
itory: inter-wiki links, article categories, and cross-
language links (Richman and Schone, 2008). The
second has been to use parallel English-foreign lan-
guage data, a high-quality NER tagger for English,
and projected annotations for the foreign language
(Yarowsky et al., 2001; Das and Petrov, 2011). Par-
allel data has also been used to improve existing
monolingual taggers or other analyzers in two lan-
guages (Burkett et al., 2010a; Burkett et al., 2010b).
</bodyText>
<footnote confidence="0.7526805">
This research was conducted during the author’s internship
at Microsoft Research
</footnote>
<bodyText confidence="0.999956466666667">
The goal of this work is to create high-accuracy
NER annotated data for foreign languages. Here
we combine elements of both Wikipedia metadata-
based approaches and projection-based approaches,
making use of parallel sentences extracted from
Wikipedia. We propose a statistical model which
can combine the two types of information. Simi-
larly to the joint model of Burkett et al. (2010a), our
model can incorporate both monolingual and bilin-
gual features in a log-linear framework. The advan-
tage of our model is that it is much more efficient
as it does not require summing over matchings of
source and target entities. It is a conditional model
for target sentence annotation given an aligned En-
glish source sentence, where the English sentence is
used only as a source of features. Exact inference is
performed using standard semi-markov CRF model
inference techniques (Sarawagi and Cohen, 2004).
Our results show that the semi-CRF model im-
proves on the performance of projection models by
more than 10 points in F-measure, and that we can
achieve tagging F-measure of over 91 using a very
small number of annotated sentence pairs.
The paper is organized as follows: We first
describe the datasets and task setting in Section
2. Next, we present our two baseline methods:
A Wikipedia metadata-based tagger and a cross-
lingual projection tagger in Sections 3 and 4, re-
spectively. We present our direct semi-CRF tagging
model in Section 5.
</bodyText>
<sectionHeader confidence="0.968306" genericHeader="introduction">
2 Data and task
</sectionHeader>
<bodyText confidence="0.999983375">
As a case study, we focus on two very dif-
ferent foreign languages: Korean and Bulgarian.
The English and foreign language sentences that
comprise our training and test data are extracted
from Wikipedia (http://www.wikipedia.org). Cur-
rently there are more than 3.8 million articles in
the English Wikipedia, 125,000 in the Bulgarian
Wikipedia, and 131,000 in the Korean Wikipedia.
</bodyText>
<page confidence="0.979963">
694
</page>
<note confidence="0.992553">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 694–702,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999897">
Figure 1: A parallel sentence-pair showing gold-standard NE labels and word alignments.
</figureCaption>
<bodyText confidence="0.9998842">
To create our dataset, we followed Smith et al.
(2010) to find parallel-foreign sentences using com-
parable documents linked by inter-wiki links. The
approach uses a small amount of manually annotated
article-pairs to train a document-level CRF model
for parallel sentence extraction. A total of 13,410
English-Bulgarian and 8,832 English-Korean sen-
tence pairs were extracted.
Of these, we manually annotated 91 English-
Bulgarian and 79 English-Korean sentence pairs
with source and target named entities as well as
word-alignment links among named entities in the
two languages. Figure 1 illustrates a Bulgarian-
English sentence pair with alignment.
The named entity annotation scheme followed has
the labels GPE (Geopolitical entity), PER (Person),
ORG (Organization), and DATE. It is based on the
MUC-7 annotation guidelines, and GPE is synony-
mous with Location. The annotation process was
not as rigorous as one might hope, due to lack of re-
sources. The English-Bulgarian and English-Korean
datasets were labeled by one annotator each and then
annotations on the English sentences were double-
checked by the other annotator. Disagreements were
rare and were resolved after discussion.
The task we evaluate on is tagging of foreign lan-
guage sentences. We measure performance by la-
beled precision, recall, and F-measure. We give par-
tial credit if entities partially overlap on their span of
words and match on their labels.
Table 1 shows the total number of English,
Bulgarian and Korean entities and the percent-
age of entities that were manually aligned to an
entity of the same type in the other language.
The data sizes are fairly small as the data is
</bodyText>
<table confidence="0.9990698">
Language Entities Aligned %
English 342 93.9%
Bulgarian 344 93.3%
English 414 88.4%
Korean 423 86.5%
</table>
<tableCaption confidence="0.9893235">
Table 1: English-Bulgarian and English-Korean data
characteristics.
</tableCaption>
<bodyText confidence="0.99984875">
used only to train models with very few coarse-
grained features and for evaluation. These datasets
are available at http://research.microsoft.com/en-
us/people/kristout/nerwikidownload.aspx.
As we can see, less than 100% of entities have
parallels in the other language. This is due to two
phenomena: one is that the parallel sentences some-
times contain different amounts of information and
one language might use more detail than the other.
The other is that the same information might be ex-
pressed using a named entity in one language, and
using a non-entity phrase in the other language (e.g.
“He is from Bulgaria” versus “He is Bulgarian”).
Both of these causes of divergence are much more
common in the English-Korean dataset than in the
English-Bulgarian one.
</bodyText>
<sectionHeader confidence="0.862161" genericHeader="method">
3 Wiki-based tagger: annotating sentences
based on Wikipedia metadata
</sectionHeader>
<bodyText confidence="0.99995425">
We followed the approach of Richman and Schone
(2008) to derive named entity annotations of both
English and foreign phrases in Wikipedia, using
Wikipedia metadata. The following sources of in-
formation were used from Wikipedia: category an-
notations on English documents, article links which
link from phrases in an article to another article in
the same language, and interwiki links which link
</bodyText>
<page confidence="0.999001">
695
</page>
<figureCaption confidence="0.981667">
Figure 2: Candidate NEs for the English and Bulgarian
sentences according to baseline taggers.
</figureCaption>
<bodyText confidence="0.999902345238096">
from articles in one language to comparable (seman-
tically equivalent) articles in the other language. In
addition to the Wikipedia-derived resources, the ap-
proach requires a manually specified map from En-
glish category key-phrases to NE tags, but does not
require expert knowledge for any non-English lan-
guage. We implemented the main ideas of the ap-
proach but some implementation details may differ.
To tag English language phrases, we first derived
named entity categorizations of English article titles,
by assigning a tag based on the article’s category
information. The category-to-NE map used for the
assignment is a small manually specified map from
phrases appearing in category titles to NE tags. For
example, if an article has categories “People by”,
“People from”, “Surnames” etc., it is classified as
PER. Looking at the example in Figure 1, the article
with title ”Igor Tudor” is classified as PER because
one of its categories is “Living people”. The full
map we use is taken from the paper (Richman and
Schone, 2008).
Using the article-level annotations and article
links we define a local English wiki-based tagger
and a global English wiki-based tagger, which will
be described in detail next.
Local English Wiki-based tagger. This Wiki-based
tagger tags phrases in an English article based on the
article links from these phrases to NE-tagged arti-
cles. For example, suppose that the phrase “Split” in
the article with title “Igor Tudor” is linked to the ar-
ticle with title “Split”, which is classified as GPE.
Thus the local English Wiki-based tagger can tag
this phrase as GPE. If, within the same article, the
phrase “Split” occurs again, it can be tagged again
even if it is not linked to a tagged article (this is
the one sense per document assumption). Addition-
ally, the tagger tags English phrases as DATE if they
match a set of manually specified regular expres-
sions. As a filter, phrases that do not contain a cap-
italized word or a number are not tagged with NE
tags.
Global English Wiki-based tagger. This tagger
tags phrases with NE tags if these phrases have ever
been linked to a categorized article (the most fre-
quent label is used). For example, if “Split” does
not have a link anywhere in the current article, but
has been linked to the GPE-labeled article with ti-
tle “Split” in another article, it will still be tagged
as GPE. We also apply a local+global Wiki-tagger,
which tags entities according to the local Wiki-
tagger and additionally tags any non-conflicting en-
tities according to the global tagger.
Local foreign Wiki-based tagger. The idea is the
same as for the local English tagger, with the dif-
ference that we first assign NE tags to foreign lan-
guage articles by using the NE tags assigned to En-
glish articles to which they are connected with inter-
wiki links. Because we do not have maps from cate-
gory phrases to NE tags for foreign languages, using
inter-wiki links is a way to transfer this knowledge
to the foreign languages. After we have categorized
foreign language articles we follow the same algo-
rithm as for the local English Wiki-based tagger. For
Bulgarian we also filtered out entities based on cap-
italization and numbers, but did not do that for Ko-
rean as it has no concept of capitalization.
Global foreign Wiki-based tagger The global and
local+global taggers are analogous, using the cate-
gorization of foreign articles as above.
Figure 2 shows the tags assigned to English and
Bulgarian strings according to the local and global
Wiki-based taggers. The global Wiki-based tag-
ger could assign multiple labels to the same string
(corresponding to different senses in different oc-
currences). In case of multiple possible labels, the
most frequent one is denoted by * in the Figure. The
Figure also shows the results of the Stanford NER
tagger for English (Finkel et al., 2005) (we used the
MUC-7 classifier).
Table 2 reports the performance of the local (L
Wiki-tagger), local+global (LG Wiki tagger) and the
Stanford tagger. We can see that the local Wiki tag-
gers have higher precision but lower recall than the
local+global Wiki taggers. The local+global taggers
</bodyText>
<page confidence="0.996985">
696
</page>
<table confidence="0.998312833333333">
Language L Wiki-tagger LG Wiki-tagger Stanford Tagger
Prec Rec F1 Prec Rec F1 Prec Rec F1
English 92.8 75.1 83.0 79.7 89.5 84.3 86.5 77.5 81.7
Bulgarian 94.1 48.7 64.2 86.8 79.9 83.2
English 92.6 75.6 83.2 84.1 86.7 85.4 82.2 71.9 76.7
Korean 89.5 57.3 69.9 43.2 78.0 55.6
</table>
<tableCaption confidence="0.998978">
Table 2: English-Bulgarian and English-Korean Wiki-based tagger performance.
</tableCaption>
<bodyText confidence="0.99987875">
are overall best for English and Bulgarian. The lo-
cal tagger is best for Korean, as the precision suffers
too much due to the global tagger. This is perhaps
due in part to the absence of the capitalization filter
for Korean which improved precision for Bulgarian
and English. The Stanford tagger is worse than the
Wiki-based tagger, but it is different enough that it
contributes useful information to the task.
</bodyText>
<sectionHeader confidence="0.998273" genericHeader="method">
4 Projection Model
</sectionHeader>
<bodyText confidence="0.99997836">
From Table 2 we can see that the English Wiki-
based taggers are better than the Bulgarian and Ko-
rean ones, which is due to the abundance and com-
pleteness of English data in Wikipedia. In such cir-
cumstances, previous research has shown that one
can project annotations from English to the more
resource-poor language (Yarowsky et al., 2001).
Here we follow the approach of Feng et al. (2004)
to train a log-linear model for projection.
Note that the Wiki-based taggers do not require
training data and can be applied to any sentences
from Wikipedia articles. The projection model de-
scribed in this section and the Semi-CRF model
described in Section 5 are trained using annotated
data. They can be applied to tag foreign sen-
tences in English-foreign sentence pairs extracted
from Wikipedia.
The task of projection is re-cast as a ranking task,
where for each source entity Si, we rank all possible
candidate target entity spans Tj and select the best
span as corresponding to this source entity. Each
target span is labeled with the NE label of the corre-
sponding source entity. The probability distribution
over target spans Tj for a given source entity Si is
defined as follows:
</bodyText>
<equation confidence="0.998556">
p(Si|Tj) = exp(λf(Si,TjT)))
�j (λf( j
</equation>
<bodyText confidence="0.998576846153846">
where λ is a parameter vector, and f(Si, Tj) is a fea-
ture vector for the candidate entity pair.
From this formulation we can see that a fixed set
of English source entities Si is required as input.
The model projects these entities to corresponding
foreign entities. We train and evaluate the projection
model using 10-fold cross-validation on the dataset
from Table 1. For training, we use the human-
annotated gold English entities and the manually-
specified entity alignments to derive corresponding
target entities. At test time we use the local+global
Wiki-based tagger to define the English entities and
we don’t use the manually annotated alignments.
</bodyText>
<subsectionHeader confidence="0.759893">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.99994925">
We present the features for this model in a lot of
detail since analogous feature types are also used in
our final direct semi-CRF model. The features are
grouped into four categories.
</bodyText>
<subsectionHeader confidence="0.577061">
Word alignment features
</subsectionHeader>
<bodyText confidence="0.874716416666666">
We exploit a feature set based on HMM word align-
ments in both directions (Och and Ney, 2000). To
define the features we make use of the posterior
alignment link probabilities as well as the most
likely (Viterbi) alignments. The posterior proba-
bilities are the probabilities of links in both direc-
tions given the source and target sentences: P(ai =
j|s, t) and P(aj = i|s, t).
If a source entity consists of positions il, ... , im
and a potential corresponding target entity consists
of positions jl, ... , jn, the word-alignment derived
features are:
</bodyText>
<listItem confidence="0.992965666666667">
• Probability that each word from one of the en-
tities is aligned to a word from the other entity,
estimated as:
</listItem>
<bodyText confidence="0.64298">
Fi∈i1 ... im Ej∈j1 ... jn P(ai = j |s, t) We use an
analogous estimate for the probability in the
other direction.
</bodyText>
<page confidence="0.980149">
697
</page>
<listItem confidence="0.6101542">
• Sum of posterior probabilities of links from
words inside one entity to words outside an-
other entity EiEi1 ... im(1 − Ej∈j1 ... jn P(ai �
j|s, t)). Probabilities from the other HMM di-
rection are estimated analogously.
• Indicator feature for whether the source and
target entity can be extracted as a phrase pair
according to the combined Viterbi alignments
(grow-diag-final) and the standard phrase ex-
traction heuristic (Koehn et al., 2003).
</listItem>
<subsectionHeader confidence="0.852945">
Phonetic similarity features
</subsectionHeader>
<bodyText confidence="0.998723736842105">
These features measure the similarity between a
source and target entity based on pronunciation. We
utilize a transliteration model (Cherry and Suzuki,
2009), trained from pairs of English person names
and corresponding foreign language names, ex-
tracted from Wikipedia. The transliteration model
can return an n-best list of transliterations of a for-
eign string, together with scores. For example the
top 3 transliterations in English of the Bulgarian
equivalent of “Igor Tudor” from Figure 1 are Igor
Twoodor, Igor Twoodore, and Igore Twoodore.
We estimate phonetic similarity between a source
and target entity by computing Levenshtein and
other distance metrics between the source entity
and the closest transliteration of the target (out of a
10-best list of transliterations). We use normalized
and un-normalized Levenshtein distance. We
also use a BLEU-type measure which estimates
character n-gram overlap.
</bodyText>
<subsectionHeader confidence="0.35038">
Position/Length features
</subsectionHeader>
<bodyText confidence="0.999868">
These report relative length and position of the
English and foreign entity following (Feng et al.,
2004).
</bodyText>
<subsectionHeader confidence="0.525849">
Wiki-based tagger features
</subsectionHeader>
<bodyText confidence="0.999982625">
These features look at the degree of match between
the source and target entities based on the tags as-
signed to them by the local and global Wiki-taggers
for English and the foreign language, and by the
Stanford tagger for English. These are indicator fea-
tures separate for the different source-target tagger
combinations, looking at whether the taggers agree
in their assignments to the candidate entities.
</bodyText>
<subsectionHeader confidence="0.994104">
4.2 Model Evaluation
</subsectionHeader>
<bodyText confidence="0.999625239130435">
We evaluate the tagging F-measure for projec-
tion models on the English-Bulgarian and English-
Korean datasets. 10-fold cross-validation was used
to estimate model performance. The foreign lan-
guage NE F-measure is reported in Table 3. The best
Wiki-based tagger performance is shown on the last
line as a baseline (repeated from Table 2).
We present a detailed evaluation of the model to
gain understanding of the strengths and limitations
of the projection approach and to motivate our direct
semi-CRF model. To give an estimate of the upper
bound on performance for the projection model, we
first present two oracles. The goal of the oracles it
to estimate the impact of two sources of error for the
projection model: the first is the error in detecting
English entities, and the second is the error in deter-
mining the corresponding foreign entity for a given
English entity.
The first oracle ORACLE1 has access to the gold-
standard English entities and gold-standard word
alignments among English and foreign words. For
each source entity, ORACLE1 selects the longest for-
eign language sequence of words that could be ex-
tracted in a phrase pair coupled with the source en-
tity word sequence (according the standard phrase
extraction heuristic (Koehn et al., 2003)), and labels
it with the label of the source entity. Note that the
word alignments do not uniquely identify the corre-
sponding foreign phrase for each English phrase and
some error is possible due to this. The performance
of this oracle is closely related to the percentage of
linked source-target entities reported in Table 1. The
second oracle ORACLE2 provides the performance
of the projection model when gold-standard source
entities are known, but the corresponding target en-
tities still have to be determined by the projection
model (gold-standard alignments are not known). In
other words, ORACLE2 is the projection model with
all features, where in the test set we provide the gold
standard English entities as input. The performance
of ORACLE2 is determined by the error in automatic
word alignment and in determining phonetic corre-
spondence. As we can see the drop due to this error
is very large, especially on Korean, where perfor-
mance drops from 90.0 to 81.9 F-measure.
The next section in the Table presents the perfor-
</bodyText>
<page confidence="0.996778">
698
</page>
<table confidence="0.999348142857143">
Method English-Bulgarian English-Korean
Prec Rec F1 Prec Rec F1
ORACLE1 98.3 92.9 95.5 95.5 85.1 90.0
ORACLE2 96.7 86.3 91.2 90.5 74.7 81.9
PM-WF 71.7 80.0 75.7 85.1 72.2 78.1
PM+WF 73.6 81.3 77.2 87.6 74.9 80.8
Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9
</table>
<tableCaption confidence="0.999903">
Table 3: English-Bulgarian and English-Korean Projection tagger performance.
</tableCaption>
<bodyText confidence="0.999983647058824">
mance of non-oracle projection models, which do
not have access to any manually labeled informa-
tion. The local+global Wiki-based tagger is used to
define English entities, and only automatically de-
rived alignment information is used. PM+WF is the
projection model using all features. The line above,
PM-WF represents the projection model without
the Wiki-tagger derived features, and is included to
show that the gain from using these features is sub-
stantial. The difference in accuracy between the pro-
jection model and ORACLE2 is very large, and is due
to the error of the Wiki-based English taggers. The
drop for Bulgarian is so large that the best projec-
tion model PM+WF does not reach the performance
of 83.2 achieved by the baseline Wiki-based tagger.
When source entities are assigned with error for this
language pair, projecting entity annotations from the
source is not better than using the target Wiki-based
annotations directly. For Korean while the trend in
model performance is similar as oracle information
is removed, the projection model achieves substan-
tially better performance (80.8 vs 69.9) due to the
much larger difference in performance between the
English and Korean Wiki-based taggers.
The drawback of the projection model is that it
determines target entities only by assigning the best
candidate for each source entity. It cannot create tar-
get entities that do not correspond to source entities,
it is not able to take into account multiple conflicting
source NE taggers as sources of information, and it
does not make use of target sentence context and en-
tity consistency constraints. To address these short-
comings we propose a direct semi-CRF model, de-
scribed in the next section.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
5 Semi-CRF Model
</sectionHeader>
<bodyText confidence="0.999798814814814">
Semi-Markov conditional random fields (semi-
CRFs) are a generalization of CRFs. They assign la-
bels to segments of an input sequence x, rather than
to individual elements xi and features can be de-
fined on complete segments. We apply Semi-CRFs
to learn a NE tagger for labeling foreign sentences in
the context of corresponding source sentences with
existing NE annotations.
The semi-CRF defines a distribution over foreign
sentence labeled segmentations (where the segments
are named entities with their labels, or segments of
length one with label “NONE”). To formally define
the distribution, we introduce some notation follow-
ing Sarawagi and Cohen (2005):
Let s = (sl, ... , sp) denote a segmentation of
the foreign sentence x, where a segment sj =
(tj, uj, yj) is determined by its start position tj, end
position uj, and label yj. Features are defined on
segments and adjacent segment labels. In our appli-
cation, we only use features on segments. The fea-
tures on segments can also use information from the
corresponding English sentence e along with exter-
nal annotations on the sentence pair A.
The feature vector for each segment can be de-
noted by F(j, s, x, e, A) and the weight vector for
features by w. The probability of a segmentation is
then defined as:
</bodyText>
<equation confidence="0.984323666666667">
P(s |Ax,e) = E exp w&apos;F(j, s, x, e, A)
j
Z(x, e, A)
</equation>
<bodyText confidence="0.99859">
In the equation above Z represents a normalizer
summing over valid segmentations.
</bodyText>
<subsectionHeader confidence="0.890215">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.999914428571429">
We use both boolean and real-valued features in the
semi-CRF model. Example features and their val-
ues are given in Table 4. The features are the ones
that fire on the segment of length 1 containing the
Bulgarian equivalent of the word “Split” and la-
beled with label GPE (tj=13,uj=13,yj=GPE), from
the English-Bulgarian sentence pair in Figure 1.
</bodyText>
<page confidence="0.99806">
699
</page>
<bodyText confidence="0.99854083908046">
The features look at the English and foreign sen-
tence as well as external annotations A. Note that
the semi-CRF model formulation does not require a
fixed labeling of the English sentence. Different and
possibly conflicting NE tags for candidate English
and foreign sentence substrings according to the
Wiki-based taggers and the Stanford tagger are spec-
ified as one type of external annotations (see Figure
2). Another annotation type is derived from HMM-
based word alignments and the transliteration model
described in Section 4. They provide two kinds of
alignment links between English and foreign tokens:
one based on the HMM-word alignments (poste-
rior probability of the link in both directions), and
another based on different character-based distance
metrics between transliterations of foreign words
and English words. The transliteration model and
distance metrics were described in Section 4 as well.
For the example Bulgarian correspondent of “Split”
in the figure, the English “Split” is linked to it ac-
cording to both the forward and backward HMM,
and according to two out of the three transliteration
distance measures. A third annotation type is au-
tomatically derived links between foreign candidate
entity strings (sequences of tokens) and best corre-
sponding English candidate entities. The candidate
English entities are defined by the union of entities
proposed by the Wiki-based taggers and the Stan-
ford tagger. Note that these English candidate en-
tities can be overlapping and inconsistent without
harming the model. We link foreign candidate seg-
ments with English candidate entities based on the
projection model described in Section 4 and trained
on the same data. The projection model scores every
source-target entity pair and selects the best source
for each target candidate entity. For our example
target segment, the corresponding source candidate
entity is “Split”, labeled GPE by the local+global
Wiki-tagger and by the global Wiki-tagger.
The features are grouped into three categories:
Group 1. Foreign Wiki-based tagger features.
These features look at target segments and extract
indicators of whether the label of the segment agrees
with the label assigned by the local, global, and/or
local+global wiki tagger. For the example segment
from the sentence in Figure 1, since neither the local
nor global tagger have assigned a label GPE, the first
three features have value zero. In addition to tags on
the whole segment, we look at tag combinations for
individual words within the segment as well as two
words to the left and right outside the segment. In
the first section in Table 4 we can see several feature
types and and their values for our example.
Group 2. Foreign surface-based features. These
features look at orthographic properties of the words
and distinguish several word types. The types are
based on capitalization and also distinguish numbers
and punctuation. In addition, we make use of word-
clusters generated by JCluster. 1
We look at properties of the individual words as
well as the concatenation for all words in the seg-
ment. In addition, there are features for words two
words to the left and two words to the right outside
the segment. The second section in the Table shows
several features of this type with their values.
Group 3. Label match between English and
aligned foreign entities. These features look at
the linked English segment for the candidate tar-
get segment and compare the tags assigned to the
English segment by the different English taggers to
the candidate target label. In addition to segment-
level comparisons, they also look at tag assignments
for individual source tokens linked to the individual
target tokens (by word alignment and transliteration
links). The last section in the Table contains sample
features with their values. The feature SOURCE-E-
WIKI-TAG-MATCH looks at whether the correspond-
ing source entity has the same local+global Wiki-
tagger assigned tag as the candidate target entity.
The next two features look at the Stanford tagger
and the global Wiki-tagger. The real-valued fea-
tures like SCORE-SOURCE-E-WIKI-TAG-MATCH re-
turn the score of the matching between the source
and target candidate entities (according to the pro-
jection model), if the labels match. In this way, more
confident matchings can impact the target tags more
than less confident ones.
</bodyText>
<subsectionHeader confidence="0.989379">
5.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.9881105">
Our main results are listed in Table 5. We perform
10-fold cross-validation as in the projection experi-
ments. The best Wiki-based and projection models
are listed as baselines at the bottom of the table.
</bodyText>
<footnote confidence="0.988944666666667">
1Software distributed by Joshua Goodman
http://research.microsoft.com/en-us/downloads/0183a49d-
c86c-4d80-aa0d-53c97ba7350a/default.aspx.
</footnote>
<page confidence="0.95813">
700
</page>
<table confidence="0.999389888888889">
Method English-Bulgarian English-Korean
Prec Rec F1 Prec Rec F1
MONO 86.7 79.4 82.9 89.1 57.1 69.6
BI 90.1 83.3 86.6 88.6 79.8 84.0
MONO-ALL 94.7 86.2 90.3 90.2 84.3 87.2
BI-ALL-WT 95.7 87.6 91.5 92.4 87.6 89.9
BI-ALL 96.4 89.4 92.8 94.7 87.9 91.2
Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9
PM+WF 73.6 81.3 77.2 87.6 74.9 80.8
</table>
<tableCaption confidence="0.983718">
Table 5: English-Bulgarian and English-Korean semi-CRF tagger performance.
</tableCaption>
<table confidence="0.99954952173913">
Feature Description Example Value
WIKI-TAG-MATCH 0
WIKI-GLOBAL-TAG-MATCH 0
WIKIGLOBAL-POSSIBLE-TAG 0
WIKI-TAG&amp;LABEL NONE&amp;GPE
WIKI-GLOBAL-TAG&amp;LABEL NONE&amp;GPE
FIRST-WORD-CAP 1
CONTAINS-NUMBER 0
PREV-WORD-CAP 0
WORD-TYPE&amp;LABEL Xxxx&amp;GPE
WORD-CLUSTER&amp; LABEL 101&amp;GPE
SEGMENT-WORD-TYPE&amp;LABEL Xxxx&amp;GPE
SEGMENT-WORD-CLUSTER&amp;LABEL Xxxx&amp;GPE
SOURCE-E-WIKI-TAG-MATCH 1
SOURCE-E-STANFORD-TAG-MATCH 0
SOURCE-E-WIKI-GLOBAL-TAG-MATCH 1
SOURCE-E-POSSIBLE-GLOBAL 1
SOURCE-E-ALL-TAG-MATCH 0
SOURCE-W-FWA-TAG &amp; LABEL GPE &amp; GPE
SOURCE-W-BWA-TAG &amp; LABEL GPE &amp; GPE
SCORE-SOURCE-E-WIKI-TAG-MATCH -0.009
SCORE-SOURCE-E-GLOBAL-TAG-MATCH -0.009
SCORE-SOURCE-E-STANFORD-TAG-MATCH -1
</table>
<tableCaption confidence="0.99968">
Table 4: Features with example values.
</tableCaption>
<bodyText confidence="0.999964090909091">
We look at performance using four sets of fea-
tures: (i) Monolingual Wiki-tagger based, using
only the features in Group 1 (MONO); (ii) Bilingual
label match and Wiki-tagger based, using features
in Groups 1 and 3 (BI); (iii) Monolingual all, us-
ing features in Groups 1 and 2 (MONO-ALL), and
(iv) Bilingual all, using all features (BI-ALL). Ad-
ditionally, we report performance of the full bilin-
gual model with all features, but when English can-
didate entities are generated only according to the
local+global Wiki-taggger (BI-ALL-WT).
The main results show that the full semi-CRF
model greatly outperforms the baseline projection
and Wiki-taggers. For Bulgarian, the F-measure of
the full model is 92.8 compared to the best base-
line result of 83.2. For Korean, the F-measure of the
semi-CRF is 91.2, more than 10 points higher than
the performance of the projection model.
Within the semi-CRF model, the contribution of
English sentence context was substantial, leading to
2.5 point increase in F-measure for Bulgarian (92.8
versus 90.3 F-measure), and 4.0 point increase for
Korean (91.2 versus 87.2).
The additional gain due to considering candidate
source entities generated from all English taggers
was 1.3 F-measure points for both language pairs
(comparing models BI-ALL and BI-ALL-WT).
If we restrict the semi-CRF to use only features
similar to the ones used by the projection model, we
still obtain performance much better than that of the
projection model: comparing BI to the projection
model, we see gains of 9.4 points for Bulgarian, and
4 points for Korean. This is due to the fact that the
semi-CRF is able to relax the assumption of one-to-
one correspondence between source and target enti-
ties, and can effectively combine information from
multiple source and target taggers.
We should note that the proposed method can only
tag foreign sentences in English-foreign sentence
pairs. The next step for this work is to train mono-
lingual NE taggers for the foreign languages, which
can work on text within or outside of Wikipedia.
Preliminary results show performance of over 80 F-
measure for such monolingual models.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.994134444444444">
As discussed throughout the paper, our model builds
upon prior work on Wikipedia metadata-based NE
tagging (Richman and Schone, 2008) and cross-
lingual projection for named entities (Feng et al.,
2004). Other interesting work on aligning named
entities in two languages is reported in (Huang and
Vogel, 2002; Moore, 2003).
Our direct semi-CRF tagging approach is related
to bilingual labeling models presented in previous
</bodyText>
<page confidence="0.99127">
701
</page>
<bodyText confidence="0.999792416666667">
work (Burkett et al., 2010a; Smith and Smith, 2004;
Snyder and Barzilay, 2008). All of these models
jointly label aligned source and target sentences. In
contrast, our model is not concerned with tagging
English sentences but only tags foreign sentences in
the context of English sentences. Compared to the
joint log-linear model of Burkett et al. (2010a), our
semi-CRF approach does not require enumeration of
n-best candidates for the English sentence and is not
limited to n-best candidates for the foreign sentence.
It enables the use of multiple unweighted and over-
lapping entity annotations on the English sentence.
</bodyText>
<sectionHeader confidence="0.999566" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999582625">
In this paper we showed that using resources from
Wikipedia, it is possible to combine metadata-based
approaches and projection-based approaches for in-
ducing named entity annotations for foreign lan-
guages. We presented a direct semi-CRF tagging
model for labeling foreign sentences in parallel sen-
tence pairs, which outperformed projection by more
than 10 F-measure points for Bulgarian and Korean.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99964934375">
David Burkett, John Blitzer, and Dan Klein. 2010a.
Joint parsing and alignment with weakly synchronized
grammars. In Proceedings of NAACL.
David Burkett, Slav Petrov, John Blitzer, and Dan
Klein. 2010b. Learning better monolingual models
with unannotated bilingual text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 46–54, Uppsala, Sweden,
July. Association for Computational Linguistics.
Colin Cherry and Hisami Suzuki. 2009. Discrimina-
tive substring decoding for transliteration. In EMNLP,
pages 1066–1075.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600–609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A new
approach for English-Chinese named entity alignment.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing EMNLP, pages
372–379.
Jenny Finkel, Trond Grenager, and Christopher D. Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
ACL.
Fei Huang and Stephan Vogel. 2002. Improved named
entity translation and bilingual named entity extrac-
tion. In ICMI.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL, pages 127–133.
Robert C. Moore. 2003. Learning translations of named-
entity phrases from parallel corpora. In EACL.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named entity
recognition. In ACL.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In In Advances in Neural Information Pro-
cessing Systems 17, pages 1185–1192.
Sunita Sarawagi and William W. Cohen. 2005. Semi-
markov conditional random fields for information ex-
traction. In In Advances in Neural Information Pro-
cessing Systems 17 (NIPS 2004).
David A. Smith and Noah A. Smith. 2004. Bilin-
gual parsing with factored estimation: using English
to parse Korean. In EMNLP.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In HLT,
pages 403–411, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Crosslin-
gual propagation for morphological analysis. In AAAI.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In HLT.
</reference>
<page confidence="0.997741">
702
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131844">
<title confidence="0.867652">Multilingual Named Entity Recognition using Parallel Data and from Wikipedia</title>
<author confidence="0.872167">South Pohang</author>
<email confidence="0.929088">subright@postech.ac.kr</email>
<author confidence="0.71701">Kristina</author>
<affiliation confidence="0.923551">Microsoft</affiliation>
<address confidence="0.989881">Redmond, WA</address>
<email confidence="0.999733">kristout@microsoft.com</email>
<author confidence="0.5094005">Hwanjo Pohang</author>
<author confidence="0.5094005">South</author>
<email confidence="0.959601">hwanjoyu@postech.ac.kr</email>
<abstract confidence="0.989528">In this paper we propose a method to automatically label multi-lingual data with named entity tags. We build on prior work utilizing Wikipedia metadata and show how to effectively combine the weak annotations stemming from Wikipedia metadata with information obtained through English-foreign language parallel Wikipedia sentences. The combination is achieved using a novel semi-CRF model for foreign sentence tagging in the context of a parallel English sentence. The model outperforms both standard annotation projection methods and methods based solely on Wikipedia metadata.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1897" citStr="Burkett et al., 2010" startWordPosition="279" endWordPosition="282">offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model of Burkett et al. (2010a), our model can incorporate both monolingual and bilingual features in a log-linear framework. The advantage</context>
<context position="31552" citStr="Burkett et al., 2010" startWordPosition="5036" endWordPosition="5039">n languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for the foreign sentence. It enables the use of multiple unweighted and overlapping entity annotations on the English sentence. 7 Co</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010a. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Slav Petrov</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Learning better monolingual models with unannotated bilingual text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>46--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1897" citStr="Burkett et al., 2010" startWordPosition="279" endWordPosition="282">offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model of Burkett et al. (2010a), our model can incorporate both monolingual and bilingual features in a log-linear framework. The advantage</context>
<context position="31552" citStr="Burkett et al., 2010" startWordPosition="5036" endWordPosition="5039">n languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for the foreign sentence. It enables the use of multiple unweighted and overlapping entity annotations on the English sentence. 7 Co</context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, Slav Petrov, John Blitzer, and Dan Klein. 2010b. Learning better monolingual models with unannotated bilingual text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46–54, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Hisami Suzuki</author>
</authors>
<title>Discriminative substring decoding for transliteration.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1066--1075</pages>
<contexts>
<context position="15685" citStr="Cherry and Suzuki, 2009" startWordPosition="2538" endWordPosition="2541">f posterior probabilities of links from words inside one entity to words outside another entity EiEi1 ... im(1 − Ej∈j1 ... jn P(ai � j|s, t)). Probabilities from the other HMM direction are estimated analogously. • Indicator feature for whether the source and target entity can be extracted as a phrase pair according to the combined Viterbi alignments (grow-diag-final) and the standard phrase extraction heuristic (Koehn et al., 2003). Phonetic similarity features These features measure the similarity between a source and target entity based on pronunciation. We utilize a transliteration model (Cherry and Suzuki, 2009), trained from pairs of English person names and corresponding foreign language names, extracted from Wikipedia. The transliteration model can return an n-best list of transliterations of a foreign string, together with scores. For example the top 3 transliterations in English of the Bulgarian equivalent of “Igor Tudor” from Figure 1 are Igor Twoodor, Igor Twoodore, and Igore Twoodore. We estimate phonetic similarity between a source and target entity by computing Levenshtein and other distance metrics between the source entity and the closest transliteration of the target (out of a 10-best li</context>
</contexts>
<marker>Cherry, Suzuki, 2009</marker>
<rawString>Colin Cherry and Hisami Suzuki. 2009. Discriminative substring decoding for transliteration. In EMNLP, pages 1066–1075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>600--609</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1765" citStr="Das and Petrov, 2011" startWordPosition="256" endWordPosition="259">rly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model o</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600–609, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghui Feng</author>
<author>Yajuan Lv</author>
<author>Ming Zhou</author>
</authors>
<title>A new approach for English-Chinese named entity alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP,</booktitle>
<pages>372--379</pages>
<contexts>
<context position="12568" citStr="Feng et al. (2004)" startWordPosition="2015" endWordPosition="2018">ion filter for Korean which improved precision for Bulgarian and English. The Stanford tagger is worse than the Wiki-based tagger, but it is different enough that it contributes useful information to the task. 4 Projection Model From Table 2 we can see that the English Wikibased taggers are better than the Bulgarian and Korean ones, which is due to the abundance and completeness of English data in Wikipedia. In such circumstances, previous research has shown that one can project annotations from English to the more resource-poor language (Yarowsky et al., 2001). Here we follow the approach of Feng et al. (2004) to train a log-linear model for projection. Note that the Wiki-based taggers do not require training data and can be applied to any sentences from Wikipedia articles. The projection model described in this section and the Semi-CRF model described in Section 5 are trained using annotated data. They can be applied to tag foreign sentences in English-foreign sentence pairs extracted from Wikipedia. The task of projection is re-cast as a ranking task, where for each source entity Si, we rank all possible candidate target entity spans Tj and select the best span as corresponding to this source ent</context>
<context position="16572" citStr="Feng et al., 2004" startWordPosition="2670" endWordPosition="2673"> English of the Bulgarian equivalent of “Igor Tudor” from Figure 1 are Igor Twoodor, Igor Twoodore, and Igore Twoodore. We estimate phonetic similarity between a source and target entity by computing Levenshtein and other distance metrics between the source entity and the closest transliteration of the target (out of a 10-best list of transliterations). We use normalized and un-normalized Levenshtein distance. We also use a BLEU-type measure which estimates character n-gram overlap. Position/Length features These report relative length and position of the English and foreign entity following (Feng et al., 2004). Wiki-based tagger features These features look at the degree of match between the source and target entities based on the tags assigned to them by the local and global Wiki-taggers for English and the foreign language, and by the Stanford tagger for English. These are indicator features separate for the different source-target tagger combinations, looking at whether the taggers agree in their assignments to the candidate entities. 4.2 Model Evaluation We evaluate the tagging F-measure for projection models on the English-Bulgarian and EnglishKorean datasets. 10-fold cross-validation was used</context>
<context position="31301" citStr="Feng et al., 2004" startWordPosition="4997" endWordPosition="5000">vely combine information from multiple source and target taggers. We should note that the proposed method can only tag foreign sentences in English-foreign sentence pairs. The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF appro</context>
</contexts>
<marker>Feng, Lv, Zhou, 2004</marker>
<rawString>Donghui Feng, Yajuan Lv, and Ming Zhou. 2004. A new approach for English-Chinese named entity alignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP, pages 372–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11105" citStr="Finkel et al., 2005" startWordPosition="1768" endWordPosition="1771"> it has no concept of capitalization. Global foreign Wiki-based tagger The global and local+global taggers are analogous, using the categorization of foreign articles as above. Figure 2 shows the tags assigned to English and Bulgarian strings according to the local and global Wiki-based taggers. The global Wiki-based tagger could assign multiple labels to the same string (corresponding to different senses in different occurrences). In case of multiple possible labels, the most frequent one is denoted by * in the Figure. The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier). Table 2 reports the performance of the local (L Wiki-tagger), local+global (LG Wiki tagger) and the Stanford tagger. We can see that the local Wiki taggers have higher precision but lower recall than the local+global Wiki taggers. The local+global taggers 696 Language L Wiki-tagger LG Wiki-tagger Stanford Tagger Prec Rec F1 Prec Rec F1 Prec Rec F1 English 92.8 75.1 83.0 79.7 89.5 84.3 86.5 77.5 81.7 Bulgarian 94.1 48.7 64.2 86.8 79.9 83.2 English 92.6 75.6 83.2 84.1 86.7 85.4 82.2 71.9 76.7 Korean 89.5 57.3 69.9 43.2 78.0 55.6 Table 2: English-Bulgarian and Eng</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Stephan Vogel</author>
</authors>
<title>Improved named entity translation and bilingual named entity extraction.</title>
<date>2002</date>
<booktitle>In ICMI.</booktitle>
<contexts>
<context position="31407" citStr="Huang and Vogel, 2002" startWordPosition="5014" endWordPosition="5017">od can only tag foreign sentences in English-foreign sentence pairs. The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-bes</context>
</contexts>
<marker>Huang, Vogel, 2002</marker>
<rawString>Fei Huang and Stephan Vogel. 2002. Improved named entity translation and bilingual named entity extraction. In ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="15497" citStr="Koehn et al., 2003" startWordPosition="2512" endWordPosition="2515">s aligned to a word from the other entity, estimated as: Fi∈i1 ... im Ej∈j1 ... jn P(ai = j |s, t) We use an analogous estimate for the probability in the other direction. 697 • Sum of posterior probabilities of links from words inside one entity to words outside another entity EiEi1 ... im(1 − Ej∈j1 ... jn P(ai � j|s, t)). Probabilities from the other HMM direction are estimated analogously. • Indicator feature for whether the source and target entity can be extracted as a phrase pair according to the combined Viterbi alignments (grow-diag-final) and the standard phrase extraction heuristic (Koehn et al., 2003). Phonetic similarity features These features measure the similarity between a source and target entity based on pronunciation. We utilize a transliteration model (Cherry and Suzuki, 2009), trained from pairs of English person names and corresponding foreign language names, extracted from Wikipedia. The transliteration model can return an n-best list of transliterations of a foreign string, together with scores. For example the top 3 transliterations in English of the Bulgarian equivalent of “Igor Tudor” from Figure 1 are Igor Twoodor, Igor Twoodore, and Igore Twoodore. We estimate phonetic si</context>
<context position="18290" citStr="Koehn et al., 2003" startWordPosition="2949" endWordPosition="2952">it to estimate the impact of two sources of error for the projection model: the first is the error in detecting English entities, and the second is the error in determining the corresponding foreign entity for a given English entity. The first oracle ORACLE1 has access to the goldstandard English entities and gold-standard word alignments among English and foreign words. For each source entity, ORACLE1 selects the longest foreign language sequence of words that could be extracted in a phrase pair coupled with the source entity word sequence (according the standard phrase extraction heuristic (Koehn et al., 2003)), and labels it with the label of the source entity. Note that the word alignments do not uniquely identify the corresponding foreign phrase for each English phrase and some error is possible due to this. The performance of this oracle is closely related to the percentage of linked source-target entities reported in Table 1. The second oracle ORACLE2 provides the performance of the projection model when gold-standard source entities are known, but the corresponding target entities still have to be determined by the projection model (gold-standard alignments are not known). In other words, ORA</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Learning translations of namedentity phrases from parallel corpora.</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="31421" citStr="Moore, 2003" startWordPosition="5018" endWordPosition="5019"> sentences in English-foreign sentence pairs. The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates f</context>
</contexts>
<marker>Moore, 2003</marker>
<rawString>Robert C. Moore. 2003. Learning translations of namedentity phrases from parallel corpora. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14364" citStr="Och and Ney, 2000" startWordPosition="2314" endWordPosition="2317">aset from Table 1. For training, we use the humanannotated gold English entities and the manuallyspecified entity alignments to derive corresponding target entities. At test time we use the local+global Wiki-based tagger to define the English entities and we don’t use the manually annotated alignments. 4.1 Features We present the features for this model in a lot of detail since analogous feature types are also used in our final direct semi-CRF model. The features are grouped into four categories. Word alignment features We exploit a feature set based on HMM word alignments in both directions (Och and Ney, 2000). To define the features we make use of the posterior alignment link probabilities as well as the most likely (Viterbi) alignments. The posterior probabilities are the probabilities of links in both directions given the source and target sentences: P(ai = j|s, t) and P(aj = i|s, t). If a source entity consists of positions il, ... , im and a potential corresponding target entity consists of positions jl, ... , jn, the word-alignment derived features are: • Probability that each word from one of the entities is aligned to a word from the other entity, estimated as: Fi∈i1 ... im Ej∈j1 ... jn P(a</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander E Richman</author>
<author>Patrick Schone</author>
</authors>
<title>Mining wiki resources for multilingual named entity recognition.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1561" citStr="Richman and Schone, 2008" startWordPosition="224" endWordPosition="227">tly needed technology in NLP applications. State-ofthe-art statistical models for NER typically require a large amount of training data and linguistic expertise to be sufficiently accurate, which makes it nearly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approach</context>
<context position="6831" citStr="Richman and Schone (2008)" startWordPosition="1054" endWordPosition="1057">guage. This is due to two phenomena: one is that the parallel sentences sometimes contain different amounts of information and one language might use more detail than the other. The other is that the same information might be expressed using a named entity in one language, and using a non-entity phrase in the other language (e.g. “He is from Bulgaria” versus “He is Bulgarian”). Both of these causes of divergence are much more common in the English-Korean dataset than in the English-Bulgarian one. 3 Wiki-based tagger: annotating sentences based on Wikipedia metadata We followed the approach of Richman and Schone (2008) to derive named entity annotations of both English and foreign phrases in Wikipedia, using Wikipedia metadata. The following sources of information were used from Wikipedia: category annotations on English documents, article links which link from phrases in an article to another article in the same language, and interwiki links which link 695 Figure 2: Candidate NEs for the English and Bulgarian sentences according to baseline taggers. from articles in one language to comparable (semantically equivalent) articles in the other language. In addition to the Wikipedia-derived resources, the appro</context>
<context position="8303" citStr="Richman and Schone, 2008" startWordPosition="1289" endWordPosition="1292">g English language phrases, we first derived named entity categorizations of English article titles, by assigning a tag based on the article’s category information. The category-to-NE map used for the assignment is a small manually specified map from phrases appearing in category titles to NE tags. For example, if an article has categories “People by”, “People from”, “Surnames” etc., it is classified as PER. Looking at the example in Figure 1, the article with title ”Igor Tudor” is classified as PER because one of its categories is “Living people”. The full map we use is taken from the paper (Richman and Schone, 2008). Using the article-level annotations and article links we define a local English wiki-based tagger and a global English wiki-based tagger, which will be described in detail next. Local English Wiki-based tagger. This Wiki-based tagger tags phrases in an English article based on the article links from these phrases to NE-tagged articles. For example, suppose that the phrase “Split” in the article with title “Igor Tudor” is linked to the article with title “Split”, which is classified as GPE. Thus the local English Wiki-based tagger can tag this phrase as GPE. If, within the same article, the p</context>
<context position="31234" citStr="Richman and Schone, 2008" startWordPosition="4986" endWordPosition="4989">e-toone correspondence between source and target entities, and can effectively combine information from multiple source and target taggers. We should note that the proposed method can only tag foreign sentences in English-foreign sentence pairs. The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the j</context>
</contexts>
<marker>Richman, Schone, 2008</marker>
<rawString>Alexander E. Richman and Patrick Schone. 2008. Mining wiki resources for multilingual named entity recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In In Advances in Neural Information Processing Systems 17,</booktitle>
<pages>1185--1192</pages>
<contexts>
<context position="2899" citStr="Sarawagi and Cohen, 2004" startWordPosition="437" endWordPosition="440">stical model which can combine the two types of information. Similarly to the joint model of Burkett et al. (2010a), our model can incorporate both monolingual and bilingual features in a log-linear framework. The advantage of our model is that it is much more efficient as it does not require summing over matchings of source and target entities. It is a conditional model for target sentence annotation given an aligned English source sentence, where the English sentence is used only as a source of features. Exact inference is performed using standard semi-markov CRF model inference techniques (Sarawagi and Cohen, 2004). Our results show that the semi-CRF model improves on the performance of projection models by more than 10 points in F-measure, and that we can achieve tagging F-measure of over 91 using a very small number of annotated sentence pairs. The paper is organized as follows: We first describe the datasets and task setting in Section 2. Next, we present our two baseline methods: A Wikipedia metadata-based tagger and a crosslingual projection tagger in Sections 3 and 4, respectively. We present our direct semi-CRF tagging model in Section 5. 2 Data and task As a case study, we focus on two very diff</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In In Advances in Neural Information Processing Systems 17, pages 1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2005</date>
<booktitle>In In Advances in Neural Information Processing Systems 17 (NIPS</booktitle>
<contexts>
<context position="22040" citStr="Sarawagi and Cohen (2005)" startWordPosition="3552" endWordPosition="3555"> fields (semiCRFs) are a generalization of CRFs. They assign labels to segments of an input sequence x, rather than to individual elements xi and features can be defined on complete segments. We apply Semi-CRFs to learn a NE tagger for labeling foreign sentences in the context of corresponding source sentences with existing NE annotations. The semi-CRF defines a distribution over foreign sentence labeled segmentations (where the segments are named entities with their labels, or segments of length one with label “NONE”). To formally define the distribution, we introduce some notation following Sarawagi and Cohen (2005): Let s = (sl, ... , sp) denote a segmentation of the foreign sentence x, where a segment sj = (tj, uj, yj) is determined by its start position tj, end position uj, and label yj. Features are defined on segments and adjacent segment labels. In our application, we only use features on segments. The features on segments can also use information from the corresponding English sentence e along with external annotations on the sentence pair A. The feature vector for each segment can be denoted by F(j, s, x, e, A) and the weight vector for features by w. The probability of a segmentation is then def</context>
</contexts>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2005. Semimarkov conditional random fields for information extraction. In In Advances in Neural Information Processing Systems 17 (NIPS 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Bilingual parsing with factored estimation: using English to parse Korean.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="31576" citStr="Smith and Smith, 2004" startWordPosition="5040" endWordPosition="5043">work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for the foreign sentence. It enables the use of multiple unweighted and overlapping entity annotations on the English sentence. 7 Conclusions In this paper </context>
</contexts>
<marker>Smith, Smith, 2004</marker>
<rawString>David A. Smith and Noah A. Smith. 2004. Bilingual parsing with factored estimation: using English to parse Korean. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Chris Quirk</author>
<author>Kristina Toutanova</author>
</authors>
<title>Extracting parallel sentences from comparable corpora using document level alignment.</title>
<date>2010</date>
<booktitle>In HLT,</booktitle>
<pages>403--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4177" citStr="Smith et al. (2010)" startWordPosition="640" endWordPosition="643">and foreign language sentences that comprise our training and test data are extracted from Wikipedia (http://www.wikipedia.org). Currently there are more than 3.8 million articles in the English Wikipedia, 125,000 in the Bulgarian Wikipedia, and 131,000 in the Korean Wikipedia. 694 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 694–702, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Figure 1: A parallel sentence-pair showing gold-standard NE labels and word alignments. To create our dataset, we followed Smith et al. (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. The approach uses a small amount of manually annotated article-pairs to train a document-level CRF model for parallel sentence extraction. A total of 13,410 English-Bulgarian and 8,832 English-Korean sentence pairs were extracted. Of these, we manually annotated 91 EnglishBulgarian and 79 English-Korean sentence pairs with source and target named entities as well as word-alignment links among named entities in the two languages. Figure 1 illustrates a BulgarianEnglish sentence pair with alignment. The na</context>
</contexts>
<marker>Smith, Quirk, Toutanova, 2010</marker>
<rawString>Jason R. Smith, Chris Quirk, and Kristina Toutanova. 2010. Extracting parallel sentences from comparable corpora using document level alignment. In HLT, pages 403–411, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
</authors>
<title>Crosslingual propagation for morphological analysis.</title>
<date>2008</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="31604" citStr="Snyder and Barzilay, 2008" startWordPosition="5044" endWordPosition="5047">outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous 701 work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for the foreign sentence. It enables the use of multiple unweighted and overlapping entity annotations on the English sentence. 7 Conclusions In this paper we showed that using resourc</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>Benjamin Snyder and Regina Barzilay. 2008. Crosslingual propagation for morphological analysis. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT.</title>
<date>2001</date>
<contexts>
<context position="1742" citStr="Yarowsky et al., 2001" startWordPosition="252" endWordPosition="255">ate, which makes it nearly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similar</context>
<context position="12517" citStr="Yarowsky et al., 2001" startWordPosition="2005" endWordPosition="2008">s perhaps due in part to the absence of the capitalization filter for Korean which improved precision for Bulgarian and English. The Stanford tagger is worse than the Wiki-based tagger, but it is different enough that it contributes useful information to the task. 4 Projection Model From Table 2 we can see that the English Wikibased taggers are better than the Bulgarian and Korean ones, which is due to the abundance and completeness of English data in Wikipedia. In such circumstances, previous research has shown that one can project annotations from English to the more resource-poor language (Yarowsky et al., 2001). Here we follow the approach of Feng et al. (2004) to train a log-linear model for projection. Note that the Wiki-based taggers do not require training data and can be applied to any sentences from Wikipedia articles. The projection model described in this section and the Semi-CRF model described in Section 5 are trained using annotated data. They can be applied to tag foreign sentences in English-foreign sentence pairs extracted from Wikipedia. The task of projection is re-cast as a ranking task, where for each source entity Si, we rank all possible candidate target entity spans Tj and selec</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>