<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002046">
<title confidence="0.990072">
The Good, the Bad, and the Unknown:
Morphosyllabic Sentiment Tagging of Unseen Words
</title>
<author confidence="0.996702">
Karo Moilanen and Stephen Pulman
</author>
<affiliation confidence="0.99732">
Oxford University Computing Laboratory
</affiliation>
<address confidence="0.940969">
Wolfson Building, Parks Road, Oxford, OX1 3QD, England
</address>
<email confidence="0.981823">
{ Karo.Moilanen  |Stephen.Pulman }@comlab.ox.ac.uk
</email>
<sectionHeader confidence="0.998426" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998751066666667">
The omnipresence of unknown words is a
problem that any NLP component needs to ad-
dress in some form. While there exist many
established techniques for dealing with un-
known words in the realm of POS-tagging, for
example, guessing unknown words’ semantic
properties is a less-explored area with greater
challenges. In this paper, we study the seman-
tic field of sentiment and propose five methods
for assigning prior sentiment polarities to un-
known words based on known sentiment carri-
ers. Tested on 2000 cases, the methods mirror
human judgements closely in three- and two-
way polarity classification tasks, and reach ac-
curacies above 63% and 81%, respectively.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955153846154">
One of the first challenges in sentiment analysis
is the vast lexical diversity of subjective language.
Gaps in lexical coverage will be a problem for any
sentiment classification algorithm that does not have
some way of intelligently guessing the polarity of
unknown words. The problem is exacerbated further
by misspellings of known words and POS-tagging
errors which are often difficult to distinguish from
genuinely unknown words. This study explores the
extent to which it is possible to categorise words
which present themselves as unknown, but which
may contain known components using morpholog-
ical, syllabic, and shallow parsing devices.
</bodyText>
<sectionHeader confidence="0.994223" genericHeader="introduction">
2 Morphosyllabic Modelling
</sectionHeader>
<bodyText confidence="0.999419333333334">
Our core sentiment lexicon contains 41109 entries
tagged with positive (+), neutral (N), or nega-
tive (-) prior polarities (e.g. lovely(+), vast(N),
murder(-)) across all word classes. Polarity rever-
sal lexemes are tagged as [�I (e.g. never(N)[-J). We
furthermore maintain an auxiliary lexicon of 314967
known neutral words such as names of people, or-
ganisations, and geographical locations.
Each unknown word is run through a series of
sentiment indicator tests that aim at identifying in it
at least one possible sentiment stem - the longest
subpart of the word with a known (+), (N), or
(-) prior polarity. An unknown word such as
healthcare-related(?) can be traced back to the stems
health(N)(+), care(+), healthcare(+), or relate(d)(N)
which are all more likely to be found in the lexica,
for example. Note that the term ‘stem’ here does not
have its usual linguistic meaning but rather means
‘known labelled form’, whether complex or not.
We employ a classifier society of five rule-driven
classifiers that require no training data. Each classi-
fier adopts a specific analytical strategy within a spe-
cific window inside the unknown word, and outputs
three separate polarity scores based on the number
of stems founds (Spos, Snt,, Sneg) (initially 1). The
score for polarity p for unknown word w is calcu-
lated as follows:
</bodyText>
<equation confidence="0.9173525">
Ls 1 Sp
(1) scr(p) = �p Lw Sw Spos + Snt, + Sneg
</equation>
<bodyText confidence="0.883932666666667">
where 4bp = polarity coefficient (default 1)
Ls = # of characters in the stem
Lw = # of characters in w
Sw = # of punctuation splits in w
Polarity coefficients balance the stem counts: in par-
ticular, (N) polarity is suppressed by a 4&apos;nt, of &lt; 1
</bodyText>
<page confidence="0.988055">
109
</page>
<reference confidence="0.224232">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 109–112,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999467462365591">
because (N) stem counts dominate in the vast major-
ity of cases. L3 reflects differing degrees of reliabil-
ity between short and long stems in order to favour
the latter. S,,, targets the increased ambiguity poten-
tial in longer punctuated constructs. The highest-
scoring polarity across the three polarity scores from
each of the five classifiers is assigned to w.
Conversion [A]. It is generally beneficial to im-
pose word class polarity constraints in the lexicon
(e.g. [smart](+) ADJ vs. [smart](-) V). Due to cre-
ative lexical conversion across word classes, hard
constraints can however become counterproductive.
The first classifier estimates zero-derived paronyms
by retagging the unknown word with different POS
tags and requerying the lexica.
Morphological Derivation [B]. The second clas-
sifier relies on regular derivational (e.g. -ism, -ify,
-esque) and inflectional (e.g. -est, -s) morphology.
The unknown word is transformed incrementally
into shorter paronymic aliases using pure affixes and
(pseudo and neo-classical) combining forms. A re-
cursive derivation table of find/replace pairs is used
to model individual affixes and their regular spelling
alternations (e.g. -pping&gt;p; -ation&gt;e; -iness&gt;y;
-some&gt;Ø; re-&gt;Ø). Polarity reversal affixes such
as -less(N)[,] and not-so-(N)[¬] are supported. The
table is traversed until a non-neutral sentiment
(NB. not morphological) stem is found. Prefixes
are matched first. Note that the prefix-driven
configuration we have adopted is an approximation
to a (theoretically) full morphemic parse. The
derivation for antirationalistic(?), for example, first
matches the prefix anti-(N)[¬], and then truncates the
immediate constituent rationalistic(?) incrementally
until a sentiment stem (e.g. rational(N)(+)) is
encountered. The polarity reversal prefix anti-(N)[¬]
then reverses the polarity of the stem: hence,
antirationalistic(?)&gt;rationalistic(?)&gt;rationalist(?)&gt;
rational(+)&gt;antirationalistic(-). 322 (N) and 67
[�] prefixes, and 174 (N) and 28 [�] suffixes
were used.
Affix-like Polarity Markers [C]. Beyond
the realm of pure morphemes, many non-
neutral sentiment markers exist. Examples
include prefix-like elements in well-built(+),
badly-behaving(-), and strange-looking(-); and
suffix-like ones in rat-infested(-), burglarproof(+),
and fruit-loving(+). Because the polarity of a
non-neutral marker commonly dominates over its
host, the marker propagates its sentiment across the
entire word. Hence, a full-blown derivation is not
required (e.g. easy-to-install(?)&gt;easy-to-install(+);
necrophobia(?)&gt;necrophobia(-)). We experimented
with 756 productive prefixes and 640 suffixes
derived from hyphenated tokens with a frequency
of &gt; 20 amongst 406253 words mined from
the WAC 2006 corpus1. Sentiment markers are
captured through simple regular expression-based
longest-first matching.
Syllables [D]. We next split unknown words into
individual syllables based on syllabic onset, nucleus,
and coda boundaries obtained from our own rule-
based syllable chunker. Starting with the longest,
the resultant monosyllabic and permutative order-
preserving polysyllabic words are used as aliases
to search the lexica. Aliases not found in our lex-
ica are treated as (N). Consider the unknown word
freedomfortibet(?). In the syllabified set of singular
syllables {free, dom, for, ti, bet} and combinatory
permutations such as {free.dom, dom.ti, for.ti.bet,
... }, free or free.dom are identified as (+) while all
others become (N). Depending on the 4b,,,t, value,
free.dom.for.ti.bet(?) can then be tagged as (+) due
to the (+) stem(s). Note that cruder substring-based
methods can always be used instead. However, a syl-
labic approach shrinks the search space and ensures
the phonotactic well-formedness of the aliases.
Shallow Parsing [E]. At a deepest level, we
approximate the internal quasi-syntactic structure
of unknown words that can be split based on
various punctuation characters. Both exotic phrasal
nonce forms (e.g. kill-the-monster-if-it’s-green-
and-ugly(-)) and simpler punctuated compounds
(e.g. butt-ugly(-), girl-friend(+)) follow observable
syntactic hierarchies amongst their subconstituents.
Similar rankings can be postulated for sentiment.
Since not all constituents are of equal importance,
the sentiment salience of each subconstituent
is estimated using a subset of the grammatical
polarity rankings and compositional processes
proposed in Moilanen and Pulman (2007). The
unknown word is split into a virtual sentence
and POS-tagged2. The rightmost subconstituent
</bodyText>
<footnote confidence="0.800969333333333">
1Fletcher, W. H. (2007). English Web Corpus 2006. www.
webascorpus.org/searchwc.html
2Connexor Machinese Syntax 3.8. www.connexor.com
</footnote>
<page confidence="0.997143">
110
</page>
<tableCaption confidence="0.999776">
Table 1: Average (A)ccuracy, kappa, and error distribution against ANN-2 and ANN-3
</tableCaption>
<table confidence="0.999568">
ALL POL NON-NTR LAZY ERROR DISTRIBUTION
Classifier 4ntr A k A k A FATAL GREEDY LAZY
[A] CONVERSION .2 76.70 .03 96.88 .94 99.53 0.08 2.47 97.44
[B] DERIVATION .8 74.15 .11 80.05 .59 93.90 2.81 22.86 74.33
[C] AFFIX MARKERS .2 72.33 .21 77.93 .55 88.05 6.10 39.07 54.83
[D] SYLLABLES .8 69.55 .23 71.88 .45 82.75 9.37 48.62 42.01
[E] PARSING .7 64.33 .25 79.09 .59 73.50 9.03 65.40 25.57
ALL 63.20 .28 80.61 .61 70.20 9.49 71.41 19.10
ALL UNSURE 64.60 .28 82.19 .64 69.71 7.43 77.95 14.62
</table>
<bodyText confidence="0.996329636363636">
in the word is expanded incrementally leftwards
by combining it with its left neighbour until the
whole word has been analysed. At each step, the
sentiment grammar in idem. controls (i) non-neutral
sentiment propagation and (ii) polarity conflict
resolution to calculate a global polarity for the
current composite construct. The unknown word
help-children-in-distress(?) follows the sequence
N:[distress(-)](-)DPP:[in(N)distress(-)](-)DNP:[child-
ren(N)[in distress](-)](-)DVP:[help(+)[children in
distress](-)](+), and is thus tagged as (+).
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999961363636364">
We compiled a dataset of 2000 infrequent words
containing hapax legomena from the BNC3 and
“junk” entries from the WAC 2006 corpus (Foot-
note 1). The dataset contains simple, medium-
complexity, and extreme complex cases cover-
ing single words, (non-)hyphenated compounds,
nonce forms, and spelling anomalies (e.g. anti-
neo-nazi-initiatives, funny-because-its-true, and
s’gonnacostyaguvna). Three human annotators clas-
sified the entries as (+), (-), or (N) (with an op-
tional UNSURE tag) with the following distribution:
</bodyText>
<table confidence="0.60084075">
Human (+) (N) (-) UNSURE
ANN-1 24.55 53.45 22 11.75
ANN-2 12.60 68.60 18.80 10.85
ANN-3 5.25 84.55 10.20 0.65
</table>
<bodyText confidence="0.92306903030303">
We report results using all polarities (ALL-POL)
and non-neutral polarities (NON-NTR) resulting in
average pairwise inter-annotator Kappa scores of
3Kilgarriff, A. (1995). BNC database and word frequency
lists. www.kilgarriff.co.uk/bnc-readme.html
.40 (ALL-POL) and .74 (NON-NTR), or .48 (ALL-
POL) and .83 (NON-NTR) without UNSURE cases.
We used ANN-1’s data to adjust the 4&apos;ntr coefficients
of individual classifiers, and evaluated the system
against both ANN-2 and ANN-3. The average scores
between ANN-2 and ANN-3 are given in Table 1.
Since even human polarity judgements become
fuzzier near the neutral/non-neutral boundary due
to differing personal degrees of sensitivity towards
neutrality (cf. low (N) agreement in Ex. 2; An-
dreevskaia and Bergler (2006)), not all classification
errors are equal for classifying a (+) case as (N)
is more tolerable than classifying it as (-), for ex-
ample. We therefore found it useful to characterise
three distinct disagreement classes between human
H and machine M encompassing FATAL (H(+)M(-)
or H(-)M(+)), GREEDY (H(N)M(-) or H(N)M(+)), and
LAZY (H(+)M(N) or H(-)M(N)) cases.
The classifiers generally mimic human judge-
ments in that accuracy is much lower in the three-
way classification task - a pattern concurring with
past observations (cf. Esuli and Sebastiani (2006);
Andreevskaia and Bergler (2006)). Crucially, FA-
TAL errors remain below 10% throughout. Further
advances can be made by fine-tuning the 4&apos;ntr coef-
ficients, and by learning weights for individual clas-
sifiers which can currently mask each other and sup-
press the correct analysis when run collectively.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99693525">
Past research in Sentiment Tagging (cf. Opinion
Mining, Sentiment Extraction) has targeted classifi-
cation along the subjectivity, sentiment polarity, and
strength/degree dimensions towards a common goal
</bodyText>
<page confidence="0.6664295">
(2)
111
</page>
<bodyText confidence="0.999899884615385">
of (semi-)automatic compilation of sentiment lexica.
The utility of word-internal sentiment clues has not
yet been explored in the area, to our knowledge.
Lexicographic Methods. Static dictionary-
/thesaurus-based methods rely on the lexical-
semantic knowledge and glosses in existing lexi-
cographic resources alongside known non-neutral
seed words. The semi-supervised learning method
in Esuli and Sebastiani (2005) involves constructing
a training set of non-neutral words using WordNet
synsets, glosses and examples by iteratively adding
syn- and antonyms to it and learning a term classifier
on the glosses of the terms in the training set. Esuli
and Sebastiani (2006) used the method to cover ob-
jective (N) cases. Kamps et al. (2004) developed a
graph-theoretic model of WordNet’s synonymy rela-
tions to determine the polarity of adjectives based on
their distance to words indicative of subjective eval-
uation, potency, and activity dimensions. Takamura
et al. (2005) apply to words’ polarities a physical
spin model inspired by the behaviour of electrons
with a (+) or (-) direction, and an iterative term-
neighbourhood matrix which models magnetisation.
Non-neutral adjectives were extracted from Word-
Net and assigned fuzzy sentiment category member-
ship/centrality scores and tags in Andreevskaia and
Bergler (2006).
Corpus-based Methods. Lexicographic methods
are necessarily confined within the underlying re-
sources. Much greater coverage can be had with
syntactic or co-occurrence patterns across large cor-
pora. Hatzivassiloglou and McKeown (1997) clus-
tered adjectives into (+) and (-) sets based on con-
junction constructions, weighted similarity graphs,
minimum-cuts, supervised learning, and clustering.
A popular, more general unsupervised method was
introduced in Turney and Littman (2003) which in-
duces the polarity of a word from its Pointwise Mu-
tual Information (PMI) or Latent Semantic Analy-
sis (LSA) scores obtained from a web search en-
gine against a few paradigmatic (+) and (-) seeds.
Kaji and Kitsuregawa (2007) describe a method
for harvesting sentiment words from non-neutral
sentences extracted from Japanese web documents
based on structural layout clues. Strong adjecti-
val subjectivity clues were mined in Wiebe (2000)
with a distributional similarity-based word clus-
tering method seeded by hand-labelled annotation.
Riloff et al. (2003) mined subjective nouns from
unannotated texts with two bootstrapping algorithms
that exploit lexico-syntactic extraction patterns and
manually-selected subjective seeds.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999725">
In this study of unknown words in the domain
of sentiment analysis, we presented five methods
for guessing the prior polarities of unknown words
based on known sentiment carriers. The evaluation
results, which mirror human sentiment judgements,
indicate that the methods can account for many un-
known words, and that over- and insensitivity to-
wards neutral polarity is the main source of errors.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999918885714285">
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for Fuzzy Sentiment: Sentiment Tag Extrac-
tion from WordNet Glosses. In Proceedings of EACL
2006.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the Semantic Orientation of Terms through Gloss
Classification. In Proceedings of CIKM 2005.
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing Term Subjectivity and Term Orientation for Opin-
ion Mining. In Proceedings of EACL 2006.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. In Proceedings of ACL 1997.
Jaap Kamps, Maarten Marx, Robert J. Mokken and
Maarten de Rijke 2004. Using WordNet to Measure
Semantic Orientations of Adjectives. In Proceedings
of LREC 2004.
Nabuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. In Proceedings of
EMNLP-CoNLL 2007.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of RANLP 2007.
Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. In Proceedings of CoNLL 2003.
Hiroya Takamura, Takashi Inui and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL 2005.
Peter Turney and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orienta-
tion from Association. ACM Transactions on Infor-
mation Systems, October, 21(4): 315–46.
Janyce Wiebe. 2000. Learning Subjective Adjectives
from Corpora. In Proceedings of AAAI 2000.
</reference>
<page confidence="0.998294">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814852">
<title confidence="0.984418">The Good, the Bad, and the Unknown: Morphosyllabic Sentiment Tagging of Unseen Words</title>
<author confidence="0.999688">Karo Moilanen</author>
<author confidence="0.999688">Stephen Pulman</author>
<affiliation confidence="0.999997">Oxford University Computing Laboratory</affiliation>
<address confidence="0.980689">Wolfson Building, Parks Road, Oxford, OX1 3QD, England</address>
<abstract confidence="0.990906625">The omnipresence of unknown words is a problem that any NLP component needs to address in some form. While there exist many established techniques for dealing with unknown words in the realm of POS-tagging, for example, guessing unknown words’ semantic properties is a less-explored area with greater challenges. In this paper, we study the semantic field of sentiment and propose five methods for assigning prior sentiment polarities to unknown words based on known sentiment carriers. Tested on 2000 cases, the methods mirror human judgements closely in threeand twoway polarity classification tasks, and reach accuracies above 63% and 81%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>109--112</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 109–112,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining WordNet for Fuzzy Sentiment: Sentiment Tag Extraction from WordNet Glosses. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the Semantic Orientation of Terms through Gloss Classification.</title>
<date>2005</date>
<booktitle>In Proceedings of CIKM</booktitle>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the Semantic Orientation of Terms through Gloss Classification. In Proceedings of CIKM 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining Term Subjectivity and Term Orientation for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Determining Term Subjectivity and Term Orientation for Opinion Mining. In Proceedings of EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the Semantic Orientation of Adjectives. In Proceedings of ACL 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>J Robert</author>
</authors>
<title>Mokken and Maarten de Rijke</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Kamps, Marx, Robert, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken and Maarten de Rijke 2004. Using WordNet to Measure Semantic Orientations of Adjectives. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nabuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nabuhiro Kaji and Masaru Kitsuregawa. 2007. Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents. In Proceedings of EMNLP-CoNLL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment Composition.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP</booktitle>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment Composition. In Proceedings of RANLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
</authors>
<title>Learning Subjective Nouns using Extraction Pattern Bootstrapping.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Ellen Riloff, Janyce Wiebe and Theresa Wilson. 2003. Learning Subjective Nouns using Extraction Pattern Bootstrapping. In Proceedings of CoNLL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
</authors>
<title>Takashi Inui and Manabu Okumura.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Takamura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring Praise and Criticism: Inference of Semantic Orientation from Association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems, October,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>315--46</pages>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring Praise and Criticism: Inference of Semantic Orientation from Association. ACM Transactions on Information Systems, October, 21(4): 315–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning Subjective Adjectives from Corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI</booktitle>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning Subjective Adjectives from Corpora. In Proceedings of AAAI 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>