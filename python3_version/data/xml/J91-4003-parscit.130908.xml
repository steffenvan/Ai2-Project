<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992208">
The Generative Lexicon
</title>
<author confidence="0.997543">
James Pustejovsky
</author>
<affiliation confidence="0.934267">
Computer Science Department
Brandeis University
</affiliation>
<bodyText confidence="0.9977944375">
In this paper, I will discuss four major topics relating to current research in lexical seman-
tics: methodology, descriptive coverage, adequacy of the representation, and the computational
usefulness of representations. In addressing these issues, I will discuss what I think are some
of the central problems facing the lexical semantics community, and suggest ways of best ap-
proaching these issues. Then, I will provide a method for the decomposition of lexical categories
and outline a theory of lexical semantics embodying a notion of cocompositionality and type
coercion, as well as several levels of semantic description, where the semantic load is spread
more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is per-
formed generatively. Rather than assuming a fixed set of primitives, I will assume a fixed
number of generative devices that can be seen as constructing semantic expressions. I develop
a theory of Qualia Structure, a representation language for lexical items, which renders much
lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy
that words carry. Finally, I discuss how individual lexical structures can be integrated into the
larger lexical knowledge base through a theory of lexical inheritance. This provides us with
the necessary principles of global organization for the lexicon, enabling us to fully integrate
our natural language lexicon into a conceptual whole.
</bodyText>
<sectionHeader confidence="0.967586" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999821153846154">
I believe we have reached an interesting turning point in research, where linguistic
studies can be informed by computational tools for lexicology as well as an appre-
ciation of the computational complexity of large lexical databases. Likewise, com-
putational research can profit from an awareness of the grammatical and syntactic
distinctions of lexical items; natural language processing systems must account for
these differences in their lexicons and grammars. The wedding of these disciplines is
so important, in fact, that I believe it will soon be difficult to carry out serious com-
putational research in the fields of linguistics and NLP without the help of electronic
dictionaries and computational lexicographic resources (cf. Walker et al. [forthcoming]
and Boguraev and Briscoe [1988]). Positioned at the center of this synthesis is the study
of word meaning, lexical semantics, which is currently witnessing a revival.
In order to achieve a synthesis of lexical semantics and NLP, I believe that the
lexical semantics community should address the following questions:
</bodyText>
<listItem confidence="0.971245">
1. Has recent work in lexical semantics been methodologically sounder
than the previous work in the field?
2. Do theories being developed today have broader coverage than the
earlier descriptive work?
</listItem>
<footnote confidence="0.527708">
* Waltham, MA 02254
</footnote>
<note confidence="0.5439305">
C) 1991 Association for Computational Linguistics
Computational Linguistics Volume 17, Number 4
</note>
<listItem confidence="0.9871585">
3. Do current theories provide any new insights into the representation of
knowledge for the global structure of the lexicon?
4. Finally, has recent work provided the computational community with
useful resources for parsing, generation, and translation research?
</listItem>
<bodyText confidence="0.999614925">
Before addressing these questions, I would like to establish two basic assumptions
that will figure prominently in my suggestions for a lexical semantics framework.
The first is that, without an appreciation of the syntactic structure of a language,
the study of lexical semantics is bound to fail. There is no way in which meaning
can be completely divorced from the structure that carries it. This is an important
methodological point, since grammatical distinctions are a useful metric in evaluating
competing semantic theories.
The second point is that the meanings of words should somehow reflect the deeper,
conceptual structures in the system and the domain it operates in. This is tantamount
to stating that the semantics of natural language should be the image of nonlinguistic
conceptual organizing principles (whatever their structure).
Computational lexical semantics should be guided by the following principles.
First, a clear notion of semantic well-formedness will be necessary to characterize a
theory of possible word meaning. This may entail abstracting the notion of lexical
meaning away from other semantic influences. For instance, this might suggest that
discourse and pragmatic factors should be handled differently or separately from the
semantic contributions of lexical items in composition.&apos; Although this is not a necessary
assumption and may in fact be wrong, it may help narrow our focus on what is
important for lexical semantic descriptions.
Secondly, lexical semantics must look for representations that are richer than the-
matic role descriptions (Gruber 1965; Fillmore 1968). As argued in Levin and Rap-
paport (1986), named roles are useful at best for establishing fairly general mapping
strategies to the syntactic structures in language. The distinctions possible with theta-
roles are much too coarse-grained to provide a useful semantic interpretation of a
sentence. What is needed, I will argue, is a principled method of lexical decomposi-
tion. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic
composition, (2) the notion of semantic well-formedness mentioned above, and (3) an
appeal to several levels of interpretation in the semantics (Scha 1983).
Thirdly, and related to the point above, the lexicon is not just verbs. Recent work
has done much to clarify the nature of verb classes and the syntactic constructions
that each allows (Levin 1985, 1989). Yet it is not clear whether we are any closer to
understanding the underlying nature of verb meaning, why the classes develop as
they do, and what consequences these distinctions have for the rest of the lexicon
and grammar. The curious thing is that there has been little attention paid to the other
lexical categories (but see Miller and Johnson-Laird [1976], Miller and Fellbaum [1991],
and Fass [1988]). That is, we have little insight into the semantic nature of adjectival
predication, and even less into the semantics of nominals. Not until all major categories
have been studied can we hope to arrive at a balanced understanding of the lexicon
and the methods of composition.
Stepping back from the lexicon for a moment, let me say briefly what I think the
</bodyText>
<footnote confidence="0.9965506">
1 This is still a contentious point and is an issue that is not at all resolved in the community. Hobbs
(1987) and Wilensky (1990), for example, argue that there should be no distinction between
commonsense knowledge and lexical knowledge. Nevertheless, I will suggest below that there are
good reasons, both methodological and empirical, for establishing just such a division. Pustejovsky and
Bergler (1991) contains a good survey on how this issue is addressed by the community.
</footnote>
<page confidence="0.992517">
410
</page>
<note confidence="0.880691">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.999960217391304">
position of lexical research should be within the larger semantic picture. Ever since
the earliest attempts at real text understanding, a major problem has been that of con-
trolling the inferences associated with the interpretation process. In other words, how
deep or shallow is the understanding of a text? What is the unit of well-formedness
when doing natural language understanding; the sentence, utterance, paragraph, or
discourse? There is no easy answer to this question because, except for the sentence,
these terms are not even formalizable in a way that most researchers would agree on.
It is my opinion that the representation of the context of an utterance should be
viewed as involving many different generative factors that account for the way that
language users create and manipulate the context under constraints, in order to be
understood. Within such a theory, where many separate semantic levels (e.g. lexical
semantics, compositional semantics, discourse structure, temporal structure) have in-
dependent interpretations, the global interpretation of a &amp;quot;discourse&amp;quot; is a highly flexible
and malleable structure that has no single interpretation. The individual sources of se-
mantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs
et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences
must be globally coherent, a state that is accomplished by processes of cooperation
among separate semantic modules. The basic result of such a view is that semantic
interpretation proceeds in a principled fashion, always aware of what the source of a
particular inference is, and what the certainty of its value is. Such an approach allows
the reasoning process to be both tractable and computationally efficient. The repre-
sentation of lexical semantics, therefore, should be seen as just one of many levels in
a richer characterization of contextual structure.
</bodyText>
<sectionHeader confidence="0.941436" genericHeader="keywords">
2. Methods in Lexical Semantics
</sectionHeader>
<bodyText confidence="0.9887782">
Given what I have said, let us examine the questions presented above in more detail.
First, let us turn to the issue of methodology. How can we determine the soundness
of our method? Are new techniques available now that have not been adequately
explored? Very briefly, one can summarize the most essential techniques assumed by
the field, in some way, as follows (see, for example Cruse [1986]):
</bodyText>
<listItem confidence="0.857375933333334">
• On the basis of categorial distinctions, establish the fundamental
differences between the grammatical classes; the typical semantic
behavior of a word of category X. For example, verbs typically behave as
predicators, nouns as arguments.
• Find distinctions between elements of the same word class on the basis
of collocation and cooccurrence tests. For example, the nouns dog and
book partition into different selectional classes because of contexts
involving animacy, while the nouns book and literature partition into
different selectional classes because of a mass/count distinction.
• Test for distinctions of a grammatical nature on the basis of diathesis; i.e.
alternations that are realized in the syntax. For example, break vs. cut in
(1) and (2) below (Fillmore 1968; Lakoff 1970; Hale and Keyser 1986):
Example 1
a. The glass broke.
b. John broke the glass.
</listItem>
<page confidence="0.991672">
411
</page>
<figure confidence="0.67063375">
Computational Linguistics Volume 17, Number 4
Example 2
a. *The bread cut.
b. John cut the bread.
</figure>
<bodyText confidence="0.983598">
Such alternations reveal subtle distinctions in the semantic and syntactic
behavior of such verbs. The lexical semantic representations of these
verbs are distinguishable on the basis of such tests.
</bodyText>
<listItem confidence="0.76989775">
• Test for entailments in the word senses of a lexical item, in different
grammatical contexts. One can distinguish, for example, between
context-free and context-sensitive entailments. When the use of a word
always entails a certain proposition, we say that the resulting entailment
</listItem>
<bodyText confidence="0.80850075">
is not dependent on the syntactic context (cf. Katz and Fodor 1963;
Karttunen 1971, 1974; Seuren 1985). This is illustrated in Example 3,
where a killing always entails a dying.
Example 3
</bodyText>
<listItem confidence="0.48655">
a. John killed Bill.
b. Bill died.
</listItem>
<bodyText confidence="0.8380625">
When the same lexical item may carry different entailments in different
contexts, we say that the entailments are sensitive to the syntactic
contexts; for example, forget in Example 4,
Example 4
a. John forgot that he locked the door.
b. John forgot to lock the door.
Example 4a has a factive interpretation of forget that 4b does not carry: in
fact, 4b is counterfactive. Other cases of contextual specification involve
aspectual verbs such as begin and finish as shown in Example 5.
Example 5
</bodyText>
<listItem confidence="0.6298835">
a. Mary finished the cigarette.
b. Mary finished her beer.
</listItem>
<bodyText confidence="0.956834">
The exact meaning of the verb finish varies depending on the object it
selects, assuming for these examples the meanings finish smoking or finish
drinking.
</bodyText>
<listItem confidence="0.8716685">
• Test for the ambiguity of a word. Distinguish between homonymy and
polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and
logical aspects of ambiguity. For example, the homonymy between the
two senses of bank in Example 6 is accidental.&apos;
</listItem>
<footnote confidence="0.736178">
Example 6
a. the bank of the river
b. the richest bank in the city
2 Cf. Weinreich (1972) distinguishes between contrastive and complementary polysemy, essentially
covering this same distinction. See Section 4 for discussion.
</footnote>
<page confidence="0.981863">
412
</page>
<figure confidence="0.642547545454546">
James Pustejovsky The Generative Lexicon
In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich
1972; Lakoff 1987).
Example 7
a. The bank raised its interest rates yesterday (i.e. the institution).
b. The store is next to the new bank (i.e. the building).
• Establish what the compositional nature of a lexical item is when applied
to other words. For example, alleged vs. female in Example 8.
Example 8
a. the alleged suspect
b. the female suspect
</figure>
<bodyText confidence="0.900857833333333">
While female behaves as a simple intersective modifier in 8b, certain
modifiers such as alleged in 8a cannot be treated as simple attributes;
rather, they create an intensional context for the head they modify. An
even more difficult problem for compositionality arises from phrases
containing frequency adjectives (cf. Stump 1981), as shown in 8c and 8d.
Example 8
</bodyText>
<listItem confidence="0.972512">
c. An occasional sailor walks by on the weekend.
d. Caution: may contain an occasional pit (notice on a box of prunes).
</listItem>
<bodyText confidence="0.9999516">
The challenge here is that the adjective doesn&apos;t modify the nominal head,
but the entire proposition containing it (cf. Partee [1985] for discussion).
A similar difficulty arises with the interpretation of scalar predicates
such as fast in Example 9. Both the scale and the relative interpretation
being selected for depends on the noun that the predicate is modifying.
</bodyText>
<subsectionHeader confidence="0.618986">
Example 9
</subsectionHeader>
<bodyText confidence="0.810052571428571">
a. a fast typist: one who types quickly
b. a fast car: one which can move quickly
c. a fast waltz: one with a fast tempo
Such data raise serious questions about the principles of compositionality
and how ambiguity should be accounted for by a theory of semantics.
This just briefly characterizes some of the techniques that have been useful for
arriving at pre-theoretic notions of word meaning. What has changed over the years
are not so much the methods themselves as the descriptive details provided by each
test. One thing that has changed, however — and this is significant — is the way
computational lexicography has provided stronger techniques and even new tools for
lexical semantics research: see Atkins (1987) for sense discrimination tasks; Amsler
(1985), Atkins et al. (forthcoming) for constructing concept taxonomies; Wilks et al.
(1988) for establishing semantic relatedness among word senses; and Boguraev and
Pustejovsky (forthcoming) for testing new ideas about semantic representations.
</bodyText>
<sectionHeader confidence="0.458129" genericHeader="introduction">
3. Descriptive Adequacy of Existing Representations
</sectionHeader>
<bodyText confidence="0.992935">
Turning now to the question of how current theories compare with the coverage of
lexical semantic data, there are two generalizations that should be made. First, the
</bodyText>
<page confidence="0.99646">
413
</page>
<note confidence="0.300962">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.999362">
taxonomic descriptions that have recently been made of verb classes are far superior
to the classifications available twenty years ago (see Levin [1985] for review). Using
mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and
subtle distinctions are drawn that were not captured in the earlier, primitives-based
approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968).
As an example of the verb classifications developed by various researchers (and
compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical
alternations in the example sentences below (cf. Dowty 1991).
</bodyText>
<figure confidence="0.484949333333333">
Example 10
a. John met Mary.
b. John and Mary met.
Example 11
a. A car ran into a truck.
b. A car and a truck ran into each other.
Example 12
a. A car ran into a tree.
b. *A car and a tree ran into each other.
</figure>
<bodyText confidence="0.974721625">
These three pairs show how the semantics of transitive motion verbs (e.g. run into)
is similar in some respects to reciprocal verbs such as meet. The important difference,
however, is that the reciprocal interpretation requires that both subject and object be
animate or moving; hence 12b is ill-formed. (cf. Levin 1989; Dowty 1991).
Another example of how diathesis reveals the underlying semantic differences
between verbs is illustrated in Examples 13 and 14 below. A construction called the
conative (see Hale and Keyser [1986] and Levin [1985]) involves adding the preposition
at to the verb, changing the verb meaning to an action directed toward an object.
</bodyText>
<subsectionHeader confidence="0.590412">
Example 13
</subsectionHeader>
<bodyText confidence="0.4302765">
a. Mary cut the bread.
b. Mary cut at the bread.
</bodyText>
<subsectionHeader confidence="0.494707">
Example 14
</subsectionHeader>
<bodyText confidence="0.855985571428571">
a. Mary broke the bread.
b. *Mary broke at the bread.
What these data indicate is that the conative is possible only with verbs of a particular
semantic class; namely, verbs that specify the manner of an action that results in a change
of state of an object.
As useful and informative as the research on verb classification is, there is a major
shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks
(1975a), and Quillian (1968), there is no general coherent view on what the entire lexi-
con will look like when semantic structures for other major categories are studied. This
can be essential for establishing a globally coherent theory of semantic representation.
On the other hand, the semantic distinctions captured by these older theories were
often too coarse-grained. It is clear, therefore, that the classifications made by Levin
and her colleagues are an important starting point for a serious theory of knowledge
representation. I claim that lexical semantics must build upon this research toward
</bodyText>
<page confidence="0.997501">
414
</page>
<note confidence="0.460062">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.893424">
constructing a theory of word meaning that is integrated into a linguistic theory, as
well as interpreted in a real knowledge representation system.
</bodyText>
<sectionHeader confidence="0.740512" genericHeader="method">
4. Explanatory Adequacy of Existing Representations
</sectionHeader>
<bodyText confidence="0.999969684210526">
In this section I turn to the question of whether current theories have changed the
way we look at representation and lexicon design. The question here is whether the
representations assumed by current theories are adequate to account for the richness
of natural language semantics. It should be pointed out here that a theory of lexical
meaning will affect the general design of our semantic theory in several ways. If
we view the goal of a semantic theory as being able to recursively assign meanings
to expressions, accounting for phenomena such as synonymy, antonymy, polysemy,
metonymy, etc., then our view of compositionality depends ultimately on what the
basic lexical categories of the language denote. Conventional wisdom on this point
paints a picture of words behaving as either active functors or passive arguments
(Montague 1974). But we will see that if we change the way in which categories can
denote, then the form of compositionality itself changes. Therefore, if done correctly,
lexical semantics can be a means to reevaluate the very nature of semantic composition
in language.
In what ways could lexical semantics affect the larger methods of composition in
semantics? I mentioned above that most of the careful representation work has been
done on verb classes. In fact, the semantic weight in both lexical and compositional
terms usually falls on the verb. This has obvious consequences for how to treat lexical
ambiguity. For example, consider the verb bake in the two sentences below.
</bodyText>
<subsectionHeader confidence="0.719679">
Example 15
</subsectionHeader>
<listItem confidence="0.6247095">
a. John baked the potato.
b. John baked the cake.
</listItem>
<bodyText confidence="0.9975924">
Atkins, Kegl, and Levin (1988) demonstrate that verbs such as bake are systematically
ambiguous, with both a change-of-state sense (15a) and a create sense (15b).
A similar ambiguity exists with verbs that allow the resulative construction, shown
in Examples 16 and 17, and discussed in Dowty (1979), Jackendoff (1983), and Levin
and Rapoport (1988).
</bodyText>
<subsectionHeader confidence="0.668506">
Example 16
</subsectionHeader>
<listItem confidence="0.6584146">
a. Mary hammered the metal.
b. Mary hammered the metal flat.
Example 17
a. John wiped the table.
b. John wiped the table clean.
</listItem>
<footnote confidence="0.5653482">
On many views, the verbs in Examples 16 and 17 are ambiguous, related by either
a lexical transformation (Levin and Rapoport 1988), or a meaning postulate (Dowty
1979). In fact, given strict requirements on the way that a verb can project its lexical
information, the verb run in Example 18 will also have two lexical entries, depending
on the syntactic environment it selects (Talmy 1985; Levin and Rappaport 1988).
</footnote>
<page confidence="0.997184">
415
</page>
<figure confidence="0.948888">
Computational Linguistics Volume 17, Number 4
Example 18
a. Mary ran to the store yesterday.
b. Mary ran yesterday.
</figure>
<bodyText confidence="0.99681675">
These two verbs differ in their semantic representations, where run in 18a means go-
to-by-means-of-running, while in 18b it means simply move-by-running (cf. Jackendoff
1983).
The methodology described above for distinguishing word senses is also assumed
by those working in more formal frameworks. For example, Dowty (1985) proposes
multiple entries for control and raising verbs, and establishes their semantic equiva-
lence with the use of meaning postulates. That is, the verbs in Examples 19 and 20 are
lexically distinct but semantically related by rules.3
</bodyText>
<figure confidence="0.7948045">
Example 19
a. It seems that John likes Mary.
b. John seems to like Mary.
Example 20
a. Mary prefers that she come.
b. Mary prefers to come.
</figure>
<bodyText confidence="0.999653">
Given the conventional notions of function application and composition, there is
little choice but to treat all of the above cases as polysemous verbs. Yet, something
about the systematicity of such ambiguity suggests that a more general and simpler
explanation should be possible. By relaxing the conditions on how the meaning of a
complex expression is derived from its parts, I will, in fact, propose a very straight-
forward explanation for these cases of logical polysemy.
</bodyText>
<sectionHeader confidence="0.909603" genericHeader="method">
5. A Framework for Computational Semantics
</sectionHeader>
<bodyText confidence="0.9982589375">
In this section, I will outline what I think are the basic requirements for a theory of
computational semantics. I will present a conservative approach to decomposition,
where lexical items are minimally decomposed into structured forms (or templates)
rather than sets of features. This will provide us with a generative framework for the
composition of lexical meanings, thereby defining the well-formedness conditions for
semantic expressions in a language.
We can distinguish between two distinct approaches to the study of word mean-
ing: primitive-based theories and relation-based theories. Those advocating primitives
assume that word meaning can be exhaustively defined in terms of a fixed set of
primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences
are made through the primitives into which a word is decomposed. In contrast to
this view, a relation-based theory of word meaning claims that there is no need for
decomposition into primitives if words (and their concepts) are associated through a
network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor
1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these
links establish any inference between words as an explicit part of a network of word
</bodyText>
<footnote confidence="0.944538">
3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating
these two forms structurally. See below and Pustejovsky (1989a) for details.
</footnote>
<page confidence="0.994083">
416
</page>
<note confidence="0.439104">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.997641727272727">
concepts.4 What I would like to do is to propose a new way of viewing primitives,
looking more at the generative or compositional aspects of lexical semantics, rather than
the decomposition into a specified number of primitives.
Most approaches to lexical semantics making use of primitives can be character-
ized as using some form of feature-based semantics, since the meaning of a word is
essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972;
Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for
word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characteri-
zation for all of the well-formed expressions in the language. Jackendoff (1983) comes
closest, but falls short of a comprehensive semantics for all categories in language.
No existing framework, in my view, provides a method for the decomposition of lexical
categories.
What exactly would a method for lexical decomposition give us? Instead of a
taxonomy of the concepts in a language, categorized by sets of features, such a method
would tell us the minimal semantic configuration of a lexical item. Furthermore, it
should tell us the compositional properties of a word, just as a grammar informs us
of the specific syntactic behavior of a certain category. What we are led to, therefore,
is a generative theory of word meaning, but one very different from the generative
semantics of the 1970s.
To explain why I am suggesting that lexical decomposition proceed in a generative
fashion rather than the traditional exhaustive approach, let me take as a classic example,
the word closed as used in Example 21 (see Lakoff 1970).
</bodyText>
<equation confidence="0.48643">
Example 21
</equation>
<bodyText confidence="0.940610428571429">
a. The door is closed.
b. The door closed.
c. John closed the door.
Lakoff (1970), Jackendoff (1972), and others have suggested that the sense in 21c must
incorporate something like cause-to-become-not-open for its meaning. Similarly, a verb
such as give specifies a transfer from one person to another, e.g., cause-to-have. Most
decomposition theories assume a set of primitives and then operate within this set
to capture the meanings of all the words in the language. These approaches can be
called exhaustive since they assume that with a fixed number of primitives, complete
definitions of lexical meaning can be given. In the sentences in 21, for example, close
is defined in terms of the negation of a primitive, open. Any method assuming a fixed
number of primitives, however, runs into some well-known problems with being able
to capture the full expressiveness of natural language.
These problems are not, however, endemic to all decomposition approaches. I
would like to suggest that lexical (and conceptual) decomposition is possible if it
is performed generatively. Rather than assuming a fixed set of primitives, let us as-
sume a fixed number of generative devices that can be seen as constructing semantic
expressions.&apos; Just as a formal language is described more in terms of the productions
in the grammar than its accompanying vocabulary, a semantic language is definable
by the rules generating the structures for expressions rather than the vocabulary of
primitives itself.6
</bodyText>
<footnote confidence="0.983408">
4 For further discussion on the advantages and disadvantages to both approaches, see Jackendoff (1983).
5 See Goodman (1951) and Chomsky (1955) for explanations of the method assumed here.
6 This approach is also better suited to the way people write systems in computational linguistics.
Different people have distinct primitives for their own domains, and rather than committing a designer
</footnote>
<page confidence="0.973155">
417
</page>
<note confidence="0.296142">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.999873454545455">
How might this be done? Consider the sentences in Example 21 again. A minimal
decomposition on the word closed is that it introduces an opposition of terms: closed
and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are
predicated of different subevents denoted by the sentences. In 21a, this opposition is
left implicit, since the sentence refers to a single state. Any minimal analysis of the
semantics of a lexical item can be termed a generative operation, since it operates on the
predicate(s) already literally provided by the word. This type of analysis is essentially
Aristotle&apos;s principle of opposition (cf. Lloyd 1968), and it will form the basis of one level
of representation for a lexical item. The essential opposition denoted by a predicate
forms part of what I will call the qualia structure of that lexical item. Briefly, the qualia
structure of a word specifies four aspects of its meaning:
</bodyText>
<listItem confidence="0.9999276">
• the relation between it and its constituent parts;
• that which distinguishes it within a larger domain (its physical
characteristics);
• its purpose and function;
• whatever brings it about.
</listItem>
<bodyText confidence="0.983936483870968">
I will call these aspects of a word&apos;s meaning its Constitutive Role, Formal Role, Telic Role,
and its Agentive Role, respectively.&apos;
This minimal semantic distinction is given expressive force when combined with
a theory of event types. For example, the predicate in 21a denotes the state of the door
being closed. No opposition is expressed by this predicate. In 21b and 21c, however,
the opposition is explicitly part of the meaning of the predicate. Both these predicates
denote what I will call transitions. The intransitive use of close in 21b makes no mention
of the causer, yet the transition from not-closed to closed is still entailed. In 2k, the event
that brings about the closed state of the door is made more explicit by specifying the
actor involved. These differences constitute what I call the event structure of a lexical
item. Both the opposition of predicates and the specification of causation are part of a
verb&apos;s semantics, and are structurally associated with slots in the event template for
the word. As we will see in the next section, there are different inferences associated
with each event type, as well as different syntactic behaviors (cf. Grimshaw 1990 and
Pustejovsky 1991).
Because the lexical semantic representation of a word is not an isolated expression,
but is in fact linked to the rest of the lexicon, in Section 7, I suggest how the global
integration of the semantics for a lexical item is achieved by structured inheritance
through the different qualia associated with a word. I call this the lexical inheritance
structure for the word.
Finally, we must realize that part of the meaning of a word is how it translates the
underlying semantic representations into expressions that are utilized by the syntax.
This is what many have called the argument structure for a lexical item. I will build on
Grimshaw&apos;s recent proposals (Grimshaw 1990) for how to define the mapping from
the lexicon to syntax.
to a particular vocabulary of primitives, a lexical semantics should provide a method for the
decomposition and composition of lexical items.
7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as
Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles
determine a minimal semantic description of a word that has both semantic and grammatical
consequences.
</bodyText>
<page confidence="0.987837">
418
</page>
<note confidence="0.489963">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.996291">
This provides us with an answer to the question of what levels of semantic rep-
resentation are necessary for a computational lexical semantics. In sum, I will argue
that lexical meaning can best be captured by assuming the following levels of repre-
sentation.
</bodyText>
<listItem confidence="0.9957291">
1. Argument Structure: The behavior of a word as a function, with its arity
specified. This is the predicate argument structure for a word, which
indicates how it maps to syntactic expressions.
2. Event Structure: Identification of the particular event type (in the sense
of Vendler [19671) for a word or phrase: e.g. as state, process, or
transition.
3. Qualia Structure: The essential attributes of an object as defined by the
lexical item.
4. Inheritance Structure: How the word is globally related to other
concepts in the lexicon.
</listItem>
<bodyText confidence="0.997807428571429">
These four structures essentially constitute the different levels of semantic expressive-
ness and representation that are needed for a computational theory of lexical semantics.
Each level contributes a different kind of information to the meaning of a word. The
important difference between this highly configurational approach to lexical semantics
and feature-based approaches is that the recursive calculus defined for word mean-
ing here also provides the foundation for a fully compositional semantics for natural
language and its interpretation into a knowledge representation model.
</bodyText>
<subsectionHeader confidence="0.981626">
5.1 Argument Structure
</subsectionHeader>
<bodyText confidence="0.999904266666667">
A logical starting point for our investigations into the meaning of words is what has
been called the functional structure or argument structure associated with verbs. What
originally began as the simple listing of the parameters or arguments associated with
a predicate has developed into a sophisticated view of the way arguments are mapped
onto syntactic expressions (for example, the f-structure in Lexical Functional Grammar
[Bresnan 1982] and the Projection Principle in Government-Binding Theory [Chomsky
1981]).
One of the most important contributions has been the view that argument structure
is highly structured independent of the syntax. Williams&apos;s (1981) distinction between
external and internal arguments and Grimshaw&apos;s proposal for a hierarchically struc-
tured representation (Grimshaw 1990) provide us with the basic syntax for one aspect
of a word&apos;s meaning.
The argument structure for a word can be seen as a minimal specification of
its lexical semantics. By itself, it is certainly inadequate for capturing the semantic
characterization of a lexical item, but it is a necessary component.
</bodyText>
<subsectionHeader confidence="0.987111">
5.2 Event Structure
</subsectionHeader>
<bodyText confidence="0.999896571428571">
As mentioned above, the theory of decomposition being outlined here is based on the
central idea that word meaning is highly structured, and not simply a set of semantic
features. Let us assume this is the case. Then the lexical items in a language will
essentially be generated by the recursive principles of our semantic theory. One level
of semantic description involves an event-based interpretation of a word or phrase.
I will call this level the event structure of a word (cf. Pustejovsky 1991; Moens and
Steedman 1988). The event structure of a word is one level of the semantic specification
</bodyText>
<page confidence="0.987121">
419
</page>
<note confidence="0.278685">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.999953444444444">
for a lexical item, along with its argument structure, qualia structure, and inheritance
structure. Because it is recursively defined on the syntax, it is also a property of phrases
and sentences.&apos;
I will assume a sortal distinction between three classes of events: states (es), pro-
cesses (eP), and transitions (eT ) . Unlike most previous sortal classifications for events,
I will adopt a subeventual analysis or predicates, as argued in Pustejovsky (1991) and
independently proposed in Croft (1991). In this view, an event sort such as eT may
be decomposed into two sequentially structured subevents, (e&amp;quot;, ss). Aspects of the
proposal will be introduced as needed in the following discussion.
</bodyText>
<sectionHeader confidence="0.891308" genericHeader="method">
6. A Theory of Qualia
</sectionHeader>
<bodyText confidence="0.9995575">
In Section 5, I demonstrated how most of the lexical semantics research has con-
centrated on verbal semantics. This bias influences our analyses of how to handle
ambiguity and certain noncompositional structures. Therefore, the only way to relate
the different senses for the verbs in the examples below was to posit separate entries.
</bodyText>
<equation confidence="0.750412636363636">
Example 22
a. John baked the potato.
(bakei -= change(x, State(y)))
b. John baked the cake.
(bake2 = create(x , y))
Example 23
a. Mary hammered the metal.
(hammeri = change(x, State(y)))
b. Mary hammered the metal flat.
(hammer2 = cause(x, Become(flat(Y))))
Example 24
</equation>
<listItem confidence="0.607617">
a. John wiped the table.
change(x, State(y)))
b. John wiped the table clean.
</listItem>
<equation confidence="0.847585">
(wipe2 = cause(x, Become(clean(y))))
Example 25
</equation>
<listItem confidence="0.8913135">
a. Mary ran yesterday.
(runi = move(x))
b. Mary ran to the store yesterday.
(run2 = go-to(x,y))
• Although the complement types selected by bake in 22, for example, are semantically
related, the two word senses are clearly distinct and therefore must be lexically distin-
guished. According to the sense enumeration view, the same argument holds for the
verbs in 23-25 as well.
</listItem>
<bodyText confidence="0.621921">
8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984).
For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual
distinctions are mapped to the syntax.
</bodyText>
<page confidence="0.985962">
420
</page>
<note confidence="0.48254">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.986504">
A similar philosophy has lead linguists to multiply word senses in constructions
involving Control and Equi-verbs, where different syntactic contexts necessitate dif-
ferent semantic types.&apos;
</bodyText>
<figure confidence="0.912862333333333">
Example 26
a. It seems that John likes Mary.
b. John seems to like Mary.
Example 27
a. Mary prefers that she come.
b. Mary prefers to come.
</figure>
<bodyText confidence="0.999378666666667">
Normally, compositionality in such structures simply refers to the application of the
functional element, the verb, to its arguments. Yet, such examples indicate that in
order to capture the systematicity of such ambiguity, something else is at play, where
a richer notion of composition is operative. What then accounts for the polysemy of
the verbs in the examples above?
The basic idea I will pursue is the following. Rather than treating the expressions
that behave as arguments to a function as simple, passive objects, imagine that they
are as active in the semantics as the verb itself. The product of function application
would be sensitive to both the function and its active argument. Something like this
is suggested in Keenan and Faltz (1985), as the Meaning—Form Correlation Principle. I
will refer to such behavior as cocompositionality (see below). What I have in mind can
best be illustrated by returning to the examples in 28.
</bodyText>
<subsectionHeader confidence="0.522876">
Example 28
</subsectionHeader>
<bodyText confidence="0.772684">
a. John baked the potato.
b. John baked the cake.
Rather than having two separate word senses for a verb such as bake, suppose there
is simply one, a change-of-state reading. Without going into the details of the analysis,
let us assume that bake can be lexically specified as denoting a process verb, and is
minimally represented as Example 29.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.847705">
Example 29
</subsectionHeader>
<bodyText confidence="0.842798">
Lexical Semantics for bake:11
</bodyText>
<subsubsectionHeader confidence="0.320428">
AyAxAeP [bake(eP) A agent (eP , x) A object (el&apos; , y)]
</subsubsectionHeader>
<bodyText confidence="0.857161692307692">
In order to explain the shift in meaning of the verb, we need to specify more clearly
what the lexical semantics of a noun is. I have argued above that lexical semantic theory
must make a logical distinction between the following qualia roles: the constitutive,
formal, telic, and agentive roles. Now let us examine these roles in more detail. One
can distinguish between potato and cake in terms of how they come about; the former
9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations.
Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting
operations to capture sense relatedness. We return to this topic below.
10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the
language are typed for a particular event-sort, and thematic roles are treated as partial functions over
the event (cf. Dowty 1989 and Chierchia 1989).
11 More precisely, the process el&apos; should reflect that it is the substance contained in the object x that is
affected. See footnote 20 for explanation.
</bodyText>
<page confidence="0.985683">
421
</page>
<note confidence="0.278662">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.9973565625">
is a natural kind, while the latter is an artifact. Knowledge of an object includes not
just being able to identify or refer, but more specifically, being able to explain how an
artifact comes into being, as well as what it is used for; the denotation of an object
must identify these roles. Thus, any artifact can be identified with the state of being
that object, relative to certain predicates.
As is well known from work on event semantics and Aktionsarten, it is a general
property of processes that they can shift their event type to become a transition event
(cf. Hinrichs 1985; Moens and Steedman 1987; and Krifka 1987). This particular fact
about event structures, together with the semantic distinction made above between
the two object types, provides us with an explanation for what I will refer to as the
logical polysemy of verbs such as bake.
As illustrated in Example 30a, when the verb takes as its complement a natural
kind such as potato, the resulting semantic interpretation is unchanged; i.e., a process
reading of a state-change. This is because the noun does not &amp;quot;project&amp;quot; an event struc-
ture of its own. That is, relative to the process of baking, potato does not denote an
event-type.12
</bodyText>
<figure confidence="0.760153833333333">
Example 30
a. bake as Process:
]eP[bake(eP) A agent(eP , j) A object(eP , a-potato)]
John baked a potato
V NP
VP
</figure>
<bodyText confidence="0.994616333333333">
What is it, then, about the semantics of cake that shifts this core meaning of bake
from a state-change predicate to its creation sense? As just suggested, this additional
meaning is contributed by specific lexical knowledge we have about artifacts, and
cake in particular; namely, there is an event associated with that object&apos;s &amp;quot;coming
into being,&amp;quot; in this case the process of baking. Thus, just as a verb can select for an
argument-type, we can imagine that an argument is itself able to select the predicates
that govern it. I will refer to such constructions as cospecifications. Informally, relative
to the process bake, the noun cake carries the selectional information that it is a process
of &amp;quot;baking&amp;quot; that brings it about.13
</bodyText>
<listItem confidence="0.63005675">
12 However, relative to the process of growing, the noun potato does denote an event:
1. Mary grew the potato.
13 Other examples of cospecifications are: a. read a book, b. smoke a cigarette, c. mail a letter, d. deliver a
lecture, and e. take a bath.
</listItem>
<page confidence="0.991147">
422
</page>
<note confidence="0.491382">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.999856428571429">
We can illustrate this schematically in Example 31, where the complement effec-
tively acts like a &amp;quot;stage-level&amp;quot; event predicate (cf. Carlson 1977) relative to the process
event-type of the verb (i.e. a function from processes to transitions, &lt;P , T&gt;).14 The
change in meaning in 31 comes not from the semantics of bake, but rather in com-
position with the complement of the verb, at the level of the entire verb phrase. The
&amp;quot;creation&amp;quot; sense arises from the semantic role of cake that specifies it is an artifact (see
below for discussion).
</bodyText>
<figure confidence="0.964559">
Example 31
a. bake as a derived Transition:15
2e&apos;, es [create (eP , es) A bake(eP) A agent(eP , j) A object(eP , x)
A cake(es) A object (es , x)]
&lt;P,T&gt;
John baked a cake
V NP
VP
</figure>
<bodyText confidence="0.960078846153846">
Thus, we can derive both word senses of verbs like bake by putting some of the
semantic weight on the NP. This view suggests that, in such cases, the verb itself is
not polysemous. Rather, the sense of &amp;quot;create&amp;quot; is part of the meaning of cake by virtue
of it being an artifact. The verb appears polysemous because certain complements add
to the basic meaning by virtue of what they denote. We return to this topic below,
There are several interesting things about such collocations. First, because the complement &amp;quot;selects&amp;quot;
the verb that governs it (by virtue of knowledge of what is done to the object), the semantics of the
phrase is changed. The semantic &amp;quot;connectedness,&amp;quot; as it were, is tighter when cospecification obtains. In
such cases, the verb is able to successfully drop the dative PP argument, as shown below in (1). When
the complement does not select the verb governing it, dative-drop is ungrammatical as seen in (2)
(although there are predicates selected by these nouns; e.g. keep a secret, read a book, and play a record).
Ia. Romeo gave the lecture.
b.Hamlet mailed a letter.
</bodyText>
<listItem confidence="0.764638333333333">
c. Cordelia told a story.
d.Gertnicle showed a movie.
e. Mary asked a question.
2a. *Bill told the secret.
b.*Mary gave a book.
c. *Cordelia showed the record.
</listItem>
<bodyText confidence="0.461004">
For discussion see Pustejovsky (in press).
</bodyText>
<page confidence="0.699868">
14 cf. Pustejovsky (forthcoming) for details.
15 As mentioned in footnote 11, this representation is incomplete. See footnote 20 for semantics of bake.
423
</page>
<note confidence="0.48262">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.99964575">
and provide a formal treatment for how the nominal semantics is expressed in these
examples.
Similar principles seem to be operating in the resultative constructions in Exam-
ples 23 and 24; namely, a systematic ambiguity is the result of principles of semantic
composition rather than lexical ambiguity of the verbs. For example, the resultative
interpretations for the verbs hammer in 23(b) and wipe in 24(b) arise from a similar
operation, where both verbs are underlyingly specified with an event type of pro-
cess. The adjectival phrases flat and clean, although clearly stative in nature, can also
be interpreted as stage-level event predicates (cf. Dowty 1979). Notice, then, how
the resultative construction requires no additional word sense for the verb, nor any
special semantic machinery for the resultative interpretation to be available. Schemat-
ically, this is shown in Example 32.
</bodyText>
<table confidence="0.98690125">
Example 32 John hammer &lt;P,T&gt;
the metal flat
NP AP
VP
</table>
<bodyText confidence="0.957753461538461">
In fact, this analysis explains why it is that only process verbs participate in the re-
sultative construction, and why the resultant phrase (the adjectival phrase) must be a
subset of the states, namely, stage-level event predicates. Because the meaning of the
sentence in 32 is determined by both function application of hammer to its arguments
and function application of flat to the event-type of the verb, this is an example of
cocompositionality (cf. Pustejovsky [forthcoming] for discussion).
Having discussed some of the behavior of logical polysemy in verbs, let us con-
tinue our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where
a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a prob-
lem for standard denotational theories of semantics. To see why, imagine how our
semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Ex-
ample 33.16
Example 33
</bodyText>
<listItem confidence="0.959643666666667">
a. Mary enjoyed the book.
b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987)
c. John began a novel.
</listItem>
<footnote confidence="0.942371">
16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy
and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of
metonymic resolution.
</footnote>
<page confidence="0.992382">
424
</page>
<note confidence="0.732002">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.9995155">
The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally
select for semantically, namely a property or action. Similarly, the verb veto normally
selects for an object that is a legislative bill or a suggestion. Syntactically, these may
simply be additional subcategorizations, but how are these examples related semanti-
cally to the normal interpretations?
I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a),
where the verb has coerced the meaning of a term phrase into a different semantic
type. Briefly, type coercion can be defined as follows:17
</bodyText>
<sectionHeader confidence="0.560292" genericHeader="method">
Definition
</sectionHeader>
<listItem confidence="0.871185185185185">
Type Coercion: A semantic operation that converts an argument to the type that is
expected by a function, where it would otherwise result in a type error.
In the case of 33(b), it is obvious that what is vetoed is some proposal relating to
the object denoted by the tunnel. In 33(a), the book is enjoyed only by virtue of some
event or process that involves the book, performed by Mary. It might furthermore be
reasonable to assume that the semantic structure of book specifies what the artifact is
used for; i.e. reading. Such a coercion results in a word sense for the NP that I will
call logical metonymy. Roughly, logical metonymy occurs when a logical argument (i.e.
subpart) of a semantic type that is selected by some function denotes the semantic
type itself.
Another interesting set of examples involves the possible subjects of causative
verbs.18 Consider the sentences in Examples 34 and 35.
Example 34
a. Driving a car in Boston frightens me.
b. To drive a car in Boston frightens me.
c. Driving frightens me.
d. John&apos;s driving frightens me.
e. Cars frighten me.
f. Listening to this music upsets me.
g. This music upsets me.
h. To listen to this music would upset me.
Example 35
a. John killed Mary.
b. The gun killed Mary.
c. John&apos;s stupidity killed Mary.
d. The war killed Mary.
e. John&apos;s pulling the trigger killed Mary.
</listItem>
<bodyText confidence="0.9992998">
As these examples illustrate, the syntactic argument to a verb is not always the same
logical argument in the semantic relation. Although superficially similar to cases of
general metonymy (cf. Lakoff and Johnson 1982; Nunberg 1978), there is an interesting
systematicity to such shifts in meaning that we will try to characterize below as logical
metonymy.
</bodyText>
<page confidence="0.788025">
17 I am following Cardelli and Wegener (1985) and their characterization of polymorphistnic behavior.
</page>
<note confidence="0.638474">
18 See Verma and Mohanan (1991) for an extensive survey of experiencer subject constructions in different
languages.
</note>
<page confidence="0.991896">
425
</page>
<note confidence="0.483404">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.999709875">
The sentences in 34 illustrate the various syntactic consequences of metonymy and
coercion involving experiencer verbs, while those in 35 show the different metonymic
extensions possible from the causing event in a killing. The generalization here is that
when a verb selects an event as one of its arguments, type coercion to an event will per-
mit a limited range of logical metonymies. For example, in sentences 34(a,b,c,d,f,h), the
entire event is directly referred to, while in 34(e,g) only a participant from the coerced
event reading is directly expressed. Other examples of coercion include &amp;quot;concealed
questions&amp;quot; 36 and &amp;quot;concealed exclamations&amp;quot; 37 (cf. Grimshaw 1979; Elliott 1974).
</bodyText>
<subsectionHeader confidence="0.425935">
Example 36
</subsectionHeader>
<listItem confidence="0.847678555555556">
a. John knows the plane&apos;s arrival time.
(= what time the plane will arrive)
b. Bill figured out the answer.
(= what the answer is)
Example 37
a. John shocked me with his bad behavior.
(= how bad his behavior is)
b. You&apos;d be surprised at the big cars he buys.
(= how big the cars he buys are)
</listItem>
<bodyText confidence="0.99284125">
That is, although the italicized phrases syntactically appear as NPs, their semantics is
the same as if the verbs had selected an overt question or exclamation.
In explaining the behavior of the systematic ambiguity above, I made reference
to properties of the noun phrase that are not typical semantic properties for nouns
in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and
Anick (1988), I suggest that there is a system of relations that characterizes the seman-
tics of nominals, very much like the argument structure of a verb. I called this the
Qualia Structure, inspired by Aristotle&apos;s theory of explanation and ideas from Moravc-
sik (1975). Essentially, the qualia structure of a noun determines its meaning as much
as the list of arguments determines a verb&apos;s meaning. The elements that make up a
qualia structure include notions such as container, space, surface, figure, artifact, and
so on.&amp;quot;
As stated earlier, there are four basic roles that constitute the qualia structure for
a lexical item. Here I will elaborate on what these roles are and why they are useful.
They are given in Example 38, where each role is defined, along with the possible
values that these roles may assume.
</bodyText>
<subsectionHeader confidence="0.8507355">
Example 38
The Structure of Qualia:
</subsectionHeader>
<listItem confidence="0.99541875">
1. Constitutive Role: the relation between an object and its constituents, or
proper parts.
• Material
• Weight
• Parts and component elements
19 These components of an object&apos;s denotation have long been considered crucial for our commonsense
understanding of how things interact in the world. Cf. Hayes (1979), Hobbs et al. (1987), and Croft
(1991) for discussion of these qualitative aspects of meaning.
</listItem>
<page confidence="0.975277">
426
</page>
<note confidence="0.564148">
James Pustejovsky The Generative Lexicon
</note>
<listItem confidence="0.996085875">
2. Formal Role: that which distinguishes the object within a larger domain.
• Orientation
• Magnitude
• Shape
• Dimensionality
• Color
• Position
3. Telic Role: purpose and function of the object.
• Purpose that an agent has in performing an act
• Built-in function or aim that specifies certain activities
4. Agentive Role: factors involved in the origin or &amp;quot;bringing about&amp;quot; of an
object.
• Creator
• Artifact
• Natural Kind
• Causal Chain
</listItem>
<bodyText confidence="0.9987745">
When we combine the qualia structure of a NP with the argument structure of a verb,
we begin to see a richer notion of compositionality emerging, one that looks very much
like object-oriented approaches to programming (cf. Ingria and Pustejovsky 1990).
To illustrate these structures at play, let us consider a few examples. Assume
that the decompositional semantics of a nominal includes a specification of its qualia
structure:
</bodyText>
<equation confidence="0.2164135">
Example 39
Object(Const, Form, Telic, Agent)
</equation>
<bodyText confidence="0.997140666666667">
For example, a minimal semantic description for the noun novel will include values for
each of these roles, as shown in Example 40, where *x* can be seen as a distinguished
variable, representing the object itself.
</bodyText>
<table confidence="0.572798666666667">
Example 40
novel (*x*)
Const: narrative(*x*)
Form: book(*x*), disk(*x*)
Telic: read(T,y,*x*)
Agentive: artifact(*x*), write(T,z,*x*)
</table>
<bodyText confidence="0.9924594">
This structures our basic knowledge about the object: it is a narrative; typically in
the form of a book; for the purpose of reading (whose event type is a transition);
and is an artifact created by a transition event of writing. Observe how this structure
differs minimally, but significantly, from the qualia structure for the noun dictionary in
Example 41.
</bodyText>
<page confidence="0.982618">
427
</page>
<table confidence="0.450931">
Computational Linguistics Volume 17, Number 4
Example 41
dictionary(*x*)
Const: alphabetized—listing(*x*)
Form: book(*x*), disk(*x*)
Telic: reference(P,y,*x*)
Agentive: artifact(*x*), compile(T,z,*x*)
</table>
<bodyText confidence="0.889933285714286">
Notice the differences in the values for the constitutive and telic roles. The purpose of
a dictionary is an activity of referencing, which has an event structure of a process.
I will now demonstrate that such structured information is not only useful for
nouns, but necessary to account for their semantic behavior. I suggested earlier, that
for cases such as 33, repeated below, there was no need to posit a separate lexical entry
for each verb, where the syntactic and semantic types had to be represented explicitly.
Example 42
</bodyText>
<listItem confidence="0.914344666666667">
a. Mary enjoyed the book.
b. Thatcher vetoed the channel tunnel.
c. John began a novel.
</listItem>
<bodyText confidence="0.998952">
Rather, the verb was analyzed as coercing its complement to the semantic type it ex-
pected. To illustrate this, consider 42(c). The type for begin within a standard typed
intensional logic is &lt;VP , &lt;NP , S&gt;&gt;, and its lexical semantics is similar to that of other
subject control verbs (cf. Klein and Sag [1985] for discussion).
</bodyText>
<equation confidence="0.5529655">
Example 43
APAP&apos;PAx[begin&apos;(P(x*))(f)]
</equation>
<bodyText confidence="0.993280666666667">
Assuming an event structure such as that of Krifka (1987) or Pustejovsky (1991),
we can convert this lexical entry into a representation consistent with a logic making
use of event-types (or sorts) by means of the following meaning postulate.2°
</bodyText>
<subsectionHeader confidence="0.520024">
Example 44
</subsectionHeader>
<bodyText confidence="0.832711666666667">
VPVX1 El [Pa (xi ) (XII) 2ea [P (Xi ) (xn)(ea)il
This allows us to type the verb begin as taking a transition event as its first argument,
represented in Example 45.
</bodyText>
<subsectionHeader confidence="0.566372">
Example 45
ApTAP&apos;PAx[begin&apos;(PT(e))(x*)]
</subsectionHeader>
<bodyText confidence="0.842524333333333">
Because the verb requires that its first argument be of type transition the complement
in 33(c) will not match without some sort of shift. It is just this kind of context where
the complement (in this case a novel) is coerced to another type. The coercion dictates to
the complement that it must conform to its type specification and the qualia roles may
20 It should be pointed out that the lexical structure for the verb bake given above in 30 and 31 can more
properly be characterized as a process acting on various qualia of the arguments.
</bodyText>
<footnote confidence="0.790736">
1. Ay Ax AeP es [bake(eP) A agent(eP , x) A object(eP , Const(y)) A cake(es) A object(es , Formal(x))]
</footnote>
<page confidence="0.995705">
428
</page>
<note confidence="0.705814">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.998753">
in fact have values matching the correct type. For purposes of illustration, the qualia
structure for novel from 41 can be represented as the logical expression in Example 46.
</bodyText>
<equation confidence="0.835834333333333">
Example 46
novel translates into:
Ax[novel(x) A Const (x) = narrative&apos; (x) A
Form(x) = book&apos; (x) A
Telic (x) = Ay, eT [read&apos; (x)(y)(eT)] A
Agent(x) = Ay, eT [write&apos; (x)(y)(eT)]]
</equation>
<bodyText confidence="0.999914">
The coercion operation on the complement in the above examples can be seen as a
request to find any transition event associated with the noun. As we saw above, the
qualia structure contains just this kind of information.
We can imagine the qualia roles as partial functions from a noun denotation into its
subconstituent denotations. For our present purposes, we abbreviate these functions
as QF, Qc, QT, QA. When applied, they return the value of a particular qualia role. For
example, the purpose of a novel is for reading it, shown in 47(a), while the mode of
creating a novel is by writing it, represented in 47(b).
</bodyText>
<equation confidence="0.933145">
Example 47
a. QT(novel) = Ay, eT [reacl(x) (Y) (eT )1
b. QA (novel) = Ay, eT [write (x) (y) (eT)]
</equation>
<bodyText confidence="0.9925065">
As the expressions in 47 suggest, there are, in fact, two obvious interpretations for this
sentence in 42(c).
</bodyText>
<subsectionHeader confidence="0.748657">
Example 48
</subsectionHeader>
<bodyText confidence="0.98331">
a. John began to read a novel.
b. John began to write a novel.
One of these is selected by the coercing verb, resulting in a complement that has a
event-predicate interpretation, without any syntactic transformations (cf. Pustejovsky
[1989a] for details).21 The derivation in 49(a) and the structure in 49(b) show the effects
of this coercion on the verb&apos;s complement, using the telic value of nove1.22
21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and
various contextual influences. But 1 maintain that there are only a finite number of default
interpretations available in such constructions. These form part of the lexical semantics of the noun.
Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al.
(1990).
22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while
also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that
each expression a may have available to it, a set of shifting operators, which we call Ea, which operate
over an expression, changing its type and denotation. By making reference to these operators directly
in the rule of function application, we can treat the functor polymorphically, as illustrated below.
</bodyText>
<listItem confidence="0.780674333333333">
1. Function Application with Coercion (FAO:
If a is of type (b, a), and is of type c, then
(a) if type c = b, then a(/3) is of type a.
(b) if there is a a E Ep such that cr(13) results in an expression of type b, then
a (a (0)) is of type a.
(c) otherwise a type error is produced.
</listItem>
<page confidence="0.903643">
429
</page>
<figure confidence="0.993963909090909">
Computational Linguistics Volume 17, Number 4
Example 49
a. John began a novel.
b. begin&apos;(QT(a novel))(John)
c. begin&apos;Pa, eT [read (a novel) (x)(eT )1) (John)
d. John-pa[begin&apos;(Ax, eT [read(a novel) (x)(eT )] (x*))(x*)])
e. John{Ax[begin&apos;(AeT[read(a novel)(x*)(eT)D(x*)11
f. begin&apos; (AeT [read (a novel)(John) (eT)]) (John)
Mary begin a novel
NP&apos; &lt; VP, &lt; NP, S » NP&apos;
&lt; NP , S &gt;
</figure>
<bodyText confidence="0.953831217391304">
The fact that this is not a unique interpretation of the elliptical event predicate is in
some ways irrelevant to the notion of type coercion. That there is some event involving
the complement is required by the lexical semantics of the governing verb and the rules
of type well-formedness, and although there are many ways to act on a novel, I argue
that certain relations are &amp;quot;privileged&amp;quot; in the lexical semantics of the noun. It is not the
role of a lexical semantic theory to say what readings are preferred, but rather which
are available.23
Assuming the semantic selection given above for begin is correct, we would predict
that, because of the process event-type associated with the telic role for dictionary, there
is only one default interpretation for the sentence in 50; namely, the agentive event of
&amp;quot;compiling.&amp;quot;
23 There are interesting differences in complement types between finish and complete. The former takes
both NP and a gerundive VP, while the latter takes only an NP (cf. for example, Freed [1979] for
discussion).
Ia. John finished the book.
b. John finished writing the book.
2a. John completed the book.
b. *John completed writing the book.
The difference would indicate that, contrary to some views (e.g. Wierzbicka [19881 and Dixon [19911),
lexical items need to carry both syntactic and semantic selectional information to determine the range
of complements they may take. Notice here also that complete tends to select the agentive role value for
its complement and not the telic role. The scope of semantic selection is explored at length in
Pustejovsky (forthcoming).
</bodyText>
<page confidence="0.991332">
430
</page>
<figure confidence="0.959078">
James Pustejovsky The Generative Lexicon
Example 50
a. Mary began a dictionary. (Agentive)
b. ?? Mary began a dictionary. (Telic)
</figure>
<bodyText confidence="0.832978333333333">
Not surprisingly, when the noun in complement position has no default interpretation
within an event predicate — as given by its qualia structure — the resulting sentence
is extremely odd.
</bodyText>
<subsectionHeader confidence="0.496242">
Example 51
</subsectionHeader>
<bodyText confidence="0.910694666666667">
a. *Mary began a rock.
b. ??John finished the flower.
The semantic distinctions that are possible once we give semantic weight to lexical
items other than verbs are quite wide-ranging. The next example I will consider con-
cerns scalar modifiers, such as fast, that modify different predicates depending on the
head they modify. If we think of certain modifiers as modifying only a subset of the
qualia for a noun, then we can view fast as modifying only the telic role of an object.
This allows us to go beyond treating adjectives such as fast as intersective modifiers
— for example, as Ax[cari (x) A fast&apos; (x)]. Let us assume that an adjective such as fast is a
member of the general type (N, N), but can be subtyped as applying to the Telic role
of the noun being modified. That is, it has as its type, ([N Telic], N). This gives rise
directly to the different interpretations in Example 52.
</bodyText>
<figure confidence="0.8079985">
Example 52
a. a fast car: driving
QT (car) = AxAyAeP[ drive(x)(y)(eP) I
b. a fast typist: typing
</figure>
<equation confidence="0.592979">
QT(typist) = AxAeP[ type(x)(eP) I
c. a fast motorway: traveling
QT(motorway) = AxAe1 travel(cars)(eP) A on(x)(cars)(eP)
</equation>
<bodyText confidence="0.9991426">
These interpretations are all derived from a single word sense for fast. Because the
lexical semantics for this adjective indicates that it modifies the telic role of the noun,
it effectively acts as an event predicate rather than an attribute over the entire noun
denotation, as illustrated in Example 53 for fast motorway (cf. Pustejovsky and Boguraev
[1991] for discussion).
</bodyText>
<equation confidence="0.743039333333333">
Example 53
)x[motorway(x) . . . [Telic(x) = AeP [ travel(cars)(eP)
A on(x)(cars)(eP) A fast(eP) ]]]
</equation>
<bodyText confidence="0.999808333333333">
As our final example of how the qualia structure contributes to the semantic in-
terpretation of a sentence, observe how the nominals window and door in Examples 54
and 55 carry two interpretations (cf. Lakoff [1987] and Pustejovsky and Anick [19881):
</bodyText>
<subsectionHeader confidence="0.484764">
Example 54
</subsectionHeader>
<bodyText confidence="0.522378">
a. John crawled through the window.
b. The window is closed.
</bodyText>
<page confidence="0.951815">
431
</page>
<figure confidence="0.98495775">
Computational Linguistics Volume 17, Number 4
Example 55
a. Mary painted the door.
b. Mary walked through the door.
</figure>
<bodyText confidence="0.842713">
Each noun appears to have two word senses: a physical object denotation and an
aperture denotation. Pustejovsky and Anick (1988) characterize the meaning of such
&amp;quot;Double Figure-Ground&amp;quot; nominals as inherently relational, where both parameters are
logically part of the meaning of the noun. In terms of the qualia structure for this class
of nouns, the formal role takes as its value the Figure of a physical object, while the
constitutive role assumes the Invert-Figure value of an aperture.24
</bodyText>
<figure confidence="0.957854285714286">
Example 56
Lexical Semantics for door:
door(*x*,*y*)
Const: aperture (*y*)
Form: phys—obj(*x*)
Telic: pass—through(T,z,*y*)
Agentive: artifact(*x*)
</figure>
<bodyText confidence="0.951254909090909">
The foregrounding or backgrounding of a nominal&apos;s qualia is very similar to argument
structure-changing operations for verbs. That is, in 55(a), paint applies to the formal
role of the door, while in 55(b), through will apply to the constitutive interpretation of
the same NP. The ambiguity with such nouns is a logical one, one that is intimately
linked to the semantic representation of the object itself. The qualia structure, then, is
a way of capturing this logical polysemy.
In conclusion, it should be pointed out that the entire lexicon is organized around
such logical ambiguities, which Pustejovsky and Anick (1988) call Lexical Conceptual
Paradigms. Pustejovsky (forthcoming) distinguishes the following systems and the
paradigms that lexical items fall into:
Example 57
</bodyText>
<listItem confidence="0.993265555555556">
a. Count/Mass Alternations
b. Container/Containee Alternations
c. Figure/Ground Reversals
d. Product/Producer Diathesis
e. Plant/Fruit Alternations
f. Process/Result Diathesis
g. Object/Place Reversals
h. State/Thing Alternations
i. Place/People
</listItem>
<bodyText confidence="0.905806714285714">
Such paradigms provide a means for accounting for the systematic ambiguity that may
exist for a lexical item. For example, a noun behaving according to paradigm 57(a)
24 There are many such classes of nominals, both two-dimensional such as those mentioned in the text,
and three-dimensional, such as &amp;quot;room,&amp;quot; &amp;quot;fireplace,&amp;quot; and &amp;quot;pipe.&amp;quot; They are interesting semantically,
because they are logically ambiguous, referring to either the object or the aperture, but not both.
Boguraev and Pustejovsky (forthcoming) show how these logical polysemies are in fact encoded in
dictionary definitions for these words.
</bodyText>
<page confidence="0.996482">
432
</page>
<note confidence="0.683163">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.8290825">
exhibits a logical polysemy involving packaging or grinding operators; e.g., haddock
or lamb (cf. Copestake and Briscoe [1991] for details).
</bodyText>
<sectionHeader confidence="0.394011" genericHeader="method">
7. Lexical Inheritance Theory
</sectionHeader>
<bodyText confidence="0.99995629032258">
In previous sections, I discussed lexical ambiguity and showed how a richer view of
lexical semantics allows us to view a word&apos;s meaning as being flexible, where word
senses could arise generatively by composition with other words. The final aspect of
this flexibility deals with the logical associations a word has in a given context; that is,
how this semantic information is organized as a global knowledge base. This involves
capturing both the inheritance relations between concepts and, just as importantly,
how the concepts are integrated into a coherent expression in a given sentence.
I will assume that there are two inheritance mechanisms at work for representing
the conceptual relations in the lexicon: fixed inheritance and projective inheritance. The
first includes the methods of inheritance traditionally assumed in Al and lexical re-
search (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and
Winograd 1977); that is, a fixed network of relations, which is traversed to discover
existing related and associated concepts (e.g. hyponyms and hypernyms). In order to
arrive at a comprehensive theory of the lexicon, we need to address the issue of global
organization, and this involves looking at the various modes of inheritance that exist
in language and conceptualization. Some of the best work addressing the issue of how
the lexical semantics of a word ties into its deeper conceptual structure includes that
of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information
structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar
(1989, 1990).
In addition to this static representation, I will introduce another mechanism for
structuring lexical knowledge, the projective inheritance, which operates generatively
from the qualia structure of a lexical item to create a relational structure for ad hoc
categories. Both are necessary for projecting the semantic representations of individual
lexical items onto a sentence level interpretation. The discussion here, however, will
be limited to a description of projective inheritance and the notion of &amp;quot;degrees of
prototypicality&amp;quot; of predication. I will argue that such degrees of salience or coherence
relations can be explained in structural terms by examining a network of related lexical
items.&amp;quot;
I will illustrate the distinction between these mechanisms by considering the two
sentences in Example 58, and their relative prototypicality.
</bodyText>
<subsectionHeader confidence="0.954233">
Example 58
</subsectionHeader>
<bodyText confidence="0.996988">
a. The prisoner escaped last night.
b. The prisoner ate dinner last night.
Both of these sentences are obviously well-formed syntactically, but there is a definite
sense that the predication in 58(a) is &amp;quot;tighter&amp;quot; or more prototypical than that in 58(b).
What would account for such a difference? Intuitively, we associate prisoner with an
escaping event more strongly than an eating event. Yet this is not information that
comes from a fixed inheritance structure, but is rather usually assumed to be corn-
monsense knowledge. In what follows, however, I will show that such distinctions
</bodyText>
<footnote confidence="0.89287">
25 Anick and Pustejovsky (1990) explore how metrics such as association ratios can be used to statistically
measure the notions of prototypicality mentioned here.
</footnote>
<page confidence="0.996149">
433
</page>
<note confidence="0.590664">
Computational Linguistics Volume 17, Number 4
</note>
<bodyText confidence="0.947582904761905">
can be captured within a theory of lexical semantics by means of generating ad hoc
categories.
First, we give a definition for the fixed inheritance structure of a lexical item (cf.
Touretzky 1986). Let Q and P be concepts in our model of lexical organization. Then:
Definition
A sequence (Qi , PO is an inheritance path, which can be read as the conjunction
of ordered pairs {(xi, y,) Ii &lt; i &lt; n}.
Furthermore, following Touretsky, from this we can define the set of concepts that lie
on an inheritance path, the conclusion space.
Definition
The conclusion space of a set of sequences (I) is the set of all pairs (Q, P) such that a
sequence (Q, , P) appears in 1.
From these two definitions we can define the traditional is-a relation, relating the
above pairs by a generalization operator, &lt;G,&amp;quot; as well as other relations that I will not
discuss.27
Let us suppose that, in addition to these fixed relational structures, our semantics
allows us to dynamically create arbitrary concepts through the application of certain
transformations to lexical meanings. For example, for any predicate, Q — e.g. the
value of a qualia role — we can generate its opposition, -02 (cf. Pustejovsky 1991).
By relating these two predicates temporally we can generate the arbitrary transition
events for this opposition (cf. Wright 1963):
</bodyText>
<figure confidence="0.6367778">
Example 59
a. -Q(x) _&lt; Q(x)
b. Q(x) &lt;-Q(x)
c. Q(x) &lt; Q(x)
d. -Q(x) --Q(x)
</figure>
<bodyText confidence="0.978434272727273">
Similarly, by operating over other qualia role values we can generate semantically
related concepts. I will call any operator that performs such an operation a projective
transformation, and define them below:
Definition
A projective transformation, 7r, on a predicate Qi generates a predicate, Q2, such that
7r(Q1) = Q2, where Q2 cl (I) . The set of transformations includes: negation, &lt;,
temporal precedence, &gt;, temporal succession, =, temporal equivalence, and act,
an operator adding agency to an argument.
Intuitively, the space of concepts traversed by the application of such operators
will be related expressions in the neighborhood of the original lexical item. This space
can be characterized by the following two definitions:
</bodyText>
<footnote confidence="0.872482">
26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts.
27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs
by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991.
</footnote>
<page confidence="0.995674">
434
</page>
<note confidence="0.820329">
James Pustejovsky The Generative Lexicon
</note>
<subsectionHeader confidence="0.86766">
Definition
</subsectionHeader>
<bodyText confidence="0.9991545">
A series of applications of transformations, , 7rn, generates a sequence of predi-
cates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ).
</bodyText>
<subsectionHeader confidence="0.954326">
Definition
</subsectionHeader>
<bodyText confidence="0.9990955">
The projective conclusion space, P(&apos;DR), is the set of projective expansions generated
from all elements of the conclusion space, on role R of predicate Q: as: P(43R) =
{(P(Q1)P(Q)) I(Qi, • • • , Qn) 430.
From this resulting representation, we can generate a relational structure that can
be considered the set of ad hoc categories and relations associated with a lexical item
(cf. Barselou 1983).
Using these definitions, let us return to the sentences in Example 58. I will assume
that the noun prisoner has a qualia structure such as that shown in 60.
</bodyText>
<figure confidence="0.9331225">
Example 60
Qualia Structure of prisoner(x):
prisoner (*x*)
Form: human(*x*)
Telic: [confine(y,*x*) &amp; location(*x*,prison)]
Furthermore, I assume the following lexical structure for escape.
Example 61
Lexical Semantics for escape:
AxAeT]eP , es [escape(eT) A act(e) A confined(eP) A agent(eP , x)
A —confined(es) A object(es , x)]
</figure>
<bodyText confidence="0.998111272727273">
Using the representation in 60 above, I now trace part of the derivation of the
projective conclusion space for prisoner. Inheritance structures are defined for each
qualia role of an element. In the case above, values are specified for only two roles.
For each role, R, we apply a projective transformation 7 onto the predicate Q that is
the value of that role. For example, from the telic role of prisoner we can generalize (e.g.
drop the conjunct) to the concept of being confined. From this concept, we can apply the
negation operator, generating the predicate opposition of not-confined and confined. To
this, we apply the two temporal operators, &lt; and &gt;, generating two states: free before
capture and free after capture. Finally, to these concepts, if we apply the operator act,
varying who is responsible for the resulting transition event, we generate the concepts:
turn in, capture, escape, and release.
</bodyText>
<subsectionHeader confidence="0.78413">
Example 62
</subsectionHeader>
<bodyText confidence="0.71651">
Projecting on Telic Role of prisoner:
</bodyText>
<equation confidence="0.685624166666667">
a. &lt;G: [confine(y, x) A loc(x, prison)] zr confine(y, x)
b. —,: 3E1 [—confine(Ei Y, x)]
c. 3E2 [confine(E2, y, x)]
d. &lt;:E1 &lt; £2 =
e. &lt;:E2 &lt; El = T2
f. act: act(x,T1) = &amp;quot;turn in&amp;quot;
</equation>
<page confidence="0.976141">
435
</page>
<note confidence="0.551809">
Computational Linguistics Volume 17, Number 4
</note>
<equation confidence="0.975251333333333">
g. act: act(y,T1) = &amp;quot;capture&amp;quot;
h. act: act(x, T2) = &amp;quot;escape&amp;quot;
i. act: act(y,T2) = &amp;quot;release&amp;quot;
</equation>
<bodyText confidence="0.9999202">
These relations constitute the projective conclusion space for the telic role of prisoner
relative to the application of the transformations mentioned above. Similar operations
on the formal role will generate concepts such as die and kill. Generating such structures
for all items in a sentence during analysis, we can take those graphs that result in no
contradictions to be the legitimate semantic interpretations of the entire sentence.
Let us now return to the sentences in Example 58. It is now clear why these two
sentences differ in their prototypicality (or the relevance conditions on their predi-
cation). The predicate eat is not within the space of related concepts generated from
the semantics of the NP the prisoner; escape, however, did fall within the projective
conclusion space for the Telic role of prisoner, as shown in Example 63.
</bodyText>
<equation confidence="0.739542214285714">
Example 63
Conclusion Space for (58):
escape E P(43T(prisoner))
eat P(1T(prisoner))
This is illustrated in Example 64 below.
Example 64
release(T, y, *x*) escape(T, *x*) capture(T, y, *x*) turn-in(T, *x*)
&lt; S2 S2 &lt; Si
—confined(S2, y, *x*)
confined(Si, y, *x*)
Formal Telic
the prisoner
Det N V
NP VP
</equation>
<bodyText confidence="0.9801885">
We can therefore use such a procedure as one metric for evaluating the &amp;quot;proximity&amp;quot;
of a predication (Quillian 1968; Hobbs 1982). In the examples above, the difference
</bodyText>
<equation confidence="0.5095215">
escape(T, *x*)
escaped
</equation>
<page confidence="0.996362">
436
</page>
<note confidence="0.762615">
James Pustejovsky The Generative Lexicon
</note>
<bodyText confidence="0.9996635">
in semanticality can now be seen as a structural distinction between the semantic
representations for the elements in the sentence.
In this section, I have shown how the lexical inheritance structure of an item
relates, in a generative fashion, the decompositional structure of a word to a much
larger set of concepts that are related in obvious ways. What we have not addressed,
however, is how the fixed inheritance information of a lexical item is formally derivable
during composition. This issue is explicitly addressed in Briscoe et al. (1990) as well
as Pustejovsky and Briscoe (1991).
</bodyText>
<sectionHeader confidence="0.678378" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.999541894736842">
In this paper I have outlined a framework for lexical semantic research that I believe
can be useful for both computational linguists and theoretical linguists alike. I argued
against the view that word meanings are fixed and inflexible, where lexical ambigu-
ity must be treated by multiple word entries in the lexicon. Rather, the lexicon can
be seen as a generative system, where word senses are related by logical operations
defined by the well-formedness rules of the semantics. In this view, much of the lex-
ical ambiguity of verbs and prepositions is eliminated because the semantic load is
spread more evenly throughout the lexicon to the other lexical categories. I described
a language for structuring the semantic information carried by nouns and adjectives,
termed Qualia structure, as well as the rules of composition that allow this information
to be incorporated into the semantic interpretation of larger expressions, including
explicit methods for type coercion. Finally, I discussed how these richer lexical repre-
sentations can be used to generate projective inheritance structures that connect the
conceptual information associated with lexical items to the global conceptual lexicon.
This suggests a way of accounting for relations such as coherence and the prototyp-
icality of a predication. Although much of what I have presented here is incomplete
and perhaps somewhat programmatic, I firmly believe this approach can help clarify
the nature of word meaning and compositionality in natural language, and at the same
time bring us closer to understanding the creative use of word senses.
</bodyText>
<sectionHeader confidence="0.925704" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.878098833333333">
I would like to thank the following for
comments on earlier drafts of this paper:
Peter Anick, Sabine Bergler, Bran Boguraev,
Ted Briscoe, Noam Chomsky, Bob Ingria,
George Miller, Sergei Nirenburg, and Rich
Thomason.
</bodyText>
<sectionHeader confidence="0.848436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983339305555555">
Anick, P., and Pustejovsky, J. (1990). &amp;quot;An
application of lexical semantics to
knowledge acquisition from corpora.&amp;quot; In
Proceedings, 13th International Conference on
Computational Linguistics, Helsinki,
Finland.
Atkins, Beryl T. (1987). &amp;quot;Semantic ID tags:
corpus evidence for dictionary senses.&amp;quot; In
Proceedings, 3rd Annual Conference at
University of Waterloo, Center for the New
OED.
Atkins, Beryl; Kegl, Judy; and Levin, Beth
(1988). &amp;quot;Anatomy of a verb entry.&amp;quot;
International Journal of Lexicography,
1:84-126.
Atkins, Beryl; Klavans, Judith; and
Boguraev, Bran. &amp;quot;Semantic verb clusters
from MRDs.&amp;quot; Forthcoming.
Bach, Emmon (1986). &amp;quot;The algebra of
events. Linguistics and Philosophy, 9: 5-16.
Barselou, Lawrence W. (1983) &amp;quot;Ad hoc
categories.&amp;quot; Memory and Cognition,
11:211-227.
Beckwith, Richard; Fellbaum, C.; Gross, D.;
and Miller, G. (1989). &amp;quot;WordNet: A lexical
database organized on psycholinguistic
principles.&amp;quot; Proceedings, First International
Workshop on Lexical Acquisition, IJCAI,
Detroit.
Bobrow, D. G., and Winograd, T. (1977).
&amp;quot;An overview of KRL, a knowledge
representation language.&amp;quot; Cognitive
Science, 1:3-46.
Boguraev, Bran, and Briscoe, Ted (eds.)
(1988). Computational Lexicography for
Natural Language Processing. Harlow,
</reference>
<page confidence="0.993613">
437
</page>
<note confidence="0.589855">
Computational Linguistics Volume 17, Number 4
</note>
<reference confidence="0.995459165289256">
Essex: Longman.
Boguraev, Bran, and Pustejovsky, James.
Lexical Knowledge: Representation and
Acquisition. Forthcoming. Cambridge,
MA: The MIT Press.
Brachman, Ronald J. (1979). &amp;quot;On the
epistemelogical status of semantic
networks.&amp;quot; In Associative Networks:
Representation and Use of Knowledge by
Computers, edited by N. Findler. New
York: Academic Press.
Brachman, R. J., and Schmolze, J. (1985).
&amp;quot;An overview of the KL-ONE knowledge
representation system.&amp;quot; Cognitive Science,
9:171-216.
Bresnan, Joan (ed.) (1982). The Mental
Representation of Grammatical Relations.
Cambridge, MA: The MIT Press.
Briscoe, E., Copestake, A., and Boguraev, B.
(1990). &amp;quot;Enjoy the paper: lexical semantics
via lexicology.&amp;quot; In Proceedings, 13th
International Conference on Computational
Linguistics, Helsinki, Finland.
Cardelli, L., and Wegner, P. (1985). &amp;quot;On
understanding types, data abstraction,
and polymorphism.&amp;quot; ACM Computing
Surveys, 17(4): 471-522.
Carlson, Gregory (1977). &amp;quot;Reference to
kinds in English.&amp;quot; Doctoral dissertation,
University of Massachusetts.
Carnap, Rudolf (1956). Meaning and
Necessity. Chicago: University of Chicago
Press.
Charniak, Eugene, and Goldman, Robert
(1988). &amp;quot;A logic for semantic
interpretation.&amp;quot; In Proceedings, 26th
Annual Meeting of the Association for
Computational Linguistics, Buffalo, NY.
Chierchia, G. (1989). &amp;quot;Structured meanings,
thematic roles, and control.&amp;quot; In Properties,
Types, and Meaning, Volume 2, edited by
G. Chierchia, B. Partee, and R. Turner.
Dordrect: Kluwer Academic Publishers.
Chomsky, Noam (1975). The Logical Structure
of Linguistic Theory. Chicago: University of
Chicago Press.
Chomsky, Noam (1981). Lectures on
Government and Binding. Dordrect: Foris
Publications.
Collins, A., and Quillian, M. (1969).
&amp;quot;Retrieval time from semantic memory.&amp;quot;
Journal of Verbal Learning and Verbal
Behavior, 9: 240-247.
Copestake, A., and Briscoe, T. (1991).
&amp;quot;Lexical operations in a unification-based
framework.&amp;quot; In Proceedings, SIGLEX
Workshop on Lexical Semantics and
Knowledge Representation, edited by
J. Pustejovsky and S. Bergler, Berkeley,
CA.
Croft, William (1991). Categories and
Relations in Syntax: The Clause-Level
Organization of Information. Chicago:
University of Chicago Press.
Cruse, D. A. (1986). Lexical Semantics.
Cambridge, U.K.: Cambridge University
Press.
Dixon, R. M. W. (1991). A New Approach to
English Grammar, on Semantic Principles,
Oxford, U.K., Oxford University Press.
Dowty, David R. (1979). Word Meaning and
Montague Grammar. Dordrecht: D. Reidel.
Dowty, David R. (1985). &amp;quot;On some recent
analyses of control.&amp;quot; Linguistics and
Philosophy, 8: 1-41.
Dowty, David R. (1989). &amp;quot;On the semantic
content of the notion &apos;thematic role&apos;.&amp;quot; In
Properties, Types, and Meaning Volume II,
edited by G. Chierchia, B. Partee, and
R. Turner. Dordrect: Kluwer Academic
Publishers.
Elliott, D. E. (1974). &amp;quot;Towards a grammar of
exclamations.&amp;quot; Foundations of Language,
11:231-246.
Evans, Roger, and Gazdar, Gerald (1989).
&amp;quot;Inference in DATR.&amp;quot; In Proceedings,
Fourth European ACL Conference.
Manchester, England.
Evans, Roger, and Gazdar, Gerald (1990).
&amp;quot;The DATR papers: February 1990,&amp;quot;
Cognitive Science Research Paper CSRP
139, School of Cognitive and Computing
Science, University of Sussex, Brighton,
England.
Fass, Dan (1988). &amp;quot;Collative semantics: A
semantics for natural language
processing,&amp;quot; MCCS-99-118, Computing
Research Laboratory, New Mexico State
University.
Fauconnier, G. (1985). Mental Spaces.
Cambridge, MA: The MIT Press.
Fillmore, Charles (1968). &amp;quot;The case for
case.&amp;quot; In Universals in Linguistic Theory,
edited by E. Bach and R. Harms. New
York: Holt, Rinehart, and Winston.
Fillmore, Charles (1985). &amp;quot;Construction
grammar.&amp;quot; Ms.
Flickinger, D.; Pollard, C.; and Wasow, T.
(1985). &amp;quot;Structure-sharing in lexical
representation.&amp;quot; In Proceedings, 23rd
Annual Meeting of the ACL. Chicago, IL.
Fodor, Jerry (1975). The Language of Thought.
Cambridge, MA: Harvard University
Press.
Freed, A. F. (1979). The Semantics of English
Aspectual Complementation. Dordrecht:
Reidel.
Gazdar, G.; Klein, E.; Pullum, G; and Sag, I.
(1985). Generalized Phrase Structure
Grammar. Cambridge, MA: Harvard
University Press.
</reference>
<page confidence="0.998206">
438
</page>
<note confidence="0.826748">
James Pustejovsky The Generative Lexicon
</note>
<reference confidence="0.999454655737705">
Goodman, Nelson (1951). The Structure of
Appearance. Dordrecht: Reidel.
Grice, H. P. (1971). &amp;quot;Meaning.&amp;quot; In Semantics:
An Interdisciplinary Reader in Philosophy,
Linguistics, and Psychology, edited by
D. Steinberg and L. Jacobovits.
Cambridge: Cambridge University Press.
Grimshaw, Jane (1979). &amp;quot;Complement
selection and the lexicon.&amp;quot; Linguistic
Inquiry, 10:279-326.
Grimshaw, Jane (1990). Argument Structure.
Cambridge, MA: The MIT Press, in press.
Gruber, Jeffrey (1965). &amp;quot;Studies in lexical
relations.&amp;quot; Doctoral dissertation,
Massachusetts Institute of Technology.
Hale, Ken, and Keyser, S. J. (1986). &amp;quot;Some
transitivity alternations in English.&amp;quot;
Lexicon Project Working Papers 7, Center
for Cognitive Science, MIT.
Hale, Ken, and Keyser, S. J. (1987). &amp;quot;A view
from the middle.&amp;quot; Lexicon Project
Working Papers 10, Center for Cognitive
Science, MIT.
Hayes, Patrick (1979). &amp;quot;Naive physics
manifesto.&amp;quot; In Expert Systems in the
Micro-Electronic Age, edited by Donald
Mitchie. Edinburgh: Edinburgh
University Press.
Higginbotham, James (1985). &amp;quot;On
semantics.&amp;quot; Linguistic Inquiry, 16:547-593.
Hinrichs, Erhard W. (1985). &amp;quot;A
compositional semantics for aktionarten
and NP reference in English.&amp;quot; Doctoral
dissertation, Ohio State University.
Hirst, Graeme (1987). Semantic Interpretation
and the Resolution of Ambiguity.
Cambridge: Cambridge University Press.
Hobbs, Jerry (1982). &amp;quot;Towards an
understanding of coherence in discourse.&amp;quot;
In Strategies for Natural Language
Processing, edited by W. Lehnert and
M. Ringle. Hillsdale, NJ: Lawrence
Erlbaum Associates.
Hobbs, Jerry (1987). &amp;quot;World knowledge and
word meaning.&amp;quot; In Proceedings, TINLAP-3.
Las Cruces, NM.
Hobbs, Jerry; Croft, William; Davies, Todd;
Edwards, Douglas; and Laws, Kenneth.
(1987). &amp;quot;Commonsense metaphysics and
lexical semantics.&amp;quot; Computational
Linguistics, 13:241-250.
Hobbs, Jerry; Croft, William; Davies, Todd;
Edwards, Douglas; and Laws, Kenneth,
&amp;quot;The TACITUS commonsense knowledge
base.&amp;quot; Artificial Intelligence Center, SRI
International.
Hobbs, Jerry; Stickel, Mark; Martin, Paul;
and Edwards, Douglas (1988).
&amp;quot;Interpretation as abduction.&amp;quot; In
Proceedings, 26th Annual Meeting of the
Association for Computational Linguistics.
Buffalo, NY.
Ingria, Robert, and Pustejovsky, James
(1990). &amp;quot;Active objects in syntax,
semantics, and parsing.&amp;quot; In The MIT
Parsing Volume, 1989-1990, edited by
Carol Tenny. Center for Cognitive Science,
MIT.
Jackendoff, Ray (1972). Semantic
Interpretation in Generative Grammar.
Cambridge, MA: The MIT Press.
Jackendoff, Ray (1983). Semantics and
Cognition. Cambridge, MA: The MIT
Press.
Katz, Jerrold J. (1972). Semantic Theory. New
York: Harper and Row.
Katz, Jerrold J., and Fodor, Jerry (1963).
&amp;quot;The structure of a semantic theory.&amp;quot;
Language, 39(2): 170-210.
Karttunen, Lauri (1971). &amp;quot;Implicative
verbs.&amp;quot; Language, 47: 340-58.
Karttunen, Lauri (1974). &amp;quot;Presupposition
and linguistic context.&amp;quot; Theoretical
Linguistics 1: 181-93.
Katz, Jerrold, and Fodor, Jerry (1963). &amp;quot;The
structure of a semantic theory&amp;quot; Language,
39: 170-210.
Keenan, Edward, and Faltz, Leonard (1985).
Boolean Semantics for Natural Language.
Dordrect: Reidel.
Klein, E., and Sag, I. (1985). &amp;quot;Type-driven
translation.&amp;quot; Linguistics and Philosophy, 8:
163-202.
Krifka, Manfred (1987). &amp;quot;Nominal reference
and temporal constitution: Towards a
semantics of quantity.&amp;quot; FNS-Bericht 17.
Forschungsstelle fur naturlich-sparchliche
System, Universitat Tubingen.
Lakoff, George (1970). Irregularity in Syntax.
Holt, Rinehart, and Winston.
Lakoff, George (1971). &amp;quot;On generative
semantics.&amp;quot; In Semantics: An
Interdisciplinary Reader, edited by
D. Steinberg and L. Jakobovits.
Cambridge University Press.
Lakoff, George (1987). Women, Fire, and
Dangerous Objects. Chicago: University of
Chicago Press.
Levin, Beth (ed.) (1985). &amp;quot;Lexical semantics
in review,&amp;quot; Lexicon Project Working
Papers Number 1, MIT.
Levin, B. Towards a Lexical Organization of
English Verbs. Chicago: University of
Chicago Press. Forthcoming.
Levin, Beth, and Rapoport, T. R. (1988).
&amp;quot;Lexical subordination.&amp;quot; Proceedings of
CLS 24, 275-289.
Levin, Beth, and Rappaport, Malka (1986).
&amp;quot;The formation of adjectival passives.&amp;quot;
Linguistic Inquiry, 17:623-663.
Levin, Beth, and Rappaport, Malka (1988).
&amp;quot;On the nature of unaccusativity.&amp;quot; In
</reference>
<page confidence="0.9794">
439
</page>
<reference confidence="0.993765438016529">
Computational Linguistics Volume 17, Number 4
Proceedings, NELS 1988.
Levin, Beth, and Rappaport, Malka.
Unaccusatives. The MIT Press,
forthcoming.
Lloyd, G. E. R. (1968). Aristotle: The Growth
and Structure of his Thought. Cambridge
University Press.
McKeon, Richard (1941). The Basic Works of
Aristotle. Random House.
Mel&apos;C&apos;uk, I. (1988). Dependency Syntax.
Albany, NY: SUNY Press.
Michalski, R. S. (1983). &amp;quot;A theory and
methodology of inductive learning.&amp;quot; In
Machine Learning I, R. S. Michalski,
J. Carbonelli, and T. Mitchell, eds. Palo
Alto, CA: Tioga Publishing.
Miller, George (1985). &amp;quot;Dictionaries of the
mind.&amp;quot; In Proceedings, 23rd Annual
Meeting of the Association for Computational
Linguistics, Chicago, IL.
Miller, G. A., and Fellbaum, C, (1991).
&amp;quot;Semantic networks of English.&amp;quot;
Cognition, October 1991.
Miller, George, and Johnson-Laird, Phillip
(1976). Language and Perception.
Cambridge, MA: Belknap, Harvard
University Press.
Moens, M., and Steedman, M. (1988).
&amp;quot;Temporal ontology and temporal
reference.&amp;quot; Computational Linguistics, 14(2):
15-28.
Montague, Richard (1974). Formal
Philosophy: The Collected Papers of Richard
Montague, edited by Richard Thomason.
New Haven: Yale University Press.
Moravcsik, J. M. (1975). &amp;quot;Aitia as generative
factor in Aristotle&apos;s philosophy.&amp;quot; Dialogue,
14:622-636.
Mourelatos, Alexander (1981), &amp;quot;Events,
processes, and states.&amp;quot; In Syntax and
Semantics: Tense and Aspect, edited by
P. Tedeschi and A. Zaenen. New York:
Academic Press.
Nunberg, G. (1978). The Pragmatics of
Reference. Bloomington, Indiana: Indiana
University Linguistics Club.
Quillian, M. Ross (1968). &amp;quot;Semantic
memory,&amp;quot; In Semantic Information
Processing, edited by M. Minsky.
Cambridge, MA: The MIT Press.
Partee, Barbara, and Rooth, Mats (1983).
&amp;quot;Generalized conjunction and type
ambiguity.&amp;quot; In Meaning, Use, and
Interpretation of Language, edited by
Bauerle, Schwarze, and von Stechow.
Walter de Gruyter.
Passonneau, Rebecca J. (1988). &amp;quot;A
computational model of the semantics of
tense and aspect.&amp;quot; Computational
Linguistics, 14(2).
Pustejovsky, James (1988). &amp;quot;The geometry of
events.&amp;quot; In Studies in Generative Approaches
to Aspect, edited by Carol Tenny. Lexicon
Project Working Papers 24, MIT.
Pustejovsky, James (1989a). &amp;quot;Type coercion
and selection.&amp;quot; Paper presented at West
Coast Conference on Formal Linguistics.
Vancouver.
Pustejovsky, James (1989b). &amp;quot;Issues in
computational lexical semantics.&amp;quot; In
Proceedings, Fourth European ACL
Conference, Manchester, England.
Pustejovsky, James (1991). &amp;quot;The syntax of
event siructure.&amp;quot; Cognition, 41.
Pustejovsky, James (in press a). &amp;quot;Principles
versus criteria: On Randall&apos;s catapult
hypothesis.&amp;quot; In Theoretical Issues in
Language Acquisition, edited by
J. Weissenborn, H. Goodluck, and
T. Roeper. Dordrecht: Kluwer Academic
Publishers.
Pustejovsky, James (in press b). &amp;quot;Type
coercion and lexical selection.&amp;quot; In
Semantics and the Lexicon, edited by
J. Pustejovsky. Dordrecht: Kluwer
Academic Publishers.
Pustejovsky, James (forthcoming). The
Generative Lexicon: A Theory of
Computational Lexical Semantics.
Cambridge, MA: The MIT Press.
Pustejovsky, James, and Anick, Peter (1988).
&amp;quot;On the semantic interpretation of
nominals.&amp;quot; In Proceedings, COLING-1988,
Budapest.
Pustejovsky, James, and Bergler, Sapine, eds.
(in press). Lexical Semantics and
Commonsense Reasoning. Berlin: Springer
Verlag.
Pustejovsky, James, and Boguraev, Bran
(1991). &amp;quot;Lexical knowledge representation
and natural language processing.&amp;quot; In IBM
Journal of Research and Development, 45:4.
Roberts, R. B., and Goldstein, (1977). The
FRL Manual, Technical Report Al Memo
409, MIT Artificial Intelligence Laboratory.
Scha, Remko J. H. (1983). &amp;quot;Logical
foundations for question answering.&amp;quot;
MS 12.331 Philips Research Laboratories,
Eindhoven, the Netherlands.
Schank, Roger (1975). Conceptual Information
Processing. Amsterdam: North-Holland.
Sueren, Pieter (1985). Discourse Semantics.
Oxford: Blackwell Publishers.
Smolka, G. (1988). &amp;quot;A feature logic with
subsorts.&amp;quot; Wissenschaftliches Zentrum
der IBM Deutschland, LILOG-Report 33.
Stump, G. (1981). &amp;quot;Frequency adjectives.&amp;quot;
Linguistics and Philosophy, 4:221-258.
Talmy, Len (1975). &amp;quot;Semantics and syntax of
motion.&amp;quot; In Syntax and Semantics 4, edited
</reference>
<page confidence="0.985882">
440
</page>
<note confidence="0.604313">
James Pustejovsky The Generative Lexicon
</note>
<reference confidence="0.999837254901961">
by]. P. Kimball. New York: Academic
Press.
Talrny, Len (1985). &amp;quot;Lexicalization patterns.&amp;quot;
In Language Typology and Syntactic
Description, edited by Timothy Shopen.
Cambridge.
Tenny, Carol (1987). &amp;quot;Grammaticalizing
aspect and affectedness.&amp;quot; Doctoral
dissertation, MIT.
Tenny, Carol (1989). &amp;quot;The aspectual interface
hypothesis,&amp;quot; Lexicon Project Working
Papers 31, MIT.
Touretzky, David S. (1986). The Mathematics
of Inheritance Systems. Los Altos, CA:
Morgan Kaufmann.
Vendler, Zeno (1967). Linguistics and
Philosophy. Ithaca, NY: Cornell University
Press.
Verma, Manindra, and Mohanan, K. P.
(1991). Experiencer Subjects in South Asian
Languages, CSLI. Distributed by
University of Chicago Press.
Walker, Donald; Zampolli, Antonio, and
Calzolari, Nicoletta, (eds.) (forthcoming).
Automating the Lexicon. Oxford University
Press.
Weinreich, Uriel (1972). Explorations in
Semantic Theory. The Hague: Mouton.
Wierzbicka, Anna (1988). The Semantics of
Grammar. Amsterdam: John Benjamins.
Wilks, Yorick (1975a). &amp;quot;A preferential
pattern seeking semantics for natural
language inference.&amp;quot; Artificial Intelligence,
6: 53-74.
Wilks, Yorick (1975b). &amp;quot;An intelligent
analyser and understander for English.&amp;quot;
Communications of the ACM, 18: 264-274.
Wilks, Yorick; Fass, Dan; Guo, Cheng-Ming;
McDonald, James; Plate, Tony; and Slator,
Brian (1988). &amp;quot;A tractable machine
dictionary as a resource for computational
semantics.&amp;quot; In Computational Lexicography
for Natural Language Processing, edited by
Bran Boguraev and Ted Briscoe. Harlow,
Essex: Longman.
Williams, Edwin (1981). &amp;quot;Argument
structure and morphology&amp;quot; Linguistic
Review, 1:81-114.
von Wright, Georg H. (1963). Norm and
Action: A Logical Inquiry. London:
Routledge and Kegan Paul.
</reference>
<page confidence="0.99869">
441
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982369">
<title confidence="0.999904">The Generative Lexicon</title>
<author confidence="0.999719">James Pustejovsky</author>
<affiliation confidence="0.9998415">Computer Science Department Brandeis University</affiliation>
<abstract confidence="0.998900875">In this paper, I will discuss four major topics relating to current research in lexical semantics: methodology, descriptive coverage, adequacy of the representation, and the computational usefulness of representations. In addressing these issues, I will discuss what I think are some of the central problems facing the lexical semantics community, and suggest ways of best approaching these issues. Then, I will provide a method for the decomposition of lexical categories outline a theory of lexical semantics embodying a notion of well as several levels of semantic description, where the semantic load is spread more evenly throughout the lexicon. I argue that lexical decomposition is possible if it is perthan assuming a fixed set of primitives, I will assume a fixed number of generative devices that can be seen as constructing semantic expressions. I develop theory of Structure, representation language for lexical items, which renders much lexical ambiguity in the lexicon unnecessary, while still explaining the systematic polysemy that words carry. Finally, I discuss how individual lexical structures can be integrated into the lexical knowledge base through a theory of inheritance. provides us with the necessary principles of global organization for the lexicon, enabling us to fully integrate our natural language lexicon into a conceptual whole.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Anick</author>
<author>J Pustejovsky</author>
</authors>
<title>An application of lexical semantics to knowledge acquisition from corpora.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 13th International Conference on Computational Linguistics,</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="67204" citStr="Anick and Pustejovsky (1990)" startWordPosition="10824" endWordPosition="10827">Example 58 a. The prisoner escaped last night. b. The prisoner ate dinner last night. Both of these sentences are obviously well-formed syntactically, but there is a definite sense that the predication in 58(a) is &amp;quot;tighter&amp;quot; or more prototypical than that in 58(b). What would account for such a difference? Intuitively, we associate prisoner with an escaping event more strongly than an eating event. Yet this is not information that comes from a fixed inheritance structure, but is rather usually assumed to be cornmonsense knowledge. In what follows, however, I will show that such distinctions 25 Anick and Pustejovsky (1990) explore how metrics such as association ratios can be used to statistically measure the notions of prototypicality mentioned here. 433 Computational Linguistics Volume 17, Number 4 can be captured within a theory of lexical semantics by means of generating ad hoc categories. First, we give a definition for the fixed inheritance structure of a lexical item (cf. Touretzky 1986). Let Q and P be concepts in our model of lexical organization. Then: Definition A sequence (Qi , PO is an inheritance path, which can be read as the conjunction of ordered pairs {(xi, y,) Ii &lt; i &lt; n}. Furthermore, follow</context>
</contexts>
<marker>Anick, Pustejovsky, 1990</marker>
<rawString>Anick, P., and Pustejovsky, J. (1990). &amp;quot;An application of lexical semantics to knowledge acquisition from corpora.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beryl T Atkins</author>
</authors>
<title>Semantic ID tags: corpus evidence for dictionary senses.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, 3rd Annual Conference</booktitle>
<institution>at University of Waterloo, Center for the New OED.</institution>
<contexts>
<context position="14236" citStr="Atkins (1987)" startWordPosition="2241" endWordPosition="2242"> a fast tempo Such data raise serious questions about the principles of compositionality and how ambiguity should be accounted for by a theory of semantics. This just briefly characterizes some of the techniques that have been useful for arriving at pre-theoretic notions of word meaning. What has changed over the years are not so much the methods themselves as the descriptive details provided by each test. One thing that has changed, however — and this is significant — is the way computational lexicography has provided stronger techniques and even new tools for lexical semantics research: see Atkins (1987) for sense discrimination tasks; Amsler (1985), Atkins et al. (forthcoming) for constructing concept taxonomies; Wilks et al. (1988) for establishing semantic relatedness among word senses; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. 3. Descriptive Adequacy of Existing Representations Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently bee</context>
</contexts>
<marker>Atkins, 1987</marker>
<rawString>Atkins, Beryl T. (1987). &amp;quot;Semantic ID tags: corpus evidence for dictionary senses.&amp;quot; In Proceedings, 3rd Annual Conference at University of Waterloo, Center for the New OED.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beryl Atkins</author>
<author>Judy Kegl</author>
<author>Beth Levin</author>
</authors>
<title>Anatomy of a verb entry.&amp;quot;</title>
<date>1988</date>
<journal>International Journal of Lexicography,</journal>
<pages>1--84</pages>
<marker>Atkins, Kegl, Levin, 1988</marker>
<rawString>Atkins, Beryl; Kegl, Judy; and Levin, Beth (1988). &amp;quot;Anatomy of a verb entry.&amp;quot; International Journal of Lexicography, 1:84-126.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Beryl Atkins</author>
<author>Judith Klavans</author>
<author>Bran Boguraev</author>
</authors>
<title>Semantic verb clusters from MRDs.&amp;quot; Forthcoming.</title>
<marker>Atkins, Klavans, Boguraev, </marker>
<rawString>Atkins, Beryl; Klavans, Judith; and Boguraev, Bran. &amp;quot;Semantic verb clusters from MRDs.&amp;quot; Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmon Bach</author>
</authors>
<title>The algebra of events.</title>
<date>1986</date>
<journal>Linguistics and Philosophy,</journal>
<volume>9</volume>
<pages>5--16</pages>
<contexts>
<context position="35264" citStr="Bach (1986)" startWordPosition="5602" endWordPosition="5603">mmer2 = cause(x, Become(flat(Y)))) Example 24 a. John wiped the table. change(x, State(y))) b. John wiped the table clean. (wipe2 = cause(x, Become(clean(y)))) Example 25 a. Mary ran yesterday. (runi = move(x)) b. Mary ran to the store yesterday. (run2 = go-to(x,y)) • Although the complement types selected by bake in 22, for example, are semantically related, the two word senses are clearly distinct and therefore must be lexically distinguished. According to the sense enumeration view, the same argument holds for the verbs in 23-25 as well. 8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984). For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual distinctions are mapped to the syntax. 420 James Pustejovsky The Generative Lexicon A similar philosophy has lead linguists to multiply word senses in constructions involving Control and Equi-verbs, where different syntactic contexts necessitate different semantic types.&apos; Example 26 a. It seems that John likes Mary. b. John seems to like Mary. Example 27 a. Mary prefers that she come. b. Mary prefers to come. Normally, compositionality in such structure</context>
</contexts>
<marker>Bach, 1986</marker>
<rawString>Bach, Emmon (1986). &amp;quot;The algebra of events. Linguistics and Philosophy, 9: 5-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barselou</author>
</authors>
<title>Ad hoc categories.&amp;quot;</title>
<date>1983</date>
<journal>Memory and Cognition,</journal>
<pages>11--211</pages>
<contexts>
<context position="70417" citStr="Barselou 1983" startWordPosition="11352" endWordPosition="11353">James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categories and relations associated with a lexical item (cf. Barselou 1983). Using these definitions, let us return to the sentences in Example 58. I will assume that the noun prisoner has a qualia structure such as that shown in 60. Example 60 Qualia Structure of prisoner(x): prisoner (*x*) Form: human(*x*) Telic: [confine(y,*x*) &amp; location(*x*,prison)] Furthermore, I assume the following lexical structure for escape. Example 61 Lexical Semantics for escape: AxAeT]eP , es [escape(eT) A act(e) A confined(eP) A agent(eP , x) A —confined(es) A object(es , x)] Using the representation in 60 above, I now trace part of the derivation of the projective conclusion space for</context>
</contexts>
<marker>Barselou, 1983</marker>
<rawString>Barselou, Lawrence W. (1983) &amp;quot;Ad hoc categories.&amp;quot; Memory and Cognition, 11:211-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>G Miller</author>
</authors>
<title>WordNet: A lexical database organized on psycholinguistic principles.&amp;quot;</title>
<date>1989</date>
<booktitle>Proceedings, First International Workshop on Lexical Acquisition, IJCAI,</booktitle>
<location>Detroit.</location>
<contexts>
<context position="69771" citStr="Beckwith et al. 1989" startWordPosition="11243" endWordPosition="11246">udes: negation, &lt;, temporal precedence, &gt;, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categories and relations asso</context>
</contexts>
<marker>Beckwith, Fellbaum, Gross, Miller, 1989</marker>
<rawString>Beckwith, Richard; Fellbaum, C.; Gross, D.; and Miller, G. (1989). &amp;quot;WordNet: A lexical database organized on psycholinguistic principles.&amp;quot; Proceedings, First International Workshop on Lexical Acquisition, IJCAI, Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
<author>T Winograd</author>
</authors>
<title>An overview of KRL, a knowledge representation language.&amp;quot;</title>
<date>1977</date>
<journal>Cognitive Science,</journal>
<pages>1--3</pages>
<contexts>
<context position="65043" citStr="Bobrow and Winograd 1977" startWordPosition="10489" endWordPosition="10492">a given context; that is, how this semantic information is organized as a global knowledge base. This involves capturing both the inheritance relations between concepts and, just as importantly, how the concepts are integrated into a coherent expression in a given sentence. I will assume that there are two inheritance mechanisms at work for representing the conceptual relations in the lexicon: fixed inheritance and projective inheritance. The first includes the methods of inheritance traditionally assumed in Al and lexical research (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures</context>
</contexts>
<marker>Bobrow, Winograd, 1977</marker>
<rawString>Bobrow, D. G., and Winograd, T. (1977). &amp;quot;An overview of KRL, a knowledge representation language.&amp;quot; Cognitive Science, 1:3-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bran Boguraev</author>
<author>Ted Briscoe</author>
</authors>
<date>1988</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<publisher>Longman.</publisher>
<location>Harlow, Essex:</location>
<marker>Boguraev, Briscoe, 1988</marker>
<rawString>Boguraev, Bran, and Briscoe, Ted (eds.) (1988). Computational Lexicography for Natural Language Processing. Harlow, Essex: Longman.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bran Boguraev</author>
<author>James Pustejovsky</author>
</authors>
<title>Lexical Knowledge: Representation and Acquisition. Forthcoming.</title>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Boguraev, Pustejovsky, </marker>
<rawString>Boguraev, Bran, and Pustejovsky, James. Lexical Knowledge: Representation and Acquisition. Forthcoming. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald J Brachman</author>
</authors>
<title>On the epistemelogical status of semantic networks.&amp;quot;</title>
<date>1979</date>
<booktitle>In Associative Networks: Representation and Use of Knowledge by Computers, edited by N. Findler.</booktitle>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="22647" citStr="Brachman 1979" startWordPosition="3582" endWordPosition="3583">ve-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approa</context>
</contexts>
<marker>Brachman, 1979</marker>
<rawString>Brachman, Ronald J. (1979). &amp;quot;On the epistemelogical status of semantic networks.&amp;quot; In Associative Networks: Representation and Use of Knowledge by Computers, edited by N. Findler. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Brachman</author>
<author>J Schmolze</author>
</authors>
<title>An overview of the KL-ONE knowledge representation system.&amp;quot;</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<pages>9--171</pages>
<contexts>
<context position="65016" citStr="Brachman and Schmolze 1985" startWordPosition="10485" endWordPosition="10488"> associations a word has in a given context; that is, how this semantic information is organized as a global knowledge base. This involves capturing both the inheritance relations between concepts and, just as importantly, how the concepts are integrated into a coherent expression in a given sentence. I will assume that there are two inheritance mechanisms at work for representing the conceptual relations in the lexicon: fixed inheritance and projective inheritance. The first includes the methods of inheritance traditionally assumed in Al and lexical research (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on sh</context>
</contexts>
<marker>Brachman, Schmolze, 1985</marker>
<rawString>Brachman, R. J., and Schmolze, J. (1985). &amp;quot;An overview of the KL-ONE knowledge representation system.&amp;quot; Cognitive Science, 9:171-216.</rawString>
</citation>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<editor>Bresnan, Joan (ed.)</editor>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>1982</marker>
<rawString>Bresnan, Joan (ed.) (1982). The Mental Representation of Grammatical Relations. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>A Copestake</author>
<author>B Boguraev</author>
</authors>
<title>Enjoy the paper: lexical semantics via lexicology.&amp;quot; In</title>
<date>1990</date>
<booktitle>Proceedings, 13th International Conference on Computational Linguistics,</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="56464" citStr="Briscoe et al. (1990)" startWordPosition="9113" endWordPosition="9116"> any syntactic transformations (cf. Pustejovsky [1989a] for details).21 The derivation in 49(a) and the structure in 49(b) show the effects of this coercion on the verb&apos;s complement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. 1. Function Application with Coercion (FAO: If a is of type (b, a), and is of type</context>
<context position="74123" citStr="Briscoe et al. (1990)" startWordPosition="11958" endWordPosition="11961">nce escape(T, *x*) escaped 436 James Pustejovsky The Generative Lexicon in semanticality can now be seen as a structural distinction between the semantic representations for the elements in the sentence. In this section, I have shown how the lexical inheritance structure of an item relates, in a generative fashion, the decompositional structure of a word to a much larger set of concepts that are related in obvious ways. What we have not addressed, however, is how the fixed inheritance information of a lexical item is formally derivable during composition. This issue is explicitly addressed in Briscoe et al. (1990) as well as Pustejovsky and Briscoe (1991). 8. Conclusion In this paper I have outlined a framework for lexical semantic research that I believe can be useful for both computational linguists and theoretical linguists alike. I argued against the view that word meanings are fixed and inflexible, where lexical ambiguity must be treated by multiple word entries in the lexicon. Rather, the lexicon can be seen as a generative system, where word senses are related by logical operations defined by the well-formedness rules of the semantics. In this view, much of the lexical ambiguity of verbs and pre</context>
</contexts>
<marker>Briscoe, Copestake, Boguraev, 1990</marker>
<rawString>Briscoe, E., Copestake, A., and Boguraev, B. (1990). &amp;quot;Enjoy the paper: lexical semantics via lexicology.&amp;quot; In Proceedings, 13th International Conference on Computational Linguistics, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cardelli</author>
<author>P Wegner</author>
</authors>
<title>On understanding types, data abstraction, and polymorphism.&amp;quot;</title>
<date>1985</date>
<journal>ACM Computing Surveys,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>471--522</pages>
<marker>Cardelli, Wegner, 1985</marker>
<rawString>Cardelli, L., and Wegner, P. (1985). &amp;quot;On understanding types, data abstraction, and polymorphism.&amp;quot; ACM Computing Surveys, 17(4): 471-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Carlson</author>
</authors>
<title>Reference to kinds in English.&amp;quot; Doctoral dissertation,</title>
<date>1977</date>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="40887" citStr="Carlson 1977" startWordPosition="6538" endWordPosition="6539"> as cospecifications. Informally, relative to the process bake, the noun cake carries the selectional information that it is a process of &amp;quot;baking&amp;quot; that brings it about.13 12 However, relative to the process of growing, the noun potato does denote an event: 1. Mary grew the potato. 13 Other examples of cospecifications are: a. read a book, b. smoke a cigarette, c. mail a letter, d. deliver a lecture, and e. take a bath. 422 James Pustejovsky The Generative Lexicon We can illustrate this schematically in Example 31, where the complement effectively acts like a &amp;quot;stage-level&amp;quot; event predicate (cf. Carlson 1977) relative to the process event-type of the verb (i.e. a function from processes to transitions, &lt;P , T&gt;).14 The change in meaning in 31 comes not from the semantics of bake, but rather in composition with the complement of the verb, at the level of the entire verb phrase. The &amp;quot;creation&amp;quot; sense arises from the semantic role of cake that specifies it is an artifact (see below for discussion). Example 31 a. bake as a derived Transition:15 2e&apos;, es [create (eP , es) A bake(eP) A agent(eP , j) A object(eP , x) A cake(es) A object (es , x)] &lt;P,T&gt; John baked a cake V NP VP Thus, we can derive both word</context>
</contexts>
<marker>Carlson, 1977</marker>
<rawString>Carlson, Gregory (1977). &amp;quot;Reference to kinds in English.&amp;quot; Doctoral dissertation, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Carnap</author>
</authors>
<title>Meaning and Necessity.</title>
<date>1956</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="22631" citStr="Carnap 1956" startWordPosition="3580" endWordPosition="3581">ning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primiti</context>
</contexts>
<marker>Carnap, 1956</marker>
<rawString>Carnap, Rudolf (1956). Meaning and Necessity. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Robert Goldman</author>
</authors>
<title>A logic for semantic interpretation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Buffalo, NY.</location>
<contexts>
<context position="8269" citStr="Charniak and Goldman 1988" startWordPosition="1270" endWordPosition="1273">nvolving many different generative factors that account for the way that language users create and manipulate the context under constraints, in order to be understood. Within such a theory, where many separate semantic levels (e.g. lexical semantics, compositional semantics, discourse structure, temporal structure) have independent interpretations, the global interpretation of a &amp;quot;discourse&amp;quot; is a highly flexible and malleable structure that has no single interpretation. The individual sources of semantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences must be globally coherent, a state that is accomplished by processes of cooperation among separate semantic modules. The basic result of such a view is that semantic interpretation proceeds in a principled fashion, always aware of what the source of a particular inference is, and what the certainty of its value is. Such an approach allows the reasoning process to be both tractable and computationally efficient. The representation of lexical semantics, therefore, should be seen as just one of many levels in a richer characterization of contextual str</context>
</contexts>
<marker>Charniak, Goldman, 1988</marker>
<rawString>Charniak, Eugene, and Goldman, Robert (1988). &amp;quot;A logic for semantic interpretation.&amp;quot; In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics, Buffalo, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Chierchia</author>
</authors>
<title>Structured meanings, thematic roles, and control.&amp;quot;</title>
<date>1989</date>
<booktitle>In Properties, Types, and Meaning,</booktitle>
<volume>2</volume>
<publisher>Dordrect: Kluwer Academic Publishers.</publisher>
<contexts>
<context position="38182" citStr="Chierchia 1989" startWordPosition="6079" endWordPosition="6080">an distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el&apos; should reflect that it is the substance contained in the object x that is affected. See footnote 20 for explanation. 421 Computational Linguistics Volume 17, Number 4 is a natural kind, while the latter is an artifact. Knowledge of an object includes not just being able to identify or refer, but more specifically, being able to explain how an artifact comes into being, as well as what it is used for; the denotation of an object must identify these roles. Thus, any artifact can be identified with the state of being that object, relative to certain predicates</context>
</contexts>
<marker>Chierchia, 1989</marker>
<rawString>Chierchia, G. (1989). &amp;quot;Structured meanings, thematic roles, and control.&amp;quot; In Properties, Types, and Meaning, Volume 2, edited by G. Chierchia, B. Partee, and R. Turner. Dordrect: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>The Logical Structure of Linguistic Theory.</title>
<date>1975</date>
<publisher>Chicago: University of Chicago Press.</publisher>
<marker>Chomsky, 1975</marker>
<rawString>Chomsky, Noam (1975). The Logical Structure of Linguistic Theory. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Dordrect:</booktitle>
<publisher>Foris Publications.</publisher>
<contexts>
<context position="22831" citStr="Chomsky (1981)" startWordPosition="3613" endWordPosition="3614">s 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a se</context>
<context position="32116" citStr="Chomsky 1981" startWordPosition="5095" endWordPosition="5096">l language and its interpretation into a knowledge representation model. 5.1 Argument Structure A logical starting point for our investigations into the meaning of words is what has been called the functional structure or argument structure associated with verbs. What originally began as the simple listing of the parameters or arguments associated with a predicate has developed into a sophisticated view of the way arguments are mapped onto syntactic expressions (for example, the f-structure in Lexical Functional Grammar [Bresnan 1982] and the Projection Principle in Government-Binding Theory [Chomsky 1981]). One of the most important contributions has been the view that argument structure is highly structured independent of the syntax. Williams&apos;s (1981) distinction between external and internal arguments and Grimshaw&apos;s proposal for a hierarchically structured representation (Grimshaw 1990) provide us with the basic syntax for one aspect of a word&apos;s meaning. The argument structure for a word can be seen as a minimal specification of its lexical semantics. By itself, it is certainly inadequate for capturing the semantic characterization of a lexical item, but it is a necessary component. 5.2 Eve</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam (1981). Lectures on Government and Binding. Dordrect: Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Collins</author>
<author>M Quillian</author>
</authors>
<title>Retrieval time from semantic memory.&amp;quot;</title>
<date>1969</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>9</volume>
<pages>240--247</pages>
<contexts>
<context position="22606" citStr="Collins and Quillian 1969" startWordPosition="3574" endWordPosition="3577">nct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a sp</context>
</contexts>
<marker>Collins, Quillian, 1969</marker>
<rawString>Collins, A., and Quillian, M. (1969). &amp;quot;Retrieval time from semantic memory.&amp;quot; Journal of Verbal Learning and Verbal Behavior, 9: 240-247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>T Briscoe</author>
</authors>
<title>Lexical operations in a unification-based framework.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, SIGLEX Workshop on Lexical Semantics and Knowledge Representation, edited</booktitle>
<location>Berkeley, CA.</location>
<marker>Copestake, Briscoe, 1991</marker>
<rawString>Copestake, A., and Briscoe, T. (1991). &amp;quot;Lexical operations in a unification-based framework.&amp;quot; In Proceedings, SIGLEX Workshop on Lexical Semantics and Knowledge Representation, edited by J. Pustejovsky and S. Bergler, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
</authors>
<title>Categories and Relations in Syntax: The Clause-Level Organization of Information. Chicago:</title>
<date>1991</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="33879" citStr="Croft (1991)" startWordPosition="5374" endWordPosition="5375">tructure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure. Because it is recursively defined on the syntax, it is also a property of phrases and sentences.&apos; I will assume a sortal distinction between three classes of events: states (es), processes (eP), and transitions (eT ) . Unlike most previous sortal classifications for events, I will adopt a subeventual analysis or predicates, as argued in Pustejovsky (1991) and independently proposed in Croft (1991). In this view, an event sort such as eT may be decomposed into two sequentially structured subevents, (e&amp;quot;, ss). Aspects of the proposal will be introduced as needed in the following discussion. 6. A Theory of Qualia In Section 5, I demonstrated how most of the lexical semantics research has concentrated on verbal semantics. This bias influences our analyses of how to handle ambiguity and certain noncompositional structures. Therefore, the only way to relate the different senses for the verbs in the examples below was to posit separate entries. Example 22 a. John baked the potato. (bakei -= ch</context>
<context position="50394" citStr="Croft (1991)" startWordPosition="8119" endWordPosition="8120"> qualia structure for a lexical item. Here I will elaborate on what these roles are and why they are useful. They are given in Example 38, where each role is defined, along with the possible values that these roles may assume. Example 38 The Structure of Qualia: 1. Constitutive Role: the relation between an object and its constituents, or proper parts. • Material • Weight • Parts and component elements 19 These components of an object&apos;s denotation have long been considered crucial for our commonsense understanding of how things interact in the world. Cf. Hayes (1979), Hobbs et al. (1987), and Croft (1991) for discussion of these qualitative aspects of meaning. 426 James Pustejovsky The Generative Lexicon 2. Formal Role: that which distinguishes the object within a larger domain. • Orientation • Magnitude • Shape • Dimensionality • Color • Position 3. Telic Role: purpose and function of the object. • Purpose that an agent has in performing an act • Built-in function or aim that specifies certain activities 4. Agentive Role: factors involved in the origin or &amp;quot;bringing about&amp;quot; of an object. • Creator • Artifact • Natural Kind • Causal Chain When we combine the qualia structure of a NP with the arg</context>
</contexts>
<marker>Croft, 1991</marker>
<rawString>Croft, William (1991). Categories and Relations in Syntax: The Clause-Level Organization of Information. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge, U.K.:</location>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. A. (1986). Lexical Semantics. Cambridge, U.K.: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M W Dixon</author>
</authors>
<title>A New Approach to English Grammar,</title>
<date>1991</date>
<booktitle>on Semantic Principles,</booktitle>
<publisher>University Press.</publisher>
<location>Oxford, U.K., Oxford</location>
<marker>Dixon, 1991</marker>
<rawString>Dixon, R. M. W. (1991). A New Approach to English Grammar, on Semantic Principles, Oxford, U.K., Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar.</title>
<date>1979</date>
<location>Dordrecht: D. Reidel.</location>
<contexts>
<context position="19538" citStr="Dowty (1979)" startWordPosition="3090" endWordPosition="3091">een done on verb classes. In fact, the semantic weight in both lexical and compositional terms usually falls on the verb. This has obvious consequences for how to treat lexical ambiguity. For example, consider the verb bake in the two sentences below. Example 15 a. John baked the potato. b. John baked the cake. Atkins, Kegl, and Levin (1988) demonstrate that verbs such as bake are systematically ambiguous, with both a change-of-state sense (15a) and a create sense (15b). A similar ambiguity exists with verbs that allow the resulative construction, shown in Examples 16 and 17, and discussed in Dowty (1979), Jackendoff (1983), and Levin and Rapoport (1988). Example 16 a. Mary hammered the metal. b. Mary hammered the metal flat. Example 17 a. John wiped the table. b. John wiped the table clean. On many views, the verbs in Examples 16 and 17 are ambiguous, related by either a lexical transformation (Levin and Rapoport 1988), or a meaning postulate (Dowty 1979). In fact, given strict requirements on the way that a verb can project its lexical information, the verb run in Example 18 will also have two lexical entries, depending on the syntactic environment it selects (Talmy 1985; Levin and Rappaport</context>
<context position="23599" citStr="Dowty 1979" startWordPosition="3735" endWordPosition="3736">enerative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would tell us the minimal semantic configuration of a lexical item. Furthermore, it should tell us the compositio</context>
<context position="43618" citStr="Dowty 1979" startWordPosition="7004" endWordPosition="7005">emantics is expressed in these examples. Similar principles seem to be operating in the resultative constructions in Examples 23 and 24; namely, a systematic ambiguity is the result of principles of semantic composition rather than lexical ambiguity of the verbs. For example, the resultative interpretations for the verbs hammer in 23(b) and wipe in 24(b) arise from a similar operation, where both verbs are underlyingly specified with an event type of process. The adjectival phrases flat and clean, although clearly stative in nature, can also be interpreted as stage-level event predicates (cf. Dowty 1979). Notice, then, how the resultative construction requires no additional word sense for the verb, nor any special semantic machinery for the resultative interpretation to be available. Schematically, this is shown in Example 32. Example 32 John hammer &lt;P,T&gt; the metal flat NP AP VP In fact, this analysis explains why it is that only process verbs participate in the resultative construction, and why the resultant phrase (the adjectival phrase) must be a subset of the states, namely, stage-level event predicates. Because the meaning of the sentence in 32 is determined by both function application </context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>Dowty, David R. (1979). Word Meaning and Montague Grammar. Dordrecht: D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
</authors>
<title>On some recent analyses of control.&amp;quot;</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<volume>8</volume>
<pages>1--41</pages>
<contexts>
<context position="20589" citStr="Dowty (1985)" startWordPosition="3259" endWordPosition="3260">al information, the verb run in Example 18 will also have two lexical entries, depending on the syntactic environment it selects (Talmy 1985; Levin and Rappaport 1988). 415 Computational Linguistics Volume 17, Number 4 Example 18 a. Mary ran to the store yesterday. b. Mary ran yesterday. These two verbs differ in their semantic representations, where run in 18a means goto-by-means-of-running, while in 18b it means simply move-by-running (cf. Jackendoff 1983). The methodology described above for distinguishing word senses is also assumed by those working in more formal frameworks. For example, Dowty (1985) proposes multiple entries for control and raising verbs, and establishes their semantic equivalence with the use of meaning postulates. That is, the verbs in Examples 19 and 20 are lexically distinct but semantically related by rules.3 Example 19 a. It seems that John likes Mary. b. John seems to like Mary. Example 20 a. Mary prefers that she come. b. Mary prefers to come. Given the conventional notions of function application and composition, there is little choice but to treat all of the above cases as polysemous verbs. Yet, something about the systematicity of such ambiguity suggests that </context>
<context position="37677" citStr="Dowty (1985)" startWordPosition="6003" endWordPosition="6004">ocess verb, and is minimally represented as Example 29.&amp;quot; Example 29 Lexical Semantics for bake:11 AyAxAeP [bake(eP) A agent (eP , x) A object (el&apos; , y)] In order to explain the shift in meaning of the verb, we need to specify more clearly what the lexical semantics of a noun is. I have argued above that lexical semantic theory must make a logical distinction between the following qualia roles: the constitutive, formal, telic, and agentive roles. Now let us examine these roles in more detail. One can distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el&apos; should reflect that it is the substance contained in the o</context>
</contexts>
<marker>Dowty, 1985</marker>
<rawString>Dowty, David R. (1985). &amp;quot;On some recent analyses of control.&amp;quot; Linguistics and Philosophy, 8: 1-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
</authors>
<title>On the semantic content of the notion &apos;thematic role&apos;.&amp;quot;</title>
<date>1989</date>
<booktitle>In Properties, Types, and Meaning Volume II,</booktitle>
<publisher>Dordrect: Kluwer Academic Publishers.</publisher>
<note>edited by</note>
<contexts>
<context position="38162" citStr="Dowty 1989" startWordPosition="6076" endWordPosition="6077">e detail. One can distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el&apos; should reflect that it is the substance contained in the object x that is affected. See footnote 20 for explanation. 421 Computational Linguistics Volume 17, Number 4 is a natural kind, while the latter is an artifact. Knowledge of an object includes not just being able to identify or refer, but more specifically, being able to explain how an artifact comes into being, as well as what it is used for; the denotation of an object must identify these roles. Thus, any artifact can be identified with the state of being that object, relative t</context>
</contexts>
<marker>Dowty, 1989</marker>
<rawString>Dowty, David R. (1989). &amp;quot;On the semantic content of the notion &apos;thematic role&apos;.&amp;quot; In Properties, Types, and Meaning Volume II, edited by G. Chierchia, B. Partee, and R. Turner. Dordrect: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Elliott</author>
</authors>
<title>Towards a grammar of exclamations.&amp;quot;</title>
<date>1974</date>
<journal>Foundations of Language,</journal>
<pages>11--231</pages>
<contexts>
<context position="48474" citStr="Elliott 1974" startWordPosition="7794" endWordPosition="7795">ymy and coercion involving experiencer verbs, while those in 35 show the different metonymic extensions possible from the causing event in a killing. The generalization here is that when a verb selects an event as one of its arguments, type coercion to an event will permit a limited range of logical metonymies. For example, in sentences 34(a,b,c,d,f,h), the entire event is directly referred to, while in 34(e,g) only a participant from the coerced event reading is directly expressed. Other examples of coercion include &amp;quot;concealed questions&amp;quot; 36 and &amp;quot;concealed exclamations&amp;quot; 37 (cf. Grimshaw 1979; Elliott 1974). Example 36 a. John knows the plane&apos;s arrival time. (= what time the plane will arrive) b. Bill figured out the answer. (= what the answer is) Example 37 a. John shocked me with his bad behavior. (= how bad his behavior is) b. You&apos;d be surprised at the big cars he buys. (= how big the cars he buys are) That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical s</context>
</contexts>
<marker>Elliott, 1974</marker>
<rawString>Elliott, D. E. (1974). &amp;quot;Towards a grammar of exclamations.&amp;quot; Foundations of Language, 11:231-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>Inference in DATR.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, Fourth European ACL Conference.</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="65721" citStr="Evans and Gazdar (1989" startWordPosition="10600" endWordPosition="10603">sed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a description of projective inheritance and the notion of &amp;quot;degrees of prototypicality&amp;quot; of predication. I will argue that such degrees of salienc</context>
</contexts>
<marker>Evans, Gazdar, 1989</marker>
<rawString>Evans, Roger, and Gazdar, Gerald (1989). &amp;quot;Inference in DATR.&amp;quot; In Proceedings, Fourth European ACL Conference. Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Gerald Gazdar</author>
</authors>
<title>The DATR papers:</title>
<date>1990</date>
<booktitle>Cognitive Science Research Paper CSRP 139, School of Cognitive and</booktitle>
<institution>Computing Science, University of Sussex,</institution>
<location>Brighton, England.</location>
<marker>Evans, Gazdar, 1990</marker>
<rawString>Evans, Roger, and Gazdar, Gerald (1990). &amp;quot;The DATR papers: February 1990,&amp;quot; Cognitive Science Research Paper CSRP 139, School of Cognitive and Computing Science, University of Sussex, Brighton, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>Collative semantics: A semantics for natural language processing,&amp;quot;</title>
<date>1988</date>
<pages>99--118</pages>
<institution>Computing Research Laboratory, New Mexico State University.</institution>
<contexts>
<context position="45108" citStr="Fass (1988)" startWordPosition="7248" endWordPosition="7249">of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Example 33.16 Example 33 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987) c. John began a novel. 16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpretations? I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a), where the verb has coerced the meani</context>
</contexts>
<marker>Fass, 1988</marker>
<rawString>Fass, Dan (1988). &amp;quot;Collative semantics: A semantics for natural language processing,&amp;quot; MCCS-99-118, Computing Research Laboratory, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Fauconnier</author>
</authors>
<title>Mental Spaces.</title>
<date>1985</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="44983" citStr="Fauconnier (1985)" startWordPosition="7227" endWordPosition="7228"> [forthcoming] for discussion). Having discussed some of the behavior of logical polysemy in verbs, let us continue our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Example 33.16 Example 33 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987) c. John began a novel. 16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpreta</context>
</contexts>
<marker>Fauconnier, 1985</marker>
<rawString>Fauconnier, G. (1985). Mental Spaces. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>The case for case.&amp;quot; In Universals in Linguistic Theory,</title>
<date>1968</date>
<location>New York: Holt, Rinehart, and Winston.</location>
<note>edited by</note>
<contexts>
<context position="4842" citStr="Fillmore 1968" startWordPosition="725" endWordPosition="726">o characterize a theory of possible word meaning. This may entail abstracting the notion of lexical meaning away from other semantic influences. For instance, this might suggest that discourse and pragmatic factors should be handled differently or separately from the semantic contributions of lexical items in composition.&apos; Although this is not a necessary assumption and may in fact be wrong, it may help narrow our focus on what is important for lexical semantic descriptions. Secondly, lexical semantics must look for representations that are richer than thematic role descriptions (Gruber 1965; Fillmore 1968). As argued in Levin and Rappaport (1986), named roles are useful at best for establishing fairly general mapping strategies to the syntactic structures in language. The distinctions possible with thetaroles are much too coarse-grained to provide a useful semantic interpretation of a sentence. What is needed, I will argue, is a principled method of lexical decomposition. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic composition, (2) the notion of semantic well-formedness mentioned above, and (3) an appeal to several levels of interpretation in the semantic</context>
<context position="10085" citStr="Fillmore 1968" startWordPosition="1557" endWordPosition="1558">X. For example, verbs typically behave as predicators, nouns as arguments. • Find distinctions between elements of the same word class on the basis of collocation and cooccurrence tests. For example, the nouns dog and book partition into different selectional classes because of contexts involving animacy, while the nouns book and literature partition into different selectional classes because of a mass/count distinction. • Test for distinctions of a grammatical nature on the basis of diathesis; i.e. alternations that are realized in the syntax. For example, break vs. cut in (1) and (2) below (Fillmore 1968; Lakoff 1970; Hale and Keyser 1986): Example 1 a. The glass broke. b. John broke the glass. 411 Computational Linguistics Volume 17, Number 4 Example 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a wo</context>
<context position="15210" citStr="Fillmore (1968)" startWordPosition="2381" endWordPosition="2382">uestion of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and subtle distinctions are drawn that were not captured in the earlier, primitives-based approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968). As an example of the verb classifications developed by various researchers (and compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical alternations in the example sentences below (cf. Dowty 1991). Example 10 a. John met Mary. b. John and Mary met. Example 11 a. A car ran into a truck. b. A car and a truck ran into each other. Example 12 a. A car ran into a tree. b. *A car and a tree ran into each other. These three pairs show how the semantics of transitive motion verbs (e.g. run into) is similar in some respects to reciprocal verbs such as meet. The important</context>
</contexts>
<marker>Fillmore, 1968</marker>
<rawString>Fillmore, Charles (1968). &amp;quot;The case for case.&amp;quot; In Universals in Linguistic Theory, edited by E. Bach and R. Harms. New York: Holt, Rinehart, and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Fillmore</author>
</authors>
<title>Construction grammar.&amp;quot;</title>
<date>1985</date>
<journal>Ms.</journal>
<contexts>
<context position="23615" citStr="Fillmore 1985" startWordPosition="3737" endWordPosition="3738">xicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would tell us the minimal semantic configuration of a lexical item. Furthermore, it should tell us the compositional properties o</context>
</contexts>
<marker>Fillmore, 1985</marker>
<rawString>Fillmore, Charles (1985). &amp;quot;Construction grammar.&amp;quot; Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>T Wasow</author>
</authors>
<title>Structure-sharing in lexical representation.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the ACL.</booktitle>
<location>Chicago, IL.</location>
<contexts>
<context position="65694" citStr="Flickinger et al. (1985)" startWordPosition="10595" endWordPosition="10598">of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a description of projective inheritance and the notion of &amp;quot;degrees of prototypicality&amp;quot; of predication. I will argue t</context>
</contexts>
<marker>Flickinger, Pollard, Wasow, 1985</marker>
<rawString>Flickinger, D.; Pollard, C.; and Wasow, T. (1985). &amp;quot;Structure-sharing in lexical representation.&amp;quot; In Proceedings, 23rd Annual Meeting of the ACL. Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Fodor</author>
</authors>
<title>The Language of Thought.</title>
<date>1975</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="22618" citStr="Fodor 1975" startWordPosition="3578" endWordPosition="3579"> of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified numb</context>
</contexts>
<marker>Fodor, 1975</marker>
<rawString>Fodor, Jerry (1975). The Language of Thought. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Freed</author>
</authors>
<title>The Semantics of English Aspectual Complementation.</title>
<date>1979</date>
<location>Dordrecht: Reidel.</location>
<marker>Freed, 1979</marker>
<rawString>Freed, A. F. (1979). The Semantics of English Aspectual Complementation. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="37771" citStr="Gazdar et al. (1985)" startWordPosition="6013" endWordPosition="6016">for bake:11 AyAxAeP [bake(eP) A agent (eP , x) A object (el&apos; , y)] In order to explain the shift in meaning of the verb, we need to specify more clearly what the lexical semantics of a noun is. I have argued above that lexical semantic theory must make a logical distinction between the following qualia roles: the constitutive, formal, telic, and agentive roles. Now let us examine these roles in more detail. One can distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el&apos; should reflect that it is the substance contained in the object x that is affected. See footnote 20 for explanation. 421 Computational Linguistics Volum</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G.; Klein, E.; Pullum, G; and Sag, I. (1985). Generalized Phrase Structure Grammar. Cambridge, MA: Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelson Goodman</author>
</authors>
<title>The Structure of Appearance.</title>
<date>1951</date>
<location>Dordrecht: Reidel.</location>
<contexts>
<context position="26308" citStr="Goodman (1951)" startWordPosition="4169" endWordPosition="4170">(and conceptual) decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, let us assume a fixed number of generative devices that can be seen as constructing semantic expressions.&apos; Just as a formal language is described more in terms of the productions in the grammar than its accompanying vocabulary, a semantic language is definable by the rules generating the structures for expressions rather than the vocabulary of primitives itself.6 4 For further discussion on the advantages and disadvantages to both approaches, see Jackendoff (1983). 5 See Goodman (1951) and Chomsky (1955) for explanations of the method assumed here. 6 This approach is also better suited to the way people write systems in computational linguistics. Different people have distinct primitives for their own domains, and rather than committing a designer 417 Computational Linguistics Volume 17, Number 4 How might this be done? Consider the sentences in Example 21 again. A minimal decomposition on the word closed is that it introduces an opposition of terms: closed and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are predicated of different subeven</context>
</contexts>
<marker>Goodman, 1951</marker>
<rawString>Goodman, Nelson (1951). The Structure of Appearance. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Meaning.&amp;quot; In Semantics: An Interdisciplinary Reader in Philosophy, Linguistics, and Psychology,</title>
<date>1971</date>
<publisher>Cambridge University Press.</publisher>
<note>edited by</note>
<marker>Grice, 1971</marker>
<rawString>Grice, H. P. (1971). &amp;quot;Meaning.&amp;quot; In Semantics: An Interdisciplinary Reader in Philosophy, Linguistics, and Psychology, edited by D. Steinberg and L. Jacobovits. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Complement selection and the lexicon.&amp;quot; Linguistic Inquiry,</title>
<date>1979</date>
<pages>10--279</pages>
<contexts>
<context position="48459" citStr="Grimshaw 1979" startWordPosition="7792" endWordPosition="7793">uences of metonymy and coercion involving experiencer verbs, while those in 35 show the different metonymic extensions possible from the causing event in a killing. The generalization here is that when a verb selects an event as one of its arguments, type coercion to an event will permit a limited range of logical metonymies. For example, in sentences 34(a,b,c,d,f,h), the entire event is directly referred to, while in 34(e,g) only a participant from the coerced event reading is directly expressed. Other examples of coercion include &amp;quot;concealed questions&amp;quot; 36 and &amp;quot;concealed exclamations&amp;quot; 37 (cf. Grimshaw 1979; Elliott 1974). Example 36 a. John knows the plane&apos;s arrival time. (= what time the plane will arrive) b. Bill figured out the answer. (= what the answer is) Example 37 a. John shocked me with his bad behavior. (= how bad his behavior is) b. You&apos;d be surprised at the big cars he buys. (= how big the cars he buys are) That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that ar</context>
</contexts>
<marker>Grimshaw, 1979</marker>
<rawString>Grimshaw, Jane (1979). &amp;quot;Complement selection and the lexicon.&amp;quot; Linguistic Inquiry, 10:279-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>Argument Structure.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA:</location>
<note>in press.</note>
<contexts>
<context position="28943" citStr="Grimshaw 1990" startWordPosition="4601" endWordPosition="4602">e causer, yet the transition from not-closed to closed is still entailed. In 2k, the event that brings about the closed state of the door is made more explicit by specifying the actor involved. These differences constitute what I call the event structure of a lexical item. Both the opposition of predicates and the specification of causation are part of a verb&apos;s semantics, and are structurally associated with slots in the event template for the word. As we will see in the next section, there are different inferences associated with each event type, as well as different syntactic behaviors (cf. Grimshaw 1990 and Pustejovsky 1991). Because the lexical semantic representation of a word is not an isolated expression, but is in fact linked to the rest of the lexicon, in Section 7, I suggest how the global integration of the semantics for a lexical item is achieved by structured inheritance through the different qualia associated with a word. I call this the lexical inheritance structure for the word. Finally, we must realize that part of the meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called th</context>
<context position="32406" citStr="Grimshaw 1990" startWordPosition="5134" endWordPosition="5135"> the simple listing of the parameters or arguments associated with a predicate has developed into a sophisticated view of the way arguments are mapped onto syntactic expressions (for example, the f-structure in Lexical Functional Grammar [Bresnan 1982] and the Projection Principle in Government-Binding Theory [Chomsky 1981]). One of the most important contributions has been the view that argument structure is highly structured independent of the syntax. Williams&apos;s (1981) distinction between external and internal arguments and Grimshaw&apos;s proposal for a hierarchically structured representation (Grimshaw 1990) provide us with the basic syntax for one aspect of a word&apos;s meaning. The argument structure for a word can be seen as a minimal specification of its lexical semantics. By itself, it is certainly inadequate for capturing the semantic characterization of a lexical item, but it is a necessary component. 5.2 Event Structure As mentioned above, the theory of decomposition being outlined here is based on the central idea that word meaning is highly structured, and not simply a set of semantic features. Let us assume this is the case. Then the lexical items in a language will essentially be generate</context>
</contexts>
<marker>Grimshaw, 1990</marker>
<rawString>Grimshaw, Jane (1990). Argument Structure. Cambridge, MA: The MIT Press, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Gruber</author>
</authors>
<title>Studies in lexical relations.&amp;quot; Doctoral dissertation,</title>
<date>1965</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="4826" citStr="Gruber 1965" startWordPosition="723" endWordPosition="724">e necessary to characterize a theory of possible word meaning. This may entail abstracting the notion of lexical meaning away from other semantic influences. For instance, this might suggest that discourse and pragmatic factors should be handled differently or separately from the semantic contributions of lexical items in composition.&apos; Although this is not a necessary assumption and may in fact be wrong, it may help narrow our focus on what is important for lexical semantic descriptions. Secondly, lexical semantics must look for representations that are richer than thematic role descriptions (Gruber 1965; Fillmore 1968). As argued in Levin and Rappaport (1986), named roles are useful at best for establishing fairly general mapping strategies to the syntactic structures in language. The distinctions possible with thetaroles are much too coarse-grained to provide a useful semantic interpretation of a sentence. What is needed, I will argue, is a principled method of lexical decomposition. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic composition, (2) the notion of semantic well-formedness mentioned above, and (3) an appeal to several levels of interpretation</context>
</contexts>
<marker>Gruber, 1965</marker>
<rawString>Gruber, Jeffrey (1965). &amp;quot;Studies in lexical relations.&amp;quot; Doctoral dissertation, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hale</author>
<author>S J Keyser</author>
</authors>
<title>Some transitivity alternations in English.&amp;quot; Lexicon Project Working Papers 7,</title>
<date>1986</date>
<institution>Center for Cognitive Science, MIT.</institution>
<contexts>
<context position="10121" citStr="Hale and Keyser 1986" startWordPosition="1561" endWordPosition="1564">lly behave as predicators, nouns as arguments. • Find distinctions between elements of the same word class on the basis of collocation and cooccurrence tests. For example, the nouns dog and book partition into different selectional classes because of contexts involving animacy, while the nouns book and literature partition into different selectional classes because of a mass/count distinction. • Test for distinctions of a grammatical nature on the basis of diathesis; i.e. alternations that are realized in the syntax. For example, break vs. cut in (1) and (2) below (Fillmore 1968; Lakoff 1970; Hale and Keyser 1986): Example 1 a. The glass broke. b. John broke the glass. 411 Computational Linguistics Volume 17, Number 4 Example 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a word always entails a certain proposit</context>
</contexts>
<marker>Hale, Keyser, 1986</marker>
<rawString>Hale, Ken, and Keyser, S. J. (1986). &amp;quot;Some transitivity alternations in English.&amp;quot; Lexicon Project Working Papers 7, Center for Cognitive Science, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hale</author>
<author>S J Keyser</author>
</authors>
<title>A view from the middle.&amp;quot; Lexicon Project Working Papers 10,</title>
<date>1987</date>
<institution>Center for Cognitive Science, MIT.</institution>
<marker>Hale, Keyser, 1987</marker>
<rawString>Hale, Ken, and Keyser, S. J. (1987). &amp;quot;A view from the middle.&amp;quot; Lexicon Project Working Papers 10, Center for Cognitive Science, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hayes</author>
</authors>
<title>Naive physics manifesto.&amp;quot; In Expert Systems in the Micro-Electronic Age, edited by Donald Mitchie. Edinburgh:</title>
<date>1979</date>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="29975" citStr="Hayes (1979)" startWordPosition="4769" endWordPosition="4770">e that part of the meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure for a lexical item. I will build on Grimshaw&apos;s recent proposals (Grimshaw 1990) for how to define the mapping from the lexicon to syntax. to a particular vocabulary of primitives, a lexical semantics should provide a method for the decomposition and composition of lexical items. 7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles determine a minimal semantic description of a word that has both semantic and grammatical consequences. 418 James Pustejovsky The Generative Lexicon This provides us with an answer to the question of what levels of semantic representation are necessary for a computational lexical semantics. In sum, I will argue that lexical meaning can best be captured by assuming the following levels of representation. 1. Argument Structure: The behavior of a word as a function, with its arity specified. This is the predicate argument str</context>
<context position="50355" citStr="Hayes (1979)" startWordPosition="8112" endWordPosition="8113">re four basic roles that constitute the qualia structure for a lexical item. Here I will elaborate on what these roles are and why they are useful. They are given in Example 38, where each role is defined, along with the possible values that these roles may assume. Example 38 The Structure of Qualia: 1. Constitutive Role: the relation between an object and its constituents, or proper parts. • Material • Weight • Parts and component elements 19 These components of an object&apos;s denotation have long been considered crucial for our commonsense understanding of how things interact in the world. Cf. Hayes (1979), Hobbs et al. (1987), and Croft (1991) for discussion of these qualitative aspects of meaning. 426 James Pustejovsky The Generative Lexicon 2. Formal Role: that which distinguishes the object within a larger domain. • Orientation • Magnitude • Shape • Dimensionality • Color • Position 3. Telic Role: purpose and function of the object. • Purpose that an agent has in performing an act • Built-in function or aim that specifies certain activities 4. Agentive Role: factors involved in the origin or &amp;quot;bringing about&amp;quot; of an object. • Creator • Artifact • Natural Kind • Causal Chain When we combine th</context>
</contexts>
<marker>Hayes, 1979</marker>
<rawString>Hayes, Patrick (1979). &amp;quot;Naive physics manifesto.&amp;quot; In Expert Systems in the Micro-Electronic Age, edited by Donald Mitchie. Edinburgh: Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Higginbotham</author>
</authors>
<title>On semantics.&amp;quot;</title>
<date>1985</date>
<journal>Linguistic Inquiry,</journal>
<pages>16--547</pages>
<contexts>
<context position="35285" citStr="Higginbotham (1985)" startWordPosition="5604" endWordPosition="5605">(x, Become(flat(Y)))) Example 24 a. John wiped the table. change(x, State(y))) b. John wiped the table clean. (wipe2 = cause(x, Become(clean(y)))) Example 25 a. Mary ran yesterday. (runi = move(x)) b. Mary ran to the store yesterday. (run2 = go-to(x,y)) • Although the complement types selected by bake in 22, for example, are semantically related, the two word senses are clearly distinct and therefore must be lexically distinguished. According to the sense enumeration view, the same argument holds for the verbs in 23-25 as well. 8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984). For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual distinctions are mapped to the syntax. 420 James Pustejovsky The Generative Lexicon A similar philosophy has lead linguists to multiply word senses in constructions involving Control and Equi-verbs, where different syntactic contexts necessitate different semantic types.&apos; Example 26 a. It seems that John likes Mary. b. John seems to like Mary. Example 27 a. Mary prefers that she come. b. Mary prefers to come. Normally, compositionality in such structures simply refers to th</context>
</contexts>
<marker>Higginbotham, 1985</marker>
<rawString>Higginbotham, James (1985). &amp;quot;On semantics.&amp;quot; Linguistic Inquiry, 16:547-593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
</authors>
<title>A compositional semantics for aktionarten and NP reference in English.&amp;quot; Doctoral dissertation,</title>
<date>1985</date>
<institution>Ohio State University.</institution>
<contexts>
<context position="38970" citStr="Hinrichs 1985" startWordPosition="6215" endWordPosition="6216">guistics Volume 17, Number 4 is a natural kind, while the latter is an artifact. Knowledge of an object includes not just being able to identify or refer, but more specifically, being able to explain how an artifact comes into being, as well as what it is used for; the denotation of an object must identify these roles. Thus, any artifact can be identified with the state of being that object, relative to certain predicates. As is well known from work on event semantics and Aktionsarten, it is a general property of processes that they can shift their event type to become a transition event (cf. Hinrichs 1985; Moens and Steedman 1987; and Krifka 1987). This particular fact about event structures, together with the semantic distinction made above between the two object types, provides us with an explanation for what I will refer to as the logical polysemy of verbs such as bake. As illustrated in Example 30a, when the verb takes as its complement a natural kind such as potato, the resulting semantic interpretation is unchanged; i.e., a process reading of a state-change. This is because the noun does not &amp;quot;project&amp;quot; an event structure of its own. That is, relative to the process of baking, potato does </context>
</contexts>
<marker>Hinrichs, 1985</marker>
<rawString>Hinrichs, Erhard W. (1985). &amp;quot;A compositional semantics for aktionarten and NP reference in English.&amp;quot; Doctoral dissertation, Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Semantic Interpretation and the Resolution of Ambiguity. Cambridge:</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11790" citStr="Hirst 1987" startWordPosition="1838" endWordPosition="1839"> a. John forgot that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such as begin and finish as shown in Example 5. Example 5 a. Mary finished the cigarette. b. Mary finished her beer. The exact meaning of the verb finish varies depending on the object it selects, assuming for these examples the meanings finish smoking or finish drinking. • Test for the ambiguity of a word. Distinguish between homonymy and polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and logical aspects of ambiguity. For example, the homonymy between the two senses of bank in Example 6 is accidental.&apos; Example 6 a. the bank of the river b. the richest bank in the city 2 Cf. Weinreich (1972) distinguishes between contrastive and complementary polysemy, essentially covering this same distinction. See Section 4 for discussion. 412 James Pustejovsky The Generative Lexicon In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich 1972; Lakoff 1987). Example 7 a. The bank raised its interest rates yesterday (i.e. the insti</context>
</contexts>
<marker>Hirst, 1987</marker>
<rawString>Hirst, Graeme (1987). Semantic Interpretation and the Resolution of Ambiguity. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Towards an understanding of coherence in discourse.&amp;quot;</title>
<date>1982</date>
<booktitle>In Strategies for Natural Language Processing,</booktitle>
<note>edited by</note>
<contexts>
<context position="73466" citStr="Hobbs 1982" startWordPosition="11856" endWordPosition="11857">concepts generated from the semantics of the NP the prisoner; escape, however, did fall within the projective conclusion space for the Telic role of prisoner, as shown in Example 63. Example 63 Conclusion Space for (58): escape E P(43T(prisoner)) eat P(1T(prisoner)) This is illustrated in Example 64 below. Example 64 release(T, y, *x*) escape(T, *x*) capture(T, y, *x*) turn-in(T, *x*) &lt; S2 S2 &lt; Si —confined(S2, y, *x*) confined(Si, y, *x*) Formal Telic the prisoner Det N V NP VP We can therefore use such a procedure as one metric for evaluating the &amp;quot;proximity&amp;quot; of a predication (Quillian 1968; Hobbs 1982). In the examples above, the difference escape(T, *x*) escaped 436 James Pustejovsky The Generative Lexicon in semanticality can now be seen as a structural distinction between the semantic representations for the elements in the sentence. In this section, I have shown how the lexical inheritance structure of an item relates, in a generative fashion, the decompositional structure of a word to a much larger set of concepts that are related in obvious ways. What we have not addressed, however, is how the fixed inheritance information of a lexical item is formally derivable during composition. Th</context>
</contexts>
<marker>Hobbs, 1982</marker>
<rawString>Hobbs, Jerry (1982). &amp;quot;Towards an understanding of coherence in discourse.&amp;quot; In Strategies for Natural Language Processing, edited by W. Lehnert and M. Ringle. Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>World knowledge and word meaning.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, TINLAP-3. Las</booktitle>
<location>Cruces, NM.</location>
<contexts>
<context position="6537" citStr="Hobbs (1987)" startWordPosition="1010" endWordPosition="1011">en little attention paid to the other lexical categories (but see Miller and Johnson-Laird [1976], Miller and Fellbaum [1991], and Fass [1988]). That is, we have little insight into the semantic nature of adjectival predication, and even less into the semantics of nominals. Not until all major categories have been studied can we hope to arrive at a balanced understanding of the lexicon and the methods of composition. Stepping back from the lexicon for a moment, let me say briefly what I think the 1 This is still a contentious point and is an issue that is not at all resolved in the community. Hobbs (1987) and Wilensky (1990), for example, argue that there should be no distinction between commonsense knowledge and lexical knowledge. Nevertheless, I will suggest below that there are good reasons, both methodological and empirical, for establishing just such a division. Pustejovsky and Bergler (1991) contains a good survey on how this issue is addressed by the community. 410 James Pustejovsky The Generative Lexicon position of lexical research should be within the larger semantic picture. Ever since the earliest attempts at real text understanding, a major problem has been that of controlling the</context>
<context position="44916" citStr="Hobbs 1987" startWordPosition="7215" endWordPosition="7216">rb, this is an example of cocompositionality (cf. Pustejovsky [forthcoming] for discussion). Having discussed some of the behavior of logical polysemy in verbs, let us continue our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Example 33.16 Example 33 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987) c. John began a novel. 16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but h</context>
</contexts>
<marker>Hobbs, 1987</marker>
<rawString>Hobbs, Jerry (1987). &amp;quot;World knowledge and word meaning.&amp;quot; In Proceedings, TINLAP-3. Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>William Croft</author>
<author>Todd Davies</author>
<author>Douglas Edwards</author>
<author>Kenneth Laws</author>
</authors>
<title>Commonsense metaphysics and lexical semantics.&amp;quot;</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--241</pages>
<contexts>
<context position="30000" citStr="Hobbs et al. (1987)" startWordPosition="4772" endWordPosition="4775"> meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure for a lexical item. I will build on Grimshaw&apos;s recent proposals (Grimshaw 1990) for how to define the mapping from the lexicon to syntax. to a particular vocabulary of primitives, a lexical semantics should provide a method for the decomposition and composition of lexical items. 7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles determine a minimal semantic description of a word that has both semantic and grammatical consequences. 418 James Pustejovsky The Generative Lexicon This provides us with an answer to the question of what levels of semantic representation are necessary for a computational lexical semantics. In sum, I will argue that lexical meaning can best be captured by assuming the following levels of representation. 1. Argument Structure: The behavior of a word as a function, with its arity specified. This is the predicate argument structure for a word, which </context>
<context position="50376" citStr="Hobbs et al. (1987)" startWordPosition="8114" endWordPosition="8117">roles that constitute the qualia structure for a lexical item. Here I will elaborate on what these roles are and why they are useful. They are given in Example 38, where each role is defined, along with the possible values that these roles may assume. Example 38 The Structure of Qualia: 1. Constitutive Role: the relation between an object and its constituents, or proper parts. • Material • Weight • Parts and component elements 19 These components of an object&apos;s denotation have long been considered crucial for our commonsense understanding of how things interact in the world. Cf. Hayes (1979), Hobbs et al. (1987), and Croft (1991) for discussion of these qualitative aspects of meaning. 426 James Pustejovsky The Generative Lexicon 2. Formal Role: that which distinguishes the object within a larger domain. • Orientation • Magnitude • Shape • Dimensionality • Color • Position 3. Telic Role: purpose and function of the object. • Purpose that an agent has in performing an act • Built-in function or aim that specifies certain activities 4. Agentive Role: factors involved in the origin or &amp;quot;bringing about&amp;quot; of an object. • Creator • Artifact • Natural Kind • Causal Chain When we combine the qualia structure of</context>
<context position="65569" citStr="Hobbs et al. (1987)" startWordPosition="10575" endWordPosition="10578">earch (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be lim</context>
</contexts>
<marker>Hobbs, Croft, Davies, Edwards, Laws, 1987</marker>
<rawString>Hobbs, Jerry; Croft, William; Davies, Todd; Edwards, Douglas; and Laws, Kenneth. (1987). &amp;quot;Commonsense metaphysics and lexical semantics.&amp;quot; Computational Linguistics, 13:241-250.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jerry Hobbs</author>
<author>William Croft</author>
<author>Todd Davies</author>
<author>Douglas Edwards</author>
<author>Kenneth Laws</author>
</authors>
<title>The TACITUS commonsense knowledge base.&amp;quot; Artificial Intelligence Center,</title>
<publisher>SRI International.</publisher>
<marker>Hobbs, Croft, Davies, Edwards, Laws, </marker>
<rawString>Hobbs, Jerry; Croft, William; Davies, Todd; Edwards, Douglas; and Laws, Kenneth, &amp;quot;The TACITUS commonsense knowledge base.&amp;quot; Artificial Intelligence Center, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Mark Stickel</author>
<author>Paul Martin</author>
<author>Douglas Edwards</author>
</authors>
<title>Interpretation as abduction.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Buffalo, NY.</location>
<contexts>
<context position="8241" citStr="Hobbs et al. 1988" startWordPosition="1266" endWordPosition="1269">ould be viewed as involving many different generative factors that account for the way that language users create and manipulate the context under constraints, in order to be understood. Within such a theory, where many separate semantic levels (e.g. lexical semantics, compositional semantics, discourse structure, temporal structure) have independent interpretations, the global interpretation of a &amp;quot;discourse&amp;quot; is a highly flexible and malleable structure that has no single interpretation. The individual sources of semantic knowledge compute local inferences with a high degree of certainty (cf. Hobbs et al. 1988; Charniak and Goldman 1988). When integrated together, these inferences must be globally coherent, a state that is accomplished by processes of cooperation among separate semantic modules. The basic result of such a view is that semantic interpretation proceeds in a principled fashion, always aware of what the source of a particular inference is, and what the certainty of its value is. Such an approach allows the reasoning process to be both tractable and computationally efficient. The representation of lexical semantics, therefore, should be seen as just one of many levels in a richer charac</context>
</contexts>
<marker>Hobbs, Stickel, Martin, Edwards, 1988</marker>
<rawString>Hobbs, Jerry; Stickel, Mark; Martin, Paul; and Edwards, Douglas (1988). &amp;quot;Interpretation as abduction.&amp;quot; In Proceedings, 26th Annual Meeting of the Association for Computational Linguistics. Buffalo, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Ingria</author>
<author>James Pustejovsky</author>
</authors>
<title>Active objects in syntax, semantics, and parsing.&amp;quot;</title>
<date>1990</date>
<booktitle>In The MIT Parsing Volume,</booktitle>
<institution>Carol Tenny. Center for Cognitive Science, MIT.</institution>
<note>edited by</note>
<contexts>
<context position="51188" citStr="Ingria and Pustejovsky 1990" startWordPosition="8248" endWordPosition="8251">ger domain. • Orientation • Magnitude • Shape • Dimensionality • Color • Position 3. Telic Role: purpose and function of the object. • Purpose that an agent has in performing an act • Built-in function or aim that specifies certain activities 4. Agentive Role: factors involved in the origin or &amp;quot;bringing about&amp;quot; of an object. • Creator • Artifact • Natural Kind • Causal Chain When we combine the qualia structure of a NP with the argument structure of a verb, we begin to see a richer notion of compositionality emerging, one that looks very much like object-oriented approaches to programming (cf. Ingria and Pustejovsky 1990). To illustrate these structures at play, let us consider a few examples. Assume that the decompositional semantics of a nominal includes a specification of its qualia structure: Example 39 Object(Const, Form, Telic, Agent) For example, a minimal semantic description for the noun novel will include values for each of these roles, as shown in Example 40, where *x* can be seen as a distinguished variable, representing the object itself. Example 40 novel (*x*) Const: narrative(*x*) Form: book(*x*), disk(*x*) Telic: read(T,y,*x*) Agentive: artifact(*x*), write(T,z,*x*) This structures our basic kn</context>
</contexts>
<marker>Ingria, Pustejovsky, 1990</marker>
<rawString>Ingria, Robert, and Pustejovsky, James (1990). &amp;quot;Active objects in syntax, semantics, and parsing.&amp;quot; In The MIT Parsing Volume, 1989-1990, edited by Carol Tenny. Center for Cognitive Science, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Interpretation in Generative Grammar.</title>
<date>1972</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="24791" citStr="Jackendoff (1972)" startWordPosition="3931" endWordPosition="3932">ld tell us the compositional properties of a word, just as a grammar informs us of the specific syntactic behavior of a certain category. What we are led to, therefore, is a generative theory of word meaning, but one very different from the generative semantics of the 1970s. To explain why I am suggesting that lexical decomposition proceed in a generative fashion rather than the traditional exhaustive approach, let me take as a classic example, the word closed as used in Example 21 (see Lakoff 1970). Example 21 a. The door is closed. b. The door closed. c. John closed the door. Lakoff (1970), Jackendoff (1972), and others have suggested that the sense in 21c must incorporate something like cause-to-become-not-open for its meaning. Similarly, a verb such as give specifies a transfer from one person to another, e.g., cause-to-have. Most decomposition theories assume a set of primitives and then operate within this set to capture the meanings of all the words in the language. These approaches can be called exhaustive since they assume that with a fixed number of primitives, complete definitions of lexical meaning can be given. In the sentences in 21, for example, close is defined in terms of the negat</context>
</contexts>
<marker>Jackendoff, 1972</marker>
<rawString>Jackendoff, Ray (1972). Semantic Interpretation in Generative Grammar. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition.</title>
<date>1983</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="15040" citStr="Jackendoff (1983)" startWordPosition="2355" endWordPosition="2356">s; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. 3. Descriptive Adequacy of Existing Representations Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and subtle distinctions are drawn that were not captured in the earlier, primitives-based approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968). As an example of the verb classifications developed by various researchers (and compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical alternations in the example sentences below (cf. Dowty 1991). Example 10 a. John met Mary. b. John and Mary met. Example 11 a. A car ran into a truck. b. A car and a truck ran into each other. Example 12 a. A car ran into a tree. b. *A car and a tree ran into e</context>
<context position="19557" citStr="Jackendoff (1983)" startWordPosition="3092" endWordPosition="3093">rb classes. In fact, the semantic weight in both lexical and compositional terms usually falls on the verb. This has obvious consequences for how to treat lexical ambiguity. For example, consider the verb bake in the two sentences below. Example 15 a. John baked the potato. b. John baked the cake. Atkins, Kegl, and Levin (1988) demonstrate that verbs such as bake are systematically ambiguous, with both a change-of-state sense (15a) and a create sense (15b). A similar ambiguity exists with verbs that allow the resulative construction, shown in Examples 16 and 17, and discussed in Dowty (1979), Jackendoff (1983), and Levin and Rapoport (1988). Example 16 a. Mary hammered the metal. b. Mary hammered the metal flat. Example 17 a. John wiped the table. b. John wiped the table clean. On many views, the verbs in Examples 16 and 17 are ambiguous, related by either a lexical transformation (Levin and Rapoport 1988), or a meaning postulate (Dowty 1979). In fact, given strict requirements on the way that a verb can project its lexical information, the verb run in Example 18 will also have two lexical entries, depending on the syntactic environment it selects (Talmy 1985; Levin and Rappaport 1988). 415 Computa</context>
<context position="23732" citStr="Jackendoff (1983)" startWordPosition="3755" endWordPosition="3756">tive or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would tell us the minimal semantic configuration of a lexical item. Furthermore, it should tell us the compositional properties of a word, just as a grammar informs us of the specific syntactic behavior of a certain category. What we are led to, </context>
<context position="26286" citStr="Jackendoff (1983)" startWordPosition="4165" endWordPosition="4166"> to suggest that lexical (and conceptual) decomposition is possible if it is performed generatively. Rather than assuming a fixed set of primitives, let us assume a fixed number of generative devices that can be seen as constructing semantic expressions.&apos; Just as a formal language is described more in terms of the productions in the grammar than its accompanying vocabulary, a semantic language is definable by the rules generating the structures for expressions rather than the vocabulary of primitives itself.6 4 For further discussion on the advantages and disadvantages to both approaches, see Jackendoff (1983). 5 See Goodman (1951) and Chomsky (1955) for explanations of the method assumed here. 6 This approach is also better suited to the way people write systems in computational linguistics. Different people have distinct primitives for their own domains, and rather than committing a designer 417 Computational Linguistics Volume 17, Number 4 How might this be done? Consider the sentences in Example 21 again. A minimal decomposition on the word closed is that it introduces an opposition of terms: closed and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are predicate</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray (1983). Semantics and Cognition. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
</authors>
<title>Semantic Theory.</title>
<date>1972</date>
<location>New York: Harper and Row.</location>
<contexts>
<context position="22235" citStr="Katz 1972" startWordPosition="3518" endWordPosition="3519">e approach to decomposition, where lexical items are minimally decomposed into structured forms (or templates) rather than sets of features. This will provide us with a generative framework for the composition of lexical meanings, thereby defining the well-formedness conditions for semantic expressions in a language. We can distinguish between two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) ass</context>
<context position="23481" citStr="Katz 1972" startWordPosition="3716" endWordPosition="3717">for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a me</context>
</contexts>
<marker>Katz, 1972</marker>
<rawString>Katz, Jerrold J. (1972). Semantic Theory. New York: Harper and Row.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold J Katz</author>
<author>Jerry Fodor</author>
</authors>
<title>The structure of a semantic theory.&amp;quot;</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<issue>2</issue>
<pages>170--210</pages>
<contexts>
<context position="10829" citStr="Katz and Fodor 1963" startWordPosition="1674" endWordPosition="1677">lume 17, Number 4 Example 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a word always entails a certain proposition, we say that the resulting entailment is not dependent on the syntactic context (cf. Katz and Fodor 1963; Karttunen 1971, 1974; Seuren 1985). This is illustrated in Example 3, where a killing always entails a dying. Example 3 a. John killed Bill. b. Bill died. When the same lexical item may carry different entailments in different contexts, we say that the entailments are sensitive to the syntactic contexts; for example, forget in Example 4, Example 4 a. John forgot that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such a</context>
<context position="16807" citStr="Katz and Fodor (1963)" startWordPosition="2657" endWordPosition="2660"> Levin [1985]) involves adding the preposition at to the verb, changing the verb meaning to an action directed toward an object. Example 13 a. Mary cut the bread. b. Mary cut at the bread. Example 14 a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks (1975a), and Quillian (1968), there is no general coherent view on what the entire lexicon will look like when semantic structures for other major categories are studied. This can be essential for establishing a globally coherent theory of semantic representation. On the other hand, the semantic distinctions captured by these older theories were often too coarse-grained. It is clear, therefore, that the classifications made by Levin and her colleagues are an important starting point for a serious theory of knowledge representation. I claim that lexical semantics must build upon this re</context>
<context position="23470" citStr="Katz and Fodor 1963" startWordPosition="3712" endWordPosition="3715">at there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, Jerrold J., and Fodor, Jerry (1963). &amp;quot;The structure of a semantic theory.&amp;quot; Language, 39(2): 170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Implicative verbs.&amp;quot;</title>
<date>1971</date>
<journal>Language,</journal>
<volume>47</volume>
<pages>340--58</pages>
<contexts>
<context position="10845" citStr="Karttunen 1971" startWordPosition="1678" endWordPosition="1679">mple 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a word always entails a certain proposition, we say that the resulting entailment is not dependent on the syntactic context (cf. Katz and Fodor 1963; Karttunen 1971, 1974; Seuren 1985). This is illustrated in Example 3, where a killing always entails a dying. Example 3 a. John killed Bill. b. Bill died. When the same lexical item may carry different entailments in different contexts, we say that the entailments are sensitive to the syntactic contexts; for example, forget in Example 4, Example 4 a. John forgot that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such as begin and fini</context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>Karttunen, Lauri (1971). &amp;quot;Implicative verbs.&amp;quot; Language, 47: 340-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Presupposition and linguistic context.&amp;quot;</title>
<date>1974</date>
<journal>Theoretical Linguistics</journal>
<volume>1</volume>
<pages>181--93</pages>
<marker>Karttunen, 1974</marker>
<rawString>Karttunen, Lauri (1974). &amp;quot;Presupposition and linguistic context.&amp;quot; Theoretical Linguistics 1: 181-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold Katz</author>
<author>Jerry Fodor</author>
</authors>
<title>The structure of a semantic theory&amp;quot;</title>
<date>1963</date>
<journal>Language,</journal>
<volume>39</volume>
<pages>170--210</pages>
<contexts>
<context position="10829" citStr="Katz and Fodor 1963" startWordPosition="1674" endWordPosition="1677">lume 17, Number 4 Example 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a word always entails a certain proposition, we say that the resulting entailment is not dependent on the syntactic context (cf. Katz and Fodor 1963; Karttunen 1971, 1974; Seuren 1985). This is illustrated in Example 3, where a killing always entails a dying. Example 3 a. John killed Bill. b. Bill died. When the same lexical item may carry different entailments in different contexts, we say that the entailments are sensitive to the syntactic contexts; for example, forget in Example 4, Example 4 a. John forgot that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such a</context>
<context position="16807" citStr="Katz and Fodor (1963)" startWordPosition="2657" endWordPosition="2660"> Levin [1985]) involves adding the preposition at to the verb, changing the verb meaning to an action directed toward an object. Example 13 a. Mary cut the bread. b. Mary cut at the bread. Example 14 a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks (1975a), and Quillian (1968), there is no general coherent view on what the entire lexicon will look like when semantic structures for other major categories are studied. This can be essential for establishing a globally coherent theory of semantic representation. On the other hand, the semantic distinctions captured by these older theories were often too coarse-grained. It is clear, therefore, that the classifications made by Levin and her colleagues are an important starting point for a serious theory of knowledge representation. I claim that lexical semantics must build upon this re</context>
<context position="23470" citStr="Katz and Fodor 1963" startWordPosition="3712" endWordPosition="3715">at there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features</context>
</contexts>
<marker>Katz, Fodor, 1963</marker>
<rawString>Katz, Jerrold, and Fodor, Jerry (1963). &amp;quot;The structure of a semantic theory&amp;quot; Language, 39: 170-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Keenan</author>
<author>Leonard Faltz</author>
</authors>
<title>Boolean Semantics for Natural Language.</title>
<date>1985</date>
<location>Dordrect: Reidel.</location>
<contexts>
<context position="36574" citStr="Keenan and Faltz (1985)" startWordPosition="5810" endWordPosition="5813">s. Yet, such examples indicate that in order to capture the systematicity of such ambiguity, something else is at play, where a richer notion of composition is operative. What then accounts for the polysemy of the verbs in the examples above? The basic idea I will pursue is the following. Rather than treating the expressions that behave as arguments to a function as simple, passive objects, imagine that they are as active in the semantics as the verb itself. The product of function application would be sensitive to both the function and its active argument. Something like this is suggested in Keenan and Faltz (1985), as the Meaning—Form Correlation Principle. I will refer to such behavior as cocompositionality (see below). What I have in mind can best be illustrated by returning to the examples in 28. Example 28 a. John baked the potato. b. John baked the cake. Rather than having two separate word senses for a verb such as bake, suppose there is simply one, a change-of-state reading. Without going into the details of the analysis, let us assume that bake can be lexically specified as denoting a process verb, and is minimally represented as Example 29.&amp;quot; Example 29 Lexical Semantics for bake:11 AyAxAeP [ba</context>
</contexts>
<marker>Keenan, Faltz, 1985</marker>
<rawString>Keenan, Edward, and Faltz, Leonard (1985). Boolean Semantics for Natural Language. Dordrect: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Klein</author>
<author>I Sag</author>
</authors>
<title>Type-driven translation.&amp;quot;</title>
<date>1985</date>
<journal>Linguistics and Philosophy,</journal>
<volume>8</volume>
<pages>163--202</pages>
<contexts>
<context position="22812" citStr="Klein and Sag (1985)" startWordPosition="3608" endWordPosition="3611">itive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially dec</context>
<context position="37818" citStr="Klein and Sag (1985)" startWordPosition="6021" endWordPosition="6024">A object (el&apos; , y)] In order to explain the shift in meaning of the verb, we need to specify more clearly what the lexical semantics of a noun is. I have argued above that lexical semantic theory must make a logical distinction between the following qualia roles: the constitutive, formal, telic, and agentive roles. Now let us examine these roles in more detail. One can distinguish between potato and cake in terms of how they come about; the former 9 For example, Dowty (1985) proposes multiple entries for verbs taking different subcategorizations. Gazdar et al. (1985), adopting the analysis in Klein and Sag (1985), propose a set of lexical type-shifting operations to capture sense relatedness. We return to this topic below. 10 I will be assuming a Davidsonian-style representation for the discussion below. Predicates in the language are typed for a particular event-sort, and thematic roles are treated as partial functions over the event (cf. Dowty 1989 and Chierchia 1989). 11 More precisely, the process el&apos; should reflect that it is the substance contained in the object x that is affected. See footnote 20 for explanation. 421 Computational Linguistics Volume 17, Number 4 is a natural kind, while the lat</context>
</contexts>
<marker>Klein, Sag, 1985</marker>
<rawString>Klein, E., and Sag, I. (1985). &amp;quot;Type-driven translation.&amp;quot; Linguistics and Philosophy, 8: 163-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krifka</author>
</authors>
<title>Nominal reference and temporal constitution: Towards a semantics of quantity.&amp;quot; FNS-Bericht 17. Forschungsstelle fur naturlich-sparchliche System, Universitat Tubingen.</title>
<date>1987</date>
<contexts>
<context position="39013" citStr="Krifka 1987" startWordPosition="6222" endWordPosition="6223">nd, while the latter is an artifact. Knowledge of an object includes not just being able to identify or refer, but more specifically, being able to explain how an artifact comes into being, as well as what it is used for; the denotation of an object must identify these roles. Thus, any artifact can be identified with the state of being that object, relative to certain predicates. As is well known from work on event semantics and Aktionsarten, it is a general property of processes that they can shift their event type to become a transition event (cf. Hinrichs 1985; Moens and Steedman 1987; and Krifka 1987). This particular fact about event structures, together with the semantic distinction made above between the two object types, provides us with an explanation for what I will refer to as the logical polysemy of verbs such as bake. As illustrated in Example 30a, when the verb takes as its complement a natural kind such as potato, the resulting semantic interpretation is unchanged; i.e., a process reading of a state-change. This is because the noun does not &amp;quot;project&amp;quot; an event structure of its own. That is, relative to the process of baking, potato does not denote an event-type.12 Example 30 a. b</context>
<context position="53367" citStr="Krifka (1987)" startWordPosition="8588" endWordPosition="8589">try for each verb, where the syntactic and semantic types had to be represented explicitly. Example 42 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. c. John began a novel. Rather, the verb was analyzed as coercing its complement to the semantic type it expected. To illustrate this, consider 42(c). The type for begin within a standard typed intensional logic is &lt;VP , &lt;NP , S&gt;&gt;, and its lexical semantics is similar to that of other subject control verbs (cf. Klein and Sag [1985] for discussion). Example 43 APAP&apos;PAx[begin&apos;(P(x*))(f)] Assuming an event structure such as that of Krifka (1987) or Pustejovsky (1991), we can convert this lexical entry into a representation consistent with a logic making use of event-types (or sorts) by means of the following meaning postulate.2° Example 44 VPVX1 El [Pa (xi ) (XII) 2ea [P (Xi ) (xn)(ea)il This allows us to type the verb begin as taking a transition event as its first argument, represented in Example 45. Example 45 ApTAP&apos;PAx[begin&apos;(PT(e))(x*)] Because the verb requires that its first argument be of type transition the complement in 33(c) will not match without some sort of shift. It is just this kind of context where the complement (in</context>
</contexts>
<marker>Krifka, 1987</marker>
<rawString>Krifka, Manfred (1987). &amp;quot;Nominal reference and temporal constitution: Towards a semantics of quantity.&amp;quot; FNS-Bericht 17. Forschungsstelle fur naturlich-sparchliche System, Universitat Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<date>1970</date>
<booktitle>Irregularity in Syntax.</booktitle>
<location>Holt, Rinehart, and Winston.</location>
<contexts>
<context position="10098" citStr="Lakoff 1970" startWordPosition="1559" endWordPosition="1560"> verbs typically behave as predicators, nouns as arguments. • Find distinctions between elements of the same word class on the basis of collocation and cooccurrence tests. For example, the nouns dog and book partition into different selectional classes because of contexts involving animacy, while the nouns book and literature partition into different selectional classes because of a mass/count distinction. • Test for distinctions of a grammatical nature on the basis of diathesis; i.e. alternations that are realized in the syntax. For example, break vs. cut in (1) and (2) below (Fillmore 1968; Lakoff 1970; Hale and Keyser 1986): Example 1 a. The glass broke. b. John broke the glass. 411 Computational Linguistics Volume 17, Number 4 Example 2 a. *The bread cut. b. John cut the bread. Such alternations reveal subtle distinctions in the semantic and syntactic behavior of such verbs. The lexical semantic representations of these verbs are distinguishable on the basis of such tests. • Test for entailments in the word senses of a lexical item, in different grammatical contexts. One can distinguish, for example, between context-free and context-sensitive entailments. When the use of a word always ent</context>
<context position="24678" citStr="Lakoff 1970" startWordPosition="3911" endWordPosition="3912">ures, such a method would tell us the minimal semantic configuration of a lexical item. Furthermore, it should tell us the compositional properties of a word, just as a grammar informs us of the specific syntactic behavior of a certain category. What we are led to, therefore, is a generative theory of word meaning, but one very different from the generative semantics of the 1970s. To explain why I am suggesting that lexical decomposition proceed in a generative fashion rather than the traditional exhaustive approach, let me take as a classic example, the word closed as used in Example 21 (see Lakoff 1970). Example 21 a. The door is closed. b. The door closed. c. John closed the door. Lakoff (1970), Jackendoff (1972), and others have suggested that the sense in 21c must incorporate something like cause-to-become-not-open for its meaning. Similarly, a verb such as give specifies a transfer from one person to another, e.g., cause-to-have. Most decomposition theories assume a set of primitives and then operate within this set to capture the meanings of all the words in the language. These approaches can be called exhaustive since they assume that with a fixed number of primitives, complete definit</context>
</contexts>
<marker>Lakoff, 1970</marker>
<rawString>Lakoff, George (1970). Irregularity in Syntax. Holt, Rinehart, and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>On generative semantics.&amp;quot; In Semantics: An Interdisciplinary Reader,</title>
<date>1971</date>
<publisher>Cambridge University Press.</publisher>
<note>edited by</note>
<contexts>
<context position="22248" citStr="Lakoff 1971" startWordPosition="3520" endWordPosition="3521">to decomposition, where lexical items are minimally decomposed into structured forms (or templates) rather than sets of features. This will provide us with a generative framework for the composition of lexical meanings, thereby defining the well-formedness conditions for semantic expressions in a language. We can distinguish between two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however,</context>
</contexts>
<marker>Lakoff, 1971</marker>
<rawString>Lakoff, George (1971). &amp;quot;On generative semantics.&amp;quot; In Semantics: An Interdisciplinary Reader, edited by D. Steinberg and L. Jakobovits. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>Women, Fire, and Dangerous Objects.</title>
<date>1987</date>
<publisher>University of Chicago Press.</publisher>
<location>Chicago:</location>
<contexts>
<context position="12315" citStr="Lakoff 1987" startWordPosition="1921" endWordPosition="1922">for the ambiguity of a word. Distinguish between homonymy and polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and logical aspects of ambiguity. For example, the homonymy between the two senses of bank in Example 6 is accidental.&apos; Example 6 a. the bank of the river b. the richest bank in the city 2 Cf. Weinreich (1972) distinguishes between contrastive and complementary polysemy, essentially covering this same distinction. See Section 4 for discussion. 412 James Pustejovsky The Generative Lexicon In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich 1972; Lakoff 1987). Example 7 a. The bank raised its interest rates yesterday (i.e. the institution). b. The store is next to the new bank (i.e. the building). • Establish what the compositional nature of a lexical item is when applied to other words. For example, alleged vs. female in Example 8. Example 8 a. the alleged suspect b. the female suspect While female behaves as a simple intersective modifier in 8b, certain modifiers such as alleged in 8a cannot be treated as simple attributes; rather, they create an intensional context for the head they modify. An even more difficult problem for compositionality ar</context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, George (1987). Women, Fire, and Dangerous Objects. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<title>Lexical semantics in review,&amp;quot;</title>
<date>1985</date>
<booktitle>Lexicon Project Working Papers Number 1, MIT.</booktitle>
<editor>Levin, Beth (ed.)</editor>
<contexts>
<context position="13269" citStr="[1985]" startWordPosition="2082" endWordPosition="2082">behaves as a simple intersective modifier in 8b, certain modifiers such as alleged in 8a cannot be treated as simple attributes; rather, they create an intensional context for the head they modify. An even more difficult problem for compositionality arises from phrases containing frequency adjectives (cf. Stump 1981), as shown in 8c and 8d. Example 8 c. An occasional sailor walks by on the weekend. d. Caution: may contain an occasional pit (notice on a box of prunes). The challenge here is that the adjective doesn&apos;t modify the nominal head, but the entire proposition containing it (cf. Partee [1985] for discussion). A similar difficulty arises with the interpretation of scalar predicates such as fast in Example 9. Both the scale and the relative interpretation being selected for depends on the noun that the predicate is modifying. Example 9 a. a fast typist: one who types quickly b. a fast car: one which can move quickly c. a fast waltz: one with a fast tempo Such data raise serious questions about the principles of compositionality and how ambiguity should be accounted for by a theory of semantics. This just briefly characterizes some of the techniques that have been useful for arriving</context>
<context position="14943" citStr="[1985]" startWordPosition="2342" endWordPosition="2342">taxonomies; Wilks et al. (1988) for establishing semantic relatedness among word senses; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. 3. Descriptive Adequacy of Existing Representations Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and subtle distinctions are drawn that were not captured in the earlier, primitives-based approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968). As an example of the verb classifications developed by various researchers (and compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical alternations in the example sentences below (cf. Dowty 1991). Example 10 a. John met Mary. b. John and Mary met. Example 11 a. A car ran into a truck. b. A car and </context>
<context position="16199" citStr="[1985]" startWordPosition="2551" endWordPosition="2551">an into a tree. b. *A car and a tree ran into each other. These three pairs show how the semantics of transitive motion verbs (e.g. run into) is similar in some respects to reciprocal verbs such as meet. The important difference, however, is that the reciprocal interpretation requires that both subject and object be animate or moving; hence 12b is ill-formed. (cf. Levin 1989; Dowty 1991). Another example of how diathesis reveals the underlying semantic differences between verbs is illustrated in Examples 13 and 14 below. A construction called the conative (see Hale and Keyser [1986] and Levin [1985]) involves adding the preposition at to the verb, changing the verb meaning to an action directed toward an object. Example 13 a. Mary cut the bread. b. Mary cut at the bread. Example 14 a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodo</context>
<context position="53254" citStr="[1985]" startWordPosition="8574" endWordPosition="8574">ested earlier, that for cases such as 33, repeated below, there was no need to posit a separate lexical entry for each verb, where the syntactic and semantic types had to be represented explicitly. Example 42 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. c. John began a novel. Rather, the verb was analyzed as coercing its complement to the semantic type it expected. To illustrate this, consider 42(c). The type for begin within a standard typed intensional logic is &lt;VP , &lt;NP , S&gt;&gt;, and its lexical semantics is similar to that of other subject control verbs (cf. Klein and Sag [1985] for discussion). Example 43 APAP&apos;PAx[begin&apos;(P(x*))(f)] Assuming an event structure such as that of Krifka (1987) or Pustejovsky (1991), we can convert this lexical entry into a representation consistent with a logic making use of event-types (or sorts) by means of the following meaning postulate.2° Example 44 VPVX1 El [Pa (xi ) (XII) 2ea [P (Xi ) (xn)(ea)il This allows us to type the verb begin as taking a transition event as its first argument, represented in Example 45. Example 45 ApTAP&apos;PAx[begin&apos;(PT(e))(x*)] Because the verb requires that its first argument be of type transition the comple</context>
</contexts>
<marker>1985</marker>
<rawString>Levin, Beth (ed.) (1985). &amp;quot;Lexical semantics in review,&amp;quot; Lexicon Project Working Papers Number 1, MIT.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Levin</author>
</authors>
<title>Towards a Lexical Organization of English Verbs.</title>
<publisher>Chicago: University of Chicago Press. Forthcoming.</publisher>
<marker>Levin, </marker>
<rawString>Levin, B. Towards a Lexical Organization of English Verbs. Chicago: University of Chicago Press. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>T R Rapoport</author>
</authors>
<title>Lexical subordination.&amp;quot;</title>
<date>1988</date>
<journal>Proceedings of CLS</journal>
<volume>24</volume>
<pages>275--289</pages>
<contexts>
<context position="19588" citStr="Levin and Rapoport (1988)" startWordPosition="3095" endWordPosition="3098">e semantic weight in both lexical and compositional terms usually falls on the verb. This has obvious consequences for how to treat lexical ambiguity. For example, consider the verb bake in the two sentences below. Example 15 a. John baked the potato. b. John baked the cake. Atkins, Kegl, and Levin (1988) demonstrate that verbs such as bake are systematically ambiguous, with both a change-of-state sense (15a) and a create sense (15b). A similar ambiguity exists with verbs that allow the resulative construction, shown in Examples 16 and 17, and discussed in Dowty (1979), Jackendoff (1983), and Levin and Rapoport (1988). Example 16 a. Mary hammered the metal. b. Mary hammered the metal flat. Example 17 a. John wiped the table. b. John wiped the table clean. On many views, the verbs in Examples 16 and 17 are ambiguous, related by either a lexical transformation (Levin and Rapoport 1988), or a meaning postulate (Dowty 1979). In fact, given strict requirements on the way that a verb can project its lexical information, the verb run in Example 18 will also have two lexical entries, depending on the syntactic environment it selects (Talmy 1985; Levin and Rappaport 1988). 415 Computational Linguistics Volume 17, N</context>
</contexts>
<marker>Levin, Rapoport, 1988</marker>
<rawString>Levin, Beth, and Rapoport, T. R. (1988). &amp;quot;Lexical subordination.&amp;quot; Proceedings of CLS 24, 275-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Rappaport</author>
</authors>
<title>The formation of adjectival passives.&amp;quot;</title>
<date>1986</date>
<journal>Linguistic Inquiry,</journal>
<pages>17--623</pages>
<location>Malka</location>
<contexts>
<context position="4883" citStr="Levin and Rappaport (1986)" startWordPosition="730" endWordPosition="734">ssible word meaning. This may entail abstracting the notion of lexical meaning away from other semantic influences. For instance, this might suggest that discourse and pragmatic factors should be handled differently or separately from the semantic contributions of lexical items in composition.&apos; Although this is not a necessary assumption and may in fact be wrong, it may help narrow our focus on what is important for lexical semantic descriptions. Secondly, lexical semantics must look for representations that are richer than thematic role descriptions (Gruber 1965; Fillmore 1968). As argued in Levin and Rappaport (1986), named roles are useful at best for establishing fairly general mapping strategies to the syntactic structures in language. The distinctions possible with thetaroles are much too coarse-grained to provide a useful semantic interpretation of a sentence. What is needed, I will argue, is a principled method of lexical decomposition. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic composition, (2) the notion of semantic well-formedness mentioned above, and (3) an appeal to several levels of interpretation in the semantics (Scha 1983). Thirdly, and related to th</context>
</contexts>
<marker>Levin, Rappaport, 1986</marker>
<rawString>Levin, Beth, and Rappaport, Malka (1986). &amp;quot;The formation of adjectival passives.&amp;quot; Linguistic Inquiry, 17:623-663.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
<author>Rappaport</author>
</authors>
<title>On the nature of unaccusativity.&amp;quot;</title>
<date>1988</date>
<booktitle>In Computational Linguistics Volume 17, Number 4 Proceedings, NELS</booktitle>
<location>Malka</location>
<contexts>
<context position="20144" citStr="Levin and Rappaport 1988" startWordPosition="3190" endWordPosition="3193">sed in Dowty (1979), Jackendoff (1983), and Levin and Rapoport (1988). Example 16 a. Mary hammered the metal. b. Mary hammered the metal flat. Example 17 a. John wiped the table. b. John wiped the table clean. On many views, the verbs in Examples 16 and 17 are ambiguous, related by either a lexical transformation (Levin and Rapoport 1988), or a meaning postulate (Dowty 1979). In fact, given strict requirements on the way that a verb can project its lexical information, the verb run in Example 18 will also have two lexical entries, depending on the syntactic environment it selects (Talmy 1985; Levin and Rappaport 1988). 415 Computational Linguistics Volume 17, Number 4 Example 18 a. Mary ran to the store yesterday. b. Mary ran yesterday. These two verbs differ in their semantic representations, where run in 18a means goto-by-means-of-running, while in 18b it means simply move-by-running (cf. Jackendoff 1983). The methodology described above for distinguishing word senses is also assumed by those working in more formal frameworks. For example, Dowty (1985) proposes multiple entries for control and raising verbs, and establishes their semantic equivalence with the use of meaning postulates. That is, the verbs</context>
</contexts>
<marker>Levin, Rappaport, 1988</marker>
<rawString>Levin, Beth, and Rappaport, Malka (1988). &amp;quot;On the nature of unaccusativity.&amp;quot; In Computational Linguistics Volume 17, Number 4 Proceedings, NELS 1988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Beth Levin</author>
<author>Malka Rappaport</author>
</authors>
<pages>forthcoming.</pages>
<publisher>Unaccusatives. The MIT Press,</publisher>
<marker>Levin, Rappaport, </marker>
<rawString>Levin, Beth, and Rappaport, Malka. Unaccusatives. The MIT Press, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E R Lloyd</author>
</authors>
<title>Aristotle: The Growth and Structure of his Thought.</title>
<date>1968</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="27285" citStr="Lloyd 1968" startWordPosition="4325" endWordPosition="4326">n Example 21 again. A minimal decomposition on the word closed is that it introduces an opposition of terms: closed and not-closed. For the verbal forms in 21b and 21c, both terms in this opposition are predicated of different subevents denoted by the sentences. In 21a, this opposition is left implicit, since the sentence refers to a single state. Any minimal analysis of the semantics of a lexical item can be termed a generative operation, since it operates on the predicate(s) already literally provided by the word. This type of analysis is essentially Aristotle&apos;s principle of opposition (cf. Lloyd 1968), and it will form the basis of one level of representation for a lexical item. The essential opposition denoted by a predicate forms part of what I will call the qualia structure of that lexical item. Briefly, the qualia structure of a word specifies four aspects of its meaning: • the relation between it and its constituent parts; • that which distinguishes it within a larger domain (its physical characteristics); • its purpose and function; • whatever brings it about. I will call these aspects of a word&apos;s meaning its Constitutive Role, Formal Role, Telic Role, and its Agentive Role, respecti</context>
</contexts>
<marker>Lloyd, 1968</marker>
<rawString>Lloyd, G. E. R. (1968). Aristotle: The Growth and Structure of his Thought. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard McKeon</author>
</authors>
<title>The Basic Works of Aristotle.</title>
<date>1941</date>
<publisher>Random House.</publisher>
<marker>McKeon, 1941</marker>
<rawString>McKeon, Richard (1941). The Basic Works of Aristotle. Random House.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel&apos;C&apos;uk</author>
</authors>
<title>Dependency Syntax.</title>
<date>1988</date>
<publisher>SUNY Press.</publisher>
<location>Albany, NY:</location>
<marker>Mel&apos;C&apos;uk, 1988</marker>
<rawString>Mel&apos;C&apos;uk, I. (1988). Dependency Syntax. Albany, NY: SUNY Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Michalski</author>
</authors>
<title>A theory and methodology of inductive learning.&amp;quot;</title>
<date>1983</date>
<booktitle>In Machine Learning I,</booktitle>
<editor>R. S. Michalski, J. Carbonelli, and T. Mitchell, eds. Palo Alto, CA:</editor>
<publisher>Tioga Publishing.</publisher>
<contexts>
<context position="69554" citStr="Michalski (1983)" startWordPosition="11210" endWordPosition="11211">a projective transformation, and define them below: Definition A projective transformation, 7r, on a predicate Qi generates a predicate, Q2, such that 7r(Q1) = Q2, where Q2 cl (I) . The set of transformations includes: negation, &lt;, temporal precedence, &gt;, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on </context>
</contexts>
<marker>Michalski, 1983</marker>
<rawString>Michalski, R. S. (1983). &amp;quot;A theory and methodology of inductive learning.&amp;quot; In Machine Learning I, R. S. Michalski, J. Carbonelli, and T. Mitchell, eds. Palo Alto, CA: Tioga Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Dictionaries of the mind.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago, IL.</location>
<contexts>
<context position="69749" citStr="Miller 1985" startWordPosition="11241" endWordPosition="11242">rmations includes: negation, &lt;, temporal precedence, &gt;, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categor</context>
</contexts>
<marker>Miller, 1985</marker>
<rawString>Miller, George (1985). &amp;quot;Dictionaries of the mind.&amp;quot; In Proceedings, 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Fellbaum</author>
</authors>
<title>Semantic networks of English.&amp;quot; Cognition,</title>
<date>1991</date>
<contexts>
<context position="69797" citStr="Miller and Fellbaum 1991" startWordPosition="11247" endWordPosition="11250">poral precedence, &gt;, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicate Q: as: P(43R) = {(P(Q1)P(Q)) I(Qi, • • • , Qn) 430. From this resulting representation, we can generate a relational structure that can be considered the set of ad hoc categories and relations associated with a lexical item</context>
</contexts>
<marker>Miller, Fellbaum, 1991</marker>
<rawString>Miller, G. A., and Fellbaum, C, (1991). &amp;quot;Semantic networks of English.&amp;quot; Cognition, October 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Phillip Johnson-Laird</author>
</authors>
<title>Language and Perception.</title>
<date>1976</date>
<publisher>Belknap, Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Miller, Johnson-Laird, 1976</marker>
<rawString>Miller, George, and Johnson-Laird, Phillip (1976). Language and Perception. Cambridge, MA: Belknap, Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moens</author>
<author>M Steedman</author>
</authors>
<title>Temporal ontology and temporal reference.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>2</issue>
<pages>15--28</pages>
<contexts>
<context position="33254" citStr="Moens and Steedman 1988" startWordPosition="5274" endWordPosition="5277"> semantic characterization of a lexical item, but it is a necessary component. 5.2 Event Structure As mentioned above, the theory of decomposition being outlined here is based on the central idea that word meaning is highly structured, and not simply a set of semantic features. Let us assume this is the case. Then the lexical items in a language will essentially be generated by the recursive principles of our semantic theory. One level of semantic description involves an event-based interpretation of a word or phrase. I will call this level the event structure of a word (cf. Pustejovsky 1991; Moens and Steedman 1988). The event structure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure. Because it is recursively defined on the syntax, it is also a property of phrases and sentences.&apos; I will assume a sortal distinction between three classes of events: states (es), processes (eP), and transitions (eT ) . Unlike most previous sortal classifications for events, I will adopt a subeventual analysis or predicates, as argued in Pustejovsky (1991) and independently</context>
</contexts>
<marker>Moens, Steedman, 1988</marker>
<rawString>Moens, M., and Steedman, M. (1988). &amp;quot;Temporal ontology and temporal reference.&amp;quot; Computational Linguistics, 14(2): 15-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy: The Collected Papers of Richard Montague, edited by Richard Thomason. New Haven:</title>
<date>1974</date>
<publisher>Yale University Press.</publisher>
<contexts>
<context position="18510" citStr="Montague 1974" startWordPosition="2922" endWordPosition="2923">count for the richness of natural language semantics. It should be pointed out here that a theory of lexical meaning will affect the general design of our semantic theory in several ways. If we view the goal of a semantic theory as being able to recursively assign meanings to expressions, accounting for phenomena such as synonymy, antonymy, polysemy, metonymy, etc., then our view of compositionality depends ultimately on what the basic lexical categories of the language denote. Conventional wisdom on this point paints a picture of words behaving as either active functors or passive arguments (Montague 1974). But we will see that if we change the way in which categories can denote, then the form of compositionality itself changes. Therefore, if done correctly, lexical semantics can be a means to reevaluate the very nature of semantic composition in language. In what ways could lexical semantics affect the larger methods of composition in semantics? I mentioned above that most of the careful representation work has been done on verb classes. In fact, the semantic weight in both lexical and compositional terms usually falls on the verb. This has obvious consequences for how to treat lexical ambigui</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, Richard (1974). Formal Philosophy: The Collected Papers of Richard Montague, edited by Richard Thomason. New Haven: Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Moravcsik</author>
</authors>
<title>Aitia as generative factor in Aristotle&apos;s philosophy.&amp;quot;</title>
<date>1975</date>
<journal>Dialogue,</journal>
<pages>14--622</pages>
<contexts>
<context position="49460" citStr="Moravcsik (1975)" startWordPosition="7963" endWordPosition="7965">NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical semantic properties for nouns in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and Anick (1988), I suggest that there is a system of relations that characterizes the semantics of nominals, very much like the argument structure of a verb. I called this the Qualia Structure, inspired by Aristotle&apos;s theory of explanation and ideas from Moravcsik (1975). Essentially, the qualia structure of a noun determines its meaning as much as the list of arguments determines a verb&apos;s meaning. The elements that make up a qualia structure include notions such as container, space, surface, figure, artifact, and so on.&amp;quot; As stated earlier, there are four basic roles that constitute the qualia structure for a lexical item. Here I will elaborate on what these roles are and why they are useful. They are given in Example 38, where each role is defined, along with the possible values that these roles may assume. Example 38 The Structure of Qualia: 1. Constitutive</context>
</contexts>
<marker>Moravcsik, 1975</marker>
<rawString>Moravcsik, J. M. (1975). &amp;quot;Aitia as generative factor in Aristotle&apos;s philosophy.&amp;quot; Dialogue, 14:622-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Mourelatos</author>
</authors>
<title>Events, processes, and states.&amp;quot;</title>
<date>1981</date>
<booktitle>In Syntax and Semantics: Tense and Aspect, edited</booktitle>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<marker>Mourelatos, 1981</marker>
<rawString>Mourelatos, Alexander (1981), &amp;quot;Events, processes, and states.&amp;quot; In Syntax and Semantics: Tense and Aspect, edited by P. Tedeschi and A. Zaenen. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nunberg</author>
</authors>
<title>The Pragmatics of Reference.</title>
<date>1978</date>
<institution>Indiana University Linguistics Club.</institution>
<location>Bloomington, Indiana:</location>
<contexts>
<context position="44961" citStr="Nunberg (1978)" startWordPosition="7224" endWordPosition="7225">ty (cf. Pustejovsky [forthcoming] for discussion). Having discussed some of the behavior of logical polysemy in verbs, let us continue our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Example 33.16 Example 33 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987) c. John began a novel. 16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to</context>
<context position="47392" citStr="Nunberg 1978" startWordPosition="7630" endWordPosition="7631">. To drive a car in Boston frightens me. c. Driving frightens me. d. John&apos;s driving frightens me. e. Cars frighten me. f. Listening to this music upsets me. g. This music upsets me. h. To listen to this music would upset me. Example 35 a. John killed Mary. b. The gun killed Mary. c. John&apos;s stupidity killed Mary. d. The war killed Mary. e. John&apos;s pulling the trigger killed Mary. As these examples illustrate, the syntactic argument to a verb is not always the same logical argument in the semantic relation. Although superficially similar to cases of general metonymy (cf. Lakoff and Johnson 1982; Nunberg 1978), there is an interesting systematicity to such shifts in meaning that we will try to characterize below as logical metonymy. 17 I am following Cardelli and Wegener (1985) and their characterization of polymorphistnic behavior. 18 See Verma and Mohanan (1991) for an extensive survey of experiencer subject constructions in different languages. 425 Computational Linguistics Volume 17, Number 4 The sentences in 34 illustrate the various syntactic consequences of metonymy and coercion involving experiencer verbs, while those in 35 show the different metonymic extensions possible from the causing e</context>
</contexts>
<marker>Nunberg, 1978</marker>
<rawString>Nunberg, G. (1978). The Pragmatics of Reference. Bloomington, Indiana: Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ross Quillian</author>
</authors>
<title>Semantic memory,&amp;quot;</title>
<date>1968</date>
<booktitle>In Semantic Information Processing, edited by M. Minsky.</booktitle>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="16843" citStr="Quillian (1968)" startWordPosition="2664" endWordPosition="2665">tion at to the verb, changing the verb meaning to an action directed toward an object. Example 13 a. Mary cut the bread. b. Mary cut at the bread. Example 14 a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks (1975a), and Quillian (1968), there is no general coherent view on what the entire lexicon will look like when semantic structures for other major categories are studied. This can be essential for establishing a globally coherent theory of semantic representation. On the other hand, the semantic distinctions captured by these older theories were often too coarse-grained. It is clear, therefore, that the classifications made by Levin and her colleagues are an important starting point for a serious theory of knowledge representation. I claim that lexical semantics must build upon this research toward 414 James Pustejovsky </context>
<context position="22579" citStr="Quillian 1968" startWordPosition="3572" endWordPosition="3573">tween two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than </context>
<context position="73453" citStr="Quillian 1968" startWordPosition="11854" endWordPosition="11855">ace of related concepts generated from the semantics of the NP the prisoner; escape, however, did fall within the projective conclusion space for the Telic role of prisoner, as shown in Example 63. Example 63 Conclusion Space for (58): escape E P(43T(prisoner)) eat P(1T(prisoner)) This is illustrated in Example 64 below. Example 64 release(T, y, *x*) escape(T, *x*) capture(T, y, *x*) turn-in(T, *x*) &lt; S2 S2 &lt; Si —confined(S2, y, *x*) confined(Si, y, *x*) Formal Telic the prisoner Det N V NP VP We can therefore use such a procedure as one metric for evaluating the &amp;quot;proximity&amp;quot; of a predication (Quillian 1968; Hobbs 1982). In the examples above, the difference escape(T, *x*) escaped 436 James Pustejovsky The Generative Lexicon in semanticality can now be seen as a structural distinction between the semantic representations for the elements in the sentence. In this section, I have shown how the lexical inheritance structure of an item relates, in a generative fashion, the decompositional structure of a word to a much larger set of concepts that are related in obvious ways. What we have not addressed, however, is how the fixed inheritance information of a lexical item is formally derivable during co</context>
</contexts>
<marker>Quillian, 1968</marker>
<rawString>Quillian, M. Ross (1968). &amp;quot;Semantic memory,&amp;quot; In Semantic Information Processing, edited by M. Minsky. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Partee</author>
<author>Mats Rooth</author>
</authors>
<title>Generalized conjunction and type ambiguity.&amp;quot;</title>
<date>1983</date>
<booktitle>In Meaning, Use, and Interpretation of Language,</booktitle>
<note>edited by</note>
<contexts>
<context position="56492" citStr="Partee and Rooth (1983)" startWordPosition="9118" endWordPosition="9121">ions (cf. Pustejovsky [1989a] for details).21 The derivation in 49(a) and the structure in 49(b) show the effects of this coercion on the verb&apos;s complement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. 1. Function Application with Coercion (FAO: If a is of type (b, a), and is of type c, then (a) if type c = b, </context>
</contexts>
<marker>Partee, Rooth, 1983</marker>
<rawString>Partee, Barbara, and Rooth, Mats (1983). &amp;quot;Generalized conjunction and type ambiguity.&amp;quot; In Meaning, Use, and Interpretation of Language, edited by Bauerle, Schwarze, and von Stechow. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>A computational model of the semantics of tense and aspect.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>2</issue>
<marker>Passonneau, 1988</marker>
<rawString>Passonneau, Rebecca J. (1988). &amp;quot;A computational model of the semantics of tense and aspect.&amp;quot; Computational Linguistics, 14(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The geometry of events.&amp;quot; In Studies in Generative Approaches to Aspect, edited by Carol Tenny. Lexicon Project Working Papers 24,</title>
<date>1988</date>
<publisher>MIT.</publisher>
<contexts>
<context position="35349" citStr="Pustejovsky (1988" startWordPosition="5614" endWordPosition="5615">, State(y))) b. John wiped the table clean. (wipe2 = cause(x, Become(clean(y)))) Example 25 a. Mary ran yesterday. (runi = move(x)) b. Mary ran to the store yesterday. (run2 = go-to(x,y)) • Although the complement types selected by bake in 22, for example, are semantically related, the two word senses are clearly distinct and therefore must be lexically distinguished. According to the sense enumeration view, the same argument holds for the verbs in 23-25 as well. 8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984). For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual distinctions are mapped to the syntax. 420 James Pustejovsky The Generative Lexicon A similar philosophy has lead linguists to multiply word senses in constructions involving Control and Equi-verbs, where different syntactic contexts necessitate different semantic types.&apos; Example 26 a. It seems that John likes Mary. b. John seems to like Mary. Example 27 a. Mary prefers that she come. b. Mary prefers to come. Normally, compositionality in such structures simply refers to the application of the functional element, the verb, to its argume</context>
</contexts>
<marker>Pustejovsky, 1988</marker>
<rawString>Pustejovsky, James (1988). &amp;quot;The geometry of events.&amp;quot; In Studies in Generative Approaches to Aspect, edited by Carol Tenny. Lexicon Project Working Papers 24, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Type coercion and selection.&amp;quot; Paper presented at West Coast Conference on Formal Linguistics.</title>
<date>1989</date>
<location>Vancouver.</location>
<contexts>
<context position="22946" citStr="Pustejovsky (1989" startWordPosition="3630" endWordPosition="3631">ecomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some</context>
<context position="45668" citStr="Pustejovsky 1989" startWordPosition="7333" endWordPosition="7334">re of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpretations? I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a), where the verb has coerced the meaning of a term phrase into a different semantic type. Briefly, type coercion can be defined as follows:17 Definition Type Coercion: A semantic operation that converts an argument to the type that is expected by a function, where it would otherwise result in a type error. In the case of 33(b), it is obvious that what is vetoed is some proposal relating to the object denoted by the tunnel. In 33(a), the book is enjoyed only by virtue of some event or process that involves the book, performed by Mary. It might furthermore be reasonable to assume that the sema</context>
<context position="49169" citStr="Pustejovsky (1989" startWordPosition="7916" endWordPosition="7917">ll arrive) b. Bill figured out the answer. (= what the answer is) Example 37 a. John shocked me with his bad behavior. (= how bad his behavior is) b. You&apos;d be surprised at the big cars he buys. (= how big the cars he buys are) That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical semantic properties for nouns in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and Anick (1988), I suggest that there is a system of relations that characterizes the semantics of nominals, very much like the argument structure of a verb. I called this the Qualia Structure, inspired by Aristotle&apos;s theory of explanation and ideas from Moravcsik (1975). Essentially, the qualia structure of a noun determines its meaning as much as the list of arguments determines a verb&apos;s meaning. The elements that make up a qualia structure include notions such as container, space, surface, figure, artifact, and so on.&amp;quot; As stated earlier, there are four basic roles that c</context>
<context position="56632" citStr="Pustejovsky (1989" startWordPosition="9143" endWordPosition="9144">omplement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. 1. Function Application with Coercion (FAO: If a is of type (b, a), and is of type c, then (a) if type c = b, then a(/3) is of type a. (b) if there is a a E Ep such that cr(13) results in an expression of type b, then a (a (0)) is of type a. (c) othe</context>
</contexts>
<marker>Pustejovsky, 1989</marker>
<rawString>Pustejovsky, James (1989a). &amp;quot;Type coercion and selection.&amp;quot; Paper presented at West Coast Conference on Formal Linguistics. Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Issues in computational lexical semantics.&amp;quot; In</title>
<date>1989</date>
<booktitle>Proceedings, Fourth European ACL Conference,</booktitle>
<location>Manchester, England.</location>
<contexts>
<context position="22946" citStr="Pustejovsky (1989" startWordPosition="3630" endWordPosition="3631">ecomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there are reasons for relating these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some</context>
<context position="45668" citStr="Pustejovsky 1989" startWordPosition="7333" endWordPosition="7334">re of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpretations? I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a), where the verb has coerced the meaning of a term phrase into a different semantic type. Briefly, type coercion can be defined as follows:17 Definition Type Coercion: A semantic operation that converts an argument to the type that is expected by a function, where it would otherwise result in a type error. In the case of 33(b), it is obvious that what is vetoed is some proposal relating to the object denoted by the tunnel. In 33(a), the book is enjoyed only by virtue of some event or process that involves the book, performed by Mary. It might furthermore be reasonable to assume that the sema</context>
<context position="49169" citStr="Pustejovsky (1989" startWordPosition="7916" endWordPosition="7917">ll arrive) b. Bill figured out the answer. (= what the answer is) Example 37 a. John shocked me with his bad behavior. (= how bad his behavior is) b. You&apos;d be surprised at the big cars he buys. (= how big the cars he buys are) That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical semantic properties for nouns in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and Anick (1988), I suggest that there is a system of relations that characterizes the semantics of nominals, very much like the argument structure of a verb. I called this the Qualia Structure, inspired by Aristotle&apos;s theory of explanation and ideas from Moravcsik (1975). Essentially, the qualia structure of a noun determines its meaning as much as the list of arguments determines a verb&apos;s meaning. The elements that make up a qualia structure include notions such as container, space, surface, figure, artifact, and so on.&amp;quot; As stated earlier, there are four basic roles that c</context>
<context position="56632" citStr="Pustejovsky (1989" startWordPosition="9143" endWordPosition="9144">omplement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. 1. Function Application with Coercion (FAO: If a is of type (b, a), and is of type c, then (a) if type c = b, then a(/3) is of type a. (b) if there is a a E Ep such that cr(13) results in an expression of type b, then a (a (0)) is of type a. (c) othe</context>
</contexts>
<marker>Pustejovsky, 1989</marker>
<rawString>Pustejovsky, James (1989b). &amp;quot;Issues in computational lexical semantics.&amp;quot; In Proceedings, Fourth European ACL Conference, Manchester, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The syntax of event siructure.&amp;quot;</title>
<date>1991</date>
<journal>Cognition,</journal>
<volume>41</volume>
<contexts>
<context position="28965" citStr="Pustejovsky 1991" startWordPosition="4604" endWordPosition="4605">transition from not-closed to closed is still entailed. In 2k, the event that brings about the closed state of the door is made more explicit by specifying the actor involved. These differences constitute what I call the event structure of a lexical item. Both the opposition of predicates and the specification of causation are part of a verb&apos;s semantics, and are structurally associated with slots in the event template for the word. As we will see in the next section, there are different inferences associated with each event type, as well as different syntactic behaviors (cf. Grimshaw 1990 and Pustejovsky 1991). Because the lexical semantic representation of a word is not an isolated expression, but is in fact linked to the rest of the lexicon, in Section 7, I suggest how the global integration of the semantics for a lexical item is achieved by structured inheritance through the different qualia associated with a word. I call this the lexical inheritance structure for the word. Finally, we must realize that part of the meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure f</context>
<context position="33228" citStr="Pustejovsky 1991" startWordPosition="5272" endWordPosition="5273"> for capturing the semantic characterization of a lexical item, but it is a necessary component. 5.2 Event Structure As mentioned above, the theory of decomposition being outlined here is based on the central idea that word meaning is highly structured, and not simply a set of semantic features. Let us assume this is the case. Then the lexical items in a language will essentially be generated by the recursive principles of our semantic theory. One level of semantic description involves an event-based interpretation of a word or phrase. I will call this level the event structure of a word (cf. Pustejovsky 1991; Moens and Steedman 1988). The event structure of a word is one level of the semantic specification 419 Computational Linguistics Volume 17, Number 4 for a lexical item, along with its argument structure, qualia structure, and inheritance structure. Because it is recursively defined on the syntax, it is also a property of phrases and sentences.&apos; I will assume a sortal distinction between three classes of events: states (es), processes (eP), and transitions (eT ) . Unlike most previous sortal classifications for events, I will adopt a subeventual analysis or predicates, as argued in Pustejovsk</context>
<context position="53389" citStr="Pustejovsky (1991)" startWordPosition="8591" endWordPosition="8592">, where the syntactic and semantic types had to be represented explicitly. Example 42 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. c. John began a novel. Rather, the verb was analyzed as coercing its complement to the semantic type it expected. To illustrate this, consider 42(c). The type for begin within a standard typed intensional logic is &lt;VP , &lt;NP , S&gt;&gt;, and its lexical semantics is similar to that of other subject control verbs (cf. Klein and Sag [1985] for discussion). Example 43 APAP&apos;PAx[begin&apos;(P(x*))(f)] Assuming an event structure such as that of Krifka (1987) or Pustejovsky (1991), we can convert this lexical entry into a representation consistent with a logic making use of event-types (or sorts) by means of the following meaning postulate.2° Example 44 VPVX1 El [Pa (xi ) (XII) 2ea [P (Xi ) (xn)(ea)il This allows us to type the verb begin as taking a transition event as its first argument, represented in Example 45. Example 45 ApTAP&apos;PAx[begin&apos;(PT(e))(x*)] Because the verb requires that its first argument be of type transition the complement in 33(c) will not match without some sort of shift. It is just this kind of context where the complement (in this case a novel) is</context>
<context position="68573" citStr="Pustejovsky 1991" startWordPosition="11057" endWordPosition="11058">et of sequences (I) is the set of all pairs (Q, P) such that a sequence (Q, , P) appears in 1. From these two definitions we can define the traditional is-a relation, relating the above pairs by a generalization operator, &lt;G,&amp;quot; as well as other relations that I will not discuss.27 Let us suppose that, in addition to these fixed relational structures, our semantics allows us to dynamically create arbitrary concepts through the application of certain transformations to lexical meanings. For example, for any predicate, Q — e.g. the value of a qualia role — we can generate its opposition, -02 (cf. Pustejovsky 1991). By relating these two predicates temporally we can generate the arbitrary transition events for this opposition (cf. Wright 1963): Example 59 a. -Q(x) _&lt; Q(x) b. Q(x) &lt;-Q(x) c. Q(x) &lt; Q(x) d. -Q(x) --Q(x) Similarly, by operating over other qualia role values we can generate semantically related concepts. I will call any operator that performs such an operation a projective transformation, and define them below: Definition A projective transformation, 7r, on a predicate Qi generates a predicate, Q2, such that 7r(Q1) = Q2, where Q2 cl (I) . The set of transformations includes: negation, &lt;, tem</context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>Pustejovsky, James (1991). &amp;quot;The syntax of event siructure.&amp;quot; Cognition, 41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>(in press a). &amp;quot;Principles versus criteria: On Randall&apos;s catapult hypothesis.&amp;quot; In Theoretical Issues in Language Acquisition,</title>
<note>edited by</note>
<marker>Pustejovsky, </marker>
<rawString>Pustejovsky, James (in press a). &amp;quot;Principles versus criteria: On Randall&apos;s catapult hypothesis.&amp;quot; In Theoretical Issues in Language Acquisition, edited by</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Weissenborn</author>
<author>H Goodluck</author>
<author>T Roeper</author>
</authors>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<marker>Weissenborn, Goodluck, Roeper, </marker>
<rawString>J. Weissenborn, H. Goodluck, and T. Roeper. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>(in press b). &amp;quot;Type coercion and lexical selection.&amp;quot;</title>
<booktitle>In Semantics and the Lexicon,</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<note>edited by</note>
<marker>Pustejovsky, </marker>
<rawString>Pustejovsky, James (in press b). &amp;quot;Type coercion and lexical selection.&amp;quot; In Semantics and the Lexicon, edited by J. Pustejovsky. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon: A Theory of Computational Lexical Semantics.</title>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Pustejovsky, </marker>
<rawString>Pustejovsky, James (forthcoming). The Generative Lexicon: A Theory of Computational Lexical Semantics. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Peter Anick</author>
</authors>
<title>On the semantic interpretation of nominals.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, COLING-1988,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="49204" citStr="Pustejovsky and Anick (1988)" startWordPosition="7919" endWordPosition="7922">ed out the answer. (= what the answer is) Example 37 a. John shocked me with his bad behavior. (= how bad his behavior is) b. You&apos;d be surprised at the big cars he buys. (= how big the cars he buys are) That is, although the italicized phrases syntactically appear as NPs, their semantics is the same as if the verbs had selected an overt question or exclamation. In explaining the behavior of the systematic ambiguity above, I made reference to properties of the noun phrase that are not typical semantic properties for nouns in linguistics; e.g., artifact, natural kind. In Pustejovsky (1989b) and Pustejovsky and Anick (1988), I suggest that there is a system of relations that characterizes the semantics of nominals, very much like the argument structure of a verb. I called this the Qualia Structure, inspired by Aristotle&apos;s theory of explanation and ideas from Moravcsik (1975). Essentially, the qualia structure of a noun determines its meaning as much as the list of arguments determines a verb&apos;s meaning. The elements that make up a qualia structure include notions such as container, space, surface, figure, artifact, and so on.&amp;quot; As stated earlier, there are four basic roles that constitute the qualia structure for </context>
<context position="56438" citStr="Pustejovsky and Anick (1988)" startWordPosition="9108" endWordPosition="9111">predicate interpretation, without any syntactic transformations (cf. Pustejovsky [1989a] for details).21 The derivation in 49(a) and the structure in 49(b) show the effects of this coercion on the verb&apos;s complement, using the telic value of nove1.22 21 There are, of course, an indefinite number of interpretations, depending on pragmatic factors and various contextual influences. But 1 maintain that there are only a finite number of default interpretations available in such constructions. These form part of the lexical semantics of the noun. Additional evidence for this distinction is given in Pustejovsky and Anick (1988) and Briscoe et al. (1990). 22 Partee and Rooth (1983) suggest that all expressions in the language can be assigned a base type, while also being associated with a type ladder. Pustejovsky (1989a) extends this proposal, and argues that each expression a may have available to it, a set of shifting operators, which we call Ea, which operate over an expression, changing its type and denotation. By making reference to these operators directly in the rule of function application, we can treat the functor polymorphically, as illustrated below. 1. Function Application with Coercion (FAO: If a is of t</context>
<context position="61738" citStr="Pustejovsky and Anick (1988)" startWordPosition="10002" endWordPosition="10005">= AeP [ travel(cars)(eP) A on(x)(cars)(eP) A fast(eP) ]]] As our final example of how the qualia structure contributes to the semantic interpretation of a sentence, observe how the nominals window and door in Examples 54 and 55 carry two interpretations (cf. Lakoff [1987] and Pustejovsky and Anick [19881): Example 54 a. John crawled through the window. b. The window is closed. 431 Computational Linguistics Volume 17, Number 4 Example 55 a. Mary painted the door. b. Mary walked through the door. Each noun appears to have two word senses: a physical object denotation and an aperture denotation. Pustejovsky and Anick (1988) characterize the meaning of such &amp;quot;Double Figure-Ground&amp;quot; nominals as inherently relational, where both parameters are logically part of the meaning of the noun. In terms of the qualia structure for this class of nouns, the formal role takes as its value the Figure of a physical object, while the constitutive role assumes the Invert-Figure value of an aperture.24 Example 56 Lexical Semantics for door: door(*x*,*y*) Const: aperture (*y*) Form: phys—obj(*x*) Telic: pass—through(T,z,*y*) Agentive: artifact(*x*) The foregrounding or backgrounding of a nominal&apos;s qualia is very similar to argument st</context>
</contexts>
<marker>Pustejovsky, Anick, 1988</marker>
<rawString>Pustejovsky, James, and Anick, Peter (1988). &amp;quot;On the semantic interpretation of nominals.&amp;quot; In Proceedings, COLING-1988, Budapest.</rawString>
</citation>
<citation valid="false">
<booktitle>Lexical Semantics and Commonsense Reasoning.</booktitle>
<editor>Pustejovsky, James, and Bergler, Sapine, eds. (in press).</editor>
<publisher>Springer Verlag.</publisher>
<location>Berlin:</location>
<marker></marker>
<rawString>Pustejovsky, James, and Bergler, Sapine, eds. (in press). Lexical Semantics and Commonsense Reasoning. Berlin: Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Bran Boguraev</author>
</authors>
<title>Lexical knowledge representation and natural language processing.&amp;quot;</title>
<date>1991</date>
<journal>In IBM Journal of Research and Development,</journal>
<pages>45--4</pages>
<marker>Pustejovsky, Boguraev, 1991</marker>
<rawString>Pustejovsky, James, and Boguraev, Bran (1991). &amp;quot;Lexical knowledge representation and natural language processing.&amp;quot; In IBM Journal of Research and Development, 45:4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Roberts</author>
<author>Goldstein</author>
</authors>
<title>The FRL Manual,</title>
<date>1977</date>
<tech>Technical Report Al Memo 409,</tech>
<institution>MIT Artificial Intelligence Laboratory.</institution>
<contexts>
<context position="64988" citStr="Roberts and Goldstein 1977" startWordPosition="10481" endWordPosition="10484">ility deals with the logical associations a word has in a given context; that is, how this semantic information is organized as a global knowledge base. This involves capturing both the inheritance relations between concepts and, just as importantly, how the concepts are integrated into a coherent expression in a given sentence. I will assume that there are two inheritance mechanisms at work for representing the conceptual relations in the lexicon: fixed inheritance and projective inheritance. The first includes the methods of inheritance traditionally assumed in Al and lexical research (e.g. Roberts and Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), </context>
</contexts>
<marker>Roberts, Goldstein, 1977</marker>
<rawString>Roberts, R. B., and Goldstein, (1977). The FRL Manual, Technical Report Al Memo 409, MIT Artificial Intelligence Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko J H Scha</author>
</authors>
<title>Logical foundations for question answering.&amp;quot;</title>
<date>1983</date>
<booktitle>MS 12.331 Philips Research Laboratories,</booktitle>
<location>Eindhoven, the Netherlands.</location>
<contexts>
<context position="5455" citStr="Scha 1983" startWordPosition="823" endWordPosition="824">s argued in Levin and Rappaport (1986), named roles are useful at best for establishing fairly general mapping strategies to the syntactic structures in language. The distinctions possible with thetaroles are much too coarse-grained to provide a useful semantic interpretation of a sentence. What is needed, I will argue, is a principled method of lexical decomposition. This presupposes, if it is to work at all, (1) a rich, recursive theory of semantic composition, (2) the notion of semantic well-formedness mentioned above, and (3) an appeal to several levels of interpretation in the semantics (Scha 1983). Thirdly, and related to the point above, the lexicon is not just verbs. Recent work has done much to clarify the nature of verb classes and the syntactic constructions that each allows (Levin 1985, 1989). Yet it is not clear whether we are any closer to understanding the underlying nature of verb meaning, why the classes develop as they do, and what consequences these distinctions have for the rest of the lexicon and grammar. The curious thing is that there has been little attention paid to the other lexical categories (but see Miller and Johnson-Laird [1976], Miller and Fellbaum [1991], and</context>
</contexts>
<marker>Scha, 1983</marker>
<rawString>Scha, Remko J. H. (1983). &amp;quot;Logical foundations for question answering.&amp;quot; MS 12.331 Philips Research Laboratories, Eindhoven, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
</authors>
<title>Conceptual Information Processing.</title>
<date>1975</date>
<publisher>Blackwell Publishers.</publisher>
<location>Amsterdam: North-Holland. Sueren, Pieter</location>
<contexts>
<context position="22262" citStr="Schank 1975" startWordPosition="3522" endWordPosition="3523">ion, where lexical items are minimally decomposed into structured forms (or templates) rather than sets of features. This will provide us with a generative framework for the composition of lexical meanings, thereby defining the well-formedness conditions for semantic expressions in a language. We can distinguish between two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsky (1981) assume, however, that there ar</context>
<context position="23507" citStr="Schank 1975" startWordPosition="3720" endWordPosition="3721">forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would tell us the min</context>
</contexts>
<marker>Schank, 1975</marker>
<rawString>Schank, Roger (1975). Conceptual Information Processing. Amsterdam: North-Holland. Sueren, Pieter (1985). Discourse Semantics. Oxford: Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Smolka</author>
</authors>
<title>A feature logic with subsorts.&amp;quot;</title>
<date>1988</date>
<journal>Wissenschaftliches Zentrum der IBM Deutschland, LILOG-Report</journal>
<volume>33</volume>
<contexts>
<context position="69572" citStr="Smolka (1988)" startWordPosition="11213" endWordPosition="11214">mation, and define them below: Definition A projective transformation, 7r, on a predicate Qi generates a predicate, Q2, such that 7r(Q1) = Q2, where Q2 cl (I) . The set of transformations includes: negation, &lt;, temporal precedence, &gt;, temporal succession, =, temporal equivalence, and act, an operator adding agency to an argument. Intuitively, the space of concepts traversed by the application of such operators will be related expressions in the neighborhood of the original lexical item. This space can be characterized by the following two definitions: 26 See, for example, Michalski (1983) and Smolka (1988) for a treatment making use of subsorts. 27 Such relations include not only hypernymy and hopyonymy, but also troponymy, which relates verbs by manner relations (cf. Miller 1985; Beckwith et al. 1989; Miller and Fellbaum 1991. 434 James Pustejovsky The Generative Lexicon Definition A series of applications of transformations, , 7rn, generates a sequence of predicates, (121 , • • • , Qn), called the projective expansion of Qi, P(Qi ). Definition The projective conclusion space, P(&apos;DR), is the set of projective expansions generated from all elements of the conclusion space, on role R of predicat</context>
</contexts>
<marker>Smolka, 1988</marker>
<rawString>Smolka, G. (1988). &amp;quot;A feature logic with subsorts.&amp;quot; Wissenschaftliches Zentrum der IBM Deutschland, LILOG-Report 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Stump</author>
</authors>
<title>Frequency adjectives.&amp;quot;</title>
<date>1981</date>
<journal>Linguistics and Philosophy,</journal>
<pages>4--221</pages>
<contexts>
<context position="12981" citStr="Stump 1981" startWordPosition="2030" endWordPosition="2031">rday (i.e. the institution). b. The store is next to the new bank (i.e. the building). • Establish what the compositional nature of a lexical item is when applied to other words. For example, alleged vs. female in Example 8. Example 8 a. the alleged suspect b. the female suspect While female behaves as a simple intersective modifier in 8b, certain modifiers such as alleged in 8a cannot be treated as simple attributes; rather, they create an intensional context for the head they modify. An even more difficult problem for compositionality arises from phrases containing frequency adjectives (cf. Stump 1981), as shown in 8c and 8d. Example 8 c. An occasional sailor walks by on the weekend. d. Caution: may contain an occasional pit (notice on a box of prunes). The challenge here is that the adjective doesn&apos;t modify the nominal head, but the entire proposition containing it (cf. Partee [1985] for discussion). A similar difficulty arises with the interpretation of scalar predicates such as fast in Example 9. Both the scale and the relative interpretation being selected for depends on the noun that the predicate is modifying. Example 9 a. a fast typist: one who types quickly b. a fast car: one which </context>
</contexts>
<marker>Stump, 1981</marker>
<rawString>Stump, G. (1981). &amp;quot;Frequency adjectives.&amp;quot; Linguistics and Philosophy, 4:221-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Talmy</author>
</authors>
<title>Semantics and syntax of motion.&amp;quot;</title>
<date>1975</date>
<journal>In Syntax and Semantics</journal>
<volume>4</volume>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="15011" citStr="Talmy (1975" startWordPosition="2351" endWordPosition="2352">edness among word senses; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. 3. Descriptive Adequacy of Existing Representations Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainly the descriptive vocabulary of Talmy (1975, 1985) and Jackendoff (1983), fine and subtle distinctions are drawn that were not captured in the earlier, primitives-based approach of Schank (1972, 1975) or the frame semantics of Fillmore (1968). As an example of the verb classifications developed by various researchers (and compiled by the MIT Lexicon Project; see Levin [1985, 1989]), consider the grammatical alternations in the example sentences below (cf. Dowty 1991). Example 10 a. John met Mary. b. John and Mary met. Example 11 a. A car ran into a truck. b. A car and a truck ran into each other. Example 12 a. A car ran into a tree. b.</context>
</contexts>
<marker>Talmy, 1975</marker>
<rawString>Talmy, Len (1975). &amp;quot;Semantics and syntax of motion.&amp;quot; In Syntax and Semantics 4, edited by]. P. Kimball. New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Talrny</author>
</authors>
<title>Lexicalization patterns.&amp;quot; In Language Typology and Syntactic Description, edited by Timothy Shopen.</title>
<date>1985</date>
<location>Cambridge.</location>
<marker>Talrny, 1985</marker>
<rawString>Talrny, Len (1985). &amp;quot;Lexicalization patterns.&amp;quot; In Language Typology and Syntactic Description, edited by Timothy Shopen. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Tenny</author>
</authors>
<title>Grammaticalizing aspect and affectedness.&amp;quot;</title>
<date>1987</date>
<note>Doctoral dissertation, MIT.</note>
<contexts>
<context position="35374" citStr="Tenny (1987)" startWordPosition="5618" endWordPosition="5619"> table clean. (wipe2 = cause(x, Become(clean(y)))) Example 25 a. Mary ran yesterday. (runi = move(x)) b. Mary ran to the store yesterday. (run2 = go-to(x,y)) • Although the complement types selected by bake in 22, for example, are semantically related, the two word senses are clearly distinct and therefore must be lexically distinguished. According to the sense enumeration view, the same argument holds for the verbs in 23-25 as well. 8 This proposal is an extension of ideas explored by Bach (1986), Higginbotham (1985), and Allen (1984). For a full discussion, see Pustejovsky (1988, 1991). See Tenny (1987) for a proposal on how aspectual distinctions are mapped to the syntax. 420 James Pustejovsky The Generative Lexicon A similar philosophy has lead linguists to multiply word senses in constructions involving Control and Equi-verbs, where different syntactic contexts necessitate different semantic types.&apos; Example 26 a. It seems that John likes Mary. b. John seems to like Mary. Example 27 a. Mary prefers that she come. b. Mary prefers to come. Normally, compositionality in such structures simply refers to the application of the functional element, the verb, to its arguments. Yet, such examples i</context>
</contexts>
<marker>Tenny, 1987</marker>
<rawString>Tenny, Carol (1987). &amp;quot;Grammaticalizing aspect and affectedness.&amp;quot; Doctoral dissertation, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Tenny</author>
</authors>
<title>The aspectual interface hypothesis,&amp;quot; Lexicon Project Working Papers 31,</title>
<date>1989</date>
<publisher>MIT.</publisher>
<marker>Tenny, 1989</marker>
<rawString>Tenny, Carol (1989). &amp;quot;The aspectual interface hypothesis,&amp;quot; Lexicon Project Working Papers 31, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Touretzky</author>
</authors>
<title>The Mathematics of Inheritance Systems.</title>
<date>1986</date>
<publisher>Morgan Kaufmann.</publisher>
<location>Los Altos, CA:</location>
<contexts>
<context position="67583" citStr="Touretzky 1986" startWordPosition="10885" endWordPosition="10886">g event. Yet this is not information that comes from a fixed inheritance structure, but is rather usually assumed to be cornmonsense knowledge. In what follows, however, I will show that such distinctions 25 Anick and Pustejovsky (1990) explore how metrics such as association ratios can be used to statistically measure the notions of prototypicality mentioned here. 433 Computational Linguistics Volume 17, Number 4 can be captured within a theory of lexical semantics by means of generating ad hoc categories. First, we give a definition for the fixed inheritance structure of a lexical item (cf. Touretzky 1986). Let Q and P be concepts in our model of lexical organization. Then: Definition A sequence (Qi , PO is an inheritance path, which can be read as the conjunction of ordered pairs {(xi, y,) Ii &lt; i &lt; n}. Furthermore, following Touretsky, from this we can define the set of concepts that lie on an inheritance path, the conclusion space. Definition The conclusion space of a set of sequences (I) is the set of all pairs (Q, P) such that a sequence (Q, , P) appears in 1. From these two definitions we can define the traditional is-a relation, relating the above pairs by a generalization operator, &lt;G,&amp;quot; </context>
</contexts>
<marker>Touretzky, 1986</marker>
<rawString>Touretzky, David S. (1986). The Mathematics of Inheritance Systems. Los Altos, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics and Philosophy.</title>
<date>1967</date>
<publisher>Cornell University Press.</publisher>
<location>Ithaca, NY:</location>
<marker>Vendler, 1967</marker>
<rawString>Vendler, Zeno (1967). Linguistics and Philosophy. Ithaca, NY: Cornell University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manindra Verma</author>
<author>K P Mohanan</author>
</authors>
<date>1991</date>
<booktitle>Experiencer Subjects in South Asian Languages, CSLI. Distributed by University of</booktitle>
<publisher>Chicago Press.</publisher>
<contexts>
<context position="47651" citStr="Verma and Mohanan (1991)" startWordPosition="7668" endWordPosition="7671">ed Mary. b. The gun killed Mary. c. John&apos;s stupidity killed Mary. d. The war killed Mary. e. John&apos;s pulling the trigger killed Mary. As these examples illustrate, the syntactic argument to a verb is not always the same logical argument in the semantic relation. Although superficially similar to cases of general metonymy (cf. Lakoff and Johnson 1982; Nunberg 1978), there is an interesting systematicity to such shifts in meaning that we will try to characterize below as logical metonymy. 17 I am following Cardelli and Wegener (1985) and their characterization of polymorphistnic behavior. 18 See Verma and Mohanan (1991) for an extensive survey of experiencer subject constructions in different languages. 425 Computational Linguistics Volume 17, Number 4 The sentences in 34 illustrate the various syntactic consequences of metonymy and coercion involving experiencer verbs, while those in 35 show the different metonymic extensions possible from the causing event in a killing. The generalization here is that when a verb selects an event as one of its arguments, type coercion to an event will permit a limited range of logical metonymies. For example, in sentences 34(a,b,c,d,f,h), the entire event is directly refer</context>
</contexts>
<marker>Verma, Mohanan, 1991</marker>
<rawString>Verma, Manindra, and Mohanan, K. P. (1991). Experiencer Subjects in South Asian Languages, CSLI. Distributed by University of Chicago Press.</rawString>
</citation>
<citation valid="false">
<booktitle>Automating the Lexicon.</booktitle>
<editor>Walker, Donald; Zampolli, Antonio, and Calzolari, Nicoletta, (eds.) (forthcoming).</editor>
<publisher>Oxford University Press.</publisher>
<marker></marker>
<rawString>Walker, Donald; Zampolli, Antonio, and Calzolari, Nicoletta, (eds.) (forthcoming). Automating the Lexicon. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uriel Weinreich</author>
</authors>
<title>Explorations in Semantic Theory.</title>
<date>1972</date>
<publisher>The Hague: Mouton.</publisher>
<contexts>
<context position="12044" citStr="Weinreich (1972)" startWordPosition="1884" endWordPosition="1885">as begin and finish as shown in Example 5. Example 5 a. Mary finished the cigarette. b. Mary finished her beer. The exact meaning of the verb finish varies depending on the object it selects, assuming for these examples the meanings finish smoking or finish drinking. • Test for the ambiguity of a word. Distinguish between homonymy and polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and logical aspects of ambiguity. For example, the homonymy between the two senses of bank in Example 6 is accidental.&apos; Example 6 a. the bank of the river b. the richest bank in the city 2 Cf. Weinreich (1972) distinguishes between contrastive and complementary polysemy, essentially covering this same distinction. See Section 4 for discussion. 412 James Pustejovsky The Generative Lexicon In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich 1972; Lakoff 1987). Example 7 a. The bank raised its interest rates yesterday (i.e. the institution). b. The store is next to the new bank (i.e. the building). • Establish what the compositional nature of a lexical item is when applied to other words. For example, alleged vs. female in Example 8. Example 8 a. the alleged suspect b. the female su</context>
</contexts>
<marker>Weinreich, 1972</marker>
<rawString>Weinreich, Uriel (1972). Explorations in Semantic Theory. The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Wierzbicka</author>
</authors>
<title>The Semantics of Grammar.</title>
<date>1988</date>
<location>Amsterdam: John Benjamins.</location>
<marker>Wierzbicka, 1988</marker>
<rawString>Wierzbicka, Anna (1988). The Semantics of Grammar. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wilks</author>
</authors>
<title>Yorick (1975a). &amp;quot;A preferential pattern seeking semantics for natural language inference.&amp;quot;</title>
<journal>Artificial Intelligence,</journal>
<volume>6</volume>
<pages>53--74</pages>
<marker>Wilks, </marker>
<rawString>Wilks, Yorick (1975a). &amp;quot;A preferential pattern seeking semantics for natural language inference.&amp;quot; Artificial Intelligence, 6: 53-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>An intelligent analyser and understander for English.&amp;quot;</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<pages>264--274</pages>
<contexts>
<context position="11802" citStr="Wilks 1975" startWordPosition="1840" endWordPosition="1841">got that he locked the door. b. John forgot to lock the door. Example 4a has a factive interpretation of forget that 4b does not carry: in fact, 4b is counterfactive. Other cases of contextual specification involve aspectual verbs such as begin and finish as shown in Example 5. Example 5 a. Mary finished the cigarette. b. Mary finished her beer. The exact meaning of the verb finish varies depending on the object it selects, assuming for these examples the meanings finish smoking or finish drinking. • Test for the ambiguity of a word. Distinguish between homonymy and polysemy, (cf. Hirst 1987; Wilks 1975b); that is, from the accidental and logical aspects of ambiguity. For example, the homonymy between the two senses of bank in Example 6 is accidental.&apos; Example 6 a. the bank of the river b. the richest bank in the city 2 Cf. Weinreich (1972) distinguishes between contrastive and complementary polysemy, essentially covering this same distinction. See Section 4 for discussion. 412 James Pustejovsky The Generative Lexicon In contrast, the senses in Example 7 exhibit a polysemy (cf. Weinreich 1972; Lakoff 1987). Example 7 a. The bank raised its interest rates yesterday (i.e. the institution). b. </context>
<context position="16820" citStr="Wilks (1975" startWordPosition="2661" endWordPosition="2662"> adding the preposition at to the verb, changing the verb meaning to an action directed toward an object. Example 13 a. Mary cut the bread. b. Mary cut at the bread. Example 14 a. Mary broke the bread. b. *Mary broke at the bread. What these data indicate is that the conative is possible only with verbs of a particular semantic class; namely, verbs that specify the manner of an action that results in a change of state of an object. As useful and informative as the research on verb classification is, there is a major shortcoming with this approach. Unlike the theories of Katz and Fodor (1963), Wilks (1975a), and Quillian (1968), there is no general coherent view on what the entire lexicon will look like when semantic structures for other major categories are studied. This can be essential for establishing a globally coherent theory of semantic representation. On the other hand, the semantic distinctions captured by these older theories were often too coarse-grained. It is clear, therefore, that the classifications made by Levin and her colleagues are an important starting point for a serious theory of knowledge representation. I claim that lexical semantics must build upon this research toward</context>
<context position="22223" citStr="Wilks 1975" startWordPosition="3516" endWordPosition="3517">a conservative approach to decomposition, where lexical items are minimally decomposed into structured forms (or templates) rather than sets of features. This will provide us with a generative framework for the composition of lexical meanings, thereby defining the well-formedness conditions for semantic expressions in a language. We can distinguish between two distinct approaches to the study of word meaning: primitive-based theories and relation-based theories. Those advocating primitives assume that word meaning can be exhaustively defined in terms of a fixed set of primitive elements (e.g. Wilks 1975a; Katz 1972; Lakoff 1971; Schank 1975). Inferences are made through the primitives into which a word is decomposed. In contrast to this view, a relation-based theory of word meaning claims that there is no need for decomposition into primitives if words (and their concepts) are associated through a network of explicitly defined links (e.g. Quillian 1968; Collins and Quillian 1969; Fodor 1975; Carnap 1956; Brachman 1979). Sometimes referred to as meaning postulates, these links establish any inference between words as an explicit part of a network of word 3 Both Klein and Sag (1985) and Chomsk</context>
<context position="23493" citStr="Wilks 1975" startWordPosition="3718" endWordPosition="3719">g these two forms structurally. See below and Pustejovsky (1989a) for details. 416 James Pustejovsky The Generative Lexicon concepts.4 What I would like to do is to propose a new way of viewing primitives, looking more at the generative or compositional aspects of lexical semantics, rather than the decomposition into a specified number of primitives. Most approaches to lexical semantics making use of primitives can be characterized as using some form of feature-based semantics, since the meaning of a word is essentially decomposable into a set of features (e.g. Katz and Fodor 1963; Katz 1972; Wilks 1975; Schank 1975). Even those theories that rely on some internal structure for word meaning (e.g. Dowty 1979; Fillmore 1985) do not provide a complete characterization for all of the well-formed expressions in the language. Jackendoff (1983) comes closest, but falls short of a comprehensive semantics for all categories in language. No existing framework, in my view, provides a method for the decomposition of lexical categories. What exactly would a method for lexical decomposition give us? Instead of a taxonomy of the concepts in a language, categorized by sets of features, such a method would t</context>
<context position="29959" citStr="Wilks (1975" startWordPosition="4767" endWordPosition="4768"> we must realize that part of the meaning of a word is how it translates the underlying semantic representations into expressions that are utilized by the syntax. This is what many have called the argument structure for a lexical item. I will build on Grimshaw&apos;s recent proposals (Grimshaw 1990) for how to define the mapping from the lexicon to syntax. to a particular vocabulary of primitives, a lexical semantics should provide a method for the decomposition and composition of lexical items. 7 Some of these roles are reminiscent of descriptors used by various computational researchers, such as Wilks (1975b), Hayes (1979), and Hobbs et al. (1987). Within the theory outlined here, these roles determine a minimal semantic description of a word that has both semantic and grammatical consequences. 418 James Pustejovsky The Generative Lexicon This provides us with an answer to the question of what levels of semantic representation are necessary for a computational lexical semantics. In sum, I will argue that lexical meaning can best be captured by assuming the following levels of representation. 1. Argument Structure: The behavior of a word as a function, with its arity specified. This is the predic</context>
<context position="45092" citStr="Wilks (1975)" startWordPosition="7245" endWordPosition="7246">e our discussion of lexical ambiguity with the issue of metonymy. Metonymy, where a subpart or related part of an object &amp;quot;stands for&amp;quot; the object itself, also poses a problem for standard denotational theories of semantics. To see why, imagine how our semantics could account for the &amp;quot;reference shifts&amp;quot; of the complements shown in Example 33.16 Example 33 a. Mary enjoyed the book. b. Thatcher vetoed the channel tunnel. (Cf. Hobbs 1987) c. John began a novel. 16 See Nunberg (1978) and Fauconnier (1985) for very clear discussions of the semantics of metonymy and the nature of reference shifts. See Wilks (1975) and Fass (1988) for computational models of metonymic resolution. 424 James Pustejovsky The Generative Lexicon The complements of enjoy in 33(a) and begin in 33(c) are not what these verbs normally select for semantically, namely a property or action. Similarly, the verb veto normally selects for an object that is a legislative bill or a suggestion. Syntactically, these may simply be additional subcategorizations, but how are these examples related semantically to the normal interpretations? I suggest that these are cases of semantic type coercion (cf. Pustejovsky 1989a), where the verb has c</context>
<context position="65586" citStr="Wilks (1975)" startWordPosition="10580" endWordPosition="10581">Goldstein 1977; Brachman and Schmolze 1985; Bobrow and Winograd 1977); that is, a fixed network of relations, which is traversed to discover existing related and associated concepts (e.g. hyponyms and hypernyms). In order to arrive at a comprehensive theory of the lexicon, we need to address the issue of global organization, and this involves looking at the various modes of inheritance that exist in language and conceptualization. Some of the best work addressing the issue of how the lexical semantics of a word ties into its deeper conceptual structure includes that of Hobbs et al. (1987) and Wilks (1975), while interesting work on shared information structures in NLP domains is that of Flickinger et al. (1985) and Evans and Gazdar (1989, 1990). In addition to this static representation, I will introduce another mechanism for structuring lexical knowledge, the projective inheritance, which operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories. Both are necessary for projecting the semantic representations of individual lexical items onto a sentence level interpretation. The discussion here, however, will be limited to a descrip</context>
</contexts>
<marker>Wilks, 1975</marker>
<rawString>Wilks, Yorick (1975b). &amp;quot;An intelligent analyser and understander for English.&amp;quot; Communications of the ACM, 18: 264-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Dan Fass</author>
<author>Cheng-Ming Guo</author>
<author>James McDonald</author>
<author>Tony Plate</author>
<author>Brian Slator</author>
</authors>
<title>A tractable machine dictionary as a resource for computational semantics.&amp;quot;</title>
<date>1988</date>
<booktitle>In Computational Lexicography for Natural Language Processing, edited by Bran Boguraev</booktitle>
<publisher>Longman.</publisher>
<location>Harlow, Essex:</location>
<contexts>
<context position="14368" citStr="Wilks et al. (1988)" startWordPosition="2257" endWordPosition="2260"> for by a theory of semantics. This just briefly characterizes some of the techniques that have been useful for arriving at pre-theoretic notions of word meaning. What has changed over the years are not so much the methods themselves as the descriptive details provided by each test. One thing that has changed, however — and this is significant — is the way computational lexicography has provided stronger techniques and even new tools for lexical semantics research: see Atkins (1987) for sense discrimination tasks; Amsler (1985), Atkins et al. (forthcoming) for constructing concept taxonomies; Wilks et al. (1988) for establishing semantic relatedness among word senses; and Boguraev and Pustejovsky (forthcoming) for testing new ideas about semantic representations. 3. Descriptive Adequacy of Existing Representations Turning now to the question of how current theories compare with the coverage of lexical semantic data, there are two generalizations that should be made. First, the 413 Computational Linguistics Volume 17, Number 4 taxonomic descriptions that have recently been made of verb classes are far superior to the classifications available twenty years ago (see Levin [1985] for review). Using mainl</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1988</marker>
<rawString>Wilks, Yorick; Fass, Dan; Guo, Cheng-Ming; McDonald, James; Plate, Tony; and Slator, Brian (1988). &amp;quot;A tractable machine dictionary as a resource for computational semantics.&amp;quot; In Computational Lexicography for Natural Language Processing, edited by Bran Boguraev and Ted Briscoe. Harlow, Essex: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin Williams</author>
</authors>
<title>Argument structure and morphology&amp;quot;</title>
<date>1981</date>
<journal>Linguistic Review,</journal>
<pages>1--81</pages>
<marker>Williams, 1981</marker>
<rawString>Williams, Edwin (1981). &amp;quot;Argument structure and morphology&amp;quot; Linguistic Review, 1:81-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg H von Wright</author>
</authors>
<title>Norm and Action: A Logical Inquiry. London: Routledge and Kegan Paul.</title>
<date>1963</date>
<marker>von Wright, 1963</marker>
<rawString>von Wright, Georg H. (1963). Norm and Action: A Logical Inquiry. London: Routledge and Kegan Paul.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>