<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040379">
<title confidence="0.970685">
Sentence Boundary Detection and the Problem with the U.S.
</title>
<author confidence="0.99868">
Dan Gillick
</author>
<affiliation confidence="0.998673">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.995391">
dgillick@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997163625">
Sentence Boundary Detection is widely used
but often with outdated tools. We discuss what
makes it difficult, which features are relevant,
and present a fully statistical system, now pub-
licly available, that gives the best known er-
ror rate on a standard news corpus: Of some
27,000 examples, our system makes 67 errors,
23 involving the word “U.S.”
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926846153846">
Many natural language processing tasks begin by
identifying sentences, but due to the semantic am-
biguity of the period, the sentence boundary detec-
tion (SBD) problem is non-trivial. While reported
error rates are low, significant improvement is pos-
sible and potentially valuable. For example, since
a single error can ruin an automatically generated
summary, reducing the error rate from 1% to 0.25%
reduces the rate of damaged 10-sentence summaries
from 1 in 10 to 1 in 40. Better SBD may improve
language models and sentence alignment as well.
SBD has been addressed only a few times in the
literature, and each result points to the importance of
developing lists of common abbreviations and sen-
tence starters. Further, most practical implementa-
tions are not readily available (with one notable ex-
ception). Here, we present a fully statistical system
that we argue benefits from avoiding manually con-
structed or tuned lists. We provide a detailed anal-
ysis of features, training variations, and errors, all
of which are under-explicated in the literature, and
discuss the possibility of a more structured classifi-
cation approach. Our implementation gives the best
performance, to our knowledge, reported on a stan-
dard Wall Street Journal task; it is open-source and
available to the public.
</bodyText>
<sectionHeader confidence="0.996518" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999687157894737">
We briefly outline the most important existing meth-
ods and cite error rates on a standard English data
set, sections 03-06 of the Wall Street Journal (WSJ)
corpus (Marcus et al., 1993), containing nearly
27,000 examples. Error rates are computed as
(number incorrect/total ambiguous periods). Am-
biguous periods are assumed to be those followed
by white space or punctuation. Guessing the major-
ity class gives a 26% baseline error rate.
A variety of systems use lists of hand-crafted reg-
ular expressions and abbreviations, notably Alem-
bic (Aberdeen et al., 1995), which gives a 0.9% er-
ror rate. Such systems are highly specialized to lan-
guage and genre.
The Satz system (Palmer and Hearst, 1997)
achieves a 1.0% error rate using part-of-speech
(POS) features as input to a neural net classifier (a
decision tree gives similar results), trained on held-
out WSJ data. Features were generated using a
5000-word lexicon and a list of 206 abbreviations.
Another statistical system, mxTerminator (Reynar
and Ratnaparkhi, 1997) employs simpler lexical fea-
tures of the words to the left and right of the can-
didate period. Using a maximum entropy classifier
trained on nearly 1 million words of additional WSJ
data, they report a 1.2% error rate with an automati-
cally generated abbreviation list and special corpus-
specific abbreviation features.
There are two notable unsupervised systems.
Punkt (Kiss and Strunk, 2006) uses a set of log-
likelihood-based heuristics to infer abbreviations
and common sentence starters from a large text
corpus. Deriving these lists from the WSJ test
data gives an error rate of 1.65%. Punkt is eas-
ily adaptable but requires a large (unlabeled) in-
domain corpus for assembling statistics. An imple-
mentation is bundled with NLTK (Loper and Bird,
2002). (Mikheev, 2002) describes a “document-
</bodyText>
<page confidence="0.971892">
241
</page>
<note confidence="0.3575175">
Proceedings of NAACL HLT 2009: Short Papers, pages 241–244,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963363636364">
centered” approach to SBD, using a set of heuris-
tics to guess which words correspond to abbrevia-
tions and names. Adding carefully tuned lists from
an extra news corpus gives an error rate of 0.45%,
though this increases to 1.41% without the abbrevi-
ation list. Combining with a supervised POS-based
system gives the best reported error rate on this task:
0.31%.
Our system is closest in spirit to mxTerminator,
and we use the same training and test data in our
experiments to aid comparison.
</bodyText>
<sectionHeader confidence="0.970497" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999385212121212">
Each example takes the general form “L. R”, where
L is the context on the left side of the period in
question, and R is the context on the right (we use
only one word token of context on each side). We
are interested in the probability of the binary sen-
tence boundary class s, conditional on its context:
P(s|“L. R”). We take a supervised learning ap-
proach, extracting features from “L. R”.
Table 1 lists our features and their performance,
using a Support Vector Machine (SVM) with a lin-
ear kernel1. Feature 1 by itself, the token ending
with the candidate period, gives surprisingly good
performance, and the combination of 1 and 2 out-
performs nearly all documented systems. While no
published result uses an SVM, we note that a simple
Naive Bayes classifier gives an error rate of 1.05%
(also considerably better than mxTerminator), sug-
gesting that the choice of classifier alone does not
explain the performance gap.
There are a few possible explanations. First,
proper tokenization is key. While there is not room
to catalog our tokenizer rules, we note that both un-
tokenized text and mismatched train-test tokeniza-
tion can increase the error rate by a factor of 2.
Second, poor feature choices can hurt classifica-
tion. In particular, adding a feature that matches a
list of abbreviations can increase the error rate; us-
ing the list (“Mr.”, “Co.”) increases the number of
errors by up to 25% in our experiments. This is be-
cause some abbreviations end sentences often, and
others do not. In the test data, 0 of 1866 instances
of “Mr.” end a sentence, compared to 24 of 86 in-
stances of “Calif.” (see Table 2). While there may
</bodyText>
<footnote confidence="0.9860525">
1We use SVM Light, with c = 1 (Joachims, 1999). Non-
linear kernels did not improve performance in our experiments.
</footnote>
<table confidence="0.999927153846154">
# Feature Description Error
1 L = wi 1.88%
2 R=wj 9.36%
3 len(L) = l 9.12%
4 is cap(R) 12.56%
5 int(log(count(L; no period))) = ci 12.14%
6 int(log(count(R; is lower)) = cj 18.79%
7 (L=wi,R=wj) 10.01%
8 (L = wi, is cap(R)) 7.54%
1+2 0.77%
1+2+3+4 0.36%
1+2+3+4+5+6 0.32%
1+2+3+4+5+6+7+8 0.25%
</table>
<tableCaption confidence="0.8299532">
Table 1: All features are binary. SVM classification re-
sults shown; Naive Bayes gives 0.35% error rate with all
features.
be meaningful abbreviation subclasses, a feature in-
dicating mere presence is too coarse.
</tableCaption>
<table confidence="0.999838714285714">
Abbr. Ends Sentence Total Ratio
Inc. 109 683 0.16
Co. 80 566 0.14
Corp. 67 699 0.10
U.S. 45 800 0.06
Calif. 24 86 0.28
Ltd. 23 112 0.21
</table>
<tableCaption confidence="0.9445245">
Table 2: The abbreviations appearing most often as sen-
tence boundaries. These top 6 account for 80% of
sentence-ending abbreviations in the test set, though only
5% of all abbreviations.
</tableCaption>
<bodyText confidence="0.99975325">
Adding features 3 and 4 better than cuts the re-
maining errors in half. These can be seen as a kind
of smoothing for sparser token features 1 and 2. Fea-
ture 3, the length of the left token, is a reasonable
proxy for the abbreviation class (mean abbreviation
length is 2.6, compared to 6.1 for non-abbreviation
sentence enders). The capitalization of the right to-
ken, feature 4, is a proxy for a sentence starter. Ev-
ery new sentence that starts with a word (as opposed
to a number or punctuation) is capitalized, but 70%
of words following abbreviations are also, so this
feature is mostly valuable in combination.
While we train on nearly 1 million words, most of
these are ignored because our features are extracted
only near possible sentence boundaries. Consider
the fragment “... the U.S. Apparently some ...”,
</bodyText>
<page confidence="0.986767">
242
</page>
<bodyText confidence="0.996093454545455">
which our system fails to split after “U.S.” The word
“Apparently” starts only 8 sentences in the train-
ing data, but since it usually appears lowercased (89
times in training), its capitalization here is meaning-
ful. Feature 6 encodes this idea, indicating the log
count of lowercased appearances of the word right
of the candidate period. Similarly, feature 5 gives
the log count of occurrences of the token left of the
candidate appearing without a final period.
Another way to incorporate all of the training
data is to build a model of P(s|“L R”), as is of-
ten used in sentence segmentation for speech recog-
nition. Without a period in the conditional, many
more negative examples are included. The resulting
SVM model is very good at placing periods given
input text without them (0.31% error rate), but when
limiting the input to examples with ambiguous peri-
ods, the error rate is not competitive with our origi-
nal model (1.45%).
Features 7 and 8 are added to model the nuances
of abbreviations at sentence boundaries, helping to
reduce errors involving the examples in Table 2.
</bodyText>
<sectionHeader confidence="0.983434" genericHeader="method">
4 Two Classes or Three?
</sectionHeader>
<bodyText confidence="0.997164833333334">
SBD has always been treated as a binary classifica-
tion problem, but there are really three classes: sen-
tence boundary only (S); abbreviation only (A); ab-
breviation at sentence boundary (A + S). The label
space of the test data, which has all periods anno-
tated, is shown in Figure 1.
</bodyText>
<figureCaption confidence="0.998713">
Figure 1: The overlapping label space of the test data:
sentence boundaries 74%; abbreviations 26%; intersec-
tion 2%. The distribution of errors given by our classifier
is shown as well (not to scale with all data).
</figureCaption>
<bodyText confidence="0.99771">
Relative to the size of the classes, A + S exam-
ples are responsible for a disproportionate number
of errors, pointing towards the problem with a bi-
nary classifier: In the absence of A + S examples,
the left context L and the right context R both help
distinguish S from A. But A + S cases have L re-
sembling the A class and R resembling the S class.
One possibility is to add a third class, but this does
not improve results, probably because we have so
few A + S examples. We also tried taking a more
structured approach, depicted in Figure 2, but this
too fails to improve performance, mostly because
the first step, identifying abbreviations without the
right context, is too hard. Certainly, the A + S cases
are more difficult to identify, but perhaps some bet-
ter structured approach could reduce the error rate
further.
</bodyText>
<figureCaption confidence="0.997458">
Figure 2: A structured classification approach. The left
context is used to separate S examples first, then those
remaining are classified as either A or A + S using the
right context.
</figureCaption>
<sectionHeader confidence="0.916042" genericHeader="method">
5 Training Data
</sectionHeader>
<bodyText confidence="0.9997864375">
One common objection to supervised SBD systems
is an observation in (Reynar and Ratnaparkhi,1997),
that training data and test data must be a good match,
limiting the applicability of a model trained from a
specific genre. Table 3 shows respectable error rates
for two quite different test sets: The Brown corpus
includes 500 documents, distributed across 15 gen-
res roughly representative of all published English;
The Complete Works of Edgar Allen Poe includes
an introduction, prose, and poetry.
A second issue is a lack of labeled data, espe-
cially in languages besides English. Table 4 shows
that results can be quite good without extensive la-
beled resources, and they are likely to continue to
improve if additional resources were available. At
the least, (Kiss and Strunk, 2006) have labeled over
</bodyText>
<figure confidence="0.9987528125">
Abbreviations (A)
(A+S)
Sentence Boundaries (S)
Errors
(A+S)
All Data
(A)
S
no
P(A  |“L.”) &gt; 0.5
yes
P(S  |“R”) &gt; 0.5
A
no
A+S
yes
</figure>
<page confidence="0.995957">
243
</page>
<table confidence="0.989465">
Corpus Examples in S SVM Err NB Err
WSJ 26977 74% 0.25% 0.35%
Brown 53688 91% 0.36% 0.45%
Poe 11249 95% 0.52% 0.44%
</table>
<tableCaption confidence="0.935540333333333">
Table 3: SVM and Naive Bayes classification error rates
on different corpora using a model trained from a disjoint
WSJ data set.
</tableCaption>
<table confidence="0.895896">
10000 sentences in each of 11 languages, though we
have not experimented with this data.
Corpus 5 50 500 5000 42317
WSJ 7.26% 3.57% 1.36% 0.52% 0.25%
Brown 5.65% 4.46% 1.65% 0.74% 0.36%
Poe 4.01% 2.68% 2.22% 0.98% 0.52%
</table>
<tableCaption confidence="0.997544">
Table 4: SVM error rates on the test corpora, using mod-
els built from different numbers of training sentences.
</tableCaption>
<bodyText confidence="0.999899428571429">
We also tried to improve results using a standard
bootstrapping method. Our WSJ-trained model was
used to annotate 100 million words of New York
Times data from the AQUAINT corpus, and we in-
cluded high-confidence examples in a new training
set. This did not degrade test error, nor did it im-
prove it.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="method">
6 Errors
</sectionHeader>
<bodyText confidence="0.999985466666667">
Our system makes 67 errors out of 26977 examples
on the WSJ test set; a representative few are shown
in Table 5. 34% of the errors involve the word “U.S.”
which distinguishes itself as the most difficult of to-
kens to classify: Not only does it appear frequently
as a sentence boundary, but even when it does not,
the next word is often capitalized (“U.S. Govern-
ment”; “U.S. Commission”), further confusing the
classifier. In fact, abbreviations for places, includ-
ing “U.K.”, “N.Y.”, “Pa.” constitute 46% of all er-
rors for the same reason. Most of the remaining er-
rors involve abbreviations like those in Table 2, and
all are quite difficult for a human to resolve without
more context. Designing features to exploit addi-
tional context might help, but could require parsing.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998975">
We have described a simple yet powerful method for
SBD. While we have not tested models in languages
other than English, we are providing the code and
our models, complete with tokenization, available
</bodyText>
<figure confidence="0.962458">
Context Label P(S)
... the U.S. Amoco already ... A + S 0.45
... the U.K. Panel on ... A 0.57
... the U.S. Prudential Insurance ... A + S 0.44
... Telephone Corp. President Haruo ... A 0.73
... Wright Jr. Room ... A 0.67
... 6 p.m. Travelers who ... A + S 0.44
</figure>
<tableCaption confidence="0.943449">
Table 5: Sample errors with the probability of being in
the S class assigned by the SVM.
</tableCaption>
<bodyText confidence="0.967741333333333">
at http://code.google.com/p/splitta. Future work in-
cludes further experiments with structured classifi-
cation to treat the three classes appropriately.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.897357333333333">
Thanks to Benoit Favre, Dilek Hakkani-T¨ur, Kofi
Boakye, Marcel Paret, James Jacobus, and Larry
Gillick for helpful discussions.
</bodyText>
<sectionHeader confidence="0.990523" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999258566666667">
J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robin-
son, and M. Vilain. 1995. MITRE: description of the
Alembic system used for MUC-6. In Proceedings of
the 6th conference on Message understanding, pages
141–155. Association for Computational Linguistics
Morristown, NJ, USA.
T. Joachims. 1999. Making large-scale support vector
machine learning practical, Advances in kernel meth-
ods: support vector learning.
T. Kiss and J. Strunk. 2006. Unsupervised Multilingual
Sentence Boundary Detection. Computational Lin-
guistics, 32(4):485–525.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational Lin-
guistics, pages 62–69.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the penn treebank. Computational Linguistics,
19(2):313–330.
A. Mikheev. 2002. Periods, Capitalized Words, etc.
Computational Linguistics, 28(3):289–318.
D.D. Palmer and M.A. Hearst. 1997. Adaptive Multilin-
gual Sentence Boundary Disambiguation. Computa-
tional Linguistics, 23(2):241–267.
J.C. Reynar and A. Ratnaparkhi. 1997. A maximum en-
tropy approach to identifying sentence boundaries. In
Proceedings of the Fifth Conference on Applied Natu-
ral Language Processing, pages 16–19.
</reference>
<page confidence="0.998478">
244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.430220">
<title confidence="0.997637">Sentence Boundary Detection and the Problem with the U.S.</title>
<author confidence="0.994308">Dan</author>
<affiliation confidence="0.999971">Computer Science University of California,</affiliation>
<email confidence="0.999917">dgillick@cs.berkeley.edu</email>
<abstract confidence="0.99170875">Sentence Boundary Detection is widely used but often with outdated tools. We discuss what makes it difficult, which features are relevant, and present a fully statistical system, now publicly available, that gives the best known error rate on a standard news corpus: Of some 27,000 examples, our system makes 67 errors,</abstract>
<intro confidence="0.446083">23 involving the word “U.S.”</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>J Burger</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>MITRE: description of the Alembic system used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th conference on Message understanding,</booktitle>
<pages>141--155</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2398" citStr="Aberdeen et al., 1995" startWordPosition="375" endWordPosition="378"> is open-source and available to the public. 2 Previous Work We briefly outline the most important existing methods and cite error rates on a standard English data set, sections 03-06 of the Wall Street Journal (WSJ) corpus (Marcus et al., 1993), containing nearly 27,000 examples. Error rates are computed as (number incorrect/total ambiguous periods). Ambiguous periods are assumed to be those followed by white space or punctuation. Guessing the majority class gives a 26% baseline error rate. A variety of systems use lists of hand-crafted regular expressions and abbreviations, notably Alembic (Aberdeen et al., 1995), which gives a 0.9% error rate. Such systems are highly specialized to language and genre. The Satz system (Palmer and Hearst, 1997) achieves a 1.0% error rate using part-of-speech (POS) features as input to a neural net classifier (a decision tree gives similar results), trained on heldout WSJ data. Features were generated using a 5000-word lexicon and a list of 206 abbreviations. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. Using a maximum entropy classifier trained on nea</context>
</contexts>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. MITRE: description of the Alembic system used for MUC-6. In Proceedings of the 6th conference on Message understanding, pages 141–155. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical, Advances in kernel methods: support vector learning.</title>
<date>1999</date>
<contexts>
<context position="5969" citStr="Joachims, 1999" startWordPosition="980" endWordPosition="981">hat both untokenized text and mismatched train-test tokenization can increase the error rate by a factor of 2. Second, poor feature choices can hurt classification. In particular, adding a feature that matches a list of abbreviations can increase the error rate; using the list (“Mr.”, “Co.”) increases the number of errors by up to 25% in our experiments. This is because some abbreviations end sentences often, and others do not. In the test data, 0 of 1866 instances of “Mr.” end a sentence, compared to 24 of 86 instances of “Calif.” (see Table 2). While there may 1We use SVM Light, with c = 1 (Joachims, 1999). Nonlinear kernels did not improve performance in our experiments. # Feature Description Error 1 L = wi 1.88% 2 R=wj 9.36% 3 len(L) = l 9.12% 4 is cap(R) 12.56% 5 int(log(count(L; no period))) = ci 12.14% 6 int(log(count(R; is lower)) = cj 18.79% 7 (L=wi,R=wj) 10.01% 8 (L = wi, is cap(R)) 7.54% 1+2 0.77% 1+2+3+4 0.36% 1+2+3+4+5+6 0.32% 1+2+3+4+5+6+7+8 0.25% Table 1: All features are binary. SVM classification results shown; Naive Bayes gives 0.35% error rate with all features. be meaningful abbreviation subclasses, a feature indicating mere presence is too coarse. Abbr. Ends Sentence Total Ra</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale support vector machine learning practical, Advances in kernel methods: support vector learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kiss</author>
<author>J Strunk</author>
</authors>
<title>Unsupervised Multilingual Sentence Boundary Detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="3245" citStr="Kiss and Strunk, 2006" startWordPosition="511" endWordPosition="514">ssifier (a decision tree gives similar results), trained on heldout WSJ data. Features were generated using a 5000-word lexicon and a list of 206 abbreviations. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. Using a maximum entropy classifier trained on nearly 1 million words of additional WSJ data, they report a 1.2% error rate with an automatically generated abbreviation list and special corpusspecific abbreviation features. There are two notable unsupervised systems. Punkt (Kiss and Strunk, 2006) uses a set of loglikelihood-based heuristics to infer abbreviations and common sentence starters from a large text corpus. Deriving these lists from the WSJ test data gives an error rate of 1.65%. Punkt is easily adaptable but requires a large (unlabeled) indomain corpus for assembling statistics. An implementation is bundled with NLTK (Loper and Bird, 2002). (Mikheev, 2002) describes a “document241 Proceedings of NAACL HLT 2009: Short Papers, pages 241–244, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics centered” approach to SBD, using a set of heuristics to g</context>
<context position="11094" citStr="Kiss and Strunk, 2006" startWordPosition="1864" endWordPosition="1867">g the applicability of a model trained from a specific genre. Table 3 shows respectable error rates for two quite different test sets: The Brown corpus includes 500 documents, distributed across 15 genres roughly representative of all published English; The Complete Works of Edgar Allen Poe includes an introduction, prose, and poetry. A second issue is a lack of labeled data, especially in languages besides English. Table 4 shows that results can be quite good without extensive labeled resources, and they are likely to continue to improve if additional resources were available. At the least, (Kiss and Strunk, 2006) have labeled over Abbreviations (A) (A+S) Sentence Boundaries (S) Errors (A+S) All Data (A) S no P(A |“L.”) &gt; 0.5 yes P(S |“R”) &gt; 0.5 A no A+S yes 243 Corpus Examples in S SVM Err NB Err WSJ 26977 74% 0.25% 0.35% Brown 53688 91% 0.36% 0.45% Poe 11249 95% 0.52% 0.44% Table 3: SVM and Naive Bayes classification error rates on different corpora using a model trained from a disjoint WSJ data set. 10000 sentences in each of 11 languages, though we have not experimented with this data. Corpus 5 50 500 5000 42317 WSJ 7.26% 3.57% 1.36% 0.52% 0.25% Brown 5.65% 4.46% 1.65% 0.74% 0.36% Poe 4.01% 2.68% 2</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>T. Kiss and J. Strunk. 2006. Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4):485–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,</booktitle>
<pages>62--69</pages>
<contexts>
<context position="3606" citStr="Loper and Bird, 2002" startWordPosition="571" endWordPosition="574">rained on nearly 1 million words of additional WSJ data, they report a 1.2% error rate with an automatically generated abbreviation list and special corpusspecific abbreviation features. There are two notable unsupervised systems. Punkt (Kiss and Strunk, 2006) uses a set of loglikelihood-based heuristics to infer abbreviations and common sentence starters from a large text corpus. Deriving these lists from the WSJ test data gives an error rate of 1.65%. Punkt is easily adaptable but requires a large (unlabeled) indomain corpus for assembling statistics. An implementation is bundled with NLTK (Loper and Bird, 2002). (Mikheev, 2002) describes a “document241 Proceedings of NAACL HLT 2009: Short Papers, pages 241–244, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics centered” approach to SBD, using a set of heuristics to guess which words correspond to abbreviations and names. Adding carefully tuned lists from an extra news corpus gives an error rate of 0.45%, though this increases to 1.41% without the abbreviation list. Combining with a supervised POS-based system gives the best reported error rate on this task: 0.31%. Our system is closest in spirit to mxTerminator, and we u</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>E. Loper and S. Bird. 2002. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2021" citStr="Marcus et al., 1993" startWordPosition="317" endWordPosition="320">its from avoiding manually constructed or tuned lists. We provide a detailed analysis of features, training variations, and errors, all of which are under-explicated in the literature, and discuss the possibility of a more structured classification approach. Our implementation gives the best performance, to our knowledge, reported on a standard Wall Street Journal task; it is open-source and available to the public. 2 Previous Work We briefly outline the most important existing methods and cite error rates on a standard English data set, sections 03-06 of the Wall Street Journal (WSJ) corpus (Marcus et al., 1993), containing nearly 27,000 examples. Error rates are computed as (number incorrect/total ambiguous periods). Ambiguous periods are assumed to be those followed by white space or punctuation. Guessing the majority class gives a 26% baseline error rate. A variety of systems use lists of hand-crafted regular expressions and abbreviations, notably Alembic (Aberdeen et al., 1995), which gives a 0.9% error rate. Such systems are highly specialized to language and genre. The Satz system (Palmer and Hearst, 1997) achieves a 1.0% error rate using part-of-speech (POS) features as input to a neural net c</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>Periods, Capitalized Words,</title>
<date>2002</date>
<journal>etc. Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3623" citStr="Mikheev, 2002" startWordPosition="575" endWordPosition="576">on words of additional WSJ data, they report a 1.2% error rate with an automatically generated abbreviation list and special corpusspecific abbreviation features. There are two notable unsupervised systems. Punkt (Kiss and Strunk, 2006) uses a set of loglikelihood-based heuristics to infer abbreviations and common sentence starters from a large text corpus. Deriving these lists from the WSJ test data gives an error rate of 1.65%. Punkt is easily adaptable but requires a large (unlabeled) indomain corpus for assembling statistics. An implementation is bundled with NLTK (Loper and Bird, 2002). (Mikheev, 2002) describes a “document241 Proceedings of NAACL HLT 2009: Short Papers, pages 241–244, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics centered” approach to SBD, using a set of heuristics to guess which words correspond to abbreviations and names. Adding carefully tuned lists from an extra news corpus gives an error rate of 0.45%, though this increases to 1.41% without the abbreviation list. Combining with a supervised POS-based system gives the best reported error rate on this task: 0.31%. Our system is closest in spirit to mxTerminator, and we use the same train</context>
</contexts>
<marker>Mikheev, 2002</marker>
<rawString>A. Mikheev. 2002. Periods, Capitalized Words, etc. Computational Linguistics, 28(3):289–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
<author>M A Hearst</author>
</authors>
<title>Adaptive Multilingual Sentence Boundary Disambiguation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="2531" citStr="Palmer and Hearst, 1997" startWordPosition="399" endWordPosition="402">rates on a standard English data set, sections 03-06 of the Wall Street Journal (WSJ) corpus (Marcus et al., 1993), containing nearly 27,000 examples. Error rates are computed as (number incorrect/total ambiguous periods). Ambiguous periods are assumed to be those followed by white space or punctuation. Guessing the majority class gives a 26% baseline error rate. A variety of systems use lists of hand-crafted regular expressions and abbreviations, notably Alembic (Aberdeen et al., 1995), which gives a 0.9% error rate. Such systems are highly specialized to language and genre. The Satz system (Palmer and Hearst, 1997) achieves a 1.0% error rate using part-of-speech (POS) features as input to a neural net classifier (a decision tree gives similar results), trained on heldout WSJ data. Features were generated using a 5000-word lexicon and a list of 206 abbreviations. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. Using a maximum entropy classifier trained on nearly 1 million words of additional WSJ data, they report a 1.2% error rate with an automatically generated abbreviation list and speci</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>D.D. Palmer and M.A. Hearst. 1997. Adaptive Multilingual Sentence Boundary Disambiguation. Computational Linguistics, 23(2):241–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>16--19</pages>
<contexts>
<context position="2855" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="448" endWordPosition="451">ajority class gives a 26% baseline error rate. A variety of systems use lists of hand-crafted regular expressions and abbreviations, notably Alembic (Aberdeen et al., 1995), which gives a 0.9% error rate. Such systems are highly specialized to language and genre. The Satz system (Palmer and Hearst, 1997) achieves a 1.0% error rate using part-of-speech (POS) features as input to a neural net classifier (a decision tree gives similar results), trained on heldout WSJ data. Features were generated using a 5000-word lexicon and a list of 206 abbreviations. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. Using a maximum entropy classifier trained on nearly 1 million words of additional WSJ data, they report a 1.2% error rate with an automatically generated abbreviation list and special corpusspecific abbreviation features. There are two notable unsupervised systems. Punkt (Kiss and Strunk, 2006) uses a set of loglikelihood-based heuristics to infer abbreviations and common sentence starters from a large text corpus. Deriving these lists from the WSJ test data gives an error rate of 1.65%. Punkt is eas</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J.C. Reynar and A. Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 16–19.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>