<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.050113">
<title confidence="0.994989">
UMND1: Unsupervised Word Sense Disambiguation Using Contextual
Semantic Relatedness
</title>
<author confidence="0.990365">
Siddharth Patwardhan
</author>
<affiliation confidence="0.908549333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112.
</affiliation>
<email confidence="0.997381">
sidd@cs.utah.edu
</email>
<author confidence="0.943414">
Satanjeev Banerjee
</author>
<affiliation confidence="0.9352755">
Language Technologies Inst.
Carnegie Mellon University
</affiliation>
<address confidence="0.734898">
Pittsburgh, PA 15217.
</address>
<email confidence="0.997859">
banerjee@cs.cmu.edu
</email>
<author confidence="0.998142">
Ted Pedersen
</author>
<affiliation confidence="0.9990555">
Dept. of Computer Science
University of Minnesota
</affiliation>
<address confidence="0.559923">
Duluth, MN 55812.
</address>
<email confidence="0.998149">
tpederse@d.umn.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999542923076923">
In this paper we describe an unsuper-
vised WordNet-based Word Sense Disam-
biguation system, which participated (as
UMND1) in the SemEval-2007 Coarse-
grained English Lexical Sample task. The
system disambiguates a target word by using
WordNet-based measures of semantic relat-
edness to find the sense of the word that
is semantically most strongly related to the
senses of the words in the context of the tar-
get word. We briefly describe this system,
the configuration options used for the task,
and present some analysis of the results.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9976003125">
WordNet::SenseRelate::TargetWord1 (Patwardhan
et al., 2005; Patwardhan et al., 2003) is an unsuper-
vised Word Sense Disambiguation (WSD) system,
which is based on the hypothesis that the intended
sense of an ambiguous word is related to the
words in its context. For example, if the “financial
institution” sense of bank is intended in a context,
then it is highly likely the context would contain
related words such as money, transaction, interest
rate, etc. The algorithm, therefore, determines
the intended sense of a word (target word) in a
given context by measuring the relatedness of each
sense of that word with the words in its context.
The sense of the target word that is most related
to its context is selected as the intended sense of
the target word. The system uses WordNet-based
</bodyText>
<footnote confidence="0.885175">
1http://senserelate.sourceforge.net
</footnote>
<bodyText confidence="0.9988732">
measures of semantic relatedness2 (Pedersen et
al., 2004) to measure the relatedness between the
different senses of the target word and the words in
its context.
This system is completely unsupervised and re-
quires no annotated data for training. The lexical
database WordNet (Fellbaum, 1998) is the only re-
source that the system uses to measure the related-
ness between words and concepts. Thus, our system
is classified under the closed track of the task.
</bodyText>
<sectionHeader confidence="0.975398" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999918">
Our WSD system consists of a modular framework,
which allows different algorithms for the different
subtasks to be plugged into the system. We divide
the disambiguation task into two primary subtasks:
context selection and sense selection. The context
selection module tries to select words from the con-
text that are most likely to be indicative of the sense
of the target word. The sense selection module then
uses the set of selected context words to choose one
of the senses of the target word as the answer.
Figure 1 shows a block schematic of the system,
which takes SemEval-2007 English Lexical Sample
instances as input. Each instance is a made up of
a few English sentences, and one word from these
sentences is marked as the target word to be dis-
ambiguated. The system processes each instance
through multiple modules arranged in a sequential
pipeline. The final output of the pipeline is the sense
that is most appropriate for the target word in the
given context.
</bodyText>
<footnote confidence="0.969454">
2http://wn-similarity.sourceforge.net
</footnote>
<page confidence="0.967238">
390
</page>
<bodyText confidence="0.5631685">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 390–393,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<figure confidence="0.994225666666667">
Target Sense
Instance
Sense Selection
Format Filter
Preprocessing
Postprocessing
Relatedness
Measure
Context Selection
</figure>
<figureCaption confidence="0.999842">
Figure 1: System Architecture
</figureCaption>
<subsectionHeader confidence="0.979956">
2.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999985428571429">
The input text is first passed through a format fil-
ter, whose task is to parse the input XML file. This
is followed by a preprocessing step. Each instance
passed to the preprocessing stage is first segmented
into words, and then all compound words are iden-
tified. Any sequence of words known to be a com-
pound in WordNet is combined into a single entity.
</bodyText>
<subsectionHeader confidence="0.99924">
2.2 Context Selection
</subsectionHeader>
<bodyText confidence="0.999938588235294">
Although each input instance consists of a large
number of words, only a few of these are likely to
be useful for disambiguating the target word. We
use the context selection algorithm to select a subset
of the context words to be used for sense selection.
By removing the unimportant words, the computa-
tional complexity of the algorithm is reduced.
In this work, we use the NearestWords context
selection algorithm. This algorithm algorithm se-
lects 2n + 1 content words surrounding the target
word (including the target word) as the context. A
stop list is used to identify closed-class non-content
words. Additionally, any word not found in Word-
Net is also discarded. The algorithm then selects n
content words before and n content words follow-
ing the target word, and passes this unordered set of
2n + 1 words to the Sense Selection module.
</bodyText>
<subsectionHeader confidence="0.999587">
2.3 Sense Selection Algorithm
</subsectionHeader>
<bodyText confidence="0.999884565217391">
The sense selection module takes the set of words
output by the context selection module, one of which
is the target word to be disambiguated. For each of
the words in this set, it retrieves a list of senses from
WordNet, based on which it determines the intended
sense of the target word.
The package provides two main algorithms for
Sense Selection: the local and the global algorithms,
as described in previous work (Banerjee and Peder-
sen, 2002; Patwardhan et al., 2003). In this work,
we use the local algorithm, which is faster and was
shown to perform as well as the global algorithm.
The local sense selection algorithm measures the
semantic relatedness of each sense of the target word
with the senses of the words in the context, and se-
lects that sense of the target word which is most re-
lated to the context word-senses. Given the 2n + 1
context words, the system scores each sense of the
target word. Suppose the target word t has T senses,
enumerated as t1, t2, ..., tT. Also, suppose w1, w2,
..., w2n are the words in the context of t, each hav-
ing W1, W2, ... , W2n senses, respectively. Then for
each ti a score is computed as
</bodyText>
<equation confidence="0.869493">
max (relatedness(ti,wjk))
k=1 to Wj
</equation>
<bodyText confidence="0.999972310344828">
where wjk is the kth sense of word wj. The sense ti
of target word t with the highest score is selected as
the intended sense of the target word.
The relatedness between two word senses is com-
puted using a measure of semantic relatedness de-
fined in the WordNet::Similarity software package
(Pedersen et al., 2004), which is a suite of Perl mod-
ules implementing a number WordNet-based mea-
sures of semantic relatedness. For this work, we
used the Context Vector measure (Patwardhan and
Pedersen, 2006). The relatedness of concepts is
computed based on word co-occurrence statistics
derived from WordNet glosses. Given two WordNet
senses, this module returns a score between 0 and 1,
indicating the relatedness of the two senses.
Our system relies on WordNet as its sense inven-
tory. However, this task used OntoNotes (Hovy et
al., 2006) as the sense inventory. OntoNotes word
senses are groupings of similar WordNet senses.
Thus, we used the training data answer key to gen-
erate a mapping between the OntoNotes senses of
the given lexical elements and their corresponding
WordNet senses. We had to manually create the
mappings for some of the WordNet senses, which
had no corresponding OntoNotes senses. The sense
selection algorithm performed all of its computa-
tions with respect to the WordNet senses, and finally
the OntoNotes sense corresponding to the selected
WordNet sense of the target word was output as the
</bodyText>
<equation confidence="0.891822333333333">
2n
score(ti) _ E
j=1
</equation>
<page confidence="0.95956">
391
</page>
<bodyText confidence="0.952759">
answer for each instance.
</bodyText>
<sectionHeader confidence="0.99834" genericHeader="evaluation">
3 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999963214285714">
For this task, we used the freely available Word-
Net::SenseRelate::TargetWord v0.10 and the Word-
Net::Similarity v1.04 packages. WordNet v2.1 was
used as the underlying knowledge base for these.
The context selection module used a window size
of five (including the target word). The semantic re-
latedness of concepts was measured using the Con-
text Vector measure, with configuration options as
defined in previous research (Patwardhan and Ped-
ersen, 2006). Since we always predict exactly one
sense for each instance, the precision and recall val-
ues of all our experiments were always the same.
Therefore, in this section we will use the name “ac-
curacy” to mean both precision and recall.
</bodyText>
<subsectionHeader confidence="0.999408">
3.1 Overall Results, and Baselines
</subsectionHeader>
<bodyText confidence="0.999936558823529">
The overall accuracy of our system on the test data
is 0.538. This represents 2,609 correctly disam-
biguated instances, out of a total of 4,851 instances.
As baseline, we compare against the random al-
gorithm where for each instance, we randomly pick
one of the WordNet senses for the lexical element
in that instance, and report the OntoNotes senseid it
maps to as the answer. This algorithm gets an ac-
curacy of 0.417. Thus, our algorithm gets an im-
provement of 12% absolute (29% relative) over this
random baseline.
Additionally, we compare our algorithm against
the WordNet SenseOne algorithm. In this algorithm,
we pick the first sense among the WordNet senses
of the lexical element in each instance, and report
its corresponding OntoNotes sense as the answer for
that instance. This algorithm leverages the fact that
(in most cases) the WordNet senses for a particular
word are listed in the database in descending order
of their frequency of occurrence in the corpora from
which the sense inventory was created. If the new
test data has a similar distribution of senses, then this
algorithm amounts to a “majority baseline”. This
algorithm achieves an accuracy of 0.681 which is
15% absolute (27% relative) better than our algo-
rithm. Although this seemingly naive algorithm out-
performs our algorithm, we choose to avoid using
this information in our algorithms because it repre-
sents a large amount of human supervision in the
form of manual sense tagging of text, whereas our
goal is to create a purely unsupervised algorithm.
Additionally, our algorithms can, with little change,
work with other sense inventories besides WordNet
that may not have this information.
</bodyText>
<subsectionHeader confidence="0.975701">
3.2 Results Disaggregated by Part of Speech
</subsectionHeader>
<bodyText confidence="0.999996941176471">
In our past experience, we have found that av-
erage disambiguation accuracy differs significantly
between words of different parts of speech. For the
given test data, we separately evaluated the noun and
verb instances. We obtained an accuracy of 0.399
for the noun targets and 0.692 for the verb targets.
Thus, we find that our algorithm performs much bet-
ter on verbs than on nouns, when evaluated using the
OntoNotes sense inventory. This is different from
our experience with SENSEVAL data from previous
years where performance on nouns was uniformly
better than that on verbs. One possible reason for the
better performance on verbs is that the OntoNotes
sense inventory has, on average, fewer senses per
verb word (4.41) than per noun word (5.71). How-
ever, additional experimentation is needed to more
fully understand the difference in performance.
</bodyText>
<subsectionHeader confidence="0.980202">
3.3 Results Disaggregated by Lexical Element
</subsectionHeader>
<bodyText confidence="0.999997705882353">
To gauge the accuracy of our algorithm on different
words (lexical elements), we disaggregated the re-
sults by individual word. Table 1 lists the accuracy
values over instances of individual verb lexical ele-
ments, and Table 2 lists the accuracy values for noun
lexical elements. Our algorithm gets all instances
correct for 13 verb lexical elements, and for none of
the noun lexical elements. More generally, our al-
gorithm gets an accuracy of 50% or more on 45 out
of the 65 verb lexical elements, and on 15 out of the
35 noun lexical elements. For nouns, when the ac-
curacy results are viewed in sorted order (as in Table
2), one can observe a sudden degradation of results
between the accuracy of the word system.n – 0.443
– and the word source.n – 0.257. It is unclear why
there is such a jump; there is no such sudden degra-
dation in the results for the verb lexical elements.
</bodyText>
<sectionHeader confidence="0.998113" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9993015">
This paper describes our system UMND1, which
participated in the SemEval-2007 Coarse-grained
</bodyText>
<page confidence="0.993663">
392
</page>
<table confidence="0.999888323529412">
Word Accuracy Word Accuracy
remove 1.000 purchase 1.000
negotiate 1.000 improve 1.000
hope 1.000 express 1.000
exist 1.000 estimate 1.000
describe 1.000 cause 1.000
avoid 1.000 attempt 1.000
affect 1.000 say 0.969
explain 0.944 complete 0.938
disclose 0.929 remember 0.923
allow 0.914 announce 0.900
kill 0.875 occur 0.864
do 0.836 replace 0.800
maintain 0.800 complain 0.786
believe 0.764 receive 0.750
approve 0.750 buy 0.739
produce 0.727 regard 0.714
propose 0.714 need 0.714
care 0.714 feel 0.706
recall 0.667 examine 0.667
claim 0.667 report 0.657
find 0.607 grant 0.600
work 0.558 begin 0.521
build 0.500 keep 0.463
go 0.459 contribute 0.444
rush 0.429 start 0.421
raise 0.382 end 0.381
prove 0.364 enjoy 0.357
see 0.296 set 0.262
promise 0.250 hold 0.250
lead 0.231 prepare 0.222
join 0.222 ask 0.207
come 0.186 turn 0.048
fix 0.000
</table>
<tableCaption confidence="0.999893">
Table 1: Verb Lexical Element Accuracies
</tableCaption>
<bodyText confidence="0.999471444444444">
English Lexical Sample task. The system is based
on WordNet::SenseRelate::TargetWord, which is a
freely available unsupervised Word Sense Disam-
biguation software package. The system uses
WordNet-based measures of semantic relatedness to
select the intended sense of an ambiguous word. The
system required no training data and using WordNet
as its only knowledge source achieved an accuracy
of 54% on the blind test set.
</bodyText>
<sectionHeader confidence="0.997518" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982165">
This research was partially supported by a National
Science Foundation Early CAREER Development
award (#0092784).
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.943685666666667">
S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Al-
gorithm for Word Sense Disambiguation Using Word-
Net. In Proceedings of the Third International Con-
</reference>
<table confidence="0.992408736842105">
Word Accuracy Word Accuracy
policy 0.949 people 0.904
future 0.870 drug 0.870
space 0.857 capital 0.789
effect 0.767 condition 0.765
job 0.692 bill 0.686
area 0.676 base 0.650
management 0.600 power 0.553
development 0.517 chance 0.467
exchange 0.459 order 0.456
part 0.451 president 0.446
system 0.443 source 0.257
network 0.218 state 0.208
share 0.192 rate 0.186
hour 0.167 plant 0.109
move 0.085 point 0.080
value 0.068 defense 0.048
position 0.044 carrier 0.000
authority 0.000
</table>
<tableCaption confidence="0.982967">
Table 2: Noun Lexical Element Accuracies
</tableCaption>
<reference confidence="0.999154647058824">
ference on Intelligent Text Processing and Computa-
tional Linguistics, pages 136–145, Mexico City, Mex-
ico, February.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the ACL, pages 57–60, New York, NY, June.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1–8, Trento, Italy, April.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing Measures of Semantic Relatedness for Word Sense
Disambiguation. In Proceedings of the Fourth In-
ternational Conference on Intelligent Text Processing
and Computational Linguistics, pages 241–257, Mex-
ico City, Mexico, February.
S. Patwardhan, T. Pedersen, and S. Banerjee. 2005.
SenseRelate::TargetWord - A Generalized Framework
for Word Sense Disambiguation. In Proceedings of
the Twentieth National Conference on Artificial In-
telligence (Intelligent Systems Demonstrations), pages
1692–1693, Pittsburgh, PA, July.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet::Similarity - Measuring the Relatedness of
Concepts. In Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics Demonstrations, pages
38–41, Boston, MA, May.
</reference>
<page confidence="0.999372">
393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.841496">
<title confidence="0.995527">UMND1: Unsupervised Word Sense Disambiguation Using Contextual Semantic Relatedness</title>
<author confidence="0.990203">Siddharth Patwardhan</author>
<affiliation confidence="0.9998015">School of Computing University of Utah</affiliation>
<address confidence="0.937622">Salt Lake City, UT 84112.</address>
<email confidence="0.999779">sidd@cs.utah.edu</email>
<author confidence="0.971294">Satanjeev Banerjee</author>
<affiliation confidence="0.994999">Language Technologies Inst. Carnegie Mellon University</affiliation>
<address confidence="0.998185">Pittsburgh, PA 15217.</address>
<email confidence="0.998774">banerjee@cs.cmu.edu</email>
<author confidence="0.999873">Ted Pedersen</author>
<affiliation confidence="0.9999785">Dept. of Computer Science University of Minnesota</affiliation>
<address confidence="0.964882">Duluth, MN 55812.</address>
<email confidence="0.999726">tpederse@d.umn.edu</email>
<abstract confidence="0.999094785714286">In this paper we describe an unsupervised WordNet-based Word Sense Disambiguation system, which participated (as UMND1) in the SemEval-2007 Coarsegrained English Lexical Sample task. The system disambiguates a target word by using WordNet-based measures of semantic relatedness to find the sense of the word that is semantically most strongly related to the senses of the words in the context of the target word. We briefly describe this system, the configuration options used for the task, and present some analysis of the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>136--145</pages>
<location>Mexico City, Mexico,</location>
<contexts>
<context position="5321" citStr="Banerjee and Pedersen, 2002" startWordPosition="845" endWordPosition="849">ts n content words before and n content words following the target word, and passes this unordered set of 2n + 1 words to the Sense Selection module. 2.3 Sense Selection Algorithm The sense selection module takes the set of words output by the context selection module, one of which is the target word to be disambiguated. For each of the words in this set, it retrieves a list of senses from WordNet, based on which it determines the intended sense of the target word. The package provides two main algorithms for Sense Selection: the local and the global algorithms, as described in previous work (Banerjee and Pedersen, 2002; Patwardhan et al., 2003). In this work, we use the local algorithm, which is faster and was shown to perform as well as the global algorithm. The local sense selection algorithm measures the semantic relatedness of each sense of the target word with the senses of the words in the context, and selects that sense of the target word which is most related to the context word-senses. Given the 2n + 1 context words, the system scores each sense of the target word. Suppose the target word t has T senses, enumerated as t1, t2, ..., tT. Also, suppose w1, w2, ..., w2n are the words in the context of t</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>S. Banerjee and T. Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics, pages 136–145, Mexico City, Mexico, February.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: The 90% Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>57--60</pages>
<location>New York, NY,</location>
<contexts>
<context position="6884" citStr="Hovy et al., 2006" startWordPosition="1123" endWordPosition="1126">f semantic relatedness defined in the WordNet::Similarity software package (Pedersen et al., 2004), which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness. For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). The relatedness of concepts is computed based on word co-occurrence statistics derived from WordNet glosses. Given two WordNet senses, this module returns a score between 0 and 1, indicating the relatedness of the two senses. Our system relies on WordNet as its sense inventory. However, this task used OntoNotes (Hovy et al., 2006) as the sense inventory. OntoNotes word senses are groupings of similar WordNet senses. Thus, we used the training data answer key to generate a mapping between the OntoNotes senses of the given lexical elements and their corresponding WordNet senses. We had to manually create the mappings for some of the WordNet senses, which had no corresponding OntoNotes senses. The sense selection algorithm performed all of its computations with respect to the WordNet senses, and finally the OntoNotes sense corresponding to the selected WordNet sense of the target word was output as the 2n score(ti) _ E j=</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2006. OntoNotes: The 90% Solution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57–60, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>T Pedersen</author>
</authors>
<title>Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="6550" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1069" endWordPosition="1072">t, each having W1, W2, ... , W2n senses, respectively. Then for each ti a score is computed as max (relatedness(ti,wjk)) k=1 to Wj where wjk is the kth sense of word wj. The sense ti of target word t with the highest score is selected as the intended sense of the target word. The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet::Similarity software package (Pedersen et al., 2004), which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness. For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). The relatedness of concepts is computed based on word co-occurrence statistics derived from WordNet glosses. Given two WordNet senses, this module returns a score between 0 and 1, indicating the relatedness of the two senses. Our system relies on WordNet as its sense inventory. However, this task used OntoNotes (Hovy et al., 2006) as the sense inventory. OntoNotes word senses are groupings of similar WordNet senses. Thus, we used the training data answer key to generate a mapping between the OntoNotes senses of the given lexical elements and their corresponding WordNet senses. We had to manu</context>
<context position="7991" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1299" endWordPosition="1303">ly the OntoNotes sense corresponding to the selected WordNet sense of the target word was output as the 2n score(ti) _ E j=1 391 answer for each instance. 3 Results and Analysis For this task, we used the freely available WordNet::SenseRelate::TargetWord v0.10 and the WordNet::Similarity v1.04 packages. WordNet v2.1 was used as the underlying knowledge base for these. The context selection module used a window size of five (including the target word). The semantic relatedness of concepts was measured using the Context Vector measure, with configuration options as defined in previous research (Patwardhan and Pedersen, 2006). Since we always predict exactly one sense for each instance, the precision and recall values of all our experiments were always the same. Therefore, in this section we will use the name “accuracy” to mean both precision and recall. 3.1 Overall Results, and Baselines The overall accuracy of our system on the test data is 0.538. This represents 2,609 correctly disambiguated instances, out of a total of 4,851 instances. As baseline, we compare against the random algorithm where for each instance, we randomly pick one of the WordNet senses for the lexical element in that instance, and report the</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>S. Patwardhan and T. Pedersen. 2006. Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Using Measures of Semantic Relatedness for Word Sense Disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<location>Mexico City, Mexico,</location>
<contexts>
<context position="1044" citStr="Patwardhan et al., 2003" startWordPosition="143" endWordPosition="146">paper we describe an unsupervised WordNet-based Word Sense Disambiguation system, which participated (as UMND1) in the SemEval-2007 Coarsegrained English Lexical Sample task. The system disambiguates a target word by using WordNet-based measures of semantic relatedness to find the sense of the word that is semantically most strongly related to the senses of the words in the context of the target word. We briefly describe this system, the configuration options used for the task, and present some analysis of the results. 1 Introduction WordNet::SenseRelate::TargetWord1 (Patwardhan et al., 2005; Patwardhan et al., 2003) is an unsupervised Word Sense Disambiguation (WSD) system, which is based on the hypothesis that the intended sense of an ambiguous word is related to the words in its context. For example, if the “financial institution” sense of bank is intended in a context, then it is highly likely the context would contain related words such as money, transaction, interest rate, etc. The algorithm, therefore, determines the intended sense of a word (target word) in a given context by measuring the relatedness of each sense of that word with the words in its context. The sense of the target word that is mo</context>
<context position="5347" citStr="Patwardhan et al., 2003" startWordPosition="850" endWordPosition="853"> n content words following the target word, and passes this unordered set of 2n + 1 words to the Sense Selection module. 2.3 Sense Selection Algorithm The sense selection module takes the set of words output by the context selection module, one of which is the target word to be disambiguated. For each of the words in this set, it retrieves a list of senses from WordNet, based on which it determines the intended sense of the target word. The package provides two main algorithms for Sense Selection: the local and the global algorithms, as described in previous work (Banerjee and Pedersen, 2002; Patwardhan et al., 2003). In this work, we use the local algorithm, which is faster and was shown to perform as well as the global algorithm. The local sense selection algorithm measures the semantic relatedness of each sense of the target word with the senses of the words in the context, and selects that sense of the target word which is most related to the context word-senses. Given the 2n + 1 context words, the system scores each sense of the target word. Suppose the target word t has T senses, enumerated as t1, t2, ..., tT. Also, suppose w1, w2, ..., w2n are the words in the context of t, each having W1, W2, ... </context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 241–257, Mexico City, Mexico, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>T Pedersen</author>
<author>S Banerjee</author>
</authors>
<title>SenseRelate::TargetWord - A Generalized Framework for Word Sense Disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Artificial Intelligence (Intelligent Systems Demonstrations),</booktitle>
<pages>1692--1693</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="1018" citStr="Patwardhan et al., 2005" startWordPosition="139" endWordPosition="142">umn.edu Abstract In this paper we describe an unsupervised WordNet-based Word Sense Disambiguation system, which participated (as UMND1) in the SemEval-2007 Coarsegrained English Lexical Sample task. The system disambiguates a target word by using WordNet-based measures of semantic relatedness to find the sense of the word that is semantically most strongly related to the senses of the words in the context of the target word. We briefly describe this system, the configuration options used for the task, and present some analysis of the results. 1 Introduction WordNet::SenseRelate::TargetWord1 (Patwardhan et al., 2005; Patwardhan et al., 2003) is an unsupervised Word Sense Disambiguation (WSD) system, which is based on the hypothesis that the intended sense of an ambiguous word is related to the words in its context. For example, if the “financial institution” sense of bank is intended in a context, then it is highly likely the context would contain related words such as money, transaction, interest rate, etc. The algorithm, therefore, determines the intended sense of a word (target word) in a given context by measuring the relatedness of each sense of that word with the words in its context. The sense of </context>
</contexts>
<marker>Patwardhan, Pedersen, Banerjee, 2005</marker>
<rawString>S. Patwardhan, T. Pedersen, and S. Banerjee. 2005. SenseRelate::TargetWord - A Generalized Framework for Word Sense Disambiguation. In Proceedings of the Twentieth National Conference on Artificial Intelligence (Intelligent Systems Demonstrations), pages 1692–1693, Pittsburgh, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations,</booktitle>
<pages>38--41</pages>
<location>Boston, MA,</location>
<contexts>
<context position="1847" citStr="Pedersen et al., 2004" startWordPosition="274" endWordPosition="277"> example, if the “financial institution” sense of bank is intended in a context, then it is highly likely the context would contain related words such as money, transaction, interest rate, etc. The algorithm, therefore, determines the intended sense of a word (target word) in a given context by measuring the relatedness of each sense of that word with the words in its context. The sense of the target word that is most related to its context is selected as the intended sense of the target word. The system uses WordNet-based 1http://senserelate.sourceforge.net measures of semantic relatedness2 (Pedersen et al., 2004) to measure the relatedness between the different senses of the target word and the words in its context. This system is completely unsupervised and requires no annotated data for training. The lexical database WordNet (Fellbaum, 1998) is the only resource that the system uses to measure the relatedness between words and concepts. Thus, our system is classified under the closed track of the task. 2 System Description Our WSD system consists of a modular framework, which allows different algorithms for the different subtasks to be plugged into the system. We divide the disambiguation task into </context>
<context position="6364" citStr="Pedersen et al., 2004" startWordPosition="1039" endWordPosition="1042">ystem scores each sense of the target word. Suppose the target word t has T senses, enumerated as t1, t2, ..., tT. Also, suppose w1, w2, ..., w2n are the words in the context of t, each having W1, W2, ... , W2n senses, respectively. Then for each ti a score is computed as max (relatedness(ti,wjk)) k=1 to Wj where wjk is the kth sense of word wj. The sense ti of target word t with the highest score is selected as the intended sense of the target word. The relatedness between two word senses is computed using a measure of semantic relatedness defined in the WordNet::Similarity software package (Pedersen et al., 2004), which is a suite of Perl modules implementing a number WordNet-based measures of semantic relatedness. For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). The relatedness of concepts is computed based on word co-occurrence statistics derived from WordNet glosses. Given two WordNet senses, this module returns a score between 0 and 1, indicating the relatedness of the two senses. Our system relies on WordNet as its sense inventory. However, this task used OntoNotes (Hovy et al., 2006) as the sense inventory. OntoNotes word senses are groupings of similar WordNet </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics Demonstrations, pages 38–41, Boston, MA, May.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>