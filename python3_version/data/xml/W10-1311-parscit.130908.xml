<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001593">
<title confidence="0.980482">
Towards a noisy-channel model of dysarthria in speech recognition
</title>
<author confidence="0.997606">
Frank Rudzicz
</author>
<affiliation confidence="0.903937">
University of Toronto, Department of Computer Science
Toronto, Ontario, Canada
</affiliation>
<email confidence="0.998663">
frank@cs.toronto.edu
</email>
<sectionHeader confidence="0.996657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999859368421053">
Modern automatic speech recognition is inef-
fective at understanding relatively unintelligi-
ble speech caused by neuro-motor disabilities
collectively called dysarthria. Since dysarthria
is primarily an articulatory phenomenon, we
are collecting a database of vocal tract mea-
surements during speech of individuals with
cerebral palsy. In this paper, we demonstrate
that articulatory knowledge can remove am-
biguities in the acoustics of dysarthric speak-
ers by reducing entropy relatively by 18.3%,
on average. Furthermore, we demonstrate
that dysarthric speech is more precisely por-
trayed as a noisy-channel distortion of an
abstract representation of articulatory goals,
rather than as a distortion of non-dysarthric
speech. We discuss what implications these
results have for our ongoing development of
speech systems for dysarthric speakers.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999510538461538">
Dysarthria is a set of congenital and traumatic
neuro-motor disorders that impair the physical pro-
duction of speech and affects approximately 0.8% of
individuals in North America (Hosom et al., 2003).
Causes of dysarthria include cerebral palsy (CP),
multiple sclerosis, Parkinson’s disease, and amy-
otrophic lateral sclerosis (ALS). These impairments
reduce or remove normal control of the primary vo-
cal articulators but do not affect the abstract produc-
tion of meaningful, syntactically correct language.
The neurological origins of dysarthria involve
damage to the cranial nerves that control the speech
articulators (Moore and Dalley, 2005). Spastic
</bodyText>
<page confidence="0.954436">
80
</page>
<bodyText confidence="0.999972764705882">
dysarthria, for instance, is partially caused by le-
sions in the facial and hypoglossal nerves, which
control the jaw and tongue respectively (Duffy,
2005), resulting in slurred speech and a less differ-
entiable vowel space (Kent and Rosen, 2004). Sim-
ilarly, damage to the glossopharyngeal nerve can re-
duce control over vocal fold vibration (i.e., phona-
tion), resulting in guttural or grating raspiness. In-
adequate control of the soft palate caused by disrup-
tion of the vagus nerve may lead to a disproportion-
ate amount of air released through the nose during
speech (i.e., hypernasality).
Unfortunately, traditional automatic speech
recognition (ASR) is incompatible with dysarthric
speech, often rendering such software inaccessible
to those whose neuro-motor disabilities might make
other forms of interaction (e.g., keyboards, touch
screens) laborious. Traditional representations in
ASR such as hidden Markov models (HMMs)
trained for speaker independence that achieve
84.8% word-level accuracy for non-dysarthric
speakers might achieve less than 4.5% accuracy
given severely dysarthric speech on short sentences
(Rudzicz, 2007). Our research group is currently
developing new ASR models that incorporate em-
pirical knowledge of dysarthric articulation for use
in assistive applications (Rudzicz, 2009). Although
these models have increased accuracy, the disparity
is still high. Our aim is to understand why ASR
fails for dysarthric speakers by understanding the
acoustic and articulatory nature of their speech.
In this paper, we cast the speech-motor interface
within the mathematical framework of the noisy-
channel model. This is motivated by the charac-
</bodyText>
<note confidence="0.984503">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80–88,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999125">
terization of dysarthria as a distortion of parallel
biological pathways that corrupt motor signals be-
fore execution (Kent and Rosen, 2004; Freund et
al., 2005), as in the examples cited above. Within
this information-theoretic framework, we aim to in-
fer the nature of the motor signal distortions given
appropriate measurements of the vocal tract. That is,
we ask the following question: Is dysarthric speech
a distortion of typical speech, or are they both distor-
tions of some common underlying representation?
</bodyText>
<sectionHeader confidence="0.969284" genericHeader="method">
2 Dysarthric articulation data
</sectionHeader>
<bodyText confidence="0.999927846153846">
Since the underlying articulatory dynamics of
dysarthric speech are intrinsically responsible for
complex acoustic irregularities, we are collecting
a database of dysarthric articulation. Time-aligned
movement and acoustic data are measured using
two systems. The first infers 3D positions of sur-
face facial markers given stereo video images. The
second uses electromagnetic articulography (EMA),
in which the speaker is placed within a cube that
produces a low-amplitude electromagnetic field, as
shown in figure 1. Tiny sensors within this field al-
low the inference of articulator positions and veloci-
ties to within 1 mm of error (Yunusova et al., 2009).
</bodyText>
<figureCaption confidence="0.998266">
Figure 1: Electromagnetic articulograph system.
</figureCaption>
<bodyText confidence="0.999946347826087">
We have so far recorded one male speaker with
ALS, five male speakers with CP, four female
speakers with CP, and age- and gender-matched
controls. Measurement coils are placed as in
other studies (e.g., the University of Edinburgh’s
MOCHA database (Wrench, 1999) and the Uni-
versity of Wisconsin-Madison’s x-ray microbeam
database (Yunusova et al., 2008)). Specifically, we
are interested in the positions of the upper and lower
lip (UL and LL), left and right mouth corners (LM
and RM), lower incisor (LI), and tongue tip, blade,
and dorsum (TT, TB, and TD). Unfortunately, a few
of our male CP subjects had a severe gag reflex, and
we found it impossible to place more than one coil
on the tongue for these few individuals. Therefore,
of the tongue positions, only TT is used in this study.
All articulatory data are smoothed with third-order
median filtering in order to minimize measurement
‘jitter’. Figure 2 shows the degree of lip aperture
(i.e., the distance between UL and LL) over time for
a control and a dysarthric speaker repeating the se-
quence /ah p iy/. Here, the dysarthric speech is no-
tably slower and has more excessive movement.
</bodyText>
<figure confidence="0.9935615">
5
Control
Dysarthric
4
3
2
10 1 2 3 4 5 6
Time (s)
</figure>
<figureCaption confidence="0.8672445">
Figure 2: Lip aperture over time for four iterations of /ah
p iy/ given a dysarthric and control speaker.
</figureCaption>
<bodyText confidence="0.990510176470588">
Our dysarthric speech data include random repeti-
tions of phonetically balanced short sentences origi-
nally used in the TIMIT database (Zue et al., 1989),
as well as pairs of monosyllabic words identified
by Kent et al. (1989) as having relevant articula-
tory contrasts (e.g., beat versus meat as a stop-
nasal contrast). All articulatory data are aligned
with associated acoustic data, which are transformed
to Mel-frequency cepstral coefficients (MFCCs).
Phoneme boundaries and pronunciation errors are
being transcribed by a speech-language pathologist
to the TIMIT phoneset. Table 1 shows pronuncia-
tion errors according to manner of articulation for
dysarthric speech. Plosives are mispronounced most
often, with substitution errors exclusively caused by
errant voicing (e.g. /d/ for /t/). By comparison, only
Lip aperture (cm)
</bodyText>
<page confidence="0.994897">
81
</page>
<listItem confidence="0.7008788">
5% of corresponding plosives in total are mispro-
nounced in regular speech. Furthermore, the preva-
lence of deleted affricates in word-final positions, al-
most all of which are alveolar, does not occur in the
corresponding control data.
</listItem>
<table confidence="0.99765975">
SUB (%) f DEL (%)
i m i m f
plosives 13.8 18.7 7.1 1.9 1.0 12.1
affricates 0.0 8.3 0.0 0.0 0.0 23.2
fricatives 8.5 3.1 5.3 22.0 5.5 13.2
nasals 0.0 0.0 1.5 0.0 0.0 1.5
glides 0.0 0.7 0.4 11.4 2.5 0.9
vowels 0.9 0.9 0.0 0.0 0.2 0.0
</table>
<tableCaption confidence="0.99502225">
Table 1: Percentage of phoneme substitution (SUB) and
deletion (DEL) errors in word-initial (i), word-medial
(m), and word-final (f) positions across categories of
manner for dysarthric data.
</tableCaption>
<bodyText confidence="0.997639142857143">
Table 2 shows the relative durations of the five
most common vowels and sonorant consonants in
our database between dysarthric and control speech.
Here, dysarthric speakers are significantly slower
than their control counterparts at the 95% confidence
interval for/eh/and at the 99.5% confidence interval
for all other phonemes.
</bodyText>
<table confidence="0.998204166666667">
Phoneme duration (µ (σ2), in ms) Avg.
Dysarthric Control diff.
/ah/ 189.3 (19.2) 120.1 (4.0) 69.2
/ae/ 211.6 (16.4) 140.0 (4.4) 71.6
/eh/ 160.5 (7.4) 107.3 (2.6) 53.2
/iy/ 177.1 (86.7) 105.8 (93.1) 71.3
/er/ 220.5 (27.9) 148.6 (59.8) 71.9
/l/ 138.5 (8.0) 91.8 (2.4) 46.7
/m/ 173.5 (13.4) 94.7 (2.1) 78.8
/n/ 168.4 (14.4) 90.9 (2.3) 77.5
/r/ 138.8 (8.3) 95.3 (3.4) 43.5
/w/ 151.5 (12.0) 84.5 (1.3) 67.0
</table>
<tableCaption confidence="0.998223">
Table 2: Average lengths (and variances in parentheses)
</tableCaption>
<bodyText confidence="0.738087285714286">
in milliseconds for the five most common vowels and
sonorant consonants for dysarthric and control speakers.
The last column is the average difference in milliseconds
between dysarthric and control subjects.
Processing and annotation of further data from
additional dysarthric speakers is ongoing, including
measurements of all three tongue positions.
</bodyText>
<sectionHeader confidence="0.86209" genericHeader="method">
3 Entropy and the noisy-channel model
</sectionHeader>
<bodyText confidence="0.9996069">
We wish to measure the degree of statistical disorder
in both acoustic and articulatory data for dysarthric
and non-dysarthric speakers, as well as the a posteri-
ori disorder of one type of data given the other. This
quantification will inform us as to the relative mer-
its of incorporating knowledge of articulatory be-
haviour into ASR systems for dysarthric speakers.
Entropy, H(X), is a measure of the degree of uncer-
tainty in a random variable X. When X is discrete,
this value is computed with the familiar
</bodyText>
<equation confidence="0.529251">
p(xi)logb p(xi),
</equation>
<bodyText confidence="0.9998498">
where b is the logarithm base, xi is a value of X,
of which there are n possible, and p(xi) is its prob-
ability. When our observations are continuous, as
they are in our acoustic and articulatory database,
we must use differential entropy defined by
</bodyText>
<equation confidence="0.993725">
ZH(X) = − X f(X)log f(X)dX,
</equation>
<bodyText confidence="0.9999455">
where f (X) is the probability density function of X.
For a number of distributions f (X), the differential
entropy has known forms (Lazo and Rathie, 1978).
For example, if f (X) is a multivariate normal,
</bodyText>
<equation confidence="0.9455455">
exp(−12(x −µ)TΣ−1(x −µ))
(1)
2 ln�(2πe)N |Σ|�,
H(X) = 1
</equation>
<bodyText confidence="0.999932375">
where µ and Σ are the mean and covariances of the
data. However, since we observe that both acous-
tic and articulatory data follow non-Gaussian dis-
tributions, we choose to represent these spaces by
mixtures of Gaussians. Huber et al. (2008) have de-
veloped an accurate algorithm for estimating differ-
ential entropy of Gaussian mixtures based on itera-
tively merging Gaussians and the approximation
</bodyText>
<equation confidence="0.9554765">
2 log((2πe)N |Σi|�,
ωi �−logωi + 1
</equation>
<bodyText confidence="0.9999486">
where ωi is the weight of the ith(1 &lt; i &lt; L) Gaussian
and Σi is that Gaussian’s covariance matrix. This
method is used to approximate entropies in the fol-
lowing study, with L = 32. Note that while differen-
tial entropies can be negative and not invariant under
</bodyText>
<equation confidence="0.99156025">
n
∑
i=1
H(X) = −
fX(x1,...,xN) = (2π)N/2|Σ|1/2
L
˜H(X) = ∑
i=1
</equation>
<page confidence="0.942154">
82
</page>
<bodyText confidence="0.998534666666667">
change of variables, other properties of entropy are
retained (Huber et al., 2008), such as the chain rule
for conditional entropy
</bodyText>
<equation confidence="0.973888">
H(Y |X) = H(Y,X) −H(X),
</equation>
<bodyText confidence="0.9995555">
which describes the uncertainty in Y given knowl-
edge of X, and the chain rule for mutual information
</bodyText>
<equation confidence="0.974457">
I(Y;X) = H(X) +H(Y) −H(X,Y),
</equation>
<bodyText confidence="0.999293333333333">
which describes the mutual dependence between X
and Y. Here, we quantize entropy with the nat,
which is the natural logarithmic unit, e (Pz� 1.44 bits).
</bodyText>
<subsectionHeader confidence="0.99927">
3.1 The noisy channel
</subsectionHeader>
<bodyText confidence="0.999996366666667">
The noisy-channel theorem states that information
passed through a channel with capacity C at a rate
R &lt; C can be reliably recovered with an arbitrarily
low probability of error given an appropriate coding.
Here, a message from a finite alphabet is encoded,
producing signal x E X. That signal is then distorted
by a medium which transmits signal y E Y accord-
ing to some distribution P(Y |X). Given that there is
some probability that the received signal, y, is cor-
rupted, the message produced by the decoder may
differ from the original (Shannon, 1949).
To what extent can we describe the effects of
dysarthria within an information-theoretic noisy
channel model? We pursue two competing hypothe-
ses within this general framework. The first hypoth-
esis models the assumption that dysarthric speech is
a distorted version of typical speech. Here, signal
X and Y represent the vocal characteristics of the
general and dysarthric populations, respectively, and
P(Y |X) models the distortion between them. The
second hypothesis models the assumption that both
dysarthric and typical speech are distorted versions
of some common abstraction. Here, Yd and Yc repre-
sent the vocal characteristics of dysarthric and con-
trol speakers, respectively, and X represents a com-
mon, underlying mechanism and that P(Yd |X) and
P(Yc |X) model distortions from that mechanism.
These two hypotheses are visualized in figure 3. In
each of these cases, signals can be acoustic, articu-
latory, or some combination thereof.
</bodyText>
<subsectionHeader confidence="0.999404">
3.2 Common underlying abstractions
</subsectionHeader>
<bodyText confidence="0.9997445">
In order to test our hypothesis that both dysarthric
and control speakers share a common high-level ab-
</bodyText>
<figure confidence="0.985266555555556">
Dysarthric speech
signal, Y
(a) Dysarthric speech as a distortion of control speech
Dysarthric speech
signal, Yd
Control speech
signal, Yc
(b) Dysarthric and control speech as distortions of a common
abstraction
</figure>
<figureCaption confidence="0.998788">
Figure 3: Sections of noisy channel models that mimic
the neuro-motor interface.
</figureCaption>
<bodyText confidence="0.9997764">
straction of the vocal tract that is in both cases dis-
torted during articulation, we incorporate the the-
ory of task dynamics (Saltzman and Munhall, 1989).
This theory represents the interface between the lex-
ical intentions and vocal tract realizations of speech
as a sequence of overlapping gestures, which are
continuous dynamical systems that describe goal-
oriented reconfigurations of the vocal tract, such as
bilabial closure during /m/. Figure 4 shows an ex-
ample of overlapping gestures for the word pub.
</bodyText>
<figureCaption confidence="0.932423833333333">
Figure 4: Canonical example pub from Saltzman
and Munhall (1989) representing overlapping goals for
tongue blade constriction degree (TBCD), lip aperture
(LA), and glottis (GLO). Boxes represent the present of
discretized goals, such as lip closure. Black curves repre-
sent the output of the TADA system.
</figureCaption>
<bodyText confidence="0.99990025">
The open-source TADA system (Nam and Gold-
stein, 2006) estimates the positions of various artic-
ulators during speech according to parameters that
have been carefully tuned by the authors of TADA
according to a generic, speaker-independent repre-
sentation of the vocal tract (Saltzman and Munhall,
1989). Given a word sequence and a syllable-to-
gesture dictionary, TADA produces the continuous
</bodyText>
<figure confidence="0.9332235">
TBCD
open
LA
closed
open
closed
open
GLO
closed
100 200 300 400
Time (ms)
Typical speech P(Y  |X)
signal, X
Abstract speech P(Yd  |X)
signal, X
P(Yc  |X)
</figure>
<page confidence="0.993048">
83
</page>
<bodyText confidence="0.999881722222222">
tract variable paths that are necessary to produce that
sequence. This takes into account various physio-
logical aspects of human speech production, such as
interarticulator co-ordination and timing (Nam and
Saltzman, 2003).
In this study, we use TADA to produce estimates
of a global, high-level representation of speech com-
mon to both dysarthric and non-dysarthric speakers
alike. Given a word sequence uttered by both types
of speaker, we produce five continuous curves pre-
scribed by that word sequence in order to match our
available EMA data. Those curves are lip aperture
and protrusion (LA and LP), tongue tip constriction
location and degree (TTCL and TTCD, representing
front-back and top-down positions of the tongue tip,
respectively), and lower incisor height (LIH). These
curves are then compared against actually observed
EMA data, as described below.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999937928571429">
First, in section 4.1, we ask whether the incorpo-
ration of articulatory data is theoretically useful in
reducing uncertainty in dysarthric speech. Second,
in section 4.2, we ask which of the two noisy chan-
nel models in figure 3 best describe the observed be-
haviour of dysarthric speech.
Data for this study are collected as described as in
section 2. Here, we use data from three dysarthric
speakers with cerebral palsy (males M01 and M04,
and female F03), as well as their age- and gender-
matched counterparts from the general population
(males MC01 and MC03, and female FC02). For
this study we restrict our analysis to 100 phrases ut-
tered in common by all six speakers.
</bodyText>
<subsectionHeader confidence="0.946939">
4.1 Entropy
</subsectionHeader>
<bodyText confidence="0.999825857142857">
We measure the differential entropy of acoustics
(H(Ac)), of articulation (H(Ar)), and of acoustics
given knowledge of the vocal tract (H(AclAr)) in
order to obtain theoretical estimates as to the utility
of articulatory data. Table 3 shows these quantities
across the six speakers in this study. As expected,
the acoustics of dysarthric speakers are much more
disordered than for non-dysarthric speakers. One
unexpected finding is that there is very little differ-
ence between speakers in terms of their entropy of
articulation. Although dysarthric speakers clearly
lack articulatory dexterity, this implies that they
nonetheless articulate with a level of consistency
similar to their non-dysarthric counterparts1. How-
ever, the equivocation H(Ac|Ar) is an order of mag-
nitude lower for non-dysarthric speakers. This im-
plies that there is very little ambiguity left in the
acoustics of non-dysarthric speakers if we have si-
multaneous knowledge of the vocal tract, but that
quite a bit of ambiguity remains for our dysarthric
speakers, despite significant reductions.
</bodyText>
<table confidence="0.999597">
Speaker H(Ac) H(Ar) H(AclAr)
Dys. M01 66.37 17.16 50.30
M04 33.36 11.31 26.25
F03 42.28 19.33 39.47
Average 47.34 15.93 38.68
Ctrl. MC01 24.40 21.49 1.14
MC03 18.63 18.34 3.93
FC02 16.12 15.97 3.11
Average 19.72 18.60 2.73
</table>
<tableCaption confidence="0.99618">
Table 3: Differential entropy, in nats, across dysarthric
</tableCaption>
<bodyText confidence="0.983672">
and control speakers for acoustic ac and articulatory ar
data.
Table 4 shows the average mutual information be-
tween acoustics and articulation for each type of
speaker, given knowledge of the phonological man-
ner of articulation. In table 1 we noted a prevalence
of pronunciation errors among dysarthric speakers
for plosives, but table 4 shows no particularly low
congruity between acoustics and articulation for this
manner of phoneme. Those pronunciation errors
tended to be voicing errors, which would involve the
glottis, which is not measured in this study.
Table 4 appears to imply that there is little mu-
tual information between acoustics and articulation
in vowels across all speakers. However, this is al-
most certainly the result of our exclusion of tongue
blade and tongue dorsum measurements in order to
standardize across speakers who could not manage
these sensors. Indeed, the configuration of the en-
tire tongue is known to be useful in discriminat-
ing among the vowels (O’Shaughnessy, 2000). An
ad hoc analysis including all three tongue sensors
for speakers F03, MC01, MC03, and FC02 revealed
mutual information between acoustics and articula-
</bodyText>
<footnote confidence="0.994146">
1This is borne out in the literature (Kent and Rosen, 2004).
</footnote>
<page confidence="0.995548">
84
</page>
<table confidence="0.99823125">
Manner I(Ac;Ar)
Dys. Ctrl.
plosives 10.92 16.47
affricates 8.71 9.23
fricatives 9.30 10.94
nasals 13.29 15.10
glides 11.92 12.68
vowels 6.76 7.15
</table>
<tableCaption confidence="0.999147666666667">
Table 4: Mutual information I(Ac;Ar) of acoustics and
articulation for dysarthric and control subjects, across
phonological manners of articulation.
</tableCaption>
<bodyText confidence="0.999957310344827">
tion of 16.81 nats for F03 and 18.73 nats for the
control speakers, for vowels. This is compared with
mutual information of 11.82 nats for F03 and 13.88
nats for the control speakers across all other man-
ners. The trend seems to be that acoustics are better
predicted given more tongue measurements.
In order to better understand these results, we
compare the distributions of the vowels in acoustic
space across dysarthric and non-dysarthric speech.
Vowels in acoustic space are characterized by the
steady-state positions of the first two formants (F1
and F2) as determined automatically by applying the
pre-emphasized Burg algorithm (Press et al., 1992).
We fit Gaussians to the first two formants for each
of the vowels in our data, as exemplified in fig-
ure 5 and compute the entropy within these distri-
butions. Surprisingly, the entropies of these distri-
butions were relatively consistent across dysarthric
(34.6 nats) and non-dysarthric (33.3 nats) speech,
with some exceptions (e.g., iy). However, vowel
spaces overlap considerably more in the dysarthric
case signifying that, while speakers with CP can be
nearly as acoustically consistent as non-dysarthric
speakers, their targets in that space are not as dis-
cernible. Some research has shown larger variance
among dysarthric vowels relative to our findings
(Kain et al., 2007). This may partially be due to our
use of natural connected speech as data, rather than
restrictive consonant-vowel-consonant non-words.
</bodyText>
<subsectionHeader confidence="0.959513">
4.2 Noisy channel
</subsectionHeader>
<bodyText confidence="0.98743625">
Our task is to determine whether dysarthric speech
is best represented as a distorted version of typi-
cal speech, or if both dysarthric and typical speech
ought to be viewed as distortions of a common ab-
</bodyText>
<figure confidence="0.997954583333333">
Dysarthric male
300 400 500 600 700 800 900 1000 1100 1200 1300 1400
F1 (Hz)
Non−dysarthric male
2500 iy
ih
2000
eh
1500
aa
300 400 500 600 700 800 900 1000 1100 1200 1300 1400
F1 (Hz)
</figure>
<figureCaption confidence="0.99878775">
Figure 5: Contours showing first standard deviation in
F1 versus F2 space for distributions of six representative
vowels in continuous speech for the dysarthric and non-
dysarthric male speakers.
</figureCaption>
<bodyText confidence="0.947054444444445">
stract representation. To explore this question, we
design a transformation system that produces the
most likely observation in one data space given its
counterpart in another and the statistical relationship
between the two spaces. This transformation in ef-
fect implements the noisy channel itself.
To accomplish this, we learn probability distri-
butions over our EMA data. First, we collect all
dysarthric data together and all non-dysarthric data
together. We then consider the acoustic (Ac) and
articulatory (Ar) subsets of these data. In each
case, we train Gaussian mixtures, each with 60 com-
ponents, over 90% of the data in both dysarthric
and non-dysarthric speech. Here, each of the 60
phonemes in the data is represented by one Gaussian
component, with the weight of that component de-
termined by the relative proportion of 10 ms frames
for that phoneme. Similarly, all training word se-
quences are passed to TADA, and we train a mixture
of Gaussians on its articulatory output.
Across all Gaussian mixtures, we end up with 5
Gaussians tuned to various aspects of each phoneme
p: its dysarthric acoustics and articulation (NAc
p (Yd)
and NAr
p (Yd)), its control acoustics and articula-
tion (NAc
</bodyText>
<listItem confidence="0.8715125">
p (Yd) and NAr
p (Yd)), and its prescribed ar-
ticulation from TADA (NAr
p (X)). Each Gaussian
NAp(B) is represented by its mean �(A,B)
p and its
</listItem>
<figure confidence="0.996955357142857">
ih
ah
eh
aa
uw
iy
F2 (Hz) 2500
2000
1500
1000
F2 (Hz)
1000
uw
ah
</figure>
<page confidence="0.984599">
85
</page>
<bodyText confidence="0.972821875">
covariance, Σ(A,B)
p . Furthermore, we compute the
cross-covariance matrix between Gaussians for a
given phoneme (e.g., Σ(Ac,Yc)→(Ac,Yd) pis the cross-
covariance matrix of the acoustics of the control (Yc)
and dysarthric (Yd) speech for phoneme p). Given
these parameters, we estimate the most likely frame
in one domain given its counterpart in another. For
example, if we are given a frame of acoustics from
a control speaker, we can synthesize the most likely
frame of acoustics for a dysarthric speaker, given an
application of the noisy channel proposed by Hosom
et al. (2003) used to transform dysarthric speech to
make it more intelligible. Namely, given a frame of
acoustics yc from a control speaker, we can estimate
the acoustics of a dysarthric speaker yd with:
</bodyText>
<equation confidence="0.992303333333333">
fAc(yc) =E(yd |yc)
hi (yc) hµi (Ac,Yd) +
� �−1
Σ(Ac,Yc)→(Ac,Yd) Σ(Ac,Yc)
· ·
i i
� ~i
yc − µ(Ac,Yc) ,
i
where αiN (yc; µ (Ac,Yc)i, Σ(Ac,Yc)1
hi(yc) = i
� �,
∑P yc; µ(Ac,Yc)
j ,Σ(Ac,Yc)
j=1 αjN j
</equation>
<bodyText confidence="0.999412941176471">
where αp is the proportion of the frames of phoneme
p in the data. Transforming between different types
and sources of data is accomplished merely by sub-
stituting in the appropriate Gaussians above.
We now measure how closely the transformed
data spaces match their true target spaces. In each
case, we transform test utterances (recorded, or syn-
thesized with TADA) according to functions learned
in training (i.e., we use the remaining 10% of the
data for each speaker type). These transformed
spaces are then compared against their target space
in our data. Table 5 shows the Gaussian mixture
phoneme-level Kullback-Leibler divergences given
various types of source and target data, weighted by
the relative proportions of the phonemes. Each pair
of N-dimensional Gaussians (Ni with mean µi and
covariance Σi) for a given phone and data type is
</bodyText>
<table confidence="0.965973428571429">
KL divergence
(10−2 nats)
Type 2 Acous. Artic.
Dys. 25.36 3.23
Ctrl. → Dys. Dys.
TADA → Ctrl. Ctrl.
TADA → Dys. Dys.
</table>
<tableCaption confidence="0.976795">
Table 5: Average weighted phoneme-level Kullback-
Leibler divergences.
</tableCaption>
<equation confidence="0.974525125">
compared with
� �|Σ1 |�
1
DKL(N0 ||N1) =ln +trace(Σ−1
1 Σ0)
2 |Σ0|
+(µ1 − µ0)TΣ−1
1 (µ1 − µ0) − N).
</equation>
<bodyText confidence="0.999689263157895">
Our baseline shows that control and dysarthric
speakers differ far more in their acoustics than in
their articulation. When our control data (both
acoustic and articulatory) are transformed to match
the dysarthric data, the result is predictably more
similar to the latter than if the conversion had not
taken place. This corresponds to the noisy channel
model of figure 3(a), whereby dysarthric speech is
modelled as a distortion of non-dysarthric speech.
However, when we model dysarthric and control
speech as distortions of a common, abstract repre-
sentation (i.e., task dynamics) as in figure 3(b), the
resulting synthesized articulatory spaces are more
similar to their respective observed data than the
articulation predicted by the first noisy channel
model. Dysarthric articulation predicted by trans-
formations from task-dynamics space differ signifi-
cantly from those predicted by transformations from
control EMA data at the 95% confidence interval.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999937777777778">
This paper demonstrates a few acoustic and articu-
latory features in speakers with cerebral palsy. First,
these speakers are likely to mistakenly voice un-
voiced plosives, and to delete fricatives regardless of
their word position. We suggest that it might be pru-
dent to modify the vocabularies of ASR systems to
account for these expected mispronunciations. Sec-
ond, dysarthric speakers produce sonorants signifi-
cantly slower than their non-dysarthric counterparts.
</bodyText>
<figure confidence="0.989424222222222">
P
∑
i=1
(2)
Type 1
Ctrl.
17.78 2.11
N/A 1.69
N/A 1.84
</figure>
<page confidence="0.987479">
86
</page>
<bodyText confidence="0.999895130434783">
This may present an increase in insertion errors in
ASR systems (Rosen and Yampolsky, 2000).
Although not quantified in this paper, we detect
that a lack of articulatory control can often lead
to observable acoustic consequences. For example,
our dysarthric data contain considerable involuntary
types of velopharyngeal or glottal noise (often as-
sociated with respiration), audible swallowing, and
stuttering. We intend to work towards methods of
explicitly identifying regions of non-speech noise in
our ASR systems for dysarthric speakers.
We have considered the amount of statistical dis-
order (i.e., entropy) in both acoustic and articula-
tory data in dysarthric and non-dysarthric speak-
ers. The use of articulatory knowledge reduces the
degree of this disorder significantly for dysarthric
speakers (18.3%, relatively), though far less than for
non-dysarthric speakers (86.2%, relatively). In real-
world applications we are not likely to have access to
measurements of the vocal tract; however, many ap-
proaches exist that estimate the configuration of the
vocal tract given only acoustic data (Richmond et al.,
2003; Toda et al., 2008), often to an average error of
less than 1 mm. The generalizability of such work
to new speakers (particularly those with dysarthria)
without training is an open research question.
We have argued for noisy channel models of
the neuro-motor interface assuming that the path-
way of motor command to motor activity is a lin-
ear sequence of dynamics. The biological reality
is much more complicated. In particular, the path-
way of verbal motor commands includes several
sources of sensory feedback (Seikel et al., 2005) that
modulate control parameters during speech (Gracco,
1995). These senses include exteroceptive stimuli
(auditory and tactile), and interoceptive stimuli (par-
ticularly proprioception and its kinesthetic sense)
(Seikel et al., 2005), the disruption of which can lead
to a number of production changes. For instance,
Abbs et al. (1976) showed that when conduction in
the mandibular branches of the trigeminal nerve is
blocked, the resulting speech has considerably more
pronunciation errors, although is generally intelligi-
ble. Barlow (1989) argues that the redundancy of
sensory messages provides the necessary input to the
motor planning stage, which relates abstract goals to
motor activity in the cerebellum. As we continue to
develop our articulatory ASR models for dysarthric
speakers, one potential avenue for future research in-
volves the incorporation of feedback from the cur-
rent state of the vocal tract to the motor planning
phase. This would be similar, in premise, to the
DIVA model (Guenther and Perkell, 2004).
In the past, we have shown that ASR systems that
adapt non-dysarthric acoustic models to dysarthric
data offer improved word-accuracy rates, but with
a clear upper bound approximately 75% below the
general population (Rudzicz, 2007). Incorporat-
ing articulatory knowledge into such adaptation im-
proved accuracy further, but with accuracy still
approximately 60% below the general population
(Rudzicz, 2009). In this paper, we have demon-
strated that dysarthric articulation can be more ac-
curately represented as a distortion of an underlying
model of abstract speech goals than as a distortion of
non-dysarthric articulation. These results will guide
our continued development of speech systems aug-
mented with articulatory knowledge, particularly the
incorporation of task dynamics.
</bodyText>
<sectionHeader confidence="0.99724" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998732">
This research is funded by Bell University Labs, the
Natural Sciences and Engineering Research Council
of Canada, and the University of Toronto.
</bodyText>
<sectionHeader confidence="0.997815" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9980522">
James H. Abbs, John W. Folkins, and Murali Sivarajan.
1976. Motor Impairment following Blockade of the
Infraorbital Nerve: Implications for the Use of Anes-
thetization Techniques in Speech Research. Journal of
Speech and Hearing Research, 19(1):19–35.
H.B. Barlow. 1989. Unsupervised learning. Neural
Computation, 1(3):295–311.
Joseph R Duffy. 2005. Motor Speech Disorders:
Substrates, Differential Diagnosis, and Management.
Mosby Inc.
Hans-Joachim Freund, Marc Jeannerod, Mark Hallett,
and Ram´on Leiguarda. 2005. Higher-order motor dis-
orders: From neuroanatomy and neurobiology to clin-
ical neurology. Oxford University Press.
Vincent L. Gracco. 1995. Central and peripheral compo-
nents in the control of speech movements. In Freder-
icka Bell-Berti and Lawrence J. Raphael, editors, In-
troducing Speech: Contempory Issues, for Katherine
Safford Harris, chapter 12, pages 417–431. American
Institute of Physics press.
</reference>
<page confidence="0.995243">
87
</page>
<reference confidence="0.999431556603774">
Frank H. Guenther and Joseph S. Perkell. 2004. A neural
model of speech production and its application to stud-
ies of the role of auditory feedback in speech. In Ben
Maassen, Raymond Kent, Herman Peters, Pascal Van
Lieshout, and Wouter Hulstijn, editors, Speech Motor
Control in Normal and Disordered Speech, chapter 4,
pages 29–49. Oxford University Press, Oxford.
John-Paul Hosom, Alexander B. Kain, Taniya Mishra,
Jan P. H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2003. Intelligibility of modifications to
dysarthric speech. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing (ICASSP ’03), volume 1, pages 924–927,
April.
Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, and
Uwe D. Hanebeck. 2008. On entropy approximation
for Gaussian mixture random vectors. In Proceed-
ings of the 2008 IEEE International Conference on In
Multisensor Fusion and Integration for Intelligent Sys-
tems, pages 181–188, Seoul, South Korea.
Alexander B. Kain, John-Paul Hosom, Xiaochuan Niu,
Jan P.H. van Santen, Melanie Fried-Oken, and Jan-
ice Staehely. 2007. Improving the intelligibil-
ity of dysarthric speech. Speech Communication,
49(9):743–759, September.
Ray D. Kent and Kristin Rosen. 2004. Motor control per-
spectives on motor speech disorders. In Ben Maassen,
Raymond Kent, Herman Peters, Pascal Van Lieshout,
and Wouter Hulstijn, editors, Speech Motor Control
in Normal and Disordered Speech, chapter 12, pages
285–311. Oxford University Press, Oxford.
Ray D. Kent, Gary Weismer, Jane F. Kent, and John C.
Rosenbek. 1989. Toward phonetic intelligibility test-
ing in dysarthria. Journal of Speech and Hearing Dis-
orders, 54:482–499.
Aida C. G. Verdugo Lazo and Pushpa N. Rathie. 1978.
On the entropy of continuous probability distributions.
IEEE Transactions on Information Theory, 23(1):120–
122, January.
Keith L. Moore and Arthur F. Dalley. 2005. Clinically
Oriented Anatomy, Fifth Edition. Lippincott, Williams
and Wilkins.
Hosung Nam and Louis Goldstein. 2006. TADA (TAsk
Dynamics Application) manual.
Hosung Nam and Elliot Saltzman. 2003. A compet-
itive, coupled oscillator model of syllable structure.
In Proceedings of the 15th International Congress of
Phonetic Sciences (ICPhS 2003), pages 2253–2256,
Barcelona, Spain.
Douglas O’Shaughnessy. 2000. Speech Communications
– Human and Machine. IEEE Press, New York, NY,
USA.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C: the art of scientific computing. Cambridge Uni-
versity Press, second edition.
Korin Richmond, Simon King, and Paul Taylor. 2003.
Modelling the uncertainty in recovering articulation
from acoustics. Computer Speech and Language,
17:153–172.
Kristin Rosen and Sasha Yampolsky. 2000. Automatic
speech recognition and a review of its functioning with
dysarthric speech. Augmentative &amp; Alternative Com-
munication, 16(1):48–60, Jan.
Frank Rudzicz. 2007. Comparing speaker-dependent
and speaker-adaptive acoustic models for recognizing
dysarthric speech. In Proceedings of the Ninth Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, Tempe, AZ, October.
Frank Rudzicz. 2009. Applying discretized articulatory
knowledge to dysarthric speech. In Proceedings of
the 2009 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP09), Taipei,
Taiwan, April.
Elliot L. Saltzman and Kevin G. Munhall. 1989. A dy-
namical approach to gestural patterning in speech pro-
duction. Ecological Psychology, 1(4):333–382.
J. Anthony Seikel, Douglas W. King, and David G.
Drumright, editors. 2005. Anatomy &amp; Physiology:
for Speech, Language, and Hearing. Thomson Del-
mar Learning, third edition.
Claude E. Shannon. 1949. A Mathematical Theory of
Communication. University of Illinois Press, Urbana,
IL.
Tomoki Toda, Alan W. Black, and Keiichi Tokuda.
2008. Statistical mapping between articulatory move-
ments and acoustic spectrum using a Gaussian mix-
ture model. Speech Communication, 50(3):215–227,
March.
Alan Wrench. 1999. The MOCHA-TIMIT articulatory
database, November.
Yana Yunusova, Gary Weismer, John R. Westbury, and
Mary J. Lindstrom. 2008. Articulatory movements
during vowels in speakers with dysarthria and healthy
controls. Journal of Speech, Language, and Hearing
Research, 51:596–611, June.
Yana Yunusova, Jordan R. Green, and Antje Mefferd.
2009. Accuracy Assessment for AG500, Electromag-
netic Articulograph. Journal of Speech, Language,
and Hearing Research, 52:547–555, April.
Victor Zue, Stephanie Seneff, and James Glass. 1989.
Speech Database Development: TIMIT and Beyond.
In Proceedings of ESCA Tutorial and Research Work-
shop on Speech Input/Output Assessment and Speech
Databases (SIOA-1989), volume 2, pages 35–40,
Noordwijkerhout, The Netherlands.
</reference>
<page confidence="0.999409">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.933117">
<title confidence="0.999859">Towards a noisy-channel model of dysarthria in speech recognition</title>
<author confidence="0.999703">Frank</author>
<affiliation confidence="0.99995">University of Toronto, Department of Computer</affiliation>
<address confidence="0.937475">Toronto, Ontario,</address>
<email confidence="0.999982">frank@cs.toronto.edu</email>
<abstract confidence="0.99978615">Modern automatic speech recognition is ineffective at understanding relatively unintelligible speech caused by neuro-motor disabilities collectively called dysarthria. Since dysarthria is primarily an articulatory phenomenon, we are collecting a database of vocal tract measurements during speech of individuals with cerebral palsy. In this paper, we demonstrate that articulatory knowledge can remove ambiguities in the acoustics of dysarthric speakby reducing entropy relatively by on average. Furthermore, we demonstrate that dysarthric speech is more precisely portrayed as a noisy-channel distortion of an abstract representation of articulatory goals, rather than as a distortion of non-dysarthric speech. We discuss what implications these results have for our ongoing development of speech systems for dysarthric speakers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James H Abbs</author>
<author>John W Folkins</author>
<author>Murali Sivarajan</author>
</authors>
<title>Motor Impairment following Blockade of the Infraorbital Nerve: Implications for the Use of Anesthetization Techniques in Speech Research.</title>
<date>1976</date>
<journal>Journal of Speech and Hearing Research,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="27833" citStr="Abbs et al. (1976)" startWordPosition="4426" endWordPosition="4429">uro-motor interface assuming that the pathway of motor command to motor activity is a linear sequence of dynamics. The biological reality is much more complicated. In particular, the pathway of verbal motor commands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). These senses include exteroceptive stimuli (auditory and tactile), and interoceptive stimuli (particularly proprioception and its kinesthetic sense) (Seikel et al., 2005), the disruption of which can lead to a number of production changes. For instance, Abbs et al. (1976) showed that when conduction in the mandibular branches of the trigeminal nerve is blocked, the resulting speech has considerably more pronunciation errors, although is generally intelligible. Barlow (1989) argues that the redundancy of sensory messages provides the necessary input to the motor planning stage, which relates abstract goals to motor activity in the cerebellum. As we continue to develop our articulatory ASR models for dysarthric speakers, one potential avenue for future research involves the incorporation of feedback from the current state of the vocal tract to the motor planning</context>
</contexts>
<marker>Abbs, Folkins, Sivarajan, 1976</marker>
<rawString>James H. Abbs, John W. Folkins, and Murali Sivarajan. 1976. Motor Impairment following Blockade of the Infraorbital Nerve: Implications for the Use of Anesthetization Techniques in Speech Research. Journal of Speech and Hearing Research, 19(1):19–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H B Barlow</author>
</authors>
<title>Unsupervised learning.</title>
<date>1989</date>
<journal>Neural Computation,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="28039" citStr="Barlow (1989)" startWordPosition="4457" endWordPosition="4458">ands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). These senses include exteroceptive stimuli (auditory and tactile), and interoceptive stimuli (particularly proprioception and its kinesthetic sense) (Seikel et al., 2005), the disruption of which can lead to a number of production changes. For instance, Abbs et al. (1976) showed that when conduction in the mandibular branches of the trigeminal nerve is blocked, the resulting speech has considerably more pronunciation errors, although is generally intelligible. Barlow (1989) argues that the redundancy of sensory messages provides the necessary input to the motor planning stage, which relates abstract goals to motor activity in the cerebellum. As we continue to develop our articulatory ASR models for dysarthric speakers, one potential avenue for future research involves the incorporation of feedback from the current state of the vocal tract to the motor planning phase. This would be similar, in premise, to the DIVA model (Guenther and Perkell, 2004). In the past, we have shown that ASR systems that adapt non-dysarthric acoustic models to dysarthric data offer impr</context>
</contexts>
<marker>Barlow, 1989</marker>
<rawString>H.B. Barlow. 1989. Unsupervised learning. Neural Computation, 1(3):295–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph R Duffy</author>
</authors>
<title>Motor Speech Disorders: Substrates, Differential Diagnosis, and Management.</title>
<date>2005</date>
<publisher>Mosby Inc.</publisher>
<contexts>
<context position="1856" citStr="Duffy, 2005" startWordPosition="261" endWordPosition="262">). Causes of dysarthria include cerebral palsy (CP), multiple sclerosis, Parkinson’s disease, and amyotrophic lateral sclerosis (ALS). These impairments reduce or remove normal control of the primary vocal articulators but do not affect the abstract production of meaningful, syntactically correct language. The neurological origins of dysarthria involve damage to the cranial nerves that control the speech articulators (Moore and Dalley, 2005). Spastic 80 dysarthria, for instance, is partially caused by lesions in the facial and hypoglossal nerves, which control the jaw and tongue respectively (Duffy, 2005), resulting in slurred speech and a less differentiable vowel space (Kent and Rosen, 2004). Similarly, damage to the glossopharyngeal nerve can reduce control over vocal fold vibration (i.e., phonation), resulting in guttural or grating raspiness. Inadequate control of the soft palate caused by disruption of the vagus nerve may lead to a disproportionate amount of air released through the nose during speech (i.e., hypernasality). Unfortunately, traditional automatic speech recognition (ASR) is incompatible with dysarthric speech, often rendering such software inaccessible to those whose neuro-</context>
</contexts>
<marker>Duffy, 2005</marker>
<rawString>Joseph R Duffy. 2005. Motor Speech Disorders: Substrates, Differential Diagnosis, and Management. Mosby Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans-Joachim Freund</author>
<author>Marc Jeannerod</author>
<author>Mark Hallett</author>
<author>Ram´on Leiguarda</author>
</authors>
<title>Higher-order motor disorders: From neuroanatomy and neurobiology to clinical neurology.</title>
<date>2005</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="3723" citStr="Freund et al., 2005" startWordPosition="530" endWordPosition="533">stand why ASR fails for dysarthric speakers by understanding the acoustic and articulatory nature of their speech. In this paper, we cast the speech-motor interface within the mathematical framework of the noisychannel model. This is motivated by the characProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80–88, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics terization of dysarthria as a distortion of parallel biological pathways that corrupt motor signals before execution (Kent and Rosen, 2004; Freund et al., 2005), as in the examples cited above. Within this information-theoretic framework, we aim to infer the nature of the motor signal distortions given appropriate measurements of the vocal tract. That is, we ask the following question: Is dysarthric speech a distortion of typical speech, or are they both distortions of some common underlying representation? 2 Dysarthric articulation data Since the underlying articulatory dynamics of dysarthric speech are intrinsically responsible for complex acoustic irregularities, we are collecting a database of dysarthric articulation. Time-aligned movement and ac</context>
</contexts>
<marker>Freund, Jeannerod, Hallett, Leiguarda, 2005</marker>
<rawString>Hans-Joachim Freund, Marc Jeannerod, Mark Hallett, and Ram´on Leiguarda. 2005. Higher-order motor disorders: From neuroanatomy and neurobiology to clinical neurology. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent L Gracco</author>
</authors>
<title>Central and peripheral components in the control of speech movements.</title>
<date>1995</date>
<booktitle>In Fredericka Bell-Berti and</booktitle>
<pages>417--431</pages>
<editor>Lawrence J. Raphael, editors,</editor>
<publisher>American Institute of Physics press.</publisher>
<contexts>
<context position="27559" citStr="Gracco, 1995" startWordPosition="4388" endWordPosition="4389">chmond et al., 2003; Toda et al., 2008), often to an average error of less than 1 mm. The generalizability of such work to new speakers (particularly those with dysarthria) without training is an open research question. We have argued for noisy channel models of the neuro-motor interface assuming that the pathway of motor command to motor activity is a linear sequence of dynamics. The biological reality is much more complicated. In particular, the pathway of verbal motor commands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). These senses include exteroceptive stimuli (auditory and tactile), and interoceptive stimuli (particularly proprioception and its kinesthetic sense) (Seikel et al., 2005), the disruption of which can lead to a number of production changes. For instance, Abbs et al. (1976) showed that when conduction in the mandibular branches of the trigeminal nerve is blocked, the resulting speech has considerably more pronunciation errors, although is generally intelligible. Barlow (1989) argues that the redundancy of sensory messages provides the necessary input to the motor planning stage, which relates </context>
</contexts>
<marker>Gracco, 1995</marker>
<rawString>Vincent L. Gracco. 1995. Central and peripheral components in the control of speech movements. In Fredericka Bell-Berti and Lawrence J. Raphael, editors, Introducing Speech: Contempory Issues, for Katherine Safford Harris, chapter 12, pages 417–431. American Institute of Physics press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank H Guenther</author>
<author>Joseph S Perkell</author>
</authors>
<title>A neural model of speech production and its application to studies of the role of auditory feedback in speech.</title>
<date>2004</date>
<booktitle>Speech Motor Control in Normal and Disordered Speech, chapter 4,</booktitle>
<pages>29--49</pages>
<editor>In Ben Maassen, Raymond Kent, Herman Peters, Pascal Van Lieshout, and Wouter Hulstijn, editors,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="28522" citStr="Guenther and Perkell, 2004" startWordPosition="4533" endWordPosition="4536"> trigeminal nerve is blocked, the resulting speech has considerably more pronunciation errors, although is generally intelligible. Barlow (1989) argues that the redundancy of sensory messages provides the necessary input to the motor planning stage, which relates abstract goals to motor activity in the cerebellum. As we continue to develop our articulatory ASR models for dysarthric speakers, one potential avenue for future research involves the incorporation of feedback from the current state of the vocal tract to the motor planning phase. This would be similar, in premise, to the DIVA model (Guenther and Perkell, 2004). In the past, we have shown that ASR systems that adapt non-dysarthric acoustic models to dysarthric data offer improved word-accuracy rates, but with a clear upper bound approximately 75% below the general population (Rudzicz, 2007). Incorporating articulatory knowledge into such adaptation improved accuracy further, but with accuracy still approximately 60% below the general population (Rudzicz, 2009). In this paper, we have demonstrated that dysarthric articulation can be more accurately represented as a distortion of an underlying model of abstract speech goals than as a distortion of non</context>
</contexts>
<marker>Guenther, Perkell, 2004</marker>
<rawString>Frank H. Guenther and Joseph S. Perkell. 2004. A neural model of speech production and its application to studies of the role of auditory feedback in speech. In Ben Maassen, Raymond Kent, Herman Peters, Pascal Van Lieshout, and Wouter Hulstijn, editors, Speech Motor Control in Normal and Disordered Speech, chapter 4, pages 29–49. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John-Paul Hosom</author>
<author>Alexander B Kain</author>
<author>Taniya Mishra</author>
<author>Jan P H van Santen</author>
<author>Melanie Fried-Oken</author>
<author>Janice Staehely</author>
</authors>
<title>Intelligibility of modifications to dysarthric speech.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’03),</booktitle>
<volume>1</volume>
<pages>924--927</pages>
<marker>Hosom, Kain, Mishra, van Santen, Fried-Oken, Staehely, 2003</marker>
<rawString>John-Paul Hosom, Alexander B. Kain, Taniya Mishra, Jan P. H. van Santen, Melanie Fried-Oken, and Janice Staehely. 2003. Intelligibility of modifications to dysarthric speech. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’03), volume 1, pages 924–927, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco F Huber</author>
<author>Tim Bailey</author>
<author>Hugh Durrant-Whyte</author>
<author>Uwe D Hanebeck</author>
</authors>
<title>On entropy approximation for Gaussian mixture random vectors.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 IEEE International Conference on In Multisensor Fusion and Integration for Intelligent Systems,</booktitle>
<pages>181--188</pages>
<location>Seoul, South</location>
<contexts>
<context position="10083" citStr="Huber et al. (2008)" startWordPosition="1567" endWordPosition="1570">are in our acoustic and articulatory database, we must use differential entropy defined by ZH(X) = − X f(X)log f(X)dX, where f (X) is the probability density function of X. For a number of distributions f (X), the differential entropy has known forms (Lazo and Rathie, 1978). For example, if f (X) is a multivariate normal, exp(−12(x −µ)TΣ−1(x −µ)) (1) 2 ln�(2πe)N |Σ|�, H(X) = 1 where µ and Σ are the mean and covariances of the data. However, since we observe that both acoustic and articulatory data follow non-Gaussian distributions, we choose to represent these spaces by mixtures of Gaussians. Huber et al. (2008) have developed an accurate algorithm for estimating differential entropy of Gaussian mixtures based on iteratively merging Gaussians and the approximation 2 log((2πe)N |Σi|�, ωi �−logωi + 1 where ωi is the weight of the ith(1 &lt; i &lt; L) Gaussian and Σi is that Gaussian’s covariance matrix. This method is used to approximate entropies in the following study, with L = 32. Note that while differential entropies can be negative and not invariant under n ∑ i=1 H(X) = − fX(x1,...,xN) = (2π)N/2|Σ|1/2 L ˜H(X) = ∑ i=1 82 change of variables, other properties of entropy are retained (Huber et al., 2008),</context>
</contexts>
<marker>Huber, Bailey, Durrant-Whyte, Hanebeck, 2008</marker>
<rawString>Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, and Uwe D. Hanebeck. 2008. On entropy approximation for Gaussian mixture random vectors. In Proceedings of the 2008 IEEE International Conference on In Multisensor Fusion and Integration for Intelligent Systems, pages 181–188, Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander B Kain</author>
<author>John-Paul Hosom</author>
<author>Xiaochuan Niu</author>
<author>Jan P H van Santen</author>
<author>Melanie Fried-Oken</author>
<author>Janice Staehely</author>
</authors>
<title>Improving the intelligibility of dysarthric speech.</title>
<date>2007</date>
<journal>Speech Communication,</journal>
<volume>49</volume>
<issue>9</issue>
<marker>Kain, Hosom, Niu, van Santen, Fried-Oken, Staehely, 2007</marker>
<rawString>Alexander B. Kain, John-Paul Hosom, Xiaochuan Niu, Jan P.H. van Santen, Melanie Fried-Oken, and Janice Staehely. 2007. Improving the intelligibility of dysarthric speech. Speech Communication, 49(9):743–759, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray D Kent</author>
<author>Kristin Rosen</author>
</authors>
<title>Motor control perspectives on motor speech disorders.</title>
<date>2004</date>
<booktitle>Speech Motor Control in Normal and Disordered Speech, chapter 12,</booktitle>
<pages>285--311</pages>
<editor>In Ben Maassen, Raymond Kent, Herman Peters, Pascal Van Lieshout, and Wouter Hulstijn, editors,</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1946" citStr="Kent and Rosen, 2004" startWordPosition="274" endWordPosition="277">n’s disease, and amyotrophic lateral sclerosis (ALS). These impairments reduce or remove normal control of the primary vocal articulators but do not affect the abstract production of meaningful, syntactically correct language. The neurological origins of dysarthria involve damage to the cranial nerves that control the speech articulators (Moore and Dalley, 2005). Spastic 80 dysarthria, for instance, is partially caused by lesions in the facial and hypoglossal nerves, which control the jaw and tongue respectively (Duffy, 2005), resulting in slurred speech and a less differentiable vowel space (Kent and Rosen, 2004). Similarly, damage to the glossopharyngeal nerve can reduce control over vocal fold vibration (i.e., phonation), resulting in guttural or grating raspiness. Inadequate control of the soft palate caused by disruption of the vagus nerve may lead to a disproportionate amount of air released through the nose during speech (i.e., hypernasality). Unfortunately, traditional automatic speech recognition (ASR) is incompatible with dysarthric speech, often rendering such software inaccessible to those whose neuro-motor disabilities might make other forms of interaction (e.g., keyboards, touch screens) </context>
<context position="3701" citStr="Kent and Rosen, 2004" startWordPosition="526" endWordPosition="529">h. Our aim is to understand why ASR fails for dysarthric speakers by understanding the acoustic and articulatory nature of their speech. In this paper, we cast the speech-motor interface within the mathematical framework of the noisychannel model. This is motivated by the characProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80–88, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics terization of dysarthria as a distortion of parallel biological pathways that corrupt motor signals before execution (Kent and Rosen, 2004; Freund et al., 2005), as in the examples cited above. Within this information-theoretic framework, we aim to infer the nature of the motor signal distortions given appropriate measurements of the vocal tract. That is, we ask the following question: Is dysarthric speech a distortion of typical speech, or are they both distortions of some common underlying representation? 2 Dysarthric articulation data Since the underlying articulatory dynamics of dysarthric speech are intrinsically responsible for complex acoustic irregularities, we are collecting a database of dysarthric articulation. Time-a</context>
<context position="18470" citStr="Kent and Rosen, 2004" startWordPosition="2915" endWordPosition="2918">ere is little mutual information between acoustics and articulation in vowels across all speakers. However, this is almost certainly the result of our exclusion of tongue blade and tongue dorsum measurements in order to standardize across speakers who could not manage these sensors. Indeed, the configuration of the entire tongue is known to be useful in discriminating among the vowels (O’Shaughnessy, 2000). An ad hoc analysis including all three tongue sensors for speakers F03, MC01, MC03, and FC02 revealed mutual information between acoustics and articula1This is borne out in the literature (Kent and Rosen, 2004). 84 Manner I(Ac;Ar) Dys. Ctrl. plosives 10.92 16.47 affricates 8.71 9.23 fricatives 9.30 10.94 nasals 13.29 15.10 glides 11.92 12.68 vowels 6.76 7.15 Table 4: Mutual information I(Ac;Ar) of acoustics and articulation for dysarthric and control subjects, across phonological manners of articulation. tion of 16.81 nats for F03 and 18.73 nats for the control speakers, for vowels. This is compared with mutual information of 11.82 nats for F03 and 13.88 nats for the control speakers across all other manners. The trend seems to be that acoustics are better predicted given more tongue measurements. I</context>
</contexts>
<marker>Kent, Rosen, 2004</marker>
<rawString>Ray D. Kent and Kristin Rosen. 2004. Motor control perspectives on motor speech disorders. In Ben Maassen, Raymond Kent, Herman Peters, Pascal Van Lieshout, and Wouter Hulstijn, editors, Speech Motor Control in Normal and Disordered Speech, chapter 12, pages 285–311. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray D Kent</author>
<author>Gary Weismer</author>
<author>Jane F Kent</author>
<author>John C Rosenbek</author>
</authors>
<title>Toward phonetic intelligibility testing in dysarthria.</title>
<date>1989</date>
<journal>Journal of Speech and Hearing Disorders,</journal>
<pages>54--482</pages>
<contexts>
<context position="6340" citStr="Kent et al. (1989)" startWordPosition="957" endWordPosition="960"> 2 shows the degree of lip aperture (i.e., the distance between UL and LL) over time for a control and a dysarthric speaker repeating the sequence /ah p iy/. Here, the dysarthric speech is notably slower and has more excessive movement. 5 Control Dysarthric 4 3 2 10 1 2 3 4 5 6 Time (s) Figure 2: Lip aperture over time for four iterations of /ah p iy/ given a dysarthric and control speaker. Our dysarthric speech data include random repetitions of phonetically balanced short sentences originally used in the TIMIT database (Zue et al., 1989), as well as pairs of monosyllabic words identified by Kent et al. (1989) as having relevant articulatory contrasts (e.g., beat versus meat as a stopnasal contrast). All articulatory data are aligned with associated acoustic data, which are transformed to Mel-frequency cepstral coefficients (MFCCs). Phoneme boundaries and pronunciation errors are being transcribed by a speech-language pathologist to the TIMIT phoneset. Table 1 shows pronunciation errors according to manner of articulation for dysarthric speech. Plosives are mispronounced most often, with substitution errors exclusively caused by errant voicing (e.g. /d/ for /t/). By comparison, only Lip aperture (c</context>
</contexts>
<marker>Kent, Weismer, Kent, Rosenbek, 1989</marker>
<rawString>Ray D. Kent, Gary Weismer, Jane F. Kent, and John C. Rosenbek. 1989. Toward phonetic intelligibility testing in dysarthria. Journal of Speech and Hearing Disorders, 54:482–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aida C G Verdugo Lazo</author>
<author>Pushpa N Rathie</author>
</authors>
<title>On the entropy of continuous probability distributions.</title>
<date>1978</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>23</volume>
<issue>1</issue>
<pages>122</pages>
<contexts>
<context position="9738" citStr="Lazo and Rathie, 1978" startWordPosition="1507" endWordPosition="1510">stems for dysarthric speakers. Entropy, H(X), is a measure of the degree of uncertainty in a random variable X. When X is discrete, this value is computed with the familiar p(xi)logb p(xi), where b is the logarithm base, xi is a value of X, of which there are n possible, and p(xi) is its probability. When our observations are continuous, as they are in our acoustic and articulatory database, we must use differential entropy defined by ZH(X) = − X f(X)log f(X)dX, where f (X) is the probability density function of X. For a number of distributions f (X), the differential entropy has known forms (Lazo and Rathie, 1978). For example, if f (X) is a multivariate normal, exp(−12(x −µ)TΣ−1(x −µ)) (1) 2 ln�(2πe)N |Σ|�, H(X) = 1 where µ and Σ are the mean and covariances of the data. However, since we observe that both acoustic and articulatory data follow non-Gaussian distributions, we choose to represent these spaces by mixtures of Gaussians. Huber et al. (2008) have developed an accurate algorithm for estimating differential entropy of Gaussian mixtures based on iteratively merging Gaussians and the approximation 2 log((2πe)N |Σi|�, ωi �−logωi + 1 where ωi is the weight of the ith(1 &lt; i &lt; L) Gaussian and Σi is </context>
</contexts>
<marker>Lazo, Rathie, 1978</marker>
<rawString>Aida C. G. Verdugo Lazo and Pushpa N. Rathie. 1978. On the entropy of continuous probability distributions. IEEE Transactions on Information Theory, 23(1):120– 122, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith L Moore</author>
<author>Arthur F Dalley</author>
</authors>
<title>Clinically Oriented Anatomy, Fifth Edition. Lippincott,</title>
<date>2005</date>
<publisher>and Wilkins.</publisher>
<location>Williams</location>
<contexts>
<context position="1689" citStr="Moore and Dalley, 2005" startWordPosition="233" endWordPosition="236">of congenital and traumatic neuro-motor disorders that impair the physical production of speech and affects approximately 0.8% of individuals in North America (Hosom et al., 2003). Causes of dysarthria include cerebral palsy (CP), multiple sclerosis, Parkinson’s disease, and amyotrophic lateral sclerosis (ALS). These impairments reduce or remove normal control of the primary vocal articulators but do not affect the abstract production of meaningful, syntactically correct language. The neurological origins of dysarthria involve damage to the cranial nerves that control the speech articulators (Moore and Dalley, 2005). Spastic 80 dysarthria, for instance, is partially caused by lesions in the facial and hypoglossal nerves, which control the jaw and tongue respectively (Duffy, 2005), resulting in slurred speech and a less differentiable vowel space (Kent and Rosen, 2004). Similarly, damage to the glossopharyngeal nerve can reduce control over vocal fold vibration (i.e., phonation), resulting in guttural or grating raspiness. Inadequate control of the soft palate caused by disruption of the vagus nerve may lead to a disproportionate amount of air released through the nose during speech (i.e., hypernasality).</context>
</contexts>
<marker>Moore, Dalley, 2005</marker>
<rawString>Keith L. Moore and Arthur F. Dalley. 2005. Clinically Oriented Anatomy, Fifth Edition. Lippincott, Williams and Wilkins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hosung Nam</author>
<author>Louis Goldstein</author>
</authors>
<date>2006</date>
<note>TADA (TAsk Dynamics Application) manual.</note>
<contexts>
<context position="13853" citStr="Nam and Goldstein, 2006" startWordPosition="2181" endWordPosition="2185">lizations of speech as a sequence of overlapping gestures, which are continuous dynamical systems that describe goaloriented reconfigurations of the vocal tract, such as bilabial closure during /m/. Figure 4 shows an example of overlapping gestures for the word pub. Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping goals for tongue blade constriction degree (TBCD), lip aperture (LA), and glottis (GLO). Boxes represent the present of discretized goals, such as lip closure. Black curves represent the output of the TADA system. The open-source TADA system (Nam and Goldstein, 2006) estimates the positions of various articulators during speech according to parameters that have been carefully tuned by the authors of TADA according to a generic, speaker-independent representation of the vocal tract (Saltzman and Munhall, 1989). Given a word sequence and a syllable-togesture dictionary, TADA produces the continuous TBCD open LA closed open closed open GLO closed 100 200 300 400 Time (ms) Typical speech P(Y |X) signal, X Abstract speech P(Yd |X) signal, X P(Yc |X) 83 tract variable paths that are necessary to produce that sequence. This takes into account various physiologic</context>
</contexts>
<marker>Nam, Goldstein, 2006</marker>
<rawString>Hosung Nam and Louis Goldstein. 2006. TADA (TAsk Dynamics Application) manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hosung Nam</author>
<author>Elliot Saltzman</author>
</authors>
<title>A competitive, coupled oscillator model of syllable structure.</title>
<date>2003</date>
<booktitle>In Proceedings of the 15th International Congress of Phonetic Sciences</booktitle>
<pages>2253--2256</pages>
<location>Barcelona,</location>
<contexts>
<context position="14566" citStr="Nam and Saltzman, 2003" startWordPosition="2294" endWordPosition="2297">t have been carefully tuned by the authors of TADA according to a generic, speaker-independent representation of the vocal tract (Saltzman and Munhall, 1989). Given a word sequence and a syllable-togesture dictionary, TADA produces the continuous TBCD open LA closed open closed open GLO closed 100 200 300 400 Time (ms) Typical speech P(Y |X) signal, X Abstract speech P(Yd |X) signal, X P(Yc |X) 83 tract variable paths that are necessary to produce that sequence. This takes into account various physiological aspects of human speech production, such as interarticulator co-ordination and timing (Nam and Saltzman, 2003). In this study, we use TADA to produce estimates of a global, high-level representation of speech common to both dysarthric and non-dysarthric speakers alike. Given a word sequence uttered by both types of speaker, we produce five continuous curves prescribed by that word sequence in order to match our available EMA data. Those curves are lip aperture and protrusion (LA and LP), tongue tip constriction location and degree (TTCL and TTCD, representing front-back and top-down positions of the tongue tip, respectively), and lower incisor height (LIH). These curves are then compared against actua</context>
</contexts>
<marker>Nam, Saltzman, 2003</marker>
<rawString>Hosung Nam and Elliot Saltzman. 2003. A competitive, coupled oscillator model of syllable structure. In Proceedings of the 15th International Congress of Phonetic Sciences (ICPhS 2003), pages 2253–2256, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas O’Shaughnessy</author>
</authors>
<title>Speech Communications – Human and Machine.</title>
<date>2000</date>
<publisher>IEEE Press,</publisher>
<location>New York, NY, USA.</location>
<marker>O’Shaughnessy, 2000</marker>
<rawString>Douglas O’Shaughnessy. 2000. Speech Communications – Human and Machine. IEEE Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: the art of scientific computing.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="19425" citStr="Press et al., 1992" startWordPosition="3062" endWordPosition="3065"> nats for the control speakers, for vowels. This is compared with mutual information of 11.82 nats for F03 and 13.88 nats for the control speakers across all other manners. The trend seems to be that acoustics are better predicted given more tongue measurements. In order to better understand these results, we compare the distributions of the vowels in acoustic space across dysarthric and non-dysarthric speech. Vowels in acoustic space are characterized by the steady-state positions of the first two formants (F1 and F2) as determined automatically by applying the pre-emphasized Burg algorithm (Press et al., 1992). We fit Gaussians to the first two formants for each of the vowels in our data, as exemplified in figure 5 and compute the entropy within these distributions. Surprisingly, the entropies of these distributions were relatively consistent across dysarthric (34.6 nats) and non-dysarthric (33.3 nats) speech, with some exceptions (e.g., iy). However, vowel spaces overlap considerably more in the dysarthric case signifying that, while speakers with CP can be nearly as acoustically consistent as non-dysarthric speakers, their targets in that space are not as discernible. Some research has shown larg</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: the art of scientific computing. Cambridge University Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Korin Richmond</author>
<author>Simon King</author>
<author>Paul Taylor</author>
</authors>
<title>Modelling the uncertainty in recovering articulation from acoustics. Computer Speech and Language,</title>
<date>2003</date>
<pages>17--153</pages>
<contexts>
<context position="26965" citStr="Richmond et al., 2003" startWordPosition="4290" endWordPosition="4293">e in our ASR systems for dysarthric speakers. We have considered the amount of statistical disorder (i.e., entropy) in both acoustic and articulatory data in dysarthric and non-dysarthric speakers. The use of articulatory knowledge reduces the degree of this disorder significantly for dysarthric speakers (18.3%, relatively), though far less than for non-dysarthric speakers (86.2%, relatively). In realworld applications we are not likely to have access to measurements of the vocal tract; however, many approaches exist that estimate the configuration of the vocal tract given only acoustic data (Richmond et al., 2003; Toda et al., 2008), often to an average error of less than 1 mm. The generalizability of such work to new speakers (particularly those with dysarthria) without training is an open research question. We have argued for noisy channel models of the neuro-motor interface assuming that the pathway of motor command to motor activity is a linear sequence of dynamics. The biological reality is much more complicated. In particular, the pathway of verbal motor commands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). Thes</context>
</contexts>
<marker>Richmond, King, Taylor, 2003</marker>
<rawString>Korin Richmond, Simon King, and Paul Taylor. 2003. Modelling the uncertainty in recovering articulation from acoustics. Computer Speech and Language, 17:153–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristin Rosen</author>
<author>Sasha Yampolsky</author>
</authors>
<title>Automatic speech recognition and a review of its functioning with dysarthric speech.</title>
<date>2000</date>
<journal>Augmentative &amp; Alternative Communication,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="25939" citStr="Rosen and Yampolsky, 2000" startWordPosition="4138" endWordPosition="4141"> 5 Discussion This paper demonstrates a few acoustic and articulatory features in speakers with cerebral palsy. First, these speakers are likely to mistakenly voice unvoiced plosives, and to delete fricatives regardless of their word position. We suggest that it might be prudent to modify the vocabularies of ASR systems to account for these expected mispronunciations. Second, dysarthric speakers produce sonorants significantly slower than their non-dysarthric counterparts. P ∑ i=1 (2) Type 1 Ctrl. 17.78 2.11 N/A 1.69 N/A 1.84 86 This may present an increase in insertion errors in ASR systems (Rosen and Yampolsky, 2000). Although not quantified in this paper, we detect that a lack of articulatory control can often lead to observable acoustic consequences. For example, our dysarthric data contain considerable involuntary types of velopharyngeal or glottal noise (often associated with respiration), audible swallowing, and stuttering. We intend to work towards methods of explicitly identifying regions of non-speech noise in our ASR systems for dysarthric speakers. We have considered the amount of statistical disorder (i.e., entropy) in both acoustic and articulatory data in dysarthric and non-dysarthric speaker</context>
</contexts>
<marker>Rosen, Yampolsky, 2000</marker>
<rawString>Kristin Rosen and Sasha Yampolsky. 2000. Automatic speech recognition and a review of its functioning with dysarthric speech. Augmentative &amp; Alternative Communication, 16(1):48–60, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rudzicz</author>
</authors>
<title>Comparing speaker-dependent and speaker-adaptive acoustic models for recognizing dysarthric speech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International ACM SIGACCESS Conference on Computers and Accessibility,</booktitle>
<location>Tempe, AZ,</location>
<contexts>
<context position="2833" citStr="Rudzicz, 2007" startWordPosition="401" endWordPosition="402"> of air released through the nose during speech (i.e., hypernasality). Unfortunately, traditional automatic speech recognition (ASR) is incompatible with dysarthric speech, often rendering such software inaccessible to those whose neuro-motor disabilities might make other forms of interaction (e.g., keyboards, touch screens) laborious. Traditional representations in ASR such as hidden Markov models (HMMs) trained for speaker independence that achieve 84.8% word-level accuracy for non-dysarthric speakers might achieve less than 4.5% accuracy given severely dysarthric speech on short sentences (Rudzicz, 2007). Our research group is currently developing new ASR models that incorporate empirical knowledge of dysarthric articulation for use in assistive applications (Rudzicz, 2009). Although these models have increased accuracy, the disparity is still high. Our aim is to understand why ASR fails for dysarthric speakers by understanding the acoustic and articulatory nature of their speech. In this paper, we cast the speech-motor interface within the mathematical framework of the noisychannel model. This is motivated by the characProceedings of the NAACL HLT 2010 Workshop on Speech and Language Process</context>
<context position="28756" citStr="Rudzicz, 2007" startWordPosition="4570" endWordPosition="4571">e, which relates abstract goals to motor activity in the cerebellum. As we continue to develop our articulatory ASR models for dysarthric speakers, one potential avenue for future research involves the incorporation of feedback from the current state of the vocal tract to the motor planning phase. This would be similar, in premise, to the DIVA model (Guenther and Perkell, 2004). In the past, we have shown that ASR systems that adapt non-dysarthric acoustic models to dysarthric data offer improved word-accuracy rates, but with a clear upper bound approximately 75% below the general population (Rudzicz, 2007). Incorporating articulatory knowledge into such adaptation improved accuracy further, but with accuracy still approximately 60% below the general population (Rudzicz, 2009). In this paper, we have demonstrated that dysarthric articulation can be more accurately represented as a distortion of an underlying model of abstract speech goals than as a distortion of non-dysarthric articulation. These results will guide our continued development of speech systems augmented with articulatory knowledge, particularly the incorporation of task dynamics. Acknowledgments This research is funded by Bell Uni</context>
</contexts>
<marker>Rudzicz, 2007</marker>
<rawString>Frank Rudzicz. 2007. Comparing speaker-dependent and speaker-adaptive acoustic models for recognizing dysarthric speech. In Proceedings of the Ninth International ACM SIGACCESS Conference on Computers and Accessibility, Tempe, AZ, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rudzicz</author>
</authors>
<title>Applying discretized articulatory knowledge to dysarthric speech.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP09),</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="3006" citStr="Rudzicz, 2009" startWordPosition="425" endWordPosition="426">often rendering such software inaccessible to those whose neuro-motor disabilities might make other forms of interaction (e.g., keyboards, touch screens) laborious. Traditional representations in ASR such as hidden Markov models (HMMs) trained for speaker independence that achieve 84.8% word-level accuracy for non-dysarthric speakers might achieve less than 4.5% accuracy given severely dysarthric speech on short sentences (Rudzicz, 2007). Our research group is currently developing new ASR models that incorporate empirical knowledge of dysarthric articulation for use in assistive applications (Rudzicz, 2009). Although these models have increased accuracy, the disparity is still high. Our aim is to understand why ASR fails for dysarthric speakers by understanding the acoustic and articulatory nature of their speech. In this paper, we cast the speech-motor interface within the mathematical framework of the noisychannel model. This is motivated by the characProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80–88, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics terization of dysarthria as a distortion of</context>
</contexts>
<marker>Rudzicz, 2009</marker>
<rawString>Frank Rudzicz. 2009. Applying discretized articulatory knowledge to dysarthric speech. In Proceedings of the 2009 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP09), Taipei, Taiwan, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliot L Saltzman</author>
<author>Kevin G Munhall</author>
</authors>
<title>A dynamical approach to gestural patterning in speech production.</title>
<date>1989</date>
<journal>Ecological Psychology,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="13140" citStr="Saltzman and Munhall, 1989" startWordPosition="2072" endWordPosition="2075"> articulatory, or some combination thereof. 3.2 Common underlying abstractions In order to test our hypothesis that both dysarthric and control speakers share a common high-level abDysarthric speech signal, Y (a) Dysarthric speech as a distortion of control speech Dysarthric speech signal, Yd Control speech signal, Yc (b) Dysarthric and control speech as distortions of a common abstraction Figure 3: Sections of noisy channel models that mimic the neuro-motor interface. straction of the vocal tract that is in both cases distorted during articulation, we incorporate the theory of task dynamics (Saltzman and Munhall, 1989). This theory represents the interface between the lexical intentions and vocal tract realizations of speech as a sequence of overlapping gestures, which are continuous dynamical systems that describe goaloriented reconfigurations of the vocal tract, such as bilabial closure during /m/. Figure 4 shows an example of overlapping gestures for the word pub. Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping goals for tongue blade constriction degree (TBCD), lip aperture (LA), and glottis (GLO). Boxes represent the present of discretized goals, such as lip clo</context>
</contexts>
<marker>Saltzman, Munhall, 1989</marker>
<rawString>Elliot L. Saltzman and Kevin G. Munhall. 1989. A dynamical approach to gestural patterning in speech production. Ecological Psychology, 1(4):333–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Anthony Seikel</author>
<author>Douglas W King</author>
<author>David G Drumright</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>Anatomy &amp; Physiology: for Speech, Language, and Hearing. Thomson Delmar Learning, third edition.</booktitle>
<contexts>
<context position="27497" citStr="Seikel et al., 2005" startWordPosition="4378" endWordPosition="4381">ate the configuration of the vocal tract given only acoustic data (Richmond et al., 2003; Toda et al., 2008), often to an average error of less than 1 mm. The generalizability of such work to new speakers (particularly those with dysarthria) without training is an open research question. We have argued for noisy channel models of the neuro-motor interface assuming that the pathway of motor command to motor activity is a linear sequence of dynamics. The biological reality is much more complicated. In particular, the pathway of verbal motor commands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). These senses include exteroceptive stimuli (auditory and tactile), and interoceptive stimuli (particularly proprioception and its kinesthetic sense) (Seikel et al., 2005), the disruption of which can lead to a number of production changes. For instance, Abbs et al. (1976) showed that when conduction in the mandibular branches of the trigeminal nerve is blocked, the resulting speech has considerably more pronunciation errors, although is generally intelligible. Barlow (1989) argues that the redundancy of sensory messages provides t</context>
</contexts>
<marker>Seikel, King, Drumright, editors, 2005</marker>
<rawString>J. Anthony Seikel, Douglas W. King, and David G. Drumright, editors. 2005. Anatomy &amp; Physiology: for Speech, Language, and Hearing. Thomson Delmar Learning, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A Mathematical Theory of Communication.</title>
<date>1949</date>
<publisher>University of Illinois Press,</publisher>
<location>Urbana, IL.</location>
<contexts>
<context position="11613" citStr="Shannon, 1949" startWordPosition="1838" endWordPosition="1839">l logarithmic unit, e (Pz� 1.44 bits). 3.1 The noisy channel The noisy-channel theorem states that information passed through a channel with capacity C at a rate R &lt; C can be reliably recovered with an arbitrarily low probability of error given an appropriate coding. Here, a message from a finite alphabet is encoded, producing signal x E X. That signal is then distorted by a medium which transmits signal y E Y according to some distribution P(Y |X). Given that there is some probability that the received signal, y, is corrupted, the message produced by the decoder may differ from the original (Shannon, 1949). To what extent can we describe the effects of dysarthria within an information-theoretic noisy channel model? We pursue two competing hypotheses within this general framework. The first hypothesis models the assumption that dysarthric speech is a distorted version of typical speech. Here, signal X and Y represent the vocal characteristics of the general and dysarthric populations, respectively, and P(Y |X) models the distortion between them. The second hypothesis models the assumption that both dysarthric and typical speech are distorted versions of some common abstraction. Here, Yd and Yc r</context>
</contexts>
<marker>Shannon, 1949</marker>
<rawString>Claude E. Shannon. 1949. A Mathematical Theory of Communication. University of Illinois Press, Urbana, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoki Toda</author>
<author>Alan W Black</author>
<author>Keiichi Tokuda</author>
</authors>
<title>Statistical mapping between articulatory movements and acoustic spectrum using a Gaussian mixture model.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>3</issue>
<contexts>
<context position="26985" citStr="Toda et al., 2008" startWordPosition="4294" endWordPosition="4297">r dysarthric speakers. We have considered the amount of statistical disorder (i.e., entropy) in both acoustic and articulatory data in dysarthric and non-dysarthric speakers. The use of articulatory knowledge reduces the degree of this disorder significantly for dysarthric speakers (18.3%, relatively), though far less than for non-dysarthric speakers (86.2%, relatively). In realworld applications we are not likely to have access to measurements of the vocal tract; however, many approaches exist that estimate the configuration of the vocal tract given only acoustic data (Richmond et al., 2003; Toda et al., 2008), often to an average error of less than 1 mm. The generalizability of such work to new speakers (particularly those with dysarthria) without training is an open research question. We have argued for noisy channel models of the neuro-motor interface assuming that the pathway of motor command to motor activity is a linear sequence of dynamics. The biological reality is much more complicated. In particular, the pathway of verbal motor commands includes several sources of sensory feedback (Seikel et al., 2005) that modulate control parameters during speech (Gracco, 1995). These senses include ext</context>
</contexts>
<marker>Toda, Black, Tokuda, 2008</marker>
<rawString>Tomoki Toda, Alan W. Black, and Keiichi Tokuda. 2008. Statistical mapping between articulatory movements and acoustic spectrum using a Gaussian mixture model. Speech Communication, 50(3):215–227, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Wrench</author>
</authors>
<date>1999</date>
<booktitle>The MOCHA-TIMIT articulatory database,</booktitle>
<contexts>
<context position="5074" citStr="Wrench, 1999" startWordPosition="736" endWordPosition="737">uses electromagnetic articulography (EMA), in which the speaker is placed within a cube that produces a low-amplitude electromagnetic field, as shown in figure 1. Tiny sensors within this field allow the inference of articulator positions and velocities to within 1 mm of error (Yunusova et al., 2009). Figure 1: Electromagnetic articulograph system. We have so far recorded one male speaker with ALS, five male speakers with CP, four female speakers with CP, and age- and gender-matched controls. Measurement coils are placed as in other studies (e.g., the University of Edinburgh’s MOCHA database (Wrench, 1999) and the University of Wisconsin-Madison’s x-ray microbeam database (Yunusova et al., 2008)). Specifically, we are interested in the positions of the upper and lower lip (UL and LL), left and right mouth corners (LM and RM), lower incisor (LI), and tongue tip, blade, and dorsum (TT, TB, and TD). Unfortunately, a few of our male CP subjects had a severe gag reflex, and we found it impossible to place more than one coil on the tongue for these few individuals. Therefore, of the tongue positions, only TT is used in this study. All articulatory data are smoothed with third-order median filtering i</context>
</contexts>
<marker>Wrench, 1999</marker>
<rawString>Alan Wrench. 1999. The MOCHA-TIMIT articulatory database, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yana Yunusova</author>
<author>Gary Weismer</author>
<author>John R Westbury</author>
<author>Mary J Lindstrom</author>
</authors>
<title>Articulatory movements during vowels in speakers with dysarthria and healthy controls.</title>
<date>2008</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>51--596</pages>
<contexts>
<context position="5165" citStr="Yunusova et al., 2008" startWordPosition="747" endWordPosition="750"> cube that produces a low-amplitude electromagnetic field, as shown in figure 1. Tiny sensors within this field allow the inference of articulator positions and velocities to within 1 mm of error (Yunusova et al., 2009). Figure 1: Electromagnetic articulograph system. We have so far recorded one male speaker with ALS, five male speakers with CP, four female speakers with CP, and age- and gender-matched controls. Measurement coils are placed as in other studies (e.g., the University of Edinburgh’s MOCHA database (Wrench, 1999) and the University of Wisconsin-Madison’s x-ray microbeam database (Yunusova et al., 2008)). Specifically, we are interested in the positions of the upper and lower lip (UL and LL), left and right mouth corners (LM and RM), lower incisor (LI), and tongue tip, blade, and dorsum (TT, TB, and TD). Unfortunately, a few of our male CP subjects had a severe gag reflex, and we found it impossible to place more than one coil on the tongue for these few individuals. Therefore, of the tongue positions, only TT is used in this study. All articulatory data are smoothed with third-order median filtering in order to minimize measurement ‘jitter’. Figure 2 shows the degree of lip aperture (i.e., </context>
</contexts>
<marker>Yunusova, Weismer, Westbury, Lindstrom, 2008</marker>
<rawString>Yana Yunusova, Gary Weismer, John R. Westbury, and Mary J. Lindstrom. 2008. Articulatory movements during vowels in speakers with dysarthria and healthy controls. Journal of Speech, Language, and Hearing Research, 51:596–611, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yana Yunusova</author>
<author>Jordan R Green</author>
<author>Antje Mefferd</author>
</authors>
<title>Accuracy Assessment for AG500, Electromagnetic Articulograph.</title>
<date>2009</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>52--547</pages>
<contexts>
<context position="4762" citStr="Yunusova et al., 2009" startWordPosition="687" endWordPosition="690"> dynamics of dysarthric speech are intrinsically responsible for complex acoustic irregularities, we are collecting a database of dysarthric articulation. Time-aligned movement and acoustic data are measured using two systems. The first infers 3D positions of surface facial markers given stereo video images. The second uses electromagnetic articulography (EMA), in which the speaker is placed within a cube that produces a low-amplitude electromagnetic field, as shown in figure 1. Tiny sensors within this field allow the inference of articulator positions and velocities to within 1 mm of error (Yunusova et al., 2009). Figure 1: Electromagnetic articulograph system. We have so far recorded one male speaker with ALS, five male speakers with CP, four female speakers with CP, and age- and gender-matched controls. Measurement coils are placed as in other studies (e.g., the University of Edinburgh’s MOCHA database (Wrench, 1999) and the University of Wisconsin-Madison’s x-ray microbeam database (Yunusova et al., 2008)). Specifically, we are interested in the positions of the upper and lower lip (UL and LL), left and right mouth corners (LM and RM), lower incisor (LI), and tongue tip, blade, and dorsum (TT, TB, </context>
</contexts>
<marker>Yunusova, Green, Mefferd, 2009</marker>
<rawString>Yana Yunusova, Jordan R. Green, and Antje Mefferd. 2009. Accuracy Assessment for AG500, Electromagnetic Articulograph. Journal of Speech, Language, and Hearing Research, 52:547–555, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>James Glass</author>
</authors>
<title>Speech Database Development: TIMIT and Beyond.</title>
<date>1989</date>
<booktitle>In Proceedings of ESCA Tutorial and Research Workshop on Speech Input/Output Assessment and Speech Databases (SIOA-1989),</booktitle>
<volume>2</volume>
<pages>35--40</pages>
<location>Noordwijkerhout, The Netherlands.</location>
<contexts>
<context position="6267" citStr="Zue et al., 1989" startWordPosition="944" endWordPosition="947">order median filtering in order to minimize measurement ‘jitter’. Figure 2 shows the degree of lip aperture (i.e., the distance between UL and LL) over time for a control and a dysarthric speaker repeating the sequence /ah p iy/. Here, the dysarthric speech is notably slower and has more excessive movement. 5 Control Dysarthric 4 3 2 10 1 2 3 4 5 6 Time (s) Figure 2: Lip aperture over time for four iterations of /ah p iy/ given a dysarthric and control speaker. Our dysarthric speech data include random repetitions of phonetically balanced short sentences originally used in the TIMIT database (Zue et al., 1989), as well as pairs of monosyllabic words identified by Kent et al. (1989) as having relevant articulatory contrasts (e.g., beat versus meat as a stopnasal contrast). All articulatory data are aligned with associated acoustic data, which are transformed to Mel-frequency cepstral coefficients (MFCCs). Phoneme boundaries and pronunciation errors are being transcribed by a speech-language pathologist to the TIMIT phoneset. Table 1 shows pronunciation errors according to manner of articulation for dysarthric speech. Plosives are mispronounced most often, with substitution errors exclusively caused </context>
</contexts>
<marker>Zue, Seneff, Glass, 1989</marker>
<rawString>Victor Zue, Stephanie Seneff, and James Glass. 1989. Speech Database Development: TIMIT and Beyond. In Proceedings of ESCA Tutorial and Research Workshop on Speech Input/Output Assessment and Speech Databases (SIOA-1989), volume 2, pages 35–40, Noordwijkerhout, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>