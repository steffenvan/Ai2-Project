<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026814">
<title confidence="0.968971">
Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition
</title>
<author confidence="0.996424">
Teemu Hirsim¨aki and Mikko Kurimo
</author>
<affiliation confidence="0.998952">
Adaptive Informatics Research Centre
Helsinki University of Technology
</affiliation>
<address confidence="0.977631">
P.O. Box 5400, 02015, TKK, Finland
</address>
<email confidence="0.998807">
teemu.hirsimaki@tkk.fi
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854692307692">
We analyze the recognition errors made by
a morph-based continuous speech recognition
system, which practically allows an unlim-
ited vocabulary. Examining the role of the
acoustic and language models in erroneous
regions shows how speaker adaptive training
(SAT) and discriminative training with mini-
mum phone frame error (MPFE) criterion de-
crease errors in different error classes. An-
alyzing the errors with respect to word fre-
quencies and manually classified error types
reveals the most potential areas for improving
the system.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994828125">
Large vocabulary speech recognizers have become
very complex. Understanding how the parts of the
system affect the results separately or together is far
from trivial. Still, analyzing the recognition errors
may suggest how to reduce the errors further.
There exist previous work on analyzing recogni-
tion errors. Chase (1997) developed error region
analysis (ERA), which reveals whether the errors
are due to acoustic or language models. Greenberg
et al. (2000) analyzed errors made by eight recog-
nition systems on the Switchboard corpus. The er-
rors correlated with the phone misclassification and
speech rate, and conclusion was that the acoustic
front ends should be improved further. Duta et al.
(2006) analyzed the main errors made by the 2004
BBN speech recognition system. They showed that
errors typically occur in clusters and differ between
broadcast news (BN) and conversational telephone
speech (CTS) domains. Named entities were a com-
mon cause for errors in the BN domain, and hesita-
tion, repeats and partially spoken words in the CTS
domain.
This paper analyzes the errors made by a Finnish
morph-based continuous recognition system (Hir-
sim¨aki et al., 2009). In addition to partitioning the
errors using ERA, we compare the number of let-
ter errors in different regions and analyze what kind
of errors are corrected when speaker adaptive train-
ing and discriminative training are taken in use. The
most potential error sources are also studied by par-
titioning the errors according to manual error classes
and word frequencies.
</bodyText>
<sectionHeader confidence="0.916265" genericHeader="introduction">
2 Data and Recognition System
</sectionHeader>
<bodyText confidence="0.999969444444444">
The language model training data used in the experi-
ments consist of 150 million words from the Finnish
Kielipankki corpus. Before training the n-gram
models, the words of the training data were split
into morphs using the Morfessor algorithm, which
has been shown to improve Finnish speech recogni-
tion (Hirsim¨aki et al., 2006). The resulting morph
lexicon contains 50 000 distinct morphs. A growing
algorithm (Siivola et al., 2007) was used for training
a Kneser-Ney smoothed high-order variable-length
n-gram model containing 52 million n-grams.
The acoustic phoneme models were trained on the
Finnish SpeechDat telephone speech database: 39
hours from 3838 speakers for training, 46 minutes
from 79 speakers for development and another simi-
lar set for evaluation. Only full sentences were used
and sentences with severe noise or mispronuncia-
tions were removed.
</bodyText>
<page confidence="0.990312">
193
</page>
<note confidence="0.471067">
Proceedings of NAACL HLT 2009: Short Papers, pages 193–196,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<table confidence="0.954277">
AM score −423 −10.8 −136 −133 −11.1 −242 −36.5
LM score −127 −6.62 −39.7 −12.9 −1.55 −203 −18.7
AM: −386.1 LM: −217.45 TOT: −603.55
</table>
<figureCaption confidence="0.98267775">
Figure 1: An example of a HYP-AM error region. The
scores are log probabilities. Word boundaries are denoted
by ’#’. The error region only contains one letter error (an
inserted ’n’).
</figureCaption>
<table confidence="0.99956">
Letter errors
Region ML ML+SAT ML+SAT+MPFE
HYP-BOTH 962 909 783
HYP-AM 1059 709 727
HYP-LM 623 597 425
REF-TOT 82 60 15
Total 2726 2275 1950
LER (%) 6.8 5.6 4.8
</table>
<tableCaption confidence="0.994735">
Table 1: SpeechDat: Letter errors for different training
methods and error regions. The reference transcript con-
tains 40355 letters in total.
</tableCaption>
<table confidence="0.78088775">
AM score −423 −10.8 −136
LM score −127 −6.62 −39.7
AM: −398.3 LM: −214.01 TOT: −612.31
−114 −15.3 −269 −36.5
</table>
<figure confidence="0.976556125">
−33.0 −0.01 −181 −18.7
tiedon
# valta
tien
#
mullista
a
#
tiedon
valta
a
#
mullista
tie
Ref.
Hyp.
</figure>
<bodyText confidence="0.999149555555556">
The acoustic front-end consist of 39-dimensional
feature vectors (Mel-frequency cepstral coefficients
with first and second time-derivatives), global max-
imum likelihood linear transform, decision-tree tied
HMM triphones with Gaussian mixture models, and
cepstral mean subtraction.
Three models are trained: The first one is a max-
imum likelihood (ML) model without any adap-
tation. The second model (ML+SAT) enhances
the ML model with three iterations of speaker
adaptive training (SAT) using constrained maxi-
mum likelihood linear regression (CMLLR) (Gales,
1998). In recognition, unsupervised adaptation
is applied in the second pass. The third model
(ML+SAT+MPFE) adds four iterations of discrim-
inative training with minimum phone frame error
(MPFE) criterion (Zheng and Stolcke, 2005) to the
ML+SAT model.
</bodyText>
<sectionHeader confidence="0.997775" genericHeader="method">
3 Analysis
</sectionHeader>
<subsectionHeader confidence="0.997314">
3.1 Error Region Analysis
</subsectionHeader>
<bodyText confidence="0.999817256410256">
Error Region Analysis (Chase, 1997) can be used
to find out whether the language model (LM), the
acoustic model (AM) or both can be blamed for
an erroneous region in the recognition output. Fig-
ure 1 illustrates the procedure. For each utter-
ance, the final hypothesis is compared to the forced
alignment of the reference transcript and segmented
into correct and error regions. An error region is
a contiguous sequence of morphs that differ from
the corresponding reference morphs with respect to
morph identity, boundary time-stamps, AM score,
LM score, or n-gram history1.
By comparing the AM and LM scores in the hy-
pothesis and reference regions, the regions can be
divided in classes. We denote the recognition hy-
pothesis as HYP, and the reference transcript as REF.
The relevant classes for the analysis are the follow-
ing. REF-TOT: the reference would have better to-
tal score, but it has been erroneously pruned. HYP-
AM: the hypothesis has better score, but only AM
favors HYP over REF. HYP-LM: the hypothesis has
better score, but only LM favors HYP over REF.
HYP-BOTH: both the AM and LM favor HYP.
Since the error regions are independent, the let-
ter error rate2 (LER) can be computed separately for
each region. Table 1 shows the error rates for three
different acoustic models: ML training, ML+SAT,
and ML+SAT+MPFE. We see that SAT decreases all
error types, but the biggest reduction is in the HYP-
AM class. This should be expected. In the ML case,
the Gaussian mixtures contain much variance due to
different unnormalized speakers, and since the test
set contains only unseen speakers, many errors are
expected for some speakers. Adapting the models to
the test set is expected to increase the acoustic score
of the reference transcript, and since in the HYP-AM
regions the LM already prefers REF, corrections be-
cause of SAT are most probable there.
On the other hand, adding MPFE after SAT seems
</bodyText>
<footnote confidence="0.976064285714286">
1A region may be defined as an error region even if the tran-
scription is correct (only the segmentation differs). However,
since we are going to analyze the number of letter errors in the
error regions, the “correct” error regions do not matter.
2The words in Finnish are often long and consist of several
morphs, so the performance is measured in letter errors instead
of word errors to have finer resolution for the results.
</footnote>
<page confidence="0.99315">
194
</page>
<table confidence="0.999881857142857">
Class label Letter errors Class description
Total HYP-BOTH HYP-AM HYP-LM REF-TOT
Foreign 156 89 61 6 Foreign proper name
Inflect 143 74 26 43 Small error in inflection
Poor 131 37 84 10 Poor pronunciation or repair
Noise 124 21 97 6 Error segment contains some noise
Name 81 29 29 23 Finnish proper name
Delete 65 29 9 27 Small word missing
Acronym 53 44 6 3 Acronym
Compound 42 11 8 23 Word boundary missing or inserted
Correct 37 15 19 3 Hypothesis can be considered correct
Rare 27 11 3 13 Reference contains a very rare word
Insert 9 3 6 Small word inserted incorrectly
Other 1082 421 379 277 5 Other error
</table>
<tableCaption confidence="0.999187">
Table 2: Manual error classes and the number of letter errors for the ML+SAT+MPFE system.
</tableCaption>
<bodyText confidence="0.9953718">
to reduce HYP-BOTH and HYP-LM errors, but not
HYP-AM errors. The number of search errors (REF-
TOT) also decreases.
All in all, for all models, there seems to be more
HYP-AM errors than HYP-LM errors. Chase (1997)
lists the following possible reasons for the HYP-
AM regions: noise, speaker pronounces badly, pro-
nunciation model is poor, some phoneme models
not trained to discriminate, or reference is plainly
wrong. The next section studies these issues further.
</bodyText>
<subsectionHeader confidence="0.999436">
3.2 Manual Error Classification
</subsectionHeader>
<bodyText confidence="0.99968282">
Next, the letter errors in the error regions were
manually classified according to the most probable
cause. Table 2 shows the classes, the total number
of letter errors for each class, and the errors divided
to different error region types.
All errors that did not seem to have an obvious
cause are put under the class Other. Some of the er-
rors were a bit surprising, since the quality of the
audio and language seemed perfectly normal, but
still the recognizer got the sentences wrong. On the
other hand, the class also contains regions where the
speech is very fast or the signal level is quite low.
The largest class with a specific cause is Foreign,
which contains about 8 % of all letter errors. Cur-
rently, the morph based recognizer does not have
any foreign pronunciation modeling, so it is natural
that words like Ching, Yem Yung, Villeneuve, Schu-
macher, Direct TV, Thunderbayssa are not recog-
nized correctly, since the mapping between the writ-
ten form and pronunciation does not follow the nor-
mal Finnish convention. In Table 2 we see, that the
acoustic model prefers the incorrect hypothesis in al-
most all cases. A better pronunciation model would
be essential to improve the recognition. However,
integrating exceptions in pronunciation to morph-
based recognition is not completely straightforward.
Another difficulty with foreign names is that they
are often rare words, so they will get low language
model probability anyway.
The errors in the Acronym class are pretty much
similar to foreign names. Since the letter-by-letter
pronunciation is not modelled, the acronyms usually
cause errors.
The next largest class is Inflect, which contains
errors where the root of the word is correctly rec-
ognized, but the inflectional form is slightly wrong
(for example: autolla/autolle, kirjeeksi/kirjeiksi). In
these errors, it is usually the language model that
prefers the erroneous hypothesis.
The most difficult classes to improve are perhaps
Poor and Noise. For bad pronunciations and repairs
it is not even clear what the correct answer should
be. Should it be the word the speaker tried to say,
or the word that was actually said? As expected, the
language model would have preferred the correct hy-
pothesis in most cases, but the acoustic model have
chosen the wrong hypothesis.
The Name and Rare are also difficult classes.
Contrary to the foreign names and acronyms, the
pronunciation model is not a problem.
</bodyText>
<page confidence="0.996298">
195
</page>
<subsectionHeader confidence="0.558268">
Subset of training data vocabulary (x 1000)
</subsectionHeader>
<bodyText confidence="0.8406766">
Figure 2: Frequency analysis of the SAT+MPFE system.
Number of letters in reference (top), number of letter er-
rors (middle), and letter error rate (bottom) partitioned
according to word frequencies. The leftmost bar corre-
sponds to the 1000 most frequent words, the next bar to
the 2000 next frequent words, and so on. The rightmost
bar corresponds to words not present in the training data.
The Compound errors are mainly in HYP-LM re-
gions, which is natural since there is usually lit-
tle acoustic evidence at the word boundary. Fur-
thermore, it is sometimes difficult even for humans
to know if two words are written together or not.
Sometimes the recognizer made a compound word
error because the compound word was often written
incorrectly in the language model training data.
</bodyText>
<subsectionHeader confidence="0.997497">
3.3 Frequency Analysis
</subsectionHeader>
<bodyText confidence="0.999886125">
In order to study the effect of rare words in more de-
tail, the words in the test data were grouped accord-
ing their frequencies in the LM training data: The
first group contained all the words that were among
the 1000 most common words, the next group con-
tained the next 2000 words, then 4000, and so on,
until the final group contained all words not present
in the training data.
Figure 2 shows the number of letters in the ref-
erence (top), number of letter errors (middle), and
letter error rate (bottom) for each group. Quite ex-
pectedly, the error rates (bottom) rise steadily for the
infrequent words and is highest for the new words
that were not seen in the training data. But looking
at the absolute number of letter errors (middle), the
majority occur in the 1000 most frequent words.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999935090909091">
SAT and MPFE training seem to correct different
error regions: SAT helps when the acoustic model
dominates and MPFE elsewhere. The manual error
classification suggests that improving the pronunci-
ation modeling of foreign words and acronyms is a
potential area for improvement. The frequency anal-
ysis shows that a major part of the recognition errors
occur still in the 1000 most common words. One
solution might be to develop methods for detecting
when the problem is in acoustics and to trust the lan-
guage model more in these regions.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9952915">
This work was partly funded from the EC’s FP7
project EMIME (213845).
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975334375">
Lin Chase. 1997. Error-Responsive Feedback Mecha-
nisms for Speech Recognizers. Ph.D. thesis, Robotics
Institute, Carnegie Mellon University.
Nicolae Duta, Richard Schwartz, and John Makhoul.
2006. Analysis of the errors produced by the 2004
BBN speech recognition system in the DARPA EARS
evaluations. IEEE Trans. Audio, Speech Lang. Pro-
cess., 14(5):1745–1753.
M. J. F. Gales. 1998. Maximum likelihood linear trans-
formations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75–98.
Steven Greenberg, Shuangyu Chang, and Joy Hollen-
back. 2000. An introduction to the diagnostic eval-
uation of the Switchboard-corpus automatic speech
recognition systems. In Proc. NIST Speech Transcrip-
tion Workshop.
Teemu Hirsim¨aki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk¨onen. 2006.
Unlimited vocabulary speech recognition with morph
language models applied to Finnish. Computer Speech
and Language, 20(4):515–541.
Teemu Hirsim¨aki, Janne Pylkk¨onen, and Mikko Kurimo.
2009. Importance of high-order n-gram models in
morph-based speech recognition. IEEE Trans. Audio,
Speech Lang. Process., 17(4):724–732.
Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpioja. 2007.
On growing and pruning Kneser-Ney smoothed n-
gram models. IEEE Trans. Audio, Speech Lang. Pro-
cess., 15(5):1617–1624.
Jing Zheng and Andreas Stolcke. 2005. Improved dis-
criminative training using phone lattices. In Proc. In-
terspeech, pages 2125–2128.
</reference>
<figure confidence="0.992476142857143">
Letters in reference
Letter errors
Letter error rate (%)
10000
5000
0
400
200
0
15
10
5
0
0−1 1−3 3−7 7−15 −31 −63 −127 −255 −511 −4116 New
</figure>
<page confidence="0.974137">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818653">
<title confidence="0.999927">Analysing Recognition Errors in Unlimited-Vocabulary Speech Recognition</title>
<author confidence="0.896953">Hirsim¨aki</author>
<affiliation confidence="0.9865425">Adaptive Informatics Research Helsinki University of</affiliation>
<address confidence="0.952614">P.O. Box 5400, 02015, TKK,</address>
<email confidence="0.976091">teemu.hirsimaki@tkk.fi</email>
<abstract confidence="0.999255285714286">We analyze the recognition errors made by a morph-based continuous speech recognition system, which practically allows an unlimited vocabulary. Examining the role of the acoustic and language models in erroneous regions shows how speaker adaptive training (SAT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lin Chase</author>
</authors>
<title>Error-Responsive Feedback Mechanisms for Speech Recognizers.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Robotics Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="1114" citStr="Chase (1997)" startWordPosition="157" endWordPosition="158">AT) and discriminative training with minimum phone frame error (MPFE) criterion decrease errors in different error classes. Analyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system. 1 Introduction Large vocabulary speech recognizers have become very complex. Understanding how the parts of the system affect the results separately or together is far from trivial. Still, analyzing the recognition errors may suggest how to reduce the errors further. There exist previous work on analyzing recognition errors. Chase (1997) developed error region analysis (ERA), which reveals whether the errors are due to acoustic or language models. Greenberg et al. (2000) analyzed errors made by eight recognition systems on the Switchboard corpus. The errors correlated with the phone misclassification and speech rate, and conclusion was that the acoustic front ends should be improved further. Duta et al. (2006) analyzed the main errors made by the 2004 BBN speech recognition system. They showed that errors typically occur in clusters and differ between broadcast news (BN) and conversational telephone speech (CTS) domains. Name</context>
<context position="5070" citStr="Chase, 1997" startWordPosition="781" endWordPosition="782"> subtraction. Three models are trained: The first one is a maximum likelihood (ML) model without any adaptation. The second model (ML+SAT) enhances the ML model with three iterations of speaker adaptive training (SAT) using constrained maximum likelihood linear regression (CMLLR) (Gales, 1998). In recognition, unsupervised adaptation is applied in the second pass. The third model (ML+SAT+MPFE) adds four iterations of discriminative training with minimum phone frame error (MPFE) criterion (Zheng and Stolcke, 2005) to the ML+SAT model. 3 Analysis 3.1 Error Region Analysis Error Region Analysis (Chase, 1997) can be used to find out whether the language model (LM), the acoustic model (AM) or both can be blamed for an erroneous region in the recognition output. Figure 1 illustrates the procedure. For each utterance, the final hypothesis is compared to the forced alignment of the reference transcript and segmented into correct and error regions. An error region is a contiguous sequence of morphs that differ from the corresponding reference morphs with respect to morph identity, boundary time-stamps, AM score, LM score, or n-gram history1. By comparing the AM and LM scores in the hypothesis and refer</context>
<context position="8283" citStr="Chase (1997)" startWordPosition="1342" endWordPosition="1343">ame Delete 65 29 9 27 Small word missing Acronym 53 44 6 3 Acronym Compound 42 11 8 23 Word boundary missing or inserted Correct 37 15 19 3 Hypothesis can be considered correct Rare 27 11 3 13 Reference contains a very rare word Insert 9 3 6 Small word inserted incorrectly Other 1082 421 379 277 5 Other error Table 2: Manual error classes and the number of letter errors for the ML+SAT+MPFE system. to reduce HYP-BOTH and HYP-LM errors, but not HYP-AM errors. The number of search errors (REFTOT) also decreases. All in all, for all models, there seems to be more HYP-AM errors than HYP-LM errors. Chase (1997) lists the following possible reasons for the HYPAM regions: noise, speaker pronounces badly, pronunciation model is poor, some phoneme models not trained to discriminate, or reference is plainly wrong. The next section studies these issues further. 3.2 Manual Error Classification Next, the letter errors in the error regions were manually classified according to the most probable cause. Table 2 shows the classes, the total number of letter errors for each class, and the errors divided to different error region types. All errors that did not seem to have an obvious cause are put under the class</context>
</contexts>
<marker>Chase, 1997</marker>
<rawString>Lin Chase. 1997. Error-Responsive Feedback Mechanisms for Speech Recognizers. Ph.D. thesis, Robotics Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolae Duta</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Analysis of the errors produced by the 2004 BBN speech recognition system in the DARPA EARS evaluations.</title>
<date>2006</date>
<journal>IEEE Trans. Audio, Speech Lang. Process.,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1494" citStr="Duta et al. (2006)" startWordPosition="215" endWordPosition="218">rts of the system affect the results separately or together is far from trivial. Still, analyzing the recognition errors may suggest how to reduce the errors further. There exist previous work on analyzing recognition errors. Chase (1997) developed error region analysis (ERA), which reveals whether the errors are due to acoustic or language models. Greenberg et al. (2000) analyzed errors made by eight recognition systems on the Switchboard corpus. The errors correlated with the phone misclassification and speech rate, and conclusion was that the acoustic front ends should be improved further. Duta et al. (2006) analyzed the main errors made by the 2004 BBN speech recognition system. They showed that errors typically occur in clusters and differ between broadcast news (BN) and conversational telephone speech (CTS) domains. Named entities were a common cause for errors in the BN domain, and hesitation, repeats and partially spoken words in the CTS domain. This paper analyzes the errors made by a Finnish morph-based continuous recognition system (Hirsim¨aki et al., 2009). In addition to partitioning the errors using ERA, we compare the number of letter errors in different regions and analyze what kind </context>
</contexts>
<marker>Duta, Schwartz, Makhoul, 2006</marker>
<rawString>Nicolae Duta, Richard Schwartz, and John Makhoul. 2006. Analysis of the errors produced by the 2004 BBN speech recognition system in the DARPA EARS evaluations. IEEE Trans. Audio, Speech Lang. Process., 14(5):1745–1753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J F Gales</author>
</authors>
<title>Maximum likelihood linear transformations for HMM-based speech recognition.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="4752" citStr="Gales, 1998" startWordPosition="734" endWordPosition="735">llista a # tiedon valta a # mullista tie Ref. Hyp. The acoustic front-end consist of 39-dimensional feature vectors (Mel-frequency cepstral coefficients with first and second time-derivatives), global maximum likelihood linear transform, decision-tree tied HMM triphones with Gaussian mixture models, and cepstral mean subtraction. Three models are trained: The first one is a maximum likelihood (ML) model without any adaptation. The second model (ML+SAT) enhances the ML model with three iterations of speaker adaptive training (SAT) using constrained maximum likelihood linear regression (CMLLR) (Gales, 1998). In recognition, unsupervised adaptation is applied in the second pass. The third model (ML+SAT+MPFE) adds four iterations of discriminative training with minimum phone frame error (MPFE) criterion (Zheng and Stolcke, 2005) to the ML+SAT model. 3 Analysis 3.1 Error Region Analysis Error Region Analysis (Chase, 1997) can be used to find out whether the language model (LM), the acoustic model (AM) or both can be blamed for an erroneous region in the recognition output. Figure 1 illustrates the procedure. For each utterance, the final hypothesis is compared to the forced alignment of the referen</context>
</contexts>
<marker>Gales, 1998</marker>
<rawString>M. J. F. Gales. 1998. Maximum likelihood linear transformations for HMM-based speech recognition. Computer Speech and Language, 12(2):75–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Greenberg</author>
<author>Shuangyu Chang</author>
<author>Joy Hollenback</author>
</authors>
<title>An introduction to the diagnostic evaluation of the Switchboard-corpus automatic speech recognition systems.</title>
<date>2000</date>
<booktitle>In Proc. NIST Speech Transcription Workshop.</booktitle>
<contexts>
<context position="1250" citStr="Greenberg et al. (2000)" startWordPosition="176" endWordPosition="179">nalyzing the errors with respect to word frequencies and manually classified error types reveals the most potential areas for improving the system. 1 Introduction Large vocabulary speech recognizers have become very complex. Understanding how the parts of the system affect the results separately or together is far from trivial. Still, analyzing the recognition errors may suggest how to reduce the errors further. There exist previous work on analyzing recognition errors. Chase (1997) developed error region analysis (ERA), which reveals whether the errors are due to acoustic or language models. Greenberg et al. (2000) analyzed errors made by eight recognition systems on the Switchboard corpus. The errors correlated with the phone misclassification and speech rate, and conclusion was that the acoustic front ends should be improved further. Duta et al. (2006) analyzed the main errors made by the 2004 BBN speech recognition system. They showed that errors typically occur in clusters and differ between broadcast news (BN) and conversational telephone speech (CTS) domains. Named entities were a common cause for errors in the BN domain, and hesitation, repeats and partially spoken words in the CTS domain. This p</context>
</contexts>
<marker>Greenberg, Chang, Hollenback, 2000</marker>
<rawString>Steven Greenberg, Shuangyu Chang, and Joy Hollenback. 2000. An introduction to the diagnostic evaluation of the Switchboard-corpus automatic speech recognition systems. In Proc. NIST Speech Transcription Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsim¨aki</author>
<author>Mathias Creutz</author>
<author>Vesa Siivola</author>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Janne Pylkk¨onen</author>
</authors>
<title>Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language,</title>
<date>2006</date>
<marker>Hirsim¨aki, Creutz, Siivola, Kurimo, Virpioja, Pylkk¨onen, 2006</marker>
<rawString>Teemu Hirsim¨aki, Mathias Creutz, Vesa Siivola, Mikko Kurimo, Sami Virpioja, and Janne Pylkk¨onen. 2006. Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language, 20(4):515–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsim¨aki</author>
<author>Janne Pylkk¨onen</author>
<author>Mikko Kurimo</author>
</authors>
<title>Importance of high-order n-gram models in morph-based speech recognition.</title>
<date>2009</date>
<journal>IEEE Trans. Audio, Speech Lang. Process.,</journal>
<volume>17</volume>
<issue>4</issue>
<marker>Hirsim¨aki, Pylkk¨onen, Kurimo, 2009</marker>
<rawString>Teemu Hirsim¨aki, Janne Pylkk¨onen, and Mikko Kurimo. 2009. Importance of high-order n-gram models in morph-based speech recognition. IEEE Trans. Audio, Speech Lang. Process., 17(4):724–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Teemu Hirsim¨aki</author>
<author>Sami Virpioja</author>
</authors>
<title>On growing and pruning Kneser-Ney smoothed ngram models.</title>
<date>2007</date>
<journal>IEEE Trans. Audio, Speech Lang. Process.,</journal>
<volume>15</volume>
<issue>5</issue>
<marker>Siivola, Hirsim¨aki, Virpioja, 2007</marker>
<rawString>Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpioja. 2007. On growing and pruning Kneser-Ney smoothed ngram models. IEEE Trans. Audio, Speech Lang. Process., 15(5):1617–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Zheng</author>
<author>Andreas Stolcke</author>
</authors>
<title>Improved discriminative training using phone lattices.</title>
<date>2005</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2125--2128</pages>
<contexts>
<context position="4976" citStr="Zheng and Stolcke, 2005" startWordPosition="764" endWordPosition="767">elihood linear transform, decision-tree tied HMM triphones with Gaussian mixture models, and cepstral mean subtraction. Three models are trained: The first one is a maximum likelihood (ML) model without any adaptation. The second model (ML+SAT) enhances the ML model with three iterations of speaker adaptive training (SAT) using constrained maximum likelihood linear regression (CMLLR) (Gales, 1998). In recognition, unsupervised adaptation is applied in the second pass. The third model (ML+SAT+MPFE) adds four iterations of discriminative training with minimum phone frame error (MPFE) criterion (Zheng and Stolcke, 2005) to the ML+SAT model. 3 Analysis 3.1 Error Region Analysis Error Region Analysis (Chase, 1997) can be used to find out whether the language model (LM), the acoustic model (AM) or both can be blamed for an erroneous region in the recognition output. Figure 1 illustrates the procedure. For each utterance, the final hypothesis is compared to the forced alignment of the reference transcript and segmented into correct and error regions. An error region is a contiguous sequence of morphs that differ from the corresponding reference morphs with respect to morph identity, boundary time-stamps, AM scor</context>
</contexts>
<marker>Zheng, Stolcke, 2005</marker>
<rawString>Jing Zheng and Andreas Stolcke. 2005. Improved discriminative training using phone lattices. In Proc. Interspeech, pages 2125–2128.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>