<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019777">
<title confidence="0.985866">
The Role of Information Retrieval in Answering Complex Questions
</title>
<author confidence="0.998523">
Jimmy Lin
</author>
<affiliation confidence="0.9985895">
College of Information Studies
Department of Computer Science
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<address confidence="0.937941">
College Park, MD 20742, USA
</address>
<email confidence="0.999535">
jimmylin@umd.edu
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999932">
This paper explores the role of informa-
tion retrieval in answering “relationship”
questions, a new class complex informa-
tion needs formally introduced in TREC
2005. Since information retrieval is of-
ten an integral component of many ques-
tion answering strategies, it is important
to understand the impact of different term-
based techniques. Within a framework of
sentence retrieval, we examine three fac-
tors that contribute to question answer-
ing performance: the use of different re-
trieval engines, relevance (both at the doc-
ument and sentence level), and redun-
dancy. Results point out the limitations
of purely term-based methods to this chal-
lenging task. Nevertheless, IR-based tech-
niques provide a strong baseline on top
of which more sophisticated language pro-
cessing techniques can be deployed.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977075471698">
The field of question answering arose from the
recognition that the document does not occupy a
privileged position in the space of information ob-
jects as the most ideal unit of retrieval. Indeed, for
certain types of information needs, sub-document
segments are preferred—an example is answers to
factoid questions such as “Who won the Nobel
Prize for literature in 1972?” By leveraging so-
phisticated language processing capabilities, fac-
toid question answering systems are able to pin-
point the exact span of text that directly satisfies
an information need.
Nevertheless, IR engines remain integral com-
ponents of question answering systems, primar-
ily as a source of candidate documents that are
subsequently analyzed in greater detail. Al-
though this two-stage architecture was initially
conceived as an expedient to overcome the com-
putational processing bottleneck associated with
more sophisticated but slower language process-
ing technology, it has worked quite well in prac-
tice. The architecture has since evolved into a
widely-accepted paradigm for building working
systems (Hirschman and Gaizauskas, 2001).
Due to the reliance of QA systems on IR tech-
nology, the relationship between them is an im-
portant area of study. For example, how sensi-
tive is answer extraction performance to the ini-
tial quality of the result set? Does better docu-
ment retrieval necessarily translate into more ac-
curate answer extraction? These answers can-
not be solely determined from first principles,
but must be addressed through empirical experi-
ments. Indeed, a number of works have specifi-
cally examined the effects of information retrieval
on question answering (Monz, 2003; Tellex et al.,
2003), including a dedicated workshop at SIGIR
2004 (Gaizauskas et al., 2004). More recently, the
importance of document retrieval has prompted
NIST to introduce a document ranking subtask in-
side the TREC 2005 QA track.
However, the connection between QA and IR
has mostly been explored in the context of factoid
questions such as “Who shot Abraham Lincoln?”,
which represent only a small fraction of all infor-
mation needs. In contrast to factoid questions,
which can be answered by short phrases found
within an individual document, there is a large
class of questions whose answers require synthe-
sis of information from multiple sources. The so-
called definition/other questions at recent TREC
evaluations (Voorhees, 2005) serve as good exam-
ples: “good answers” to these questions include in-
</bodyText>
<page confidence="0.98248">
523
</page>
<note confidence="0.815811666666667">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 523–530,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Qid 25: The analyst is interested in the status of Fidel Castro’s brother. Specifically, the analyst would like
information on his current plans and what role he may play after Fidel Castro’s death.
vital Raul Castro was formally designated his brother’s successor
vital Raul is the head of the Armed Forces
okay Raul is five years younger than Castro
okay Raul has enjoyed a more public role in running Cuba’s Government.
okay Raul is the number two man in the government’s ruling Council of State
</note>
<figureCaption confidence="0.999673">
Figure 1: An example relationship question from TREC 2005 with its answer nuggets.
</figureCaption>
<bodyText confidence="0.999947833333333">
teresting “nuggets” about a particular person, or-
ganization, entity, or event. No single document
can provide a complete answer, and hence systems
must integrate information from multiple sources;
cf. (Amig´o et al., 2004; Dang, 2005).
This work focuses on so-called relationship
questions, which represent a new and underex-
plored area in question answering. Although they
require systems to extract information nuggets
from multiple documents (just like definition/other
questions), relationship questions demand a differ-
ent approach (see Section 2). This paper explores
the role of information retrieval in answering such
questions, focusing primarily on three aspects:
document retrieval performance, term-based mea-
sures of relevance, and term-based approaches to
reducing redundancy. The overall goal is to push
the limits of information retrieval technology and
provide strong baselines against which linguistic
processing capabilities can be compared.
The rest of this paper is organized as follows:
Section 2 provides an overview of relationship
questions. Section 3 describes experiments fo-
cused on document retrieval performance. An ap-
proach to answering relationship questions based
on sentence retrieval is discussed in Section 4. A
simple utility model that incorporates both rele-
vance and redundancy is explored in Section 5.
Before concluding, we discuss the implications of
our experimental results in Section 6.
</bodyText>
<sectionHeader confidence="0.993869" genericHeader="method">
2 Relationship Questions
</sectionHeader>
<bodyText confidence="0.997668636363636">
Relationship questions represent a new class of in-
formation needs formally introduced as a subtask
in the NIST-sponsored TREC QA evaluations in
2005 (Voorhees, 2005). Previously, they were the
focus of a small pilot study within the AQUAINT
program, which resulted in an understanding of a
“relationship” as the ability for one object to in-
fluence another. Objects in these questions can
denote either entities (people, organization, coun-
tries, etc.) or events. Consider the following ex-
amples:
</bodyText>
<listItem confidence="0.9734595">
• Has pressure from China affected America’s
willingness to sell weaponry to Taiwan?
• Do the military personnel exchanges between
Israel and India show an increase in cooper-
ation? If so, what are the driving factors be-
hind this increase?
</listItem>
<bodyText confidence="0.989893366666667">
Evidence for a relationship includes both the
means to influence some entity and the motiva-
tion for doing so. Eight types of relationships
(“spheres of influence”) were noted: financial,
movement of goods, family ties, co-location, com-
mon interest, and temporal connection.
Relationship questions are significantly dif-
ferent from definition questions, which can be
paraphrased as “Tell me interesting things about
x.” Definition questions have received significant
amounts of attention recently, e.g., (Hildebrandt et
al., 2004; Prager et al., 2004; Xu et al., 2004; Cui
et al., 2005). Research has shown that certain cue
phrases serve as strong indicators for nuggets, and
thus an approach based on matching surface pat-
terns (e.g., appositives, parenthetical expressions)
works quite well. Unfortunately, such techniques
do not generalize to relationship questions because
their answers are not usually captured by patterns
or marked by surface cues.
Unlike answers to factoid questions, answers to
relationship questions consist of an unsorted set
of passages. For assessing system output, NIST
employs the nugget-based evaluation methodol-
ogy originally developed for definition questions;
see (Voorhees, 2005) for a detailed description.
Answers consist of units of information called
“nuggets”, which assessors manually create from
system submissions and their own research (see
example in Figure 1). Nuggets are divided into
</bodyText>
<page confidence="0.996616">
524
</page>
<bodyText confidence="0.999991392857143">
two types (“vital” and “okay”), and this distinc-
tion plays an important role in scoring. The offi-
cial metric is an F3-score, where nugget recall is
computed on vital nuggets, and precision is based
on a length allowance derived from the number of
both vital and okay nuggets retrieved.
In the original NIST setup, human assessors
were required to manually determine whether a
particular system’s response contained a nugget.
This posed a problem for researchers who wished
to conduct formative evaluations outside the an-
nual TREC cycle—the necessity of human in-
volvement meant that system responses could
not be rapidly, consistently, and automatically
assessed. However, the recent introduction of
POURPRE, an automatic evaluation metric for the
nugget-based evaluation methodology (Lin and
Demner-Fushman, 2005), fills this evaluation gap
and makes possible the work reported here; cf.
Nuggeteer (Marton and Radul, 2006).
This paper describes experiments with the 25
relationship questions used in the secondary task
of the TREC 2005 QA track (Voorhees, 2005),
which attracted a total of eleven submissions. Sys-
tems used the AQUAINT corpus, a three gigabyte
collection of approximately one million news ar-
ticles from the Associated Press, the New York
Times, and the Xinhua News Agency.
</bodyText>
<sectionHeader confidence="0.977667" genericHeader="method">
3 Document Retrieval
</sectionHeader>
<bodyText confidence="0.999835476190476">
Since information retrieval systems supply the ini-
tial set of documents on which a question answer-
ing system operates, it makes sense to optimize
document retrieval performance in isolation. The
issue of end–to–end system performance will be
taken up in Section 4.
Retrieval performance can be evaluated based
on the assumption that documents which contain
one or more relevant nuggets (either vital or okay)
are themselves relevant. From system submissions
to TREC 2005, we created a set of relevance judg-
ments, which averaged 8.96 relevant documents
per question (median 7, min 1, max 21).
Our first goal was to examine the effect
of different retrieval systems on performance.
Two freely-available IR engines were compared:
Lucene and Indri. The former is an open-source
implementation of what amounts to be a modified
tfidf weighting scheme, while the latter employs
a language modeling approach. In addition, we
experimented with blind relevance feedback, a re-
</bodyText>
<table confidence="0.9902442">
MAP R50
Lucene 0.206 0.469
Lucene+brf 0.190 (− 7.6%)° 0.442 (− 5.6%)°
Indri 0.195 (− 5.2%)° 0.442 (− 5.6%)°
Indri+brf 0.158 (− 23.3%)° 0.377 (− 19.5%)°
</table>
<tableCaption confidence="0.990922">
Table 1: Document retrieval performance, with
</tableCaption>
<bodyText confidence="0.989356704545455">
and without blind relevance feedback.
trieval technique commonly employed to improve
performance (Salton and Buckley, 1990). Fol-
lowing settings in typical IR experiments, the top
twenty terms (by ifidf value) from the top twenty
documents were added to the original query in the
feedback iteration.
For each question, fifty documents from the
AQUAINT collection were retrieved, represent-
ing the number of documents that a typical QA
system might consider. The question itself was
used verbatim as the IR query (see Section 6 for
discussion). Performance is shown in Table 1.
We measured Mean Average Precision (MAP), the
most informative single-point metric for ranked
retrieval, and recall, since it places an upper bound
on the number of relevant documents available for
subsequent downstream processing.
For all experiments reported in this paper, we
applied the Wilcoxon signed-rank test to deter-
mine the statistical significance of the results. This
test is commonly used in information retrieval
research because it makes minimal assumptions
about the underlying distribution of differences.
Significance at the 0.90 level is denoted with a �
or &apos;, depending on the direction of change; at the
0.95 level, ° or v; at the 0.99 level, • or •. Differ-
ences not statistically significant are marked with
°. Although the differences between Lucene and
Indri are not significant, blind relevance feedback
was found to hurt performance, significantly so in
the case of Indri. These results are consistent with
the findings of Monz (2003), who made the same
observation in the factoid QA task.
There are a few caveats to consider when in-
terpreting these results. First, the test set of 25
questions is rather small. Second, the number of
relevant documents per question is also relatively
small, and hence likely to be incomplete. Buck-
ley and Voorhees (2004) have shown that evalua-
tion metrics are not stable with respect to incom-
plete relevance judgments. Third, the distribution
of relevant documents may be biased due to the
small number of submissions, many of which used
</bodyText>
<page confidence="0.990259">
525
</page>
<bodyText confidence="0.9998355">
Lucene. Due to these factors, one should interpret
the results reported here as suggestive, not defini-
tive. Follow-up experiments with larger data sets
are required to produce more conclusive results.
</bodyText>
<equation confidence="0.9948845">
EtESnQ idf (t) = EtESnQ idf (t)
P = EtEA idf (t) , � EtEQ idf(t)
</equation>
<sectionHeader confidence="0.942479" genericHeader="method">
4 Selecting Relevant Sentences
</sectionHeader>
<bodyText confidence="0.99999125">
We adopted an extractive approach to answering
relationship questions that views the task as sen-
tence retrieval, a conception in line with the think-
ing of many researchers today (but see discussion
in Section 6). Although oversimplified, there are
several reasons why this formulation is produc-
tive: since answers consist of unordered text seg-
ments, the task is similar to passage retrieval, a
well-studied problem (Callan, 1994; Tellex et al.,
2003) where sentences form a natural unit of re-
trieval. In addition, the TREC novelty tracks have
specifically tackled the questions of relevance and
redundancy at the sentence level (Harman, 2002).
Empirically, a sentence retrieval approach per-
forms quite well: when definition questions
were first introduced in TREC 2003, a simple
sentence-ranking algorithm outperformed all but
the highest-scoring system (Voorhees, 2003). In
addition, viewing the task of answering relation-
ship questions as sentence retrieval allows one
to leverage work in multi-document summariza-
tion, where extractive approaches have been ex-
tensively studied. This section examines the task
of independently selecting the best sentences for
inclusion in an answer; attempts to reduce redun-
dancy will be discussed in the next section.
There are a number of term-based features as-
sociated with a candidate sentence that may con-
tribute to its relevance. In general, such features
can be divided into two types: properties of the
document containing the sentence and properties
of the sentence itself. Regarding the former type,
two features come into play: the relevance score
of the document (from the IR engine) and its rank
in the result set. For sentence-based features, we
experimented with the following:
</bodyText>
<listItem confidence="0.994418">
• Passage match score, which sums the idf val-
ues of unique terms that appear in both the
candidate sentence (S) and the question (Q):
</listItem>
<equation confidence="0.5407775">
� idf(t)
tESnQ
</equation>
<listItem confidence="0.987499">
• Term idf precision and recall scores; cf. (Katz
et al., 2005):
• Length of the sentence (in non-whitespace
characters).
</listItem>
<bodyText confidence="0.992797536585366">
Note that precision and recall values are
bounded between zero and one, while the passage
match score and the length of the sentence are both
unbounded features.
Our baseline sentence retriever employed the
passage match score to rank all sentences in the
top n retrieved documents. By default, we used
documents retrieved by Lucene, using the ques-
tion verbatim as the query. To generate answers,
the system selected sentences based on their scores
until a hard length quota has been filled (trim-
ming the final sentence if necessary). After ex-
perimenting with different values, we discovered
that a document cutoff of ten yielded the highest
performance in terms of POURPRE scores, i.e., all
but the ten top-ranking documents were discarded.
In addition, we built a linear regression model
that employed the above features to predict the
nugget score of a sentence (the dependent vari-
able). For the training samples, the nugget match-
ing component within POURPRE was employed
to compute the nugget score—this value quanti-
fied the “goodness” of a particular sentence in
terms of nugget content.1 Due to known issues
with the vital/okay distinction (Hildebrandt et al.,
2004), it was ignored for this computation; how-
ever, see (Lin and Demner-Fushman, 2006b) for
recent attempts to address this issue.
When presented with a test question, the sys-
tem ranked all sentences from the top ten retrieved
documents using the regression model. Answers
were generated by filling a quota of characters,
just as in the baseline. Once again, no attempt was
made to reduce redundancy.
We conducted a five-fold cross validation ex-
periment using all sentences from the top 100
Lucene documents as training samples. After ex-
perimenting with different features, we discov-
ered that a regression model with the following
performed best: passage match score, document
score, and sentence length. Surprisingly, adding
</bodyText>
<footnote confidence="0.983682">
1Since the count variant of POURPRE achieved the highest
correlation with official rankings, the nugget score is simply
the highest fraction in terms of word overlap between the sen-
tence and any of the reference nuggets.
</footnote>
<page confidence="0.972656">
526
</page>
<table confidence="0.999740230769231">
Length 1000 2000 3000 4000 5000
F-Score 0.268 0.255 0.234 0.225
baseline 0.275
regression 0.294 (+7.0%)° 0.268 (+0.0%)° 0.257 (+1.0%)° 0.240 (+2.5%)° 0.228 (+1.6%)°
Recall 0.308 0.333 0.336 0.352
baseline 0.282
regression 0.302 (+7.2%)° 0.308 (+0.0%)° 0.336 (+0.8%)° 0.343 (+2.3%)° 0.358 (+1.7%)°
F-Score (all-vital) 0.672 0.632 0.592 0.558
baseline 0.699
regression 0.722 (+3.3%)° 0.672 (+0.0%)° 0.632 (+0.0%)° 0.593 (+0.2%)° 0.554 (−0.7%)°
Recall (all-vital) 0.774 0.816 0.834 0.856
baseline 0.723
regression 0.747 (+3.3%)° 0.774 (+0.0%)° 0.814 (−0.2%)° 0.834 (+0.0%)° 0.848 (−0.8%)°
</table>
<tableCaption confidence="0.987895">
Table 2: Question answering performance at different answer length cutoffs, as measured by POURPRE.
</tableCaption>
<table confidence="0.999833818181818">
Length 1000 2000 3000 4000 5000
F-Score
Lucene 0.275 0.268 0.255 0.234 0.225
Lucene+brf 0.278 (+1.3 %)° 0.268 (+0.0 %)° 0.251 (− 1.6%)° 0.231 (− 1.2 %)° 0.215 (− 4.3%)°
Indri 0.264 (−4.1 %)° 0.260 (−2.7 %)° 0.241 (− 5.4%)° 0.222 (− 5.0 %)° 0.212 (− 5.8%)°
Indri+brf 0.270 (−1.8 %)° 0.257 (−3.8 %)° 0.235 (− 7.8%)° 0.221 (− 5.7 %)° 0.206 (− 8.2%)°
Recall
Lucene 0.282 0.308 0.333 0.336 0.352
Lucene+brf 0.285 (+1.3 %)° 0.308 (+0.0 %)° 0.319 (− 4.2%)° 0.322 (− 4.2 %)° 0.324 (− 7.9%)°
Indri 0.270 (−4.1 %)° 0.300 (−2.5 %)° 0.306 (− 8.2%)° 0.308 (− 8.1 %)° 0.320 (− 9.2%)°
Indri+brf 0.276 (−2.0 %)° 0.296 (−3.6 %)° 0.299 (− 10.4%)° 0.307 (− 8.5 %)° 0.312 (− 11.3%)°
</table>
<tableCaption confidence="0.999937">
Table 3: The effect of using different document retrieval systems on answer quality.
</tableCaption>
<bodyText confidence="0.999913981481482">
the term match precision and recall features to the
regression model decreased overall performance
slightly. We believe that precision and recall en-
codes information already captured by the other
features.
Results of our experiments are shown in Ta-
ble 2 for different answer lengths. Following
the TREC QA track convention, all lengths are
measured in non-whitespace characters. Both the
baseline and regression conditions employed the
top ten documents supplied by Lucene. In addi-
tion to the F3-score, we report the recall compo-
nent only (on vital nuggets). For this and all sub-
sequent experiments, we used the (count, macro)
variant of POURPRE, which was validated as pro-
ducing the highest correlation with official rank-
ings. The regression model yields higher scores
at shorter lengths, although none of these differ-
ences were significant. In general, performance
decreases with longer answers because both vari-
ants tend to rank relevant sentences before non-
relevant ones.
Our results compare favorably to runs submit-
ted to the TREC 2005 relationship task. In that
evaluation, the best performing automatic run ob-
tained a POURPRE score of 0.243, with an average
answer length of 4051 character per question.
Since the vital/okay nugget distinction was ig-
nored when training our regression model, we also
evaluated system output under the assumption that
all nuggets were vital. These scores are also shown
in Table 2. Once again, results show higher POUR-
PRE scores for shorter answers, but these differ-
ences are not statistically significant. Why might
this be so? It appears that features based on term
statistics alone are insufficient to capture nugget
relevance. We verified this hypothesis by building
a regression model for all 25 questions: the model
exhibited an R2 value of only 0.207.
How does IR performance affect the final sys-
tem output? To find out, we applied the base-
line sentence retrieval algorithm (which uses the
passage match score only) on the output of differ-
ent document retrieval variants. These results are
shown in Table 3 for the four conditions discussed
in the previous section: Lucene and Indri, with and
without blind relevance feedback.
Just as with the document retrieval results,
Lucene alone (without blind relevance feedback)
yielded the highest POURPRE scores. However,
none of the differences observed were statistically
significant. These numbers point to an interesting
interaction between document retrieval and ques-
tion answering. The decreases in performance at-
</bodyText>
<page confidence="0.993949">
527
</page>
<table confidence="0.999621181818182">
Length 1000 2000 3000 4000 5000
F-Score 0.275 0.268 0.255 0.234 0.225
baseline
baseline+max 0.311 (+13.2%)∧ 0.302 (+12.8%)&apos; 0.281 (+10.5%)&apos; 0.256 (+9.5%)° 0.235 (+4.6%)◦
baseline+avg 0.301 (+9.6%)◦ 0.294 (+9.8%)∧ 0.271 (+6.5%)∧ 0.256 (+9.5%)° 0.237 (+5.6%)◦
regression+max 0.275 (+0.3%)◦ 0.303 (+13.3%)∧ 0.275 (+8.1%)◦ 0.258 (+10.4%)◦ 0.244 (+8.4%)◦
Recall 0.282 0.308 0.333 0.336 0.352
baseline
baseline+max 0.324 (+15.1%)∧ 0.355 (+15.4%)° 0.369 (+10.6%)° 0.369 (+9.8%)° 0.369 (+4.7%)◦
baseline+avg 0.314 (+11.4%)◦ 0.346 (+12.3%)∧ 0.354 (+6.2%)∧ 0.369 (+9.8%)° 0.371 (+5.5%)◦
regression+max 0.287 (+2.0%)◦ 0.357 (+16.1%)∧ 0.360 (+8.0%)◦ 0.371 (+10.4%)∧ 0.379 (+7.6%)◦
</table>
<tableCaption confidence="0.999888">
Table 4: Evaluation of different utility settings.
</tableCaption>
<bodyText confidence="0.998992625">
tributed to blind relevance feedback in end–to–end
QA were in general less than the drops observed
in the document retrieval runs. It appears possi-
ble that the sentence retrieval algorithm was able
to recover from a lower-quality result set, i.e., one
with relevant documents ranked lower. Neverthe-
less, just as with factoid QA, the coupling between
IR and answer extraction merits further study.
</bodyText>
<sectionHeader confidence="0.847007" genericHeader="method">
5 Reducing Redundancy
</sectionHeader>
<bodyText confidence="0.999940466666667">
The methods described in the previous section
for choosing relevant sentences do not take into
account information that may be conveyed more
than once. Drawing inspiration from research in
sentence-level redundancy within the context of
the TREC novelty track (Allan et al., 2003) and
work in multi-document summarization, we ex-
perimented with term-based approaches to reduc-
ing redundancy.
Instead of selecting sentences for inclusion in
the answer based on relevance alone, we imple-
mented a simple utility model, which takes into
account sentences that have already been added to
the answer A. For each candidate c, utility is de-
fined as follows:
</bodyText>
<equation confidence="0.8790255">
Utility(c) = Relevance(c) − A max
SEA
</equation>
<bodyText confidence="0.999825156862745">
This model is the baseline variant of the Maxi-
mal Marginal Relevance method for summariza-
tion (Goldstein et al., 2000). Each candidate is
compared to all sentences that have already been
selected for inclusion in the answer. The maxi-
mum of these pairwise similarity comparisons is
deducted from the relevance score of the sentence,
subjected to A, a parameter that we tune. For our
experiments, we used cosine distance as the simi-
larity function. All relevance scores were normal-
ized to a range between zero and one.
At each step in the answer generation process,
utility values are computed for all candidate sen-
tences. The one with the highest score is selected
for inclusion in the final answer. Utility values are
then recomputed, and the process iterates until the
length quota has been filled.
We experimented with two different sources
for the relevance scores: the baseline sentence re-
triever (passage match score only) and the regres-
sion model. In addition to taking the max of all
pairwise similarity values, as in the above formula,
we also experimented with the average.
Results of our runs are shown in Table 4. We
report values for the baseline relevance score with
the max and avg aggregation functions, as well as
the regression relevance scores with max. These
experimental conditions were compared against
the baseline run that used the relevance score only
(no redundancy penalty). To compute the optimal
A, we swept across the parameter space from zero
to one in increments of a tenth. We determined the
optimal value of A by averaging POURPRE scores
across all length intervals. For all three conditions,
we discovered 0.4 to be the optimal value.
These experiments suggest that a simple term-
based approach to reducing redundancy yields sta-
tistically significant gains in performance. This
result is not surprising since similar techniques
have proven effective in multi-document summa-
rization. Empirically, we found that the max op-
erator outperforms the avg operator in quantify-
ing the degree of redundancy. The observation
that performance improvements are more notice-
able at shorter answer lengths confirms our intu-
itions. Redundancy is better tolerated in longer
answers because a redundant nugget is less likely
to “squeeze out” a relevant, novel nugget.
While it is productive to model the relationship
task as sentence retrieval where independent de-
cisions are made about sentence-level relevance,
</bodyText>
<equation confidence="0.585706">
sim(s, c)
</equation>
<page confidence="0.981556">
528
</page>
<bodyText confidence="0.99994425">
this simplification fails to capture overlap in infor-
mation content, and leads to redundant answers.
We found that a simple term-based approach was
effective in tackling this issue.
</bodyText>
<sectionHeader confidence="0.997494" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999982718750001">
Although this work represents the first formal
study of relationship questions that we are aware
of, by no means are we claiming a solution—we
see this as merely the first step in addressing a
complex problem. Nevertheless, information re-
trieval techniques lay the groundwork for systems
aimed at answering complex questions. The meth-
ods described here will hopefully serve as a start-
ing point for future work.
Relationship questions represent an important
problem because they exemplify complex infor-
mation needs, generally acknowledged as the fu-
ture of QA research. Other types of complex needs
include analytical questions such as “How close is
Iran to acquiring nuclear weapons?”, which are the
focus of the AQUAINT program in the U.S., and
opinion questions such as “How does the Chilean
government view attempts at having Pinochet tried
in Spanish Court?”, which were explored in a 2005
pilot study also funded by AQUAINT. In 2006,
there will be a dedicated task within the TREC
QA track exploring complex questions within an
interactive setting. Furthermore, we note the con-
vergence of the QA and summarization commu-
nities, as demonstrated by the shift from generic
to query-focused summaries starting with DUC
2005 (Dang, 2005). This development is also
compatible with the conception of “distillation”
in the current DARPA GALE program. All these
trends point to same problem: how do we build
advanced information systems to address complex
information needs?
The value of this work lies in the generality
of IR-based approaches. Sophisticated linguis-
tic processing algorithms are typically unable to
cope with the enormous quantities of text avail-
able. To render analysis more computationally
tractable, researchers commonly employ IR tech-
niques to reduce the amount of text under consid-
eration. We believe that the techniques introduced
in this paper are applicable to the different types
of information needs discussed above.
While information retrieval techniques form a
strong baseline for answering relationship ques-
tions, there are clear limitations of term-based ap-
proaches. Although we certainly did not exper-
iment with every possible method, this work ex-
amined several common IR techniques (e.g., rel-
evance feedback, different term-based features,
etc.). In our regression experiments, we discov-
ered that our feature set was unable to adequately
capture sentence relevance. On the other hand,
simple IR-based techniques appeared to work well
at reducing redundancy, suggesting that determin-
ing content overlap is a simpler problem.
To answer relationship questions well, NLP
technology must take over where IR techniques
leave off. Yet, there are a number of challenges,
the biggest of which is that question classification
and named-entity recognition, which have worked
well for factoid questions, are not applicable to re-
lationship questions, since answer types are diffi-
cult to anticipate. For factoids, there exists a sig-
nificant amount of work on question analysis—the
results of which include important query terms and
the expected answer type (e.g., person, organiza-
tion, etc.). Relationship questions are more diffi-
cult to process: for one, they are often not phrased
as direct wh-questions, but rather as indirect re-
quests for information, statements of doubt, etc.
Furthermore, since these complex questions can-
not be answered by short noun phrases, existing
answer type ontologies are not very useful. For our
experiments, we decided to simply use the ques-
tion verbatim as the query to the IR systems, but
undoubtedly performance can be gained by bet-
ter query formulation strategies. These are diffi-
cult challenges, but recent work on applying se-
mantic models to QA (Narayanan and Harabagiu,
2004; Lin and Demner-Fushman, 2006a) provide
a promising direction.
While our formulation of answering relation-
ship questions as sentence retrieval is produc-
tive, it clearly has limitations. The assumption
that information nuggets do not span sentence
boundaries is false and neglects important work in
anaphora resolution and discourse modeling. The
current setup of the task, where answers consist
of unordered strings, does not place any value on
coherence and readability of the responses, which
will be important if the answers are intended for
human consumption. Clearly, there are ample op-
portunities here for NLP techniques to shine.
The other value of this work lies in its use of an
automatic evaluation metric (POURPRE) for sys-
tem development—the first instance in complex
</bodyText>
<page confidence="0.994319">
529
</page>
<bodyText confidence="0.999920333333333">
QA that we are aware of. Prior to the introduc-
tion of this automatic scoring technique, studies
such as this were difficult to conduct due to the
necessity of involving humans in the evaluation
process. POURPRE was developed to enable rapid
exploration of the solution space, and experiments
reported here demonstrate its usefulness in doing
just that. Although automatic evaluation metrics
are no stranger to other fields such as machine
translation (e.g., BLEU) and document summa-
rization (e.g., ROUGE, BE, etc.), this represents a
new development in question answering research.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999378">
Although many findings in this paper are negative,
the conclusions are positive for NLP researchers.
An exploration of a variety of term-based ap-
proaches for answering relationship questions has
demonstrated the impact of different techniques,
but more importantly, this work highlights limita-
tions of purely IR-based methods. With a strong
baseline as a foundation, the door is wide open for
the integration of natural language understanding
techniques.
</bodyText>
<sectionHeader confidence="0.998763" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.997734">
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE). I would like
to thank Esther and Kiri for their loving support.
</bodyText>
<sectionHeader confidence="0.99904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999557044117647">
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval
and novelty detection at the sentence level. In SIGIR
2003.
E. Amig´o, J. Gonzalo, V. Peinado, A. Pe˜nas, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In ACL 2004.
C. Buckley and E. Voorhees. 2004. Retrieval evalua-
tion with incomplete information. In SIGIR 2004.
J. Callan. 1994. Passage-level evidence in document
retrieval. In SIGIR 1994.
H. Cui, M.-Y. Kan, and T.-S. Chua. 2005. Generic soft
pattern models for definitional question answering.
In SIGIR 2005.
H. Dang. 2005. Overview of DUC 2005. In DUC
2005.
R. Gaizauskas, M. Hepple, and M. Greenwood. 2004.
Proceedings of the SIGIR 2004 Workshop on Infor-
mation Retrieval for Question Answering (IR4QA).
J. Goldstein, V. Mittal, J. Carbonell, and J. Callan.
2000. Creating and evaluating multi-document sen-
tence extract summaries. In CIKM 2000.
D. Harman. 2002. Overview of the TREC 2002 nov-
elty track. In TREC 2002.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answer-
ing definition questions with multiple knowledge
sources. In HLT/NAACL 2004.
L. Hirschman and R. Gaizauskas. 2001. Natural
language question answering: The view from here.
Natural Language Engineering, 7(4):275–300.
B. Katz, G. Marton, G. Borchardt, A. Brownell,
S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox. 2005.
External knowledge sources for question answering.
In TREC 2005.
J. Lin and D. Demner-Fushman. 2005. Automati-
cally evaluating answers to definition questions. In
HLT/EMNLP 2005.
J. Lin and D. Demner-Fushman. 2006a. The role of
knowledge in conceptual retrieval: A study in the
domain of clinical medicine. In SIGIR 2006.
J. Lin and D. Demner-Fushman. 2006b. Will pyramids
built of nuggets topple over? In HLT/NAACL 2006.
G. Marton and A. Radul. 2006. Nuggeteer: Au-
tomatic nugget-based evaluation using descriptions
and judgements. In HLT/NAACL 2006.
C. Monz. 2003. From Document Retrieval to Question
Answering. Ph.D. thesis, Institute for Logic, Lan-
guage, and Computation, University of Amsterdam.
S. Narayanan and S. Harabagiu. 2004. Question an-
swering based on semantic structures. In COLING
2004.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: QA–
by–Dossier–with–Constraints. In ACL 2004.
G. Salton and C. Buckley. 1990. Improving re-
trieval performance by relevance feedback. Jour-
nal of the American Society for Information Science,
41(4):288–297.
S. Tellex, B. Katz, J. Lin, G. Marton, and A. Fernandes.
2003. Quantitative evaluation of passage retrieval
algorithms for question answering. In SIGIR 2003.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. In TREC 2003.
E. Voorhees. 2005. Overview of the TREC 2005 ques-
tion answering track. In TREC 2005.
J. Xu, R. Weischedel, and A. Licuanan. 2004. Evalu-
ation of an extraction-based approach to answering
definition questions. In SIGIR 2004.
</reference>
<page confidence="0.997127">
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949055">
<title confidence="0.999831">The Role of Information Retrieval in Answering Complex Questions</title>
<author confidence="0.999968">Jimmy Lin</author>
<affiliation confidence="0.99767175">College of Information Studies Department of Computer Science Institute for Advanced Computer Studies University of Maryland</affiliation>
<address confidence="0.999942">College Park, MD 20742, USA</address>
<email confidence="0.99986">jimmylin@umd.edu</email>
<abstract confidence="0.997999476190476">This paper explores the role of information retrieval in answering “relationship” questions, a new class complex information needs formally introduced in TREC 2005. Since information retrieval is often an integral component of many question answering strategies, it is important to understand the impact of different termbased techniques. Within a framework of sentence retrieval, we examine three factors that contribute to question answering performance: the use of different retrieval engines, relevance (both at the document and sentence level), and redundancy. Results point out the limitations of purely term-based methods to this challenging task. Nevertheless, IR-based techniques provide a strong baseline on top of which more sophisticated language processing techniques can be deployed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>C Wade</author>
<author>A Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="22223" citStr="Allan et al., 2003" startWordPosition="3440" endWordPosition="3443">the drops observed in the document retrieval runs. It appears possible that the sentence retrieval algorithm was able to recover from a lower-quality result set, i.e., one with relevant documents ranked lower. Nevertheless, just as with factoid QA, the coupling between IR and answer extraction merits further study. 5 Reducing Redundancy The methods described in the previous section for choosing relevant sentences do not take into account information that may be conveyed more than once. Drawing inspiration from research in sentence-level redundancy within the context of the TREC novelty track (Allan et al., 2003) and work in multi-document summarization, we experimented with term-based approaches to reducing redundancy. Instead of selecting sentences for inclusion in the answer based on relevance alone, we implemented a simple utility model, which takes into account sentences that have already been added to the answer A. For each candidate c, utility is defined as follows: Utility(c) = Relevance(c) − A max SEA This model is the baseline variant of the Maximal Marginal Relevance method for summarization (Goldstein et al., 2000). Each candidate is compared to all sentences that have already been selecte</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and novelty detection at the sentence level. In SIGIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Amig´o</author>
<author>J Gonzalo</author>
<author>V Peinado</author>
<author>A Pe˜nas</author>
<author>F Verdejo</author>
</authors>
<title>An empirical study of information synthesis task.</title>
<date>2004</date>
<booktitle>In ACL</booktitle>
<marker>Amig´o, Gonzalo, Peinado, Pe˜nas, Verdejo, 2004</marker>
<rawString>E. Amig´o, J. Gonzalo, V. Peinado, A. Pe˜nas, and F. Verdejo. 2004. An empirical study of information synthesis task. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>E Voorhees</author>
</authors>
<title>Retrieval evaluation with incomplete information.</title>
<date>2004</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="12221" citStr="Buckley and Voorhees (2004)" startWordPosition="1873" endWordPosition="1877">.99 level, • or •. Differences not statistically significant are marked with °. Although the differences between Lucene and Indri are not significant, blind relevance feedback was found to hurt performance, significantly so in the case of Indri. These results are consistent with the findings of Monz (2003), who made the same observation in the factoid QA task. There are a few caveats to consider when interpreting these results. First, the test set of 25 questions is rather small. Second, the number of relevant documents per question is also relatively small, and hence likely to be incomplete. Buckley and Voorhees (2004) have shown that evaluation metrics are not stable with respect to incomplete relevance judgments. Third, the distribution of relevant documents may be biased due to the small number of submissions, many of which used 525 Lucene. Due to these factors, one should interpret the results reported here as suggestive, not definitive. Follow-up experiments with larger data sets are required to produce more conclusive results. EtESnQ idf (t) = EtESnQ idf (t) P = EtEA idf (t) , � EtEQ idf(t) 4 Selecting Relevant Sentences We adopted an extractive approach to answering relationship questions that views </context>
</contexts>
<marker>Buckley, Voorhees, 2004</marker>
<rawString>C. Buckley and E. Voorhees. 2004. Retrieval evaluation with incomplete information. In SIGIR 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
</authors>
<title>Passage-level evidence in document retrieval.</title>
<date>1994</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="13168" citStr="Callan, 1994" startWordPosition="2030" endWordPosition="2031">w-up experiments with larger data sets are required to produce more conclusive results. EtESnQ idf (t) = EtESnQ idf (t) P = EtEA idf (t) , � EtEQ idf(t) 4 Selecting Relevant Sentences We adopted an extractive approach to answering relationship questions that views the task as sentence retrieval, a conception in line with the thinking of many researchers today (but see discussion in Section 6). Although oversimplified, there are several reasons why this formulation is productive: since answers consist of unordered text segments, the task is similar to passage retrieval, a well-studied problem (Callan, 1994; Tellex et al., 2003) where sentences form a natural unit of retrieval. In addition, the TREC novelty tracks have specifically tackled the questions of relevance and redundancy at the sentence level (Harman, 2002). Empirically, a sentence retrieval approach performs quite well: when definition questions were first introduced in TREC 2003, a simple sentence-ranking algorithm outperformed all but the highest-scoring system (Voorhees, 2003). In addition, viewing the task of answering relationship questions as sentence retrieval allows one to leverage work in multi-document summarization, where e</context>
</contexts>
<marker>Callan, 1994</marker>
<rawString>J. Callan. 1994. Passage-level evidence in document retrieval. In SIGIR 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>Generic soft pattern models for definitional question answering.</title>
<date>2005</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="7043" citStr="Cui et al., 2005" startWordPosition="1072" endWordPosition="1075">ng factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evaluation methodology originally developed for definition que</context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>H. Cui, M.-Y. Kan, and T.-S. Chua. 2005. Generic soft pattern models for definitional question answering. In SIGIR 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2005</date>
<booktitle>In DUC</booktitle>
<contexts>
<context position="4506" citStr="Dang, 2005" startWordPosition="697" endWordPosition="698">Raul Castro was formally designated his brother’s successor vital Raul is the head of the Armed Forces okay Raul is five years younger than Castro okay Raul has enjoyed a more public role in running Cuba’s Government. okay Raul is the number two man in the government’s ruling Council of State Figure 1: An example relationship question from TREC 2005 with its answer nuggets. teresting “nuggets” about a particular person, organization, entity, or event. No single document can provide a complete answer, and hence systems must integrate information from multiple sources; cf. (Amig´o et al., 2004; Dang, 2005). This work focuses on so-called relationship questions, which represent a new and underexplored area in question answering. Although they require systems to extract information nuggets from multiple documents (just like definition/other questions), relationship questions demand a different approach (see Section 2). This paper explores the role of information retrieval in answering such questions, focusing primarily on three aspects: document retrieval performance, term-based measures of relevance, and term-based approaches to reducing redundancy. The overall goal is to push the limits of info</context>
<context position="26499" citStr="Dang, 2005" startWordPosition="4129" endWordPosition="4130">s such as “How close is Iran to acquiring nuclear weapons?”, which are the focus of the AQUAINT program in the U.S., and opinion questions such as “How does the Chilean government view attempts at having Pinochet tried in Spanish Court?”, which were explored in a 2005 pilot study also funded by AQUAINT. In 2006, there will be a dedicated task within the TREC QA track exploring complex questions within an interactive setting. Furthermore, we note the convergence of the QA and summarization communities, as demonstrated by the shift from generic to query-focused summaries starting with DUC 2005 (Dang, 2005). This development is also compatible with the conception of “distillation” in the current DARPA GALE program. All these trends point to same problem: how do we build advanced information systems to address complex information needs? The value of this work lies in the generality of IR-based approaches. Sophisticated linguistic processing algorithms are typically unable to cope with the enormous quantities of text available. To render analysis more computationally tractable, researchers commonly employ IR techniques to reduce the amount of text under consideration. We believe that the technique</context>
</contexts>
<marker>Dang, 2005</marker>
<rawString>H. Dang. 2005. Overview of DUC 2005. In DUC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>M Greenwood</author>
</authors>
<date>2004</date>
<booktitle>Proceedings of the SIGIR 2004 Workshop on Information Retrieval for Question Answering (IR4QA).</booktitle>
<contexts>
<context position="2820" citStr="Gaizauskas et al., 2004" startWordPosition="428" endWordPosition="431">ce of QA systems on IR technology, the relationship between them is an important area of study. For example, how sensitive is answer extraction performance to the initial quality of the result set? Does better document retrieval necessarily translate into more accurate answer extraction? These answers cannot be solely determined from first principles, but must be addressed through empirical experiments. Indeed, a number of works have specifically examined the effects of information retrieval on question answering (Monz, 2003; Tellex et al., 2003), including a dedicated workshop at SIGIR 2004 (Gaizauskas et al., 2004). More recently, the importance of document retrieval has prompted NIST to introduce a document ranking subtask inside the TREC 2005 QA track. However, the connection between QA and IR has mostly been explored in the context of factoid questions such as “Who shot Abraham Lincoln?”, which represent only a small fraction of all information needs. In contrast to factoid questions, which can be answered by short phrases found within an individual document, there is a large class of questions whose answers require synthesis of information from multiple sources. The socalled definition/other questio</context>
</contexts>
<marker>Gaizauskas, Hepple, Greenwood, 2004</marker>
<rawString>R. Gaizauskas, M. Hepple, and M. Greenwood. 2004. Proceedings of the SIGIR 2004 Workshop on Information Retrieval for Question Answering (IR4QA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>J Callan</author>
</authors>
<title>Creating and evaluating multi-document sentence extract summaries.</title>
<date>2000</date>
<booktitle>In CIKM</booktitle>
<contexts>
<context position="22747" citStr="Goldstein et al., 2000" startWordPosition="3526" endWordPosition="3529">arch in sentence-level redundancy within the context of the TREC novelty track (Allan et al., 2003) and work in multi-document summarization, we experimented with term-based approaches to reducing redundancy. Instead of selecting sentences for inclusion in the answer based on relevance alone, we implemented a simple utility model, which takes into account sentences that have already been added to the answer A. For each candidate c, utility is defined as follows: Utility(c) = Relevance(c) − A max SEA This model is the baseline variant of the Maximal Marginal Relevance method for summarization (Goldstein et al., 2000). Each candidate is compared to all sentences that have already been selected for inclusion in the answer. The maximum of these pairwise similarity comparisons is deducted from the relevance score of the sentence, subjected to A, a parameter that we tune. For our experiments, we used cosine distance as the similarity function. All relevance scores were normalized to a range between zero and one. At each step in the answer generation process, utility values are computed for all candidate sentences. The one with the highest score is selected for inclusion in the final answer. Utility values are </context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Callan, 2000</marker>
<rawString>J. Goldstein, V. Mittal, J. Carbonell, and J. Callan. 2000. Creating and evaluating multi-document sentence extract summaries. In CIKM 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<booktitle>In TREC</booktitle>
<contexts>
<context position="13382" citStr="Harman, 2002" startWordPosition="2064" endWordPosition="2065">oach to answering relationship questions that views the task as sentence retrieval, a conception in line with the thinking of many researchers today (but see discussion in Section 6). Although oversimplified, there are several reasons why this formulation is productive: since answers consist of unordered text segments, the task is similar to passage retrieval, a well-studied problem (Callan, 1994; Tellex et al., 2003) where sentences form a natural unit of retrieval. In addition, the TREC novelty tracks have specifically tackled the questions of relevance and redundancy at the sentence level (Harman, 2002). Empirically, a sentence retrieval approach performs quite well: when definition questions were first introduced in TREC 2003, a simple sentence-ranking algorithm outperformed all but the highest-scoring system (Voorhees, 2003). In addition, viewing the task of answering relationship questions as sentence retrieval allows one to leverage work in multi-document summarization, where extractive approaches have been extensively studied. This section examines the task of independently selecting the best sentences for inclusion in an answer; attempts to reduce redundancy will be discussed in the ne</context>
</contexts>
<marker>Harman, 2002</marker>
<rawString>D. Harman. 2002. Overview of the TREC 2002 novelty track. In TREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hildebrandt</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>Answering definition questions with multiple knowledge sources.</title>
<date>2004</date>
<booktitle>In HLT/NAACL</booktitle>
<contexts>
<context position="6986" citStr="Hildebrandt et al., 2004" startWordPosition="1060" endWordPosition="1063">India show an increase in cooperation? If so, what are the driving factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evalu</context>
<context position="15911" citStr="Hildebrandt et al., 2004" startWordPosition="2464" endWordPosition="2467">menting with different values, we discovered that a document cutoff of ten yielded the highest performance in terms of POURPRE scores, i.e., all but the ten top-ranking documents were discarded. In addition, we built a linear regression model that employed the above features to predict the nugget score of a sentence (the dependent variable). For the training samples, the nugget matching component within POURPRE was employed to compute the nugget score—this value quantified the “goodness” of a particular sentence in terms of nugget content.1 Due to known issues with the vital/okay distinction (Hildebrandt et al., 2004), it was ignored for this computation; however, see (Lin and Demner-Fushman, 2006b) for recent attempts to address this issue. When presented with a test question, the system ranked all sentences from the top ten retrieved documents using the regression model. Answers were generated by filling a quota of characters, just as in the baseline. Once again, no attempt was made to reduce redundancy. We conducted a five-fold cross validation experiment using all sentences from the top 100 Lucene documents as training samples. After experimenting with different features, we discovered that a regressio</context>
</contexts>
<marker>Hildebrandt, Katz, Lin, 2004</marker>
<rawString>W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering definition questions with multiple knowledge sources. In HLT/NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>R Gaizauskas</author>
</authors>
<title>Natural language question answering: The view from here.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="2177" citStr="Hirschman and Gaizauskas, 2001" startWordPosition="323" endWordPosition="326">to pinpoint the exact span of text that directly satisfies an information need. Nevertheless, IR engines remain integral components of question answering systems, primarily as a source of candidate documents that are subsequently analyzed in greater detail. Although this two-stage architecture was initially conceived as an expedient to overcome the computational processing bottleneck associated with more sophisticated but slower language processing technology, it has worked quite well in practice. The architecture has since evolved into a widely-accepted paradigm for building working systems (Hirschman and Gaizauskas, 2001). Due to the reliance of QA systems on IR technology, the relationship between them is an important area of study. For example, how sensitive is answer extraction performance to the initial quality of the result set? Does better document retrieval necessarily translate into more accurate answer extraction? These answers cannot be solely determined from first principles, but must be addressed through empirical experiments. Indeed, a number of works have specifically examined the effects of information retrieval on question answering (Monz, 2003; Tellex et al., 2003), including a dedicated works</context>
</contexts>
<marker>Hirschman, Gaizauskas, 2001</marker>
<rawString>L. Hirschman and R. Gaizauskas. 2001. Natural language question answering: The view from here. Natural Language Engineering, 7(4):275–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Katz</author>
<author>G Marton</author>
<author>G Borchardt</author>
<author>A Brownell</author>
<author>S Felshin</author>
<author>D Loreto</author>
<author>J Louis-Rosenberg</author>
<author>B Lu</author>
<author>F Mora</author>
<author>S Stiller</author>
<author>O Uzuner</author>
<author>A Wilcox</author>
</authors>
<title>External knowledge sources for question answering. In TREC</title>
<date>2005</date>
<contexts>
<context position="14680" citStr="Katz et al., 2005" startWordPosition="2267" endWordPosition="2270">e sentence that may contribute to its relevance. In general, such features can be divided into two types: properties of the document containing the sentence and properties of the sentence itself. Regarding the former type, two features come into play: the relevance score of the document (from the IR engine) and its rank in the result set. For sentence-based features, we experimented with the following: • Passage match score, which sums the idf values of unique terms that appear in both the candidate sentence (S) and the question (Q): � idf(t) tESnQ • Term idf precision and recall scores; cf. (Katz et al., 2005): • Length of the sentence (in non-whitespace characters). Note that precision and recall values are bounded between zero and one, while the passage match score and the length of the sentence are both unbounded features. Our baseline sentence retriever employed the passage match score to rank all sentences in the top n retrieved documents. By default, we used documents retrieved by Lucene, using the question verbatim as the query. To generate answers, the system selected sentences based on their scores until a hard length quota has been filled (trimming the final sentence if necessary). After </context>
</contexts>
<marker>Katz, Marton, Borchardt, Brownell, Felshin, Loreto, Louis-Rosenberg, Lu, Mora, Stiller, Uzuner, Wilcox, 2005</marker>
<rawString>B. Katz, G. Marton, G. Borchardt, A. Brownell, S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu, F. Mora, S. Stiller, O. Uzuner, and A. Wilcox. 2005. External knowledge sources for question answering. In TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Automatically evaluating answers to definition questions.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP</booktitle>
<contexts>
<context position="8706" citStr="Lin and Demner-Fushman, 2005" startWordPosition="1316" endWordPosition="1319">sion is based on a length allowance derived from the number of both vital and okay nuggets retrieved. In the original NIST setup, human assessors were required to manually determine whether a particular system’s response contained a nugget. This posed a problem for researchers who wished to conduct formative evaluations outside the annual TREC cycle—the necessity of human involvement meant that system responses could not be rapidly, consistently, and automatically assessed. However, the recent introduction of POURPRE, an automatic evaluation metric for the nugget-based evaluation methodology (Lin and Demner-Fushman, 2005), fills this evaluation gap and makes possible the work reported here; cf. Nuggeteer (Marton and Radul, 2006). This paper describes experiments with the 25 relationship questions used in the secondary task of the TREC 2005 QA track (Voorhees, 2005), which attracted a total of eleven submissions. Systems used the AQUAINT corpus, a three gigabyte collection of approximately one million news articles from the Associated Press, the New York Times, and the Xinhua News Agency. 3 Document Retrieval Since information retrieval systems supply the initial set of documents on which a question answering s</context>
</contexts>
<marker>Lin, Demner-Fushman, 2005</marker>
<rawString>J. Lin and D. Demner-Fushman. 2005. Automatically evaluating answers to definition questions. In HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>The role of knowledge in conceptual retrieval: A study in the domain of clinical medicine.</title>
<date>2006</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="15992" citStr="Lin and Demner-Fushman, 2006" startWordPosition="2477" endWordPosition="2480">lded the highest performance in terms of POURPRE scores, i.e., all but the ten top-ranking documents were discarded. In addition, we built a linear regression model that employed the above features to predict the nugget score of a sentence (the dependent variable). For the training samples, the nugget matching component within POURPRE was employed to compute the nugget score—this value quantified the “goodness” of a particular sentence in terms of nugget content.1 Due to known issues with the vital/okay distinction (Hildebrandt et al., 2004), it was ignored for this computation; however, see (Lin and Demner-Fushman, 2006b) for recent attempts to address this issue. When presented with a test question, the system ranked all sentences from the top ten retrieved documents using the regression model. Answers were generated by filling a quota of characters, just as in the baseline. Once again, no attempt was made to reduce redundancy. We conducted a five-fold cross validation experiment using all sentences from the top 100 Lucene documents as training samples. After experimenting with different features, we discovered that a regression model with the following performed best: passage match score, document score, a</context>
<context position="29021" citStr="Lin and Demner-Fushman, 2006" startWordPosition="4513" endWordPosition="4516">are more difficult to process: for one, they are often not phrased as direct wh-questions, but rather as indirect requests for information, statements of doubt, etc. Furthermore, since these complex questions cannot be answered by short noun phrases, existing answer type ontologies are not very useful. For our experiments, we decided to simply use the question verbatim as the query to the IR systems, but undoubtedly performance can be gained by better query formulation strategies. These are difficult challenges, but recent work on applying semantic models to QA (Narayanan and Harabagiu, 2004; Lin and Demner-Fushman, 2006a) provide a promising direction. While our formulation of answering relationship questions as sentence retrieval is productive, it clearly has limitations. The assumption that information nuggets do not span sentence boundaries is false and neglects important work in anaphora resolution and discourse modeling. The current setup of the task, where answers consist of unordered strings, does not place any value on coherence and readability of the responses, which will be important if the answers are intended for human consumption. Clearly, there are ample opportunities here for NLP techniques to</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>J. Lin and D. Demner-Fushman. 2006a. The role of knowledge in conceptual retrieval: A study in the domain of clinical medicine. In SIGIR 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Will pyramids built of nuggets topple over?</title>
<date>2006</date>
<booktitle>In HLT/NAACL</booktitle>
<contexts>
<context position="15992" citStr="Lin and Demner-Fushman, 2006" startWordPosition="2477" endWordPosition="2480">lded the highest performance in terms of POURPRE scores, i.e., all but the ten top-ranking documents were discarded. In addition, we built a linear regression model that employed the above features to predict the nugget score of a sentence (the dependent variable). For the training samples, the nugget matching component within POURPRE was employed to compute the nugget score—this value quantified the “goodness” of a particular sentence in terms of nugget content.1 Due to known issues with the vital/okay distinction (Hildebrandt et al., 2004), it was ignored for this computation; however, see (Lin and Demner-Fushman, 2006b) for recent attempts to address this issue. When presented with a test question, the system ranked all sentences from the top ten retrieved documents using the regression model. Answers were generated by filling a quota of characters, just as in the baseline. Once again, no attempt was made to reduce redundancy. We conducted a five-fold cross validation experiment using all sentences from the top 100 Lucene documents as training samples. After experimenting with different features, we discovered that a regression model with the following performed best: passage match score, document score, a</context>
<context position="29021" citStr="Lin and Demner-Fushman, 2006" startWordPosition="4513" endWordPosition="4516">are more difficult to process: for one, they are often not phrased as direct wh-questions, but rather as indirect requests for information, statements of doubt, etc. Furthermore, since these complex questions cannot be answered by short noun phrases, existing answer type ontologies are not very useful. For our experiments, we decided to simply use the question verbatim as the query to the IR systems, but undoubtedly performance can be gained by better query formulation strategies. These are difficult challenges, but recent work on applying semantic models to QA (Narayanan and Harabagiu, 2004; Lin and Demner-Fushman, 2006a) provide a promising direction. While our formulation of answering relationship questions as sentence retrieval is productive, it clearly has limitations. The assumption that information nuggets do not span sentence boundaries is false and neglects important work in anaphora resolution and discourse modeling. The current setup of the task, where answers consist of unordered strings, does not place any value on coherence and readability of the responses, which will be important if the answers are intended for human consumption. Clearly, there are ample opportunities here for NLP techniques to</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>J. Lin and D. Demner-Fushman. 2006b. Will pyramids built of nuggets topple over? In HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Marton</author>
<author>A Radul</author>
</authors>
<title>Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements.</title>
<date>2006</date>
<booktitle>In HLT/NAACL</booktitle>
<contexts>
<context position="8815" citStr="Marton and Radul, 2006" startWordPosition="1333" endWordPosition="1336">al NIST setup, human assessors were required to manually determine whether a particular system’s response contained a nugget. This posed a problem for researchers who wished to conduct formative evaluations outside the annual TREC cycle—the necessity of human involvement meant that system responses could not be rapidly, consistently, and automatically assessed. However, the recent introduction of POURPRE, an automatic evaluation metric for the nugget-based evaluation methodology (Lin and Demner-Fushman, 2005), fills this evaluation gap and makes possible the work reported here; cf. Nuggeteer (Marton and Radul, 2006). This paper describes experiments with the 25 relationship questions used in the secondary task of the TREC 2005 QA track (Voorhees, 2005), which attracted a total of eleven submissions. Systems used the AQUAINT corpus, a three gigabyte collection of approximately one million news articles from the Associated Press, the New York Times, and the Xinhua News Agency. 3 Document Retrieval Since information retrieval systems supply the initial set of documents on which a question answering system operates, it makes sense to optimize document retrieval performance in isolation. The issue of end–to–e</context>
</contexts>
<marker>Marton, Radul, 2006</marker>
<rawString>G. Marton and A. Radul. 2006. Nuggeteer: Automatic nugget-based evaluation using descriptions and judgements. In HLT/NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
</authors>
<title>From Document Retrieval to Question Answering.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Institute for Logic, Language, and Computation, University of Amsterdam.</institution>
<contexts>
<context position="2726" citStr="Monz, 2003" startWordPosition="415" endWordPosition="416">for building working systems (Hirschman and Gaizauskas, 2001). Due to the reliance of QA systems on IR technology, the relationship between them is an important area of study. For example, how sensitive is answer extraction performance to the initial quality of the result set? Does better document retrieval necessarily translate into more accurate answer extraction? These answers cannot be solely determined from first principles, but must be addressed through empirical experiments. Indeed, a number of works have specifically examined the effects of information retrieval on question answering (Monz, 2003; Tellex et al., 2003), including a dedicated workshop at SIGIR 2004 (Gaizauskas et al., 2004). More recently, the importance of document retrieval has prompted NIST to introduce a document ranking subtask inside the TREC 2005 QA track. However, the connection between QA and IR has mostly been explored in the context of factoid questions such as “Who shot Abraham Lincoln?”, which represent only a small fraction of all information needs. In contrast to factoid questions, which can be answered by short phrases found within an individual document, there is a large class of questions whose answers</context>
<context position="11901" citStr="Monz (2003)" startWordPosition="1821" endWordPosition="1822">nificance of the results. This test is commonly used in information retrieval research because it makes minimal assumptions about the underlying distribution of differences. Significance at the 0.90 level is denoted with a � or &apos;, depending on the direction of change; at the 0.95 level, ° or v; at the 0.99 level, • or •. Differences not statistically significant are marked with °. Although the differences between Lucene and Indri are not significant, blind relevance feedback was found to hurt performance, significantly so in the case of Indri. These results are consistent with the findings of Monz (2003), who made the same observation in the factoid QA task. There are a few caveats to consider when interpreting these results. First, the test set of 25 questions is rather small. Second, the number of relevant documents per question is also relatively small, and hence likely to be incomplete. Buckley and Voorhees (2004) have shown that evaluation metrics are not stable with respect to incomplete relevance judgments. Third, the distribution of relevant documents may be biased due to the small number of submissions, many of which used 525 Lucene. Due to these factors, one should interpret the res</context>
</contexts>
<marker>Monz, 2003</marker>
<rawString>C. Monz. 2003. From Document Retrieval to Question Answering. Ph.D. thesis, Institute for Logic, Language, and Computation, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Narayanan</author>
<author>S Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In COLING</booktitle>
<contexts>
<context position="28991" citStr="Narayanan and Harabagiu, 2004" startWordPosition="4509" endWordPosition="4512"> etc.). Relationship questions are more difficult to process: for one, they are often not phrased as direct wh-questions, but rather as indirect requests for information, statements of doubt, etc. Furthermore, since these complex questions cannot be answered by short noun phrases, existing answer type ontologies are not very useful. For our experiments, we decided to simply use the question verbatim as the query to the IR systems, but undoubtedly performance can be gained by better query formulation strategies. These are difficult challenges, but recent work on applying semantic models to QA (Narayanan and Harabagiu, 2004; Lin and Demner-Fushman, 2006a) provide a promising direction. While our formulation of answering relationship questions as sentence retrieval is productive, it clearly has limitations. The assumption that information nuggets do not span sentence boundaries is false and neglects important work in anaphora resolution and discourse modeling. The current setup of the task, where answers consist of unordered strings, does not place any value on coherence and readability of the responses, which will be important if the answers are intended for human consumption. Clearly, there are ample opportunit</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>S. Narayanan and S. Harabagiu. 2004. Question answering based on semantic structures. In COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>J Chu-Carroll</author>
<author>K Czuba</author>
</authors>
<title>Question answering using constraint satisfaction: QA– by–Dossier–with–Constraints.</title>
<date>2004</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="7007" citStr="Prager et al., 2004" startWordPosition="1064" endWordPosition="1067">cooperation? If so, what are the driving factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evaluation methodology ori</context>
</contexts>
<marker>Prager, Chu-Carroll, Czuba, 2004</marker>
<rawString>J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question answering using constraint satisfaction: QA– by–Dossier–with–Constraints. In ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Improving retrieval performance by relevance feedback.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="10489" citStr="Salton and Buckley, 1990" startWordPosition="1594" endWordPosition="1597">ms on performance. Two freely-available IR engines were compared: Lucene and Indri. The former is an open-source implementation of what amounts to be a modified tfidf weighting scheme, while the latter employs a language modeling approach. In addition, we experimented with blind relevance feedback, a reMAP R50 Lucene 0.206 0.469 Lucene+brf 0.190 (− 7.6%)° 0.442 (− 5.6%)° Indri 0.195 (− 5.2%)° 0.442 (− 5.6%)° Indri+brf 0.158 (− 23.3%)° 0.377 (− 19.5%)° Table 1: Document retrieval performance, with and without blind relevance feedback. trieval technique commonly employed to improve performance (Salton and Buckley, 1990). Following settings in typical IR experiments, the top twenty terms (by ifidf value) from the top twenty documents were added to the original query in the feedback iteration. For each question, fifty documents from the AQUAINT collection were retrieved, representing the number of documents that a typical QA system might consider. The question itself was used verbatim as the IR query (see Section 6 for discussion). Performance is shown in Table 1. We measured Mean Average Precision (MAP), the most informative single-point metric for ranked retrieval, and recall, since it places an upper bound </context>
</contexts>
<marker>Salton, Buckley, 1990</marker>
<rawString>G. Salton and C. Buckley. 1990. Improving retrieval performance by relevance feedback. Journal of the American Society for Information Science, 41(4):288–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>G Marton</author>
<author>A Fernandes</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="2748" citStr="Tellex et al., 2003" startWordPosition="417" endWordPosition="420"> working systems (Hirschman and Gaizauskas, 2001). Due to the reliance of QA systems on IR technology, the relationship between them is an important area of study. For example, how sensitive is answer extraction performance to the initial quality of the result set? Does better document retrieval necessarily translate into more accurate answer extraction? These answers cannot be solely determined from first principles, but must be addressed through empirical experiments. Indeed, a number of works have specifically examined the effects of information retrieval on question answering (Monz, 2003; Tellex et al., 2003), including a dedicated workshop at SIGIR 2004 (Gaizauskas et al., 2004). More recently, the importance of document retrieval has prompted NIST to introduce a document ranking subtask inside the TREC 2005 QA track. However, the connection between QA and IR has mostly been explored in the context of factoid questions such as “Who shot Abraham Lincoln?”, which represent only a small fraction of all information needs. In contrast to factoid questions, which can be answered by short phrases found within an individual document, there is a large class of questions whose answers require synthesis of </context>
<context position="13190" citStr="Tellex et al., 2003" startWordPosition="2032" endWordPosition="2035">ts with larger data sets are required to produce more conclusive results. EtESnQ idf (t) = EtESnQ idf (t) P = EtEA idf (t) , � EtEQ idf(t) 4 Selecting Relevant Sentences We adopted an extractive approach to answering relationship questions that views the task as sentence retrieval, a conception in line with the thinking of many researchers today (but see discussion in Section 6). Although oversimplified, there are several reasons why this formulation is productive: since answers consist of unordered text segments, the task is similar to passage retrieval, a well-studied problem (Callan, 1994; Tellex et al., 2003) where sentences form a natural unit of retrieval. In addition, the TREC novelty tracks have specifically tackled the questions of relevance and redundancy at the sentence level (Harman, 2002). Empirically, a sentence retrieval approach performs quite well: when definition questions were first introduced in TREC 2003, a simple sentence-ranking algorithm outperformed all but the highest-scoring system (Voorhees, 2003). In addition, viewing the task of answering relationship questions as sentence retrieval allows one to leverage work in multi-document summarization, where extractive approaches h</context>
</contexts>
<marker>Tellex, Katz, Lin, Marton, Fernandes, 2003</marker>
<rawString>S. Tellex, B. Katz, J. Lin, G. Marton, and A. Fernandes. 2003. Quantitative evaluation of passage retrieval algorithms for question answering. In SIGIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In TREC</booktitle>
<contexts>
<context position="13610" citStr="Voorhees, 2003" startWordPosition="2094" endWordPosition="2095"> reasons why this formulation is productive: since answers consist of unordered text segments, the task is similar to passage retrieval, a well-studied problem (Callan, 1994; Tellex et al., 2003) where sentences form a natural unit of retrieval. In addition, the TREC novelty tracks have specifically tackled the questions of relevance and redundancy at the sentence level (Harman, 2002). Empirically, a sentence retrieval approach performs quite well: when definition questions were first introduced in TREC 2003, a simple sentence-ranking algorithm outperformed all but the highest-scoring system (Voorhees, 2003). In addition, viewing the task of answering relationship questions as sentence retrieval allows one to leverage work in multi-document summarization, where extractive approaches have been extensively studied. This section examines the task of independently selecting the best sentences for inclusion in an answer; attempts to reduce redundancy will be discussed in the next section. There are a number of term-based features associated with a candidate sentence that may contribute to its relevance. In general, such features can be divided into two types: properties of the document containing the </context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>E. Voorhees. 2003. Overview of the TREC 2003 question answering track. In TREC 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>question answering track.</title>
<date>2005</date>
<journal>Overview of the TREC</journal>
<booktitle>In TREC</booktitle>
<contexts>
<context position="3466" citStr="Voorhees, 2005" startWordPosition="532" endWordPosition="533"> of document retrieval has prompted NIST to introduce a document ranking subtask inside the TREC 2005 QA track. However, the connection between QA and IR has mostly been explored in the context of factoid questions such as “Who shot Abraham Lincoln?”, which represent only a small fraction of all information needs. In contrast to factoid questions, which can be answered by short phrases found within an individual document, there is a large class of questions whose answers require synthesis of information from multiple sources. The socalled definition/other questions at recent TREC evaluations (Voorhees, 2005) serve as good examples: “good answers” to these questions include in523 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 523–530, Sydney, July 2006. c�2006 Association for Computational Linguistics Qid 25: The analyst is interested in the status of Fidel Castro’s brother. Specifically, the analyst would like information on his current plans and what role he may play after Fidel Castro’s death. vital Raul Castro was formally designated his brother’s successor vital Raul is the head of the Armed Forces okay Raul is five years younger than Castro okay Raul has enjoyed a </context>
<context position="5890" citStr="Voorhees, 2005" startWordPosition="895" endWordPosition="896">s: Section 2 provides an overview of relationship questions. Section 3 describes experiments focused on document retrieval performance. An approach to answering relationship questions based on sentence retrieval is discussed in Section 4. A simple utility model that incorporates both relevance and redundancy is explored in Section 5. Before concluding, we discuss the implications of our experimental results in Section 6. 2 Relationship Questions Relationship questions represent a new class of information needs formally introduced as a subtask in the NIST-sponsored TREC QA evaluations in 2005 (Voorhees, 2005). Previously, they were the focus of a small pilot study within the AQUAINT program, which resulted in an understanding of a “relationship” as the ability for one object to influence another. Objects in these questions can denote either entities (people, organization, countries, etc.) or events. Consider the following examples: • Has pressure from China affected America’s willingness to sell weaponry to Taiwan? • Do the military personnel exchanges between Israel and India show an increase in cooperation? If so, what are the driving factors behind this increase? Evidence for a relationship inc</context>
<context position="7671" citStr="Voorhees, 2005" startWordPosition="1162" endWordPosition="1163">s shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evaluation methodology originally developed for definition questions; see (Voorhees, 2005) for a detailed description. Answers consist of units of information called “nuggets”, which assessors manually create from system submissions and their own research (see example in Figure 1). Nuggets are divided into 524 two types (“vital” and “okay”), and this distinction plays an important role in scoring. The official metric is an F3-score, where nugget recall is computed on vital nuggets, and precision is based on a length allowance derived from the number of both vital and okay nuggets retrieved. In the original NIST setup, human assessors were required to manually determine whether a pa</context>
<context position="8954" citStr="Voorhees, 2005" startWordPosition="1357" endWordPosition="1358">for researchers who wished to conduct formative evaluations outside the annual TREC cycle—the necessity of human involvement meant that system responses could not be rapidly, consistently, and automatically assessed. However, the recent introduction of POURPRE, an automatic evaluation metric for the nugget-based evaluation methodology (Lin and Demner-Fushman, 2005), fills this evaluation gap and makes possible the work reported here; cf. Nuggeteer (Marton and Radul, 2006). This paper describes experiments with the 25 relationship questions used in the secondary task of the TREC 2005 QA track (Voorhees, 2005), which attracted a total of eleven submissions. Systems used the AQUAINT corpus, a three gigabyte collection of approximately one million news articles from the Associated Press, the New York Times, and the Xinhua News Agency. 3 Document Retrieval Since information retrieval systems supply the initial set of documents on which a question answering system operates, it makes sense to optimize document retrieval performance in isolation. The issue of end–to–end system performance will be taken up in Section 4. Retrieval performance can be evaluated based on the assumption that documents which co</context>
</contexts>
<marker>Voorhees, 2005</marker>
<rawString>E. Voorhees. 2005. Overview of the TREC 2005 question answering track. In TREC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Weischedel</author>
<author>A Licuanan</author>
</authors>
<title>Evaluation of an extraction-based approach to answering definition questions.</title>
<date>2004</date>
<booktitle>In SIGIR</booktitle>
<contexts>
<context position="7024" citStr="Xu et al., 2004" startWordPosition="1068" endWordPosition="1071">hat are the driving factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evaluation methodology originally developed</context>
</contexts>
<marker>Xu, Weischedel, Licuanan, 2004</marker>
<rawString>J. Xu, R. Weischedel, and A. Licuanan. 2004. Evaluation of an extraction-based approach to answering definition questions. In SIGIR 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>