<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.989152">
A Tensor-based Factorization Model of Semantic Compositionality
</title>
<author confidence="0.62165">
Tim Van de Cruys
</author>
<note confidence="0.7259455">
IRIT – UMR 5505
CNRS
</note>
<address confidence="0.884812">
Toulouse, France
</address>
<email confidence="0.986527">
tim.vandecruys@irit.fr
</email>
<note confidence="0.677228666666667">
Thierry Poibeau*
LaTTiCe – UMR 8094
CNRS &amp; ENS
</note>
<address confidence="0.882452">
Paris, France
</address>
<email confidence="0.994395">
thierry.poibeau@ens.fr
</email>
<author confidence="0.969972">
Anna Korhonen
</author>
<affiliation confidence="0.958813">
Computer Laboratory &amp; DTAL*
University of Cambridge
</affiliation>
<address confidence="0.797299">
United Kingdom
</address>
<email confidence="0.995947">
anna.korhonen@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755466666667">
In this paper, we present a novel method for the
computation of compositionality within a distri-
butional framework. The key idea is that com-
positionality is modeled as a multi-way interac-
tion between latent factors, which are automat-
ically constructed from corpus data. We use
our method to model the composition of sub-
ject verb object triples. The method consists
of two steps. First, we compute a latent factor
model for nouns from standard co-occurrence
data. Next, the latent factors are used to induce
a latent model of three-way subject verb object
interactions. Our model has been evaluated on
a similarity task for transitive phrases, in which
it exceeds the state of the art.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998234">
In the course of the last two decades, significant
progress has been made with regard to the automatic
extraction of lexical semantic knowledge from large-
scale text corpora. Most work relies on the distribu-
tional hypothesis of meaning (Harris, 1954), which
states that words that appear within the same contexts
tend to be semantically similar. A large number of
researchers have taken this dictum to heart, giving
rise to a plethora of algorithms that try to capture
the semantics of words by looking at their distribu-
tion in text. Up till now, however, most work on the
automatic acquisition of semantics only deals with
individual words. The modeling of meaning beyond
the level of individual words – i.e. the combination
of words into larger units – is to a large degree left
unexplored.
The principle of compositionality, often attributed
to Frege, is the principle that states that the meaning
of a complex expression is a function of the meaning
of its parts and the way those parts are (syntactically)
combined (Frege, 1892). It is the fundamental prin-
ciple that allows language users to understand the
meaning of sentences they have never heard before,
by constructing the meaning of the complex expres-
sion from the meanings of the individual words. Re-
cently, a number of researchers have tried to reconcile
the framework of distributional semantics with the
principle of compositionality (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Coecke et al.,
2010; Socher et al., 2012). However, the absolute
gains of the systems remain a bit unclear, and a sim-
ple method of composition – vector multiplication –
often seems to produce the best results (Blacoe and
Lapata, 2012).
In this paper, we present a novel method for the
joint composition of a verb with its subject and di-
rect object. The key idea is that compositionality is
modeled as a multi-way interaction between latent
factors, which are automatically constructed from
corpus data. In order to adequately model the multi-
way interaction between a verb and its subject and
objects, a significant part of our method relies on
tensor algebra. Additionally, our method makes use
of a factorization model appropriate for tensors.
The remainder of the paper is structured as follows.
In section 2, we give an overview of previous work
that is relevant to the task of computing composition-
ality within a distributional framework. Section 3
presents a detailed description of our method, in-
cluding an overview of the necessary mathematical
</bodyText>
<page confidence="0.958312">
1142
</page>
<note confidence="0.470609">
Proceedings of NAACL-HLT 2013, pages 1142–1151,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999609">
machinery. Section 4 illustrates our method with a
number of detailed examples. Section 5 presents a
quantitative evaluation, and compares our method
to other models of distributional compositionality.
Section 6, then, concludes and lays out a number of
directions for future work.
</bodyText>
<sectionHeader confidence="0.996522" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99996891954023">
In recent years, a number of methods have been de-
veloped that try to capture compositional phenomena
within a distributional framework. One of the first
approaches to tackle compositional phenomena in a
systematic way is Mitchell and Lapata’s (2008) ap-
proach. They explore a number of different models
for vector composition, of which vector addition (the
sum of each feature) and vector multiplication (the
elementwise multiplication of each feature) are the
most important. They evaluate their models on a
noun-verb phrase similarity task, and find that the
multiplicative model yields the best results, along
with a weighted combination of the additive and mul-
tiplicative model.
Baroni and Zamparelli (2010) present a method
for the composition of adjectives and nouns. In their
model, an adjective is a linear function of one vector
(the noun vector) to another vector (the vector for the
adjective-noun pair). The linear transformation for a
particular adjective is represented by a matrix, and
is learned automatically from a corpus, using partial
least-squares regression.
Coecke et al. (2010) present an abstract theoreti-
cal framework in which a sentence vector is a func-
tion of the Kronecker product of its word vectors,
which allows for greater interaction between the dif-
ferent word features. A number of instantiations of
the framework are tested experimentally in Grefen-
stette and Sadrzadeh (2011a) and Grefenstette and
Sadrzadeh (2011b). The key idea is that relational
words (e.g. adjectives or verbs) have a rich (multi-
dimensional) structure that acts as a filter on their
arguments. Our model uses an intuition similar to
theirs.
Socher et al. (2012) present a model for composi-
tionality based on recursive neural networks. Each
node in a parse tree is assigned both a vector and
a matrix; the vector captures the actual meaning of
the constituent, while the matrix models the way
it changes the meaning of neighbouring words and
phrases.
Closely related to the work on compositionality
is research on the computation of word meaning in
context. Erk and Pad´o (2008, 2009) make use of
selectional preferences to express the meaning of a
word in context; the meaning of a word in the pres-
ence of an argument is computed by multiplying the
word’s vector with a vector that captures the inverse
selectional preferences of the argument. Thater et
al. (2009, 2010) extend the approach based on se-
lectional preferences by incorporating second-order
co-occurrences in their model. And Dinu and La-
pata (2010) propose a probabilistic framework that
models the meaning of words as a probability distri-
bution over latent factors. This allows them to model
contextualized meaning as a change in the original
sense distribution. Dinu and Lapata use non-negative
matrix factorization (NMF) to induce latent factors.
Similar to their work, our model uses NMF – albeit
in a slightly different configuration – as a first step
towards our final factorization model.
In general, latent models have proven to be useful
for the modeling of word meaning. One of the best
known latent models of semantics is Latent Seman-
tic Analysis (Landauer and Dumais, 1997), which
uses singular value decomposition in order to auto-
matically induce latent factors from term-document
matrices. Another well known latent model of mean-
ing, which takes a generative approach, is Latent
Dirichlet Allocation (Blei et al., 2003).
Tensor factorization has been used before for the
modeling of natural language. Giesbrecht (2010)
describes a tensor factorization model for the con-
struction of a distributional model that is sensitive to
word order. And Van de Cruys (2010) uses a tensor
factorization model in order to construct a three-way
selectional preference model of verbs, subjects, and
objects. Our underlying tensor factorization – Tucker
decomposition – is the same as Giesbrecht’s; and
similar to Van de Cruys (2010), we construct a la-
tent model of verb, subject, and object interactions.
The way our model is constructed, however, is sig-
nificantly different. The former research does not
use any syntactic information for the construction
of the tensor, while the latter makes use of a more
restricted tensor factorization model, viz. parallel
factor analysis (Harshman and Lundy, 1994).
</bodyText>
<page confidence="0.978466">
1143
</page>
<bodyText confidence="0.981633777777778">
The idea of modeling compositionality by means A special case of the Kronecker product is the
of tensor (Kronecker) product has been proposed outer product of two vectors a E RI and b E RJ, de-
in the literature before (Clark and Pulman, 2007; noted a o b. The result is a matrix A E RIxJ obtained
Coecke et al., 2010). However, the method presented by multiplying each element of a with each element
here is the first that tries to capture compositional of b.
phenomena by exploiting the multi-way interactions Finally, the Hadamard product, denoted A * B,
between latent factors, induced by a suitable tensor is the elementwise multiplication of two matrices
factorization model. A E RIxJ and B E RIxJ, which produces a matrix
that is equally of size I x J.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999936">
3.2 The construction of latent noun factors
3.1 Mathematical preliminaries
</subsectionHeader>
<bodyText confidence="0.994976727272727">
The methodology presented in this paper requires
a number of concepts and mathematical operations
from tensor algebra, which are briefly reviewed in
this section. The interested reader is referred to Kolda
and Bader (2009) for a more thorough introduction
to tensor algebra (including an overview of various
factorization methods).
A tensor is a multidimensional array; it is the gen-
eralization of a matrix to more than two dimensions,
or modes. Whereas matrices are only able to cap-
ture two-way co-occurrences, tensors are able to cap-
ture multi-way co-occurrences.1 Following prevail-
ing convention, tensors are represented by boldface
Euler script notation (X), matrices by boldface capi-
tal letters (X), vectors by boldface lower case letters
(x), and scalars by italic letters (x).
The n-mode product of a tensor X E RI1xI2x...xIN
with a matrix U E RJxIn is denoted by X xn U, and
is defined elementwise as
The first step of our method consists in the construc-
tion of a latent factor model for nouns, based on their
context words. For this purpose, we make use of non-
negative matrix factorization (Lee and Seung, 2000).
Non-negative matrix factorization (NMF) minimizes
an objective function – in our case the Kullback-
Leibler (KL) divergence – between an original matrix
VIxJ and WIxKHKxJ (the matrix multiplication of
matrices W and H) subject to the constraint that all
values in the three matrices be non-negative. Param-
eter K is set « I,J so that a reduction is obtained
over the original data. The factorization model is
represented graphically in figure 1.
context words k context words
</bodyText>
<equation confidence="0.844795">
V W k H
nouns = nouns x
</equation>
<figureCaption confidence="0.954007">
Figure 1: Graphical representation of NMF
</figureCaption>
<equation confidence="0.91485325">
In
(X xn U)i1...in−1 jin+1...iN = E xi1i2...iNujin (1)
in=1
The Kronecker product of matrices A E RIxJ and
B E RKxL is denoted by A®B. The result is a matrix
of size (IK) x (JL), and is defined by
�a11B a12B ··· a1JB
� �a21B a22B ··· a2JB
� ...
� . .. ....
. .
aI1B aI2B ... aIJB
</equation>
<footnote confidence="0.872236">
1In this research, we limit ourselves to three-way co-
occurrences of verbs, subject, and objects, modelled using a
three-mode tensor.
</footnote>
<bodyText confidence="0.9964082">
NMF can be computed fairly straightforwardly,
alternating between the two iterative update rules
represented in equations 3 and 4. The update rules
are guaranteed to converge to a local minimum in the
KL divergence.
</bodyText>
<figure confidence="0.907554833333333">
Viµ
EiWia(WH)iµ
Ek Wka
Viµ
Eµ Haµ(WH)iµ
Ev Hav
</figure>
<subsectionHeader confidence="0.999298">
3.3 Modeling multi-way interactions
</subsectionHeader>
<bodyText confidence="0.998873">
In our second step, we construct a multi-way interac-
tion model for subject verb object (svo) triples, based
</bodyText>
<figure confidence="0.699951666666667">
A®B =
Haµ I— Haµ
(2) Wia I— Wia
</figure>
<page confidence="0.975793">
1144
</page>
<bodyText confidence="0.999996571428571">
on the latent factors induced in the first step. Our
latent interaction model is inspired by a tensor factor-
ization model called Tucker decomposition (Tucker,
1966), although our own model instantiation differs
significantly. In order to explain our method, we
first revisit Tucker decomposition, and subsequently
explain how our model is constructed.
</bodyText>
<subsectionHeader confidence="0.746471">
3.3.1 Tucker decomposition
</subsectionHeader>
<bodyText confidence="0.9994666">
Tucker decomposition is a multilinear generaliza-
tion of the well-known singular value decomposition,
used in Latent Semantic Analysis. It is also known as
higher order singular value decomposition (HOSVD,
De Lathauwer et al. (2000)). In Tucker decomposi-
tion, a tensor is decomposed into a core tensor, multi-
plied by a matrix along each mode. For a three-mode
tensor X E RIXJXL, the model is defined as
Setting P,Q,R « I,J,L, the core tensor G repre-
sents a compressed, latent version of the original ten-
sor X; matrices A E RIXP, B E RJXQ, and C E RLXR
represent the latent factors for each mode, while
G E RPXQXR indicates the level of interaction be-
tween the different latent factors. Figure 2 shows a
graphical representation of Tucker decomposition.2
</bodyText>
<figureCaption confidence="0.9550335">
Figure 2: A graphical representation of Tucker decompo-
sition
</figureCaption>
<footnote confidence="0.8789715">
2where P = Q = R = K, i.e. the same number of latent factors
K is used for each mode
</footnote>
<subsectionHeader confidence="0.709108">
3.3.2 Reconstructing a Tucker model from
two-way factors
</subsectionHeader>
<bodyText confidence="0.999889666666667">
Computing the Tucker decomposition of a tensor
is rather costly in terms of time and memory require-
ments. Moreover, the decomposition is not unique:
the core tensor G can be modified without affecting
the model’s fit by applying the inverse modification
to the factor matrices. These two drawbacks led us
to consider an alternative method for the construc-
tion of the Tucker model. Specifically, we consider
the factor matrices as given (as the output from our
first step), and proceed to compute the core tensor G.
Additionally, we do not use a latent representation
for the first mode, which means that the first mode is
represented by its original instances.
Our model can be straightforwardly applied to lan-
guage data. The core tensor G models the latent
interactions between verbs, subject, and objects. G
is computed by applying the n-mode product to the
appropriate mode of the original tensor (equation 7),
</bodyText>
<equation confidence="0.993996">
G = XX2 WT X3 WT (7)
</equation>
<bodyText confidence="0.999980666666667">
where XVXNXN is our original data tensor, consisting
of the weighted co-occurrence frequencies of svo
triples (extracted from corpus data), and WNXK is
our latent factor matrix for nouns. Note that we do
not use a latent representation for the verb mode. To
be able to efficiently compute the similarity of verbs
(both within and outside of compositional phrases),
only the subject and object mode are represented by
latent factors, while the verb mode is represented
by its original instances. This means that our core
tensor G will be of size V X K X K.3 A graphical
representation is given in figure 3.
Note that both tensor X and factor matrices W are
non-negative, which means our core tensor G will
also be non-negative.
</bodyText>
<subsectionHeader confidence="0.980178">
3.4 The composition of svo triples
</subsectionHeader>
<bodyText confidence="0.760987375">
In order to compute the composition of a particular
subject verb object triple (s,v,o), we first extract the
appropriate subject vector ws and object vector wo
(both of length K) from our factor matrix W, and
3It is straightforward to also construct a latent factor model
for verbs using NMF, and include it in the construction of our
core tensor; we believe such a model might have interesting
applications, but we save this as an exploration for future work.
</bodyText>
<equation confidence="0.959180181818182">
X = GX1 AX2 BX3 C (5)
gpqrap o bq o cr (6)
=
R
E
r=1
Q
E
q=1
P
E
p=1
subjects
subjects
k
=
k
k
verbs
k
k
verbs
</equation>
<page confidence="0.949303">
1145
</page>
<figureCaption confidence="0.996834">
Figure 3: A graphical representation of our model instan-
tiation without the latent verb mode
</figureCaption>
<bodyText confidence="0.9944095">
compute the outer product of both vectors, resulting
in a matrix Y of size K x K.
</bodyText>
<equation confidence="0.997082">
Y = ws o wo (8)
</equation>
<bodyText confidence="0.999626166666667">
Our second and final step is then to weight the
original verb matrix Gv of latent interactions (the
appropriate verb slice of tensor 9) with matrix Y,
containing the latent interactions of the specific sub-
ject and object. This is carried out by taking the
Hadamard product of Gv and Y.
</bodyText>
<equation confidence="0.994463">
Z = Gv *Y (9)
</equation>
<sectionHeader confidence="0.997847" genericHeader="method">
4 Example
</sectionHeader>
<bodyText confidence="0.997891833333333">
In this section, we present a number of example com-
putations that clarify how our model is able to capture
compositionality. All examples come from actual cor-
pus data, and are computed in a fully automatic and
unsupervised way.
Consider the following two sentences:
</bodyText>
<listItem confidence="0.973731">
(1) The athlete runs a race.
(2) The user runs a command.
</listItem>
<bodyText confidence="0.988568263157895">
Both sentences contain the verb run, but they rep-
resent clearly different actions. When we compute
the composition of both instances of run with their
respective subject and object, we want our model to
show this difference.
To compute the compositional representation of
sentences (1) and (2), we proceed as follows. First,
we extract the latent vectors for subject and object
(wathlete and wrace for the first sentence, wuser and
wcommand for the second sentence) from matrix W.
Next, we compute the outer product of subject and
object – wathlete o wrace and wuser o wcommand – which
yields matrices Y(athlete,race) and Y(user,command). By
virtue of the outer product, the matrices Y – of size
K xK – represent the level of interaction between the
latent factors of the subject and the latent factors of
the object. We can inspect these interactions by look-
ing up the factor pairs (i.e. matrix cells) with the high-
est values in the matrices Y. Table 1 presents the fac-
tor pairs with highest value for matrix Y(athlete,race);
table 2 represents the factor pairs with highest value
for matrix Y(user,command). In order to render the fac-
tors interpretable, we include the three most salient
words for the various factors (i.e. the words with the
highest value for a particular factor).
The examples in tables 1 and 2 give an impression
of the effect of the outer product: semantic features
of the subject combine with semantic features of the
object, indicating the extent to which these features
interact within the expression. In table 1, we notice
that animacy features (28, 195) and a sport feature
(25) combine with a ‘sport event’ feature (119). In
table 2, we see that similar animacy features (40,
195) and technological features (7, 45) combine with
another technological feature (89).
Similarly, we can inspect the latent interactions of
the verb run, which are represented in the tensor slice
Grun. Note that this matrix contains the verb seman-
tics computed over the complete corpus. The most
salient factor interactions for Grun are represented in
table 3.
Table 3 illustrates that different senses of the verb
run are represented within the matrix Grun. The first
two factor pairs hint at the ‘organize’ sense of the
verb (run a seminar). The third factor pair repre-
sents the ‘transport’ sense of the verb (the bus runs
every hour).4 And the fourth factor pair represents
the ‘execute’ or ‘deploy’ sense of run (run Linux,
run a computer program). Note that we only show
the factor pairs with the highest value; matrix G con-
tains a value for each pairwise combination of the
latent factors, effectively representing a rich latent
semantics for the verb in question.
The last step is to take the Hadamard product of
matrices Y with verb matrix G, which yields our final
4Obviously, hour is not an object of the verb, but due to
parsing errors it is thus represented.
</bodyText>
<figure confidence="0.94073975">
subjects
k
subjects
k
=
k
verbs
verbs
</figure>
<page confidence="0.952646">
1146
</page>
<table confidence="0.949343666666667">
factors subject object value
(195,119) people (.008), child (.008), adolescent (.007) cup (.007), championship (.006), final (.005) .007
(25,119) hockey (.007), poker (.007), tennis (.006) cup (.007), championship (.006), final (.005) .004
(90,119) professionalism (.007), teamwork (.007), confi- cup (.007), championship (.006), final (.005) .003
dence (.006)
(28,119) they (.004), pupil (.003), participant (.003) cup (.007), championship (.006), final (.005) .003
</table>
<tableCaption confidence="0.995603">
Table 1: Factor pairs with highest value for matrix Y(athlete,race)
</tableCaption>
<table confidence="0.9496258">
factors subject object value
(7,89) password (.009), login (.007), username (.007) filename (.007), null (.006), integer (.006) .010
(40,89) anyone (.004), reader (.004), anybody (.003) filename (.007), null (.006), integer (.006) .007
(195,89) people (.008), child (.008), adolescent (.007) filename (.007), null (.006), integer (.006) .006
(45,89) website (.004), Click (.003), site (.003) filename (.007), null (.006), integer (.006) .006
</table>
<tableCaption confidence="0.999492">
Table 2: Factor pairs with highest value for matrix Y(user,command)
</tableCaption>
<bodyText confidence="0.99984636">
matrices, Zrun,(athlete,race) and Zrun (user, command) . The
Hadamard product will act as a bidirectional filter
on the semantics of both the verb and its subject
and object: interactions of semantic features that are
present in both matrix Y and G will be highlighted,
while the other interactions are played down. The
result is a representation of the verb’s semantics tuned
to its particular subject-object combination. Note that
this final step can be viewed as an instance of function
application (Baroni and Zamparelli, 2010). Also
note the similarity to Grefenstette and Sadrzadeh’s
(2011a,2011b) approach, who equally make use of
the elementwise matrix product in order to weight
the semantics of the verb.
We can now go back to our original tensor 9, and
compute the most similar verbs (i.e. the most similar
tensor slices) for our newly computed matrices Z.5
If we do this for matrix Zrun (athlete,race), our model
comes up with verbs finish (.29), attend (.27), and
win (.25). If, instead, we compute the most similar
verbs for Zrun,(user,command), our model yields execute
(.42), modify (.40), invoke (.39).
Finally, note that the design of our model natu-
rally takes into account word order. Consider the
following two sentences:
</bodyText>
<listItem confidence="0.99392">
(3) man damages car
(4) car damages man
</listItem>
<footnote confidence="0.675462">
5Similarity is calculated by measuring the cosine of the vec-
torized and normalized representation of the verb matrices.
</footnote>
<bodyText confidence="0.998089777777778">
Both sentences contain the exact same words, but the
process of damaging described in sentences (3) and
(4) is of a rather different nature. Our model is able
to take this difference into account: if we compute
Zdamage,(man,car) following sentence (3), our model
yields crash (.43), drive (.35), ride (.35) as most sim-
ilar verbs. If we do the same for Zdamage,(car,man) fol-
lowing sentence (4), our model instead yields scare
(.26), kill (.23), hurt (.23).
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.847358">
5.1 Methodology
</subsectionHeader>
<bodyText confidence="0.9998900625">
In order to evaluate the performance of our tensor-
based factorization model of compositionality, we
make use of the sentence similarity task for transi-
tive sentences, defined in Grefenstette and Sadrzadeh
(2011a). This is an extension of the similarity task
for compositional models developed by Mitchell and
Lapata (2008), and constructed according to the same
guidelines. The dataset contains 2500 similarity
judgements, provided by 25 participants, and is pub-
licly available.6
The data consists of transitive verbs, each paired
with both a subject and an object noun – thus form-
ing a small transitive sentence. Additionally, a ‘land-
mark’ verb is provided. The idea is to compose both
the target verb and the landmark verb with subject
and noun, in order to form two small compositional
</bodyText>
<footnote confidence="0.9966715">
6http://www.cs.ox.ac.uk/activities/
CompDistMeaning/GS2011data.txt
</footnote>
<page confidence="0.986038">
1147
</page>
<table confidence="0.973931142857143">
factors subject object value
(128,181) Mathematics (.004), Science (.004), Economics course (.005), tutorial (.005), seminar (.005) .058
(.004)
(293,181) organization (.007), association (.007), federa- course (.005), tutorial (.005), seminar (.005) .053
tion (.006)
(60,140) rail (.011), bus (.009), ferry (.008) third (.004), decade (.004), hour (.004) .038
(268,268) API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038
</table>
<tableCaption confidence="0.999904">
Table 3: Factor combinations for Grun
</tableCaption>
<bodyText confidence="0.885918333333333">
phrases. The system is then required to come up with
a suitable similarity score for these phrases. The cor-
relation of the model’s judgements with human judge-
ments (scored 1–7) is then calculated using Spear-
man’s p. Two examples of the task are provided in
table 4.
</bodyText>
<table confidence="0.989925">
p target subject object landmark sim
19 meet system criterion visit 1
21 write student name spell 6
</table>
<tableCaption confidence="0.984016">
Table 4: Two example judgements from the phrase simi-
larity task defined by Grefenstette and Sadrzadeh (2011a)
</tableCaption>
<bodyText confidence="0.999798444444445">
Grefenstette and Sadrzadeh (2011a) seem to cal-
culate the similarity score contextualizing both the
target verb and the landmark verb. Another possibil-
ity is to contextualize only the target verb, and com-
pute the similarity score with the non-contextualized
landmark verb. In our view, the latter option pro-
vides a better assessment of the model’s similar-
ity judgements, since contextualizing low-similarity
landmarks often yields non-sensical phrases (e.g. sys-
tem visits criterion). We provide scores for both
contextualized and non-contextualized landmarks.
We compare our results to a number of different
models. The first is Mitchell and Lapata’s (2008)
model, which computes the elementwise vector mul-
tiplication of verb, subject and object. The second
is Grefenstette and Sadrzadeh’s (2011b) best scoring
model instantiation of the categorical distributional
compositional model (Coecke et al., 2010). This
model computes the outer product of the subject and
object vector, the outer product of the verb vector
with itself, and finally the elementwise product of
both results. It yields the best score on the transitive
sentence similarity task reported to date.
As a baseline, we compute the non-contextualized
similarity score for target verb and landmark. The up-
per bound is provided by Grefenstette and Sadrzadeh
(2011a), based on interannotator agreement.
</bodyText>
<subsectionHeader confidence="0.998245">
5.2 Implementational details
</subsectionHeader>
<bodyText confidence="0.999875333333333">
All models have been constructed using the UKWAC
corpus (Baroni et al., 2009), a 2 billion word corpus
automatically harvested from the web. From this data,
we accumulate the input matrix V for our first NMF
step. We use the 10K most frequent nouns, cross-
classified by the 2K most frequent context words.7
Matrix V is weighted using pointwise mutual infor-
mation (PMI, Church and Hanks (1990)).
A parsed version of the corpus is available, which
has been parsed with MaltParser (Nivre et al., 2006).
We use this version in order to extract our svo triples.
From these triples, we construct our tensor SIC, using
1K verbs x 10K subjects x 10K objects. Note once
again that the subject and object instances in the sec-
ond step are exactly the same as the noun instances
in the first step. Tensor SIC has been weighted using a
three-way extension of PMI, following equation 10
(Van de Cruys, 2011).
</bodyText>
<equation confidence="0.995235">
pmi3(x,y,z) = log p(x,y,z) (10)
p(x)p(y)p(z)
</equation>
<bodyText confidence="0.99992175">
We set K = 300 as our number of latent factors.
The value was chosen as a trade-off between a model
that is both rich enough, and does not require an
excessive amount of memory (for the modeling of
the core tensor). The algorithm runs fairly effi-
ciently. Each NMF step is computed in a matter of
seconds, with convergence after 50–100 iterations.
The construction of the core tensor is somewhat more
</bodyText>
<footnote confidence="0.818532">
7We use a context window of 5 words, both before and after
the target word; a stop list was used to filter out grammatical
function words.
</footnote>
<page confidence="0.993262">
1148
</page>
<bodyText confidence="0.9994626">
evolved, but does not exceed a wall time of 30 min-
utes. Results have been computed on a machine with
Intel Xeon 2.93Ghz CPU and 32GB of RAM.
In our experience, PMI performs better than weight-
ing with conditional probabilities.9
</bodyText>
<sectionHeader confidence="0.997848" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<subsectionHeader confidence="0.985595">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.998792">
The results of the various models are presented in ta-
ble 5; multiplicative represents Mitchell and Lapata’s
(2008) multiplicative model, categorical represents
Grefenstette and Sadrzadeh’s (2011b) model, and
latent represents the model presented in this paper.
</bodyText>
<table confidence="0.994982833333333">
model contextualized non-contextualized
baseline .23
multiplicative .32 .34
categorical .32 .35
latent .32 .37
upper bound .62
</table>
<tableCaption confidence="0.984605">
Table 5: Results of the different compositionality models
on the phrase similarity task
</tableCaption>
<bodyText confidence="0.995505358208955">
In the contextualized version of the similarity task
(in which the landmark is combined with subject
and object), all three models obtain the same result
(.32). However, in the non-contextualized version
(in which only the target verb is combined with sub-
ject and object), the models differ in performance.
These differences are statistically significant.8 As
mentioned before, we believe the non-contextualized
version of the task gives a better impression of the
systems’ ability to capture compositionality. The
contextualization of the landmark verb often yields
non-sensical combinations, such as system visits crite-
rion. We therefore deem it preferable to compute the
similarity of the target verb in composition (system
meets criterion) to the non-contextualized semantics
of the landmark verb (visit).
Note that the scores presented in this evalua-
tion (including the baseline score) are significantly
higher than the scores presented in Grefenstette and
Sadrzadeh (2011b). This is not surprising, since the
corpus we use – UKWAC – is an order of magni-
tude larger than the corpus used in their research –
the British National Corpus (BNC). Presumably, the
scores are also favoured by our weighting measure.
8p &lt; 0.01; model differences have been tested using stratified
shuffling (Yeh, 2000).
In this paper, we presented a novel method for the
computation of compositionality within a distribu-
tional framework. The key idea is that composition-
ality is modeled as a multi-way interaction between
latent factors, which are automatically constructed
from corpus data. We used our method to model
the composition of subject verb object combinations.
The method consists of two steps. First, we com-
pute a latent factor model for nouns from standard
co-occurrence data. Next, the latent factors are used
to induce a latent model of three-way subject verb
object interactions, represented by a core tensor. Our
model has been evaluated on a similarity task for tran-
sitive phrases, in which it matches and even exceeds
the state of the art.
We conclude with a number of future work issues.
First of all, we would like to extend our framework in
order to incorporate more compositional phenomena.
Our current model is designed to deal with the latent
modeling of subject verb object combinations. We
would like to investigate how other compositional
phenomena might fit within our latent interaction
framework, and how our model is able to tackle the
computation of compositionality across a differing
number of modes.
Secondly, we would like to further explore the
possibilities of our model in which all three modes
are represented by latent factors. The instantiation
of our model presented in this paper has two latent
modes, using the original instances of the verb mode
in order to efficiently compute verb similarity. We
think a full-blown latent interaction model might
prove to have interesting applications in a number of
NLP tasks, such as the paraphrasing of compositional
expressions.
Finally, we would like to test our method using a
number of different evaluation frameworks. We think
tasks of similarity judgement have their merits, but in
a way are also somewhat limited. In our opinion, re-
search on the modeling of compositional phenomena
within a distributional framework would substantially
</bodyText>
<footnote confidence="0.984037">
9Contrary to the findings of Mitchell and Lapata (2008), who
report a high correlation with human similarity judgements.
</footnote>
<page confidence="0.996369">
1149
</page>
<bodyText confidence="0.999969">
benefit from new evaluation frameworks. In particu-
lar, we think of a lexical substitution or paraphrasing
task along the lines of McCarthy and Navigli (2009),
but specifically aimed at the assessment of composi-
tional phenomena.
</bodyText>
<sectionHeader confidence="0.995668" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.64003425">
Tim Van de Cruys and Thierry Poibeau are supported
by the Centre National de la Recherche Scientifique
(CNRS, France), Anna Korhonen is supported by the
Royal Society (UK).
</reference>
<sectionHeader confidence="0.969818" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999314561797753">
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab ten-
sor toolbox version 2.5. http://www.sandia.gov/
-tgkolda/TensorToolbox/.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1183–1193, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evaluation,
43(3):209–226.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic compo-
sition. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546–556, Jeju Island, Korea, July. Association
for Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. The Journal of Ma-
chine Learning Research, 3:993–1022.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information &amp; lexicography.
Computational Linguistics, 16(1):22–29.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In Pro-
ceedings of the AAAI Spring Symposium on Quantum
Interaction, pages 52–55.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.
2010. Mathematical foundations for a compositional
distributed model of meaning. Lambek Festschrift, Lin-
guistic Analysis, vol. 36, 36.
Lieven De Lathauwer, Bart De Moor, and Joseph Vande-
walle. 2000. A multilinear singular value decomposi-
tion. SIAM Journal on Matrix Analysis and Applica-
tions, 21(4):1253–1278.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162–1172, Cambridge,
MA, October.
Katrin Erk and Sebastian Pad´o. 2008. A structured vector
space model for word meaning in context. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 897–906, Waikiki,
Hawaii, USA.
Katrin Erk and Sebastian Pad´o. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics, pages
57–65, Athens, Greece.
Gottlob Frege. 1892. ¨Uber Sinn und Bedeutung.
Zeitschrift f¨ur Philosophie und philosophische Kritik,
100:25–50.
Eugenie Giesbrecht. 2010. Towards a matrix-based dis-
tributional model of meaning. In Proceedings of the
NAACL HLT 2010 Student Research Workshop, pages
23–28. Association for Computational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 1394–1404, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b.
Experimenting with transitive verbs in a discocat. In
Proceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
62–66, Edinburgh, UK, July. Association for Computa-
tional Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Richard A Harshman and Margaret E Lundy. 1994.
Parafac: Parallel factor analysis. Computational Statis-
tics &amp; Data Analysis, 18(1):39–72.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455–500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor
decompositions for multi-aspect data mining. In ICDM
2008: Proceedings of the 8th IEEE International Con-
ference on Data Mining, pages 363–372, December.
Thomas Landauer and Susan Dumais. 1997. A solution
to Plato’s problem: The Latent Semantic Analysis the-
</reference>
<page confidence="0.810571">
1150
</page>
<reference confidence="0.99961029787234">
ory of the acquisition, induction, and representation of
knowledge. Psychology Review, 104:211–240.
Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms
for non-negative matrix factorization. In Advances in
Neural Information Processing Systems 13, pages 556–
562.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language resources and
evaluation, 43(2):139–159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. proceedings of ACL-
08: HLT, pages 236–244.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of LREC-2006, pages 2216–
2219.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings
of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Nat-
ural Language Learning, pages 1201–1211, Jeju Island,
Korea, July. Association for Computational Linguistics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009.
Ranking paraphrases in context. In Proceedings of the
2009 Workshop on Applied Textual Inference, pages
44–47, Suntec, Singapore.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings of
the 48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 948–957, Uppsala, Sweden.
Ledyard R. Tucker. 1966. Some mathematical notes on
three-mode factor analysis. Psychometrika, 31(3):279–
311.
Tim Van de Cruys. 2010. A non-negative tensor fac-
torization model for selectional preference induction.
Natural Language Engineering, 16(4):417–437.
Tim Van de Cruys. 2011. Two multivariate generaliza-
tions of pointwise mutual information. In Proceedings
of the Workshop on Distributional Semantics and Com-
positionality, pages 16–20, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics,
pages 947–953, Saarbr¨ucken, Germany.
</reference>
<page confidence="0.994554">
1151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.147153">
<title confidence="0.999929">A Tensor-based Factorization Model of Semantic Compositionality</title>
<author confidence="0.999995">Tim Van_de</author>
<affiliation confidence="0.989728">IRIT – UMR</affiliation>
<address confidence="0.822715">Toulouse,</address>
<email confidence="0.997312">tim.vandecruys@irit.fr</email>
<affiliation confidence="0.7939175">LaTTiCe – UMR CNRS &amp;</affiliation>
<address confidence="0.654414">Paris,</address>
<email confidence="0.983745">thierry.poibeau@ens.fr</email>
<author confidence="0.614055">Anna</author>
<affiliation confidence="0.924836">Computer Laboratory &amp; University of United</affiliation>
<email confidence="0.958254">anna.korhonen@cl.cam.ac.uk</email>
<abstract confidence="0.9995455">In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use method to model the composition of subverb object The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce latent model of three-way verb object interactions. Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Tim Van</author>
</authors>
<title>de Cruys and Thierry Poibeau are supported by the Centre National de la Recherche Scientifique (CNRS, France), Anna Korhonen is supported by the Royal</title>
<publisher>Society (UK).</publisher>
<marker>Van, </marker>
<rawString>Tim Van de Cruys and Thierry Poibeau are supported by the Centre National de la Recherche Scientifique (CNRS, France), Anna Korhonen is supported by the Royal Society (UK).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Matlab tensor toolbox version 2.5.</title>
<date>2012</date>
<note>http://www.sandia.gov/ -tgkolda/TensorToolbox/.</note>
<marker>Bader, Kolda, 2012</marker>
<rawString>Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab tensor toolbox version 2.5. http://www.sandia.gov/ -tgkolda/TensorToolbox/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2496" citStr="Baroni and Zamparelli, 2010" startWordPosition="393" endWordPosition="396"> often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the joint composition of a verb with its subject and direct object. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. In order to adequately model the multiway interaction between a verb and its subject and object</context>
<context position="4694" citStr="Baroni and Zamparelli (2010)" startWordPosition="738" endWordPosition="741">sitional phenomena within a distributional framework. One of the first approaches to tackle compositional phenomena in a systematic way is Mitchell and Lapata’s (2008) approach. They explore a number of different models for vector composition, of which vector addition (the sum of each feature) and vector multiplication (the elementwise multiplication of each feature) are the most important. They evaluate their models on a noun-verb phrase similarity task, and find that the multiplicative model yields the best results, along with a weighted combination of the additive and multiplicative model. Baroni and Zamparelli (2010) present a method for the composition of adjectives and nouns. In their model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A numb</context>
<context position="20557" citStr="Baroni and Zamparelli, 2010" startWordPosition="3385" endWordPosition="3388"> (.006), integer (.006) .006 Table 2: Factor pairs with highest value for matrix Y(user,command) matrices, Zrun,(athlete,race) and Zrun (user, command) . The Hadamard product will act as a bidirectional filter on the semantics of both the verb and its subject and object: interactions of semantic features that are present in both matrix Y and G will be highlighted, while the other interactions are played down. The result is a representation of the verb’s semantics tuned to its particular subject-object combination. Note that this final step can be viewed as an instance of function application (Baroni and Zamparelli, 2010). Also note the similarity to Grefenstette and Sadrzadeh’s (2011a,2011b) approach, who equally make use of the elementwise matrix product in order to weight the semantics of the verb. We can now go back to our original tensor 9, and compute the most similar verbs (i.e. the most similar tensor slices) for our newly computed matrices Z.5 If we do this for matrix Zrun (athlete,race), our model comes up with verbs finish (.29), attend (.27), and win (.25). If, instead, we compute the most similar verbs for Zrun,(user,command), our model yields execute (.42), modify (.40), invoke (.39). Finally, no</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="25213" citStr="Baroni et al., 2009" startWordPosition="4104" endWordPosition="4107">tributional compositional model (Coecke et al., 2010). This model computes the outer product of the subject and object vector, the outer product of the verb vector with itself, and finally the elementwise product of both results. It yields the best score on the transitive sentence similarity task reported to date. As a baseline, we compute the non-contextualized similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor SIC, using 1K verbs x 10K subjects x 10K objects. Note once again that the su</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2732" citStr="Blacoe and Lapata, 2012" startWordPosition="434" endWordPosition="437"> that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the joint composition of a verb with its subject and direct object. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. In order to adequately model the multiway interaction between a verb and its subject and objects, a significant part of our method relies on tensor algebra. Additionally, our method makes use of a factorization model appropriate for tensors. The remainder of the paper is structured as follows. In section 2, we give an overview of</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7386" citStr="Blei et al., 2003" startWordPosition="1173" endWordPosition="1176">on (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic Analysis (Landauer and Dumais, 1997), which uses singular value decomposition in order to automatically induce latent factors from term-document matrices. Another well known latent model of meaning, which takes a generative approach, is Latent Dirichlet Allocation (Blei et al., 2003). Tensor factorization has been used before for the modeling of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. Our underlying tensor factorization – Tucker decomposition – is the same as Giesbrecht’s; and similar to Van de Cruys (2010), we construct a latent model of verb, subject, and object interactions. The way our model is construct</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information &amp; lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="25527" citStr="Church and Hanks (1990)" startWordPosition="4158" endWordPosition="4161">ate. As a baseline, we compute the non-contextualized similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor SIC, using 1K verbs x 10K subjects x 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor SIC has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x,y,z) = log p(x,y,z) (10) p(x)p(y)p(z) We set K = 300 as our number of latent factors. The </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth W. Church and Patrick Hanks. 1990. Word association norms, mutual information &amp; lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Quantum Interaction,</booktitle>
<pages>52--55</pages>
<contexts>
<context position="8501" citStr="Clark and Pulman, 2007" startWordPosition="1353" endWordPosition="1356">010), we construct a latent model of verb, subject, and object interactions. The way our model is constructed, however, is significantly different. The former research does not use any syntactic information for the construction of the tensor, while the latter makes use of a more restricted tensor factorization model, viz. parallel factor analysis (Harshman and Lundy, 1994). 1143 The idea of modeling compositionality by means A special case of the Kronecker product is the of tensor (Kronecker) product has been proposed outer product of two vectors a E RI and b E RJ, dein the literature before (Clark and Pulman, 2007; noted a o b. The result is a matrix A E RIxJ obtained Coecke et al., 2010). However, the method presented by multiplying each element of a with each element here is the first that tries to capture compositional of b. phenomena by exploiting the multi-way interactions Finally, the Hadamard product, denoted A * B, between latent factors, induced by a suitable tensor is the elementwise multiplication of two matrices factorization model. A E RIxJ and B E RIxJ, which produces a matrix that is equally of size I x J. 3 Methodology 3.2 The construction of latent noun factors 3.1 Mathematical prelimi</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of the AAAI Spring Symposium on Quantum Interaction, pages 52–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis,</title>
<date>2010</date>
<volume>36</volume>
<pages>36</pages>
<contexts>
<context position="2517" citStr="Coecke et al., 2010" startWordPosition="397" endWordPosition="400">s the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the joint composition of a verb with its subject and direct object. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. In order to adequately model the multiway interaction between a verb and its subject and objects, a significant part</context>
<context position="5084" citStr="Coecke et al. (2010)" startWordPosition="799" endWordPosition="802"> evaluate their models on a noun-verb phrase similarity task, and find that the multiplicative model yields the best results, along with a weighted combination of the additive and multiplicative model. Baroni and Zamparelli (2010) present a method for the composition of adjectives and nouns. In their model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The key idea is that relational words (e.g. adjectives or verbs) have a rich (multidimensional) structure that acts as a filter on their arguments. Our model uses an intuition similar to theirs. Socher et al. (2012) present a model for compositiona</context>
<context position="8577" citStr="Coecke et al., 2010" startWordPosition="1370" endWordPosition="1373">he way our model is constructed, however, is significantly different. The former research does not use any syntactic information for the construction of the tensor, while the latter makes use of a more restricted tensor factorization model, viz. parallel factor analysis (Harshman and Lundy, 1994). 1143 The idea of modeling compositionality by means A special case of the Kronecker product is the of tensor (Kronecker) product has been proposed outer product of two vectors a E RI and b E RJ, dein the literature before (Clark and Pulman, 2007; noted a o b. The result is a matrix A E RIxJ obtained Coecke et al., 2010). However, the method presented by multiplying each element of a with each element here is the first that tries to capture compositional of b. phenomena by exploiting the multi-way interactions Finally, the Hadamard product, denoted A * B, between latent factors, induced by a suitable tensor is the elementwise multiplication of two matrices factorization model. A E RIxJ and B E RIxJ, which produces a matrix that is equally of size I x J. 3 Methodology 3.2 The construction of latent noun factors 3.1 Mathematical preliminaries The methodology presented in this paper requires a number of concepts</context>
<context position="24646" citStr="Coecke et al., 2010" startWordPosition="4016" endWordPosition="4019">the latter option provides a better assessment of the model’s similarity judgements, since contextualizing low-similarity landmarks often yields non-sensical phrases (e.g. system visits criterion). We provide scores for both contextualized and non-contextualized landmarks. We compare our results to a number of different models. The first is Mitchell and Lapata’s (2008) model, which computes the elementwise vector multiplication of verb, subject and object. The second is Grefenstette and Sadrzadeh’s (2011b) best scoring model instantiation of the categorical distributional compositional model (Coecke et al., 2010). This model computes the outer product of the subject and object vector, the outer product of the verb vector with itself, and finally the elementwise product of both results. It yields the best score on the transitive sentence similarity task reported to date. As a baseline, we compute the non-contextualized similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automat</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributed model of meaning. Lambek Festschrift, Linguistic Analysis, vol. 36, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieven De Lathauwer</author>
<author>Bart De Moor</author>
<author>Joseph Vandewalle</author>
</authors>
<title>A multilinear singular value decomposition.</title>
<date>2000</date>
<journal>SIAM Journal on Matrix Analysis and Applications,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>De Lathauwer, De Moor, Vandewalle, 2000</marker>
<rawString>Lieven De Lathauwer, Bart De Moor, and Joseph Vandewalle. 2000. A multilinear singular value decomposition. SIAM Journal on Matrix Analysis and Applications, 21(4):1253–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1162--1172</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="6501" citStr="Dinu and Lapata (2010)" startWordPosition="1032" endWordPosition="1036">ay it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009, 2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Dinu and Lapata use non-negative matrix factorization (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<location>Waikiki, Hawaii, USA.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897–906, Waikiki, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>57--65</pages>
<location>Athens, Greece.</location>
<marker>Erk, Pad´o, 2009</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 57–65, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gottlob Frege</author>
</authors>
<date>1892</date>
<booktitle>Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und philosophische Kritik,</booktitle>
<pages>100--25</pages>
<contexts>
<context position="2075" citStr="Frege, 1892" startWordPosition="330" endWordPosition="331">to a plethora of algorithms that try to capture the semantics of words by looking at their distribution in text. Up till now, however, most work on the automatic acquisition of semantics only deals with individual words. The modeling of meaning beyond the level of individual words – i.e. the combination of words into larger units – is to a large degree left unexplored. The principle of compositionality, often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often se</context>
</contexts>
<marker>Frege, 1892</marker>
<rawString>Gottlob Frege. 1892. ¨Uber Sinn und Bedeutung. Zeitschrift f¨ur Philosophie und philosophische Kritik, 100:25–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Towards a matrix-based distributional model of meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Student Research Workshop,</booktitle>
<pages>23--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7485" citStr="Giesbrecht (2010)" startWordPosition="1189" endWordPosition="1190">different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic Analysis (Landauer and Dumais, 1997), which uses singular value decomposition in order to automatically induce latent factors from term-document matrices. Another well known latent model of meaning, which takes a generative approach, is Latent Dirichlet Allocation (Blei et al., 2003). Tensor factorization has been used before for the modeling of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. Our underlying tensor factorization – Tucker decomposition – is the same as Giesbrecht’s; and similar to Van de Cruys (2010), we construct a latent model of verb, subject, and object interactions. The way our model is constructed, however, is significantly different. The former research does not use any syntactic information</context>
</contexts>
<marker>Giesbrecht, 2010</marker>
<rawString>Eugenie Giesbrecht. 2010. Towards a matrix-based distributional model of meaning. In Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 23–28. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="5393" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="848" endWordPosition="852">heir model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The key idea is that relational words (e.g. adjectives or verbs) have a rich (multidimensional) structure that acts as a filter on their arguments. Our model uses an intuition similar to theirs. Socher et al. (2012) present a model for compositionality based on recursive neural networks. Each node in a parse tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is re</context>
<context position="22123" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3639" endWordPosition="3642"> damaging described in sentences (3) and (4) is of a rather different nature. Our model is able to take this difference into account: if we compute Zdamage,(man,car) following sentence (3), our model yields crash (.43), drive (.35), ride (.35) as most similar verbs. If we do the same for Zdamage,(car,man) following sentence (4), our model instead yields scare (.26), kill (.23), hurt (.23). 5 Evaluation 5.1 Methodology In order to evaluate the performance of our tensorbased factorization model of compositionality, we make use of the sentence similarity task for transitive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is publicly available.6 The data consists of transitive verbs, each paired with both a subject and an object noun – thus forming a small transitive sentence. Additionally, a ‘landmark’ verb is provided. The idea is to compose both the target verb and the landmark verb with subject and noun, in order to form two small compositional 6http://www.cs.ox.ac.</context>
<context position="23738" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3885" endWordPosition="3888">de (.004), hour (.004) .038 (268,268) API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038 Table 3: Factor combinations for Grun phrases. The system is then required to come up with a suitable similarity score for these phrases. The correlation of the model’s judgements with human judgements (scored 1–7) is then calculated using Spearman’s p. Two examples of the task are provided in table 4. p target subject object landmark sim 19 meet system criterion visit 1 21 write student name spell 6 Table 4: Two example judgements from the phrase similarity task defined by Grefenstette and Sadrzadeh (2011a) Grefenstette and Sadrzadeh (2011a) seem to calculate the similarity score contextualizing both the target verb and the landmark verb. Another possibility is to contextualize only the target verb, and compute the similarity score with the non-contextualized landmark verb. In our view, the latter option provides a better assessment of the model’s similarity judgements, since contextualizing low-similarity landmarks often yields non-sensical phrases (e.g. system visits criterion). We provide scores for both contextualized and non-contextualized landmarks. We compare our results to a number of </context>
<context position="25068" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4084" endWordPosition="4087">vector multiplication of verb, subject and object. The second is Grefenstette and Sadrzadeh’s (2011b) best scoring model instantiation of the categorical distributional compositional model (Coecke et al., 2010). This model computes the outer product of the subject and object vector, the outer product of the verb vector with itself, and finally the elementwise product of both results. It yields the best score on the transitive sentence similarity task reported to date. As a baseline, we compute the non-contextualized similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to e</context>
<context position="28325" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4607" endWordPosition="4610">gnificant.8 As mentioned before, we believe the non-contextualized version of the task gives a better impression of the systems’ ability to capture compositionality. The contextualization of the landmark verb often yields non-sensical combinations, such as system visits criterion. We therefore deem it preferable to compute the similarity of the target verb in composition (system meets criterion) to the non-contextualized semantics of the landmark verb (visit). Note that the scores presented in this evaluation (including the baseline score) are significantly higher than the scores presented in Grefenstette and Sadrzadeh (2011b). This is not surprising, since the corpus we use – UKWAC – is an order of magnitude larger than the corpus used in their research – the British National Corpus (BNC). Presumably, the scores are also favoured by our weighting measure. 8p &lt; 0.01; model differences have been tested using stratified shuffling (Yeh, 2000). In this paper, we presented a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We us</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011a. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1394–1404, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a discocat.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>62--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="5393" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="848" endWordPosition="852">heir model, an adjective is a linear function of one vector (the noun vector) to another vector (the vector for the adjective-noun pair). The linear transformation for a particular adjective is represented by a matrix, and is learned automatically from a corpus, using partial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The key idea is that relational words (e.g. adjectives or verbs) have a rich (multidimensional) structure that acts as a filter on their arguments. Our model uses an intuition similar to theirs. Socher et al. (2012) present a model for compositionality based on recursive neural networks. Each node in a parse tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is re</context>
<context position="22123" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3639" endWordPosition="3642"> damaging described in sentences (3) and (4) is of a rather different nature. Our model is able to take this difference into account: if we compute Zdamage,(man,car) following sentence (3), our model yields crash (.43), drive (.35), ride (.35) as most similar verbs. If we do the same for Zdamage,(car,man) following sentence (4), our model instead yields scare (.26), kill (.23), hurt (.23). 5 Evaluation 5.1 Methodology In order to evaluate the performance of our tensorbased factorization model of compositionality, we make use of the sentence similarity task for transitive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is publicly available.6 The data consists of transitive verbs, each paired with both a subject and an object noun – thus forming a small transitive sentence. Additionally, a ‘landmark’ verb is provided. The idea is to compose both the target verb and the landmark verb with subject and noun, in order to form two small compositional 6http://www.cs.ox.ac.</context>
<context position="23738" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="3885" endWordPosition="3888">de (.004), hour (.004) .038 (268,268) API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038 Table 3: Factor combinations for Grun phrases. The system is then required to come up with a suitable similarity score for these phrases. The correlation of the model’s judgements with human judgements (scored 1–7) is then calculated using Spearman’s p. Two examples of the task are provided in table 4. p target subject object landmark sim 19 meet system criterion visit 1 21 write student name spell 6 Table 4: Two example judgements from the phrase similarity task defined by Grefenstette and Sadrzadeh (2011a) Grefenstette and Sadrzadeh (2011a) seem to calculate the similarity score contextualizing both the target verb and the landmark verb. Another possibility is to contextualize only the target verb, and compute the similarity score with the non-contextualized landmark verb. In our view, the latter option provides a better assessment of the model’s similarity judgements, since contextualizing low-similarity landmarks often yields non-sensical phrases (e.g. system visits criterion). We provide scores for both contextualized and non-contextualized landmarks. We compare our results to a number of </context>
<context position="25068" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4084" endWordPosition="4087">vector multiplication of verb, subject and object. The second is Grefenstette and Sadrzadeh’s (2011b) best scoring model instantiation of the categorical distributional compositional model (Coecke et al., 2010). This model computes the outer product of the subject and object vector, the outer product of the verb vector with itself, and finally the elementwise product of both results. It yields the best score on the transitive sentence similarity task reported to date. As a baseline, we compute the non-contextualized similarity score for target verb and landmark. The upper bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to e</context>
<context position="28325" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="4607" endWordPosition="4610">gnificant.8 As mentioned before, we believe the non-contextualized version of the task gives a better impression of the systems’ ability to capture compositionality. The contextualization of the landmark verb often yields non-sensical combinations, such as system visits criterion. We therefore deem it preferable to compute the similarity of the target verb in composition (system meets criterion) to the non-contextualized semantics of the landmark verb (visit). Note that the scores presented in this evaluation (including the baseline score) are significantly higher than the scores presented in Grefenstette and Sadrzadeh (2011b). This is not surprising, since the corpus we use – UKWAC – is an order of magnitude larger than the corpus used in their research – the British National Corpus (BNC). Presumably, the scores are also favoured by our weighting measure. 8p &lt; 0.01; model differences have been tested using stratified shuffling (Yeh, 2000). In this paper, we presented a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We us</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011b. Experimenting with transitive verbs in a discocat. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62–66, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1292" citStr="Harris, 1954" startWordPosition="198" endWordPosition="199">triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art. 1 Introduction In the course of the last two decades, significant progress has been made with regard to the automatic extraction of lexical semantic knowledge from largescale text corpora. Most work relies on the distributional hypothesis of meaning (Harris, 1954), which states that words that appear within the same contexts tend to be semantically similar. A large number of researchers have taken this dictum to heart, giving rise to a plethora of algorithms that try to capture the semantics of words by looking at their distribution in text. Up till now, however, most work on the automatic acquisition of semantics only deals with individual words. The modeling of meaning beyond the level of individual words – i.e. the combination of words into larger units – is to a large degree left unexplored. The principle of compositionality, often attributed to Fr</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Harshman</author>
<author>Margaret E Lundy</author>
</authors>
<title>Parafac: Parallel factor analysis.</title>
<date>1994</date>
<journal>Computational Statistics &amp; Data Analysis,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="8254" citStr="Harshman and Lundy, 1994" startWordPosition="1308" endWordPosition="1311">uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, subjects, and objects. Our underlying tensor factorization – Tucker decomposition – is the same as Giesbrecht’s; and similar to Van de Cruys (2010), we construct a latent model of verb, subject, and object interactions. The way our model is constructed, however, is significantly different. The former research does not use any syntactic information for the construction of the tensor, while the latter makes use of a more restricted tensor factorization model, viz. parallel factor analysis (Harshman and Lundy, 1994). 1143 The idea of modeling compositionality by means A special case of the Kronecker product is the of tensor (Kronecker) product has been proposed outer product of two vectors a E RI and b E RJ, dein the literature before (Clark and Pulman, 2007; noted a o b. The result is a matrix A E RIxJ obtained Coecke et al., 2010). However, the method presented by multiplying each element of a with each element here is the first that tries to capture compositional of b. phenomena by exploiting the multi-way interactions Finally, the Hadamard product, denoted A * B, between latent factors, induced by a </context>
</contexts>
<marker>Harshman, Lundy, 1994</marker>
<rawString>Richard A Harshman and Margaret E Lundy. 1994. Parafac: Parallel factor analysis. Computational Statistics &amp; Data Analysis, 18(1):39–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Brett W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="9330" citStr="Kolda and Bader (2009)" startWordPosition="1490" endWordPosition="1493">ional of b. phenomena by exploiting the multi-way interactions Finally, the Hadamard product, denoted A * B, between latent factors, induced by a suitable tensor is the elementwise multiplication of two matrices factorization model. A E RIxJ and B E RIxJ, which produces a matrix that is equally of size I x J. 3 Methodology 3.2 The construction of latent noun factors 3.1 Mathematical preliminaries The methodology presented in this paper requires a number of concepts and mathematical operations from tensor algebra, which are briefly reviewed in this section. The interested reader is referred to Kolda and Bader (2009) for a more thorough introduction to tensor algebra (including an overview of various factorization methods). A tensor is a multidimensional array; it is the generalization of a matrix to more than two dimensions, or modes. Whereas matrices are only able to capture two-way co-occurrences, tensors are able to capture multi-way co-occurrences.1 Following prevailing convention, tensors are represented by boldface Euler script notation (X), matrices by boldface capital letters (X), vectors by boldface lower case letters (x), and scalars by italic letters (x). The n-mode product of a tensor X E RI1</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>Tamara G. Kolda and Brett W. Bader. 2009. Tensor decompositions and applications. SIAM Review, 51(3):455–500, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara G Kolda</author>
<author>Jimeng Sun</author>
</authors>
<title>Scalable tensor decompositions for multi-aspect data mining.</title>
<date>2008</date>
<booktitle>In ICDM 2008: Proceedings of the 8th IEEE International Conference on Data Mining,</booktitle>
<pages>363--372</pages>
<marker>Kolda, Sun, 2008</marker>
<rawString>Tamara G. Kolda and Jimeng Sun. 2008. Scalable tensor decompositions for multi-aspect data mining. In ICDM 2008: Proceedings of the 8th IEEE International Conference on Data Mining, pages 363–372, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychology Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="7138" citStr="Landauer and Dumais, 1997" startWordPosition="1136" endWordPosition="1139">probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Dinu and Lapata use non-negative matrix factorization (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In general, latent models have proven to be useful for the modeling of word meaning. One of the best known latent models of semantics is Latent Semantic Analysis (Landauer and Dumais, 1997), which uses singular value decomposition in order to automatically induce latent factors from term-document matrices. Another well known latent model of meaning, which takes a generative approach, is Latent Dirichlet Allocation (Blei et al., 2003). Tensor factorization has been used before for the modeling of natural language. Giesbrecht (2010) describes a tensor factorization model for the construction of a distributional model that is sensitive to word order. And Van de Cruys (2010) uses a tensor factorization model in order to construct a three-way selectional preference model of verbs, su</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychology Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 13,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="10228" citStr="Lee and Seung, 2000" startWordPosition="1641" endWordPosition="1644">ccurrences, tensors are able to capture multi-way co-occurrences.1 Following prevailing convention, tensors are represented by boldface Euler script notation (X), matrices by boldface capital letters (X), vectors by boldface lower case letters (x), and scalars by italic letters (x). The n-mode product of a tensor X E RI1xI2x...xIN with a matrix U E RJxIn is denoted by X xn U, and is defined elementwise as The first step of our method consists in the construction of a latent factor model for nouns, based on their context words. For this purpose, we make use of nonnegative matrix factorization (Lee and Seung, 2000). Non-negative matrix factorization (NMF) minimizes an objective function – in our case the KullbackLeibler (KL) divergence – between an original matrix VIxJ and WIxKHKxJ (the matrix multiplication of matrices W and H) subject to the constraint that all values in the three matrices be non-negative. Parameter K is set « I,J so that a reduction is obtained over the original data. The factorization model is represented graphically in figure 1. context words k context words V W k H nouns = nouns x Figure 1: Graphical representation of NMF In (X xn U)i1...in−1 jin+1...iN = E xi1i2...iNujin (1) in=1</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, pages 556– 562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--2</pages>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language resources and evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition. proceedings of ACL08: HLT,</title>
<date>2008</date>
<pages>236--244</pages>
<contexts>
<context position="2467" citStr="Mitchell and Lapata, 2008" startWordPosition="389" endWordPosition="392">nciple of compositionality, often attributed to Frege, is the principle that states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the joint composition of a verb with its subject and direct object. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. In order to adequately model the multiway interaction between a ve</context>
<context position="22235" citStr="Mitchell and Lapata (2008)" startWordPosition="3656" endWordPosition="3659">e into account: if we compute Zdamage,(man,car) following sentence (3), our model yields crash (.43), drive (.35), ride (.35) as most similar verbs. If we do the same for Zdamage,(car,man) following sentence (4), our model instead yields scare (.26), kill (.23), hurt (.23). 5 Evaluation 5.1 Methodology In order to evaluate the performance of our tensorbased factorization model of compositionality, we make use of the sentence similarity task for transitive sentences, defined in Grefenstette and Sadrzadeh (2011a). This is an extension of the similarity task for compositional models developed by Mitchell and Lapata (2008), and constructed according to the same guidelines. The dataset contains 2500 similarity judgements, provided by 25 participants, and is publicly available.6 The data consists of transitive verbs, each paired with both a subject and an object noun – thus forming a small transitive sentence. Additionally, a ‘landmark’ verb is provided. The idea is to compose both the target verb and the landmark verb with subject and noun, in order to form two small compositional 6http://www.cs.ox.ac.uk/activities/ CompDistMeaning/GS2011data.txt 1147 factors subject object value (128,181) Mathematics (.004), Sc</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. proceedings of ACL08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="25633" citStr="Nivre et al., 2006" startWordPosition="4176" endWordPosition="4179"> bound is provided by Grefenstette and Sadrzadeh (2011a), based on interannotator agreement. 5.2 Implementational details All models have been constructed using the UKWAC corpus (Baroni et al., 2009), a 2 billion word corpus automatically harvested from the web. From this data, we accumulate the input matrix V for our first NMF step. We use the 10K most frequent nouns, crossclassified by the 2K most frequent context words.7 Matrix V is weighted using pointwise mutual information (PMI, Church and Hanks (1990)). A parsed version of the corpus is available, which has been parsed with MaltParser (Nivre et al., 2006). We use this version in order to extract our svo triples. From these triples, we construct our tensor SIC, using 1K verbs x 10K subjects x 10K objects. Note once again that the subject and object instances in the second step are exactly the same as the noun instances in the first step. Tensor SIC has been weighted using a three-way extension of PMI, following equation 10 (Van de Cruys, 2011). pmi3(x,y,z) = log p(x,y,z) (10) p(x)p(y)p(z) We set K = 300 as our number of latent factors. The value was chosen as a trade-off between a model that is both rich enough, and does not require an excessiv</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC-2006, pages 2216– 2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2539" citStr="Socher et al., 2012" startWordPosition="401" endWordPosition="404">states that the meaning of a complex expression is a function of the meaning of its parts and the way those parts are (syntactically) combined (Frege, 1892). It is the fundamental principle that allows language users to understand the meaning of sentences they have never heard before, by constructing the meaning of the complex expression from the meanings of the individual words. Recently, a number of researchers have tried to reconcile the framework of distributional semantics with the principle of compositionality (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Coecke et al., 2010; Socher et al., 2012). However, the absolute gains of the systems remain a bit unclear, and a simple method of composition – vector multiplication – often seems to produce the best results (Blacoe and Lapata, 2012). In this paper, we present a novel method for the joint composition of a verb with its subject and direct object. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. In order to adequately model the multiway interaction between a verb and its subject and objects, a significant part of our method relies </context>
<context position="5651" citStr="Socher et al. (2012)" startWordPosition="891" endWordPosition="894">artial least-squares regression. Coecke et al. (2010) present an abstract theoretical framework in which a sentence vector is a function of the Kronecker product of its word vectors, which allows for greater interaction between the different word features. A number of instantiations of the framework are tested experimentally in Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The key idea is that relational words (e.g. adjectives or verbs) have a rich (multidimensional) structure that acts as a filter on their arguments. Our model uses an intuition similar to theirs. Socher et al. (2012) present a model for compositionality based on recursive neural networks. Each node in a parse tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vecto</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Georgiana Dinu</author>
<author>Manfred Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6353" citStr="Thater et al. (2009" startWordPosition="1011" endWordPosition="1014"> in a parse tree is assigned both a vector and a matrix; the vector captures the actual meaning of the constituent, while the matrix models the way it changes the meaning of neighbouring words and phrases. Closely related to the work on compositionality is research on the computation of word meaning in context. Erk and Pad´o (2008, 2009) make use of selectional preferences to express the meaning of a word in context; the meaning of a word in the presence of an argument is computed by multiplying the word’s vector with a vector that captures the inverse selectional preferences of the argument. Thater et al. (2009, 2010) extend the approach based on selectional preferences by incorporating second-order co-occurrences in their model. And Dinu and Lapata (2010) propose a probabilistic framework that models the meaning of words as a probability distribution over latent factors. This allows them to model contextualized meaning as a change in the original sense distribution. Dinu and Lapata use non-negative matrix factorization (NMF) to induce latent factors. Similar to their work, our model uses NMF – albeit in a slightly different configuration – as a first step towards our final factorization model. In g</context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44–47, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ledyard R Tucker</author>
</authors>
<title>Some mathematical notes on three-mode factor analysis.</title>
<date>1966</date>
<journal>Psychometrika,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>311</pages>
<contexts>
<context position="11790" citStr="Tucker, 1966" startWordPosition="1916" endWordPosition="1917">sor. NMF can be computed fairly straightforwardly, alternating between the two iterative update rules represented in equations 3 and 4. The update rules are guaranteed to converge to a local minimum in the KL divergence. Viµ EiWia(WH)iµ Ek Wka Viµ Eµ Haµ(WH)iµ Ev Hav 3.3 Modeling multi-way interactions In our second step, we construct a multi-way interaction model for subject verb object (svo) triples, based A®B = Haµ I— Haµ (2) Wia I— Wia 1144 on the latent factors induced in the first step. Our latent interaction model is inspired by a tensor factorization model called Tucker decomposition (Tucker, 1966), although our own model instantiation differs significantly. In order to explain our method, we first revisit Tucker decomposition, and subsequently explain how our model is constructed. 3.3.1 Tucker decomposition Tucker decomposition is a multilinear generalization of the well-known singular value decomposition, used in Latent Semantic Analysis. It is also known as higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). In Tucker decomposition, a tensor is decomposed into a core tensor, multiplied by a matrix along each mode. For a three-mode tensor X E RIXJXL, the mod</context>
</contexts>
<marker>Tucker, 1966</marker>
<rawString>Ledyard R. Tucker. 1966. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A non-negative tensor factorization model for selectional preference induction.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<marker>Van de Cruys, 2010</marker>
<rawString>Tim Van de Cruys. 2010. A non-negative tensor factorization model for selectional preference induction. Natural Language Engineering, 16(4):417–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>Two multivariate generalizations of pointwise mutual information.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>16--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Van de Cruys, 2011</marker>
<rawString>Tim Van de Cruys. 2011. Two multivariate generalizations of pointwise mutual information. In Proceedings of the Workshop on Distributional Semantics and Compositionality, pages 16–20, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="28646" citStr="Yeh, 2000" startWordPosition="4664" endWordPosition="4665">the target verb in composition (system meets criterion) to the non-contextualized semantics of the landmark verb (visit). Note that the scores presented in this evaluation (including the baseline score) are significantly higher than the scores presented in Grefenstette and Sadrzadeh (2011b). This is not surprising, since the corpus we use – UKWAC – is an order of magnitude larger than the corpus used in their research – the British National Corpus (BNC). Presumably, the scores are also favoured by our weighting measure. 8p &lt; 0.01; model differences have been tested using stratified shuffling (Yeh, 2000). In this paper, we presented a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We used our method to model the composition of subject verb object combinations. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions, represented by a core</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics, pages 947–953, Saarbr¨ucken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>