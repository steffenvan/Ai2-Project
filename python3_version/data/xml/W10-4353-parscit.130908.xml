<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007051">
<title confidence="0.956796">
‘How was your day?’ An affective companion ECA prototype
</title>
<author confidence="0.996512">
Marc Cavazza
</author>
<affiliation confidence="0.9978195">
School of Computing
Teesside University
</affiliation>
<address confidence="0.903946">
Middlesbrough TS1 3BA
</address>
<email confidence="0.999276">
m.o.cavazza@tees.ac.uk
</email>
<note confidence="0.82768">
Raúl Santos de la Cámara
Telefónica I+D
C/ Emilio Vargas 6
</note>
<address confidence="0.634365">
28043 Madrid
</address>
<email confidence="0.994617">
e.rsai@tid.es
</email>
<author confidence="0.966513">
Markku Turunen
</author>
<affiliation confidence="0.979867">
University of Tampere
</affiliation>
<address confidence="0.7391265">
Kanslerinrinne 1
FI-33014
</address>
<email confidence="0.99502">
mturunen@cs.uta.fi
</email>
<author confidence="0.843063">
José Relaño Gil Jaakko Hakulinen Nigel Crook Debora Field
</author>
<affiliation confidence="0.943284">
Telefónica I+D University of Tampere Oxford University Computer Science
</affiliation>
<address confidence="0.8492415">
C/ Emilio Vargas 6 Kanslerinrinne 1 Computing Laboratory Sheffield University
28043 Madrid FI-33014 Oxford OX1 3QD Sheffield S1 4DP
</address>
<email confidence="0.602741">
joserg@tid.es jh@cs.uta.fi nigc@comlab.ox. d.field@shef.
ac.uk ac.uk
</email>
<sectionHeader confidence="0.985669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983933333333">
This paper presents a dialogue system in
the form of an ECA that acts as a socia-
ble and emotionally intelligent compan-
ion for the user. The system dialogue is
not task-driven but is social conversation
in which the user talks about his/her day
at the office. During conversations the
system monitors the emotional state of
the user and uses that information to in-
form its dialogue turns. The system is
able to respond to spoken interruptions
by the user, for example, the user can in-
terrupt to correct the system. The system
is already fully implemented and aspects
of actual output will be used to illustrate.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99972752173913">
Historically, Embodied Conversational Agents
(ECAs) have been used in research and industry
make information and complex tasks more ac-
cessible to customers and users. With the rise of
new technologies in affective dialogue systems,
we are beginning to see a future in which ECA
dialogues are not all task-driven, but some will
be focused on the social aspects of conversation.
We envisage the development of ECAs that en-
hance the social quality of life of the user, and
that build deep relationships with their users over
years of use. Our affective companion ECA is
our first contribution to this emerging technol-
ogy.
Our emotionally intelligent dialogue system
requires a high level of understanding of the
meaning of the user turns, and so is quite unlike
a ‘chatbot’. Affective aspects of the situation are
also key in influencing the course of the conver-
sation. The system attempts to balance both se-
mantic and affective understanding in order to
create an engaging and enjoyable companion for
social conversation.
</bodyText>
<sectionHeader confidence="0.917477" genericHeader="method">
2 Scenario description
</sectionHeader>
<bodyText confidence="0.999682">
The system conversation is free-ranging and fol-
lows the user’s initiative, rather than being task-
driven. We have for the time being restricted the
range of conversation topics to things that hap-
pen in the life of someone who works in an of-
fice (the ‘How was your day?’ (HWYD) sce-
nario). The user ‘offloads’ about his/her day at
work by chatting to the ECA, and the ECA tries
to empathise with and advise the user. The ECA
asks questions to elicit information from the user,
makes empathetic and sympathetic comments,
and offers advice.
The system is able to process long user turns,
as well as to generate long system turns (around
50 words). If the user perhaps dislikes what
he/she hears, he/she can interrupt the system by
barging in, and the ECA will respond in a hu-
man-like way. The conversation style is therefore
considerably different from traditional task-based
dialogues that consist of short dialogue turns and
in which the system takes all the initiative.
The dialogue in Table 1 (actual output from a
user session) illustrates some of the significant
features of the system, including long system
</bodyText>
<subsubsectionHeader confidence="0.677093">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277–280,
</subsubsectionHeader>
<affiliation confidence="0.891037">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.998756">
277
</page>
<figureCaption confidence="0.999897">
Figure 1: Screenshot of the prototype interface
</figureCaption>
<bodyText confidence="0.963707828571429">
turns, user interruptions and emotional analysis
of user input.
User1 Worst day ever. I arrived late be-
cause of the traffic.
System1 You have my sympathy. What
happened next?
User2 I missed the meeting on project
responsibilities; got more work
dumped on me. Just what I don’t
need. I’m already likely to miss
the deadline. [ECA looks worried
when user’s negative tone is de-
tected]
System2 Sorry to hear that. Who was at the
meeting?
User3 Only my team. They gave me a
set of PowerPoint slides from the
meeting. I spent the rest of the day
reading them but my computer
kept crashing because of updates.
System3 Oh dear! It is understandable how
you must be feeling. It’s right to
be unhappy about such a bad situ-
ation. Try not to worry about
missing the deadline because of
the technical problems. [At this
point user interrupts ECA by
beginning turn User4] It’s sad that
your motivation could be affected
by them. [System stops speaking]
User4 No, I’m not worried about the
technical problems, I’m worried
about the amount of extra work on
this project and the deadline.
System4 What is the project about?
</bodyText>
<tableCaption confidence="0.970384">
Table 1: Dialogue example
</tableCaption>
<bodyText confidence="0.92867725">
A video demonstration of the system in action
is available at:
http://www.youtube.com/watch?v=
BmDMNguQUmM
</bodyText>
<sectionHeader confidence="0.99174" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.999936483870968">
Figure 1 shows a screen shot taken at run-time of
actual system output. The ECA is represented on
a screen as a woman (waist up) who displays
natural, human-like movements and performs a
wide range of complex facial expressions, bodily
movements, and hand and arm gestures.
The screen also displays a transcript of the
user and system turns. The user turns shown con-
stitute the output of the Automatic Speech Rec-
ogniser (ASR). The system’s analysis of the
user’s emotional state is also shown.
The right-most panel of the screen shows
graphics which convey real-time information
about how the dialogue is being processed. It
presents a streamlined view of the software
modules that comprise the system. Module activ-
ity is visually represented at run-time by flashing
colours. This ‘glass-box’ approach enables de-
tailed observation and analysis of system
procedure at run-time.
The system comprises a number of distinct
modules that are connected using Inamode, a
hub-based message-passing framework using
XML formatted messages over plain text sock-
ets.
The system’s ASR is the NuanceTM dictation
engine. This is run in parallel with our own a-
coustic analysis pipeline which extracts low level
(pitch, tone) speech features and also high-level
features such as emotional characteristics.
Analysis of the emotions is currently carried out
</bodyText>
<page confidence="0.98749">
278
</page>
<bodyText confidence="0.999945661764706">
by EmoVoice (Vogt et al. (2008)). The ASR
output strings are analysed for sentiment by the
AFFECTiS system (Moilanen and Pulman (2007,
2009)) and classed as positive, neutral, or nega-
tive. This output is fused with the output from
EmoVoice to generate a value that represents the
user’s current emotional state, which is ex-
pressed as a valence+arousal pairing (with five
possible values).
The ASR output goes to our own Natural Lan-
guage Understanding (NLU) module which per-
forms syntactic and semantic analysis of user
utterances and derives noun phrases and verb
groups and associated arguments. Events rele-
vant to the scenario (e.g., promotions, redundan-
cies, meetings, arguments, etc.) are recognised
by the NLU and are used to populate an ontology
(a model of the conversation content). The sys-
tem is currently able to recognize and respond to
more than 30 event types.
The events recognised in a user turn are
labelled with the output of the Emotion Module
for that turn; the result is a representation of both
the semantic and affective information that the
user might be trying to convey.
Our own rule-based Dialogue Manager (DM)
takes the affect-annotated semantic output of the
NLU, and from that and its model of the conver-
sation content determines the next system turn. It
will either ask a question about the events that
occurred in the user’s day, express an opinion on
the events already described, or make empathetic
comments. Whenever the system has gained suf-
ficient understanding of a key event in the user’s
day, it generates a complex long turn that encap-
sulates comfort, opinion, warnings and advice to
the user.
These long system turns are generated by our
own plan-based Affective Strategy Module that
makes an appraisal of the user’s situation and
generates an appropriate emotional strategy
(Cavazza et al. (2010)). This strategy—expressed
as an abstract, conceptual representation—is han-
ded to our own Natural Language Generator
(NLG) that maps it into a series of linguistic sur-
face forms (usually 4 or 5 sentences). We use a
style-controllable system using Tree-Furcating
Grammars (an extension of the Tree-Adjoining
Grammars formalism (Joshi et al. (1997)). This
ensures the generation of a large set of different
surface forms from the same semantic input.
The output of the NLG is passed to a module
that adds this information to its system turn
instructions for the ECA. The ECA has been de-
veloped around the HaptekTM toolkit and is con-
trolled using an FML-like language (after
Hernández et al. (2008)). This 2-D embodiment
produces gestures, facial expressions, and body
movements that convey the emotional state of
the ECA. Its movements and expressions enable
it to visually display interest and enjoyment in
talking to the user, and to display empathy with
the user. The speech synthesis module is our own
emotion-focused extension of the LoquendoTM
TTS system. It includes paralinguistic elements
such as exclamations and laughter, and emo-
tional prosody generation for negative and posi-
tive utterances.
</bodyText>
<sectionHeader confidence="0.967431" genericHeader="method">
4 Special procedural features
</sectionHeader>
<bodyText confidence="0.99997225">
A significant processing design feature of the
system is that there are two main processing
loops from user input to system output; a ‘long
loop’ which passes through all the components
of the system; and a ‘short loop’ or ‘feedback
loop’ which will now be discussed (the proce-
dure already described in Section 3 is the long
loop procedure).
</bodyText>
<subsectionHeader confidence="0.996721">
4.1 Feedback loop
</subsectionHeader>
<bodyText confidence="0.997226892857143">
The feedback loop (‘short loop’) bypasses many
linguistic components and generates immediate
reactions to user activity. The main function of
the short loop is maintain user engagement by
preventing unnaturally long gaps of ECA inactiv-
ity. The feedback loop engages the acoustic
analysis components, the TTS, and the ECA. It is
responsible for the generation of real-time (&lt; 500
ms) reactions in the ECA in response to the emo-
tional state of the user. It attempts to align both
verbal behaviour (backchannelling) and non-
verbal behaviour (facial expressions, gestures,
and general body language) to the emotions de-
tected during most recent user turn. In order to
achieve a reasonable level of realism, these sys-
tem reactions to the perceived emotional state of
the user need to be perceptibly instantaneous.
Using this short feedback loop that bypasses
many of the linguistic components ensures this.
The feedback loop is also occasionally used to
make sympathetic comments immediately after
the user stops speaking. These act as acknowl-
edgements of the emotion expressed by the user.
An example can be seen in the System2 turn of
the example dialogue in Table 1:
1.“Sorry to hear that. Who was at the meeting?”
Here, the first utterance was spoken by the sys-
tem within a few tenths of a second after the end
</bodyText>
<page confidence="0.995219">
279
</page>
<bodyText confidence="0.99997695">
of the previous user turn (User2). The system
tried to identify the user’s emotion in the previ-
ous turn and then to behave linguistically and
visually in an empathetic way. The actual sympa-
thetic utterance was randomly chosen from a set
of ‘negative emotion utterances’ (there are also
‘positive’ and ‘neutral’ sets).
The second half of the system turn in (1) was
derived by the system’s ‘long loop’. It is a ques-
tion which refers to a meeting that the user men-
tioned in the previous turn. This ‘meeting’ event
has been heard by the ASR, understood by the
NLU system, remembered by the DM, and is
now referred to by an appropriate definite noun
phrase in the output of the NLG.
The feedback and main loops run in parallel.
However, the feedback loop generates its speech
output almost immediately, giving time for the
main dialogue loop to complete its more detailed
analysis of the user’s utterance.
</bodyText>
<subsectionHeader confidence="0.996893">
4.2 Handling user interruptions
</subsectionHeader>
<bodyText confidence="0.999994965517241">
This system has a complex strategy for handling
situations in which the user interrupts long
system turns. The system’s response to ‘barge-
in’ user interruptions is overseen by the Interrup-
tion Manager (IM), which is alerted by the
acoustic input modules whenever a genuine user
interruption (as opposed to, say, a backchannel)
is detected during a long system utterance. When
alerted, the IM instructs the ECA to stop speak-
ing when it reaches a natural stopping point in its
current turn (usually the end of the current
phrase). The user’s interruption utterance is
processed by the long loop. Its progress is
tracked and controlled by the IM, for example, it
makes sure that the linguistic modules know that
the current utterance is an interruption, whic
means it requires special treatment. The DM has
a range of strategies for system recoveries from
user interruptions, including different ways of
continuing, replanning, and aborting. An exam-
ple of a user interruption is shown in Table 1.
The user interrupts the long system utterance in
the System3 turn. The system’s response to the
interruption is to stop the speech output from the
ECA, abort the long system turn altogether, and
instead to ask for more details about the project
that the user has just mentioned during the inter-
ruption. (See (Crook et al. (2010)) for a more
detailed description of the IM.)
</bodyText>
<sectionHeader confidence="0.990278" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.907010307692308">
This work was funded by Companions, a Eu-
ropean Commission Sixth Framework Pro-
gramme Information Society Technologies Inte-
grated Project (IST-34434).
We would also like to thank the following
people for their valuable contributions to the
work presented here: Stephen Pulman, Ramon
Granell, and Simon Dobnick (Oxford Univer-
sity), Johan Boye (KTH Stockholm), Cameron
Smith and Daniel Charlton (Teesside Univer-
sity), Roger Moore, WeiWei Cheng and Lei Ye
(University of Sheffield), Morena Danieli and
Enrico Zovato (Loquendo).
</bodyText>
<sectionHeader confidence="0.976743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999912842105263">
Cavazza, M., Smith, C., Charlton, D., Crook, N.,
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive
Dialogue based on a Narrative Theory: an ECA
Implementation, Proc. of the 5th Int. Conf. on Per-
suasive Technology (Persuasive 2010), to appear
2010.
Crook, N., Smith, C., Cavazza, M., Pulman, S.,
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent In
proc. of AAMAS 2010.
Hernández, A., López, B., Pardo, D., Santos, R.,
Hernández, L., Relafio Gil, J. and Rodríguez, M.C.
(2008) Modular definition of multimodal ECA
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp,
S., Marsella, S., Pelachaud, C., and Vilhjálmsson,
H. (Eds.), AAMAS 2008 Workshop on Functional
Markup Language.
Joshi, A.K. &amp; Schabes, Y. (1997) Tree-adjoining
Grammars. Handbook of formal languages, vol. 3:
Beyond Words, Springer-Verlag New York, Inc.,
New York, NY, 1997.
Moilanen, K. and Pulman. S. (2009). Multi-entity
Sentiment Scoring. Proc. Recent Advances in
Natural Language Processing (RANLP 2009).
September 14-16, Borovets, Bulgaria. pp. 258--
263.
Moilanen, K. and Pulman. S. (2007). Sentiment Com-
position. Proc. Recent Advances in Natural Lan-
guage Processing (RANLP 2007). September 27-
29, Borovets, Bulgaria. pp. 378--382.
Vogt, T., André, E. and Bee, N. 2008. EmoVoice – A
framework for online recognition of emotions
from voice. Proc. Workshop on Perception and
Interactive Technologies for Speech-Based Sys-
tems, Springer, Kloster Irsee, Germany, (June
2008).
</reference>
<page confidence="0.997135">
280
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025868">
<title confidence="0.997925">How was your day?’ An affective companion ECA prototype</title>
<author confidence="0.999341">Marc</author>
<affiliation confidence="0.895484">School of Teesside</affiliation>
<address confidence="0.957326">Middlesbrough TS1 3BA</address>
<email confidence="0.995494">m.o.cavazza@tees.ac.uk</email>
<author confidence="0.827304">Raúl Santos de_la</author>
<affiliation confidence="0.411933">Telefónica</affiliation>
<address confidence="0.7234225">C/ Emilio Vargas 28043 Madrid</address>
<email confidence="0.974753">e.rsai@tid.es</email>
<author confidence="0.758753">Markku</author>
<affiliation confidence="0.8811925">University of Kanslerinrinne</affiliation>
<address confidence="0.738417">FI-33014</address>
<email confidence="0.97416">mturunen@cs.uta.fi</email>
<author confidence="0.879989">José Relaño Gil Jaakko Hakulinen Nigel Crook Debora Field</author>
<affiliation confidence="0.835205">Telefónica I+D University of Tampere Oxford University Computer Science C/ Emilio Vargas 6 Kanslerinrinne 1 Computing Laboratory Sheffield University</affiliation>
<address confidence="0.991419">28043 Madrid FI-33014 Oxford OX1 3QD Sheffield S1 4DP</address>
<abstract confidence="0.576353">joserg@tid.es jh@cs.uta.fi nigc@comlab.ox. ac.uk d.field@shef.</abstract>
<email confidence="0.914894">ac.uk</email>
<abstract confidence="0.9974299375">This paper presents a dialogue system in the form of an ECA that acts as a sociable and emotionally intelligent companion for the user. The system dialogue is not task-driven but is social conversation in which the user talks about his/her day at the office. During conversations the system monitors the emotional state of the user and uses that information to inform its dialogue turns. The system is able to respond to spoken interruptions by the user, for example, the user can interrupt to correct the system. The system is already fully implemented and aspects of actual output will be used to illustrate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Cavazza</author>
<author>C Smith</author>
<author>D Charlton</author>
<author>N Crook</author>
<author>J Boye</author>
<author>S Pulman</author>
<author>K Moilanen</author>
<author>D Pizzi</author>
<author>Santos de la Camara</author>
<author>R Turunen</author>
<author>M</author>
</authors>
<title>Persuasive Dialogue based on a Narrative Theory: an ECA</title>
<date>2010</date>
<booktitle>Implementation, Proc. of the 5th Int. Conf. on Persuasive Technology (Persuasive</booktitle>
<note>to appear</note>
<contexts>
<context position="8150" citStr="Cavazza et al. (2010)" startWordPosition="1317" endWordPosition="1320">el of the conversation content determines the next system turn. It will either ask a question about the events that occurred in the user’s day, express an opinion on the events already described, or make empathetic comments. Whenever the system has gained sufficient understanding of a key event in the user’s day, it generates a complex long turn that encapsulates comfort, opinion, warnings and advice to the user. These long system turns are generated by our own plan-based Affective Strategy Module that makes an appraisal of the user’s situation and generates an appropriate emotional strategy (Cavazza et al. (2010)). This strategy—expressed as an abstract, conceptual representation—is handed to our own Natural Language Generator (NLG) that maps it into a series of linguistic surface forms (usually 4 or 5 sentences). We use a style-controllable system using Tree-Furcating Grammars (an extension of the Tree-Adjoining Grammars formalism (Joshi et al. (1997)). This ensures the generation of a large set of different surface forms from the same semantic input. The output of the NLG is passed to a module that adds this information to its system turn instructions for the ECA. The ECA has been developed around t</context>
</contexts>
<marker>Cavazza, Smith, Charlton, Crook, Boye, Pulman, Moilanen, Pizzi, Camara, Turunen, M, 2010</marker>
<rawString>Cavazza, M., Smith, C., Charlton, D., Crook, N., Boye, J., Pulman, S., Moilanen, K., Pizzi, D., Santos de la Camara, R., Turunen, M. 2010 Persuasive Dialogue based on a Narrative Theory: an ECA Implementation, Proc. of the 5th Int. Conf. on Persuasive Technology (Persuasive 2010), to appear 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Crook</author>
<author>C Smith</author>
<author>M Cavazza</author>
<author>S Pulman</author>
<author>R Moore</author>
<author>J Boye</author>
</authors>
<title>Handling User Interruptions in an Embodied Conversational Agent In</title>
<date>2010</date>
<booktitle>proc. of AAMAS</booktitle>
<marker>Crook, Smith, Cavazza, Pulman, Moore, Boye, 2010</marker>
<rawString>Crook, N., Smith, C., Cavazza, M., Pulman, S., Moore, R., and Boye, J. 2010 Handling User Interruptions in an Embodied Conversational Agent In proc. of AAMAS 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hernández</author>
<author>B López</author>
<author>D Pardo</author>
<author>R Santos</author>
<author>L Hernández</author>
<author>Relafio Gil</author>
<author>J</author>
<author>M C Rodríguez</author>
</authors>
<title>Modular definition of multimodal ECA communication acts to improve dialogue robustness and depth of intention. In: Heylen,</title>
<date>2008</date>
<booktitle>AAMAS 2008 Workshop on Functional Markup Language.</booktitle>
<contexts>
<context position="8845" citStr="Hernández et al. (2008)" startWordPosition="1430" endWordPosition="1433">handed to our own Natural Language Generator (NLG) that maps it into a series of linguistic surface forms (usually 4 or 5 sentences). We use a style-controllable system using Tree-Furcating Grammars (an extension of the Tree-Adjoining Grammars formalism (Joshi et al. (1997)). This ensures the generation of a large set of different surface forms from the same semantic input. The output of the NLG is passed to a module that adds this information to its system turn instructions for the ECA. The ECA has been developed around the HaptekTM toolkit and is controlled using an FML-like language (after Hernández et al. (2008)). This 2-D embodiment produces gestures, facial expressions, and body movements that convey the emotional state of the ECA. Its movements and expressions enable it to visually display interest and enjoyment in talking to the user, and to display empathy with the user. The speech synthesis module is our own emotion-focused extension of the LoquendoTM TTS system. It includes paralinguistic elements such as exclamations and laughter, and emotional prosody generation for negative and positive utterances. 4 Special procedural features A significant processing design feature of the system is that t</context>
</contexts>
<marker>Hernández, López, Pardo, Santos, Hernández, Gil, J, Rodríguez, 2008</marker>
<rawString>Hernández, A., López, B., Pardo, D., Santos, R., Hernández, L., Relafio Gil, J. and Rodríguez, M.C. (2008) Modular definition of multimodal ECA communication acts to improve dialogue robustness and depth of intention. In: Heylen, D., Kopp, S., Marsella, S., Pelachaud, C., and Vilhjálmsson, H. (Eds.), AAMAS 2008 Workshop on Functional Markup Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining Grammars. Handbook of formal languages,</title>
<date>1997</date>
<volume>3</volume>
<publisher>Beyond Words, Springer-Verlag</publisher>
<location>New York,</location>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Joshi, A.K. &amp; Schabes, Y. (1997) Tree-adjoining Grammars. Handbook of formal languages, vol. 3: Beyond Words, Springer-Verlag New York, Inc., New York, NY, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>Multi-entity Sentiment Scoring.</title>
<date>2009</date>
<booktitle>Proc. Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>258--263</pages>
<location>Borovets,</location>
<marker>S, 2009</marker>
<rawString>Moilanen, K. and Pulman. S. (2009). Multi-entity Sentiment Scoring. Proc. Recent Advances in Natural Language Processing (RANLP 2009). September 14-16, Borovets, Bulgaria. pp. 258--263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>Sentiment Composition.</title>
<date>2007</date>
<booktitle>Proc. Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>378--382</pages>
<location>Borovets,</location>
<marker>S, 2007</marker>
<rawString>Moilanen, K. and Pulman. S. (2007). Sentiment Composition. Proc. Recent Advances in Natural Language Processing (RANLP 2007). September 27-29, Borovets, Bulgaria. pp. 378--382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Vogt</author>
<author>E André</author>
<author>N Bee</author>
</authors>
<title>EmoVoice – A framework for online recognition of emotions from voice.</title>
<date>2008</date>
<booktitle>Proc. Workshop on Perception and Interactive Technologies for Speech-Based Systems,</booktitle>
<publisher>Springer,</publisher>
<location>Kloster Irsee, Germany,</location>
<contexts>
<context position="6343" citStr="Vogt et al. (2008)" startWordPosition="1020" endWordPosition="1023"> by flashing colours. This ‘glass-box’ approach enables detailed observation and analysis of system procedure at run-time. The system comprises a number of distinct modules that are connected using Inamode, a hub-based message-passing framework using XML formatted messages over plain text sockets. The system’s ASR is the NuanceTM dictation engine. This is run in parallel with our own acoustic analysis pipeline which extracts low level (pitch, tone) speech features and also high-level features such as emotional characteristics. Analysis of the emotions is currently carried out 278 by EmoVoice (Vogt et al. (2008)). The ASR output strings are analysed for sentiment by the AFFECTiS system (Moilanen and Pulman (2007, 2009)) and classed as positive, neutral, or negative. This output is fused with the output from EmoVoice to generate a value that represents the user’s current emotional state, which is expressed as a valence+arousal pairing (with five possible values). The ASR output goes to our own Natural Language Understanding (NLU) module which performs syntactic and semantic analysis of user utterances and derives noun phrases and verb groups and associated arguments. Events relevant to the scenario (e</context>
</contexts>
<marker>Vogt, André, Bee, 2008</marker>
<rawString>Vogt, T., André, E. and Bee, N. 2008. EmoVoice – A framework for online recognition of emotions from voice. Proc. Workshop on Perception and Interactive Technologies for Speech-Based Systems, Springer, Kloster Irsee, Germany, (June 2008).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>