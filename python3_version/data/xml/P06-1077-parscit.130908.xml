<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.948247">
Tree-to-String Alignment Template for Statistical Machine Translation
</title>
<author confidence="0.999451">
Yang Liu, Qun Liu, and Shouxun Lin
</author>
<affiliation confidence="0.9682575">
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.813323">
No.6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
</address>
<email confidence="0.991037">
{yliu,liuqun,sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.994872" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966777777778">
We present a novel translation model
based on tree-to-string alignment template
(TAT) which describes the alignment be-
tween a source parse tree and a target
string. A TAT is capable of generating
both terminals and non-terminals and per-
forming reordering at both low and high
levels. The model is linguistically syntax-
based because TATs are extracted auto-
matically from word-aligned, source side
parsed parallel texts. To translate a source
sentence, we first employ a parser to pro-
duce a source parse tree and then ap-
ply TATs to transform the tree into a tar-
get string. Our experiments show that
the TAT-based model significantly outper-
forms Pharaoh, a state-of-the-art decoder
for phrase-based models.
</bodyText>
<sectionHeader confidence="0.997239" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998145333333333">
Phrase-based translation models (Marcu and
Wong, 2002; Koehn et al., 2003; Och and Ney,
2004), which go beyond the original IBM trans-
lation models (Brown et al., 1993) 1 by model-
ing translations of phrases rather than individual
words, have been suggested to be the state-of-the-
art in statistical machine translation by empirical
evaluations.
In phrase-based models, phrases are usually
strings of adjacent words instead of syntactic con-
stituents, excelling at capturing local reordering
and performing translations that are localized to
</bodyText>
<footnote confidence="0.378564">
1The mathematical notation we use in this paper is taken
</footnote>
<bodyText confidence="0.999084928571429">
from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is
to be translated into a target string eI1 = el, ... , ei, ... , eI.
Here, I is the length of the target string, and J is the length
of the source string.
substrings that are common enough to be observed
on training data. However, a key limitation of
phrase-based models is that they fail to model re-
ordering at the phrase level robustly. Typically,
phrase reordering is modeled in terms of offset po-
sitions at the word level (Koehn, 2004; Och and
Ney, 2004), making little or no direct use of syn-
tactic information.
Recent research on statistical machine transla-
tion has lead to the development of syntax-based
models. Wu (1997) proposes Inversion Trans-
duction Grammars, treating translation as a pro-
cess of parallel parsing of the source and tar-
get language via a synchronized grammar. Al-
shawi et al. (2000) represent each production in
parallel dependency tree as a finite transducer.
Melamed (2004) formalizes machine translation
problem as synchronous parsing based on multi-
text grammars. Graehl and Knight (2004) describe
training and decoding algorithms for both gen-
eralized tree-to-tree and tree-to-string transduc-
ers. Chiang (2005) presents a hierarchical phrase-
based model that uses hierarchical phrase pairs,
which are formally productions of a synchronous
context-free grammar. Ding and Palmer (2005)
propose a syntax-based translation model based
on a probabilistic synchronous dependency in-
sert grammar, a version of synchronous gram-
mars defined on dependency trees. All these ap-
proaches, though different in formalism, make use
of synchronous grammars or tree-based transduc-
tion rules to model both source and target lan-
guages.
Another class of approaches make use of syn-
tactic information in the target language alone,
treating the translation problem as a parsing prob-
lem. Yamada and Knight (2001) use a parser in
the target language to train probabilities on a set of
</bodyText>
<page confidence="0.529613">
609
</page>
<note confidence="0.861659333333333">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616,
Sydney, July 2006. c�2006 Association for Computational Linguistics
�
</note>
<bodyText confidence="0.999323515151515">
operations that transform a target parse tree into a
source string.
Paying more attention to source language anal-
ysis, Quirk et al. (2005) employ a source language
dependency parser, a target language word seg-
mentation component, and an unsupervised word
alignment component to learn treelet translations
from parallel corpus.
In this paper, we propose a statistical translation
model based on tree-to-string alignment template
which describes the alignment between a source
parse tree and a target string. A TAT is capa-
ble of generating both terminals and non-terminals
and performing reordering at both low and high
levels. The model is linguistically syntax-based
because TATs are extracted automatically from
word-aligned, source side parsed parallel texts.
To translate a source sentence, we first employ a
parser to produce a source parse tree and then ap-
ply TATs to transform the tree into a target string.
One advantage of our model is that TATs can
be automatically acquired to capture linguistically
motivated reordering at both low (word) and high
(phrase, clause) levels. In addition, the training of
TAT-based model is less computationally expen-
sive than tree-to-tree models. Similarly to (Galley
et al., 2004), the tree-to-string alignment templates
discussed in this paper are actually transformation
rules. The major difference is that we model the
syntax of the source language instead of the target
side. As a result, the task of our decoder is to find
the best target string while Galley’s is to seek the
most likely target tree.
</bodyText>
<sectionHeader confidence="0.869759" genericHeader="method">
2 Tree-to-String Alignment Template
</sectionHeader>
<bodyText confidence="0.9460852">
A tree-to-string alignment template z is a triple
T, 5, A), which describes the alignment A be-
tween a source parse tree T = T(FJ&apos;
1 ) 2 and
a target string 5 = E�&apos;
</bodyText>
<listItem confidence="0.954884666666667">
1 . A source string FJ&apos;
1 ,
which is the sequence of leaf nodes of T(FJ&apos;
</listItem>
<equation confidence="0.42457675">
1 ),
consists of both terminals (source words) and non-
terminals (phrasal categories). A target string E�&apos;
1
</equation>
<bodyText confidence="0.68464125">
is also composed of both terminals (target words)
and non-terminals (placeholders). An alignment
A is defined as a subset of the Cartesian product
of source and target symbol positions:
</bodyText>
<equation confidence="0.964387">
A C {(j, i) : j = 1,... , it; i = 1, ... , I&apos;} (1)
</equation>
<bodyText confidence="0.992425">
2We use T(·) to denote a parse tree. To reduce notational
overhead, we use T(z) to represent the parse tree in z. Simi-
larly, S(z) denotes the string in z.
</bodyText>
<figureCaption confidence="0.7833305">
Figure 1 shows three TATs automatically
learned from training data. Note that when
demonstrating a TAT graphically, we represent
non-terminals in the target strings by blanks.
Figure 1: Examples of tree-to-string alignment
templates obtained in training
</figureCaption>
<bodyText confidence="0.997138">
In the following, we formally describe how to
introduce tree-to-string alignment templates into
probabilistic dependencies to model Pr(ei|fJ1 ) 3.
In a first step, we introduce the hidden variable
T(fJ1 ) that denotes a parse tree of the source sen-
</bodyText>
<equation confidence="0.9756712">
tence fJ1 :
Pr(e� 1|fJ 1 ) = E Pr(ei, T (fJ1 )|fJ1 ) (2)
T(fi )
E= Pr(T(fJ1 )|fJ1 )Pr(e,|T(fJ1 ),fJ1 ) (3)
T(fi )
</equation>
<bodyText confidence="0.992285666666667">
Next, another hidden variable D is introduced
to detach the source parse tree T (fJ1 ) into a se-
quence of K subtrees T 1K with a preorder transver-
sal. We assume that each subtree Tk produces
a target string 5k. As a result, the sequence
of subtrees T�1K produces a sequence of target
strings �5K1 , which can be combined serially to
generate the target sentence ei. We assume that
Pr(e,|D,T(fJ1 ),fJ1) = Pr(�5K1|T�1K) because ei
is actually generated by the derivation of �5K1 .
Note that we omit an explicit dependence on the
detachment D to avoid notational overhead.
</bodyText>
<equation confidence="0.999628125">
P r(e� 1|T (fJ 1 ), fJ 1 ) = E Pr(ei,D|T(fJ1 ),fJ1 ) (4)
D
=E Pr(D|T(fJ1 ),fJ1 )Pr(ei|D,T(fJ1 ),fJ1 ) (5)
D
Pr(D|T(fJ1 ), fJ1 )Pr(�5K1 |T�1K) (6)
K
Pr(D|T(fJ1 ), fJ1 ) H Pr(�5k|�Tk) (7)
k=1
</equation>
<bodyText confidence="0.72592425">
3The notational convention will be as follows. We use
the symbol Pr(·) to denote general probability distribution
with no specific assumptions. In contrast, for model-based
probability distributions, we use generic symbol p(·).
</bodyText>
<figure confidence="0.994639086956522">
NP LCP NP
LC
NR
Vt
NN
NP
DNP
NP
U�
CC
t7
NR
NR
NP DEG
XM
�
between United States and
President Bush
=E
D
=E
D
610
</figure>
<figureCaption confidence="0.9402695">
Figure 2: Graphic illustration for translation pro-
cess
</figureCaption>
<bodyText confidence="0.9969695">
To further decompose Pr(˜S |T˜), the tree-to-
string alignment template, denoted by the variable
z, is introduced as a hidden variable.
⇒ X3 X4 of China
⇒ economic X4 of China
⇒ economic development of China
Following Och and Ney (2002), we base our
model on log-linear framework. Hence, all knowl-
edge sources are described as feature functions
that include the given source string fJ1 , the target
string eI1, and hidden variables. The hidden vari-
able T(fJ1 ) is omitted because we usually make
use of only single best output of a parser. As we
assume that all detachment have the same proba-
bility, the hidden variable D is also omitted. As
a result, the model we actually adopt for exper-
iments is limited because the parse, detachment,
and TAT application sub-models are simplified.
</bodyText>
<equation confidence="0.993359">
Pr(eI1, zK1 |fJ1 )
exp[PMm=1 λmhm(eI1, fJ1 , zK1)]
1/2 X 3/4
ˆeI 1 = argmax λmhm(eI 1, fJ 1 , zK 1 )
ez1,zx 1 m=1
</equation>
<bodyText confidence="0.999666">
For our experiments we use the following seven
feature functions 4 that are analogous to default
feature set of Pharaoh (Koehn, 2004). To simplify
the notation, we omit the dependence on the hid-
den variables of the model.
</bodyText>
<figure confidence="0.909154764705882">
NP DEG
中国
development
的
economic
China
of
combination
economic development of China
Pr( ˜S |X Pr(˜S,z |T˜) (8)
T˜ ) =
z
X= Pr(z |T˜)Pr(˜S|z, T˜) (9)
z
Pe/z z�x exp[PMm=1 λmhm(e&apos;l, .f1 , z,K)]
1, &gt;
M
</figure>
<bodyText confidence="0.965458">
Therefore, the TAT-based translation model can
be decomposed into four sub-models:
</bodyText>
<listItem confidence="0.999705">
1. parse model: Pr(T (fJ1 )|fJ1 )
2. detachment model: Pr(D|T (fJ1 ), fJ1 )
3. TAT selection model: Pr(z |T˜)
4. TAT application model: Pr(˜S|z, T˜)
</listItem>
<bodyText confidence="0.998786625">
Figure 2 shows how TATs work to perform
translation. First, the input source sentence is
parsed. Next, the parse tree is detached into five
subtrees with a preorder transversal. For each sub-
tree, a TAT is selected and applied to produce a
string. Finally, these strings are combined serially
to generate the translation (we use X to denote the
non-terminal):
</bodyText>
<equation confidence="0.992511681818182">
X1 ⇒ X2 of X3
⇒ X2 of China
= log K N(z) · δ(T(z), ˜Tk)
= log Y
= log k=1
= log K
= K Y
k=1
K
Y
k=1
K
Y
k=1
N(T(z))
N(z) · δ(T(z), ˜Tk)
N(S(z))
lex(T(z)|S(z)) · δ(T(z), ˜Tk)
lex(S(z)|T(z)) · δ(T(z), ˜Tk)
h6(eI1, fJ1 ) = log YI p(ei|ei−2, ei−1)
i=1
h7(eI1, fJ1 ) = I
</equation>
<bodyText confidence="0.999292">
4When computing lexical weighting features (Koehn et
al., 2003), we take only terminals into account. If there are
no terminals, we set the feature value to 1. We use lex(·)
to denote lexical weighting. We denote the number of TATs
used for decoding by K and the length of target string by L
</bodyText>
<equation confidence="0.9964228">
h1(eI1, fJ1 )
h2(eI1, fJ1 )
h3(eI1, fJ1 )
h4(eI1, fJ1 )
h5(eI1, fJ1 )
</equation>
<page confidence="0.396479">
611
</page>
<table confidence="0.999925142857143">
Tree String Alignment
( NR布什 ) Bush 1:1
( NN 总统 ) President 1:1
( VV 发表 ) made 1:1
( NN 演讲 ) speech 1:1
(NP(NR)(NN)) X1  |X2 1:2 2:1
( NP ( NR 布什 ) ( NN ) ) X  |Bush 1:2 2:1
( NP ( NR ) ( NN 总统 ) ) President  |X 1:2 2:1
( NP ( NR 布什 ) ( NN 总统 ) ) President  |Bush 1:2 2:1
(VP(VV)(NN)) X1  |a  |X2 1:1 2:3
( VP ( VV 发表 ) ( NN ) ) made  |a  |X 1:1 2:3
( VP ( VV ) ( NN 演讲 ) ) X  |a  |speech 1:1 2:3
( VP ( VV 发表 ) ( NN 演讲 ) ) made  |a  |speech 1:1 2:3
( IP ( NP ) ( VP ) ) X1  |X2 1:1 2:2
</table>
<tableCaption confidence="0.99966">
Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2
</tableCaption>
<sectionHeader confidence="0.993716" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.9995862">
To extract tree-to-string alignment templates from
a word-aligned, source side parsed sentence pair
hT (fJ1 ), eI1, Ai, we need first identify TSAs (Tree-
String-Alignment) using similar criterion as sug-
gested in (Och and Ney, 2004). A TSA is a triple
</bodyText>
<equation confidence="0.93884728125">
hT (fj2
j1 ), ei2
i1, �A)i that is in accordance with the
following constraints:
1. ∀(i,j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2
2. T (fj2
j1 ) is a subtree of T (fJ1 )
Given a TSA hT(fj2
j1 ), ei2
i1, �Ai, a triple
hT (fj4
j3 ), ei4
i3, Ai is its sub TSA if and only
if:
1. T (fj4
j3 ), ei4
i3, Ai is a TSA
2. T (fj4
j3 ) is rooted at the direct descendant of
the root node of T (fj1
j2 )
3. i1 ≤ i3 ≤ i4 ≤ i2
4. ∀(i,j) ∈ A: i3 ≤ i ≤ i4 ↔ j3 ≤ j ≤ j4
Basically, we extract TATs from a TSA
hT (fj2
j1 ), ei2
i1, �Ai using the following two rules:
1. If T (fj2
j1 ) contains only one node,
then hT(fj2
j1 ), ei2
i1, �Ai is a TAT
</equation>
<listItem confidence="0.366113">
2. If the height of T (fj2
</listItem>
<figure confidence="0.578168666666667">
j1 ) is greater than one,
then build TATs using those extracted from
sub TSAs of hT(fj2
j1 ), ei2
i1, �Ai.
IP
</figure>
<figureCaption confidence="0.996912">
Figure 3: An example of TSA
</figureCaption>
<bodyText confidence="0.9999146">
Usually, we can extract a very large amount of
TATs from training data using the above rules,
making both training and decoding very slow.
Therefore, we impose three restrictions to reduce
the magnitude of extracted TATs:
</bodyText>
<listItem confidence="0.995682">
1. A third constraint is added to the definition of
TSA:
</listItem>
<equation confidence="0.595319">
∃j&apos;,j&apos;&apos; : j1 ≤ j&apos; ≤ j2 and j1 ≤ j&apos;&apos; ≤ j2
and (i1, j&apos;) ∈ A and (i2, j&apos;&apos;) ∈ A
</equation>
<bodyText confidence="0.986298333333333">
This constraint requires that both the first
and last symbols in the target string must be
aligned to some source symbols.
</bodyText>
<listItem confidence="0.73096075">
2. The height of T(z) is limited to no greater
than h.
3. The number of direct descendants of a node
of T(z) is limited to no greater than c.
</listItem>
<bodyText confidence="0.8279698">
Table 1 shows the TATs extracted from the TSA
in Figure 3 with h = 2 and c = 2.
As we restrict that T(fj2
j1 ) must be a subtree of
T(fJ1 ), TATs may be treated as syntactic hierar-
</bodyText>
<figure confidence="0.943571333333333">
NN
VV
总A
发表
NP
VP
President Bush made a speech
NN
演讲
NR
Vt
612
</figure>
<bodyText confidence="0.990328125">
chical phrase pairs (Chiang, 2005) with tree struc-
ture on the source side. At the same time, we face
the risk of losing some useful non-syntactic phrase
pairs. For example, the phrase pair
+{&apos; AOL ALA H President Bush made
can never be obtained in form of TAT from the
TSA in Figure 3 because there is no subtree for
that source string.
</bodyText>
<sectionHeader confidence="0.996664" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999547">
We approach the decoding problem as a bottom-up
beam search.
To translate a source sentence, we employ a
parser to produce a parse tree. Moving bottom-
up through the source parse tree, we compute a
list of candidate translations for the input subtree
rooted at each node with a postorder transversal.
Candidate translations of subtrees are placed in
stacks. Figure 4 shows the organization of can-
didate translation stacks.
</bodyText>
<figure confidence="0.704119">
4 7
</figure>
<figureCaption confidence="0.714006666666667">
Figure 4: Candidate translations of subtrees are
placed in stacks according to the root index set by
postorder transversal
</figureCaption>
<bodyText confidence="0.958597">
A candidate translation contains the following
information:
</bodyText>
<listItem confidence="0.998704666666667">
1. the partial translation
2. the accumulated feature values
3. the accumulated probability
</listItem>
<bodyText confidence="0.9991172">
A TAT z is usable to a parse tree T if and only
if T(z) is rooted at the root of T and covers part
of nodes of T. Given a parse tree T, we find all
usable TATs. Given a usable TAT z, if T(z) is
equal to T, then 5(z) is a candidate translation of
T. If T(z) covers only a portion of T, we have
to compute a list of candidate translations for T
by replacing the non-terminals of 5(z) with can-
didate translations of the corresponding uncovered
subtrees.
</bodyText>
<figure confidence="0.683071">
2 3
NP DEG
</figure>
<figureCaption confidence="0.997595">
Figure 5: Candidate translation construction
</figureCaption>
<bodyText confidence="0.999963269230769">
For example, when computing the candidate
translations for the tree rooted at node 8, the TAT
used in Figure 5 covers only a portion of the parse
tree in Figure 4. There are two uncovered sub-
trees that are rooted at node 2 and node 7 respec-
tively. Hence, we replace the third symbol with
the candidate translations in stack 2 and the first
symbol with the candidate translations in stack 7.
At the same time, the feature values and probabil-
ities are also accumulated for the new candidate
translations.
To speed up the decoder, we limit the search
space by reducing the number of TATs used for
each input node. There are two ways to limit the
TAT table size: by a fixed limit (tatTable-limit) of
how many TATs are retrieved for each input node,
and by a probability threshold (tatTable-threshold)
that specify that the TAT probability has to be
above some value. On the other hand, instead of
keeping the full list of candidates for a given node,
we keep a top-scoring subset of the candidates.
This can also be done by a fixed limit (stack-limit)
or a threshold (stack-threshold). To perform re-
combination, we combine candidate translations
that share the same leading and trailing bigrams
in each stack.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.994236">
Our experiments were on Chinese-to-English
translation. The training corpus consists of 31,149
sentence pairs with 843,256 Chinese words and
</bodyText>
<figure confidence="0.946012909090909">
8NP
1 NR � ft?j� X&amp;
t�
1 2 3 4 5 6 7 8
DNP
���
���
���
NP
���
2
3 5NN 6
NN
NP
DEG
���
���
���
���
8NP
1 2 3 4 5 6 7 8
4 7
DNP NP
���
���
���
的
of
���
���
���
���
���
</figure>
<page confidence="0.310318">
613
</page>
<table confidence="0.909575">
System Features BLEU4
Pharaoh d + O(e|f) 0.0573 ± 0.0033
d + lm + O(e|f) + wp 0.2019 ± 0.0083
d + lm + O(f|e) + lex(f|e) + O(e|f) + lex(e|f) + pp + wp 0.2089 ± 0.0089
Lynx h1 0.1639 ± 0.0077
h1 + h6 + h7 0.2100 ± 0.0089
h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ± 0.0080
</table>
<tableCaption confidence="0.999176">
Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus
</tableCaption>
<bodyText confidence="0.999125615384615">
949, 583 English words. For the language model,
we used SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a trigram model with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) on the 31,149 English sentences. We se-
lected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development cor-
pus, and used the 2005 NIST MT Evaluation test
set as our test corpus. We evaluated the transla-
tion quality using the BLEU metric (Papineni et
al., 2002), as calculated by mteval-v11b.pl with its
default setting except that we used case-sensitive
matching of n-grams.
</bodyText>
<subsectionHeader confidence="0.988706">
5.1 Pharaoh
</subsectionHeader>
<bodyText confidence="0.99865325">
The baseline system we used for comparison was
Pharaoh (Koehn et al., 2003; Koehn, 2004), a
freely available decoder for phrase-based transla-
tion models:
</bodyText>
<equation confidence="0.9984755">
p(e|f) = po(f|e)A-I x pLM(e)ALM x
pD(e, f)AD x wlength(e)AW(e) (10)
</equation>
<bodyText confidence="0.999069809523809">
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule “diag-
and” described in (Koehn et al., 2003) to obtain
a single many-to-many word alignment for each
sentence pair. After that, we used some heuristics,
which including rule-based translation of num-
bers, dates, and person names, to further improve
the alignment accuracy.
Given the word-aligned bilingual corpus, we
obtained 1, 231, 959 bilingual phrases (221, 453
used on test corpus) using the training toolkits
publicly released by Philipp Koehn with its default
setting.
To perform minimum error rate training (Och,
2003) to tune the feature weights to maximize the
system’s BLEU score on development set, we used
optimizeV5IBMBLEU.m (Venugopal and Vogel,
2005). We used default pruning settings for
Pharaoh except that we set the distortion limit to
4.
</bodyText>
<subsectionHeader confidence="0.998139">
5.2 Lynx
</subsectionHeader>
<bodyText confidence="0.999842642857143">
On the same word-aligned training data, it took
us about one month to parse all the 31,149 Chi-
nese sentences using a Chinese parser written by
Deyi Xiong (Xiong et al., 2005). The parser was
trained on articles 1 − 270 of Penn Chinese Tree-
bank version 1.0 and achieved 79.4% (F1 mea-
sure) as well as a 4.4% relative decrease in er-
ror rate. Then, we performed TAT extraction de-
scribed in section 3 with h = 3 and c = 5
and obtained 350,575 TATs (88,066 used on test
corpus). To run our decoder Lynx on develop-
ment and test corpus, we set tatTable-limit = 20,
tatTable-threshold = 0, stack-limit = 100, and
stack-threshold = 0.00001.
</bodyText>
<subsectionHeader confidence="0.870571">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.99961455">
Table 2 shows the results on test set using Pharaoh
and Lynx with different feature settings. The 95%
confidence intervals were computed using Zhang’s
significance tester (Zhang et al., 2004). We mod-
ified it to conform to NIST’s current definition
of the BLEU brevity penalty. For Pharaoh, eight
features were used: distortion model d, a trigram
language model lm, phrase translation probabili-
ties O(f|e) and O(e|f), lexical weightings lex(f|e)
and lex(e|f), phrase penalty pp, and word penalty
wp. For Lynx, seven features described in sec-
tion 2 were used. We find that Lynx outperforms
Pharaoh with all feature settings. With full fea-
tures, Lynx achieves an absolute improvement of
0.006 over Pharaoh (3.1% relative). This differ-
ence is statistically significant (p &lt; 0.01). Note
that Lynx made use of only 88,066 TATs on test
corpus while 221, 453 bilingual phrases were used
for Pharaoh.
The feature weights obtained by minimum er-
</bodyText>
<page confidence="0.635326">
614
</page>
<table confidence="0.99859375">
System Features
d lm φ(f|e) lex(f|e) φ(e|f) lex(e|f) pp wp
Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000
Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620
</table>
<tableCaption confidence="0.961314">
Table 3: Feature weights obtained by minimum error rate training on the development corpus
</tableCaption>
<table confidence="0.966791333333333">
BLEU4
tat 0.2178 f 0.0080
tat + bp 0.2240 f 0.0083
</table>
<tableCaption confidence="0.958165">
Table 4: Effect of using bilingual phrases for Lynx
</tableCaption>
<bodyText confidence="0.999260625">
ror rate training for both Pharaoh and Lynx are
shown in Table 3. We find that φ(f|e) (i.e. h2) is
not a helpful feature for Lynx. The reason is that
we use only a single non-terminal symbol instead
of assigning phrasal categories to the target string.
In addition, we allow the target string consists of
only non-terminals, making translation decisions
not always based on lexical evidence.
</bodyText>
<subsectionHeader confidence="0.998709">
5.4 Using bilingual phrases
</subsectionHeader>
<bodyText confidence="0.9984663125">
It is interesting to use bilingual phrases to
strengthen the TAT-based model. As we men-
tioned before, some useful non-syntactic phrase
pairs can never be obtained in form of TAT be-
cause we restrict that there must be a correspond-
ing parse tree for the source phrase. Moreover,
it takes more time to obtain TATs than bilingual
phrases on the same training data because parsing
is usually very time-consuming.
Given an input subtree T (Fj�
j� ), if Fj�
j� is a string
of terminals, we find all bilingual phrases that the
source phrase is equal to Fj�
j� . Then we build a
TAT for each bilingual phrase (fJ,
</bodyText>
<equation confidence="0.399017">
1 , eI,
</equation>
<bodyText confidence="0.993321">
1 , A): the
tree of the TAT is T (Fj�
j� ), the string is eI,
1 , and
the alignment is A. If a TAT built from a bilingual
phrase is the same with a TAT in the TAT table, we
prefer to the greater translation probabilities.
Table 4 shows the effect of using bilingual
phrases for Lynx. Note that these bilingual phrases
are the same with those used for Pharaoh.
</bodyText>
<subsectionHeader confidence="0.755251">
5.5 Results on large data
</subsectionHeader>
<bodyText confidence="0.999980933333333">
We also conducted an experiment on large data to
further examine our design philosophy. The train-
ing corpus contains 2.6 million sentence pairs. We
used all the data to extract bilingual phrases and
a portion of 800K pairs to obtain TATs. Two tri-
gram language models were used for Lynx. One
was trained on the 2.6 million English sentences
and another was trained on the first 1/3 of the Xin-
hua portion of Gigaword corpus. We also included
rule-based translations of named entities, dates,
and numbers. By making use of these data, Lynx
achieves a BLEU score of 0.2830 on the 2005
NIST Chinese-to-English MT evaluation test set,
which is a very promising result for linguistically
syntax-based models.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998825">
In this paper, we introduce tree-to-string align-
ment templates, which can be automatically
learned from syntactically-annotated training data.
The TAT-based translation model improves trans-
lation quality significantly compared with a state-
of-the-art phrase-based decoder. Treated as spe-
cial TATs without tree on the source side, bilingual
phrases can be utilized for the TAT-based model to
get further improvement.
It should be emphasized that the restrictions
we impose on TAT extraction limit the expressive
power of TAT. Preliminary experiments reveal that
removing these restrictions does improve transla-
tion quality, but leads to large memory require-
ments. We feel that both parsing and word align-
ment qualities have important effects on the TAT-
based model. We will retrain the Chinese parser
on Penn Chinese Treebank version 5.0 and try to
improve word alignment quality using log-linear
models as suggested in (Liu et al., 2005).
</bodyText>
<sectionHeader confidence="0.954448" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999925888888889">
This work is supported by National High Tech-
nology Research and Development Program con-
tract “Generally Technical Research and Ba-
sic Database Establishment of Chinese Plat-
form”(Subject No. 2004AA114010). We are
grateful to Deyi Xiong for providing the parser and
Haitao Mi for making the parser more efficient and
robust. Thanks to Dr. Yajuan Lv for many helpful
comments on an earlier draft of this paper.
</bodyText>
<page confidence="0.903172">
615
</page>
<sectionHeader confidence="0.995443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999859357894736">
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite-state head transducers.
Computational Linguistics, 26(1):45-60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen and Joshua Goodman. 1998. Am
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of 43rd Annual Meeting of the ACL, pages
263-270.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insert grammars. In Proceedings of 43rd Annual
Meeting of the ACL, pages 541-548.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of NAACL-HLT 2004, pages 273-
280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-HLT
2004, pages 105-112.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
ofHLT-NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trnasla-
tion models. In Proceedings of the Sixth Confer-
ence of the Association for Machine Translation in
the Americas, pages 115-124.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of 43rd Annual Meeting of the ACL, pages 459-466.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 133-139.
Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of 42nd Annual Meeting
of the ACL, pages 653-660.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of 38th
Annual Meeting of the ACL, pages 440-447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the ACL, pages 295-302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
41st Annual Meeting of the ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the ACL, pages 311-318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of 43rd An-
nual Meeting of the ACL, pages 271-279.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and min-
imum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Con-
ference of the European Association for Machine
Translation (EAMT-05).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
treebank with semantic knowledge. In Proceedings
ofIJCNLP 2005, pages 70-81.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the ACL, pages 523-530.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC),
pages 2051-2054.
</reference>
<page confidence="0.94321">
616
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.726127">
<title confidence="0.999727">Tree-to-String Alignment Template for Statistical Machine Translation</title>
<author confidence="0.996853">Qun Liu Liu</author>
<author confidence="0.996853">Lin</author>
<affiliation confidence="0.990236">Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.8522005">No.6 Kexueyuan South Road, Haidian District P. O. Box 2704, Beijing, 100080, China</address>
<abstract confidence="0.998015736842105">We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite-state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--1</pages>
<contexts>
<context position="2475" citStr="Alshawi et al. (2000)" startWordPosition="396" endWordPosition="400"> to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert </context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite-state head transducers. Computational Linguistics, 26(1):45-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1175" citStr="Brown et al., 1993" startWordPosition="176" endWordPosition="179">and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translated into a target string eI1 = el, ... , ei, ... , eI. Here, I is the length of the target stri</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>Am empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="16491" citStr="Chen and Goodman, 1998" startWordPosition="3016" endWordPosition="3019"> ��� 8NP 1 2 3 4 5 6 7 8 4 7 DNP NP ��� ��� ��� 的 of ��� ��� ��� ��� ��� 613 System Features BLEU4 Pharaoh d + O(e|f) 0.0573 ± 0.0033 d + lm + O(e|f) + wp 0.2019 ± 0.0083 d + lm + O(f|e) + lex(f|e) + O(e|f) + lex(e|f) + pp + wp 0.2089 ± 0.0089 Lynx h1 0.1639 ± 0.0077 h1 + h6 + h7 0.2100 ± 0.0089 h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ± 0.0080 Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus 949, 583 English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = po(f|e)A-I x pLM(e)ALM x pD</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. Am empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="2806" citStr="Chiang (2005)" startWordPosition="444" endWordPosition="445">earch on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treat</context>
<context position="12892" citStr="Chiang, 2005" startWordPosition="2346" endWordPosition="2347">&apos;&apos; : j1 ≤ j&apos; ≤ j2 and j1 ≤ j&apos;&apos; ≤ j2 and (i1, j&apos;) ∈ A and (i2, j&apos;&apos;) ∈ A This constraint requires that both the first and last symbols in the target string must be aligned to some source symbols. 2. The height of T(z) is limited to no greater than h. 3. The number of direct descendants of a node of T(z) is limited to no greater than c. Table 1 shows the TATs extracted from the TSA in Figure 3 with h = 2 and c = 2. As we restrict that T(fj2 j1 ) must be a subtree of T(fJ1 ), TATs may be treated as syntactic hierarNN VV 总A 发表 NP VP President Bush made a speech NN 演讲 NR Vt 612 chical phrase pairs (Chiang, 2005) with tree structure on the source side. At the same time, we face the risk of losing some useful non-syntactic phrase pairs. For example, the phrase pair +{&apos; AOL ALA H President Bush made can never be obtained in form of TAT from the TSA in Figure 3 because there is no subtree for that source string. 4 Decoding We approach the decoding problem as a bottom-up beam search. To translate a source sentence, we employ a parser to produce a parse tree. Moving bottomup through the source parse tree, we compute a list of candidate translations for the input subtree rooted at each node with a postorder</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of 43rd Annual Meeting of the ACL, pages 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insert grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the ACL,</booktitle>
<pages>541--548</pages>
<contexts>
<context position="2978" citStr="Ding and Palmer (2005)" startWordPosition="466" endWordPosition="469">on as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 Proceedings of the 21s</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insert grammars. In Proceedings of 43rd Annual Meeting of the ACL, pages 541-548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>273--280</pages>
<contexts>
<context position="4978" citStr="Galley et al., 2004" startWordPosition="775" endWordPosition="778">h low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels. In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models. Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules. The major difference is that we model the syntax of the source language instead of the target side. As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree. 2 Tree-to-String Alignment Template A tree-to-string alignment template z is a triple T, 5, A), which describes the alignment A between a source parse tree T = T(FJ&apos; 1 ) 2 and a target string 5 = E�&apos; 1 . A source string FJ&apos; 1 , which is the sequence of leaf nodes of T(FJ&apos; 1</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of NAACL-HLT 2004, pages 273-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>105--112</pages>
<contexts>
<context position="2684" citStr="Graehl and Knight (2004)" startWordPosition="426" endWordPosition="429">set positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both sour</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proceedings of NAACL-HLT 2004, pages 105-112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1081" citStr="Koehn et al., 2003" startWordPosition="159" endWordPosition="162">capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translat</context>
<context position="10088" citStr="Koehn et al., 2003" startWordPosition="1701" endWordPosition="1704"> source sentence is parsed. Next, the parse tree is detached into five subtrees with a preorder transversal. For each subtree, a TAT is selected and applied to produce a string. Finally, these strings are combined serially to generate the translation (we use X to denote the non-terminal): X1 ⇒ X2 of X3 ⇒ X2 of China = log K N(z) · δ(T(z), ˜Tk) = log Y = log k=1 = log K = K Y k=1 K Y k=1 K Y k=1 N(T(z)) N(z) · δ(T(z), ˜Tk) N(S(z)) lex(T(z)|S(z)) · δ(T(z), ˜Tk) lex(S(z)|T(z)) · δ(T(z), ˜Tk) h6(eI1, fJ1 ) = log YI p(ei|ei−2, ei−1) i=1 h7(eI1, fJ1 ) = I 4When computing lexical weighting features (Koehn et al., 2003), we take only terminals into account. If there are no terminals, we set the feature value to 1. We use lex(·) to denote lexical weighting. We denote the number of TATs used for decoding by K and the length of target string by L h1(eI1, fJ1 ) h2(eI1, fJ1 ) h3(eI1, fJ1 ) h4(eI1, fJ1 ) h5(eI1, fJ1 ) 611 Tree String Alignment ( NR布什 ) Bush 1:1 ( NN 总统 ) President 1:1 ( VV 发表 ) made 1:1 ( NN 演讲 ) speech 1:1 (NP(NR)(NN)) X1 |X2 1:2 2:1 ( NP ( NR 布什 ) ( NN ) ) X |Bush 1:2 2:1 ( NP ( NR ) ( NN 总统 ) ) President |X 1:2 2:1 ( NP ( NR 布什 ) ( NN 总统 ) ) President |Bush 1:2 2:1 (VP(VV)(NN)) X1 |a |X2 1:1 2:</context>
<context position="16975" citStr="Koehn et al., 2003" startWordPosition="3098" endWordPosition="3101">sed SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = po(f|e)A-I x pLM(e)ALM x pD(e, f)AD x wlength(e)AW(e) (10) We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL 2003, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine trnaslation models.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="2104" citStr="Koehn, 2004" startWordPosition="338" endWordPosition="339">ng and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translated into a target string eI1 = el, ... , ei, ... , eI. Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training a</context>
<context position="8880" citStr="Koehn, 2004" startWordPosition="1478" endWordPosition="1479">n variables. The hidden variable T(fJ1 ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified. Pr(eI1, zK1 |fJ1 ) exp[PMm=1 λmhm(eI1, fJ1 , zK1)] 1/2 X 3/4 ˆeI 1 = argmax λmhm(eI 1, fJ 1 , zK 1 ) ez1,zx 1 m=1 For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004). To simplify the notation, we omit the dependence on the hidden variables of the model. NP DEG 中国 development 的 economic China of combination economic development of China Pr( ˜S |X Pr(˜S,z |T˜) (8) T˜ ) = z X= Pr(z |T˜)Pr(˜S|z, T˜) (9) z Pe/z z�x exp[PMm=1 λmhm(e&apos;l, .f1 , z,K)] 1, &gt; M Therefore, the TAT-based translation model can be decomposed into four sub-models: 1. parse model: Pr(T (fJ1 )|fJ1 ) 2. detachment model: Pr(D|T (fJ1 ), fJ1 ) 3. TAT selection model: Pr(z |T˜) 4. TAT application model: Pr(˜S|z, T˜) Figure 2 shows how TATs work to perform translation. First, the input source sen</context>
<context position="16989" citStr="Koehn, 2004" startWordPosition="3102" endWordPosition="3103">eling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = po(f|e)A-I x pLM(e)ALM x pD(e, f)AD x wlength(e)AW(e) (10) We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, we obtained 1,</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine trnaslation models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the ACL,</booktitle>
<pages>459--466</pages>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of 43rd Annual Meeting of the ACL, pages 459-466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrasebased, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>133--139</pages>
<contexts>
<context position="1061" citStr="Marcu and Wong, 2002" startWordPosition="155" endWordPosition="158">rget string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1, ... ,fj, ... , </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrasebased, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of 42nd Annual Meeting of the ACL,</booktitle>
<pages>653--660</pages>
<contexts>
<context position="2568" citStr="Melamed (2004)" startWordPosition="412" endWordPosition="413"> to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches,</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings of 42nd Annual Meeting of the ACL, pages 653-660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="17156" citStr="Och and Ney, 2000" startWordPosition="3127" endWordPosition="3130">71 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = po(f|e)A-I x pLM(e)ALM x pD(e, f)AD x wlength(e)AW(e) (10) We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting. To perform minimum </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of 38th Annual Meeting of the ACL, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="8084" citStr="Och and Ney (2002)" startWordPosition="1333" endWordPosition="1336">al convention will be as follows. We use the symbol Pr(·) to denote general probability distribution with no specific assumptions. In contrast, for model-based probability distributions, we use generic symbol p(·). NP LCP NP LC NR Vt NN NP DNP NP U� CC t7 NR NR NP DEG XM � between United States and President Bush =E D =E D 610 Figure 2: Graphic illustration for translation process To further decompose Pr(˜S |T˜), the tree-tostring alignment template, denoted by the variable z, is introduced as a hidden variable. ⇒ X3 X4 of China ⇒ economic X4 of China ⇒ economic development of China Following Och and Ney (2002), we base our model on log-linear framework. Hence, all knowledge sources are described as feature functions that include the given source string fJ1 , the target string eI1, and hidden variables. The hidden variable T(fJ1 ) is omitted because we usually make use of only single best output of a parser. As we assume that all detachment have the same probability, the hidden variable D is also omitted. As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified. Pr(eI1, zK1 |fJ1 ) exp[PMm=1 λmhm(eI1, fJ1 , zK1)] </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 295-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="1101" citStr="Och and Ney, 2004" startWordPosition="163" endWordPosition="166">g both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string. Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models. 1 Introduction Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations. In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to 1The mathematical notation we use in this paper is taken from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translated into a target str</context>
<context position="11188" citStr="Och and Ney, 2004" startWordPosition="1958" endWordPosition="1961">NR ) ( NN 总统 ) ) President |X 1:2 2:1 ( NP ( NR 布什 ) ( NN 总统 ) ) President |Bush 1:2 2:1 (VP(VV)(NN)) X1 |a |X2 1:1 2:3 ( VP ( VV 发表 ) ( NN ) ) made |a |X 1:1 2:3 ( VP ( VV ) ( NN 演讲 ) ) X |a |speech 1:1 2:3 ( VP ( VV 发表 ) ( NN 演讲 ) ) made |a |speech 1:1 2:3 ( IP ( NP ) ( VP ) ) X1 |X2 1:1 2:2 Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2 3 Training To extract tree-to-string alignment templates from a word-aligned, source side parsed sentence pair hT (fJ1 ), eI1, Ai, we need first identify TSAs (TreeString-Alignment) using similar criterion as suggested in (Och and Ney, 2004). A TSA is a triple hT (fj2 j1 ), ei2 i1, �A)i that is in accordance with the following constraints: 1. ∀(i,j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 2. T (fj2 j1 ) is a subtree of T (fJ1 ) Given a TSA hT(fj2 j1 ), ei2 i1, �Ai, a triple hT (fj4 j3 ), ei4 i3, Ai is its sub TSA if and only if: 1. T (fj4 j3 ), ei4 i3, Ai is a TSA 2. T (fj4 j3 ) is rooted at the direct descendant of the root node of T (fj1 j2 ) 3. i1 ≤ i3 ≤ i4 ≤ i2 4. ∀(i,j) ∈ A: i3 ≤ i ≤ i4 ↔ j3 ≤ j ≤ j4 Basically, we extract TATs from a TSA hT (fj2 j1 ), ei2 i1, �Ai using the following two rules: 1. If T (fj2 j1 ) contains only one nod</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of 41st Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="17787" citStr="Och, 2003" startWordPosition="3227" endWordPosition="3228">us in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting. To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005). We used default pruning settings for Pharaoh except that we set the distortion limit to 4. 5.2 Lynx On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005). The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of 41st Annual Meeting of the ACL, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="16774" citStr="Papineni et al., 2002" startWordPosition="3067" endWordPosition="3070">100 ± 0.0089 h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ± 0.0080 Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus 949, 583 English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: p(e|f) = po(f|e)A-I x pLM(e)ALM x pD(e, f)AD x wlength(e)AW(e) (10) We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pa</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the ACL, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of 43rd Annual Meeting of the ACL,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="3893" citStr="Quirk et al. (2005)" startWordPosition="609" endWordPosition="612"> both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, Sydney, July 2006. c�2006 Association for Computational Linguistics � operations that transform a target parse tree into a source string. Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus. In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels. The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of 43rd Annual Meeting of the ACL, pages 271-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="16406" citStr="Stolcke, 2002" startWordPosition="3003" endWordPosition="3005">X&amp; t� 1 2 3 4 5 6 7 8 DNP ��� ��� ��� NP ��� 2 3 5NN 6 NN NP DEG ��� ��� ��� ��� 8NP 1 2 3 4 5 6 7 8 4 7 DNP NP ��� ��� ��� 的 of ��� ��� ��� ��� ��� 613 System Features BLEU4 Pharaoh d + O(e|f) 0.0573 ± 0.0033 d + lm + O(e|f) + wp 0.2019 ± 0.0083 d + lm + O(f|e) + lex(f|e) + O(e|f) + lex(e|f) + pp + wp 0.2089 ± 0.0089 Lynx h1 0.1639 ± 0.0077 h1 + h6 + h7 0.2100 ± 0.0089 h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ± 0.0080 Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus 949, 583 English words. For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences. We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus. We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams. 5.1 Pharaoh The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely availa</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05).</booktitle>
<contexts>
<context position="17927" citStr="Venugopal and Vogel, 2005" startWordPosition="3246" endWordPosition="3249">2003) to obtain a single many-to-many word alignment for each sentence pair. After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy. Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting. To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005). We used default pruning settings for Pharaoh except that we set the distortion limit to 4. 5.2 Lynx On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005). The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus). To run our decoder Lynx o</context>
</contexts>
<marker>Venugopal, Vogel, 2005</marker>
<rawString>Ashish Venugopal and Stephan Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="2295" citStr="Wu (1997)" startWordPosition="369" endWordPosition="370">to a target string eI1 = el, ... , ei, ... , eI. Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data. However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly. Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information. Recent research on statistical machine translation has lead to the development of syntax-based models. Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar. Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer. Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="18203" citStr="Xiong et al., 2005" startWordPosition="3296" endWordPosition="3299">ed 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting. To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005). We used default pruning settings for Pharaoh except that we set the distortion limit to 4. 5.2 Lynx On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005). The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus). To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001. 5.3 Results Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings. The 95% confidence intervals were com</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Proceedings ofIJCNLP 2005, pages 70-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3480" citStr="Yamada and Knight (2001)" startWordPosition="544" endWordPosition="547">es hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar. Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages. Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609–616, Sydney, July 2006. c�2006 Association for Computational Linguistics � operations that transform a target parse tree into a source string. Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of 39th Annual Meeting of the ACL, pages 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2051--2054</pages>
<contexts>
<context position="18863" citStr="Zhang et al., 2004" startWordPosition="3414" endWordPosition="3417">270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate. Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus). To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001. 5.3 Results Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings. The 95% confidence intervals were computed using Zhang’s significance tester (Zhang et al., 2004). We modified it to conform to NIST’s current definition of the BLEU brevity penalty. For Pharaoh, eight features were used: distortion model d, a trigram language model lm, phrase translation probabilities O(f|e) and O(e|f), lexical weightings lex(f|e) and lex(e|f), phrase penalty pp, and word penalty wp. For Lynx, seven features described in section 2 were used. We find that Lynx outperforms Pharaoh with all feature settings. With full features, Lynx achieves an absolute improvement of 0.006 over Pharaoh (3.1% relative). This difference is statistically significant (p &lt; 0.01). Note that Lynx</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC), pages 2051-2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>