<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004638">
<title confidence="0.9972975">
Spice it Up? Mining Refinements to Online
Instructions from User Generated Content
</title>
<author confidence="0.987898">
Gregory Druck Bo Pang
</author>
<affiliation confidence="0.878234">
Yahoo! Research Yahoo! Research
</affiliation>
<email confidence="0.99543">
gdruck@gmail.com bopang42@gmail.com
</email>
<sectionHeader confidence="0.995575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964142857143">
There are a growing number of popular web
sites where users submit and review instruc-
tions for completing tasks as varied as build-
ing a table and baking a pie. In addition to pro-
viding their subjective evaluation, reviewers
often provide actionable refinements. These
refinements clarify, correct, improve, or pro-
vide alternatives to the original instructions.
However, identifying and reading all relevant
reviews is a daunting task for a user. In this
paper, we propose a generative model that
jointly identifies user-proposed refinements in
instruction reviews at multiple granularities,
and aligns them to the appropriate steps in the
original instructions. Labeled data is not read-
ily available for these tasks, so we focus on
the unsupervised setting. In experiments in the
recipe domain, our model provides 90.1% Fl
for predicting refinements at the review level,
and 77.0% Fl for predicting refinement seg-
ments within reviews.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901813953488">
People turn to the web to seek advice on a wide
variety of subjects. An analysis of web search
queries posed as questions revealed that “how to”
questions are the most popular (Pang and Kumar,
2011). People consult online resources to answer
technical questions like “how to put music on my
ipod,” and to find instructions for tasks like tying
a tie and cooking Thanksgiving dinner. Not sur-
prisingly, there are many Web sites dedicated to
providing instructions. For instance, on the pop-
ular DIY site instructables.com (“share what you
make”), users post instructions for making a wide
variety of objects ranging from bed frames to “The
Stirling Engine, absorb energy from candles, coffee,
and more!1” There are also sites like allrecipes.com
that are dedicated to a specific domain. On these
community-based instruction sites, instructions are
posted and reviewed by users. For instance, the
aforementioned “Stirling engine” has received over
350 reviews on instructables.com.
While user-generated instructions greatly increase
the variety of instructions available online, they
are not necessarily foolproof, or appropriate for all
users. For instance, in the case of recipes, a user
missing a certain ingredient at home might wonder
whether it can be safely omitted; a user who wants
to get a slightly different flavor might want to find
out what substitutions can be used to achieve that ef-
fect. Reviews posted by other users provide a great
resource for mining such information. In recipe re-
views, users often offer their customized version of
the recipe by describing changes they made: e.g., “I
halved the salt” or “I used honey instead of sugar.”
In addition, they may clarify portions of the instruc-
tions that are too concise for a novice to follow, or
describe changes to the cooking method that result
in a better dish. We refer to such actionable infor-
mation as a refinement.
Refinements can be quite prevalent in instruction
reviews. In a random sample of recipe reviews
from allrecipes.com, we found that 57.8% contain
refinements of the original recipe. However, sift-
ing through all reviews for refinements is a daunting
</bodyText>
<footnote confidence="0.7587515">
1http://www.instructables.com/id/
The-Sterling-Engine-absorb-energy-from-candles-c
</footnote>
<page confidence="0.950992">
545
</page>
<note confidence="0.985983">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 545–553,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999965025641026">
task for a user. Instead, we would like to automat-
ically identify refinements in reviews, summarize
them, and either create an annotated version of the
instructions that reflects the collective experience of
the community, or, more ambitiously, revise the in-
structions directly.
In this paper, we take first steps toward these goals
by addressing the following tasks: (1) identifying re-
views that contain refinements, (2) identifying text
segments within reviews that describe refinements,
and (3) aligning these refinement segments to steps
in the instructions being reviewed (Figure 1 provides
an example). Solving these tasks provides a foun-
dation for downstream summarization and seman-
tic analysis, and also suggests intermediate applica-
tions. For example, we can use review classifica-
tion to filter or rank reviews as they are presented to
future users, since reviews that contain refinements
are more informative than a review which only says
“Great recipe, thanks for posting!”
To the best of our knowledge, no previous work
has explored this aspect of user-generated text.
While review mining has been studied extensively,
we differ from previous work in that instead of fo-
cusing on evaluative information, we focus action-
able information in the reviews. (See Section 2 for a
more detailed discussion.)
There is no existing labeled data for the tasks of
interest, and we would like the methods we develop
to be easily applied in multiple domains. Motivated
by this, we propose a generative model for solving
these tasks jointly without labeled data. Interest-
ingly, we find that jointly modeling refinements at
both the review and segment level is beneficial. We
created a new recipe data set, and manually labeled
a random sample to evaluate our model and several
baselines. We obtain 90.1% Fl for predicting refine-
ments at the review level, and 77.0% Fl for predict-
ing refinement segments within reviews.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999970574074074">
At first glance, the task of identifying refinements
appears similar to subjectivity detection (see (Pang
and Lee, 2008) for a survey). However, note that an
objective sentence is not necessarily a refinement:
e.g., “I took the cake to work”; and a subjective sen-
tence can still contain a refinement: e.g., “I reduced
the sugar and it came out perfectly.”
Our end goal is similar to review summarization.
However, previous work on review summarization
(Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov
and McDonald, 2008) in product or service domains
focused on summarizing evaluative information —
more specifically, identifying ratable aspects (e.g.,
“food” and “service” for restaurants) and summariz-
ing the overall sentiment polarity for each aspect. In
contrast, we are interested in extracting a subset of
the non-evaluative information. Rather than ratable
aspects that are common across the entire domain
(e.g., “ingredient”, “cooking method”), we are in-
terested in actionable information that is related and
specific to the subject of the review.
Note that while our end goal is to summa-
rize objective information, it is still very differ-
ent from standard multi-document summarization
(Radev et al., 2002) of news articles. Apart from
differences in the quantity and the nature of the in-
put, we aim to summarize a distribution over what
should or can be changed, rather than produce a con-
sensus using different accounts of an event. In terms
of modeling approaches, in the context of extractive
summarization, Barzilay and Lee (2004) model con-
tent structure (i.e., the order in which topics appear)
in documents. We also model document structure,
but we do so to help identify refinement segments.
We share with previous work on predicting re-
view quality or helpfulness an interest in identify-
ing “informative” text. Early work tried to exploit
the intuition that a helpful review is one that com-
ments on product details. However, incorporating
product-aspect-mention count (Kim et al., 2006) or
similarity between the review and product specifi-
cation (Zhang and Varadarajan, 2006) as features
did not seem to improve the performance when the
task was predicting the percentage of helpfulness
votes. Instead of using the helpfulness votes, Liu
et al. (2007) manually annotated reviews with qual-
ity judgements, where a best review was defined as
one that contains complete and detailed comments.
Our notion of informativeness differs from previ-
ous work. We do not seek reviews that contain de-
tailed evaluative information; instead, we seek re-
views that contain detailed actionable information.
Furthermore, we are not expecting any single review
to be comprehensive; rather, we seek to extract a
</bodyText>
<page confidence="0.997064">
546
</page>
<bodyText confidence="0.999909">
collection of refinements representing the collective
wisdom of the community.
To the best of our knowledge, there is little pre-
vious work on mining user-generated data for ac-
tionable information. However, there has been in-
creasing interest in language grounding. In partic-
ular, recent work has studied learning to act in an
external environment by following textual instruc-
tions (Branavan et al., 2009, 2010, 2011; Vogel and
Jurafsky, 2010). This line of research is complemen-
tary to our work. While we do not utilize extensive
linguistic knowledge to analyze actionable informa-
tion, we view this is an interesting future direction.
We propose a generative model that makes pre-
dictions at both the review and review segment level.
Recent work uses a discriminative model with a sim-
ilar structure to perform sentence-level sentiment
analysis with review-level supervision (T¨ackstr¨om
and McDonald, 2011). However, sentiment polarity
labels at the review level are easily obtained. In con-
trast, refinement labels are not naturally available,
motivating the use of unsupervised learning. Note
that the model of T¨ackstr¨om and McDonald (2011)
cannot be used in a fully unsupervised setting.
</bodyText>
<sectionHeader confidence="0.998593" genericHeader="method">
3 Refinements
</sectionHeader>
<bodyText confidence="0.999988088235294">
In this section, we define refinements more pre-
cisely. We use recipes as our running example, but
our problem formulation and models are not specific
to this domain.
A refinement is a piece of text containing action-
able information that is not entailed by the original
instructions, but can be used to modify or expand the
original instructions. A refinement could propose an
alternative method or an improvement (e.g., “I re-
placed half of the shortening with butter”, “Let the
shrimp sit in 1/2 marinade for 3 hours”), as well as
provide clarification (“definitely use THIN cut pork
chops, otherwise your panko will burn before your
chops are cooked”).
Furthermore, we distinguish between a verified
refinement (what the user actually did) and a hy-
pothetical refinement (“next time I think I will try
evaporated milk”). In domains similar to recipes,
where instructions may be carried out repeatedly,
there exist refinements in both forms. Since instruc-
tions should, in principle, contain information that
has been well tested, in this work, we consider only
the former as our target class. In a small percent-
age of reviews we observed “failed attempts” where
a user did not follow a certain step and regretted the
diversion. In this work, we do not consider them to
be refinements. We refer to text that does not contain
refinements as background.
Finally, we note that the presence of a past tense
verb does not imply a refinement (e.g., “Everyone
loved this dish”, “I got many compliments”). In fact,
not all text segments that describe an action are re-
finements (e.g., “I took the cake to work”, “I fol-
lowed the instructions to a T”).
</bodyText>
<sectionHeader confidence="0.996481" genericHeader="method">
4 Models
</sectionHeader>
<bodyText confidence="0.999988071428571">
In this section we describe our models. To iden-
tify refinements without labeled data, we propose
a generative model of reviews (or more gener-
ally documents) with latent variables. We assume
that each review x is divided into segments, x =
(x1,... , xT). Each segment is a sub-sentence-level
text span. We assume that the segmentation is ob-
served, and hence it is not modeled. The segmenta-
tion procedure we use is described in Section 5.1.
While we focus on the unsupervised setting, note
that the model can also be used in a semi-supervised
setting. In particular, coarse (review-level) labels
can be used to guide the induction of fine-grained
latent structure (segment labels, alignments).
</bodyText>
<subsectionHeader confidence="0.997845">
4.1 Identifying Refinements
</subsectionHeader>
<bodyText confidence="0.999985625">
We start by directly modeling refinements at the seg-
ment level. Our first intuition is that refinement and
background segments can often be identified by lex-
ical differences. Based on this intuition, we can ig-
nore document structure and generate the segments
with a segment-level mixture of multinomials (S-
Mix). In general we could use n multinomials to
represent refinements and m multinomials to repre-
sent background text, but in this paper we simply use
n = m = 1. Therefore, unsupervised learning in S-
Mix can be viewed as clustering the segments with
two latent states. As is standard practice in unsu-
pervised learning, we subsequently map these latent
states onto the labels of interest: r and b, for refine-
ment and background, respectively. Note, however,
that this model ignores potential sequential depen-
</bodyText>
<page confidence="0.984354">
547
</page>
<bodyText confidence="0.999852875">
dencies among segments. A segment following a re-
finement segment in a review may be more likely to
be a refinement than background, for example.
To incorporate this intuition, we could instead
generate reviews with a HMM (Rabiner, 1989) over
segments (S-HMM) with two latent states. Let zi
be the latent label variable for the ith segment. The
joint probability of a review and segment labeling is
</bodyText>
<equation confidence="0.998379333333333">
T
p(x, z; θ) = H p(zj|zj−1; θ)p(xj|zj; θ), (1)
j=1
</equation>
<bodyText confidence="0.991427478260869">
where p(zj|zj−1; θ) are multinomial transition dis-
tributions, allowing the model to learn that p(zj =
r|zj−1 = r; θ) &gt; p(zj = b|zj−1 = r; θ) as moti-
vated above, and p(xj|zj; θ) are multinomial emis-
sion distributions. Note that all words in a segment
are generated independently conditioned on zj.
While S-HMM models sequential dependencies,
note that it imposes the same transition probabili-
ties on each review. In a manually labeled random
sample of recipe reviews, we find that refinement
segments tend to be clustered together in certain re-
views (“bursty”), rather than uniformly distributed
across all reviews. Specifically, while we estimate
that 23% of all segments are refinements, 42% of
reviews do not contain any refinements. In reviews
that contain a refinement, 34% of segments are re-
finements. S-HMM cannot model this phenomenon.
Consequently, we extend S-HMM to include a la-
tent label variable y for each review that takes val-
ues yes (contains refinement) and no (does not con-
tain refinement). The extended model is a mixture
of HMMs (RS-MixHMM) where y is the mixture
component.
</bodyText>
<equation confidence="0.994455">
p(x, y, z; θ) = p(y; θ)p(x, z|y; θ) (2)
</equation>
<bodyText confidence="0.999236142857143">
The two HMMs p(x, z  |y=yes; θ) and p(x, z  |y=
no; θ) can learn different transition multinomials
and consequently different distributions over z for
different y. On the other hand, we do not believe
the textual content of the background segments in a
y = yes review should be different from those in
a y = no review. Thus, the emission distributions
are shared between the two HMMs, p(xj|zj, y; θ) =
p(xj|zj; θ).
Note that the definition of y imposes additional
constraints on RS-MixHMM: 1) reviews with y =no
cannot contain refinement segments, and 2) reviews
with y = yes must contain at least one refinement
segment. We enforce constraint (1) by disallow-
ing refinement segments zj = r when y = no:
p(zj = r|zj−1, y = no; θ) = 0. Therefore, with
one background label, only the all background la-
bel sequence has non-zero probability when y = no.
Enforcing constraint (2) is more challenging, as the
y = yes HMM must assign zero probability when
all segments are background, but permit background
segments when refinement segments are present.
To enforce constraint (2), we “rewire” the HMM
structure for y = yes so that a path that does not
go through the refinement state r is impossible. We
first expand the state representation by replacing b
with two states that encode whether or not the first
r has been encountered yet: bnot−yet encodes that
all previous states in the path have also been back-
ground; bok encodes that at least one refinement state
has been encountered2. We prohibit paths from end-
ing with bnot−yet by augmenting RS-MixHMM with
a special final state f, and fixing p(zT+1 = f|zT =
bnot−yet, y = yes; θ) = 0. Furthermore, to enforce
the correct semantics of each state, paths cannot start
with bok, p(z1 = bok|y = yes; θ) = 0, and transi-
tions from bnot−yet to bok, bok to bnot−yet, and r to
bnot−yet are prohibited.
Note that RS-MixHMM also generalizes to the
case where there are multiple refinement (n&gt;1) and
background (m &gt; 1) labels. Let Zt be the set of
refinement labels, and Zb be the set of background
labels. The transition structure is analogous to the
n = m = 1 case, but statements involving r are ap-
plied for each z ∈ Z, and statements involving b are
applied for each z ∈ Zb. For example, the y = yes
HMM contains 2|Zb |background states.
In summary, the generative process of RS-
MixHMM involves first selecting whether the re-
view will contain a refinement. If the answer is yes,
a sequence of background segments and at least one
refinement segment are generated using the y = yes
HMM. If the answer is no, only background seg-
ments are generated. Interestingly, by enforcing
constraints (1) and (2), we break the label symme-
try that necessitates mapping latent states onto labels
</bodyText>
<footnote confidence="0.989143666666667">
2In this paper, the two background states share emission
multinomials, p(xj|zj = bnat−yet; 9) = p(xj|zj = bak; 9),
though this is not required.
</footnote>
<page confidence="0.989791">
548
</page>
<bodyText confidence="0.999987870967742">
when using S-Mix and S-HMM. Indeed, in the ex-
periments we present in Section 5.3, mapping is not
necessary for RS-MixHMM.
Note that the relationship between document-
level labels and segment-level labels that we model
is related to the multiple-instance setting (Dietterich
et al., 1997) in the machine learning literature. In
multiple-instance learning (MIL), rather than having
explicit labels at the instance (e.g., segment) level,
labels are given for bags of instances (e.g., docu-
ments). In the binary case, a bag is negative only
if all of its instances are negative. While we share
this problem formulation, work on MIL has mostly
focussed on supervised learning settings, and thus
it is not directly applicable to our unsupervised set-
ting. Foulds and Smyth (2011) propose a generative
model for MIL in which the generation of the bag
label y is conditioned on the instance labels z. As a
result of this setup, their model reduces to our S-Mix
baseline in a fully unsupervised setting.
Finally, although we motivated including the
review-level latent variable y as a way to improve
segment-level prediction of z, note that predictions
of y are useful in and of themselves. They provide
some notion of review usefulness and can be used to
filter reviews for search and browsing. They addi-
tionally give us a way to measure whether a set of
instructions is often modified or performed as speci-
fied. Finally, if we want to provide supervision, it is
much easier to annotate whether a review contains a
refinement than to annotate each segment.
</bodyText>
<subsectionHeader confidence="0.997603">
4.2 Alignment with the Instructions
</subsectionHeader>
<bodyText confidence="0.997598222222222">
In addition to the review x, we also observe the set of
instructions s being discussed. Often a review will
reference specific parts of the instructions. We as-
sume that each set of instructions is segmented into
steps, s = (s1, ... , sS). We augment our model
with latent alignment variables a = (a1, ... , aT),
where aj = n denotes that the jth review segment is
referring to the nth step of s. We also define a special
NULL instruction step. An alignment to NULL sig-
nifies that the segment does not refer to a specific in-
struction step. Note that this encoding assumes that
each review segment refers to at most one instruction
step. Alignment predictions could facilitate further
analysis of how refinements affect the instructions,
as well as aid in summarization and visualization of
refinements.
The joint probability under the augmented model,
which we refer to as RSA-MixHMM, is
</bodyText>
<equation confidence="0.997465">
p(a, x, y, z|s; θ) = p(y; θ)p(a, x, z|y, s; θ) (3)
T
p(a, x, z|y, s; θ) = H p(aj, zj|aj−1, zj−1, y, s; θ)
j=1
× p(xj|aj, zj, s; θ).
</equation>
<bodyText confidence="0.999907769230769">
Note that the instructions s are assumed to be ob-
served and hence are not generated by the model.
RSA-MixHMM can be viewed as a mixture of
HMMs where each state encodes both a segment la-
bel zj and an alignment variable aj. Encoding an
alignment problem as a sequence labeling problem
was first proposed by Vogel et al. (1996). Note that
RSA-MixHMM uses a similar expanded state rep-
resentation and transition structure as RS-MixHMM
to encode the semantics of y.
In our current model, the transition probability de-
composes into the product of independent label tran-
sition and alignment transition probabilities
</bodyText>
<equation confidence="0.845013">
p(aj, zj|aj−1, zj−1, y, s; θ) =p(aj|aj−1, y, s; θ)
× p(zj|zj−1, y, s; θ),
</equation>
<bodyText confidence="0.99946925">
and p(aj|aj−1, y, s; θ) = p(aj|y, s; θ) simply en-
codes the probability that segments align to a (non-
NULL) instruction step given y. This allows the
model to learn, for example, that reviews that con-
tain refinements refer to the instructions more often.
Intuitively, a segment and the step it refers to
should be lexically similar. Consequently, RSA-
MixHMM generates segments using a mixture of the
multinomial distribution for the segment label zj and
the (fixed) multinomial distribution3 for the step sai.
In this paper, we do not model the mixture proba-
bility and simply assume that all overlapping words
are generated by the instruction step. When aj =
NULL, only the segment label multinomial is used.
Finally, we disallow an alignment to a non-NULL
step if no words overlap: p(xj|aj, zj, s; θ) = 0.
</bodyText>
<subsectionHeader confidence="0.974127">
4.3 Inference and Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9996735">
Because our model is tree-structured, we can
efficiently compute exact marginal distributions
</bodyText>
<footnote confidence="0.788087">
3Stopwords are removed from the instruction step.
</footnote>
<page confidence="0.998039">
549
</page>
<bodyText confidence="0.9856265">
over latent variables using the sum-product algo-
rithm (Koller and Friedman, 2009). Similarly, to
find maximum probability assignments, we use the
max-product algorithm.
At training time we observe a set of re-
views and corresponding instructions, D =
{(x1, s1), ... , (xN, sN)1. The other variables, y, z,
and a, are latent. For all models, we estimate param-
eters to maximize the marginal likelihood of the ob-
served reviews. For example, for RSA-MixHMM,
we estimate parameters using
p(a, x2, y, z|s2; 0).
This problem cannot be solved analytically, so we
use the Expectation Maximization (EM) algorithm.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.844003">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999956777777778">
In this paper, we use recipes and reviews from
allrecipes.com, an active community where we es-
timate that the mean number of reviews per recipe is
54.2. We randomly selected 22,437 reviews for our
data set. Of these, we randomly selected a subset
of 550 reviews and determined whether or not each
contains a refinement, using the definition provided
in Section 3. In total, 318 of the 550 (57.8%) con-
tain a refinement. We then randomly selected 119 of
the 550 and labeled the individual segments. Of the
712 segments in the selected reviews, 165 (23.2%)
are refinements and 547 are background.
We now define our review segmentation scheme.
Most prior work on modeling latent document sub-
structure uses sentence-level labels (Barzilay and
Lee, 2004; T¨ackstr¨om and McDonald, 2011). In
the recipe data, we find that sentences often con-
tain both refinement and background segments: “[I
used a slow cooker with this recipe and] [it turned
out great!]” Additionally, we find that sentences of-
ten contain several distinct refinements: “[I set them
on top and around the pork and] [tossed in a can
of undrained french cut green beans and] [cooked
everything on high for about 3 hours].” To make re-
finements easier to identify, and to facilitate down-
stream processing, we allow sub-sentence segments.
Our segmentation procedure leverages a phrase
structure parser. In this paper we use the Stanford
Parser4. Based on a quick manual inspection, do-
main shift and ungrammatical sentences do cause
a significant degradation in parsing accuracy when
compared to in-domain data. However, this is ac-
ceptable because we only use the parser for segmen-
tation. We first parse the entire review, and subse-
quently iterate through the tokens, adding a segment
break when any of the following conditions is met:
</bodyText>
<listItem confidence="0.998864">
• sentence break (determined by the parser)
• token is a coordinating conjunction (CC) with
parent other than NP, PP, ADJP
• token is a comma (,) with parent other than NP,
PP, ADJP
• token is a colon (:)
</listItem>
<bodyText confidence="0.997658">
The resulting segmentations are fixed during learn-
ing. In future work we could extend our model to
additionally identify segment boundaries.
</bodyText>
<subsectionHeader confidence="0.997767">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998325">
We first describe the methods we evaluate. For com-
parison, we provide results with a baseline that ran-
domly guesses according to the class distribution for
each task. We also evaluate a Review-level model:
</bodyText>
<listItem confidence="0.8180335">
• R-Mix: A review-level mixture of multinomi-
als with two latent states.
</listItem>
<bodyText confidence="0.995885571428571">
Note that this is similar to clustering at the review
level, except that class priors are estimated. R-Mix
does not provide segment labels, though they can be
obtained by labeling all segments with the review
label.
We also evaluate the two Segment-level models
described in Section 4.1 (with two latent states):
</bodyText>
<listItem confidence="0.9999995">
• S-Mix: A segment-level mixture model.
• S-IEMM: A segment-level HMM (Eq. 1).
</listItem>
<bodyText confidence="0.9993246">
These models do not provide review labels. To ob-
tain them, we assign y = yes if any segment is la-
beled as a refinement, and y =no otherwise.
Finally, we evaluate three versions of our model
(Review + Segment and Review + Segment +
</bodyText>
<footnote confidence="0.723727">
4http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<figure confidence="0.978866285714286">
arg max
θ
�
log
a,z,y
N
2=1
</figure>
<page confidence="0.990523">
550
</page>
<bodyText confidence="0.996056">
Alignment) with one refinement segment label and
one background segment label5:
</bodyText>
<listItem confidence="0.996273">
• RS-MixHMM: A mixture of HMMs (Eq. 2)
with constraints (1) and (2) (see Section 4).
• RS-MixMix: A variant of RS-MixHMM with-
out sequential dependencies.
• RSA-MixHMM: The full model that also in-
corporates alignment (Eq. 3).
</listItem>
<bodyText confidence="0.999824588235294">
Segment multinomials are initialized with a small
amount of random noise to break the initial symme-
try. RSA-MixHMM segment multinomials are in-
stead initialized to the RS-MixHMM solution. We
apply add-0.01 smoothing to the emission multino-
mials and add-1 smoothing to the transition multi-
nomials in the M-step. We estimate parameters with
21,887 unlabeled reviews by running EM until the
relative percentage decrease in the marginal likeli-
hood is &lt; 10−4 (typically 10-20 iterations).
The models are evaluated on refinement F1 and
accuracy for both review and segment predictions
using the annotated data described in Section 5.1.
For R-Mix and the segment (S-) models, we select
the 1:1 mapping of latent states to labels that maxi-
mizes F1. For RSA-MixHMM and the RS- models
this was not necessary (see Section 4.1).
</bodyText>
<subsectionHeader confidence="0.815397">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.9999666875">
Table 1 displays the results. R-Mix fails to ac-
curately distinguish refinement and background re-
views. The words that best discriminate the two
discovered review classes are “savory ingredients”
(chicken, pepper, meat, garlic, soup) and “bak-
ing/dessert ingredients” (chocolate, cake, pie, these,
flour). In other words, reviews naturally cluster by
topics rather than whether they contain refinements.
The segment models (S-) substantially outper-
form R-Mix on all metrics, demonstrating the ben-
efit of segment-level modeling and our segmenta-
tion scheme. However, S-HMM fails to model
the “burstiness” of refinement segments (see Sec-
tion 4.1). It predicts that 76.2% of reviews con-
tain refinements, and additionally that 40.9% of seg-
ments contain refinements, whereas the true values
</bodyText>
<footnote confidence="0.859895333333333">
5Attempts at modeling refinement and background sub-
types by increasing the number of latent states failed to sub-
stantially improve the results.
</footnote>
<bodyText confidence="0.999937097560976">
are 57.8% and 23.2%, respectively. As a result, these
models provide high recall but low precision.
In comparison, our models, which model the re-
view labels6 y, yield more accurate refinement pre-
dictions. They provide statistically significant im-
provements in review and segment F1, as well as
accuracy, over the baseline models. RS-MixHMM
predicts that 62.9% of reviews contain refinements
and 28.2% of segments contain refinements, values
that are much closer to the ground truth. The re-
finement emission distributions for S-HMM and RS-
MixHMM are fairly similar, but the probabilities of
several key terms like added, used, and instead are
higher with RS-MixHMM.
The review F1 results demonstrate that our mod-
els are able to very accurately distinguish refinement
reviews from background reviews. As motivated in
Section 4.1, there are several applications that can
benefit from review-level predictions directly. Addi-
tionally, note that review labeling is not a trivial task.
We trained a supervised logistic regression model
with bag-of-words and length features (for both the
number of segments and the number of words) using
10-fold cross validation on the labeled dataset. This
supervised model yields mean review F1 of 78.4,
11.7 F1 points below the best unsupervised result7.
Augmenting RS-MixMix with sequential depen-
dencies, yielding RS-MixHMM, provides a mod-
erate (though not statistically significant) improve-
ment in segment F1. RS-MixHMM learns that re-
finement reviews typically begin and end with back-
ground segments, and that refinement segments tend
to appear in succession.
RSA-MixHMM additionally learns that segments
in refinement reviews are more likely to align to non-
NULL recipe steps. It also encourages the segment
multinomials to focus modeling effort on words that
appear only in the reviews. As a result, in addition to
yielding alignments, RSA-MixHMM provides small
improvements over RS-MixHMM (though they are
not statistically significant).
</bodyText>
<footnote confidence="0.50233875">
6We note that enforcing the constraint that a refinement re-
view must contain at least one refinement segment using the
method in Section 4.1 provides a statistically significant signif-
icant improvement in review Fl of 4.0 for RS-MixHMM.
7Note that we do not consider this performance to be the
upper-bound of supervised approaches; clearly, supervised ap-
proaches could benefit from additional labeled data. However,
labeled data is relatively expensive to obtain for this task.
</footnote>
<page confidence="0.985677">
551
</page>
<table confidence="0.999566888888889">
Model review (57.8% refinement) segment (23.2% refinement)
acc prec rec F1 acc prec rec F1
random baseline 51.21 57.8 57.8 57.81 64.41 23.2 23.2 23.21
R-Mix 61.51 69.1 60.4 64.41 55.81 27.9 57.6 37.61
S-Mix 77.51 72.4 98.7 83.51 80.61 54.7 95.2 69.51
S-HMM 79.81 74.7 98.4 84.91 80.31 54.3 95.8 69.31
RS-MixMix 87.1 85.4 93.7 89.4 86.4 65.6 86.7 74.7
RS-MixHMM 87.3 85.6 93.7 89.5 87.9 69.7 84.8 76.5
RSA-MixHMM 88.2 87.1 93.4 90.1 88.5 71.7 83.0 77.0
</table>
<tableCaption confidence="0.76333">
Table 1: Unsupervised experiments comparing models for review and segment refinement identification on the recipe
data. Bold indicates the best result, and a † next to an accuracy or Fl value indicates that the improvements obtained
by RS-MixMix, RS-MixHMM, and RSA-MixHMM are significant (p = 0.05 according to a bootstrap test).
</tableCaption>
<table confidence="0.765028888888889">
1. Preheat oven to 375 degrees F (190 degrees C).
2. Lightly oil 18 muffin cups, or coat with nonstick
cooking spray.
3. In a medium bowl, whisk together eggs, egg whites,
apple butter, oil and vanilla.
4. In a large bowl, stir together flours, sugar, cinnamon,
baking powder, baking soda and salt.
5. Stir in carrots, apples and raisins.
6. Stir in apple butter mixture until just moistened.
</table>
<bodyText confidence="0.9322736875">
7. Spoon the batter into the prepared muffin cups, filling
them about 3/4 full.
8. In a small bowl, combine walnuts and wheat germ;
sprinkle over the muffin tops.
9. Bake at 375 degrees F (190 degrees C) for 15 to 20
minutes, or until the tops are golden and spring back
when lightly pressed.
[ I loved these muffins! ] [ I used walnuts inside
the batter and ] [ used whole wheat flour only
as well as flaxseed instead of wheat germ. ]
[ They turned out great! ] [ I couldn&apos;t stop eating
them. ] [ I&apos;ve made several batches of these
muffins and all have been great. ] [ I make tiny
alterations each time usually. ] [ These muffins
are great with pears as well. ] [ I think golden
raisins are much better than regular also! ]
</bodyText>
<figureCaption confidence="0.999023">
Figure 1: Example output (best viewed in color). Bold segments in the review (left) are those predicted to be refine-
ments. Red indicates an incorrect segment label, according to our gold labels. Alignments to recipe steps (right) are
indicated with colors and arrows. Segments without colors and arrows align to the NULL recipe step (see Section 4.2).
</figureCaption>
<bodyText confidence="0.999995166666667">
We provide an example alignment in Figure 1.
Annotating ground truth alignments is challenging
and time-consuming due to ambiguity, and we feel
that the alignments are best evaluated via a down-
stream task. Therefore, we leave thorough evalua-
tion of the quality of the alignments to future work.
</bodyText>
<sectionHeader confidence="0.997012" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999973153846154">
In this paper, we developed unsupervised meth-
ods based on generative models for mining refine-
ments to online instructions from reviews. The pro-
posed models leverage lexical differences in refine-
ment and background segments. By augmenting the
base models with additional structure (review labels,
alignments), we obtained more accurate predictions.
However, to further improve accuracy, more lin-
guistic knowledge and structure will need to be in-
corporated. The current models provide many false
positives in the more subtle cases, when some words
that typically indicate a refinement are present, but
the text does not describe a refinement according to
the definition in Section 3. Examples include hypo-
thetical refinements (“next time I will substitute...”)
and discussion of the recipe without modification (“I
found it strange to... but it worked ...”, “I love bal-
samic vinegar and herbs”, “they baked up nicely”).
Other future directions include improving the
alignment model, for example by allowing words in
the instruction step to be “translated” into words in
the review segment. Though we focussed on recipes,
the models we proposed are general, and could be
applied to other domains. We also plan to consider
this task in other settings such as online forums, and
develop methods for summarizing refinements.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99466">
We thank Andrei Broder and the anonymous reviewers
for helpful discussions and comments.
</bodyText>
<page confidence="0.996134">
552
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999677288888889">
Regina Barzilay and Lillian Lee. Catching the drift:
Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113–120, 2004.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. Reinforcement learning for
mapping instructions to actions. In Proceedings
of the Association for Computational Linguistics
(ACL), 2009.
S.R.K Branavan, Luke Zettlemoyer, and Regina
Barzilay. Reading between the lines: Learning
to map high-level instructions to commands. In
Proceedings of the Association for Computational
Linguistics (ACL), 2010.
S.R.K. Branavan, David Silver, and Regina Barzilay.
Learning to win by reading manuals in a monte-
carlo framework. In Proceedings of the Associa-
tion for Computational Linguistics (ACL), 2011.
Thomas G. Dietterich, Richard H. Lathrop, and
Tom´as Lozano-P´erez. Solving the multiple in-
stance problem with axis-parallel rectangles. Ar-
tificial Intelligence, 89(1 - 2):31 – 71, 1997.
J. R. Foulds and P. Smyth. Multi-instance mixture
models and semi-supervised learning. In SIAM
International Conference on Data Mining, 2011.
Minqing Hu and Bing Liu. Mining and summa-
rizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168–177,
2004.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. Automatically assessing re-
view helpfulness. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 423–430, 2006.
D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques. MIT Press,
2009.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. Low-quality product
review detection in opinion summarization. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 334–342, 2007.
Bo Pang and Ravi Kumar. Search in the lost sense
of query: Question formulation in web search
queries and its temporal changes. In Proceedings
of the Association for Computational Linguistics
(ACL), 2011.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135, 2008.
Ana-Maria Popescu and Oren Etzioni. Extract-
ing product features and opinions from reviews.
In Proceedings of the Human Language Tech-
nology Conference and the Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP), 2005.
Lawrence Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257–286,
1989.
Dragomir R. Radev, Eduard Hovy, and Kathleen
McKeown. Introduction to the special issue on
summarization. Computational Linguistics, 28
(4):399–408, 2002. ISSN 0891-2017.
Oscar T¨ackstr¨om and Ryan McDonald. Discovering
fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the
33rd European conference on Advances in infor-
mation retrieval, ECIR’11, pages 368–374, 2011.
Ivan Titov and Ryan McDonald. A joint model of
text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL), 2008.
Adam Vogel and Daniel Jurafsky. Learning to fol-
low navigational directions. In Proceedings of the
Association for Computational Linguistics (ACL),
2010.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics - Volume 2, COL-
ING ’96, pages 836–841, 1996.
Zhu Zhang and Balaji Varadarajan. Utility scoring
of product reviews. In Proceedings of the ACM
SIGIR Conference on Information and Knowledge
Management (CIKM), pages 51–57, 2006.
</reference>
<page confidence="0.99901">
553
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900355">
<title confidence="0.996206">Spice it Up? Mining Refinements to Online Instructions from User Generated Content</title>
<author confidence="0.999483">Gregory Druck Bo Pang</author>
<affiliation confidence="0.949403">Yahoo! Research Yahoo! Research</affiliation>
<email confidence="0.975448">gdruck@gmail.combopang42@gmail.com</email>
<abstract confidence="0.998943954545455">There are a growing number of popular web sites where users submit and review instructions for completing tasks as varied as building a table and baking a pie. In addition to providing their subjective evaluation, reviewers provide These refinements clarify, correct, improve, or provide alternatives to the original instructions. However, identifying and reading all relevant reviews is a daunting task for a user. In this paper, we propose a generative model that jointly identifies user-proposed refinements in instruction reviews at multiple granularities, and aligns them to the appropriate steps in the original instructions. Labeled data is not readily available for these tasks, so we focus on the unsupervised setting. In experiments in the domain, our model provides 90.1% for predicting refinements at the review level, 77.0% predicting refinement segments within reviews.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Proceedings of the Main Conference,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="7023" citStr="Barzilay and Lee (2004)" startWordPosition="1093" endWordPosition="1096"> “ingredient”, “cooking method”), we are interested in actionable information that is related and specific to the subject of the review. Note that while our end goal is to summarize objective information, it is still very different from standard multi-document summarization (Radev et al., 2002) of news articles. Apart from differences in the quantity and the nature of the input, we aim to summarize a distribution over what should or can be changed, rather than produce a consensus using different accounts of an event. In terms of modeling approaches, in the context of extractive summarization, Barzilay and Lee (2004) model content structure (i.e., the order in which topics appear) in documents. We also model document structure, but we do so to help identify refinement segments. We share with previous work on predicting review quality or helpfulness an interest in identifying “informative” text. Early work tried to exploit the intuition that a helpful review is one that comments on product details. However, incorporating product-aspect-mention count (Kim et al., 2006) or similarity between the review and product specification (Zhang and Varadarajan, 2006) as features did not seem to improve the performance</context>
<context position="22673" citStr="Barzilay and Lee, 2004" startWordPosition="3726" endWordPosition="3729">reviews per recipe is 54.2. We randomly selected 22,437 reviews for our data set. Of these, we randomly selected a subset of 550 reviews and determined whether or not each contains a refinement, using the definition provided in Section 3. In total, 318 of the 550 (57.8%) contain a refinement. We then randomly selected 119 of the 550 and labeled the individual segments. Of the 712 segments in the selected reviews, 165 (23.2%) are refinements and 547 are background. We now define our review segmentation scheme. Most prior work on modeling latent document substructure uses sentence-level labels (Barzilay and Lee, 2004; T¨ackstr¨om and McDonald, 2011). In the recipe data, we find that sentences often contain both refinement and background segments: “[I used a slow cooker with this recipe and] [it turned out great!]” Additionally, we find that sentences often contain several distinct refinements: “[I set them on top and around the pork and] [tossed in a can of undrained french cut green beans and] [cooked everything on high for about 3 hours].” To make refinements easier to identify, and to facilitate downstream processing, we allow sub-sentence segments. Our segmentation procedure leverages a phrase structu</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004: Proceedings of the Main Conference, pages 113–120, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="8591" citStr="Branavan et al., 2009" startWordPosition="1340" endWordPosition="1343"> contain detailed evaluative information; instead, we seek reviews that contain detailed actionable information. Furthermore, we are not expecting any single review to be comprehensive; rather, we seek to extract a 546 collection of refinements representing the collective wisdom of the community. To the best of our knowledge, there is little previous work on mining user-generated data for actionable information. However, there has been increasing interest in language grounding. In particular, recent work has studied learning to act in an external environment by following textual instructions (Branavan et al., 2009, 2010, 2011; Vogel and Jurafsky, 2010). This line of research is complementary to our work. While we do not utilize extensive linguistic knowledge to analyze actionable information, we view this is an interesting future direction. We propose a generative model that makes predictions at both the review and review segment level. Recent work uses a discriminative model with a similar structure to perform sentence-level sentiment analysis with review-level supervision (T¨ackstr¨om and McDonald, 2011). However, sentiment polarity labels at the review level are easily obtained. In contrast, refinem</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. Reinforcement learning for mapping instructions to actions. In Proceedings of the Association for Computational Linguistics (ACL), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to win by reading manuals in a montecarlo framework.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>S.R.K. Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a montecarlo framework. In Proceedings of the Association for Computational Linguistics (ACL), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
<author>Richard H Lathrop</author>
<author>Tom´as Lozano-P´erez</author>
</authors>
<title>Solving the multiple instance problem with axis-parallel rectangles.</title>
<date>1997</date>
<journal>Artificial Intelligence, 89(1 - 2):31 –</journal>
<volume>71</volume>
<marker>Dietterich, Lathrop, Lozano-P´erez, 1997</marker>
<rawString>Thomas G. Dietterich, Richard H. Lathrop, and Tom´as Lozano-P´erez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1 - 2):31 – 71, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Foulds</author>
<author>P Smyth</author>
</authors>
<title>Multi-instance mixture models and semi-supervised learning.</title>
<date>2011</date>
<booktitle>In SIAM International Conference on Data Mining,</booktitle>
<contexts>
<context position="17795" citStr="Foulds and Smyth (2011)" startWordPosition="2894" endWordPosition="2897">p between documentlevel labels and segment-level labels that we model is related to the multiple-instance setting (Dietterich et al., 1997) in the machine learning literature. In multiple-instance learning (MIL), rather than having explicit labels at the instance (e.g., segment) level, labels are given for bags of instances (e.g., documents). In the binary case, a bag is negative only if all of its instances are negative. While we share this problem formulation, work on MIL has mostly focussed on supervised learning settings, and thus it is not directly applicable to our unsupervised setting. Foulds and Smyth (2011) propose a generative model for MIL in which the generation of the bag label y is conditioned on the instance labels z. As a result of this setup, their model reduces to our S-Mix baseline in a fully unsupervised setting. Finally, although we motivated including the review-level latent variable y as a way to improve segment-level prediction of z, note that predictions of y are useful in and of themselves. They provide some notion of review usefulness and can be used to filter reviews for search and browsing. They additionally give us a way to measure whether a set of instructions is often modi</context>
</contexts>
<marker>Foulds, Smyth, 2011</marker>
<rawString>J. R. Foulds and P. Smyth. Multi-instance mixture models and semi-supervised learning. In SIAM International Conference on Data Mining, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>168--177</pages>
<contexts>
<context position="5947" citStr="Hu and Liu, 2004" startWordPosition="925" endWordPosition="928"> We obtain 90.1% Fl for predicting refinements at the review level, and 77.0% Fl for predicting refinement segments within reviews. 2 Related Work At first glance, the task of identifying refinements appears similar to subjectivity detection (see (Pang and Lee, 2008) for a survey). However, note that an objective sentence is not necessarily a refinement: e.g., “I took the cake to work”; and a subjective sentence can still contain a refinement: e.g., “I reduced the sugar and it came out perfectly.” Our end goal is similar to review summarization. However, previous work on review summarization (Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov and McDonald, 2008) in product or service domains focused on summarizing evaluative information — more specifically, identifying ratable aspects (e.g., “food” and “service” for restaurants) and summarizing the overall sentiment polarity for each aspect. In contrast, we are interested in extracting a subset of the non-evaluative information. Rather than ratable aspects that are common across the entire domain (e.g., “ingredient”, “cooking method”), we are interested in actionable information that is related and specific to the subject of the review. Note that </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 168–177, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="7482" citStr="Kim et al., 2006" startWordPosition="1166" endWordPosition="1169">produce a consensus using different accounts of an event. In terms of modeling approaches, in the context of extractive summarization, Barzilay and Lee (2004) model content structure (i.e., the order in which topics appear) in documents. We also model document structure, but we do so to help identify refinement segments. We share with previous work on predicting review quality or helpfulness an interest in identifying “informative” text. Early work tried to exploit the intuition that a helpful review is one that comments on product details. However, incorporating product-aspect-mention count (Kim et al., 2006) or similarity between the review and product specification (Zhang and Varadarajan, 2006) as features did not seem to improve the performance when the task was predicting the percentage of helpfulness votes. Instead of using the helpfulness votes, Liu et al. (2007) manually annotated reviews with quality judgements, where a best review was defined as one that contains complete and detailed comments. Our notion of informativeness differs from previous work. We do not seek reviews that contain detailed evaluative information; instead, we seek reviews that contain detailed actionable information.</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. Automatically assessing review helpfulness. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423–430, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="21381" citStr="Koller and Friedman, 2009" startWordPosition="3512" endWordPosition="3515"> and the (fixed) multinomial distribution3 for the step sai. In this paper, we do not model the mixture probability and simply assume that all overlapping words are generated by the instruction step. When aj = NULL, only the segment label multinomial is used. Finally, we disallow an alignment to a non-NULL step if no words overlap: p(xj|aj, zj, s; θ) = 0. 4.3 Inference and Parameter Estimation Because our model is tree-structured, we can efficiently compute exact marginal distributions 3Stopwords are removed from the instruction step. 549 over latent variables using the sum-product algorithm (Koller and Friedman, 2009). Similarly, to find maximum probability assignments, we use the max-product algorithm. At training time we observe a set of reviews and corresponding instructions, D = {(x1, s1), ... , (xN, sN)1. The other variables, y, z, and a, are latent. For all models, we estimate parameters to maximize the marginal likelihood of the observed reviews. For example, for RSA-MixHMM, we estimate parameters using p(a, x2, y, z|s2; 0). This problem cannot be solved analytically, so we use the Expectation Maximization (EM) algorithm. 5 Experiments 5.1 Data In this paper, we use recipes and reviews from allrecip</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingjing Liu</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Yalou Huang</author>
<author>Ming Zhou</author>
</authors>
<title>Low-quality product review detection in opinion summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>334--342</pages>
<contexts>
<context position="7747" citStr="Liu et al. (2007)" startWordPosition="1208" endWordPosition="1211">cture, but we do so to help identify refinement segments. We share with previous work on predicting review quality or helpfulness an interest in identifying “informative” text. Early work tried to exploit the intuition that a helpful review is one that comments on product details. However, incorporating product-aspect-mention count (Kim et al., 2006) or similarity between the review and product specification (Zhang and Varadarajan, 2006) as features did not seem to improve the performance when the task was predicting the percentage of helpfulness votes. Instead of using the helpfulness votes, Liu et al. (2007) manually annotated reviews with quality judgements, where a best review was defined as one that contains complete and detailed comments. Our notion of informativeness differs from previous work. We do not seek reviews that contain detailed evaluative information; instead, we seek reviews that contain detailed actionable information. Furthermore, we are not expecting any single review to be comprehensive; rather, we seek to extract a 546 collection of refinements representing the collective wisdom of the community. To the best of our knowledge, there is little previous work on mining user-gene</context>
</contexts>
<marker>Liu, Cao, Lin, Huang, Zhou, 2007</marker>
<rawString>Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang, and Ming Zhou. Low-quality product review detection in opinion summarization. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 334–342, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Ravi Kumar</author>
</authors>
<title>Search in the lost sense of query: Question formulation in web search queries and its temporal changes.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="1330" citStr="Pang and Kumar, 2011" startWordPosition="202" endWordPosition="205">d refinements in instruction reviews at multiple granularities, and aligns them to the appropriate steps in the original instructions. Labeled data is not readily available for these tasks, so we focus on the unsupervised setting. In experiments in the recipe domain, our model provides 90.1% Fl for predicting refinements at the review level, and 77.0% Fl for predicting refinement segments within reviews. 1 Introduction People turn to the web to seek advice on a wide variety of subjects. An analysis of web search queries posed as questions revealed that “how to” questions are the most popular (Pang and Kumar, 2011). People consult online resources to answer technical questions like “how to put music on my ipod,” and to find instructions for tasks like tying a tie and cooking Thanksgiving dinner. Not surprisingly, there are many Web sites dedicated to providing instructions. For instance, on the popular DIY site instructables.com (“share what you make”), users post instructions for making a wide variety of objects ranging from bed frames to “The Stirling Engine, absorb energy from candles, coffee, and more!1” There are also sites like allrecipes.com that are dedicated to a specific domain. On these commu</context>
</contexts>
<marker>Pang, Kumar, 2011</marker>
<rawString>Bo Pang and Ravi Kumar. Search in the lost sense of query: Question formulation in web search queries and its temporal changes. In Proceedings of the Association for Computational Linguistics (ACL), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="5598" citStr="Pang and Lee, 2008" startWordPosition="866" endWordPosition="869">ied in multiple domains. Motivated by this, we propose a generative model for solving these tasks jointly without labeled data. Interestingly, we find that jointly modeling refinements at both the review and segment level is beneficial. We created a new recipe data set, and manually labeled a random sample to evaluate our model and several baselines. We obtain 90.1% Fl for predicting refinements at the review level, and 77.0% Fl for predicting refinement segments within reviews. 2 Related Work At first glance, the task of identifying refinements appears similar to subjectivity detection (see (Pang and Lee, 2008) for a survey). However, note that an objective sentence is not necessarily a refinement: e.g., “I took the cake to work”; and a subjective sentence can still contain a refinement: e.g., “I reduced the sugar and it came out perfectly.” Our end goal is similar to review summarization. However, previous work on review summarization (Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov and McDonald, 2008) in product or service domains focused on summarizing evaluative information — more specifically, identifying ratable aspects (e.g., “food” and “service” for restaurants) and summarizing the overal</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<contexts>
<context position="5974" citStr="Popescu and Etzioni, 2005" startWordPosition="929" endWordPosition="932">l for predicting refinements at the review level, and 77.0% Fl for predicting refinement segments within reviews. 2 Related Work At first glance, the task of identifying refinements appears similar to subjectivity detection (see (Pang and Lee, 2008) for a survey). However, note that an objective sentence is not necessarily a refinement: e.g., “I took the cake to work”; and a subjective sentence can still contain a refinement: e.g., “I reduced the sugar and it came out perfectly.” Our end goal is similar to review summarization. However, previous work on review summarization (Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov and McDonald, 2008) in product or service domains focused on summarizing evaluative information — more specifically, identifying ratable aspects (e.g., “food” and “service” for restaurants) and summarizing the overall sentiment polarity for each aspect. In contrast, we are interested in extracting a subset of the non-evaluative information. Rather than ratable aspects that are common across the entire domain (e.g., “ingredient”, “cooking method”), we are interested in actionable information that is related and specific to the subject of the review. Note that while our end goal is to su</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. Extracting product features and opinions from reviews. In Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="12814" citStr="Rabiner, 1989" startWordPosition="2034" endWordPosition="2035">aper we simply use n = m = 1. Therefore, unsupervised learning in SMix can be viewed as clustering the segments with two latent states. As is standard practice in unsupervised learning, we subsequently map these latent states onto the labels of interest: r and b, for refinement and background, respectively. Note, however, that this model ignores potential sequential depen547 dencies among segments. A segment following a refinement segment in a review may be more likely to be a refinement than background, for example. To incorporate this intuition, we could instead generate reviews with a HMM (Rabiner, 1989) over segments (S-HMM) with two latent states. Let zi be the latent label variable for the ith segment. The joint probability of a review and segment labeling is T p(x, z; θ) = H p(zj|zj−1; θ)p(xj|zj; θ), (1) j=1 where p(zj|zj−1; θ) are multinomial transition distributions, allowing the model to learn that p(zj = r|zj−1 = r; θ) &gt; p(zj = b|zj−1 = r; θ) as motivated above, and p(xj|zj; θ) are multinomial emission distributions. Note that all words in a segment are generated independently conditioned on zj. While S-HMM models sequential dependencies, note that it imposes the same transition proba</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence Rabiner. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Eduard Hovy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Introduction to the special issue on summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>4--399</pages>
<contexts>
<context position="6695" citStr="Radev et al., 2002" startWordPosition="1037" endWordPosition="1040">n — more specifically, identifying ratable aspects (e.g., “food” and “service” for restaurants) and summarizing the overall sentiment polarity for each aspect. In contrast, we are interested in extracting a subset of the non-evaluative information. Rather than ratable aspects that are common across the entire domain (e.g., “ingredient”, “cooking method”), we are interested in actionable information that is related and specific to the subject of the review. Note that while our end goal is to summarize objective information, it is still very different from standard multi-document summarization (Radev et al., 2002) of news articles. Apart from differences in the quantity and the nature of the input, we aim to summarize a distribution over what should or can be changed, rather than produce a consensus using different accounts of an event. In terms of modeling approaches, in the context of extractive summarization, Barzilay and Lee (2004) model content structure (i.e., the order in which topics appear) in documents. We also model document structure, but we do so to help identify refinement segments. We share with previous work on predicting review quality or helpfulness an interest in identifying “informa</context>
</contexts>
<marker>Radev, Hovy, McKeown, 2002</marker>
<rawString>Dragomir R. Radev, Eduard Hovy, and Kathleen McKeown. Introduction to the special issue on summarization. Computational Linguistics, 28 (4):399–408, 2002. ISSN 0891-2017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR’11,</booktitle>
<pages>368--374</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. Discovering fine-grained sentiment with latent variable structured prediction models. In Proceedings of the 33rd European conference on Advances in information retrieval, ECIR’11, pages 368–374, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="6001" citStr="Titov and McDonald, 2008" startWordPosition="933" endWordPosition="936">s at the review level, and 77.0% Fl for predicting refinement segments within reviews. 2 Related Work At first glance, the task of identifying refinements appears similar to subjectivity detection (see (Pang and Lee, 2008) for a survey). However, note that an objective sentence is not necessarily a refinement: e.g., “I took the cake to work”; and a subjective sentence can still contain a refinement: e.g., “I reduced the sugar and it came out perfectly.” Our end goal is similar to review summarization. However, previous work on review summarization (Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov and McDonald, 2008) in product or service domains focused on summarizing evaluative information — more specifically, identifying ratable aspects (e.g., “food” and “service” for restaurants) and summarizing the overall sentiment polarity for each aspect. In contrast, we are interested in extracting a subset of the non-evaluative information. Rather than ratable aspects that are common across the entire domain (e.g., “ingredient”, “cooking method”), we are interested in actionable information that is related and specific to the subject of the review. Note that while our end goal is to summarize objective informati</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of the Association for Computational Linguistics (ACL), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="8630" citStr="Vogel and Jurafsky, 2010" startWordPosition="1346" endWordPosition="1349">ation; instead, we seek reviews that contain detailed actionable information. Furthermore, we are not expecting any single review to be comprehensive; rather, we seek to extract a 546 collection of refinements representing the collective wisdom of the community. To the best of our knowledge, there is little previous work on mining user-generated data for actionable information. However, there has been increasing interest in language grounding. In particular, recent work has studied learning to act in an external environment by following textual instructions (Branavan et al., 2009, 2010, 2011; Vogel and Jurafsky, 2010). This line of research is complementary to our work. While we do not utilize extensive linguistic knowledge to analyze actionable information, we view this is an interesting future direction. We propose a generative model that makes predictions at both the review and review segment level. Recent work uses a discriminative model with a similar structure to perform sentence-level sentiment analysis with review-level supervision (T¨ackstr¨om and McDonald, 2011). However, sentiment polarity labels at the review level are easily obtained. In contrast, refinement labels are not naturally available,</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Daniel Jurafsky. Learning to follow navigational directions. In Proceedings of the Association for Computational Linguistics (ACL), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics - Volume 2, COLING ’96,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="19951" citStr="Vogel et al. (1996)" startWordPosition="3280" endWordPosition="3283">tructions, as well as aid in summarization and visualization of refinements. The joint probability under the augmented model, which we refer to as RSA-MixHMM, is p(a, x, y, z|s; θ) = p(y; θ)p(a, x, z|y, s; θ) (3) T p(a, x, z|y, s; θ) = H p(aj, zj|aj−1, zj−1, y, s; θ) j=1 × p(xj|aj, zj, s; θ). Note that the instructions s are assumed to be observed and hence are not generated by the model. RSA-MixHMM can be viewed as a mixture of HMMs where each state encodes both a segment label zj and an alignment variable aj. Encoding an alignment problem as a sequence labeling problem was first proposed by Vogel et al. (1996). Note that RSA-MixHMM uses a similar expanded state representation and transition structure as RS-MixHMM to encode the semantics of y. In our current model, the transition probability decomposes into the product of independent label transition and alignment transition probabilities p(aj, zj|aj−1, zj−1, y, s; θ) =p(aj|aj−1, y, s; θ) × p(zj|zj−1, y, s; θ), and p(aj|aj−1, y, s; θ) = p(aj|y, s; θ) simply encodes the probability that segments align to a (nonNULL) instruction step given y. This allows the model to learn, for example, that reviews that contain refinements refer to the instructions m</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics - Volume 2, COLING ’96, pages 836–841, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhu Zhang</author>
<author>Balaji Varadarajan</author>
</authors>
<title>Utility scoring of product reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>51--57</pages>
<contexts>
<context position="7571" citStr="Zhang and Varadarajan, 2006" startWordPosition="1179" endWordPosition="1182"> approaches, in the context of extractive summarization, Barzilay and Lee (2004) model content structure (i.e., the order in which topics appear) in documents. We also model document structure, but we do so to help identify refinement segments. We share with previous work on predicting review quality or helpfulness an interest in identifying “informative” text. Early work tried to exploit the intuition that a helpful review is one that comments on product details. However, incorporating product-aspect-mention count (Kim et al., 2006) or similarity between the review and product specification (Zhang and Varadarajan, 2006) as features did not seem to improve the performance when the task was predicting the percentage of helpfulness votes. Instead of using the helpfulness votes, Liu et al. (2007) manually annotated reviews with quality judgements, where a best review was defined as one that contains complete and detailed comments. Our notion of informativeness differs from previous work. We do not seek reviews that contain detailed evaluative information; instead, we seek reviews that contain detailed actionable information. Furthermore, we are not expecting any single review to be comprehensive; rather, we seek</context>
</contexts>
<marker>Zhang, Varadarajan, 2006</marker>
<rawString>Zhu Zhang and Balaji Varadarajan. Utility scoring of product reviews. In Proceedings of the ACM SIGIR Conference on Information and Knowledge Management (CIKM), pages 51–57, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>