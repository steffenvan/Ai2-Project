<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.846589">
Reading to Learn: Constructing Features from Semantic Abstracts
</title>
<author confidence="0.998569">
Jacob Eisenstein∗ James Clarke† Dan Goldwasser† Dan Roth∗†
</author>
<affiliation confidence="0.997048">
∗Beckman Institute for Advanced Science and Technology, †Department of Computer Science
University of Illinois
</affiliation>
<address confidence="0.768687">
Urbana, IL 61801
</address>
<email confidence="0.995644">
{jacobe,clarkeje,goldwas1,danr}@illinois.edu
</email>
<sectionHeader confidence="0.982815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971421052632">
Machine learning offers a range of tools
for training systems from data, but these
methods are only as good as the underly-
ing representation. This paper proposes to
acquire representations for machine learn-
ing by reading text written to accommo-
date human learning. We propose a novel
form of semantic analysis called read-
ing to learn, where the goal is to obtain
a high-level semantic abstract of multi-
ple documents in a representation that fa-
cilitates learning. We obtain this abstract
through a generative model that requires
no labeled data, instead leveraging repe-
tition across multiple documents. The se-
mantic abstract is converted into a trans-
formed feature space for learning, result-
ing in improved generalization on a rela-
tional learning task.
</bodyText>
<sectionHeader confidence="0.995169" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999742174603175">
Machine learning offers a range of powerful tools
for training systems to act in complex environ-
ments, but these methods depend on a well-chosen
representation for features. For learning to suc-
ceed the representation often must be crafted with
knowledge about the application domain. This
poses a bottleneck, requiring expertise in both ma-
chine learning and the application domain. How-
ever, domain experts often express their knowl-
edge through text; one direct expression is through
text designed to aid human learning. In this paper
we exploit text written by domain experts in or-
der to build a more expressive representation for
learning. We term this approach reading to learn.
The following scenario demonstrates the moti-
vation for reading to learn. Imagine an agent given
a task within its world/environment. The agent has
no prior knowledge of the task but can perceive the
world through low-level sensors. Learning directly
from the sensors may be difficult, as interesting
tasks typically require a complex combination of
sensors. Our goal is to acquire domain knowledge
through the semantic analysis of text, so as to pro-
duce higher-level relations through combinations
of sensors.
As a concrete example consider the problem of
learning how to make legal moves in Freecell soli-
taire. Relevant sensors may indicate if an object
is a card or a freecell, whether a card is a certain
value, and whether two values are in sequence.
Although it is possible to express the rules with
a combination of sensors, learning this combina-
tion is difficult. Text can facilitate learning by pro-
viding relations at the appropriate level of gen-
eralization. For example, the sentence: “You can
place a card on an empty freecell,” suggests not
only which sensors are useful together but also
how these sensors should be linked. Assuming the
sensors are represented as predicates, one possi-
ble relation this sentence suggests is: r(x, y) =
card(x) ∧ freecell(y) ∧ empty(y). Armed
with this new relation the agent’s learning task
may be simpler. Throughout the paper we refer to
low-level sensory input as sensor or predicate, and
to a higher level concept as a logical formula or re-
lation.
Our approach to semantic analysis does not re-
quire a complete semantic representation of the
text. We merely wish to acquire a semantic ab-
stract of a document or document collection, and
use the discovered relations to facilitate data-
driven learning. This will allow us to directly eval-
uate the contribution of the extracted relations for
learning.
We develop an approach to recover semantic ab-
stracts that uses minimal supervision: we assume
only a very small set of lexical glosses, which map
from words to sensors. This marks a substantial
departure from previous work on semantic pars-
ing, which requires either annotations of the mean-
ings of each individual sentence (Zettlemoyer and
Collins, 2005; Liang et al., 2009), or alignments
of sentences to grounded representations of the
</bodyText>
<note confidence="0.850056666666667">
958
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958–967,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999592333333333">
world (Chen and Mooney, 2008). For the purpose
of learning, this approach may be inapplicable, as
such text is often written at a high level of abstrac-
tion that permits no grounded representation.
There are two properties of our setting that
make unsupervised learning feasible. First, it is
not necessary to extract a semantic representation
of each individual sentence, but rather a summary
of the semantics of the document collection. Er-
rors in the semantic abstract are not fatal, as long
it guides the learning component towards a more
useful representation. Second, we can exploit rep-
etition across documents, which should generally
express the same underlying meaning. Logical for-
mulae that are well-supported by multiple docu-
ments are especially likely to be useful.
The rest of this paper describes our approach
for recovering semantic abstracts and outlines how
we apply and evaluate this approach on the Free-
cell domain. The paper contributes the following
key ideas: (1) Interpreting abstract “instructional”
text, written at a level that does not correspond
to concrete sensory inputs in the world, so that
no grounded representation is possible, (2) read-
ing to learn, a new setting in which extracted se-
mantic representations are evaluated by whether
they facilitate learning; (3) abstractive semantic
summarization, aimed at capturing broad seman-
tic properties of a multi-document dataset, rather
than a semantic parse of individual sentences; (4) a
novel, minimally-supervised generative model for
semantic analysis which leverages both lexical and
syntactic properties of text.
</bodyText>
<sectionHeader confidence="0.953498" genericHeader="method">
2 Approach Overview
</sectionHeader>
<bodyText confidence="0.997240794520548">
We describe our approach to text analysis as mul-
tidocument semantic abstraction, with the goal of
discovering a compact set of logical formulae to
explain the text in a document collection. To this
end, we develop a novel generative model in which
natural language sentences (e.g., “You can always
place cards in empty freecells”) are stochastically
generated from logical formulae (e.g., card(x) n
freecell(y) n empty(y)). We formally define
a generative process that reflects our intuitions
about the relationship between formulae and sen-
tences (Section 3), and perform sampling-based
inference to recover the formulae most likely to
have generated the observed data (Section 4). The
top N such formulae can then be added as addi-
tional predicates for relational learning.
Our semantic representation consists of con-
junctions of literals, each of which includes a sin-
gle predicate (e.g., empty) and one or more vari-
ables (e.g., x). Predicates describe atomic seman-
tic concepts, while variables construct networks
of relationships between them. While the impor-
tance of the predicates is obvious, the variable
assignments also exert a crucial influence on the
semantics of the conjunction: modifying a sin-
gle variable in the formula above from empty(y)
to empty(x) yields a formula that is trivially
false for all groundings (since cards can never be
empty).
Thus, our generative model must account for the
influence of both predicates and variables on the
sentences in the documents. A natural choice is to
use the predicates to influence the lexical items,
while letting the variables determine the syntac-
tic structure. For example, the formula card(x)n
freecell(y) n empty(y) contains three pred-
icates and two variables. The predicates influence
the lexical items in a direct way: we expect that
sentences generated from this formula will include
a member of the gloss set for each predicate –
the sentence “Put the cards on the empty free-
cells” should be more likely than “Columns are
constructed by playing cards in alternating colors.”
The impact of the variables on the generative
process is more subtle. The sharing of the variable
y suggests a relationship between the predicates
freecell and empty. This should be realized
in the syntactic structure of the sentence. Model-
ing syntax using a dependency tree, we expect that
the glosses for predicates that share terms will ap-
pear in compact sub-trees, while predicates that do
not share terms should be more distant. One pos-
sible surface realization of this logical formula is
the sentence, “Put the card on the empty freecell,”
whose dependency parse is shown in the left tree
of Figure 1. The glosses empty and freecell are im-
mediately adjacent, while card is more remote.
We develop two metrics that quantify the com-
pactness of a set of variable assignments with
respect to a dependency tree: excess terms, and
shared terms. The number of excess terms in a
subtree is the number of unique terms assigned
to words in the subtree, minus the maximum arity
of any predicate in the subtree. Shared terms arise
whenever a node has multiple subtrees which each
contain the same variable. We will use the alterna-
tive alignments in Figure 1 to provide a more de-
tailed explanation. In each tree, the variables are
written in the nodes belonging to the associated
lexical items; variables are written over arrows to
indicate membership in some node in the subtree.
Excess Terms Alignment A of Fig-
ure 1, corresponding to the formula
</bodyText>
<figure confidence="0.834353222222222">
959
XXX XXY XYY XYZ
card
the
the
Put
freecell
empty
on
</figure>
<equation confidence="0.91003632">
X
X XX
XX
X
X
X
X
X XY
XY
X
Y
Y
X
X YY
YY
Y
Y
Y
X
X YZ
YZ
Z
Y
Z
Dependency tree Alignment A Alignment B Alignment C Alignment D
</equation>
<figureCaption confidence="0.754643">
Figure 1: A dependency parse and four different variable assignments. Each literal is aligned to a word (a
node in the graph), and the associated variables are written in the box. Variables belonging to descendant
nodes are written over the arrows.
</figureCaption>
<bodyText confidence="0.999865592592592">
card(x) n freecell(x) n empty(x), has zero
excess terms in every subtree; there is a total of one
variable, and all the predicates are unary. In Align-
ment B, card(x) n freecell(x) n empty(y),
there are excess terms at the root, and in the top
two subtrees on the right-hand side. Alignment C
contains an excess term at only the root node.
Even though it contains the same number of
unique variables as Alignment B, it is not penal-
ized as harshly because the alignment of variables
better corresponds to the syntactic structure.
Alignment D contains the greatest number of
excess terms: two at the root of the tree, and one
in each of the top two subtrees on the right side.
Shared Terms According to the excess term
metric, the best choice is simply to introduce as
few variables as possible. For this reason, we also
penalize shared terms which occur when a node
has subtree children that share a variable. In Fig-
ure 1, Alignments A and B each contain a shared
term at the top node; Alignments C and D contain
no shared terms.
Overall, we note that Alignment B is penalized
on both metrics, as it contains both excess terms
and shared terms; the syntactic structure of the
sentence makes such a variable assignment rela-
tively improbable.
</bodyText>
<note confidence="0.691596">
Put the card on the empty freecell
</note>
<figureCaption confidence="0.837710666666667">
Figure 2: A graphical depiction of the generative
process by which sentences are produced from for-
mulae
</figureCaption>
<sectionHeader confidence="0.997632" genericHeader="method">
3 Generative Model
</sectionHeader>
<bodyText confidence="0.999968545454545">
These intuitions are formalized in a generative
account of how sentences are stochastically pro-
duced from a set of logical formulae. This gener-
ative story guides an inference procedure for re-
covering logical formulae that are likely to have
generated any observed set of texts, which is de-
scribed in Section 4.
The outline of the generative process is depicted
in Figure 2. For each sentence, we begin in step (a)
by drawing a formula f from a Dirichlet pro-
cess (Ferguson, 1973). The Dirichlet process de-
</bodyText>
<equation confidence="0.968678285714286">
card(x) &amp; *eecell(y) &amp; ermpb(y)
c(�)
c(�)
e(y)
e(y) f(y)
f(y)
960
</equation>
<bodyText confidence="0.99990625">
fines a non-parametric mixture model, and has the
effect of adaptively selecting the appropriate num-
ber of formulae to explain the observed sentences
in the corpus.1 We then draw the sentence length
from some distribution over positive integers; as
the sentence length is always observed, we need
not define the distribution (step (b)). In step (c), a
dependency tree is drawn from a uniform distribu-
tion over spanning trees with a number of nodes
equal to the length of the sentence. In step (d) we
draw an alignment of the literals in f to nodes in
the dependency tree, written at(f). The distribu-
tion over alignments is described in Section 3.1.
Finally, the aligned literals are used to generate the
words at each slot in the dependency tree. A more
formal definition of this process is as follows:
</bodyText>
<listItem confidence="0.99473784">
• Draw A, the expected number of literals per
formula, from a Gamma distribution g(u, v).
• Draw an infinite set of formulae f. For each
formula fi,
– Draw the formula length #|fi |from a
Poisson distribution, ni — Poisson(A).
– Draw ni literals from a uniform distri-
bution.
• Draw 7r, an infinite multinomial distribution
over formulae: 7r — GEM(7r0), where GEM
refers to the stick-breaking prior (Sethura-
man, 1994) and 7r0 = 1 is the concentra-
tion parameter. By attaching the multinomial
7r to the infinite set of formulae f, we cre-
ate a Dirichlet process. This is conventionally
written DP(7r0, G0), where the base distribu-
tion G0 encodes only the distribution over the
number of literals, Poisson(A).
• For each of D documents, draw the number
of sentences T — Poisson. For each of the T
sentences in the document,
– Draw a formula f — DP(7r0, G0) from
the Dirichlet Process described above.
– Draw a sentence length #|s |— Poisson.
– Draw a dependency graph t (a spanning
</listItem>
<bodyText confidence="0.936676235294118">
tree of size #|s|) from a uniform distri-
bution.
– Draw an alignment at(f), an injective
mapping from literals in f to nodes in
the dependency structure t. The distribu-
tion over alignments is described in Sec-
tion 3.1.
1There are many recent applications of Dirichlet pro-
cesses in natural language processing, e.g. Goldwater et al.
(2006).
– Draw the sentence s from the formula
f and the alignment a(f). For each
word token wi E s is drawn from
p(wi|at(f, i)), where at(f, i) indicates
the (possibly empty) literal assigned
to slot i in the alignment at(f) (Sec-
tion 3.2).
</bodyText>
<subsectionHeader confidence="0.999906">
3.1 Distribution over Alignments
</subsectionHeader>
<bodyText confidence="0.9995915">
The distribution over alignments reflects our intu-
ition that when literals share variables, they will
be aligned to word slots that are nearby in the de-
pendency structure; literals that do not share vari-
ables should be more distant. This is formalized by
applying the concepts of excess terms and shared
terms defined in Section 2. After computing the
number of excess and shared terms in each sub-
tree ti, we can compute a local score (LS) for that
subtree:
</bodyText>
<equation confidence="0.9963915">
LS(at(f);ti) = α · NShared(at(f), ti)
+ β · NExcess(at(f), ti) · height(ti).
</equation>
<bodyText confidence="0.920416666666667">
This scoring function can be applied recursively to
each subtree in t; the overall score of the tree is the
recursive sum,
</bodyText>
<equation confidence="0.980897">
score(at(f);t) = LS(at(f);t)+
(1)
</equation>
<bodyText confidence="0.99998775">
where ti indicates the ith subtree of t. We hypoth-
esize a generative process that produces all possi-
ble alignments, scores them using score(at(f); t),
and selects an alignment with probability,
</bodyText>
<equation confidence="0.803478333333333">
p(at(f)) a exp{—score(at(f); t)}. (2)
In our experiments, we define the parameters α =
1,β = 1.
</equation>
<subsectionHeader confidence="0.999926">
3.2 Generation of Lexical Items
</subsectionHeader>
<bodyText confidence="0.999900538461538">
Once the logical formula is aligned to the parse
structure, the generation of the lexical items in
the sentence is straightforward. For word slots to
which no literals are aligned, the lexical item is
drawn from a language model B, estimated from
the entire document collection. For slots to which
at least one literal is aligned, we construct a lan-
guage model φ in which the probability mass is
divided equally among all glosses of aligned pred-
icates. The language model B is used as a backoff,
so that there is a strong bias in favor of generating
glosses, but some probability mass is reserved for
the other lexical items.
</bodyText>
<equation confidence="0.60077975">
n
score(at(f); ti),
i
961
</equation>
<bodyText confidence="0.940094611111111">
4 Inference To compute the probability of a parsed sentence
given a formula, we sum over alignments,
This section describes a sampling-based inference
procedure for obtaining a set of formulae f that
explain the observed text s and dependency struc-
tures t. We perform Gibbs sampling over the
formulae assigned to each sentence. Using the
Chinese Restaurant Process interpretation of the
Dirichlet Process (Aldous, 1985), we marginalize
π, the infinite multinomial over all possible for-
mulae: at each sampling step we select either an
existing formula, or stochastically generate a new
formula. After each full round of Gibbs sampling,
a set of Metropolis-Hastings moves are applied to
explore modifications of the formulae. This proce-
dure converges on a stationary Markov chain cen-
tered on a set of formulae that cohere well with the
lexical and syntactic properties of the text.
</bodyText>
<subsectionHeader confidence="0.997558">
4.1 Assigning Sentences to Formulae
</subsectionHeader>
<bodyText confidence="0.999575">
For each sentence si and dependency tree ti, a hid-
den variable yi indicates the index of the formula
that generates the text. We can resample yi using
Gibbs sampling. In the non-parametric setting, yi
ranges over all non-negative integers; the Chinese
Restaurant Process formulation marginalizes the
infinite-dimensional parameter π, yielding a prior
based on the counts for each “active” formula (to
which at least one other sentence is assigned), and
a pseudo-count representing all non-active formu-
lae. Given K formulae, the prior on selecting for-
mula j is:
</bodyText>
<equation confidence="0.93000375">
(
n−i(j) j &lt; K
p(yi = j|y−i, π0) a (3)
π0 j = K,
</equation>
<bodyText confidence="0.936282571428571">
where y−i refers to the assignments of all y other
than yi and n−i refers to the counts over these as-
signments. Each j &lt; K identifies an existing for-
mula in f, to which at least one other sentence is
assigned. When j = K, this means a new formula
f* must be generated.
To perform Gibbs sampling, we draw from the
posterior distribution over yi,
p(yi|si, tif, f*, y−i, π0) a
p(yi|y−i, π0)p(si, ti|yi, f, f*),
where the first term is the prior defined in Equa-
tion 3 and the latter term is the likelihood of gener-
ating the parsed sentence (si, ti) from the formula
indexed by yi.
</bodyText>
<equation confidence="0.9993185">
p(s,t|f) = X p(s, t, at(f)|f)
at(f)
X= p(s|at(f))p(t, at(f)|f)
at(f)
X= p(s|at(f))p(at(f)|t, f)p(t|f),
at(f)
</equation>
<bodyText confidence="0.999742692307692">
applying the chain rule and independence assump-
tions from the generative model. The result is a
product of three terms: the likelihood of the lexi-
cal items given the aligned predicates (defined in
Section 3.2; the likelihood of the alignment given
the dependency tree and formula (defined in equa-
tion 2), and the probability of the dependency tree
given the formula, which is uniform.
Equation 4 takes a sum across alignments, but
most of the probability mass of p(s|at(f)) will
be concentrated on alignments in which predicates
cover words that gloss them. Thus, we can apply
an approximation,
</bodyText>
<equation confidence="0.991896333333333">
N
p(s, t|f) � X p(s|at(f))p(at(f)|t, f)p(t|f),
at(f)
</equation>
<bodyText confidence="0.999886444444444">
in which we draw N samples in which predicates
are aligned to their glosses whenever possible.
Similarly, Equation 2 quantifies the likelihood
of an alignment only to a constant of proportional-
ity; again, a sum over possible alignments is nec-
essary. We do not expect the prior on alignments
to be strongly peaked like the sentence likelihood,
so we approximate the normalization term by sam-
pling M alignments at random and extrapolating:
</bodyText>
<equation confidence="0.945837">
p(at(f)|t, f) a q(at(f);t)
q(at(f); t)
Pat(f) q (at (f); t)
#|at(f) |q(at(f); t)
� PM ,
M a� t(f) q(a� t(f); t)
</equation>
<bodyText confidence="0.996351">
where q(at(f); t) = exp{−score(at(f); t)}, de-
fined in Equation 2. In our experiments, we set N
to at most 10, and M = 20. Drawing larger num-
bers of samples had no discernible effect on sys-
tem output.
</bodyText>
<page confidence="0.411492">
962
</page>
<subsectionHeader confidence="0.43928">
4.1.1 Generating new formulae
</subsectionHeader>
<bodyText confidence="0.998314083333333">
Chinese Restaurant Process sampling requires the
generation of new candidate formulae at each re-
sampling stage. To generate a new formula, we
first sample the number of literals. As described
in the generative story (Section 3), the number
of literals is drawn from a Poisson distribution
with parameter θ. We treat θ as unknown and
marginalize, using the Gamma hyperprior G(u, v).
Due to Poisson-Gamma conjugacy, this marginal-
ization can be performed analytically, yielding
a Negative-Binomial distribution with parameters
hu+Ei #|fi|, (1+K +v)−1i, where Ei #|fi |is
the sum of the number of literals in each formula,
and K is the number of formulae which generate
at least one sentence. In this sense, the hyperpriors
u and v act as pseudo counts. We set u = 3, v = 1,
reflecting a weak prior expectation of three literals
per predicate.
After drawing the size of the formula, the predi-
cates are selected from a uniform random distribu-
tion. Finally, the terms are assigned: at each slot,
we reuse a previous term with probability 0.5, un-
less none is available; otherwise a new term is gen-
erated.
</bodyText>
<subsectionHeader confidence="0.988064">
4.2 Proposing changes to formulae
</subsectionHeader>
<bodyText confidence="0.999415666666667">
The assignment resampling procedure has the
ability to generate new formulae, thus exploring
the space of relational features. However, to ex-
plore this space more rapidly, we introduce four
Metropolis-Hastings moves that modify existing
formulae (Gilks, 1995): adding a literal, deleting
a literal, substituting a literal, and rearranging the
terms of the formula. For each proposed move, we
recompute the joint likelihood of the formula and
all aligned sentences. The move is stochastically
accepted based on the ratio of the joint likelihoods
of the new and old configurations, multiplied by a
Hastings correction.
The joint likelihood with respect to formula f
is computed as p(s, t, f ) = p(f ) Hi p(si, ti|f).
The prior on f considers only the number of liter-
als, using a Negative-Binomial distribution as de-
scribed in section 4.1.1. The likelihood p(si, ti|f)
is given in equation 4. The Hastings correction is
˜p(f0 → f)/˜p(f → f0), with˜p(f → f0) indicat-
ing the probability of proposing a move from f
to f0,and ˜p(f0 → f) indicating the probability of
proposing the reverse move. The Hastings correc-
tions depend on the arity of the predicates being
added and removed; the derivation is straightfor-
ward but tedious. We plan to release a technical
report with complete details.
</bodyText>
<subsectionHeader confidence="0.999075">
4.3 Summary of inference
</subsectionHeader>
<bodyText confidence="0.999987666666667">
The final inference procedure iterates between
Gibbs sampling of assignments of formulae to
sentences, and manipulating the formulae through
Metropolis-Hastings moves. A full iteration com-
prises proposing a move to each formula, and then
using Gibbs sampling to reconsider all assign-
ments. If a formula no longer has any sentences
assigned to it, then it is dropped from the active
set, and can no longer be selected in Gibbs sam-
pling – this is standard in the Chinese Restaurant
Process.
Five separate Markov chains are maintained in
parallel. To allow the sampling procedure to con-
verge to a stationary distribution, each chain be-
gins with 100 iterations of “burn-in” sampling,
without storing the output. At this point, we per-
form another 100 iterations, storing the state at the
end of each iteration.2 All formulae are ranked ac-
cording to the cumulative number of sentences to
which they are assigned (across all five Markov
chains), aggregating the counts for multiple in-
stances of identical formulae. This yields a ranked
list of formulae which will be used in our frame-
work as features for relational learning.
</bodyText>
<sectionHeader confidence="0.995635" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999972">
Our experimental setup is designed to evaluate the
quality of the semantic abstraction performed by
our model. The logical formulae obtained by our
system are applied as features for relational learn-
ing of the rules of the game of Freecell solitaire.
We investigate whether these features enable bet-
ter generalization given varying number of train-
ing examples of Freecell game states. We also
quantify the specific role of syntax, lexical choice,
and feature expressivity in learning performance.
This section describes the details of this evalua-
tion.
</bodyText>
<subsectionHeader confidence="0.978946">
5.1 Relational Learning
</subsectionHeader>
<bodyText confidence="0.999053846153846">
We perform relational learning using Inductive
Logic Programming (ILP), which constructs gen-
eralized rules by assembling smaller logical for-
mulae to explain observed propositional exam-
ples (Muggleton, 1995). The lowest level formu-
lae consist of basic sensors that describe the en-
vironment. ILP’s expressivity enables it to build
complex conjunctions of these building blocks,
but at the cost of tractability. Our evaluation asks
whether the logical formulae abstracted from text
2Sampling for more iterations was not found to affect per-
formance on development data, and the model likelihood ap-
peared stationary after 100 iterations.
</bodyText>
<equation confidence="0.894152066666667">
963
Predicate Glosses
card(x) card
tableau(x) column, tableau
freecell(x) freecell, cell
homecell(x) foundation, cell, homecell
value(x,y) ace, king, rank, 8, 3, 7, lowest,
highest
successor(x,y) higher, sequence, sequential
color(x,y) black, red, color
suit(x,y) suit, club, diamond, spade,
heart
on(x,y) onto
top(x,y) bottom, available, top
empty(x) empty
</equation>
<tableCaption confidence="0.83731">
Table 1: Predicates in the Freecell world model,
</tableCaption>
<bodyText confidence="0.988541090909091">
with natural language glosses obtained from the
development set text.
can transform the representation to facilitate learn-
ing. We compare against both the sensor-level rep-
resentation as well as richer representations that do
not benefit from the full power of our model’s se-
mantic analysis.
The ALEPH3 ILP system, which is primarily
based on PROGOL (Muggleton, 1995), was used
to induce the rules of game. The search parame-
ters remained constant for all experiments.
</bodyText>
<subsectionHeader confidence="0.981835">
5.2 Resources
</subsectionHeader>
<bodyText confidence="0.978354366197183">
There are four types of resources required to work
in the reading-to-learn setting: a world model, in-
structional text, a small set of glosses that map
from text to elements of the world model, and la-
beled examples of correct and incorrect actions
in the world. In our experiments, we consider
the domain of Freecell solitaire, a popular card
game (Morehead and Mott-Smith, 1983) in which
cards are moved between various types of loca-
tions, depending on their suit and rank. We now
describe the resources for the Freecell domain in
more detail.
World Model Freecell solitaire can be described
formally using first order logic; we consider a
slightly modified version of the representation
from the Planning Domain Definition Language
(PDDL), which is used in automatic game-playing
competitions. Specifically, there are 87 constants:
52 cards, 16 locations, 13 values, four suits, and
two colors. These constants are combined with a
fixed set of 11 predicates, listed in Table 1.
Instructional Text Our approach relies on text
that describes how to operate in the Freecell soli-
taire domain. A total of five instruction sets were
3Freely available from http://www.comlab.ox.
ac.uk/activities/machinelearning/Aleph/
obtained from the Internet. Due to the popular-
ity of the Microsoft implementation of Freecell,
instructions often contain information specific to
playing Freecell on a computer. We manually re-
moved sentences which did not focus on the card
aspects of Freecell (e.g., how to set up the board
and information regarding where to click to move
cards). In order to use our semantic abstraction
model, the instructions were part-of-speech tagged
with the Stanford POS Tagger (Toutanova and
Manning, 2000) and dependency parses were ob-
tained using Malt (Nivre, 2006).
Glosses Our reading to learn setting requires a
small set of glosses, which are surface forms com-
monly used to represent predicates from the world
model. We envision an application scenario in
which a designer manually specifies a few glosses
for each predicate. However, for the purposes of
evaluation, it would be unprincipled for the exper-
imenters to handcraft the ideal set of glosses. In-
stead, we gathered a development set of text and
annotated the lexical mentions of the world model
predicates in text. This annotation is used to ob-
tain glosses to apply to the evaluation text. This
approximates a scenario in which the designer has
a reasonable idea of how the domain will be de-
scribed in text, but no prior knowledge of the spe-
cific details of the text instructions. Our exper-
iments used glosses that occurred two or more
times in the instructions: this yields a total of 32
glosses for 11 predicates, as shown in Table 1.
Evaluation game data Ultimately, the seman-
tic abstraction obtained from the text is applied
to learning on labeled examples of correct and
incorrect actions in the world model. For evalu-
ation, we automatically generated a set of move
scenarios: game states with one positive example
(a legal move) and one negative example (an ille-
gal move). To avoid bias in the data we generate
an equal number of move scenarios from each of
three types: moves to the freecells, homecells, and
tableaux. For our experiments we vary the number
of move scenarios in the training set; the develop-
ment and test sets consist of 900 and 1500 move
scenarios respectively.
</bodyText>
<subsectionHeader confidence="0.998665">
5.3 Evaluation Settings
</subsectionHeader>
<bodyText confidence="0.9999945">
We compare four different feature sets, which
will be provided to the ALEPH ILP learner. All
feature sets include the sensor-level predicates
shown in Table 1. The FULL-MODEL feature
set also includes the top logical formulae ob-
tained in our model’s semantic abstract (see Sec-
</bodyText>
<page confidence="0.771595">
964
</page>
<bodyText confidence="0.999973153846154">
tion 4.3). The NO-SYNTAX feature set is obtained
from a variant of our model in which the in-
fluence of syntax is removed by setting parame-
ters α, Q = 0. The SENSORS-ONLY feature set
uses only the sensor-level predicates. Finally, the
RELATIONAL-RANDOM feature set is constructed
by replacing each feature in the FULL-MODEL set
with a randomly generated relational feature of
identical expressivity (each predicate is replaced
by a randomly chosen alternative with identical
arity; terms are also assigned randomly). This en-
sures that any performance gains obtained by our
model were not due merely to the greater expres-
sivity of its relational features. The number of fea-
tures included in each scenario is tuned on a de-
velopment set of test examples.
The performance metric assesses the ability
of the ILP learner to classify proposed Freecell
moves as legal or illegal. As the evaluation set
contains an equal number of positive and negative
examples, accuracy is the appropriate metric. The
training scenarios are randomly generated; we re-
peat each run 50 times and average our results. For
the RELATIONAL-RANDOM feature set – in which
predicates and terms are chosen randomly – we
also regenerate the formulae per run.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999590576923077">
Table 2 shows a comparison of the results
using the setup described above. Our FULL-
MODEL achieves the best performance at ev-
ery training set size, consistently outperforming
the SENSORS-ONLY representation by an abso-
lute difference of three to four percent. This
demonstrates the semantic abstract obtained by
our model does indeed facilitate machine learning
in this domain.
RELATIONAL-RANDOM provides a baseline of
relational features with equal expressivity to those
chosen by our model, but with the predicates and
terms selected randomly. We consistently outper-
form this baseline, demonstrate that the improve-
ment obtained over the sensors only representation
is not due merely to the added expressivity of our
features.
The third row compares against NO-SYNTAX,
a crippled version of our model that incorpo-
rates lexical features but not the syntactic struc-
ture. The results are stronger than the SENSORS-
ONLY and RELATIONAL-RANDOM baselines, but
still weaker than our full system. This demon-
strates the syntactic features incorporated by our
model result in better semantic representations of
the underlying text.
</bodyText>
<table confidence="0.9883615">
Features Number of training 60 scenarios
15 30 120
SENSORS-ONLY 79.12 88.07 92.77 93.73
RELATIONAL-RANDOM 82.72 89.14 93.08 94.17
NO-SYNTAX 80.98 89.79 94.11 97.04
FULL-MODEL 82.89 91.00 95.23 97.45
</table>
<tableCaption confidence="0.962579333333333">
Table 2: Results as number of training examples
varied. Each value represents the accuracy of the
induced rules obtained with the given feature set.
</tableCaption>
<figure confidence="0.743029333333333">
card(x1) n tableau(x2)
card(x1) n freecell(x2)
homecell(x1) n value(x2,x3)
empty(x1) n freecell(x1)
card(x1) n top(x1,x2)
card(x1) n homecell(x2)
freecell(x1) n homecell(x2)
card(x1) n tableau(x1)
card(x1) n top(x2,x1)
homecell(x1)
card(x1) n homecell(x1)
color(x1,x2) n value(x3,x4)
suit(x1,x2) n value(x3,x4)
value(x1,x2) n value(x3,x4)
homecell(x1) n successor(x2,x3)
</figure>
<figureCaption confidence="0.9325315">
Figure 3: The top 15 features recovered by the se-
mantic abstraction of our full model.
</figureCaption>
<bodyText confidence="0.998484894736842">
Figure 3 shows the top 15 formulae recovered
by the full model running on the evaluation text.
Features such as empty(x1) ∧ freecell(x1)
are useful because they reuse variables to ensure
that objects have key properties – in this case, en-
suring that a freecell is empty. Other features, such
as homecell(x1) ∧ value(x2, x3), help to fo-
cus the search on useful conjunctions of predicates
(in Freecell, the legality of playing a card on a
homecell depends on the value of the card). Note
that three of these 15 formulae are trivially use-
less, in that they are always false: e.g., card(x1)
∧ tableau(x1). This illustrates the importance
of term assignment in obtaining useful features
for learning. In the NO-SYNTAX system, which
ignores the relationship between term assignment
and syntactic structure, eight of the top 15 formu-
lae were trivially useless due to term incompatibil-
ity.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999912333333333">
This paper draws on recent literature on extract-
ing logical forms from surface text (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005; Downey
et al., 2005; Liang et al., 2009), interpreting lan-
guage in the context of a domain (Chen and
Mooney, 2008), and using an actionable domain
to guide text interpretation (Branavan et al., 2009).
We differentiate our research in several dimen-
sions:
</bodyText>
<page confidence="0.744749">
965
</page>
<bodyText confidence="0.999873189189189">
Language Interpretation Instructional text de-
scribes generalized statements about entities in
the domain and the way they interact, thus the
text does not correspond directly to concrete sen-
sory inputs in the world (i.e., a specific world
state). Our interpretation captures these general-
izations as first-order logic statements that can be
evaluated given a specific state. This contrasts to
previous work which interprets “directions” and
thus assumes a direct correspondence between text
and world state (Branavan et al., 2009; Chen and
Mooney, 2008).
Supervision Our work avoids supervision in the
form of labeled examples, using only a minimal
set of natural language glosses per predicate. Pre-
vious work also considered the supervision signal
obtained by interpreting natural language in the
context of a formal domain. Branavan et al. (2009)
use feedback from a world model as a supervi-
sion signal. Chen and Mooney (2008) use tempo-
ral alignment of text and grounded descriptions of
the world state. In these approaches, concrete do-
main entities are grounded in language interpreta-
tion, and therefore require only a propositional se-
mantic representation. Previous approaches for in-
terpreting generalized natural language statements
are trained from labeled examples (Zettlemoyer
and Collins, 2005; Lu et al., 2008).
Level of analysis We aim for an abstractive
semantic summary across multiple documents,
whereas other approaches attempt to produce log-
ical forms for individual sentences (Zettlemoyer
and Collins, 2005; Ge and Mooney, 2005). We
avoid the requirement that each sentence have a
meaningful interpretation within the domain, al-
lowing us to handle relatively unstructured text.
Evaluation We do not evaluate the representa-
tions obtained by our model; rather we assess
whether these representations improve learning
performance. This is similar to work on Geo-
Query (Wong and Mooney, 2007; Ge and Mooney,
2005), and also to recent work on following step-
by-step directions (Branavan et al., 2009). While
these evaluations are performed on the basis of in-
dividual sentences, actions, or system responses,
we evaluate the holistic semantic analysis obtained
by our system.
Model We treat surface text as generated from a
latent semantic description. Lu et al. (2008) ap-
ply a generative model, but require a complete
derivation from semantics to the lexical represen-
tation, while we favor a more flexible semantic
analysis that can be learned without annotation
and applied to noisy text. More similar is the work
of Liang et al. (2009), which models the gener-
ation of semantically-relevant fields using lexical
and discourse features. Our approach differs by
accounting for syntax, which enables a more ex-
pressive semantic representation that includes un-
grounded variables.
Relational learning The output of our semantic
analysis is applied to learning in a structured rela-
tional space, using ILP. A key difficulty with ILP
is that the increased expressivity dramatically ex-
pands the hypothesis space, and it is widely agreed
that some learning bias is required for ILP to be
tractable (N´edellec et al., 1996; Cumby and Roth,
2003). Our work can be viewed as a new method
for acquiring such bias from text; moreover, our
approach is not specialized for ILP and may be
used to transform the feature space in other forms
of relational learning as well (Roth and Yih, 2001;
Cumby and Roth, 2003; Richardson and Domin-
gos, 2006).
</bodyText>
<sectionHeader confidence="0.994294" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.994268833333333">
This paper demonstrates a new setting for seman-
tic analysis, which we term reading to learn. We
handle text which describes the world in gen-
eral terms rather than refereing to concrete enti-
ties in the domain. We obtain a semantic abstract
of multiple documents, using a novel, minimally-
supervised generative model that accounts for both
syntax and lexical choice. The semantic abstract
is represented as a set of predicate logic formu-
lae, which are applied as higher-order features for
learning. We demonstrate that these features im-
prove learning performance, and that both the lex-
ical and syntactic aspects of our model yield sub-
stantial contributions.
In the current setup, we produce an “overgener-
ated” semantic representation comprised of useful
features for learning but also some false positives.
Learning in our system can be seen as the process
of pruning this representation by selecting useful
formulae based on interaction with the training
data. In the future we hope to explore ways to in-
terleave semantic analysis with exploration of the
learning domain, by using the environment as a
supervision signal for linguistic analysis.
Acknowledgments We thank Gerald DeJong,
Julia Hockenmaier, Alex Klementiev and the
anonymous reviewers for their helpful feedback.
This work is supported by DARPA funding under
the Bootstrap Learning Program and the Beckman
Institute Postdoctoral Fellowship.
</bodyText>
<page confidence="0.860133">
966
</page>
<sectionHeader confidence="0.99477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999198569999999">
Aldous, David J. 1985. Exchangeability and re-
lated topics. Lecture Notes in Math 1117:1–198.
Branavan, S. R. K., Harr Chen, Luke Zettle-
moyer, and Regina Barzilay. 2009. Reinforce-
ment learning for mapping instructions to ac-
tions. In Proceedings of the Joint Conference
of the Association for Computational Linguis-
tics and International Joint Conference on Nat-
ural Language Processing Processing (ACL-
IJCNLP 2009). Singapore.
Chen, David L. and Raymond J. Mooney. 2008.
Learning to sportscast: A test of grounded lan-
guage acquisition. In Proceedings of 25th In-
ternational Conference on Machine Learning
(ICML 2008). Helsinki, Finland, pages 128–
135.
Cumby, Chad and Dan Roth. 2003. On kernel
methods for relational learning. In Proceed-
ings of the Twentieth International Conference
(ICML 2003). Washington, DC, pages 107–114.
Downey, Doug, Oren Etzioni, and Stephen Soder-
land. 2005. A probabilistic model of redun-
dancy in information extraction. In Proceedings
of the International Joint Conference on Arti-
ficial Intelligence (IJCAI 2005). pages 1034–
1041.
Ferguson, Thomas S. 1973. A bayesian analysis
of some nonparametric problems. The Annals
of Statistics 1(2):209–230.
Ge, Ruifang and Raymond J. Mooney. 2005. A
statistical semantic parser that integrates syn-
tax and semantics. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL-2005). Ann Arbor,
MI, pages 128–135.
Gilks, Walter R. 1995. Markov Chain Monte
Carlo in Practice. Chapman &amp; Hall/CRC.
Goldwater, Sharon, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings
of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics
(COLING-ACL 2006). Sydney, Australia, pages
673–680.
Liang, Percy, Michael Jordan, and Dan Klein.
2009. Learning semantic correspondences with
less supervision. In Proceedings of the Joint
Conference of the Association for Computa-
tional Linguistics and International Joint Con-
ference on Natural Language Processing Pro-
cessing (ACL-IJCNLP 2009). Singapore.
Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for
parsing natural language to meaning representa-
tions. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2008).
Honolulu, Hawaii, pages 783–792.
Morehead, Albert H. and Geoffrey Mott-Smith.
1983. The Complete Book of Solitaire and Pa-
tience Games. Bantam.
Muggleton, Stephen. 1995. Inverse entailment and
progol. New Generation Computing Journal
13:245–286.
N´edellec, C., C. Rouveirol, H. Ad´e, F. Bergadano,
and B. Tausend. 1996. Declarative bias in ILP.
In L. De Raedt, editor, Advances in Inductive
Logic Programming, IOS Press, pages 82–103.
Nivre, Joakim. 2006. Inductive dependency pars-
ing. Springer.
Richardson, Matthew and Pedro Domingos. 2006.
Markov logic networks. Machine Learning
62:107–136.
Roth, Dan and Wen-tau Yih. 2001. Relational
learning via propositional algorithms: An infor-
mation extraction case study. In Proceedings of
the International Joint Conference on Artificial
Intelligence (IJCAI2001). pages 1257–1263.
Sethuraman, Jayaram. 1994. A constructive def-
inition of dirichlet priors. Statistica Sinica
4(2):639–650.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used
in a maximum entropy part-of-speech tagger.
In Proceedings of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora
(EMNLP/VLC-2000). pages 63–70.
Wong, Yuk Wah and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic
parsing with lambda calculus. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 2007). Prague,
Czech Republic, pages 128–135.
Zettlemoyer, Luke S. and Michael Collins. 2005.
Learning to map sentences to logical form:
Structured classification with probabilistic cat-
egorial grammars. In Proceedings of the 21st
Conference on Uncertainty in Artificial Intelli-
gence (UAI 2005). pages 658–666.
</reference>
<page confidence="0.855052">
967
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931815">
<title confidence="0.994191">Reading to Learn: Constructing Features from Semantic Abstracts</title>
<author confidence="0.999979">James Dan Dan</author>
<affiliation confidence="0.9998085">Institute for Advanced Science and Technology, of Computer University of</affiliation>
<address confidence="0.947791">Urbana, IL</address>
<abstract confidence="0.99943355">Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel of semantic analysis called readto where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no labeled data, instead leveraging repetition across multiple documents. The semantic abstract is converted into a transformed feature space for learning, resulting in improved generalization on a relational learning task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David J Aldous</author>
</authors>
<title>Exchangeability and related topics.</title>
<date>1985</date>
<journal>Lecture Notes in Math</journal>
<pages>1117--1</pages>
<contexts>
<context position="16157" citStr="Aldous, 1985" startWordPosition="2690" endWordPosition="2691">anguage model B is used as a backoff, so that there is a strong bias in favor of generating glosses, but some probability mass is reserved for the other lexical items. n score(at(f); ti), i 961 4 Inference To compute the probability of a parsed sentence given a formula, we sum over alignments, This section describes a sampling-based inference procedure for obtaining a set of formulae f that explain the observed text s and dependency structures t. We perform Gibbs sampling over the formulae assigned to each sentence. Using the Chinese Restaurant Process interpretation of the Dirichlet Process (Aldous, 1985), we marginalize π, the infinite multinomial over all possible formulae: at each sampling step we select either an existing formula, or stochastically generate a new formula. After each full round of Gibbs sampling, a set of Metropolis-Hastings moves are applied to explore modifications of the formulae. This procedure converges on a stationary Markov chain centered on a set of formulae that cohere well with the lexical and syntactic properties of the text. 4.1 Assigning Sentences to Formulae For each sentence si and dependency tree ti, a hidden variable yi indicates the index of the formula th</context>
</contexts>
<marker>Aldous, 1985</marker>
<rawString>Aldous, David J. 1985. Exchangeability and related topics. Lecture Notes in Math 1117:1–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing Processing (ACLIJCNLP</booktitle>
<contexts>
<context position="33103" citStr="Branavan et al., 2009" startWordPosition="5438" endWordPosition="5441">strates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This contrasts to previous work which interprets “directions” and thus assumes a direct correspondence between text and world state (Branavan et al., 2009; Chen and M</context>
<context position="35172" citStr="Branavan et al., 2009" startWordPosition="5754" endWordPosition="5757">s multiple documents, whereas other approaches attempt to produce logical forms for individual sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). We avoid the requirement that each sentence have a meaningful interpretation within the domain, allowing us to handle relatively unstructured text. Evaluation We do not evaluate the representations obtained by our model; rather we assess whether these representations improve learning performance. This is similar to work on GeoQuery (Wong and Mooney, 2007; Ge and Mooney, 2005), and also to recent work on following stepby-step directions (Branavan et al., 2009). While these evaluations are performed on the basis of individual sentences, actions, or system responses, we evaluate the holistic semantic analysis obtained by our system. Model We treat surface text as generated from a latent semantic description. Lu et al. (2008) apply a generative model, but require a complete derivation from semantics to the lexical representation, while we favor a more flexible semantic analysis that can be learned without annotation and applied to noisy text. More similar is the work of Liang et al. (2009), which models the generation of semantically-relevant fields u</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>Branavan, S. R. K., Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing Processing (ACLIJCNLP 2009). Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to sportscast: A test of grounded language acquisition.</title>
<date>2008</date>
<booktitle>In Proceedings of 25th International Conference on Machine Learning (ICML 2008).</booktitle>
<pages>128--135</pages>
<location>Helsinki, Finland,</location>
<contexts>
<context position="4221" citStr="Chen and Mooney, 2008" startWordPosition="667" endWordPosition="670">We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the 958 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958–967, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP world (Chen and Mooney, 2008). For the purpose of learning, this approach may be inapplicable, as such text is often written at a high level of abstraction that permits no grounded representation. There are two properties of our setting that make unsupervised learning feasible. First, it is not necessary to extract a semantic representation of each individual sentence, but rather a summary of the semantics of the document collection. Errors in the semantic abstract are not fatal, as long it guides the learning component towards a more useful representation. Second, we can exploit repetition across documents, which should </context>
<context position="33018" citStr="Chen and Mooney, 2008" startWordPosition="5425" endWordPosition="5428">ially useless, in that they are always false: e.g., card(x1) ∧ tableau(x1). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This contrasts to previous work which interprets “directions” and thus assumes a </context>
</contexts>
<marker>Chen, Mooney, 2008</marker>
<rawString>Chen, David L. and Raymond J. Mooney. 2008. Learning to sportscast: A test of grounded language acquisition. In Proceedings of 25th International Conference on Machine Learning (ICML 2008). Helsinki, Finland, pages 128– 135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>On kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference (ICML 2003).</booktitle>
<pages>107--114</pages>
<location>Washington, DC,</location>
<contexts>
<context position="36306" citStr="Cumby and Roth, 2003" startWordPosition="5935" endWordPosition="5938">k of Liang et al. (2009), which models the generation of semantically-relevant fields using lexical and discourse features. Our approach differs by accounting for syntax, which enables a more expressive semantic representation that includes ungrounded variables. Relational learning The output of our semantic analysis is applied to learning in a structured relational space, using ILP. A key difficulty with ILP is that the increased expressivity dramatically expands the hypothesis space, and it is widely agreed that some learning bias is required for ILP to be tractable (N´edellec et al., 1996; Cumby and Roth, 2003). Our work can be viewed as a new method for acquiring such bias from text; moreover, our approach is not specialized for ILP and may be used to transform the feature space in other forms of relational learning as well (Roth and Yih, 2001; Cumby and Roth, 2003; Richardson and Domingos, 2006). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervi</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Cumby, Chad and Dan Roth. 2003. On kernel methods for relational learning. In Proceedings of the Twentieth International Conference (ICML 2003). Washington, DC, pages 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>A probabilistic model of redundancy in information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>1034--1041</pages>
<contexts>
<context position="32923" citStr="Downey et al., 2005" startWordPosition="5408" endWordPosition="5411"> a homecell depends on the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e.g., card(x1) ∧ tableau(x1). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a sp</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>Downey, Doug, Oren Etzioni, and Stephen Soderland. 2005. A probabilistic model of redundancy in information extraction. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI 2005). pages 1034– 1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="11596" citStr="Ferguson, 1973" startWordPosition="1905" endWordPosition="1906">mpty freecell Figure 2: A graphical depiction of the generative process by which sentences are produced from formulae 3 Generative Model These intuitions are formalized in a generative account of how sentences are stochastically produced from a set of logical formulae. This generative story guides an inference procedure for recovering logical formulae that are likely to have generated any observed set of texts, which is described in Section 4. The outline of the generative process is depicted in Figure 2. For each sentence, we begin in step (a) by drawing a formula f from a Dirichlet process (Ferguson, 1973). The Dirichlet process decard(x) &amp; *eecell(y) &amp; ermpb(y) c(�) c(�) e(y) e(y) f(y) f(y) 960 fines a non-parametric mixture model, and has the effect of adaptively selecting the appropriate number of formulae to explain the observed sentences in the corpus.1 We then draw the sentence length from some distribution over positive integers; as the sentence length is always observed, we need not define the distribution (step (b)). In step (c), a dependency tree is drawn from a uniform distribution over spanning trees with a number of nodes equal to the length of the sentence. In step (d) we draw an </context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Ferguson, Thomas S. 1973. A bayesian analysis of some nonparametric problems. The Annals of Statistics 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005).</booktitle>
<pages>128--135</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="32902" citStr="Ge and Mooney, 2005" startWordPosition="5404" endWordPosition="5407"> of playing a card on a homecell depends on the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e.g., card(x1) ∧ tableau(x1). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be</context>
<context position="34707" citStr="Ge and Mooney, 2005" startWordPosition="5681" endWordPosition="5684">08) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). Level of analysis We aim for an abstractive semantic summary across multiple documents, whereas other approaches attempt to produce logical forms for individual sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). We avoid the requirement that each sentence have a meaningful interpretation within the domain, allowing us to handle relatively unstructured text. Evaluation We do not evaluate the representations obtained by our model; rather we assess whether these representations improve learning performance. This is similar to work on GeoQuery (Wong and Mooney, 2007; Ge and Mooney, 2005), and also to recent work on following stepby-step directions (Branavan et al., 2009). While these evaluations are performed on the basis of individual sentences, actions, or system responses, we evaluate the holistic se</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ge, Ruifang and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005). Ann Arbor, MI, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter R Gilks</author>
</authors>
<title>Markov Chain Monte Carlo in Practice.</title>
<date>1995</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="20773" citStr="Gilks, 1995" startWordPosition="3473" endWordPosition="3474">ng a weak prior expectation of three literals per predicate. After drawing the size of the formula, the predicates are selected from a uniform random distribution. Finally, the terms are assigned: at each slot, we reuse a previous term with probability 0.5, unless none is available; otherwise a new term is generated. 4.2 Proposing changes to formulae The assignment resampling procedure has the ability to generate new formulae, thus exploring the space of relational features. However, to explore this space more rapidly, we introduce four Metropolis-Hastings moves that modify existing formulae (Gilks, 1995): adding a literal, deleting a literal, substituting a literal, and rearranging the terms of the formula. For each proposed move, we recompute the joint likelihood of the formula and all aligned sentences. The move is stochastically accepted based on the ratio of the joint likelihoods of the new and old configurations, multiplied by a Hastings correction. The joint likelihood with respect to formula f is computed as p(s, t, f ) = p(f ) Hi p(si, ti|f). The prior on f considers only the number of literals, using a Negative-Binomial distribution as described in section 4.1.1. The likelihood p(si,</context>
</contexts>
<marker>Gilks, 1995</marker>
<rawString>Gilks, Walter R. 1995. Markov Chain Monte Carlo in Practice. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006).</booktitle>
<pages>673--680</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="13804" citStr="Goldwater et al. (2006)" startWordPosition="2288" endWordPosition="2291">s, Poisson(A). • For each of D documents, draw the number of sentences T — Poisson. For each of the T sentences in the document, – Draw a formula f — DP(7r0, G0) from the Dirichlet Process described above. – Draw a sentence length #|s |— Poisson. – Draw a dependency graph t (a spanning tree of size #|s|) from a uniform distribution. – Draw an alignment at(f), an injective mapping from literals in f to nodes in the dependency structure t. The distribution over alignments is described in Section 3.1. 1There are many recent applications of Dirichlet processes in natural language processing, e.g. Goldwater et al. (2006). – Draw the sentence s from the formula f and the alignment a(f). For each word token wi E s is drawn from p(wi|at(f, i)), where at(f, i) indicates the (possibly empty) literal assigned to slot i in the alignment at(f) (Section 3.2). 3.1 Distribution over Alignments The distribution over alignments reflects our intuition that when literals share variables, they will be aligned to word slots that are nearby in the dependency structure; literals that do not share variables should be more distant. This is formalized by applying the concepts of excess terms and shared terms defined in Section 2. </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006). Sydney, Australia, pages 673–680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning semantic correspondences with less supervision.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing Processing (ACL-IJCNLP</booktitle>
<contexts>
<context position="3972" citStr="Liang et al., 2009" startWordPosition="630" endWordPosition="633">erely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning. This will allow us to directly evaluate the contribution of the extracted relations for learning. We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the 958 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958–967, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP world (Chen and Mooney, 2008). For the purpose of learning, this approach may be inapplicable, as such text is often written at a high level of abstraction that permits no grounded representation. There are two properties of our setting that make unsupervised learning feasible. First, it is not necessary to extract a semantic representation of each individual sentence, but rath</context>
<context position="32944" citStr="Liang et al., 2009" startWordPosition="5412" endWordPosition="5415">n the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e.g., card(x1) ∧ tableau(x1). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This co</context>
<context position="35709" citStr="Liang et al. (2009)" startWordPosition="5842" endWordPosition="5845">nd also to recent work on following stepby-step directions (Branavan et al., 2009). While these evaluations are performed on the basis of individual sentences, actions, or system responses, we evaluate the holistic semantic analysis obtained by our system. Model We treat surface text as generated from a latent semantic description. Lu et al. (2008) apply a generative model, but require a complete derivation from semantics to the lexical representation, while we favor a more flexible semantic analysis that can be learned without annotation and applied to noisy text. More similar is the work of Liang et al. (2009), which models the generation of semantically-relevant fields using lexical and discourse features. Our approach differs by accounting for syntax, which enables a more expressive semantic representation that includes ungrounded variables. Relational learning The output of our semantic analysis is applied to learning in a structured relational space, using ILP. A key difficulty with ILP is that the increased expressivity dramatically expands the hypothesis space, and it is widely agreed that some learning bias is required for ILP to be tractable (N´edellec et al., 1996; Cumby and Roth, 2003). O</context>
</contexts>
<marker>Liang, Jordan, Klein, 2009</marker>
<rawString>Liang, Percy, Michael Jordan, and Dan Klein. 2009. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing Processing (ACL-IJCNLP 2009). Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2008).</booktitle>
<pages>783--792</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="34482" citStr="Lu et al., 2008" startWordPosition="5648" endWordPosition="5651"> work also considered the supervision signal obtained by interpreting natural language in the context of a formal domain. Branavan et al. (2009) use feedback from a world model as a supervision signal. Chen and Mooney (2008) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). Level of analysis We aim for an abstractive semantic summary across multiple documents, whereas other approaches attempt to produce logical forms for individual sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). We avoid the requirement that each sentence have a meaningful interpretation within the domain, allowing us to handle relatively unstructured text. Evaluation We do not evaluate the representations obtained by our model; rather we assess whether these representations improve learning performance. This is similar to work on GeoQuery (Wong and Mooney, 2007; Ge and Mooney, </context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Lu, Wei, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP 2008). Honolulu, Hawaii, pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert H Morehead</author>
<author>Geoffrey Mott-Smith</author>
</authors>
<date>1983</date>
<journal>The Complete Book of Solitaire and Patience Games. Bantam.</journal>
<contexts>
<context position="25430" citStr="Morehead and Mott-Smith, 1983" startWordPosition="4212" endWordPosition="4215">benefit from the full power of our model’s semantic analysis. The ALEPH3 ILP system, which is primarily based on PROGOL (Muggleton, 1995), was used to induce the rules of game. The search parameters remained constant for all experiments. 5.2 Resources There are four types of resources required to work in the reading-to-learn setting: a world model, instructional text, a small set of glosses that map from text to elements of the world model, and labeled examples of correct and incorrect actions in the world. In our experiments, we consider the domain of Freecell solitaire, a popular card game (Morehead and Mott-Smith, 1983) in which cards are moved between various types of locations, depending on their suit and rank. We now describe the resources for the Freecell domain in more detail. World Model Freecell solitaire can be described formally using first order logic; we consider a slightly modified version of the representation from the Planning Domain Definition Language (PDDL), which is used in automatic game-playing competitions. Specifically, there are 87 constants: 52 cards, 16 locations, 13 values, four suits, and two colors. These constants are combined with a fixed set of 11 predicates, listed in Table 1.</context>
</contexts>
<marker>Morehead, Mott-Smith, 1983</marker>
<rawString>Morehead, Albert H. and Geoffrey Mott-Smith. 1983. The Complete Book of Solitaire and Patience Games. Bantam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
</authors>
<title>Inverse entailment and progol.</title>
<date>1995</date>
<journal>New Generation Computing Journal</journal>
<pages>13--245</pages>
<contexts>
<context position="23737" citStr="Muggleton, 1995" startWordPosition="3954" endWordPosition="3955"> as features for relational learning of the rules of the game of Freecell solitaire. We investigate whether these features enable better generalization given varying number of training examples of Freecell game states. We also quantify the specific role of syntax, lexical choice, and feature expressivity in learning performance. This section describes the details of this evaluation. 5.1 Relational Learning We perform relational learning using Inductive Logic Programming (ILP), which constructs generalized rules by assembling smaller logical formulae to explain observed propositional examples (Muggleton, 1995). The lowest level formulae consist of basic sensors that describe the environment. ILP’s expressivity enables it to build complex conjunctions of these building blocks, but at the cost of tractability. Our evaluation asks whether the logical formulae abstracted from text 2Sampling for more iterations was not found to affect performance on development data, and the model likelihood appeared stationary after 100 iterations. 963 Predicate Glosses card(x) card tableau(x) column, tableau freecell(x) freecell, cell homecell(x) foundation, cell, homecell value(x,y) ace, king, rank, 8, 3, 7, lowest, </context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Muggleton, Stephen. 1995. Inverse entailment and progol. New Generation Computing Journal 13:245–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C N´edellec</author>
<author>C Rouveirol</author>
<author>H Ad´e</author>
<author>F Bergadano</author>
<author>B Tausend</author>
</authors>
<title>Declarative bias in ILP.</title>
<date>1996</date>
<booktitle>Advances in Inductive Logic Programming,</booktitle>
<pages>82--103</pages>
<editor>In L. De Raedt, editor,</editor>
<publisher>IOS Press,</publisher>
<marker>N´edellec, Rouveirol, Ad´e, Bergadano, Tausend, 1996</marker>
<rawString>N´edellec, C., C. Rouveirol, H. Ad´e, F. Bergadano, and B. Tausend. 1996. Declarative bias in ILP. In L. De Raedt, editor, Advances in Inductive Logic Programming, IOS Press, pages 82–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive dependency parsing.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="26824" citStr="Nivre, 2006" startWordPosition="4428" endWordPosition="4429">w.comlab.ox. ac.uk/activities/machinelearning/Aleph/ obtained from the Internet. Due to the popularity of the Microsoft implementation of Freecell, instructions often contain information specific to playing Freecell on a computer. We manually removed sentences which did not focus on the card aspects of Freecell (e.g., how to set up the board and information regarding where to click to move cards). In order to use our semantic abstraction model, the instructions were part-of-speech tagged with the Stanford POS Tagger (Toutanova and Manning, 2000) and dependency parses were obtained using Malt (Nivre, 2006). Glosses Our reading to learn setting requires a small set of glosses, which are surface forms commonly used to represent predicates from the world model. We envision an application scenario in which a designer manually specifies a few glosses for each predicate. However, for the purposes of evaluation, it would be unprincipled for the experimenters to handcraft the ideal set of glosses. Instead, we gathered a development set of text and annotated the lexical mentions of the world model predicates in text. This annotation is used to obtain glosses to apply to the evaluation text. This approxi</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Nivre, Joakim. 2006. Inductive dependency parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<journal>Machine Learning</journal>
<pages>62--107</pages>
<contexts>
<context position="36598" citStr="Richardson and Domingos, 2006" startWordPosition="5987" endWordPosition="5991">The output of our semantic analysis is applied to learning in a structured relational space, using ILP. A key difficulty with ILP is that the increased expressivity dramatically expands the hypothesis space, and it is widely agreed that some learning bias is required for ILP to be tractable (N´edellec et al., 1996; Cumby and Roth, 2003). Our work can be viewed as a new method for acquiring such bias from text; moreover, our approach is not specialized for ILP and may be used to transform the feature space in other forms of relational learning as well (Roth and Yih, 2001; Cumby and Roth, 2003; Richardson and Domingos, 2006). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervised generative model that accounts for both syntax and lexical choice. The semantic abstract is represented as a set of predicate logic formulae, which are applied as higher-order features for learning. We demonstrate that these features improve learning performance, and that both the lexica</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Richardson, Matthew and Pedro Domingos. 2006. Markov logic networks. Machine Learning 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Relational learning via propositional algorithms: An information extraction case study.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI2001).</booktitle>
<pages>1257--1263</pages>
<contexts>
<context position="36544" citStr="Roth and Yih, 2001" startWordPosition="5979" endWordPosition="5982">ungrounded variables. Relational learning The output of our semantic analysis is applied to learning in a structured relational space, using ILP. A key difficulty with ILP is that the increased expressivity dramatically expands the hypothesis space, and it is widely agreed that some learning bias is required for ILP to be tractable (N´edellec et al., 1996; Cumby and Roth, 2003). Our work can be viewed as a new method for acquiring such bias from text; moreover, our approach is not specialized for ILP and may be used to transform the feature space in other forms of relational learning as well (Roth and Yih, 2001; Cumby and Roth, 2003; Richardson and Domingos, 2006). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervised generative model that accounts for both syntax and lexical choice. The semantic abstract is represented as a set of predicate logic formulae, which are applied as higher-order features for learning. We demonstrate that these features </context>
</contexts>
<marker>Roth, Yih, 2001</marker>
<rawString>Roth, Dan and Wen-tau Yih. 2001. Relational learning via propositional algorithms: An information extraction case study. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI2001). pages 1257–1263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayaram Sethuraman</author>
</authors>
<title>A constructive definition of dirichlet priors.</title>
<date>1994</date>
<journal>Statistica Sinica</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="12907" citStr="Sethuraman, 1994" startWordPosition="2130" endWordPosition="2132">n over alignments is described in Section 3.1. Finally, the aligned literals are used to generate the words at each slot in the dependency tree. A more formal definition of this process is as follows: • Draw A, the expected number of literals per formula, from a Gamma distribution g(u, v). • Draw an infinite set of formulae f. For each formula fi, – Draw the formula length #|fi |from a Poisson distribution, ni — Poisson(A). – Draw ni literals from a uniform distribution. • Draw 7r, an infinite multinomial distribution over formulae: 7r — GEM(7r0), where GEM refers to the stick-breaking prior (Sethuraman, 1994) and 7r0 = 1 is the concentration parameter. By attaching the multinomial 7r to the infinite set of formulae f, we create a Dirichlet process. This is conventionally written DP(7r0, G0), where the base distribution G0 encodes only the distribution over the number of literals, Poisson(A). • For each of D documents, draw the number of sentences T — Poisson. For each of the T sentences in the document, – Draw a formula f — DP(7r0, G0) from the Dirichlet Process described above. – Draw a sentence length #|s |— Poisson. – Draw a dependency graph t (a spanning tree of size #|s|) from a uniform distr</context>
</contexts>
<marker>Sethuraman, 1994</marker>
<rawString>Sethuraman, Jayaram. 1994. A constructive definition of dirichlet priors. Statistica Sinica 4(2):639–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000).</booktitle>
<pages>63--70</pages>
<contexts>
<context position="26763" citStr="Toutanova and Manning, 2000" startWordPosition="4416" endWordPosition="4419">omain. A total of five instruction sets were 3Freely available from http://www.comlab.ox. ac.uk/activities/machinelearning/Aleph/ obtained from the Internet. Due to the popularity of the Microsoft implementation of Freecell, instructions often contain information specific to playing Freecell on a computer. We manually removed sentences which did not focus on the card aspects of Freecell (e.g., how to set up the board and information regarding where to click to move cards). In order to use our semantic abstraction model, the instructions were part-of-speech tagged with the Stanford POS Tagger (Toutanova and Manning, 2000) and dependency parses were obtained using Malt (Nivre, 2006). Glosses Our reading to learn setting requires a small set of glosses, which are surface forms commonly used to represent predicates from the world model. We envision an application scenario in which a designer manually specifies a few glosses for each predicate. However, for the purposes of evaluation, it would be unprincipled for the experimenters to handcraft the ideal set of glosses. Instead, we gathered a development set of text and annotated the lexical mentions of the world model predicates in text. This annotation is used to</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Toutanova, Kristina and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000). pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007).</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="35065" citStr="Wong and Mooney, 2007" startWordPosition="5736" endWordPosition="5739">er and Collins, 2005; Lu et al., 2008). Level of analysis We aim for an abstractive semantic summary across multiple documents, whereas other approaches attempt to produce logical forms for individual sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). We avoid the requirement that each sentence have a meaningful interpretation within the domain, allowing us to handle relatively unstructured text. Evaluation We do not evaluate the representations obtained by our model; rather we assess whether these representations improve learning performance. This is similar to work on GeoQuery (Wong and Mooney, 2007; Ge and Mooney, 2005), and also to recent work on following stepby-step directions (Branavan et al., 2009). While these evaluations are performed on the basis of individual sentences, actions, or system responses, we evaluate the holistic semantic analysis obtained by our system. Model We treat surface text as generated from a latent semantic description. Lu et al. (2008) apply a generative model, but require a complete derivation from semantics to the lexical representation, while we favor a more flexible semantic analysis that can be learned without annotation and applied to noisy text. Mor</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Yuk Wah and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007). Prague, Czech Republic, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI</booktitle>
<pages>658--666</pages>
<contexts>
<context position="3951" citStr="Zettlemoyer and Collins, 2005" startWordPosition="626" endWordPosition="629">epresentation of the text. We merely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning. This will allow us to directly evaluate the contribution of the extracted relations for learning. We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the 958 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 958–967, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP world (Chen and Mooney, 2008). For the purpose of learning, this approach may be inapplicable, as such text is often written at a high level of abstraction that permits no grounded representation. There are two properties of our setting that make unsupervised learning feasible. First, it is not necessary to extract a semantic representation of each individu</context>
<context position="32881" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5400" endWordPosition="5403">ates (in Freecell, the legality of playing a card on a homecell depends on the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e.g., card(x1) ∧ tableau(x1). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic s</context>
<context position="34464" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5644" endWordPosition="5647">glosses per predicate. Previous work also considered the supervision signal obtained by interpreting natural language in the context of a formal domain. Branavan et al. (2009) use feedback from a world model as a supervision signal. Chen and Mooney (2008) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). Level of analysis We aim for an abstractive semantic summary across multiple documents, whereas other approaches attempt to produce logical forms for individual sentences (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). We avoid the requirement that each sentence have a meaningful interpretation within the domain, allowing us to handle relatively unstructured text. Evaluation We do not evaluate the representations obtained by our model; rather we assess whether these representations improve learning performance. This is similar to work on GeoQuery (Wong and Mooney, 200</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Zettlemoyer, Luke S. and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI 2005). pages 658–666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>