<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002078">
<title confidence="0.9993275">
Real-World Semi-Supervised Learning
of POS-Taggers for Low-Resource Languages
</title>
<author confidence="0.999883">
Dan Garrette&apos; Jason Mielens2 Jason Baldridge2
</author>
<affiliation confidence="0.998744">
&apos;Department of Computer Science 2Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
</affiliation>
<email confidence="0.998988">
dhg@cs.utexas.edu {jmielens,jbaldrid}@utexas.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994408">
Developing natural language processing
tools for low-resource languages often re-
quires creating resources from scratch.
While a variety of semi-supervised meth-
ods exist for training from incomplete
data, there are open questions regarding
what types of training data should be used
and how much is necessary. We dis-
cuss a series of experiments designed to
shed light on such questions in the con-
text of part-of-speech tagging. We obtain
timed annotations from linguists for the
low-resource languages Kinyarwanda and
Malagasy (as well as English) and eval-
uate how the amounts of various kinds
of data affect performance of a trained
POS-tagger. Our results show that an-
notation of word types is the most im-
portant, provided a sufficiently capable
semi-supervised learning infrastructure is
in place to project type information onto
a raw corpus. We also show that finite-
state morphological analyzers are effective
sources of type information when few la-
beled examples are available.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982943396227">
Low-resource languages present a particularly dif-
ficult challenge for natural language processing
tasks. For example, supervised learning meth-
ods can provide high accuracy for part-of-speech
(POS) tagging (Manning, 2011), but they per-
form poorly when little supervision is avail-
able. Good results in weakly-supervised tagging
have been obtained by training sequence models
such as hidden Markov models (HMM) using the
Expectation-Maximization algorithm (EM), how-
ever most work in this area has still relied on rel-
atively large amounts of data, both annotated and
unannotated, as well as an assumption that the an-
notations are very clean (Kupiec, 1992; Merialdo,
1994).
The ability to learn taggers using very little data
is enticing: only a tiny fraction of the world’s lan-
guages have enough data for standard supervised
models to work well. The collection or develop-
ment of resources is a time-consuming and expen-
sive process, creating a significant barrier for an
under-studied language where there are few ex-
perts and little funding. It is thus important to
develop approaches that achieve good accuracy
based on the amount of data that can be reasonably
obtained, for example, in just a few hours by a lin-
guist doing fieldwork on a non-native language.
Previous work explored learning taggers from
weak information, but the type, amount, quality,
and sources of data raise questions about the appli-
cability of those results to real-world low-resource
scenarios (Toutanova and Johnson, 2008; Ravi and
Knight, 2009; Hasan and Ng, 2009; Garrette and
Baldridge, 2012). Most research simulated weak
supervision with tag dictionaries extracted from
existing large, expertly-annotated corpora. These
resources have been developed over long periods
of time by trained annotators who collaborate to
produce high-quality analyses. They are also bi-
ased towards including only the most likely tag
for each word type, resulting in a cleaner dictio-
nary than one would find in a real scenario. As
such, these experiments do not reflect real-world
constraints.
One exception to this work is Goldberg et al.
(2008): they use a manually-constructed lexicon
for Hebrew in order to learn an HMM tagger. How-
ever, this lexicon was constructed by trained lexi-
cographers over a long period of time and achieves
very high coverage of the language with very good
quality, much better than could be achieved by
our non-expert linguistics graduate student anno-
tators in just a few hours. Cucerzan and Yarowsky
</bodyText>
<page confidence="0.985583">
583
</page>
<note confidence="0.913912">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583–592,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9909785">
(2002) learn a POS-tagger from existing linguis-
tic resources, namely a dictionary and a refer-
ence grammar, but these resources are not avail-
able, much less digitized, for most under-studied
languages. Haghighi and Klein (2006) develop a
model in which a POS-tagger is learned from a list
of POS tags and just three “prototype” word types
for each tag, but their approach requires a vector
space to compute the distributional similarity be-
tween prototypes and other word types in the cor-
pus. Such distributional models are not feasible
for low-resource languages because they require
immense amounts of raw text, much more than is
available in these settings (Abney and Bird, 2010).
Further, they extracted their prototype lists directly
from a labeled corpus, something we are specif-
ically avoiding. T¨ackstr¨om et al. (2013) evalu-
ate the use of mixed type and token constraints
generated by projecting information from a high-
resource language to a low-resource language via
a parallel corpus. However, large parallel corpora
are not available for most low-resource languages.
These are also expensive resources to create and
would take considerably more effort to produce
than the monolingual resources that our annotators
were able to generate in a four-hour timeframe.
Of course, if they are available, such parallel text
links could be incorporated into our approach.
In our previous work, we developed a differ-
ent strategy based on generalizing linguistic input
with a computational model: linguists annotated
either types or tokens for two hours, these anno-
tations are projected onto a corpus of unlabeled
tokens using label propagation and HMMs, and
a final POS-tagger is trained on this larger auto-
labeled corpus (Garrette and Baldridge, 2013).
That approach uses much more realistic types
and quantities of resources than previous work;
nonetheless, it leaves many open questions regard-
ing the effectiveness of incrementally more anno-
tation, the role of unannotated data, and whether
there is a good balance to be found using a combi-
nation of type- and token-supervision. We also did
not consider morphological analyzers as a form
of type supervision, as suggested by Merialdo
(1994).
This paper addresses these questions via a se-
ries of experiments designed to quantify the ef-
fect on performance given by the amount of time
spent finding or annotating training materials. We
specifically look at the impact of four types of data
collection:
</bodyText>
<listItem confidence="0.99981">
1. Time annotating sentences (token supervision)
2. Time creating tag dictionary (type supervision)
3. Time constructing a finite state transducer
(FST) to analyze word-type morphology
4. Amount of raw data available for training
</listItem>
<bodyText confidence="0.999682740740741">
We explore these strategies in the context of POS-
tagging for Kinyarwanda and Malagasy. We also
include experiments for English, pretending as
though it is a low-resource language. The over-
whelming take away from our results is that type
supervision—when backed by an effective semi-
supervised learning approach—is the most impor-
tant source of linguistic information. Also, mor-
phological analyzers help for morphologically rich
languages when there are few labeled types or to-
kens (and, it never hurts to use them). Finally, per-
formance improves with more raw data, though we
see diminishing returns past 400,000 tokens. With
just four hours of type annotation, our system ob-
tains good accuracy across the three languages:
89.8% on English, 81.9% on Kinyarwanda, and
81.2% on Malagasy.
Our results compare favorably with previous
work despite using considerably less supervision
and a more difficult set of tags. For example, Li et
al. (2012) use the entirety of English Wiktionary
directly as a tag dictionary to obtain 87.1% accu-
racy on English, below our result. T¨ackstr¨om et al.
(2013) average 88.8% across 8 major languages,
but for Turkish, a morphologically rich language,
they achieve only 65.2%, significantly below our
81.9% for morphologically-rich Kinyarwanda.
</bodyText>
<sectionHeader confidence="0.991733" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.836251416666667">
Kinyarwanda (KIN) and Malagasy (MLG) are low-
resource, KIN is morphologically rich, and English
(ENG) is used for comparison. For each language,
sentences were divided into four sets: training data
to be labeled by annotators, raw training data, de-
velopment data, and test data.
Data sources The KIN texts are transcripts of
testimonies by survivors of the Rwandan geno-
cide provided by the Kigali Genocide Memorial
Center. The MLG texts are articles from the web-
sites1 Lakroa and La Gazette and Malagasy Global
Voices.2 Texts in both KIN and MLG were tok-
</bodyText>
<footnote confidence="0.999402">
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
</footnote>
<page confidence="0.994722">
584
</page>
<table confidence="0.995314666666667">
KIN MLG ENG - Experienced ENG - Novice
time type token type token type token type token
1:00 801 559 (1093) 660 422 (899) 910 522 (1124) 210 308 (599)
2:00 1814 948 (2093) 1363 785 (1923) 2660 1036 (2375) 631 646 (1429)
3:00 2539 1324 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178)
4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933)
</table>
<tableCaption confidence="0.995167">
Table 1: Annotations for each language and annotator as time increases. Shows the number of tag
</tableCaption>
<bodyText confidence="0.997537642857143">
dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses).
For brevity, the table only shows hourly progress.
enized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
12 and 23 distinct POS tags, respectively.
The Penn Treebank (PTB) (Marcus et al., 1993)
is used as ENG data. Section 01 was used for
token-supervised annotation, sections 02-14 were
used as raw data, 15-18 for development of the
FST, 19-21 as a dev set and 22-24 as a test set.
The PTB uses 45 distinct POS tags.
Collecting annotations Linguists with non-
native knowledge of KIN and MLG produced anno-
tations for four hours (in 30-minute intervals) for
two tasks. In the first task, type-supervision, the
annotator was given a list of the words in the tar-
get language (ranked from most to least frequent),
and they annotated each word type with its poten-
tial POS tags. The word types and frequencies used
for this task were taken from the raw training data
and did not include the test sets. In the second
task, token-supervision, full sentences were anno-
tated with POS tags. The 30-minute intervals allow
us to investigate the incremental benefit of addi-
tional annotation of each type as well as how both
annotation types might be combined within a fixed
annotation budget.
Baldridge and Palmer (2009) found that anno-
tator expertise greatly influences effectiveness of
active learning for morpheme glossing, a related
task. To see how differences in annotator speed
and quality impact our task, we obtained ENG data
from an experienced annotator and a novice one.
Ngai and Yarowsky (2000) investigated the ef-
fectiveness of rule-writing versus annotation (us-
ing active learning) for chunking, and found the
latter to be far more effective. While we do not
explore a rule-writing approach to POS-tagging,
we do consider the impact of rule-based morpho-
logical analyzers as a component in our semi-
supervised POS-tagging system.
</bodyText>
<table confidence="0.980187666666667">
ENG - Exp. ENG - Nov.
time type tok type tok
1:00 0.05 0.03 0.01 0.02
2:00 0.15 0.05 0.03 0.03
3:00 0.24 0.06 0.07 0.05
4:00 0.32 0.08 0.11 0.06
</table>
<tableCaption confidence="0.980995">
Table 2: Tag dictionary recall against the test set
</tableCaption>
<bodyText confidence="0.98807359375">
for ENG annotators on type and token annotations.
Annotations Table 1 gives statistics for all lan-
guages and annotators showing progress during
the 4-hour tasks. With token-annotation, tag
dictionary growth slows because high-frequency
words are repeatedly annotated, producing only
additional frequency and sequence information.
In contrast, every type-annotation label is a new
tag dictionary entry. For types, growth increases
over time, reflecting the fact that high-frequency
words (which are addressed first) tend to be more
ambiguous and thus require more careful thought
than later words. For ENG, we can compare the
tagging speed of the experienced annotator with
the novice: 50% more tokens and 3 times as many
types. The token-tagging speed stayed fairly con-
stant for the experienced annotator, but the novice
increased his rate, showing the result of practice.
Checking the annotators’ output against the
gold tags in the PTB shows that both had good
tagging accuracy on tokens: 94-95%. Comparing
the tag dictionary entries versus the test data, pre-
cision starts in the high 80%s and falls to to the
mid-70%s in all cases. However, the differences
in recall, shown in Table 2, are more interesting.
On types, the experienced annotator maxed out at
32%, but the novice only reaches 11%. More-
over, the maximum for token annotations is much
lower due to high repeat-annotation. The discrep-
ancies between experienced and novice, and be-
tween type and token recall explain a great deal of
the performance disparity seen in the experiments.
</bodyText>
<page confidence="0.997153">
585
</page>
<sectionHeader confidence="0.989556" genericHeader="method">
3 Morphological Transducers
</sectionHeader>
<bodyText confidence="0.999588142857143">
Finite-state transducers (FSTs) accept regular lan-
guages and can be constructed easily using regu-
lar expressions, which makes them quite useful for
phonology, morphology and limited areas of syn-
tax (Karttunen, 2001). Past work has used FSTs
for direct POS-tagging (Roche and Schabes, 1995),
but this requires tight coupling between the FST
and target tagset. We use FSTs for morphologi-
cal analysis: the FST accepts a word type and pro-
duces a set of morphological features. If there are
multiple possible analyses for a given word type,
the FST returns them all. For instance the Kin-
yarwanda verb sibatarazuka “he is not yet resur-
rected” is analyzed in several ways:
</bodyText>
<listItem confidence="0.996417333333333">
• +NEG+CL2+1PL+V+arazuk+IMP
• +NEG+CL2+NOT.YET+PRES+zuk+IMP
• +NEG+CL2+NOT.YET+razuk+IMP
</listItem>
<bodyText confidence="0.999896064516129">
FSTs are particularly valuable for their ability
to analyze out-of-vocabulary items. By looking
for known affixes, FSTs can guess the stem of
a word and produce an analysis despite not hav-
ing knowledge of that stem. For morphologically
complex languages like KIN, this ability is espe-
cially useful. Other factors, such as a large num-
ber of morphologically-conditioned phonological
changes (seen in MLG) make out-of-vocabulary
guessing more challenging because of the large
number of potential stems (high ambiguity).
Development of the FSTs for all three languages
was done by iteratively adding rules and lexical
items with the goal of increasing coverage on a
raw dataset. To accomplish this on a fixed time
budget, the most frequently occurring unanalyzed
tokens were examined, and their stems plus any
observable morphological or phonological pat-
terns were added to the transducer. Addition-
ally, developers searched for known morpholog-
ical alternations to locate instances of phonolog-
ical change for inclusion. Coverage was checked
against a raw dataset which did not include the test
data used for the POS experiments.
The KIN and MLG FSTs were created by
English-speaking linguists who were familiar with
their respective language. They also used dictio-
naries and grammars. Each FST was developed
in 10 hours. To evaluate the benefits of more de-
velopment time, a version of the English FST was
saved every 30 minutes, as shown in Table 3.
</bodyText>
<table confidence="0.989232428571429">
elapsed tokens types
time count pct count pct
2:00 130k 61% 2.1k 12%
4:00 159k 75% 4.1k 24%
6:00 170k 80% 6.7k 39%
8:00 182k 86% 7.7k 44%
10:00 192k 91% 10.7k 62%
</table>
<tableCaption confidence="0.743544666666667">
Table 3: Coverage of the English morphological
FST during development. For brevity, showing 2-
hour increments instead of 30-minute segments.
</tableCaption>
<table confidence="0.9462906">
tokens types
cov. ambig. cov. ambig.
KIN 86% 2.62 82% 5.31
MLG 78% 2.98 37% 1.13
ENG 91% 1.19 62% 1.97
</table>
<tableCaption confidence="0.8861535">
Table 4: Coverage and ambiguity of the final FST
for each language.
</tableCaption>
<sectionHeader confidence="0.991232" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999944214285714">
Learning under low-resource conditions is more
difficult than scenarios in most previous POS work
because the vast majority of the word types in the
training and test data are not covered by the an-
notations. When most words are unknown, learn-
ing algorithms such as EM struggle (Garrette and
Baldridge, 2012). Recall that most work on learn-
ing POS-taggers from tag dictionaries used tag dic-
tionaries culled from test sets (even when consid-
ering incomplete dictionaries). We thus build on
our previous approach, which exploits extremely
sparse, human-generated annotations that are pro-
duced without knowledge of which words appear
in the test set (Garrette and Baldridge, 2013).
This approach generalizes a small initial tag dic-
tionary to include unannotated word types appear-
ing in raw data. It estimates word/tag pair and
tag-transition frequency information using model-
minimization, which also reduces noise intro-
duced by automatic tag dictionary expansion. The
approach exploits type annotations effectively to
learn parameters for out-of-vocabulary words and
infer missing frequency and sequence informa-
tion. This pipeline is described in detail in the
previous work, so we give only a brief overview
and describe our additions.
The purpose of tag dictionary expansion is to es-
timate label distributions for tokens in a raw cor-
</bodyText>
<page confidence="0.992786">
586
</page>
<bodyText confidence="0.999989271428572">
pus, including words missing in the annotations.
For this, a graph connecting annotated words to
unannotated words via features is constructed and
POS labels are pushed between these items using
label propagation (LP) (Talukdar and Crammer,
2009). LP has been used successfully for extend-
ing POS labels from high-resource languages to
low via parallel corpora (Das and Petrov, 2011;
T¨ackstr¨om et al., 2013; Ding, 2011) or high- to
low-resource domains (Subramanya et al., 2010),
among other tasks. These works have typically
used n-gram features (capturing basic syntax) and
character affixes (basic morphology).
The character n-gram affix-as-morphology ap-
proach produces many features, but only a fraction
of them represent actual morphemes. Incorrect
features end up pushing noise around the graph,
so affixes can lead to more false labels that drown
out the true labels. While affixes may be suffi-
cient for languages with limited morphology, their
effectiveness diminishes for morphology-rich lan-
guages, which have much higher type-to-token ra-
tios. More types means sparser word frequency
statistics and more out-of-vocabulary items, and
thus problems for EM. Here, we modify the LP
graph by supplementing or replacing generic af-
fix features with a focused set of morphological
features produced by an FST. These targeted mor-
phological features are effective during LP because
words that share them are much more likely to ac-
tually share POS tags.
FSTs produce multiple analyses, which is actu-
ally advantageous for LP. Ambiguities need not be
resolved since we just take the union of all mor-
phological features for all analyses and use them
as features in the graph. Note that each FST pro-
duces its own POS-tags as features, but these do
not correspond to the target POS tagset used by the
tagger. This is important because it decouples FST
development and the final POS task. Thus, any FST
for the language, regardless of its provenance, can
be used with any target POS tagset.
Since the LP graph contains a node for each cor-
pus token, and each node is labeled with a distri-
bution over POS tags, the graph provides a corpus
of sentences labeled with noisy tag distributions
along with an expanded tag dictionary. This out-
put is useful as input to EM because it contains
labels for all seen word types as well as sequence
and frequency information. There is a high degree
of noise in the LP output, so we employ the model
minimization strategy of Ravi et al. (2010), which
finds a minimal set of tag bigrams needed to ex-
plain the sentences in the raw corpus. It outputs
a corpus of tagged sentences, which are used as
a good starting point for EM training of an HMM.
The expanded tag dictionary constrains the EM
search space by providing a limited tagset for each
word type, steering EM towards a desirable result.
Because the HMM trained by EM will con-
tain zero-probabilities for words that did not ap-
pear in the training corpus, we use the “auto-
supervision” step from our previous work: a Max-
imum Entropy Markov Model tagger is trained
on a corpus that is noisily labeled by the HMM
(Garrette and Baldridge, 2012). While training
an HMM before the MEMM is not strictly neces-
sary, our tests have shown that this generative-
then-discriminative combination generally results
in around 3% accuracy improvement.
</bodyText>
<sectionHeader confidence="0.962123" genericHeader="evaluation">
5 Experiments3
</sectionHeader>
<bodyText confidence="0.999934866666667">
To better understand the effect that each type of
supervision has on tagger accuracy, we perform a
series of experiments, with KIN and MLG as true
low-resource languages. English experiments, for
which we had both experienced and novice an-
notators, allow for further exploration into issues
concerning data collection and preparation.
The overall best accuracies achieved by lan-
guage are 81.9% for KIN using all types, 81.2% for
MLG using half types and half tokens, and 89.8%
for ENG using all types and the maximal amount
of raw data. All of these best values were achieved
using both FST and affix LP features.
All results described in this section are averaged
over five folds of raw data.
</bodyText>
<subsectionHeader confidence="0.965433">
5.1 Types versus tokens
</subsectionHeader>
<bodyText confidence="0.999959375">
Our primary question was the relationship be-
tween annotation type and time. Annotation must
be done by someone familiar with the target lan-
guage, linguistics, and the target POS tagset. For
many low-resource languages, such people, and
the time they have to spend, are likely to be in
short supply. To make the best use of their time,
we need to know which annotations are most use-
</bodyText>
<footnote confidence="0.909129333333333">
3Code and all MLG data available at github.com/
dhgarrette/low-resource-pos-tagging-2013
We are unable to provide the KIN or ENG data for down-
load due to licensing restrictions. However, ENG data may
be shared with those holding a license for the Penn Treebank
and KIN data may be shared on a case-by-case basis.
</footnote>
<page confidence="0.991255">
587
</page>
<figure confidence="0.997063818181818">
80
75
70
Accuracy
65
60
55
50
70
65
Accuracy
80
75
60
55
50
No LP
Affixes only
FST only
Affixes+FST
No LP
Affixes only
FST only
Affixes+FST
(a) KIN type annotations − Elapsed Annotation Time
(c) MLG type annotations − Elapsed Annotation Time
(b) KIN token annotations − Elapsed Annotation Time
(d) MLG token annotations − Elapsed Annotation Time
No LP
Affixes only
FST only
Affixes+FST
No LP
Affixes only
FST only
Affixes+FST
Accuracy 80
75
70
65
Accuracy 80
75
70
65
</figure>
<figureCaption confidence="0.873934">
Figure 1: Annotation time vs. tagger accuracy for type-only and token-only annotations.
Figure 2: Annotation time vs. tagger accuracy for
ENG type-only and token-only annotations with
affix and FST LP features.
</figureCaption>
<bodyText confidence="0.999730303030303">
ful so that efforts can be concentrated there. Ad-
ditionally, it is useful to identify when returns on
annotation effort diminish so that annotators do
not spend time doing work that is unlikely to add
much value.
The annotators produced four hours each of
type and token annotations, each in 30-minute in-
crements. To assess the effects of annotation time,
we trained taggers cumulatively on each increment
and determine the value of each additional half-
hour of effort. Results are shown for KIN and MLG
in Figure 1 and ENG in Figure 2. In all scenarios,
the use of LP (and model minimization) delivers
huge performance gains. Additionally, the use of
FST features, usually along with affixes, yielded
better results than without. This indicates the LP
procedure makes effective use of the morpholog-
ical features produced by the FST and that the af-
fix features are able to capture missing information
without adding too much noise to the LP graph.
Furthermore, performance is considerably bet-
ter when type annotations are used than only to-
kens. Type annotations plateau much faster, so
a shorter amount of time must be spent annotat-
ing types than if token annotations are used. For
KIN it takes approximately 1.5 hours to reach near-
maximum accuracy for types, but 2.5 hours for to-
kens. This difference is due to the fact that the type
annotations started with the most frequent words
whereas the token annotations were on random
sentences. Thus, type annotations quickly cover a
significant portion of the language’s tokens. With
annotations directly on tokens, some of the highest
</bodyText>
<figure confidence="0.984769617647059">
Experienced annotator − Types
Experienced annotator − Tokens
Novice annotator − Types
Novice annotator − Tokens
Elapsed Annotation Time
Accuracy 85
80
75
70
65
588
(b) MLG − Type/Token Annotation Mixture
80
75
70
Accuracy
65
60
(a) KIN − Type/Token Annotation Mixture
No LP
Affixes only
FST only
Affixes+FST
Accuracy
80
78
76
74
72
70
No LP
Affixes only
FST only
Affixes+FST
</figure>
<figureCaption confidence="0.99971">
Figure 3: Annotation mixture vs. tagger accuracy. X-axis labels give annotation proportions, e.g. “t2/s6”
indicates 2/8 of the time (1 hour) was spent annotating types and 6/8 (3 hours), full sentences.
</figureCaption>
<figure confidence="0.779473444444445">
Exp. − With LP
Nov. − With LP
Exp. − No LP
Nov. − No LP
85
Accuracy 80
75
70
Type/Token Annotation Mixture
</figure>
<figureCaption confidence="0.944814">
Figure 4: Annotation mixture vs. tagger accuracy
on ENG using affix and FST LP features for experi-
enced (Exp.) and novice (Nov.) annotators.
</figureCaption>
<bodyText confidence="0.997462294117647">
frequency types are covered, but annotation time
is also ineffectively used on low-frequency types
that happen to appear in those sentences.
Finally, the use of FST features yields the largest
gains for KIN, but only when small amounts of
annotation are available. This makes sense: KIN
is a morphologically rich language, so sparsity is
greater and crude affixes capture less actual mor-
phology. With little annotated data, LP relies heav-
ily on morphological features to make clean links
between words. But, with more annotations, the
gains of the FST over affix features alone dimin-
ishes: the affix features eventually capture enough
of the morphology to make up the difference.
Figure 2 shows the dramatic differences be-
tween the experienced and novice ENG annota-
tors.4 For the former, results using types and to-
</bodyText>
<footnote confidence="0.98967225">
4The ENG graph omits “No LP” results since they fol-
lowed patterns similar to KIN and MLG. Additionally, the
results without FST features are not shown because they were
nearly identical (though slightly lower) than with the FST.
</footnote>
<bodyText confidence="0.99992575">
kens were similar after 30 minutes, but type an-
notations proved much more useful beyond that.
In contrast, the novice annotated types much more
slowly, so early on there were not enough anno-
tated types for the training to be as effective. Even
so, after three hours of annotation, type annota-
tions still win with the novice, and even beat the
experienced annotator labeling tokens.
</bodyText>
<subsectionHeader confidence="0.99995">
5.2 Mixing type and token annotations
</subsectionHeader>
<bodyText confidence="0.996173076923077">
Because type and token annotations are each bet-
ter at providing different information — a tag dic-
tionary of high-frequency words vs. sequence and
frequency information — it is reasonable to ex-
pect that a combination of the two might yield
higher performance by each contributing differ-
ent but complementary information during train-
ing. This matters in low-resource settings because
type or token annotations will likely be produced
by the same people, so there is a tradeoff between
spending resources on one form of annotation over
the other. Understanding the best mixture of an-
notations can inform us on how to maximize the
benefit of a set annotation budget. To this end, we
ran experiments fixing the annotation time to four
hours while varying the mix of type and token an-
notations. Results are shown for KIN and MLG in
Figure 3 and ENG in Figure 4.
For KIN and ENG, tagger accuracy increases as
the proportion of type annotations increases for all
LP feature configurations. For MLG, however, as
the reliance on the FST increases, the optimal mix-
ture shifts toward higher type proportions. When
only affix features are used, the optimal mixture is
1 hour of types and 3 hours of tokens. When FST
and affix features are used, the optimum is 2 hours
</bodyText>
<page confidence="0.996818">
589
</page>
<bodyText confidence="0.999961769230769">
each of types and tokens. When only FST features
are used, it is best to use 3.5 hours of types and
only 30 minutes of tokens. Because the FST op-
erates on word types, it is effective at exploiting
type annotations. Thus, when the LP focuses more
on FST features, it becomes more desirable to have
larger amounts of type annotations.
Types clearly win for ENG. The experienced an-
notator was much faster at annotating types and
the speed difference was less pronounced for to-
kens, so accuracy is most similar when only token
annotations are used. The performance disparity
grows with increasing the type proportion.
T¨ackstr¨om et al. (2013) explore the use of
mixed type and token annotations in which a tag-
ger is learned by projecting information via par-
allel text. In their experiments, they—like us—
found that type information is more valuable than
token information. However, they were able to see
gains through the complementary effects of mix-
ing type and token annotations. It is likely that this
difference in our results is due to the amount of an-
notated data used. It seems that the amount of type
information collected in four hours is not sufficient
to saturate the system, meaning that switching to
annotating tokens tends to hurt performance.
</bodyText>
<subsectionHeader confidence="0.99221">
5.3 FST development
</subsectionHeader>
<bodyText confidence="0.999998722222222">
The third set of experiments evaluate how the
amount of time spent developing an FST affects
the performance of trained tagger. To do this,
we had our ENG FST developer save progress af-
ter each hour (for ten hours). The results show
that, for ENG, the FST provided no value, regard-
less of how much time was spent on its develop-
ment. Moreover, since large gains in accuracy can
be achieved by spending a small amount of time
just annotating word types with POS tags, we are
led to conclude that time should be spent annotat-
ing types or tokens instead of developing an FST.
While it is likely that FST development time would
have a greater impact for morphologically rich
languages, we suspect that greater gains can still
be obtained by instead annotating types. Nonethe-
less, FSTs never seems to hurt performance, so if
one is readily available, it should be used.
</bodyText>
<subsectionHeader confidence="0.957554">
5.4 The effect of more raw data
</subsectionHeader>
<bodyText confidence="0.943136620689655">
In addition to annotations, semi-supervised tagger
training requires a corpus of raw text. Raw data
can be easier to acquire since it does not need
the attention of a linguist. Even so, for many
Figure 5: Amount of raw data vs. tagger accuracy
for ENG using high vs. low amounts of annotation
and using LP vs. no LP., for experienced annotator
(novice results were similar).
low-resource languages, the amount of digitized
text, such as transcripts or websites, is very lim-
ited and may, in fact, require substantial effort
to accumulate, even with assistance from compu-
tational tools (Bird, 2011). Therefore, the col-
lection of raw data can be considered another
time-sensitive task for which the tradeoffs with
previously-discussed annotation efforts must con-
tend.
It could be the case that more raw data for train-
ing could make up for additional annotation and
FST development effort or make the LP proce-
dure unnecessary. Figure 5 shows that that in-
creased raw data does provide increasing gains,
but they diminish after 200k tokens. The best per-
formance is achieved by using more annotation
and LP. Most importantly, however, removing ei-
ther annotations or LP results in a significant de-
cline in accuracy, such that even with 600k train-
ing tokens, we are unable to achieve the results of
high annotation and LP using only 100k tokens.
</bodyText>
<subsectionHeader confidence="0.997158">
5.5 Correcting existing annotations
</subsectionHeader>
<bodyText confidence="0.999929">
For all of the ENG experiments, we also ran “or-
acle” experiments using gold tags for the same
sentences or a tag dictionary containing the same
number of type/tag entries as the annotator pro-
duced, but containing only the most frequent
entries as determined by the gold-labeled cor-
pus. Using this simulated “perfect annotator” data
shows we lose accuracy due to annotator mistakes:
for our experienced annotator and maximal FST,
using 4 hours of types the oracle accuracy is 90.5
vs. 88.5 while using only tokens we see 83.9 vs.
</bodyText>
<figure confidence="0.974209571428571">
90
88
Accuracy
86
84
82
80
</figure>
<footnote confidence="0.79988675">
4hr types, FST, With LP
4hr types, FST, No LP
1hr types, No FST, With LP
Number of Raw Data Tokens
</footnote>
<page confidence="0.978345">
590
</page>
<bodyText confidence="0.999302181818182">
81.5. This indicates that there are gains to be made
by correcting mistakes in the annotations. This
is true even after the point of diminishing returns
on the learning curve, meaning that even when
adding more annotations no longer improves per-
formance, progress can still be made by correcting
errors, so it may be reasonable to ask annotators to
attempt to correct errors in their past annotations.
Automated techniques for facilitating error identi-
fication can be employed for this (Dickinson and
Meurers, 2003).
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981666666667">
Care must be taken when drawing conclusions
from small-scale annotation studies such as those
presented in this paper. Nonetheless, we have
explored realistic annotation scenarios for POS-
tagging for low-resource languages and found sev-
eral consistent patterns. Most importantly, it is
clear that type annotations are the most useful in-
put one can obtain from a linguist—provided a
semi-supervised algorithm for projecting that in-
formation reliably onto raw tokens is available. In
a sense, this result validates the research trajectory
of efforts of the past two decades put into learning
taggers from tag dictionaries: papers have succes-
sively removed layers of unrealistic assumptions,
and in doing so have produced pipelines for type-
supervision that easily beat token-supervision pre-
pared in comparable amounts of time.
The result of most immediate practical value is
that we show it is possible to train effective POS-
taggers on actual low-resource languages given
only a relatively small amount of unlabeled text
and a few hours of annotation by a non-native
linguist. Instead of having annotators label full
sentences as one might expect the natural choice
would be, it is much more effective to simply
extract a list of the most frequent word types in
the language and concentrate efforts on annotat-
ing these types with their potential parts of speech.
Furthermore, for languages with rich morphology,
a morphological transducer can yield significant
performance gains when large amounts of other
annotated resources are unavailable. (And it never
hurts performance.)
Finally, additional raw text does improve per-
formance. However, using substantial amounts of
raw text is unlikely to produce gains larger than
only a few hours spent annotating types. Thus,
when deciding whether to spend time locating
larger volumes of digitized text or to spend time
annotating types, choose types.
Despite the consistent superiority of type anno-
tations in our experiments, it of course may be the
case that techniques such as active learning may
better select sentences for token annotation, so this
should be explored in future work.
</bodyText>
<sectionHeader confidence="0.997963" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998046">
We thank Kyle Jerro, Vijay John, Jim Evans, Yoav
Goldberg, Slav Petrov, and the reviewers for their
assistance and feedback. This work was sup-
ported by the U.S. Department of Defense through
the U.S. Army Research Office (grant number
W911NF-10-1-0533) and through a National De-
fense Science and Engineering Graduate Fellow-
ship for the first author. Experiments were run
on the UTCS Mastodon Cluster, provided by NSF
grant EIA-0303609.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997609">
Steven Abney and Steven Bird. 2010. The human lan-
guage project: Building a universal corpus of the
worlds languages. In Proceedings of ACL.
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP, Singa-
pore.
Steven Bird. 2011. Bootstrapping the language
archive: New prospects for natural language pro-
cessing in preserving linguistic heritage. Linguistic
Issues in Language Technology, 6.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Tai-
wan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT, Portland,
Oregon, USA.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of EACL.
Weiwei Ding. 2011. Weakly supervised part-of-
speech tagging for Chinese using label propagation.
Master’s thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP, Jeju, Korea.
</reference>
<page confidence="0.976576">
591
</page>
<reference confidence="0.99972685">
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL, Atlanta, Georgia.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings NAACL.
Kazi Saidul Hasan and Vincent Ng. 2009.
Weakly supervised part-of-speech tagging for
morphologically-rich, resource-scarce languages. In
Proceedings of EACL, Athens, Greece.
Lauri Karttunen. 2001. Applications of finite-state
transducers in natural language processing. Lecture
Notes in Computer Science, 2088.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &amp;
Language, 6(3).
Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Emmanuel Roche and Yves Schabes. 1995. Determin-
istic part-of-speech tagging with finite-state trans-
ducers. Computational Linguistics, 21(2).
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings EMNLP, Cambridge, MA.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. In Transactions of the ACL. Association for
Computational Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In Proceedings of ECML-PKDD, Bled, Slove-
nia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
</reference>
<page confidence="0.997722">
592
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655995">
<title confidence="0.9532115">Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</title>
<affiliation confidence="0.8516425">of Computer Science of Linguistics The University of Texas at Austin The University of Texas at Austin</affiliation>
<abstract confidence="0.999517807692308">Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and evaluate how the amounts of various kinds of data affect performance of a trained Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Steven Bird</author>
</authors>
<title>The human language project: Building a universal corpus of the worlds languages.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4664" citStr="Abney and Bird, 2010" startWordPosition="716" endWordPosition="719">s, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages. Haghighi and Klein (2006) develop a model in which a POS-tagger is learned from a list of POS tags and just three “prototype” word types for each tag, but their approach requires a vector space to compute the distributional similarity between prototypes and other word types in the corpus. Such distributional models are not feasible for low-resource languages because they require immense amounts of raw text, much more than is available in these settings (Abney and Bird, 2010). Further, they extracted their prototype lists directly from a labeled corpus, something we are specifically avoiding. T¨ackstr¨om et al. (2013) evaluate the use of mixed type and token constraints generated by projecting information from a highresource language to a low-resource language via a parallel corpus. However, large parallel corpora are not available for most low-resource languages. These are also expensive resources to create and would take considerably more effort to produce than the monolingual resources that our annotators were able to generate in a four-hour timeframe. Of cours</context>
</contexts>
<marker>Abney, Bird, 2010</marker>
<rawString>Steven Abney and Steven Bird. 2010. The human language project: Building a universal corpus of the worlds languages. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alexis Palmer</author>
</authors>
<title>How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<contexts>
<context position="10436" citStr="Baldridge and Palmer (2009)" startWordPosition="1650" endWordPosition="1653">t task, type-supervision, the annotator was given a list of the words in the target language (ranked from most to least frequent), and they annotated each word type with its potential POS tags. The word types and frequencies used for this task were taken from the raw training data and did not include the test sets. In the second task, token-supervision, full sentences were annotated with POS tags. The 30-minute intervals allow us to investigate the incremental benefit of additional annotation of each type as well as how both annotation types might be combined within a fixed annotation budget. Baldridge and Palmer (2009) found that annotator expertise greatly influences effectiveness of active learning for morpheme glossing, a related task. To see how differences in annotator speed and quality impact our task, we obtained ENG data from an experienced annotator and a novice one. Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. While we do not explore a rule-writing approach to POS-tagging, we do consider the impact of rule-based morphological analyzers as a component in our semisupervis</context>
</contexts>
<marker>Baldridge, Palmer, 2009</marker>
<rawString>Jason Baldridge and Alexis Palmer. 2009. How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation. In Proceedings of EMNLP, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>Bootstrapping the language archive: New prospects for natural language processing in preserving linguistic heritage.</title>
<date>2011</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>6</volume>
<contexts>
<context position="30176" citStr="Bird, 2011" startWordPosition="4931" endWordPosition="4932"> more raw data In addition to annotations, semi-supervised tagger training requires a corpus of raw text. Raw data can be easier to acquire since it does not need the attention of a linguist. Even so, for many Figure 5: Amount of raw data vs. tagger accuracy for ENG using high vs. low amounts of annotation and using LP vs. no LP., for experienced annotator (novice results were similar). low-resource languages, the amount of digitized text, such as transcripts or websites, is very limited and may, in fact, require substantial effort to accumulate, even with assistance from computational tools (Bird, 2011). Therefore, the collection of raw data can be considered another time-sensitive task for which the tradeoffs with previously-discussed annotation efforts must contend. It could be the case that more raw data for training could make up for additional annotation and FST development effort or make the LP procedure unnecessary. Figure 5 shows that that increased raw data does provide increasing gains, but they diminish after 200k tokens. The best performance is achieved by using more annotation and LP. Most importantly, however, removing either annotations or LP results in a significant decline i</context>
</contexts>
<marker>Bird, 2011</marker>
<rawString>Steven Bird. 2011. Bootstrapping the language archive: New prospects for natural language processing in preserving linguistic heritage. Linguistic Issues in Language Technology, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Bootstrapping a multilingual part-of-speech tagger in one person-day.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<location>Taipei, Taiwan.</location>
<marker>Cucerzan, Yarowsky, 2002</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 2002. Bootstrapping a multilingual part-of-speech tagger in one person-day. In Proceedings of CoNLL, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="17236" citStr="Das and Petrov, 2011" startWordPosition="2736" endWordPosition="2739">nce information. This pipeline is described in detail in the previous work, so we give only a brief overview and describe our additions. The purpose of tag dictionary expansion is to estimate label distributions for tokens in a raw cor586 pus, including words missing in the annotations. For this, a graph connecting annotated words to unannotated words via features is constructed and POS labels are pushed between these items using label propagation (LP) (Talukdar and Crammer, 2009). LP has been used successfully for extending POS labels from high-resource languages to low via parallel corpora (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ding, 2011) or high- to low-resource domains (Subramanya et al., 2010), among other tasks. These works have typically used n-gram features (capturing basic syntax) and character affixes (basic morphology). The character n-gram affix-as-morphology approach produces many features, but only a fraction of them represent actual morphemes. Incorrect features end up pushing noise around the graph, so affixes can lead to more false labels that drown out the true labels. While affixes may be sufficient for languages with limited morphology, their effectiveness diminishes for</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings of ACL-HLT, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting errors in part-of-speech annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="32127" citStr="Dickinson and Meurers, 2003" startWordPosition="5260" endWordPosition="5263"> 84 82 80 4hr types, FST, With LP 4hr types, FST, No LP 1hr types, No FST, With LP Number of Raw Data Tokens 590 81.5. This indicates that there are gains to be made by correcting mistakes in the annotations. This is true even after the point of diminishing returns on the learning curve, meaning that even when adding more annotations no longer improves performance, progress can still be made by correcting errors, so it may be reasonable to ask annotators to attempt to correct errors in their past annotations. Automated techniques for facilitating error identification can be employed for this (Dickinson and Meurers, 2003). 6 Conclusions and Future Work Care must be taken when drawing conclusions from small-scale annotation studies such as those presented in this paper. Nonetheless, we have explored realistic annotation scenarios for POStagging for low-resource languages and found several consistent patterns. Most importantly, it is clear that type annotations are the most useful input one can obtain from a linguist—provided a semi-supervised algorithm for projecting that information reliably onto raw tokens is available. In a sense, this result validates the research trajectory of efforts of the past two decad</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and W. Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Ding</author>
</authors>
<title>Weakly supervised part-ofspeech tagging for Chinese using label propagation. Master’s thesis,</title>
<date>2011</date>
<institution>University of Texas at Austin.</institution>
<contexts>
<context position="17275" citStr="Ding, 2011" startWordPosition="2744" endWordPosition="2745">etail in the previous work, so we give only a brief overview and describe our additions. The purpose of tag dictionary expansion is to estimate label distributions for tokens in a raw cor586 pus, including words missing in the annotations. For this, a graph connecting annotated words to unannotated words via features is constructed and POS labels are pushed between these items using label propagation (LP) (Talukdar and Crammer, 2009). LP has been used successfully for extending POS labels from high-resource languages to low via parallel corpora (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ding, 2011) or high- to low-resource domains (Subramanya et al., 2010), among other tasks. These works have typically used n-gram features (capturing basic syntax) and character affixes (basic morphology). The character n-gram affix-as-morphology approach produces many features, but only a fraction of them represent actual morphemes. Incorrect features end up pushing noise around the graph, so affixes can lead to more false labels that drown out the true labels. While affixes may be sufficient for languages with limited morphology, their effectiveness diminishes for morphology-rich languages, which have </context>
</contexts>
<marker>Ding, 2011</marker>
<rawString>Weiwei Ding. 2011. Weakly supervised part-ofspeech tagging for Chinese using label propagation. Master’s thesis, University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Typesupervised hidden Markov models for part-ofspeech tagging with incomplete tag dictionaries.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP, Jeju,</booktitle>
<contexts>
<context position="2874" citStr="Garrette and Baldridge, 2012" startWordPosition="434" endWordPosition="437"> significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed lexicon for Hebrew in order to l</context>
<context position="15828" citStr="Garrette and Baldridge, 2012" startWordPosition="2520" endWordPosition="2523">ble 3: Coverage of the English morphological FST during development. For brevity, showing 2- hour increments instead of 30-minute segments. tokens types cov. ambig. cov. ambig. KIN 86% 2.62 82% 5.31 MLG 78% 2.98 37% 1.13 ENG 91% 1.19 62% 1.97 Table 4: Coverage and ambiguity of the final FST for each language. 4 Approach Learning under low-resource conditions is more difficult than scenarios in most previous POS work because the vast majority of the word types in the training and test data are not covered by the annotations. When most words are unknown, learning algorithms such as EM struggle (Garrette and Baldridge, 2012). Recall that most work on learning POS-taggers from tag dictionaries used tag dictionaries culled from test sets (even when considering incomplete dictionaries). We thus build on our previous approach, which exploits extremely sparse, human-generated annotations that are produced without knowledge of which words appear in the test set (Garrette and Baldridge, 2013). This approach generalizes a small initial tag dictionary to include unannotated word types appearing in raw data. It estimates word/tag pair and tag-transition frequency information using modelminimization, which also reduces nois</context>
<context position="19976" citStr="Garrette and Baldridge, 2012" startWordPosition="3201" endWordPosition="3204">l set of tag bigrams needed to explain the sentences in the raw corpus. It outputs a corpus of tagged sentences, which are used as a good starting point for EM training of an HMM. The expanded tag dictionary constrains the EM search space by providing a limited tagset for each word type, steering EM towards a desirable result. Because the HMM trained by EM will contain zero-probabilities for words that did not appear in the training corpus, we use the “autosupervision” step from our previous work: a Maximum Entropy Markov Model tagger is trained on a corpus that is noisily labeled by the HMM (Garrette and Baldridge, 2012). While training an HMM before the MEMM is not strictly necessary, our tests have shown that this generativethen-discriminative combination generally results in around 3% accuracy improvement. 5 Experiments3 To better understand the effect that each type of supervision has on tagger accuracy, we perform a series of experiments, with KIN and MLG as true low-resource languages. English experiments, for which we had both experienced and novice annotators, allow for further exploration into issues concerning data collection and preparation. The overall best accuracies achieved by language are 81.9</context>
</contexts>
<marker>Garrette, Baldridge, 2012</marker>
<rawString>Dan Garrette and Jason Baldridge. 2012. Typesupervised hidden Markov models for part-ofspeech tagging with incomplete tag dictionaries. In Proceedings of EMNLP, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="5735" citStr="Garrette and Baldridge, 2013" startWordPosition="881" endWordPosition="884">and would take considerably more effort to produce than the monolingual resources that our annotators were able to generate in a four-hour timeframe. Of course, if they are available, such parallel text links could be incorporated into our approach. In our previous work, we developed a different strategy based on generalizing linguistic input with a computational model: linguists annotated either types or tokens for two hours, these annotations are projected onto a corpus of unlabeled tokens using label propagation and HMMs, and a final POS-tagger is trained on this larger autolabeled corpus (Garrette and Baldridge, 2013). That approach uses much more realistic types and quantities of resources than previous work; nonetheless, it leaves many open questions regarding the effectiveness of incrementally more annotation, the role of unannotated data, and whether there is a good balance to be found using a combination of type- and token-supervision. We also did not consider morphological analyzers as a form of type supervision, as suggested by Merialdo (1994). This paper addresses these questions via a series of experiments designed to quantify the effect on performance given by the amount of time spent finding or </context>
<context position="16196" citStr="Garrette and Baldridge, 2013" startWordPosition="2576" endWordPosition="2579"> difficult than scenarios in most previous POS work because the vast majority of the word types in the training and test data are not covered by the annotations. When most words are unknown, learning algorithms such as EM struggle (Garrette and Baldridge, 2012). Recall that most work on learning POS-taggers from tag dictionaries used tag dictionaries culled from test sets (even when considering incomplete dictionaries). We thus build on our previous approach, which exploits extremely sparse, human-generated annotations that are produced without knowledge of which words appear in the test set (Garrette and Baldridge, 2013). This approach generalizes a small initial tag dictionary to include unannotated word types appearing in raw data. It estimates word/tag pair and tag-transition frequency information using modelminimization, which also reduces noise introduced by automatic tag dictionary expansion. The approach exploits type annotations effectively to learn parameters for out-of-vocabulary words and infer missing frequency and sequence information. This pipeline is described in detail in the previous work, so we give only a brief overview and describe our additions. The purpose of tag dictionary expansion is </context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of NAACL, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings ACL.</booktitle>
<contexts>
<context position="3408" citStr="Goldberg et al. (2008)" startWordPosition="516" endWordPosition="519"> Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed lexicon for Hebrew in order to learn an HMM tagger. However, this lexicon was constructed by trained lexicographers over a long period of time and achieves very high coverage of the language with very good quality, much better than could be achieved by our non-expert linguistics graduate student annotators in just a few hours. Cucerzan and Yarowsky 583 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583–592, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (2002) learn a POS-tagge</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototypedriven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings NAACL.</booktitle>
<contexts>
<context position="4210" citStr="Haghighi and Klein (2006)" startWordPosition="640" endWordPosition="643">and achieves very high coverage of the language with very good quality, much better than could be achieved by our non-expert linguistics graduate student annotators in just a few hours. Cucerzan and Yarowsky 583 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583–592, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (2002) learn a POS-tagger from existing linguistic resources, namely a dictionary and a reference grammar, but these resources are not available, much less digitized, for most under-studied languages. Haghighi and Klein (2006) develop a model in which a POS-tagger is learned from a list of POS tags and just three “prototype” word types for each tag, but their approach requires a vector space to compute the distributional similarity between prototypes and other word types in the corpus. Such distributional models are not feasible for low-resource languages because they require immense amounts of raw text, much more than is available in these settings (Abney and Bird, 2010). Further, they extracted their prototype lists directly from a labeled corpus, something we are specifically avoiding. T¨ackstr¨om et al. (2013) </context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototypedriven learning for sequence models. In Proceedings NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2843" citStr="Hasan and Ng, 2009" startWordPosition="430" endWordPosition="433"> process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a manually-constructed l</context>
</contexts>
<marker>Hasan, Ng, 2009</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2009. Weakly supervised part-of-speech tagging for morphologically-rich, resource-scarce languages. In Proceedings of EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Applications of finite-state transducers in natural language processing.</title>
<date>2001</date>
<journal>Lecture Notes in Computer Science,</journal>
<contexts>
<context position="13049" citStr="Karttunen, 2001" startWordPosition="2069" endWordPosition="2070">in Table 2, are more interesting. On types, the experienced annotator maxed out at 32%, but the novice only reaches 11%. Moreover, the maximum for token annotations is much lower due to high repeat-annotation. The discrepancies between experienced and novice, and between type and token recall explain a great deal of the performance disparity seen in the experiments. 585 3 Morphological Transducers Finite-state transducers (FSTs) accept regular languages and can be constructed easily using regular expressions, which makes them quite useful for phonology, morphology and limited areas of syntax (Karttunen, 2001). Past work has used FSTs for direct POS-tagging (Roche and Schabes, 1995), but this requires tight coupling between the FST and target tagset. We use FSTs for morphological analysis: the FST accepts a word type and produces a set of morphological features. If there are multiple possible analyses for a given word type, the FST returns them all. For instance the Kinyarwanda verb sibatarazuka “he is not yet resurrected” is analyzed in several ways: • +NEG+CL2+1PL+V+arazuk+IMP • +NEG+CL2+NOT.YET+PRES+zuk+IMP • +NEG+CL2+NOT.YET+razuk+IMP FSTs are particularly valuable for their ability to analyze </context>
</contexts>
<marker>Karttunen, 2001</marker>
<rawString>Lauri Karttunen. 2001. Applications of finite-state transducers in natural language processing. Lecture Notes in Computer Science, 2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="1957" citStr="Kupiec, 1992" startWordPosition="288" endWordPosition="289">cularly difficult challenge for natural language processing tasks. For example, supervised learning methods can provide high accuracy for part-of-speech (POS) tagging (Manning, 2011), but they perform poorly when little supervision is available. Good results in weakly-supervised tagging have been obtained by training sequence models such as hidden Markov models (HMM) using the Expectation-Maximization algorithm (EM), however most work in this area has still relied on relatively large amounts of data, both annotated and unannotated, as well as an assumption that the annotations are very clean (Kupiec, 1992; Merialdo, 1994). The ability to learn taggers using very little data is enticing: only a tiny fraction of the world’s languages have enough data for standard supervised models to work well. The collection or development of resources is a time-consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native lan</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech &amp; Language, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP, Jeju Island,</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In Proceedings of EMNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
<date>2011</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<contexts>
<context position="1527" citStr="Manning, 2011" startWordPosition="219" endWordPosition="220">ect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available. 1 Introduction Low-resource languages present a particularly difficult challenge for natural language processing tasks. For example, supervised learning methods can provide high accuracy for part-of-speech (POS) tagging (Manning, 2011), but they perform poorly when little supervision is available. Good results in weakly-supervised tagging have been obtained by training sequence models such as hidden Markov models (HMM) using the Expectation-Maximization algorithm (EM), however most work in this area has still relied on relatively large amounts of data, both annotated and unannotated, as well as an assumption that the annotations are very clean (Kupiec, 1992; Merialdo, 1994). The ability to learn taggers using very little data is enticing: only a tiny fraction of the world’s languages have enough data for standard supervised</context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9423" citStr="Marcus et al., 1993" startWordPosition="1475" endWordPosition="1478">24 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178) 4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933) Table 1: Annotations for each language and annotator as time increases. Shows the number of tag dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses). For brevity, the table only shows hourly progress. enized and labeled with POS tags by two linguistics graduate students, each of which was studying one of the languages. The KIN and MLG data have 12 and 23 distinct POS tags, respectively. The Penn Treebank (PTB) (Marcus et al., 1993) is used as ENG data. Section 01 was used for token-supervised annotation, sections 02-14 were used as raw data, 15-18 for development of the FST, 19-21 as a dev set and 22-24 as a test set. The PTB uses 45 distinct POS tags. Collecting annotations Linguists with nonnative knowledge of KIN and MLG produced annotations for four hours (in 30-minute intervals) for two tasks. In the first task, type-supervision, the annotator was given a list of the words in the target language (ranked from most to least frequent), and they annotated each word type with its potential POS tags. The word types and f</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1974" citStr="Merialdo, 1994" startWordPosition="290" endWordPosition="291">ult challenge for natural language processing tasks. For example, supervised learning methods can provide high accuracy for part-of-speech (POS) tagging (Manning, 2011), but they perform poorly when little supervision is available. Good results in weakly-supervised tagging have been obtained by training sequence models such as hidden Markov models (HMM) using the Expectation-Maximization algorithm (EM), however most work in this area has still relied on relatively large amounts of data, both annotated and unannotated, as well as an assumption that the annotations are very clean (Kupiec, 1992; Merialdo, 1994). The ability to learn taggers using very little data is enticing: only a tiny fraction of the world’s languages have enough data for standard supervised models to work well. The collection or development of resources is a time-consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous w</context>
<context position="6176" citStr="Merialdo (1994)" startWordPosition="953" endWordPosition="954">ojected onto a corpus of unlabeled tokens using label propagation and HMMs, and a final POS-tagger is trained on this larger autolabeled corpus (Garrette and Baldridge, 2013). That approach uses much more realistic types and quantities of resources than previous work; nonetheless, it leaves many open questions regarding the effectiveness of incrementally more annotation, the role of unannotated data, and whether there is a good balance to be found using a combination of type- and token-supervision. We also did not consider morphological analyzers as a form of type supervision, as suggested by Merialdo (1994). This paper addresses these questions via a series of experiments designed to quantify the effect on performance given by the amount of time spent finding or annotating training materials. We specifically look at the impact of four types of data collection: 1. Time annotating sentences (token supervision) 2. Time creating tag dictionary (type supervision) 3. Time constructing a finite state transducer (FST) to analyze word-type morphology 4. Amount of raw data available for training We explore these strategies in the context of POStagging for Kinyarwanda and Malagasy. We also include experime</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>David Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proceedings ACL.</booktitle>
<contexts>
<context position="10723" citStr="Ngai and Yarowsky (2000)" startWordPosition="1695" endWordPosition="1698">not include the test sets. In the second task, token-supervision, full sentences were annotated with POS tags. The 30-minute intervals allow us to investigate the incremental benefit of additional annotation of each type as well as how both annotation types might be combined within a fixed annotation budget. Baldridge and Palmer (2009) found that annotator expertise greatly influences effectiveness of active learning for morpheme glossing, a related task. To see how differences in annotator speed and quality impact our task, we obtained ENG data from an experienced annotator and a novice one. Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. While we do not explore a rule-writing approach to POS-tagging, we do consider the impact of rule-based morphological analyzers as a component in our semisupervised POS-tagging system. ENG - Exp. ENG - Nov. time type tok type tok 1:00 0.05 0.03 0.01 0.02 2:00 0.15 0.05 0.03 0.03 3:00 0.24 0.06 0.07 0.05 4:00 0.32 0.08 0.11 0.06 Table 2: Tag dictionary recall against the test set for ENG annotators on type and token annotations. Annotations Table</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Grace Ngai and David Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In Proceedings ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-AFNLP.</booktitle>
<contexts>
<context position="2823" citStr="Ravi and Knight, 2009" startWordPosition="426" endWordPosition="429">consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al. (2008): they use a ma</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of ACL-AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Ashish Vaswani</author>
<author>Kevin Knight</author>
<author>David Chiang</author>
</authors>
<title>Fast, greedy model minimization for unsupervised tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="19325" citStr="Ravi et al. (2010)" startWordPosition="3084" endWordPosition="3087">pment and the final POS task. Thus, any FST for the language, regardless of its provenance, can be used with any target POS tagset. Since the LP graph contains a node for each corpus token, and each node is labeled with a distribution over POS tags, the graph provides a corpus of sentences labeled with noisy tag distributions along with an expanded tag dictionary. This output is useful as input to EM because it contains labels for all seen word types as well as sequence and frequency information. There is a high degree of noise in the LP output, so we employ the model minimization strategy of Ravi et al. (2010), which finds a minimal set of tag bigrams needed to explain the sentences in the raw corpus. It outputs a corpus of tagged sentences, which are used as a good starting point for EM training of an HMM. The expanded tag dictionary constrains the EM search space by providing a limited tagset for each word type, steering EM towards a desirable result. Because the HMM trained by EM will contain zero-probabilities for words that did not appear in the training corpus, we use the “autosupervision” step from our previous work: a Maximum Entropy Markov Model tagger is trained on a corpus that is noisil</context>
</contexts>
<marker>Ravi, Vaswani, Knight, Chiang, 2010</marker>
<rawString>Sujith Ravi, Ashish Vaswani, Kevin Knight, and David Chiang. 2010. Fast, greedy model minimization for unsupervised tagging. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
<author>Yves Schabes</author>
</authors>
<title>Deterministic part-of-speech tagging with finite-state transducers.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="13123" citStr="Roche and Schabes, 1995" startWordPosition="2079" endWordPosition="2082">tor maxed out at 32%, but the novice only reaches 11%. Moreover, the maximum for token annotations is much lower due to high repeat-annotation. The discrepancies between experienced and novice, and between type and token recall explain a great deal of the performance disparity seen in the experiments. 585 3 Morphological Transducers Finite-state transducers (FSTs) accept regular languages and can be constructed easily using regular expressions, which makes them quite useful for phonology, morphology and limited areas of syntax (Karttunen, 2001). Past work has used FSTs for direct POS-tagging (Roche and Schabes, 1995), but this requires tight coupling between the FST and target tagset. We use FSTs for morphological analysis: the FST accepts a word type and produces a set of morphological features. If there are multiple possible analyses for a given word type, the FST returns them all. For instance the Kinyarwanda verb sibatarazuka “he is not yet resurrected” is analyzed in several ways: • +NEG+CL2+1PL+V+arazuk+IMP • +NEG+CL2+NOT.YET+PRES+zuk+IMP • +NEG+CL2+NOT.YET+razuk+IMP FSTs are particularly valuable for their ability to analyze out-of-vocabulary items. By looking for known affixes, FSTs can guess the </context>
</contexts>
<marker>Roche, Schabes, 1995</marker>
<rawString>Emmanuel Roche and Yves Schabes. 1995. Deterministic part-of-speech tagging with finite-state transducers. Computational Linguistics, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semisupervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In Proceedings EMNLP,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="17334" citStr="Subramanya et al., 2010" startWordPosition="2751" endWordPosition="2754">brief overview and describe our additions. The purpose of tag dictionary expansion is to estimate label distributions for tokens in a raw cor586 pus, including words missing in the annotations. For this, a graph connecting annotated words to unannotated words via features is constructed and POS labels are pushed between these items using label propagation (LP) (Talukdar and Crammer, 2009). LP has been used successfully for extending POS labels from high-resource languages to low via parallel corpora (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ding, 2011) or high- to low-resource domains (Subramanya et al., 2010), among other tasks. These works have typically used n-gram features (capturing basic syntax) and character affixes (basic morphology). The character n-gram affix-as-morphology approach produces many features, but only a fraction of them represent actual morphemes. Incorrect features end up pushing noise around the graph, so affixes can lead to more false labels that drown out the true labels. While affixes may be sufficient for languages with limited morphology, their effectiveness diminishes for morphology-rich languages, which have much higher type-to-token ratios. More types means sparser </context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semisupervised learning of structured tagging models. In Proceedings EMNLP, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<booktitle>In Transactions of the ACL. Association for Computational Linguistics.</booktitle>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. In Transactions of the ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Pratim Talukdar</author>
<author>Koby Crammer</author>
</authors>
<title>New regularized algorithms for transductive learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ECML-PKDD,</booktitle>
<location>Bled, Slovenia.</location>
<contexts>
<context position="17101" citStr="Talukdar and Crammer, 2009" startWordPosition="2714" endWordPosition="2717">sion. The approach exploits type annotations effectively to learn parameters for out-of-vocabulary words and infer missing frequency and sequence information. This pipeline is described in detail in the previous work, so we give only a brief overview and describe our additions. The purpose of tag dictionary expansion is to estimate label distributions for tokens in a raw cor586 pus, including words missing in the annotations. For this, a graph connecting annotated words to unannotated words via features is constructed and POS labels are pushed between these items using label propagation (LP) (Talukdar and Crammer, 2009). LP has been used successfully for extending POS labels from high-resource languages to low via parallel corpora (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Ding, 2011) or high- to low-resource domains (Subramanya et al., 2010), among other tasks. These works have typically used n-gram features (capturing basic syntax) and character affixes (basic morphology). The character n-gram affix-as-morphology approach produces many features, but only a fraction of them represent actual morphemes. Incorrect features end up pushing noise around the graph, so affixes can lead to more false labels th</context>
</contexts>
<marker>Talukdar, Crammer, 2009</marker>
<rawString>Partha Pratim Talukdar and Koby Crammer. 2009. New regularized algorithms for transductive learning. In Proceedings of ECML-PKDD, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian LDA-based model for semi-supervised part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="2800" citStr="Toutanova and Johnson, 2008" startWordPosition="422" endWordPosition="425">pment of resources is a time-consuming and expensive process, creating a significant barrier for an under-studied language where there are few experts and little funding. It is thus important to develop approaches that achieve good accuracy based on the amount of data that can be reasonably obtained, for example, in just a few hours by a linguist doing fieldwork on a non-native language. Previous work explored learning taggers from weak information, but the type, amount, quality, and sources of data raise questions about the applicability of those results to real-world low-resource scenarios (Toutanova and Johnson, 2008; Ravi and Knight, 2009; Hasan and Ng, 2009; Garrette and Baldridge, 2012). Most research simulated weak supervision with tag dictionaries extracted from existing large, expertly-annotated corpora. These resources have been developed over long periods of time by trained annotators who collaborate to produce high-quality analyses. They are also biased towards including only the most likely tag for each word type, resulting in a cleaner dictionary than one would find in a real scenario. As such, these experiments do not reflect real-world constraints. One exception to this work is Goldberg et al</context>
</contexts>
<marker>Toutanova, Johnson, 2008</marker>
<rawString>Kristina Toutanova and Mark Johnson. 2008. A Bayesian LDA-based model for semi-supervised part-of-speech tagging. In Proceedings of NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>