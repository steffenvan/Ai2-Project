<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006086">
<title confidence="0.853303333333333">
Briefly Noted
Hidden Markov Models for Speech
Recognition
</title>
<author confidence="0.975333">
X. D. Huang, Y. Ariki, and M. A. Jack
</author>
<affiliation confidence="0.942616">
(University of Edinburgh)
Edinburgh: Edinburgh University Press
(Edinburgh Information Technology Series
</affiliation>
<bodyText confidence="0.990428240740741">
7, edited by S. Michaelson and
M. Steedman), 1990, x + 276 pp.
Hardbound, ISBN 0-7486-0162-7 (distributed
in North America by Columbia University
Press), $60.00
The art of automatic speech recognition has
advanced remarkably in the past decade.
With the advances in accuracy and scope,
there has come, for the time being, a strong
convergence on a class of statistical methods
based on a structure called a hidden Markov
model (HMM). HMM-based systems domi-
nate speech recognition, and success in the
speech domain has spawned many attempts
to extend HMM methods to related pattern-
recognition fields such as document recogni-
tion and handwriting recognition.
The book under review answers a clear
need. It introduces most of the theory
and techniques needed to build a com-
plete HMM-based speech recognition sys-
tem. Huang, Akin, and Jack use the first
90 pages to cover general methods of pat-
tern recognition, speech-signal processing,
and statistical language modeling. The next
90 pages cover signal quantization and the
theory of HMMs for speech recognition. The
last 60 pages cover practical issues with ex-
amples provided and explained.
The style of the book is very dry but
clear; it reads like an abstract of an engi-
neering text. The majority of the pages are
thick with equations, but the authors seem
to use only as much mathematics as will
be needed to actually implement the algo-
rithms. There are few derivations and no
proofs. The authors present the basic algo-
rithms and the best algorithms (in their judg-
ment), but offer neither historical perspec-
tive nor critical review of current research.
The book gives algorithm examples written
in a high-level English pseudo-code. These
examples are concrete and will be helpful for
a novice. The tone of the book is decidedly
practical, although it is too concise. The book
would need substantial expansion to work as
a graduate-level text.
In sum, for the reader-cum-system-builder
who is ready to implement an HMM speech
recognition system from the bottom up, this
is the best available book that I have seen.
Huang, Akin, and Jack have distilled and
presented the essential techniques of the
trade.---Jared Bernstein, SRI International
</bodyText>
<title confidence="0.4780185">
Conceptual Information Retrieval: A
Case Study in Adaptive Partial Parsing
</title>
<author confidence="0.696747">
Michael L. Mauldin
</author>
<affiliation confidence="0.5270305">
(Carnegie Mellon University)
Boston: Kluwer Academic Publishers (The
</affiliation>
<bodyText confidence="0.988929833333333">
Kluwer International Series in Engineering
and Computer Science: Natural Language
Processing and Machine Translation, edited
by Jaime Carbonell), 1991, xx + 215 pp.
Hardbound, ISBN 0-7923-9214-0, $62.50,
£43.25, Dfl 145.00
Michael Mauldin has attempted the diffi-
cult task of building a full-fledged informa-
tion retrieval system in the traditional de-
sign, but one with a language-understanding
flavor. His purpose is to demonstrate that
use of semantic knowledge will improve our
capability to retrieve information. And he
demonstrated his achievement by testing his
system, FERRET, against a Boolean retrieval
prototype of contemporary design. Unfor-
tunately, his effort is marred by the com-
monly recognized flaws of the IR experimen-
tal methodology. However, given the realities
of human interaction, he had little choice but
to employ that methodology to convince his
target audience.
For the computational linguist, the work is
of interest because it demonstrates the practi-
cality of recognized techniques. There is little
new here; however, in putting the pieces to-
gether, the author has provided us with con-
vincing evidence that a high-level parse can
be extremely useful for a number of activities
requiring analysis of text. Mauldin includes a
catalog of suggested uses in his last chapter.
FERRET employs an adapted version of
DeJong&apos;s (1979) FRUMP to parse articles
from UseNet concerning astronomy. The
parse is really a skimming of the text. Com-
plex bits are passed over. Then &amp;quot;sketchy
scripts&amp;quot; and case frames are compiled from
the resulting conceptual dependencies (CD)
(Schank et al. 1975). It was the author&apos;s pur-
pose to demonstrate that a wide range of tex-
tual domains could be successfully treated
in such a way, and the resulting representa-
</bodyText>
<page confidence="0.989944">
217
</page>
<note confidence="0.558482">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999954444444445">
tion reminds one of a topical index expressed
in frames. The technique, by his own ad-
mission, works best with &amp;quot;paragraph-sized
chunks of text&amp;quot;.
As well as the text-derived frames, FER-
RET allows access to Webster&apos;s Seventh New
Collegiate Dictionary in order to add to the
available real-world knowledge. A genetic-
like learning algorithm further enhances
knowledge at search time by offering a capa-
bility similar to relevance feedback in vector
searching. Test results indicate that although
the algorithm offers other capabilities, its
most effective use is to generalize concepts
when attempts at matching are to be im-
proved upon. Since the matching mechanism
is primitive, allowing only yes/no matches at
present, &apos;learning&apos; serves to amplify answers
by allowing adaptation of abstracts relevant
to the query. All relevance judgments were
made by the author.
Queries were solicited by questionnaire
from the UseNet group readers. There were
some constraints placed on the subject matter
in order to channel the questions toward the
material contained in the knowledge base,
again narrowing the domain of demonstra-
tion. And the contributors of the final ques-
tions were 23 in number. Their queries were
automatically translated into CD graphs and
the matching and learning retrieval process
was undertaken, comparisons being made
with the Boolean-keyword retrieval system
built as a control.
FERRET prevailed by proving to do some-
what better than the traditional system. But
it was, as its author himself recognized, just a
cut above the keyword system. The semantic
representation, although extremely robust,
was not suitably expressive to impress with
the power of representing meaning. It al-
lowed some advantage in dealing with prob-
lems of synonymy and ambiguity but was
not capable of coping with paraphrase. Both
the matcher and the knowledge representa-
tion are slated for further investigation. In the
meantime, IR people can point to a success-
ful implementation of a conceptual retrieval
system as a beacon, and computational lin-
guists can see a clear use for work that is all
too often regarded as the result of iconoclas-
tic academic pursuit, thanks to the author&apos;s
courage in attempting to implement a holis-
tic conceptual retrieval design as a one-man
</bodyText>
<affiliation confidence="0.571535">
operation.—Judith Dick, University of Maryland
</affiliation>
<sectionHeader confidence="0.898975" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.974916428571429">
Dejong, Gerald F. (1979). Skimming stories in
real time: An experiment in integrated
understanding. Doctoral dissertation,
Department of Computer Science, Yale
University.
Schank, Roger C.; Goldman, Neil M.; Rieger,
Charles J. III; and Riesbeck, Christopher
K. (1975). Conceptual Information Processing.
(Fundamental Studies in Computer
Science, Volume 3). North-Holland.
Research in Humanities Computing 1:
Selected Papers from the ALLC/ACH
Conference, Toronto, June 1989
Ian Lancashire (editor)
(University of Toronto)
Oxford: Clarendon Press (Research in
Humanities Computing Series, edited by
Susan Hockey and Nancy Ide), 1991, xvi
353 pp. Hardbound, ISBN 0-19-824251-4, no
price listed
&amp;quot;Research in Humanities Computing is an an-
</reference>
<bodyText confidence="0.990412333333333">
nual publication which represents the state
of the art in humanities computing. Each vol-
ume contains a selection of papers presented
at the joint annual conference of the Associa-
tion for Computers in the Humanities (ACH)
and the Association for Literary and Linguis-
tic Computing (ALLC). ACH and ALLC are
the two major associations for the use of com-
puters in scholarly research and teaching in
humanities disciplines such as archaeology,
art, history, languages, literature, music, and
philosophy. This, the first volume, contains
twenty papers from the Dynamic Text ACH–
ALLC Conference held in Toronto in June
1989.&amp;quot;—From the publisher&apos;s announcement
</bodyText>
<page confidence="0.99581">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.999413">Briefly Noted Hidden Markov Models for Speech Recognition</title>
<author confidence="0.999981">X D Huang</author>
<author confidence="0.999981">Y Ariki</author>
<author confidence="0.999981">M A Jack</author>
<affiliation confidence="0.959143333333333">(University of Edinburgh) Edinburgh: Edinburgh University Press (Edinburgh Information Technology Series</affiliation>
<note confidence="0.780787333333333">7, edited by S. Michaelson and M. Steedman), 1990, x + 276 pp. Hardbound, ISBN 0-7486-0162-7 (distributed</note>
<affiliation confidence="0.563063">in North America by Columbia University</affiliation>
<address confidence="0.645826">Press), $60.00</address>
<abstract confidence="0.98943175">The art of automatic speech recognition has advanced remarkably in the past decade. With the advances in accuracy and scope, there has come, for the time being, a strong convergence on a class of statistical methods based on a structure called a hidden Markov model (HMM). HMM-based systems dominate speech recognition, and success in the speech domain has spawned many attempts to extend HMM methods to related patternrecognition fields such as document recognition and handwriting recognition. The book under review answers a clear need. It introduces most of the theory and techniques needed to build a com- HMM-based speech recognition sys- Akin, and Jack use the first 90 pages to cover general methods of pattern recognition, speech-signal processing, and statistical language modeling. The next 90 pages cover signal quantization and the theory of HMMs for speech recognition. The last 60 pages cover practical issues with examples provided and explained. The style of the book is very dry but clear; it reads like an abstract of an engineering text. The majority of the pages are thick with equations, but the authors seem to use only as much mathematics as will be needed to actually implement the algorithms. There are few derivations and no proofs. The authors present the basic algorithms and the best algorithms (in their judgment), but offer neither historical perspective nor critical review of current research. The book gives algorithm examples written in a high-level English pseudo-code. These examples are concrete and will be helpful for a novice. The tone of the book is decidedly practical, although it is too concise. The book would need substantial expansion to work as a graduate-level text. In sum, for the reader-cum-system-builder who is ready to implement an HMM speech recognition system from the bottom up, this is the best available book that I have seen. Huang, Akin, and Jack have distilled and presented the essential techniques of the</abstract>
<affiliation confidence="0.98315">SRI International</affiliation>
<title confidence="0.727671">Conceptual Information Retrieval: A Case Study in Adaptive Partial Parsing</title>
<author confidence="0.999906">Michael L Mauldin</author>
<affiliation confidence="0.76968125">(Carnegie Mellon University) Boston: Kluwer Academic Publishers (The Kluwer International Series in Engineering and Computer Science: Natural Language</affiliation>
<abstract confidence="0.9808426875">Processing and Machine Translation, edited by Jaime Carbonell), 1991, xx + 215 pp. Hardbound, ISBN 0-7923-9214-0, $62.50, £43.25, Dfl 145.00 Michael Mauldin has attempted the difficult task of building a full-fledged information retrieval system in the traditional design, but one with a language-understanding flavor. His purpose is to demonstrate that use of semantic knowledge will improve our capability to retrieve information. And he demonstrated his achievement by testing his system, FERRET, against a Boolean retrieval prototype of contemporary design. Unfortunately, his effort is marred by the commonly recognized flaws of the IR experimental methodology. However, given the realities of human interaction, he had little choice but to employ that methodology to convince his target audience. For the computational linguist, the work is of interest because it demonstrates the practicality of recognized techniques. There is little new here; however, in putting the pieces together, the author has provided us with convincing evidence that a high-level parse can be extremely useful for a number of activities requiring analysis of text. Mauldin includes a catalog of suggested uses in his last chapter. FERRET employs an adapted version of DeJong&apos;s (1979) FRUMP to parse articles from UseNet concerning astronomy. The parse is really a skimming of the text. Complex bits are passed over. Then &amp;quot;sketchy scripts&amp;quot; and case frames are compiled from the resulting conceptual dependencies (CD) (Schank et al. 1975). It was the author&apos;s purpose to demonstrate that a wide range of textual domains could be successfully treated such a way, and the resulting representa- 217 Computational Linguistics Volume 19, Number 1 tion reminds one of a topical index expressed in frames. The technique, by his own admission, works best with &amp;quot;paragraph-sized chunks of text&amp;quot;. As well as the text-derived frames, FERallows access to Seventh New Dictionary order to add to the available real-world knowledge. A geneticlike learning algorithm further enhances knowledge at search time by offering a capability similar to relevance feedback in vector searching. Test results indicate that although the algorithm offers other capabilities, its most effective use is to generalize concepts when attempts at matching are to be improved upon. Since the matching mechanism is primitive, allowing only yes/no matches at present, &apos;learning&apos; serves to amplify answers by allowing adaptation of abstracts relevant to the query. All relevance judgments were made by the author. Queries were solicited by questionnaire from the UseNet group readers. There were some constraints placed on the subject matter in order to channel the questions toward the material contained in the knowledge base, again narrowing the domain of demonstration. And the contributors of the final questions were 23 in number. Their queries were automatically translated into CD graphs and the matching and learning retrieval process was undertaken, comparisons being made with the Boolean-keyword retrieval system built as a control. FERRET prevailed by proving to do somewhat better than the traditional system. But it was, as its author himself recognized, just a cut above the keyword system. The semantic representation, although extremely robust, was not suitably expressive to impress with the power of representing meaning. It allowed some advantage in dealing with problems of synonymy and ambiguity but was not capable of coping with paraphrase. Both the matcher and the knowledge representation are slated for further investigation. In the meantime, IR people can point to a successful implementation of a conceptual retrieval system as a beacon, and computational linguists can see a clear use for work that is all too often regarded as the result of iconoclastic academic pursuit, thanks to the author&apos;s courage in attempting to implement a holistic conceptual retrieval design as a one-man</abstract>
<affiliation confidence="0.752171">University of Maryland</affiliation>
<title confidence="0.589632">References</title>
<author confidence="0.501423">F Gerald</author>
<degree confidence="0.454685">real time: An experiment in integrated</degree>
<email confidence="0.640324">dissertation,</email>
<affiliation confidence="0.954413">Department of Computer Science, Yale University.</affiliation>
<author confidence="0.541124">Roger C Schank</author>
<author confidence="0.541124">Neil M Goldman</author>
<author confidence="0.541124">Charles J Rieger</author>
<author confidence="0.541124">Christopher Riesbeck</author>
<note confidence="0.859853">(1975). Information Processing. (Fundamental Studies in Computer Science, Volume 3). North-Holland. Research in Humanities Computing 1: Selected Papers from the ALLC/ACH</note>
<title confidence="0.516756">Toronto, June</title>
<author confidence="0.80241">Ian Lancashire</author>
<affiliation confidence="0.573582333333333">(University of Toronto) Oxford: Clarendon Press (Research in Humanities Computing Series, edited by</affiliation>
<abstract confidence="0.957442526315789">Hockey and Nancy Ide),1991, xvi 353 pp. Hardbound, ISBN 0-19-824251-4, no price listed in Humanities Computing an annual publication which represents the state of the art in humanities computing. Each volume contains a selection of papers presented at the joint annual conference of the Association for Computers in the Humanities (ACH) and the Association for Literary and Linguistic Computing (ALLC). ACH and ALLC are the two major associations for the use of computers in scholarly research and teaching in humanities disciplines such as archaeology, art, history, languages, literature, music, and philosophy. This, the first volume, contains twenty papers from the Dynamic Text ACH– ALLC Conference held in Toronto in June publisher&apos;s announcement</abstract>
<intro confidence="0.548725">218</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gerald F Dejong</author>
</authors>
<title>Skimming stories in real time: An experiment in integrated understanding.</title>
<date>1979</date>
<institution>Department of Computer Science, Yale University.</institution>
<note>Doctoral dissertation,</note>
<marker>Dejong, 1979</marker>
<rawString>Dejong, Gerald F. (1979). Skimming stories in real time: An experiment in integrated understanding. Doctoral dissertation, Department of Computer Science, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Neil M Goldman</author>
<author>Charles J Rieger</author>
<author>Christopher K Riesbeck</author>
</authors>
<date>1975</date>
<booktitle>Conceptual Information Processing. (Fundamental Studies in Computer Science,</booktitle>
<volume>3</volume>
<publisher>North-Holland.</publisher>
<contexts>
<context position="4156" citStr="Schank et al. 1975" startWordPosition="650" endWordPosition="653">of recognized techniques. There is little new here; however, in putting the pieces together, the author has provided us with convincing evidence that a high-level parse can be extremely useful for a number of activities requiring analysis of text. Mauldin includes a catalog of suggested uses in his last chapter. FERRET employs an adapted version of DeJong&apos;s (1979) FRUMP to parse articles from UseNet concerning astronomy. The parse is really a skimming of the text. Complex bits are passed over. Then &amp;quot;sketchy scripts&amp;quot; and case frames are compiled from the resulting conceptual dependencies (CD) (Schank et al. 1975). It was the author&apos;s purpose to demonstrate that a wide range of textual domains could be successfully treated in such a way, and the resulting representa217 Computational Linguistics Volume 19, Number 1 tion reminds one of a topical index expressed in frames. The technique, by his own admission, works best with &amp;quot;paragraph-sized chunks of text&amp;quot;. As well as the text-derived frames, FERRET allows access to Webster&apos;s Seventh New Collegiate Dictionary in order to add to the available real-world knowledge. A geneticlike learning algorithm further enhances knowledge at search time by offering a cap</context>
</contexts>
<marker>Schank, Goldman, Rieger, Riesbeck, 1975</marker>
<rawString>Schank, Roger C.; Goldman, Neil M.; Rieger, Charles J. III; and Riesbeck, Christopher K. (1975). Conceptual Information Processing. (Fundamental Studies in Computer Science, Volume 3). North-Holland.</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<booktitle>Research in Humanities Computing 1: Selected Papers from the ALLC/ACH Conference,</booktitle>
<location>Toronto,</location>
<marker>1989</marker>
<rawString>Research in Humanities Computing 1: Selected Papers from the ALLC/ACH Conference, Toronto, June 1989</rawString>
</citation>
<citation valid="false">
<booktitle>(University of Toronto) Oxford: Clarendon Press (Research in Humanities Computing Series, edited by Susan Hockey and Nancy Ide), 1991, xvi 353 pp. Hardbound, ISBN</booktitle>
<pages>0--19</pages>
<editor>Ian Lancashire (editor)</editor>
<note>no price listed</note>
<marker></marker>
<rawString>Ian Lancashire (editor) (University of Toronto) Oxford: Clarendon Press (Research in Humanities Computing Series, edited by Susan Hockey and Nancy Ide), 1991, xvi 353 pp. Hardbound, ISBN 0-19-824251-4, no price listed</rawString>
</citation>
<citation valid="false">
<title>Research in Humanities Computing is an an-</title>
<marker></marker>
<rawString>&amp;quot;Research in Humanities Computing is an an-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>