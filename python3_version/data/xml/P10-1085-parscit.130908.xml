<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9964265">
Improving Statistical Machine Translation with
Monolingual Collocation
</title>
<author confidence="0.999895">
Zhanyi Liu1, Haifeng Wang2, Hua Wu2, Sheng Li1
</author>
<affiliation confidence="0.999389">
1Harbin Institute of Technology, Harbin, China
</affiliation>
<address confidence="0.532053">
2Baidu.com Inc., Beijing, China
</address>
<email confidence="0.945576666666667">
zhanyiliu@gmail.com
{wanghaifeng, wu_hua}@baidu.com
lisheng@hit.edu.cn
</email>
<sectionHeader confidence="0.994322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870470588235">
This paper proposes to use monolingual
collocations to improve Statistical Ma-
chine Translation (SMT). We make use
of the collocation probabilities, which are
estimated from monolingual corpora, in
two aspects, namely improving word
alignment for various kinds of SMT sys-
tems and improving phrase table for
phrase-based SMT. The experimental re-
sults show that our method improves the
performance of both word alignment and
translation quality significantly. As com-
pared to baseline systems, we achieve ab-
solute improvements of 2.40 BLEU score
on a phrase-based SMT system and 1.76
BLEU score on a parsing-based SMT
system.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996345724137931">
Statistical bilingual word alignment (Brown et al.
1993) is the base of most SMT systems. As com-
pared to single-word alignment, multi-word
alignment is more difficult to be identified. Al-
though many methods were proposed to improve
the quality of word alignments (Wu, 1997; Och
and Ney, 2000; Marcu and Wong, 2002; Cherry
and Lin, 2003; Liu et al., 2005; Huang, 2009),
the correlation of the words in multi-word
alignments is not fully considered.
In phrase-based SMT (Koehn et al., 2003), the
phrase boundary is usually determined based on
the bi-directional word alignments. But as far as
we know, few previous studies exploit the collo-
cation relations of the words in a phrase. Some
This work was partially done at Toshiba (China) Research
and Development Center.
researches used soft syntactic constraints to pre-
dict whether source phrase can be translated to-
gether (Marton and Resnik, 2008; Xiong et al.,
2009). However, the constraints were learned
from the parsed corpus, which is not available
for many languages.
In this paper, we propose to use monolingual
collocations to improve SMT. We first identify
potentially collocated words and estimate collo-
cation probabilities from monolingual corpora
using a Monolingual Word Alignment (MWA)
method (Liu et al., 2009), which does not need
any additional resource or linguistic preprocess-
ing, and which outperforms previous methods on
the same experimental data. Then the collocation
information is employed to improve Bilingual
Word Alignment (BWA) for various kinds of
SMT systems and to improve phrase table for
phrase-based SMT.
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. A cept is the
set of source words that are connected to the
same target word (Brown et al., 1993). An
alignment between a source multi-word cept and
a target word is a many-to-one multi-word
alignment.
To improve phrase table, we calculate phrase
collocation probabilities based on word colloca-
tion probabilities. Then the phrase collocation
probabilities are used as additional features in
phrase-based SMT systems.
The evaluation results show that the proposed
method in this paper significantly improves mul-
ti-word alignment, achieving an absolute error
rate reduction of 29%. The alignment improve-
ment results in an improvement of 2.16 BLEU
score on phrase-based SMT system and an im-
provement of 1.76 BLEU score on parsing-based
SMT system. If we use phrase collocation proba-
bilities as additional features, the phrase-based
</bodyText>
<page confidence="0.467946">
825
</page>
<note confidence="0.9895665">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 825–833,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.978317333333333">
SMT performance is further improved by 0.24
BLEU score.
The paper is organized as follows: In section 2,
we introduce the collocation model based on the
MWA method. In section 3 and 4, we show how
to improve the BWA method and the phrase ta-
ble using collocation models respectively. We
describe the experimental results in section 5, 6
and 7. Lastly, we conclude in section 8.
</bodyText>
<sectionHeader confidence="0.996307" genericHeader="method">
2 Collocation Model
</sectionHeader>
<bodyText confidence="0.998911727272727">
Collocation is generally defined as a group of
words that occur together more often than by
chance (McKeown and Radev, 2000). A colloca-
tion is composed of two words occurring as ei-
ther a consecutive word sequence or an inter-
rupted word sequence in sentences, such as &amp;quot;by
accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. In this paper, we
use the MWA method (Liu et al., 2009) for col-
location extraction. This method adapts the bi-
lingual word alignment algorithm to monolingual
scenario to extract collocations only from mono-
lingual corpora. And the experimental results in
(Liu et al., 2009) showed that this method
achieved higher precision and recall than pre-
vious methods on the same experimental data.
In the MWA method, the similar algorithm to
bilingual word alignment is used to estimate the
parameters of the models, except that a word
cannot be aligned to itself.
Figure 1 shows an example of the potentially
collocated word pairs aligned by the MWA me-
thod.
</bodyText>
<equation confidence="0.9121488">
p(wj  |w) i freq w w
 ( , )

The team leader plays a key role in the project undertaking.
The team leader plays a key role in the project undertaking.
</equation>
<figureCaption confidence="0.993677">
Figure 1. MWA Example
</figureCaption>
<subsectionHeader confidence="0.999259">
2.2 Collocation probability
</subsectionHeader>
<bodyText confidence="0.99981">
Given the monolingual word aligned corpus, we
calculate the frequency of two words aligned in
the corpus, denoted as . We filtered
the aligned words occurring only once. Then the
probability for each aligned word pair is esti-
mated as follows:
</bodyText>
<equation confidence="0.878422333333333">
(2)
w
freq(wi , wj
)
i
2.1 Monolingual word alignment
</equation>
<bodyText confidence="0.999636454545454">
The monolingual corpus is first replicated to
generate a parallel corpus, where each sentence
pair consists of two identical sentences in the
same language. Then the monolingual word
alignment algorithm is employed to align the
potentially collocated words in the monolingual
sentences.
According to Liu et al. (2009), we employ the
MWA Model 3 (corresponding to IBM Model 3)
to calculate the probability of the monolingual
word alignment sequence, as shown in Eq. (1).
</bodyText>
<equation confidence="0.994980933333333">
i
 |)
w i
 t w w d
(  |) 
j a j
(  |, )
j a l
l
pMWA Model 3(S,A |S)n( 
i
(1)

1
l
</equation>
<bodyText confidence="0.996386111111111">
Where S = w, is a monolingual sentence, 0;
denotes the number of words that are aligned
with w; . Since a word never collocates with itself,
the alignment set is denoted as
A  {(i, ai)  |i  [1, l] &amp; ai  i} . Three kinds of prob-
abilities are involved in this model: word collo-
cation probability t(w; I waj) , position colloca-
tion probability d (j I a� ,1) and fertility probabili-
ty n(O, I w;) .
</bodyText>
<equation confidence="0.809128">
(3)
</equation>
<bodyText confidence="0.999957111111111">
In this paper, the words of collocation are
symmetric and we do not determine which word
is the head and which word is the modifier. Thus,
the collocation probability of two words is de-
fined as the average of both probabilities, as in
Eq. (4).
If we have multiple monolingual corpora to
estimate the collocation probabilities, we interpo-
late the probabilities as shown in Eq. (5).
</bodyText>
<equation confidence="0.9983605">
r(wi,wj)krk(wi,wj) (5)
k
</equation>
<bodyText confidence="0.988917">
akdenotes the interpolation coefficient for
the probabilities estimated on the l!&apos;&apos; corpus.
</bodyText>
<sectionHeader confidence="0.9284895" genericHeader="method">
3 Improving Statistical Bilingual Word
Alignment
</sectionHeader>
<bodyText confidence="0.9997662">
We use the collocation information to improve
both one-directional and bi-directional bilingual
word alignments. The alignment probabilities are
re-estimated by using the collocation probabili-
ties of words in the same cept.
</bodyText>
<figure confidence="0.5894315">
j
1
j
r(wi,wj) p(wi  |wj)p(wj  |wi) (4)
2
826
</figure>
<subsectionHeader confidence="0.989942">
3.1 Improving one-directional bilingual
word alignment
</subsectionHeader>
<bodyText confidence="0.99167425">
According to the BWA method, given a bilingual
sentence pair and , the optimal
alignment sequence between E and F can be
obtained as in Eq. (6).
</bodyText>
<equation confidence="0.741251">
(6)
</equation>
<bodyText confidence="0.998033777777778">
The method is implemented in a series of five
models (IBM Models). IBM Model 1 only em-
ploys the word translation model to calculate the
probabilities of alignments. In IBM Model 2,
both the word translation model and position dis-
tribution model are used. IBM Model 3, 4 and 5
consider the fertility model in addition to the
word translation model and position distribution
model. And these three models are similar, ex-
cept for the word distortion models.
One-to-one and many-to-one alignments could
be produced by using IBM models. Although the
fertility model is used to restrict the number of
source words in a cept and the position distortion
model is used to describe the correlation of the
positions of the source words, the quality of
many-to-one alignments is lower than that of
one-to-one alignments.
Intuitively, the probability of the source words
aligned to a target word is not only related to the
fertility ability and their relative positions, but
also related to lexical tokens of words, such as
common phrase or idiom. In this paper, we use
the collocation probability of the source words in
a cept to measure their correlation strength. Giv-
en source words aligned to , their
collocation probability is calculated as in Eq. (7).
</bodyText>
<equation confidence="0.99471">
 
1
i  i
</equation>
<bodyText confidence="0.999103285714286">
Here, f,.]k and f i]g denote the word and
g`h word in { fj [ I a &gt; . = i} • &gt; r(fi]kI [ fi]g ) denotes
the collocation probability of and , as
shown in Eq. (4).
Thus, the collocation probability of the align-
ment sequence of a sentence pair can be calcu-
lated according to Eq. (8).
</bodyText>
<equation confidence="0.987050666666667">
l
r(F,A|E)=IIr({fj |aj =i}) (8)
i=1
</equation>
<bodyText confidence="0.996959642857143">
Based on maximum entropy framework, we
combine the collocation model and the BWA
model to calculate the word alignment probabili-
ty of a sentence pair, as shown in Eq. (9).
Here, and denote features and
feature weights, respectively. We use two fea-
tures in this paper, namely alignment probabili-
ties and collocation probabilities.
Thus, we obtain the decision rule:
Based on the GIZA++ package1, we imple-
mented a tool for the improved BWA method.
We first train IBM Model 4 and collocation
model on bilingual corpus and monolingual cor-
pus respectively. Then we employ the hill-
climbing algorithm (Al-Onaizan et al., 1999) to
search for the optimal alignment sequence of a
given sentence pair, where the score of an align-
ment sequence is calculated as in Eq. (10).
We note that Eq. (8) only deals with many-to-
one alignments, but the alignment sequence of a
sentence pair also includes one-to-one align-
ments. To calculate the collocation probability of
the alignment sequence, we should also consider
the collocation probabilities of such one-to-one
alignments. To solve this problem, we use the
collocation probability of the whole source sen-
tence, , as the collocation probability of
one-word cept.
</bodyText>
<subsectionHeader confidence="0.877478">
3.2 Improving bi-directional bilingual word
alignments
</subsectionHeader>
<bodyText confidence="0.998268071428571">
In word alignment models implemented in GI-
ZA++, only one-to-one and many-to-one word
alignment links can be found. Thus, some multi-
word units cannot be correctly aligned. The
symmetrization method is used to effectively
overcome this deficiency (Och and Ney, 2003).
Bi-directional alignments are generally obtained
from source-to-target alignments and target-
to-source alignments , using some heuristic
rules (Koehn et al., 2005). This method ignores
the correlation of the words in the same align-
ment unit, so an alignment may include many
unrelated words2, which influences the perfor-
mances of SMT systems.
</bodyText>
<equation confidence="0.791157875">
1 http://www.fjoch.com/GIZA++.html
2 In our experiments, a multi-word unit may include up to
40 words.
 
2
k1 gk1
r(f f )
[i]k ° [i]g

1)
i * (i
r({
fj  |aj  i})

(7)
827
</equation>
<bodyText confidence="0.997021923076923">
In order to solve the above problem, we incor-
porate the collocation probabilities into the bi-
directional word alignment process.
Given alignment sets and . We can
obtain the union . The source
sentence can be segmented into cepts
f,&apos;&amp;quot;&apos; . The target sentence can also be seg-
mented into cepts . The words in the same
cept can be a consecutive word sequence or an
interrupted word sequence.
Finally, the optimal alignments between
f,&apos;&amp;quot; and can be obtained from using the
following decision rule.
</bodyText>
<equation confidence="0.9737495">
e f A
l m
&apos; &apos;
1 1
( , , )
 arg max { p(
e f r e r f
, ) ( ) (
 
1 2
 
i j i j
i j
ei *
e)) / 2
*
</equation>
<tableCaption confidence="0.983681">
Table 1. Statistics of training data
</tableCaption>
<bodyText confidence="0.999409571428571">
word pair calculated according to Eq. (4). For the
phrase only including one word, we set a fixed
collocation probability that is the average of the
collocation probabilities of the sentences on a
development set. These collocation probabilities
are incorporated into the phrase-based SMT sys-
tem as features.
</bodyText>
<equation confidence="0.845932454545455">
}
3
(11)
AAt (ei,fj)A
( , )
e f
 
)
e e f f
i j
wordswords
</equation>
<bodyText confidence="0.995359111111111">
Bilingual corpus 6.3M 8.5
models, besides the monolingual part
s of FBIS,
we also employ some other larger Chinese and
English monolingual corpora, namely, Chinese
Gigaword (LDC2007T38), English Gigaword
(LDC2007T07), UN corpus (LDC2004E12), Si-
norama corpus (LDC2005T10), as shown in Ta-
ble 1.
</bodyText>
<table confidence="0.49076">
Using these corpora, we got three kinds of col-
location models:
E E (p(e  |f)  p(f |
p    ||fj  |(12)
extracted fr
</table>
<bodyText confidence="0.994878">
om word aligned corpus, we calculate
the collocation probabilities of source phrase and
target phrase respectively, according to Eq. (13).
</bodyText>
<equation confidence="0.9964588">
(
1
n  n
1 j i 1
w n i   
1 ) 
(wi,w
2r
j)
n*(n 1)(13)
</equation>
<bodyText confidence="0.98604">
Here, w; denotes a phrase with n words;
r(wi , wj)denotes the collocation probability of a
an
d Ney (2000), except that we consider each
alignment as a sure link.
</bodyText>
<equation confidence="0.813346">
r
Here,r(f;) andr(eZ) denote the collocation
</equation>
<bodyText confidence="0.932305428571429">
probabilities of the words in the source language
and target language respectively, which are cal-
culated by using Eq. (7).p(eZ, f;) denotes the
word tran
slation probability that is calculated
according to Eq. (12). denotes the weights of
these probabilities.
</bodyText>
<table confidence="0.9245367">
p(e If) andp(f I e) are the source-to-target
and target-to-source translation probabilities
trained fr
om the word aligned bilingual corpus.
4 Improving Phrase Table
Phrase-based SMT system automatically extracts
bilingual phrase pairs from the word aligned bi-
lingual corpus. In such a system, an idiomatic
expression may be split into several fragments,
and the phrases may include irrelevant words. In
this paper, we use the collocation probability to
measure the possibility of words composing a
phrase.
For each bilingual phrase pair automatically
Corpora
Chinese
English
M
Additional monolingual 312M 203M
corpora
</table>
<sectionHeader confidence="0.849002" genericHeader="method">
5 Experiments on Word Alignment
</sectionHeader>
<subsectionHeader confidence="0.946327">
5.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.99933">
We use a bilingual corpus, FBIS (LDC2003E14),
to train the IBM models. To train the collocation
</bodyText>
<listItem confidence="0.990898">
CM-1: the training data is the additional mo-
nolingual corpora;
CM-2: the training data is either side of the bi-
lingual corpus;
CM-3: the interpolation of CM-1 and CM-2.
</listItem>
<bodyText confidence="0.999523">
To investigate the quality of the generated
word alignments, we randomly selected a subset
from the bilingual corpus as test set, including
500 sentence pairs. Then word alignments in the
subset were manually labeled, referring to the
guideline of the Chinese-to-English alignment
(LDC2006E93), but we made some modifica-
tions for the guideline. For example, if a preposi-
tion appears after a verb as a phrase aligned to
one single word in the corresponding sentence,
then they are glued together.
There are several different evaluation metrics
for word alignment (Ahrenberg et al., 2000). We
use precision (P), recall (R) and alignment error
ratio (AER), which are similar to those in Och
</bodyText>
<page confidence="0.699667">
828
</page>
<table confidence="0.999870666666667">
Experiments Single word alignments Multi-word alignments
P R AER P R AER
Baseline 0.77 0.45 0.43 0.23 0.71 0.65
Improved BWA methods CM-1 0.70 0.50 0.42 0.35 0.86 0.50
CM-2 0.73 0.48 0.42 0.36 0.89 0.49
CM-3 0.73 0.48 0.41 0.39 0.78 0.47
</table>
<tableCaption confidence="0.999803">
Table 2. English-to-Chinese word alignment results
</tableCaption>
<figure confidence="0.666257555555556">
中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。
g
R 
China&apos;s science and technology research has made achievements which have gained the attention of the people of the world .
中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就
2* |Sg Sr  | 
zhong-guo de ke-xue-ji-shu yan-jiu qu-de le xu-duo ling shi-ren zhu-mu de cheng-jiu .
china DE science and research obtain LE many let common attract DE achievement .
technology people attention
</figure>
<figureCaption confidence="0.944673333333333">
Figure 2. Example of the English-to-Chinese word alignments generated by the BWA method and
the improved BWA method using CM-3. &amp;quot; &amp;quot; denotes the alignments of our method; &amp;quot; &amp;quot; denotes
the alignments of the baseline method.
</figureCaption>
<equation confidence="0.528702333333333">
。
IJr
AER 1 |Sg||Sr |(16)
</equation>
<bodyText confidence="0.984548615384616">
Where, Sg and S, denote the automatically
generated alignments and the reference align-
ments.
In order to tune the interpolation coefficients
in Eq. (5) and the weights of the probabilities in
Eq. (11), we also manually labeled a develop-
ment set including 100 sentence pairs, in the
same manner as the test set. By minimizing the
AER on the development set, the interpolation
coefficients of the collocation probabilities on
CM-1 and CM-2 were set to 0.1 and 0.9. And the
weights of probabilities were set as z, = 0.6 ,
Z-2= 0.2 and Z3 = 0.2.
</bodyText>
<subsectionHeader confidence="0.996183">
5.2 Evaluation results
One-directional alignment results
</subsectionHeader>
<bodyText confidence="0.999892033333333">
To train a Chinese-to-English SMT system,
we need to perform both Chinese-to-English and
English-to-Chinese word alignment. We only
evaluate the English-to-Chinese word alignment
here. GIZA++ with the default settings is used as
the baseline method. The evaluation results in
Table 2 indicate that the performances of our
methods on single word alignments are close to
that of the baseline method. For multi-word
alignments, our methods significantly outper-
form the baseline method in terms of both preci-
sion and recall, achieving up to 18% absolute
error rate reduction.
Although the size of the bilingual corpus is
much smaller than that of additional monolingual
corpora, our methods using CM-1 and CM-2
achieve comparable performances. It is because
CM-2 and the BWA model are derived from the
same resource. By interpolating CM1 and CM2,
i.e. CM-3, the error rate of multi-word alignment
results is further reduced.
Figure 2 shows an example of word alignment
results generated by the baseline method and the
improved method using CM-3. In this example,
our method successfully identifies many-to-one
alignments such as &amp;quot;the people of the world
世人&amp;quot;. In our collocation model, the collocation
probability of &amp;quot;the people of the world&amp;quot; is much
higher than that of &amp;quot;people world&amp;quot;. And our me-
thod is also effective to prevent the unrelated
</bodyText>
<page confidence="0.738355">
829
</page>
<table confidence="0.9998685">
Experiments Single word alignments Multi-word alignments All alignments
P R AER P R AER P R AER
Baseline 0.84 0.43 0.42 0.18 0.74 0.70 0.52 0.45 0.51
Our methods WA-1 0.80 0.51 0.37 0.30 0.89 0.55 0.58 0.51 0.45
WA-2 0.81 0.50 0.37 0.33 0.81 0.52 0.62 0.50 0.44
WA-3 0.78 0.56 0.34 0.44 0.88 0.41 0.63 0.54 0.40
</table>
<tableCaption confidence="0.998679">
Table 3. Bi-directional word alignment results
</tableCaption>
<bodyText confidence="0.973443857142857">
words from being aligned. For example, in the
baseline alignment &amp;quot;has made ... have rX得&amp;quot;,
&amp;quot;have&amp;quot; and &amp;quot;has&amp;quot; are unrelated to the target word,
while our method only generated &amp;quot;made rX
得&amp;quot;, this is because that the collocation probabili-
ties of &amp;quot;has/have&amp;quot; and &amp;quot;made&amp;quot; are much lower
than that of the whole source sentence.
</bodyText>
<subsectionHeader confidence="0.85361">
Bi-directional alignment results
</subsectionHeader>
<bodyText confidence="0.943189484848485">
We build a bi-directional alignment baseline
in two steps: (1) GIZA++ is used to obtain the
source-to-target and target-to-source alignments;
(2) the bi-directional alignments are generated by
using &amp;quot;grow-diag-final&amp;quot;. We use the methods
proposed in section 3 to replace the correspond-
ing steps in the baseline method. We evaluate
three methods:
WA-1: one-directional alignment method pro-
posed in section 3.1 and grow-diag-final;
WA-2: GIZA++ and the bi-directional bilin-
gual word alignments method proposed in
section 3.2;
WA-3: both methods proposed in section 3.
Here, CM-3 is used in our methods. The re-
sults are shown in Table 3.
We can see that WA-1 achieves lower align-
ment error rate as compared to the baseline me-
thod, since the performance of the improved one-
directional alignment method is better than that
of GIZA++. This result indicates that improving
one-directional word alignment results in bi-
directional word alignment improvement.
The results also show that the AER of WA-2
is lower than that of the baseline. This is because
the proposed bi-directional alignment method
can effectively recognize the correct alignments
from the alignment union, by leveraging colloca-
tion probabilities of the words in the same cept.
Our method using both methods proposed in
section 3 produces the best alignment perfor-
mance, achieving 11% absolute error rate reduc-
tion.
</bodyText>
<table confidence="0.999081363636364">
Experiments BLEU (%)
Baseline 29.62
CM-1 30.85
WA-1 CM-2 31.28
CM-3 31.48
CM-1 31.00
Our methods WA-2 CM-2 31.33
CM-3 31.51
CM-1 31.43
WA-3 CM-2 31.62
CM-3 31.78
</table>
<tableCaption confidence="0.923468333333333">
Table 4. Performances of Moses using the dif-
ferent bi-directional word alignments (Signifi-
cantly better than baseline with &lt; 0.01)
</tableCaption>
<sectionHeader confidence="0.892042" genericHeader="method">
6 Experiments on Phrase-Based SMT
</sectionHeader>
<subsectionHeader confidence="0.996223">
6.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.9999948125">
We use FBIS corpus to train the Chinese-to-
English SMT systems. Moses (Koehn et al., 2007)
is used as the baseline phrase-based SMT system.
We use SRI language modeling toolkit (Stolcke,
2002) to train a 5-gram language model on the
English sentences of FBIS corpus. We used the
NIST MT-2002 set as the development set and
the NIST MT-2004 test set as the test set. And
Koehn&apos;s implementation of minimum error rate
training (Och, 2003) is used to tune the feature
weights on the development set.
We use BLEU (Papineni et al., 2002) as eval-
uation metrics. We also calculate the statistical
significance differences between our methods
and the baseline method by using paired boot-
strap re-sample method (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.969283">
6.2 Effect of improved word alignment on
phrase-based SMT
</subsectionHeader>
<bodyText confidence="0.999794666666667">
We investigate the effectiveness of the improved
word alignments on the phrase-based SMT sys-
tem. The bi-directional alignments are obtained
</bodyText>
<figure confidence="0.610291333333333">
830
T1: We must adopt effective measures in order to avoid problems .
我们 必须 采取 有效 措施 才能 避免 出 问题 。
wo-men bi-xu cai-qu you-xiao cuo-shi cai-neng bi-mian chu wen-ti .
we must use effective measure can avoid out problem .
T2: We must adopt effective measures can we avoid out of the question .
</figure>
<figureCaption confidence="0.9315695">
Figure 3. Example of the translations generated by the baseline system and the system where the
phrase collocation probabilities are added
</figureCaption>
<table confidence="0.999114">
Experiments BLEU (%)
Moses 29.62
+ Phrase collocation probability 30.47
+ Improved word alignments 32.02
+ Phrase collocation probability
</table>
<tableCaption confidence="0.89488">
Table 5. Performances of Moses employing
our proposed methods (Significantly better than
baseline with p &lt; 0.01)
</tableCaption>
<bodyText confidence="0.994639214285714">
using the same methods as those shown in Table
3. Here, we investigate three different collocation
models for translation quality improvement. The
results are shown in Table 4.
From the results of Table 4, it can be seen that
the systems using the improved bi-directional
alignments achieve higher quality of translation
than the baseline system. If the same alignment
method is used, the systems using CM-3 got the
highest BLEU scores. And if the same colloca-
tion model is used, the systems using WA-3
achieved the higher scores. These results are
consistent with the evaluations of word align-
ments as shown in Tables 2 and 3.
</bodyText>
<subsectionHeader confidence="0.969793">
6.3 Effect of phrase collocation probabili-
ties
</subsectionHeader>
<bodyText confidence="0.999956902439024">
To investigate the effectiveness of the method
proposed in section 4, we only use the colloca-
tion model CM-3 as described in section 5.1. The
results are shown in Table 5. When the phrase
collocation probabilities are incorporated into the
SMT system, the translation quality is improved,
achieving an absolute improvement of 0.85
BLEU score. This result indicates that the collo-
cation probabilities of phrases are useful in de-
termining the boundary of phrase and predicting
whether phrases should be translated together,
which helps to improve the phrase-based SMT
performance.
Figure 3 shows an example: T1 is generated
by the system where the phrase collocation prob-
abilities are used and T2 is generated by the
baseline system. In this example, since the collo-
cation probability of &amp;quot;出 问题&amp;quot; is much higher
than that of &amp;quot;问题 。&amp;quot;, our method tends to split
&amp;quot;出 问题 。&amp;quot; into &amp;quot;(出 问题) (。)&amp;quot;, rather than
&amp;quot;(出) (问题 。)&amp;quot;. For the phrase &amp;quot;才能 避免&amp;quot; in
the source sentence, the collocation probability
of the translation &amp;quot;in order to avoid&amp;quot; is higher
than that of the translation &amp;quot;can we avoid&amp;quot;. Thus,
our method selects the former as the translation.
Although the phrase &amp;quot;我们 必须 采取 有效
施&amp;quot; in the source sentence has the same transla-
tion &amp;quot;We must adopt effective measures&amp;quot;, our
method splits this phrase into two parts &amp;quot;我们
须&amp;quot; and &amp;quot;采取 有效 措施&amp;quot;, because two parts
have higher collocation probabilities than the
whole phrase.
We also investigate the performance of the
system employing both the word alignment im-
provement and phrase table improvement me-
thods. From the results in Table 5, it can be seen
that the quality of translation is future improved.
As compared with the baseline system, an abso-
lute improvement of 2.40 BLEU score is
achieved. And this result is also better than the
results shown in Table 4.
</bodyText>
<sectionHeader confidence="0.986732" genericHeader="method">
7 Experiments on Parsing-Based SMT
</sectionHeader>
<bodyText confidence="0.9996711">
We also investigate the effectiveness of the im-
proved word alignments on the parsing-based
SMT system, Joshua (Li et al., 2009). In this sys-
tem, the Hiero-style SCFG model is used
(Chiang, 2007), without syntactic information.
The rules are extracted only based on the FBIS
corpus, where words are aligned by &amp;quot;MW-3 &amp;
CM-3&amp;quot;. And the language model is the same as
that in Moses. The feature weights are tuned on
the development set using the minimum error
</bodyText>
<page confidence="0.513933">
831
</page>
<table confidence="0.981736">
Experiments BLEU (%)
Joshua 30.05
+ Improved word alignments 31.81
</table>
<bodyText confidence="0.888444666666667">
Table 6. Performances of Joshua using the dif-
ferent word alignments (Significantly better than
baseline with p &lt; 0.01)
rate training method. We use the same evaluation
measure as described in section 6.1.
The translation results on Joshua are shown in
Table 6. The system using the improved word
alignments achieves an absolute improvement of
1.76 BLEU score, which indicates that the im-
provements of word alignments are also effective
to improve the performance of the parsing-based
SMT systems.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999977">
We presented a novel method to use monolingual
collocations to improve SMT. We first used the
MWA method to identify potentially collocated
words and estimate collocation probabilities only
from monolingual corpora, no additional re-
source or linguistic preprocessing is needed.
Then the collocation information was employed
to improve BWA for various kinds of SMT sys-
tems and to improve phrase table for phrase-
based SMT.
To improve BWA, we re-estimate the align-
ment probabilities by using the collocation prob-
abilities of words in the same cept. To improve
phrase table, we calculate phrase collocation
probabilities based on word collocation probabil-
ities. Then the phrase collocation probabilities
are used as additional features in phrase-based
SMT systems.
The evaluation results showed that the pro-
posed method significantly improved word
alignment, achieving an absolute error rate re-
duction of 29% on multi-word alignment. The
improved word alignment results in an improve-
ment of 2.16 BLEU score on a phrase-based
SMT system and an improvement of 1.76 BLEU
score on a parsing-based SMT system. When we
also used phrase collocation probabilities as ad-
ditional features, the phrase-based SMT perfor-
mance is finally improved by 2.40 BLEU score
as compared with the baseline system.
</bodyText>
<sectionHeader confidence="0.943936" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.997610589285714">
Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein,
and Jorg Tiedemann. 2000. Evaluation of Word
Alignment Systems. In Proceedings of the Second
International Conference on Language Resources
and Evaluation, pp. 1255-1261.
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David Ya-
rowsky. 1999. Statistical Machine Translation. Fi-
nal Report. In Johns Hopkins University Workshop.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert. L. Mercer. 1993. The Ma-
thematics of Statistical Machine Translation: Pa-
rameter estimation. Computational Linguistics,
19(2): 263-311.
Colin Cherry and Dekang Lin. 2003. A Probability
Model to Improve Word Alignment. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pp. 88-95.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):
201-228.
Fei Huang. 2009. Confidence Measure for Word
Alignment. In Proceedings of the 47th Annual
Meeting of the ACL and the 4th IJCNLP, pp. 932-
940.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in
Natural Language Processing, pp. 388-395.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evalua-
tion. In Processings of the International Workshop
on Spoken Language Translation 2005.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. In Proceed-
ings of the Human Language Technology Confe-
rence and the North American Association for
Computational Linguistics, pp. 127-133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL, Poster and Demonstration Sessions, pp. 177-
180.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ga-
nitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Demonstration of Joshua: An Open Source
Toolkit for Parsing-based Machine Translation. In
Proceedings of the 47th Annual Meeting of the As-
</reference>
<page confidence="0.507694">
832
</page>
<reference confidence="0.999772679245283">
sociation for Computational Linguistics, Software
Demonstrations, pp. 25-28.
Yang Liu, Qun Liu, and Shouxun Lin. Log-linear
Models for Word Alignment. 2005. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pp. 459-466.
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
2009. Collocation Extraction Using Monolingual
Word Alignment Method. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pp. 487-495.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. In Proceedings of the 2002 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pp. 133-139.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrase-Based Transla-
tion. In Proceedings of the 46st Annual Meeting of
the Association for Computational Linguistics, pp.
1003-1011.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proceedings of
the 38th Annual Meeting of the Association for
Computational Linguistics, pp. 440-447.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Syste-
matic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1): 19-52.
Kishore Papineni, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th annual meeting of the Association
for Computational Linguistics, pp. 311-318.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language
Processing, pp. 901-904.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for
Phrase-Based Translation. In Proceedings of the
47th Annual Meeting of the ACL and the 4th
IJCNLP, pp. 315-323.
</reference>
<page confidence="0.964609">
833
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749041">
<title confidence="0.9992375">Improving Statistical Machine Translation with Monolingual Collocation</title>
<author confidence="0.996596">Haifeng Hua Sheng</author>
<affiliation confidence="0.999093">Institute of Technology, Harbin, China</affiliation>
<address confidence="0.968703">Inc., Beijing, China</address>
<email confidence="0.934267">zhanyiliu@gmail.com{wanghaifeng,wu_hua}@baidu.comlisheng@hit.edu.cn</email>
<abstract confidence="0.997570055555556">This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Ahrenberg</author>
<author>Magnus Merkel</author>
<author>Anna Sagvall Hein</author>
<author>Jorg Tiedemann</author>
</authors>
<title>Evaluation of Word Alignment Systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1255--1261</pages>
<contexts>
<context position="14700" citStr="Ahrenberg et al., 2000" startWordPosition="2464" endWordPosition="2467"> the interpolation of CM-1 and CM-2. To investigate the quality of the generated word alignments, we randomly selected a subset from the bilingual corpus as test set, including 500 sentence pairs. Then word alignments in the subset were manually labeled, referring to the guideline of the Chinese-to-English alignment (LDC2006E93), but we made some modifications for the guideline. For example, if a preposition appears after a verb as a phrase aligned to one single word in the corresponding sentence, then they are glued together. There are several different evaluation metrics for word alignment (Ahrenberg et al., 2000). We use precision (P), recall (R) and alignment error ratio (AER), which are similar to those in Och 828 Experiments Single word alignments Multi-word alignments P R AER P R AER Baseline 0.77 0.45 0.43 0.23 0.71 0.65 Improved BWA methods CM-1 0.70 0.50 0.42 0.35 0.86 0.50 CM-2 0.73 0.48 0.42 0.36 0.89 0.49 CM-3 0.73 0.48 0.41 0.39 0.78 0.47 Table 2. English-to-Chinese word alignment results 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 。 g R  China&apos;s science and technology research has made achievements which have gained the attention of the people of the world . 中国 的 科学技术 研究 取得 了 许多 令 世人 瞩目 的 成就 2* |Sg</context>
</contexts>
<marker>Ahrenberg, Merkel, Hein, Tiedemann, 2000</marker>
<rawString>Lars Ahrenberg, Magnus Merkel, Anna Sagvall Hein, and Jorg Tiedemann. 2000. Evaluation of Word Alignment Systems. In Proceedings of the Second International Conference on Language Resources and Evaluation, pp. 1255-1261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical Machine Translation. Final Report. In</title>
<date>1999</date>
<institution>Johns Hopkins University Workshop.</institution>
<contexts>
<context position="9669" citStr="Al-Onaizan et al., 1999" startWordPosition="1606" endWordPosition="1609"> i=1 Based on maximum entropy framework, we combine the collocation model and the BWA model to calculate the word alignment probability of a sentence pair, as shown in Eq. (9). Here, and denote features and feature weights, respectively. We use two features in this paper, namely alignment probabilities and collocation probabilities. Thus, we obtain the decision rule: Based on the GIZA++ package1, we implemented a tool for the improved BWA method. We first train IBM Model 4 and collocation model on bilingual corpus and monolingual corpus respectively. Then we employ the hillclimbing algorithm (Al-Onaizan et al., 1999) to search for the optimal alignment sequence of a given sentence pair, where the score of an alignment sequence is calculated as in Eq. (10). We note that Eq. (8) only deals with many-toone alignments, but the alignment sequence of a sentence pair also includes one-to-one alignments. To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, , as the collocation probability of one-word cept. 3.2 Improving bi-direc</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical Machine Translation. Final Report. In Johns Hopkins University Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A Probability Model to Improve Word Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1249" citStr="Cherry and Lin, 2003" startWordPosition="180" endWordPosition="183">w that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the const</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>201--228</pages>
<contexts>
<context position="24420" citStr="Chiang, 2007" startWordPosition="4088" endWordPosition="4089">o investigate the performance of the system employing both the word alignment improvement and phrase table improvement methods. From the results in Table 5, it can be seen that the quality of translation is future improved. As compared with the baseline system, an absolute improvement of 2.40 BLEU score is achieved. And this result is also better than the results shown in Table 4. 7 Experiments on Parsing-Based SMT We also investigate the effectiveness of the improved word alignments on the parsing-based SMT system, Joshua (Li et al., 2009). In this system, the Hiero-style SCFG model is used (Chiang, 2007), without syntactic information. The rules are extracted only based on the FBIS corpus, where words are aligned by &amp;quot;MW-3 &amp; CM-3&amp;quot;. And the language model is the same as that in Moses. The feature weights are tuned on the development set using the minimum error 831 Experiments BLEU (%) Joshua 30.05 + Improved word alignments 31.81 Table 6. Performances of Joshua using the different word alignments (Significantly better than baseline with p &lt; 0.01) rate training method. We use the same evaluation measure as described in section 6.1. The translation results on Joshua are shown in Table 6. The syst</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2): 201-228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Confidence Measure for Word Alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP,</booktitle>
<pages>932--940</pages>
<contexts>
<context position="1281" citStr="Huang, 2009" startWordPosition="188" endWordPosition="189">ce of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the par</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Fei Huang. 2009. Confidence Measure for Word Alignment. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 932-940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="20844" citStr="Koehn, 2004" startWordPosition="3488" endWordPosition="3489">the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 采取 有效 措施 才能 避免 出 问题 。 wo-men bi-xu cai-qu you-xiao cuo-shi cai-neng bi-mian chu wen-ti . we must use effective measure can avoid out problem . T2: We must adopt effective measures can we avoid out of the question . Figure 3. Example of the translations generated by the baseline system and the system where the phrase colloc</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Processings of the International Workshop on Spoken Language Translation</booktitle>
<contexts>
<context position="10730" citStr="Koehn et al., 2005" startWordPosition="1767" endWordPosition="1770">solve this problem, we use the collocation probability of the whole source sentence, , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments and targetto-source alignments , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words2, which influences the performances of SMT systems. 1 http://www.fjoch.com/GIZA++.html 2 In our experiments, a multi-word unit may include up to 40 words.   2 k1 gk1 r(f f ) [i]k ° [i]g  1) i * (i r({ fj |aj  i})  (7) 827 In order to solve the above problem, we incorporate the collocation probabilities into the bidirectional word alignment process. Given alignment sets and . We can obtain the union . The source sentence can be segmented into cepts f,&apos;&amp;quot;&apos; . Th</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Processings of the International Workshop on Spoken Language Translation 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1402" citStr="Koehn et al., 2003" startWordPosition="205" endWordPosition="208">ute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to impro</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="20220" citStr="Koehn et al., 2007" startWordPosition="3382" endWordPosition="3385">ilities of the words in the same cept. Our method using both methods proposed in section 3 produces the best alignment performance, achieving 11% absolute error rate reduction. Experiments BLEU (%) Baseline 29.62 CM-1 30.85 WA-1 CM-2 31.28 CM-3 31.48 CM-1 31.00 Our methods WA-2 CM-2 31.33 CM-3 31.51 CM-1 31.43 WA-3 CM-2 31.62 CM-3 31.78 Table 4. Performances of Moses using the different bi-directional word alignments (Significantly better than baseline with &lt; 0.01) 6 Experiments on Phrase-Based SMT 6.1 Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sam</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, Software Demonstrations,</booktitle>
<pages>25--28</pages>
<contexts>
<context position="24353" citStr="Li et al., 2009" startWordPosition="4074" endWordPosition="4077">ts have higher collocation probabilities than the whole phrase. We also investigate the performance of the system employing both the word alignment improvement and phrase table improvement methods. From the results in Table 5, it can be seen that the quality of translation is future improved. As compared with the baseline system, an absolute improvement of 2.40 BLEU score is achieved. And this result is also better than the results shown in Table 4. 7 Experiments on Parsing-Based SMT We also investigate the effectiveness of the improved word alignments on the parsing-based SMT system, Joshua (Li et al., 2009). In this system, the Hiero-style SCFG model is used (Chiang, 2007), without syntactic information. The rules are extracted only based on the FBIS corpus, where words are aligned by &amp;quot;MW-3 &amp; CM-3&amp;quot;. And the language model is the same as that in Moses. The feature weights are tuned on the development set using the minimum error 831 Experiments BLEU (%) Joshua 30.05 + Improved word alignments 31.81 Table 6. Performances of Joshua using the different word alignments (Significantly better than baseline with p &lt; 0.01) rate training method. We use the same evaluation measure as described in section 6.</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Demonstration of Joshua: An Open Source Toolkit for Parsing-based Machine Translation. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, Software Demonstrations, pp. 25-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Log-linear Models for Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<contexts>
<context position="1267" citStr="Liu et al., 2005" startWordPosition="184" endWordPosition="187">oves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learne</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. Log-linear Models for Word Alignment. 2005. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 459-466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhanyi Liu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Sheng Li</author>
</authors>
<title>Collocation Extraction Using Monolingual Word Alignment Method.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>487--495</pages>
<contexts>
<context position="2187" citStr="Liu et al., 2009" startWordPosition="327" endWordPosition="330">f the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for various kinds of SMT systems and to improve phrase table for phrase-based SMT. To improve BWA, we re-estimate the alignment probabilities by using the collocation probabilities of words in the same cept. A cept is the set of source words that are connected to the same target word (Brown et al., 1993). An alignment between a source multi-word cept and a target wo</context>
<context position="4402" citStr="Liu et al., 2009" startWordPosition="684" endWordPosition="687">l based on the MWA method. In section 3 and 4, we show how to improve the BWA method and the phrase table using collocation models respectively. We describe the experimental results in section 5, 6 and 7. Lastly, we conclude in section 8. 2 Collocation Model Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000). A collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as &amp;quot;by accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. In this paper, we use the MWA method (Liu et al., 2009) for collocation extraction. This method adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations only from monolingual corpora. And the experimental results in (Liu et al., 2009) showed that this method achieved higher precision and recall than previous methods on the same experimental data. In the MWA method, the similar algorithm to bilingual word alignment is used to estimate the parameters of the models, except that a word cannot be aligned to itself. Figure 1 shows an example of the potentially collocated word pairs aligned by the MWA method. p(wj |w)</context>
<context position="5812" citStr="Liu et al. (2009)" startWordPosition="921" endWordPosition="924">y Given the monolingual word aligned corpus, we calculate the frequency of two words aligned in the corpus, denoted as . We filtered the aligned words occurring only once. Then the probability for each aligned word pair is estimated as follows: (2) w freq(wi , wj ) i 2.1 Monolingual word alignment The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language. Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences. According to Liu et al. (2009), we employ the MWA Model 3 (corresponding to IBM Model 3) to calculate the probability of the monolingual word alignment sequence, as shown in Eq. (1). i |) w i  t w w d ( |)  j a j ( |, ) j a l l pMWA Model 3(S,A |S)n(  i (1)  1 l Where S = w, is a monolingual sentence, 0; denotes the number of words that are aligned with w; . Since a word never collocates with itself, the alignment set is denoted as A  {(i, ai) |i  [1, l] &amp; ai  i} . Three kinds of probabilities are involved in this model: word collocation probability t(w; I waj) , position collocation probability d (j I a� ,1) and</context>
</contexts>
<marker>Liu, Wang, Wu, Li, 2009</marker>
<rawString>Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A PhraseBased, Joint Probability Model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="1227" citStr="Marcu and Wong, 2002" startWordPosition="176" endWordPosition="179">perimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 200</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A PhraseBased, Joint Probability Model for Statistical Machine Translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pp. 133-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrase-Based Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1808" citStr="Marton and Resnik, 2008" startWordPosition="270" endWordPosition="273">7; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Align</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrase-Based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics, pp. 1003-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Collocations. In</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing,</booktitle>
<pages>507--523</pages>
<contexts>
<context position="4168" citStr="McKeown and Radev, 2000" startWordPosition="640" endWordPosition="643">ics, pages 825–833, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics SMT performance is further improved by 0.24 BLEU score. The paper is organized as follows: In section 2, we introduce the collocation model based on the MWA method. In section 3 and 4, we show how to improve the BWA method and the phrase table using collocation models respectively. We describe the experimental results in section 5, 6 and 7. Lastly, we conclude in section 8. 2 Collocation Model Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000). A collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as &amp;quot;by accident&amp;quot; or &amp;quot;take ... advice&amp;quot;. In this paper, we use the MWA method (Liu et al., 2009) for collocation extraction. This method adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations only from monolingual corpora. And the experimental results in (Liu et al., 2009) showed that this method achieved higher precision and recall than previous methods on the same experimental data. In the MWA method, the similar algo</context>
</contexts>
<marker>McKeown, Radev, 2000</marker>
<rawString>Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="1205" citStr="Och and Ney, 2000" startWordPosition="172" endWordPosition="175">e-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pp. 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="20565" citStr="Och, 2003" startWordPosition="3443" endWordPosition="3444">mances of Moses using the different bi-directional word alignments (Significantly better than baseline with &lt; 0.01) 6 Experiments on Phrase-Based SMT 6.1 Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 采取 有效 措施 才能 避免 出 问题 。 wo-men bi-xu cai-qu you</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10566" citStr="Och and Ney, 2003" startWordPosition="1746" endWordPosition="1749">ments. To calculate the collocation probability of the alignment sequence, we should also consider the collocation probabilities of such one-to-one alignments. To solve this problem, we use the collocation probability of the whole source sentence, , as the collocation probability of one-word cept. 3.2 Improving bi-directional bilingual word alignments In word alignment models implemented in GIZA++, only one-to-one and many-to-one word alignment links can be found. Thus, some multiword units cannot be correctly aligned. The symmetrization method is used to effectively overcome this deficiency (Och and Ney, 2003). Bi-directional alignments are generally obtained from source-to-target alignments and targetto-source alignments , using some heuristic rules (Koehn et al., 2005). This method ignores the correlation of the words in the same alignment unit, so an alignment may include many unrelated words2, which influences the performances of SMT systems. 1 http://www.fjoch.com/GIZA++.html 2 In our experiments, a multi-word unit may include up to 40 words.   2 k1 gk1 r(f f ) [i]k ° [i]g  1) i * (i r({ fj |aj  i})  (7) 827 In order to solve the above problem, we incorporate the collocation probabil</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1): 19-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20661" citStr="Papineni et al., 2002" startWordPosition="3459" endWordPosition="3462">tter than baseline with &lt; 0.01) 6 Experiments on Phrase-Based SMT 6.1 Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the effectiveness of the improved word alignments on the phrase-based SMT system. The bi-directional alignments are obtained 830 T1: We must adopt effective measures in order to avoid problems . 我们 必须 采取 有效 措施 才能 避免 出 问题 。 wo-men bi-xu cai-qu you-xiao cuo-shi cai-neng bi-mian chu wen-ti . we must use effective measure can avoid out problem </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th annual meeting of the Association for Computational Linguistics, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings for the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="20322" citStr="Stolcke, 2002" startWordPosition="3400" endWordPosition="3401">t alignment performance, achieving 11% absolute error rate reduction. Experiments BLEU (%) Baseline 29.62 CM-1 30.85 WA-1 CM-2 31.28 CM-3 31.48 CM-1 31.00 Our methods WA-2 CM-2 31.33 CM-3 31.51 CM-1 31.43 WA-3 CM-2 31.62 CM-3 31.78 Table 4. Performances of Moses using the different bi-directional word alignments (Significantly better than baseline with &lt; 0.01) 6 Experiments on Phrase-Based SMT 6.1 Experimental settings We use FBIS corpus to train the Chinese-toEnglish SMT systems. Moses (Koehn et al., 2007) is used as the baseline phrase-based SMT system. We use SRI language modeling toolkit (Stolcke, 2002) to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST MT-2002 set as the development set and the NIST MT-2004 test set as the test set. And Koehn&apos;s implementation of minimum error rate training (Och, 2003) is used to tune the feature weights on the development set. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using paired bootstrap re-sample method (Koehn, 2004). 6.2 Effect of improved word alignment on phrase-based SMT We investigate the</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="1186" citStr="Wu, 1997" startWordPosition="170" endWordPosition="171"> for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1 Introduction Statistical bilingual word alignment (Brown et al. 1993) is the base of most SMT systems. As compared to single-word alignment, multi-word alignment is more difficult to be identified. Although many methods were proposed to improve the quality of word alignments (Wu, 1997; Och and Ney, 2000; Marcu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (M</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>A Syntax-Driven Bracketing Model for Phrase-Based Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP,</booktitle>
<pages>315--323</pages>
<contexts>
<context position="1829" citStr="Xiong et al., 2009" startWordPosition="274" endWordPosition="277">cu and Wong, 2002; Cherry and Lin, 2003; Liu et al., 2005; Huang, 2009), the correlation of the words in multi-word alignments is not fully considered. In phrase-based SMT (Koehn et al., 2003), the phrase boundary is usually determined based on the bi-directional word alignments. But as far as we know, few previous studies exploit the collocation relations of the words in a phrase. Some This work was partially done at Toshiba (China) Research and Development Center. researches used soft syntactic constraints to predict whether source phrase can be translated together (Marton and Resnik, 2008; Xiong et al., 2009). However, the constraints were learned from the parsed corpus, which is not available for many languages. In this paper, we propose to use monolingual collocations to improve SMT. We first identify potentially collocated words and estimate collocation probabilities from monolingual corpora using a Monolingual Word Alignment (MWA) method (Liu et al., 2009), which does not need any additional resource or linguistic preprocessing, and which outperforms previous methods on the same experimental data. Then the collocation information is employed to improve Bilingual Word Alignment (BWA) for variou</context>
</contexts>
<marker>Xiong, Zhang, Aw, Li, 2009</marker>
<rawString>Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based Translation. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP, pp. 315-323.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>